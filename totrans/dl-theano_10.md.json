["```py\n_is_training = T.iscalar('is_training')\n_noise_x = T.matrix('noise_x')\ninputs = apply_dropout(_is_training, inputs, T.shape_padright(_noise_x.T))\n```", "```py\nembedding = shared_uniform(( config.vocab_size,config.hidden_size), config.init_scale)\nparams = [embedding]\ninputs = embedding[_input_data.T]\n```", "```py\n_lr = theano.shared(cast_floatX(config.learning_rate), 'lr')\n```", "```py\ndef model(inputs, _is_training, params, batch_size, hidden_size, drop_i, drop_s, init_scale, init_H_bias):\n    noise_i_for_H = get_dropout_noise((batch_size, hidden_size), drop_i)\n    i_for_H = apply_dropout(_is_training, inputs, noise_i_for_H)\n    i_for_H = linear.model(i_for_H, params, hidden_size, \n                   hidden_size, init_scale, bias_init=init_H_bias)\n\n    # Dropout noise for recurrent hidden state.\n    noise_s = get_dropout_noise((batch_size, hidden_size), drop_s)\n\n    def step(i_for_H_t, y_tm1, noise_s):\n        s_lm1_for_H = apply_dropout(_is_training,y_tm1, noise_s)\n        return T.tanh(i_for_H_t + linear.model(s_lm1_for_H, \n                  params, hidden_size, hidden_size, init_scale))\n\n    y_0 = shared_zeros((batch_size, hidden_size), name='h0')\n    y, _ = theano.scan(step, sequences=i_for_H, outputs_info=[y_0], non_sequences = [noise_s])\n\n    y_last = y[-1]\n    sticky_state_updates = [(y_0, y_last)]\n\n    return y, y_0, sticky_state_updates\n```", "```py\ndef model(inputs, _is_training, params, batch_size, hidden_size, drop_i, drop_s, init_scale, init_H_bias, tied_noise):\n    noise_i_for_i = get_dropout_noise((batch_size, hidden_size), drop_i)\n    noise_i_for_f = get_dropout_noise((batch_size, hidden_size), drop_i) if not tied_noise else noise_i_for_i\n    noise_i_for_c = get_dropout_noise((batch_size, hidden_size), drop_i) if not tied_noise else noise_i_for_i\n    noise_i_for_o = get_dropout_noise((batch_size, hidden_size), drop_i) if not tied_noise else noise_i_for_i\n\n    i_for_i = apply_dropout(_is_training, inputs, noise_i_for_i)\n    i_for_f = apply_dropout(_is_training, inputs, noise_i_for_f)\n    i_for_c = apply_dropout(_is_training, inputs, noise_i_for_c)\n    i_for_o = apply_dropout(_is_training, inputs, noise_i_for_o)\n\n    i_for_i = linear.model(i_for_i, params, hidden_size, hidden_size, init_scale, bias_init=init_H_bias)\n    i_for_f = linear.model(i_for_f, params, hidden_size, hidden_size, init_scale, bias_init=init_H_bias)\n    i_for_c = linear.model(i_for_c, params, hidden_size, hidden_size, init_scale, bias_init=init_H_bias)\n    i_for_o = linear.model(i_for_o, params, hidden_size, hidden_size, init_scale, bias_init=init_H_bias)\n\n    # Dropout noise for recurrent hidden state.\n    noise_s = get_dropout_noise((batch_size, hidden_size), drop_s)\n    if not tied_noise:\n      noise_s = T.stack(noise_s, get_dropout_noise((batch_size, hidden_size), drop_s),\n get_dropout_noise((batch_size, hidden_size), drop_s), get_dropout_noise((batch_size, hidden_size), drop_s))\n\n    def step(i_for_i_t,i_for_f_t,i_for_c_t,i_for_o_t, y_tm1, c_tm1, noise_s):\n        noise_s_for_i = noise_s if tied_noise else noise_s[0]\n        noise_s_for_f = noise_s if tied_noise else noise_s[1]\n        noise_s_for_c = noise_s if tied_noise else noise_s[2]\n        noise_s_for_o = noise_s if tied_noise else noise_s[3]\n\n        s_lm1_for_i = apply_dropout(_is_training,y_tm1, noise_s_for_i)\n        s_lm1_for_f = apply_dropout(_is_training,y_tm1, noise_s_for_f)\n        s_lm1_for_c = apply_dropout(_is_training,y_tm1, noise_s_for_c)\n        s_lm1_for_o = apply_dropout(_is_training,y_tm1, noise_s_for_o)\n\n        i_t = T.nnet.sigmoid(i_for_i_t + linear.model(s_lm1_for_i, params, hidden_size, hidden_size, init_scale))\n        f_t = T.nnet.sigmoid(i_for_o_t + linear.model(s_lm1_for_f, params, hidden_size, hidden_size, init_scale))\n        c_t = f_t * c_tm1 + i_t * T.tanh(i_for_c_t + linear.model(s_lm1_for_c, params, hidden_size, hidden_size, init_scale))\n        o_t = T.nnet.sigmoid(i_for_o_t + linear.model(s_lm1_for_o, params, hidden_size, hidden_size, init_scale))\n        return o_t * T.tanh(c_t), c_t\n\n    y_0 = shared_zeros((batch_size,hidden_size), name='h0')\n    c_0 = shared_zeros((batch_size,hidden_size), name='c0')\n    [y, c], _ = theano.scan(step, sequences=[i_for_i,i_for_f,i_for_c,i_for_o], outputs_info=[y_0,c_0], non_sequences = [noise_s])\n\n  y_last = y[-1]\n    sticky_state_updates = [(y_0, y_last)]\n\n    return y, y_0, sticky_state_updates\n```", "```py\npython train_stacked.py --model=rnn\npython train_stacked.py --model=lstm\n\n```", "```py\ny_0 = shared_zeros((batch_size, hidden_size))\ny, _ = theano.scan(deep_step_fn, sequences = [i_for_H, i_for_T],\n            outputs_info = [y_0], non_sequences = [noise_s])\n```", "```py\ndef deep_step_fn(i_for_H_t, i_for_T_t, y_tm1, noise_s):\n  s_lm1 = y_tm1\n  for l in range(transition_depth):\n    if l == 0:\n      H = T.tanh(i_for_H_t + linear(s_lm1, params, hidden_size, hidden_size, init_scale))\n      Tr = T.nnet.sigmoid(i_for_T_t + linear(s_lm1, params, hidden_size, hidden_size, init_scale))\n    else:\n      H = T.tanh(linear(s_lm1, params, hidden_size, hidden_size, init_scale, bias_init=init_H_bias))\n      Tr = T.nnet.sigmoid(linear(s_lm1, params, hidden_size, hidden_size, init_scale, bias_init=init_T_bias))\n    s_l = H * Tr + s_lm1 * ( 1 - Tr )\n    s_lm1 = s_l\n  y_t = s_l\n  return y_t\n```", "```py\npython train_stacked.py\n```"]