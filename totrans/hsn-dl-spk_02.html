<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Learning Basics</h1>
                </header>
            
            <article>
                
<p>In this chapter, I am going to introduce the core concepts of <strong>Deep Learning</strong> (<strong>DL</strong>), the relationship it has with <strong>Machine Learning</strong> (<strong>ML</strong>) and <strong>Artificial Intelligence</strong> (<strong>AI</strong>), the different types of multilayered neural networks, and a list of real-world practical applications. I will try to skip mathematical equations as much as possible and keep the description very high level, with no reference to code examples. The goal of this chapter is to make readers aware of what DL really is and what you can do with it, while the following chapters will go much more into the details of this, with lots of practical code examples in Scala and Python (where this programming language can be used).</p>
<p><span>This chapter will cover the following topics:</span></p>
<ul>
<li><span>DL concepts</span></li>
<li><span><strong>Deep neural networks</strong> (<strong>DNNs</strong>)</span></li>
<li>Practical<span> </span>applications of DL</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing DL</h1>
                </header>
            
            <article>
                
<p>DL is a subset of ML that can solve particularly hard and large-scale problems in areas such as <strong>Natural Language Processing</strong> (<strong>NLP</strong>) and image classification. The expression DL is sometimes used in an interchangeable way with ML and AI, but both ML and DL are subsets of AI. AI is the broader concept that is implemented through ML. DL is a way of implementing ML, and involves neural network-based algorithms:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/127ce50b-0d72-4411-84c9-51b9f69d9eca.png" style="width:28.75em;height:22.42em;"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.1</div>
<p>AI is considered the ability of a machine (it could be any computer-controlled device or robot) to perform tasks that are typically associated with humans. It was introduced in the 1950s, with the goal of reducing human interaction, thereby making the machine do all the work. This concept is mainly applied to the development of systems that typically require human intellectual processes and/or the ability to learn from past experiences.</p>
<p>ML is an approach that's used to implement AI. It is a field of computer science that gives computer systems the ability to learn from data without being explicitly programmed. Basically, it uses algorithms to find patterns in data and then uses a model that recognizes those patterns to make predictions on new data. The following diagram shows the typical process that's used to train and build a model:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/889161c2-bb5f-492b-8a5b-3acdb5deb12d.png" style="width:39.33em;height:22.00em;"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.2</div>
<p>ML can be classified into three types:</p>
<ul>
<li>Supervised learning algorithms, which use labeled data</li>
<li>Unsupervised learning algorithms, which find patterns, starting from unlabeled data</li>
<li>Semi-supervised learning, which uses a mix of the two (labeled and unlabeled data)</li>
</ul>
<p>At the time of writing, supervised learning is the most common type of ML algorithm. Supervised learning can be divided into two groups <span>– </span>regression and classification problems.</p>
<p><span>The following graph shows a simple regression problem:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1001 image-border" src="assets/3c36b426-c6d8-4403-b913-662050ffd368.png" style="width:29.67em;height:17.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.3</div>
<p>As you can see, there are two inputs (or features), <strong>Size</strong> and <strong>Price</strong>, which are used to generate a curve-fitting line and make subsequent predictions of the property price.</p>
<p class="mce-root"><span>The following graph shows an example of supervised classification:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1002 image-border" src="assets/acc798aa-87f2-4f2c-98e4-145c0bcb6373.png" style="width:23.25em;height:17.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.4</div>
<p>The dataset is labeled with benign (circles) and malignant (crosses) tumors for breast cancer patients. A supervised classification algorithm attempts, by fitting a line through the data, to part the tumors into two different classifications. Future data would then be classified as benign or malignant based on that straight-line classification. The case in the preceding graph has only two discrete outputs, but there are cases where there could be more than two classifications as well</p>
<p>While in supervised learning, labeled datasets help the algorithm determine what the correct answer is, in unsupervised learning, an algorithm is provided with an unlabeled dataset and depends on the algorithm itself to uncover structures and patterns in the data. In the following graphs (the graph on the right can be found at <a href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/Images/supervised_unsupervised.png">https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/</a><a href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/Images/supervised_unsupervised.png">Images/supervised_unsupervised.png</a>), no information is provided about the meaning of each data point. We ask the algorithm to find a structure in the data in a way that is independent of supervision. An unsupervised learning algorithm could find that there are two distinct clusters and then perform straight-line classification between them:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1003 image-border" src="assets/d705080d-f433-4b30-af21-182ca64e5d72.png" style="width:13.67em;height:22.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.5</div>
<p><strong>DL</strong> is the name for multilayered neural networks, which are networks that are composed of several hidden layers of nodes between the input and output. DL is a refinement of <strong>Artificial Neural Networks</strong> (<strong>ANNs</strong>), which emulate how the human brain learns (even if not closely) and how it solves problems. ANNs consist of an interconnected group of neurons, similar to the way neurons work in the human brain. The following diagram represents the general model of ANNs:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5f6589c0-3379-4137-aac8-dcb67a14a15b.png" style="width:42.92em;height:24.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.6</div>
<p>A neuron is the atomic unit of an ANN. It receives a given number of input (<em>x<sub>i</sub></em>) before executing computation on it and finally sends the output to other neurons in the same network. The weights (<em>w<sub>j</sub></em>), or <em>parameters</em>, represent the strength of the input connection <span>– </span>they can assume positive or negative values. The net input can be calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em><span class="MathJax"><span class="math"><span class="mrow"><span class="msubsup"><span class="mi">y</span><span class="texatom"><sub>in</sub></span></span></span></span></span> = <span class="MathJax"><span class="math"><span class="mrow"><span class="msubsup"><span class="mi">x</span><sub><span class="texatom"><span class="mn">1</span></span></sub></span> <span class="mo">X</span> <span class="msubsup"><span class="mi">w</span><sub><span class="texatom"><span class="mn">1</span></span></sub></span> <span class="mo">+</span> <span class="msubsup"><span class="mi">x</span><sub><span class="texatom"><span class="mn">2</span></span></sub></span> <span class="mo">X</span> <span class="msubsup"><span class="mi">w</span><sub><span class="texatom"><span class="mn">2</span></span></sub></span> <span class="mo">+</span> <span class="msubsup"><span class="mi">x</span><sub><span class="texatom"><span class="mn">3</span></span></sub></span> <span class="mo">X</span> <span class="msubsup"><span class="mi">w<sub>3</sub></span> <span class="texatom"><span class="mn">+</span></span></span> <span class="mo">… +</span> <span class="msubsup"><span class="mi">x</span><span class="texatom"><span class="mi"><sub>n</sub> X</span></span></span> <span class="msubsup"><span class="mi">w</span><sub><span class="texatom"><span class="mi">n</span></span></sub></span></span></span></span></em></p>
<p>The output can be calculated by applying the activation function over the net input:</p>
<p class="CDPAlignCenter CDPAlign"><em>y = f(<span class="MathJax"><span class="math"><span class="mrow"><span class="msubsup"><span class="mi">y</span><span class="texatom"><sub>in</sub></span></span></span></span></span>)</em></p>
<p>The activation function allows an ANN to model complex non-linear patterns that simpler models may not represent correctly.</p>
<p class="mce-root"/>
<p>The following diagram represents a neural network:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/77e12f41-77b6-40ad-b0a9-6279e6220ca3.png" style="width:27.25em;height:11.58em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.7</div>
<p>The first layer is the input layer <span>– this</span> is where features are put into the network. The last one is the output layer. Any layer in between that is not an input or output layer is a hidden layer. The term DL is used because of the multiple levels of hidden layers in neural networks that are used to resolve complex non-linear problems. At each layer level, any single node receives input data and a weight, and will then output a confidence score to the nodes of the next layer. This process happens until the output layer is reached. The error of the score is calculated on that layer. The errors are then sent back and the weights of the network are adjusted to improve the model (this is called <strong>backpropagation</strong> and happens inside a process called <strong>gradient descent</strong>, which we will discuss in <a href="f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml" target="_blank">Chapter 6</a>, <em>Recurrent Neural Networks</em>). There are many variations of neural networks <span>– </span>more about them in the next section.</p>
<p>Before moving on, a final observation. You're probably wondering why most of the concepts behind AI, ML, and DL have been around for decades, but have only been hyped up in the past 4 or 5 years. There are several factors that accelerated their implementation and made it possible to move them from theory to real-world applications:</p>
<ul>
<li><strong>Cheaper computation</strong>: In the last few decades, hardware has been a constraining factor for AI/ML/DL. Recent advances in both hardware (coupled with improved tools and software frameworks) and new computational models (including those around GPUs) have accelerated AI/ML/DL adoption.</li>
<li><strong>Greater data availability</strong>: AI/ML/DL needs a huge amount of data to learn. The digital transformation of society is providing tons of raw material to move forward quickly. Big data now comes from diverse sources such as IoT sensors, social and mobile computing, smart cars, healthcare devices, and many others that are or will be used to train models. </li>
<li><strong>Cheaper storage</strong>: The increased amount of available data means that more space is needed for storage. Advances in hardware, cost reduction, and improved performance have made the implementation of new storage systems possible, all without the typical limitations of relational databases.</li>
<li><strong>More advanced algorithms</strong>: Less expensive computation and storage enable the development and training of more advanced algorithms that also have impressive accuracy when solving specific problems such as image classification and fraud detection.</li>
<li><strong>More, and bigger, investments</strong>: Last but not least, investment in AI is no longer confined to universities or research institutes, but comes from many other entities, such as tech giants, governments, start-ups, and large enterprises across almost every business area.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DNNs overview</h1>
                </header>
            
            <article>
                
<p>As stated in the previous section, a DNN is an ANN with multiple hidden layers between the input and output layers. Typically, they are feedforward networks in which data flows from the input layer to the output layer without looping back, but there are different flavors of DNNs <span>– a</span>mong them, those with the most practical applications are <strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>) and <strong>Recurrent Neural Networks</strong> (<strong>RNNs</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNNs</h1>
                </header>
            
            <article>
                
<p class="mce-root">The most common use case scenarios of CNNs are all to do with image processing, but are not restricted to other types of input, whether it be audio or video. A typical use case is image classification <span>– </span>the network is fed with images so that it can classify the data. For example, it outputs a lion if you give it a lion picture, a tiger when you give it a tiger picture, and so on. The reason why this kind of network is used for image classification is because it uses relatively little preprocessing compared to other algorithms in the same space <span>– </span>the network learns the filters that, in traditional algorithms, were hand-engineered.</p>
<p class="mce-root">Being a multilayered neural network, A CNN consists of an input and an output layer, as well as multiple hidden layers. The hidden layers can be convolutional, pooling, fully connected, and normalization layers. Convolutional layers apply a convolution operation (<a href="https://en.wikipedia.org/wiki/Convolution">https://en.wikipedia.org/wiki/Convolution</a>) to an input, before passing the result to the next layer. This operation emulates how the response of an individual physical neuron to a visual stimulus is generated. Each convolutional neuron processes only the data for its receptive field (which is the particular region of the sensory space of an individual sensory neuron in which a change in the environment will modify the firing of that neuron). Pooling layers are responsible for combining the outputs of clusters of neurons in a layer into a single neuron in the next layer. There are different implementations of poolings—max pooling, which uses the maximum value from each cluster from the prior layer; average pooling, which uses the average value from any cluster of neurons on the prior layer; and so on. Fully connected layers, instead, as you will clearly realize from their name, connect every neuron in a layer to every other neuron in another layer.</p>
<p class="mce-root">CNNs don't parse all the training data at once, but they usually start with a sort of input scanner. For example, consider an image of 200 x 200 pixels as input. In this case, the model doesn't have a layer with 40,000 nodes, but a scanning input layer of 20 x 20, which is fed using the first 20 x 20 pixels of the original image (usually, starting in the upper-left corner). Once we have passed that input (and possibly used it for training), we feed it using the next 20 x 20 pixels (this will be explained better and in a more detailed manner in <a href="fbec1d8a-a92f-4899-af0f-11f3d545e0eb.xhtml" target="_blank">Chapter 5</a>, <em>Convolutional Neural Networks</em>; the process is similar to the movement of a scanner, one pixel to the right). Please note that the image isn't dissected into 20 x 20 blocks, but the scanner moves over it. This input data is then fed through one or more convolutional layers. Each node of those layers only has to work with its close neighboring cells—not all of the nodes are connected to each other. The deeper a network becomes, the more its convolutional layers shrink, typically following a divisible factor of the input (if we started with a layer of 20, then, most probably, the next one would be a layer of 10 and the following a layer of 5). Powers of two are commonly used as divisible factors.</p>
<p class="mce-root">The following diagram (by Aphex34—own work, CC BY-SA 4.0, <a href="https://commons.wikimedia.org/w/index.php?curid=45679374">https://commons.wikimedia.org/w/index.php?curid=45679374</a>) shows the typical architecture of a CNN:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-826 image-border" src="assets/dccc596d-f722-451b-9a59-b5dac59e4f8f.png" style="width:38.67em;height:25.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.8</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RNNs</h1>
                </header>
            
            <article>
                
<p>RNNs are primarily popular for many NLP tasks (even if they are currently being used in different scenarios, which we will talk about in <a href="f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml" target="_blank">Chapter 6</a>, <em>Recurrent Neural Networks</em>). What's different about RNNs? Their peculiarity is that the connections between units form a directed graph along a sequence. This means that an RNN can exhibit a dynamic temporal behavior for a given time sequence. Therefore, they can use their internal state (memory) to process sequences of inputs, while in a traditional neural network, we assume that all inputs and outputs are independent of each other. This makes RNNs suitable for cases such as those, for example, when we want to predict the next word in a sentence <span>– </span>it is definitely better to know which words came before it. Now, you can understand why they are called recurrent <span>–</span> the same task is performed for every element of a sequence, with the output being dependent on the previous computations.</p>
<p>RNNs have loops in them, allowing information to persist, like so:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c18d6c33-bc28-4b76-8804-5d5ccff8990a.png" style="width:12.17em;height:15.33em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 2.9</div>
<p>In the preceding diagram, a chunk of the neural network, <strong>H</strong>, receives some input, <strong>x</strong> and outputs a value, <strong>o</strong>. A loop allows information to be passed from one step of the network to the next. By unfolding the RNN in this diagram into a full network (as shown in the following diagram), it can be thought of as multiple copies of the same network, each passing information to a successor:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1105 image-border" src="assets/582aac54-452c-4688-b416-f4fc1b84bce4.png" style="width:39.75em;height:15.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2.10</div>
<p>Here, <strong>x<sub>t</sub></strong> is the input at time step <strong>t</strong>, <strong>H<sub>t</sub></strong> is the hidden state at time step <strong>t</strong> (and represents the memory of the network), and <strong>o<sub>t</sub></strong> is the output at step <strong>t</strong>. The hidden states capture information about what happened in all the previous time steps. The output at a given step is calculated based only on the memory at time <strong>t</strong>. An RNN shares the same parameters across every step—that's because the same task is performed at each step; it just has different inputs—drastically reduces the total number of parameters it needs to learn. Outputs aren't necessary at each step, since this depends on the task at hand. Similarly, inputs aren't always needed at each time step.</p>
<p>RNNs were first developed in the 1980s and only lately have they come in many new variants. Here's a list of some of those architectures:</p>
<ul>
<li><strong>Fully recurrent</strong>: Every element has a weighted one-way connection to every other element in the architecture and has a single feedback connection to itself.</li>
<li><strong>Recursive</strong>: The same set of weights is applied recursively over a structure, which resembles a graph structure. During this process, the structure is traversed in topological sorting (<a href="https://en.wikipedia.org/wiki/Topological_sorting">https://en.wikipedia.org/wiki/Topological_sorting</a>).</li>
<li><strong>Hopfield</strong>: All of the connections are symmetrical. This is not suitable in scenarios where sequences of patterns need to be processed, as it requires stationary inputs only.</li>
<li><strong>Elman network</strong>: This is a three-layer network, arranged horizontally, plus a set of so-called <strong>context units</strong>. The middle hidden layer is connected to all of them, with a fixed weight of 1. What happens at each time step is that the input is fed forward and then a learning rule is applied. Because the back-connections are fixed, a copy of the previous values of the hidden units is saved in the context units. This is the way the network can maintain a state. For this reason, this kind of RNN allows you to perform tasks that are beyond the power of a standard multilayered neural network.</li>
<li><strong>Long short-term memory (LSTM)</strong>: This is a DL that prevents back-propagated errors from vanishing or exploding gradients (this will be covered in more detail in <a href="f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml" target="_blank">Chapter 6</a>, <em>Recurrent Neural Networks</em>). Errors can flow backward through (in theory) an unlimited number of virtual layers unfolded in space. This means that an LSTM can learn tasks that require memories of events that could have happened several time steps earlier.</li>
<li><strong>Bi-directional</strong>: By concatenating the outputs of two RNNs, it can predict each element of a finite sequence. The first RNN processes the sequence from left to right, while the second one does so in the opposite direction.</li>
<li><strong>Recurrent multilayer perceptron network</strong>: This consists of cascaded subnetworks, each containing multiple layers of nodes. Each subnetwork, except for the last layer (the only one that can have feedback connections), is feed-forward.</li>
</ul>
<p><a href="fbec1d8a-a92f-4899-af0f-11f3d545e0eb.xhtml" target="_blank">Chapter 5</a>, <em>Convolutional Neural Networks</em>, and <a href="f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml" target="_blank">Chapter 6</a>, <em>Recurrent Neural Networks</em>, will go into more detail about CNNs and RNNs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Practical applications of DL</h1>
                </header>
            
            <article>
                
<p>The DL concepts and models that were illustrated in the previous two sections aren't just pure theory <span>– </span>practical applications have been implemented from them. DL excels at identifying patterns in unstructured data; most use cases are related to media such as images, sound, video, and text. Nowadays, DL is applied in a number of use case scenarios across different business domains, such as the following:</p>
<ul>
<li><strong>Computer vision</strong>: A number of applications in the automotive industry, facial recognition, motion detection, and real-time threat detection</li>
<li><strong>NLP</strong>: Sentiment analysis in social media, fraud detection in finance and insurance, augmented search, and log analysis</li>
<li><strong>Medical diagnosis</strong>: Anomaly detection, pathology identification</li>
<li><strong>Search engines</strong>: Image searching</li>
<li><strong>IoT</strong>: Smart homes, predictive analysis using sensor data</li>
<li><strong>Manufacturing</strong>: Predictive maintenance</li>
<li><strong>Marketing</strong>: Recommendation engines, automated target identification</li>
<li><strong>Audio analysis</strong>: Speech recognition, voice searching, and machine translation</li>
</ul>
<p>There are many others that are yet to come.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, the basics of DL were introduced. This overview was kept very high-level to help readers who are new to this topic and prepare them to tackle the more detailed and hands-on topics that are covered in the following chapters.</p>


            </article>

            
        </section>
    </body></html>