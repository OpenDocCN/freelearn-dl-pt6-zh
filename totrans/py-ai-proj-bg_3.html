<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Applications for Comment Classification</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we'll overview the bag-of-words model for text classification. We will look at predicting YouTube comment spam with the bag-of-words and the random forest techniques. Then we'll look at the Word2Vec models and prediction of positive and negative reviews with the Word2Vec approach and the k-nearest neighbor classifier. </p>
<p class="calibre2">In this chapter, we will particularly focus on text and words and classify internet comments as spam or not spam or to identify internet reviews as positive or negative. We will also have an overview for bag of words for text classification and prediction model to predict YouTube comments are spam or not using bag of words and random forest techniques. We will also look at Word2Vec models an k-nearest neighbor classifier.</p>
<p class="calibre2">But, before we start, <span class="calibre5">we'll answer the following question: <em class="calibre16">w</em></span><em class="calibre16">hat makes text classification an interesting problem?</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Text classification</h1>
                
            
            <article>
                
<p class="calibre2">To find the answer to our question, we will consider the famous iris flower dataset as an example dataset. The following image is of iris versicolor species. To identify the species, we need some more information other than just an image of the species, such as the <span class="calibre5">flower's </span><strong class="calibre4">Petal length</strong><span class="calibre5">, </span><strong class="calibre4">Petal<span class="calibre5"> </span>width</strong><span class="calibre5">, </span><strong class="calibre4">Sepal length</strong><span class="calibre5">, and </span><strong class="calibre4">Sepal<span class="calibre5"> </span>width </strong>would help us identify the image better:</p>
<div class="cdpaligncenter"><img class="alignnone53" src="../images/00064.jpeg"/></div>
<p class="calibre2">The dataset not only contains examples of versicolor but also contains examples of setosa and virginica as well. Every example in the dataset contains these four measurements. The dataset contains around 150 examples, with 50 examples of each species. We can use a decision tree or any other model to predict the species of a new flower, if provided with the same four measurements. As we know same species will have almost similar measurements. Since similarity has different definition all together but here we consider similarity as the closeness on a graph, if we consider each point is a flower. The following graph is a comparison between sepal width versus petal width:</p>
<div class="cdpaligncenter"><img class="alignnone54" src="../images/00065.jpeg"/></div>
<p class="calibre2">If we had no way of measuring similarity, if, say, every flower had different measurements, then there'd be no way to use machine learning to build a classifier.</p>
<p class="calibre2">As we are aware of the fact that flowers of same species have same measurement and that helps us to distinguish different species. Consider what if every flower had different measurement, it would of no use to build classifier using machine learning to identify images of species.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Machine learning techniques</h1>
                
            
            <article>
                
<p class="calibre2">Before to that we considered images, let's now consider text. For example, consider the following sentences and try to find what makes the first pair of phrases similar to the second pair:</p>
<div class="cdpaligncenter"><img class="alignnone55" src="../images/00066.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2">I hope you got the answer to that question, otherwise we will not be able to build a decision tree, a random forest or anything else to predict the model. To answer the question, notice that the top pair of phrases are similar as they contain some words in common, such as <strong class="calibre4">subscribe</strong> and <strong class="calibre4">channel</strong>, while the second pair of sentences have fewer words in common, such as <strong class="calibre4">to </strong>and <strong class="calibre4">the</strong>. Consider the each phrase representing vector of numbers in a way that the top pair is similar to the numbers in the second pair. Only then we will be able to use random forest <span class="calibre5">or another technique for classification, in this case, to detect YouTube comment spam. To achieve this, we need to use the bag-of-words model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Bag of words</h1>
                
            
            <article>
                
<p class="calibre2">The bag-of-words model does exactly we want that is to convert the phrases or sentences and counts the number of times a similar word appears. In the world of computer science, a bag refers to a data structure that keeps track of objects like an array or list does, but in such cases the order does not matter and if an object appears more than once, we just keep track of the count rather we keep repeating them.</p>
<p class="calibre2">For example, consider the first phrase from the previous diagram, it has a bag of words that contents words such as <strong class="calibre4">channel</strong>, with one occurrence, <strong class="calibre4">plz</strong>, with one occurrence, <strong class="calibre4">subscribe</strong>, two occurrences, and so on. Then, we would collect all these counts in a vector, where one vector per phrase or sentence or document, depending on what you are working with. Again, the order in which the words appeared originally doesn't matter.</p>
<p class="calibre2">The vector that we created can also be used to sort data alphabetically, but it needs to be done consistently for all the different phrases. However, we still have the same problem. Each phrase has a vector with different columns, because each phrase has different words and a different number of columns, as shown in the following two tables:</p>
<div class="cdpaligncenter"><img class="alignnone56" src="../images/00067.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2">If we make a larger vector with all the unique words across both phrases, we get a proper matrix representation. With each row representing a different phrase, notice the use of <span class="calibre5">0</span> to indicate that a phrase doesn't have a word:</p>
<div class="cdpaligncenter"><img class="alignnone57" src="../images/00068.jpeg"/></div>
<p class="calibre2">If you want to have a bag of words with lots of phrases, documents, or  we would need to collect all the unique words that occur across all the examples and create a huge matrix, <em class="calibre16">N</em> x <em class="calibre16">M</em>, where <em class="calibre16">N</em> is the number of examples and <em class="calibre16">M</em> is the number of occurrences. We could easily have thousands of dimensions compared in a four-dimensional model for the iris dataset. The bag of words matrix is likely to be sparse, meaning mostly zeros, since most phrases don't have most words.</p>
<p class="calibre2">Before we start building our bag of words model, we need to take care of a few things, such as the following:</p>
<ul class="calibre10">
<li class="calibre11">Lowercase every word</li>
<li class="calibre11">Drop punctuation</li>
<li class="calibre11">Drop very common words (stop words)</li>
<li class="calibre11">Remove plurals (for example, bunnies =&gt; bunny)</li>
<li class="calibre11">Perform lemmatization (for example, reader =&gt; read, reading = read)</li>
<li class="calibre11">Use n-grams, such as bigrams (two-word pairs) or trigrams</li>
<li class="calibre11">Keep only frequent words (for example, must appear in &gt;10 examples)</li>
<li class="calibre11">Keep only the most frequent <em class="calibre21">M</em> words (for example, keep only 1,000)</li>
<li class="calibre11">Record binary counts (<em class="calibre21">1</em> = present, <em class="calibre21">0</em> = absent) rather than true counts</li>
</ul>
<p class="calibre2">There are many other combinations for best practice, and finding the best that suits the particular data needs some research.</p>
<p class="calibre2">The problem that we face with long documents is that they will have higher word counts generally, but we may still want to consider long documents about some topic to be considered, similar to a short document about the same topic, even though the word counts will differ significantly.</p>
<p class="calibre2">Furthermore, if we still wanted to reduce very common words and highlight the rare ones, what we would need to do is record the relative importance of each word rather than its raw count. This is known as <strong class="calibre4">term frequency inverse document frequency</strong> (<span class="calibre5"><strong class="calibre4">TF-IDF</strong>), </span>which measures how common a word or term is in the document. </p>
<p class="calibre2">We use logarithms to ensure that long documents with many words are very similar to short documents with similar words. TF-IDF has two components that multiply, that is when TF is high, the result is high but IDF measures how common the word is among all the documents and that will affect the common words. So, a word that is common in other documents will have a low score, regardless of how many times it appeared.</p>
<p class="calibre2">If a document has a low score which means the word appeared rarely and if the score is high it means the word appears frequently in the document. But if the word is quite common in all the documents then it becomes irrelevant to score on this document. It is anyhow considered to have low score. This shows that the formula for TF-IDF exhibits in a way we want our model to be. The following graph explains our theory:</p>
<div class="cdpaligncenter"><img class="alignnone58" src="../images/00069.jpeg"/></div>
<p class="calibre2">We will be using the bag-of-words method to detect whether YouTube comments are spam or .</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Detecting YouTube comment spam</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we're going to look at a technique for detecting YouTube comment spam using bags of words and random forests. The dataset is pretty straightforward. We'll use a dataset that has about 2,000 comments from popular YouTube videos (<a href="https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection" target="_blank" class="calibre9">https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection</a>). The dataset is formatted in a way where each row has a comment followed by a value marked as 1 or 0 for spam or not spam.</p>
<p class="calibre2">First, we will import a single dataset. This dataset is actually split into four different files. Our set of comments comes from the PSY-Gangnam Style video:</p>
<div class="cdpaligncenter"><img class="alignnone59" src="../images/00070.jpeg"/></div>
<p class="calibre2">Then we will print a few comments as follows:</p>
<div class="cdpaligncenter"><img class="alignnone60" src="../images/00071.jpeg"/></div>
<p class="calibre2">Here we are able to see that there are more than two columns, but we will only require the content and the class columns. The content column contains the comments and the class column contains the values 1 or 0 for spam or not spam. For example, notice that the first two comments are marked as not spam, but then the comment <strong class="calibre4">subscribe to me for call of duty vids</strong> is spam and <strong class="calibre4">hi guys please my android photo editor download yada yada</strong> is spam as well. Before we start sorting comments, let's look at the count of how many rows in the dataset are spam and how many are not spam. The result we acquired is <span class="calibre5">175 and </span>175 respectively, which sums up to 350 rows overall in this file:</p>
<div class="cdpaligncenter"><img class="alignnone61" src="../images/00072.gif"/></div>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">In scikit-learn, the bag of words technique is actually called <kbd class="calibre12">CountVectorizer</kbd>, which means counting how many times each word appears and puts them into a vector. To create a vector, we need to make an object for <kbd class="calibre12">CountVectorizer</kbd>, and then perform the fit and transform simultaneously:</p>
<div class="cdpaligncenter"><img class="alignnone62" src="../images/00073.jpeg"/></div>
<p class="calibre2">This performed in two different steps. First comes the fit step, where it discovers which words are present in the dataset, and second is the transform step, which gives you the bag of words matrix for those phrases. The result obtained in that matrix is 350 rows by 1,418 columns:</p>
<div class="cdpaligncenter"><img class="alignnone63" src="../images/00074.jpeg"/></div>
<p class="calibre2">There are 350 rows, which means we have 350 different comments and 1,418 words. 1418 word apparently are word that appear across all of these phrases. </p>
<p class="calibre2"><span class="calibre5">Now let's print a single comment and then run the analyzer on that comment so that we can see how well the phrases breaks it apart. As seen in the following screenshot, the comment has been printed first and then we are analyzing it below, which is just to see how it broke it into words:</span></p>
<div class="cdpaligncenter"><img class="alignnone64" src="../images/00075.jpeg"/></div>
<p class="calibre2">We can use the vectorizer feature to find out which word the dataset found after vectorizing. The following is the result found after vectorizing where it starts with numbers and ends with regular words:</p>
<div class="cdpaligncenter"><img class="alignnone65" src="../images/00076.gif"/></div>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Execute the following command to shuffle the dataset with fraction 100% that is adding <kbd class="calibre12">frac=1</kbd>:</p>
<div class="cdpaligncenter"><img class="alignnone66" src="../images/00077.gif"/></div>
<p class="calibre2">Now we will split the dataset into training and testing sets. Let's assume that the first 300 will be for training, while the latter 50 will be for testing:</p>
<div class="cdpaligncenter"><img class="alignnone67" src="../images/00078.jpeg"/></div>
<p class="calibre2">In the preceding code, <kbd class="calibre12">vectorizer.fit_transform(d_train['CONTENT'])</kbd> is an important step. At that stage, you have a training set that you want to perform a fit transform on, which means it will learn the words and also produce the matrix. However, for the testing set, we don't perform a fit transform again, since we don't want the model to learn different words for the testing data. We will use the same words that it learned on the training set. Suppose that the testing set has different words <span class="calibre5">out of which s</span>ome of them are unique to the testing set that might have never appeared in the training set. That's perfectly fine and anyhow we are going to ignore it. Because we are using the training set to build a random forest or decision tree or whatever would be the case, we have to use a certain set of words, and those words will have to be the same words, used on the testing set. We cannot introduce new words to the testing set since the random forest or any other model would not be able to gauge the new words.</p>
<p class="calibre2">Now we perform the transform on the dataset, and later we will use the answers for training and testing. The training set now has 300 rows and 1,287 different words or columns, and the testing set has 50 rows, but we have the same 1,287 columns:</p>
<div class="cdpaligncenter"><img class="alignnone68" src="../images/00079.gif"/></div>
<p class="calibre2"><span class="calibre5">Even though the testing set has different words, we need to make sure it is transformed in the same way as the training set with the same columns. </span>Now we will begin with the building of the random forest classifier. We will be converting this dataset into 80 different trees and we will fit the training set so that we can score its performance on the testing set:</p>
<div class="cdpaligncenter"><img class="alignnone69" src="../images/00080.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2">The output of the score that we received is 98%; that's really good. Here it seems it got confused between spam and not-spam. We need be sure that the accuracy is high; for that, we will perform a cross validation with five different splits. To perform a cross validation, we will use all the training data and let it split it into four different groups: 20%, 80%, and 20% will be testing data, and 80% will be the training data:</p>
<div class="cdpaligncenter"><img class="alignnone70" src="../images/00081.jpeg"/></div>
<p class="calibre2">We will now perform an average to the scores that we just obtained, which comes to about 95% accuracy. Now we will print all the data as seen in the following screenshot:</p>
<div class="cdpaligncenter"><img class="alignnone71" src="../images/00082.gif"/></div>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">The entire dataset has five different videos with comments, which means all together we have around 2,000 rows. On checking all the comments, we noticed that there are <kbd class="calibre12">1005</kbd> spam comments and <kbd class="calibre12">951</kbd> not-spam comments, that quite close enough to split it in to even parts:</p>
<div class="cdpaligncenter"><img class="alignnone72" src="../images/00083.gif"/></div>
<p class="calibre2">Here we will shuffle the entire dataset and separate the comments and the answers:</p>
<div class="cdpaligncenter"><img class="alignnone73" src="../images/00084.gif"/></div>
<p class="calibre2">We need to perform a couple of steps here with <kbd class="calibre12">CountVectorizer</kbd> followed by the random forest. For this, we will use a feature in scikit-learn called a <strong class="calibre4">Pipeline</strong>. Pipeline is really convenient and will bring together two or more steps so that all the steps are treated as one. So, we will build a pipeline with the bag of words, and then use <kbd class="calibre12">countVectorizer</kbd> followed by the random forest classifier. Then we will print the pipeline, and it the steps required:</p>
<div class="cdpaligncenter"><img class="alignnone74" src="../images/00085.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2">We can let the pipeline name of each step by itself by adding <kbd class="calibre12">CountVectorizer</kbd> <span class="calibre5">in our </span><kbd class="calibre12">RandomForestClassifier</kbd> and it will name them <kbd class="calibre12">CountVectorizer</kbd> and <kbd class="calibre12">RandomForestclassifier</kbd>:</p>
<div class="cdpaligncenter"><img class="alignnone75" src="../images/00086.jpeg"/></div>
<p class="calibre2">Once the pipeline is created you can just call it fit and it will perform the rest that is first it perform the fit and then transform with the <kbd class="calibre12">CountVectorizer</kbd>, followed by a fit with the <kbd class="calibre12">RandomForest</kbd> classifier. That's the benefit of having a pipeline:</p>
<div class="cdpaligncenter"><img class="alignnone76" src="../images/00087.jpeg"/></div>
<p class="calibre2">Now you call score so that it knows that when we are scoring it will to run it through the bag of words <kbd class="calibre12">countVectorizer</kbd>, followed by predicting with the <kbd class="calibre12">RandomForestClassifier</kbd>:</p>
<div class="cdpaligncenter"><img class="alignnone77" src="../images/00088.jpeg"/></div>
<p class="calibre2">This whole procedure will produce a score of about 94. We can only predict a single example with the pipeline. For example, imagine we have a new comment after the dataset has been trained, and we want to know whether the user has just typed this comment or whether it's spam:</p>
<div class="cdpaligncenter"><img class="alignnone78" src="../images/00089.jpeg"/></div>
<p class="calibre2">As seen, it's detected correctly; but what about the following comment:</p>
<div class="cdpaligncenter"><img class="alignnone79" src="../images/00090.jpeg"/></div>
<p class="calibre2">To overcome this and deploy this classifier into an environment and predict whether it is a <kbd class="calibre12">spm</kbd> or not when someone types a new comment. We will use our pipeline to figure out how accurate our cross-validation was. We find in this case that the average accuracy was about 94:</p>
<div class="cdpaligncenter"><img class="alignnone80" src="../images/00091.jpeg"/></div>
<p class="calibre2">It's pretty good. Now let's add TF-IDF to our model to make it more precise:</p>
<div class="cdpaligncenter"><img class="alignnone81" src="../images/00092.jpeg"/></div>
<p class="calibre2">This will be placed after <kbd class="calibre12">countVectorizer</kbd>. After we have produced the counts, we can then produce a TF-IDF score for these counts. Now we will add this in the pipeline and perform another cross-validation check with the same accuracy:</p>
<div class="cdpaligncenter"><img class="alignnone82" src="../images/00093.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2">This show the steps required for the pipeline:</p>
<div class="cdpaligncenter"><img class="alignnone83" src="../images/00094.jpeg"/></div>
<p class="calibre2">The following output got us <kbd class="calibre12">CountVectorizer</kbd>, a TF-IDF transformer, and <kbd class="calibre12">RandomForestClassifier</kbd>. Notice that <kbd class="calibre12">countvectorizer</kbd> can be lower case or upper case in the dataset; it is on us to decide how many words you want to have. We can either use single words or bigrams, which would be pairs of words, or trigrams, which can be triples of words. We can also remove stop words, which are really common English words such as <strong class="calibre4">and</strong>, <strong class="calibre4">or</strong>, and <strong class="calibre4">the</strong>. With TF-IDF, you can turn off the <kbd class="calibre12">idf</kbd> component and just keep the <kbd class="calibre12">tf</kbd> component, which would just be a log of the count. You can use <kbd class="calibre12">idf</kbd> as well. With random forests, you've got a choice of how many trees you use, which is the number of estimators.</p>
<p class="calibre2">There's another feature of scikit-learn available that allows us to search all of these parameters. For that, it finds out what the best parameters are:</p>
<div class="cdpaligncenter"><img class="alignnone84" src="../images/00095.jpeg"/></div>
<p class="calibre2">We can make a little dictionary where we say the name of the pipeline step and then mention what the parameter name would be and this gives us our options. For demonstration, we are going to try maximum number of words or maybe just a maximum of 1,000 or 2,000 words. </p>
<p class="calibre2">Using <kbd class="calibre12">ngrams</kbd>, we can mention just single words or pairs of words that are stop words, use the English dictionary of stop words, or don't use stop words, which means in the first case we need to get rid of common words, and in the second case we do not get rid of common words. Using TF-IDF, we use <kbd class="calibre12">idf</kbd> to state whether it's yes or no. The random forest we created uses 20, 50, or 100 trees. Using this, we can perform a grid search, which runs through all of the combinations of parameters and finds out what the best combination is. So, let's give our pipeline number 2, which has the TF-IDF along with it. We will use <kbd class="calibre12">fit</kbd> to perform the search and the outcome can be seen in the following screenshot:</p>
<div class="cdpaligncenter"><img class="alignnone85" src="../images/00096.jpeg"/></div>
<p class="calibre2">Since there is a large number of words, it takes a little while, around 40 seconds, and ultimately finds the best parameters. We can get the best parameters out of the grid search and print them to see what the score is:</p>
<div class="cdpaligncenter"><img class="alignnone86" src="../images/00097.gif"/></div>
<p class="calibre2">So, we got nearly 96% accuracy. We used around 1,000 words, only single words, used yes to get rid of stop words, had 100 trees in the random forest, and used yes and the IDF and the TF-IDF computation. Here we've demonstrated not only bag of words, TF-IDF, and random forest, but also the pipeline feature and the parameter search feature known as grid search.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Word2Vec models</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we'll learn about Word2Vec, a modern and popular technique for working with text. Usually, Word2Vec performs better than simple bag of words models. A bag of words model only counts how many times each word appears in each document. Given two such bag of words vectors, we can compare documents to see how similar they are. This is the same as comparing the words used in the documents. In other words, if the two documents have many similar words that appear a similar number of times, they will be considered similar.</p>
<p class="calibre2">But bag of words models have no information about how similar the words are. So, if two documents do not use exactly the same words but do use synonyms, such as <strong class="calibre4">please</strong> and <strong class="calibre4">plz</strong>, they're not regarded as similar for the bag of words model. Word2Vec can figure out that some words are similar to each other and we can exploit that fact to get better performance when doing machine learning with text.</p>
<p class="calibre2"/>
<p class="calibre2">In Word2Vec, each word itself is a vector, with perhaps 300 dimensions. For example, in a pre-trained Google Word2Vec model that examined millions or billions of pages of text, we can see that cat, dog, and spatula are 300-dimensional vectors:</p>
<ul class="calibre10">
<li class="calibre11">Cat = &lt;0.012, 0.204, ..., -0.275, 0.056&gt; (300 dimensions)</li>
<li class="calibre11">Dog = &lt;0.051, -0.022, ..., -0.355, 0.227&gt;</li>
<li class="calibre11">Spatula = &lt;-0.191, -0.043, ..., -0.348, 0.398&gt;</li>
<li class="calibre11">Similarity (distance) between cat and dog—0.761</li>
<li class="calibre11"><span>Similarity between cat and spatula—0.124</span></li>
</ul>
<p class="calibre2">If we compare the similarity of the dog and cat vectors, we will get 0.761 or 76% of similarity. If we do the same with cat and spatula, we get 0.124. It's clear that Word2Vec learned that dog and cat are similar words but cat and spatula are not. Word2Vec uses neural networks to learn these word vectors. At a high level, a neural network is similar to random forest or a decision tree and other machine learning techniques because they're given a bunch of inputs and a bunch of outputs, and they learn how to predict the outputs from the inputs.</p>
<p class="calibre2">For Word2Vec, the input is a single word, the word whose vector we want to learn, and the output is its nearby words from the text. Word2Vec also supports the reverse of this input-output configuration. Thus, Word2Vec learns the word vectors by remembering its context words. So, dog and cat will have similar word vectors because these two words are used in similar ways, like <em class="calibre16">she pet the dog</em> and <em class="calibre16">she pet the cat</em>. Neural networking with Word2Vec can take one of two forms because Word2Vec supports two different techniques for training.</p>
<p class="calibre2">The first technique is known as continuous bag of words, where the context words are the input, leaving out the middle word and the word whose vector we're learning, the middle word, is the output. In the following diagram, you can see three words before and after the word <strong class="calibre4">channel</strong>:</p>
<div class="cdpaligncenter"><img src="../images/00098.jpeg" class="calibre20"/></div>
<p class="calibre2">Those are the context words. The continuous bag of words model slides over the whole sentence with every word acting as a center word in turn. The neural network learns the 300-dimensional vectors for each word so that the vector can predict the center word given the context words. In other words, it can predict the output given its inputs.</p>
<p class="calibre2">In the second technique, we're going to flip this. This is known as <strong class="calibre4">skip-gram</strong>, and the center word is the input and the context words are the outputs:</p>
<div class="cdpaligncenter"><img class="alignnone87" src="../images/00099.jpeg"/></div>
<p class="calibre2">In this technique, the center word vector is used to predict the context words given that center word.</p>
<p class="calibre2">Both of these techniques perform well for most situations. They each have minor pros and cons that will not be important for our use case.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Doc2Vec</h1>
                
            
            <article>
                
<p class="calibre2">We're going to use Word2Vec to detect positive and negative product, restaurant, and movie reviews. We will do so with a slightly different form of Word2Vec known as <strong class="calibre4">Doc2Vec</strong>. In this case, the input is a document name, such as the filename, and the output is the sliding window of the words from the document. This time, we will not have a center word:</p>
<div class="cdpaligncenter"><img class="alignnone88" src="../images/00100.jpeg"/></div>
<p class="calibre2">In this case, as a vector that helps us predict the words, from knowing the filename. In fact, the input is not very important, which in this case is the filename. We just need to keep track of the words on the right side, and that they all came from the same document. So, all of those words will be connected to that filename, but the actual content of that filename is not important. Since we can predict the document's words based on its filename, we can effectively have a model that knows which words go together in a document. In other words, that documents usually talk about just one thing, for example, learning that a lot of different positive words are used in positive reviews and a lot of negative words are used in negative reviews.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Document vector</h1>
                
            
            <article>
                
<p class="calibre2">After training, we have a new document and we want to find its document vector. We'll use the word similarities learned during training to construct a vector that will predict the words in the new document. We will use a dummy filename since the actual name is not important. What's important is that it's just one name. So, all of these words get connected together under that one name:</p>
<div class="cdpaligncenter"><img class="alignnone89" src="../images/00101.jpeg"/></div>
<p class="calibre2">Once we get that new document vector, we can compare it with other document vectors and find which known document from the past is the most similar, as follows:</p>
<div class="cdpaligncenter"><img class="alignnone90" src="../images/00102.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2">Thus, we can use <kbd class="calibre12">Doc2Vec</kbd> to find which documents are most similar to each other. This will help us detect positive and negative reviews because, ideally, the positive reviews will have document vectors that are similar to each other and this will be the same for negative reviews. We expect <kbd class="calibre12">Doc2Vec</kbd> to perform better than bag of words because <kbd class="calibre12">Doc2Vec</kbd> learns the words that are used together in the same document, so those words that are similar to bag of words never actually learned any information about how similar the words are different.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Detecting positive or negative sentiments in user reviews</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we're going to look at detecting positive and negative sentiments in user reviews. In other words, we are going to detect whether the user is typing a positive comment or a negative comment about the product or service. We're going to use <kbd class="calibre12">Word2Vec</kbd> and <kbd class="calibre12">Doc2Vec</kbd> specifically and the <kbd class="calibre12">gensim</kbd> Python library for those services. There are two categories, which are positive and negative, and we have over 3,000 different reviews to look at. These come from Yelp, IMDb, and Amazon. Let's begin the code by importing the <kbd class="calibre12">gensim</kbd> library, which provides <kbd class="calibre12">Word2Vec</kbd> and <kbd class="calibre12">Doc2Vec</kbd> for logging <span class="calibre5">to note status of the messages</span>:</p>
<div class="cdpaligncenter"><img class="alignnone91" src="../images/00103.jpeg"/></div>
<p class="calibre2">First, we will see how to load a pre-built <kbd class="calibre12">Word2Vec</kbd> model, provided by Google, that has been trained on billions of pages of text and has ultimately produced 300-dimensional vectors for all the different words. Once the model is loaded, we will look at the vector for <kbd class="calibre12">cat</kbd>. This shows that the model is a <span class="calibre5">300-dimensional vector, as represented by the word <kbd class="calibre12">cat</kbd></span>:</p>
<div class="cdpaligncenter"><img class="alignnone92" src="../images/00104.gif"/></div>
<p class="calibre2">The following screenshot shows the <span class="calibre5">300-dimensional vector for the word <kbd class="calibre12">dog</kbd>:</span></p>
<div class="cdpaligncenter"><img class="alignnone93" src="../images/00105.gif"/></div>
<p class="calibre2">The following screenshot shows the 300-dimensional vector for the word <kbd class="calibre12"><span><span>spatula</span></span></kbd>:</p>
<div class="cdpaligncenter"><img class="alignnone94" src="../images/00106.jpeg"/></div>
<p class="calibre2">We obtain a result of 76% when computing the similarity of dog and cat, as follows:</p>
<div class="cdpaligncenter"><img class="alignnone95" src="../images/00107.jpeg"/></div>
<p class="calibre2">The similarity between cat and spatula is 12%; it is a bit lower, as it should be:</p>
<div class="cdpaligncenter"><img class="alignnone96" src="../images/00108.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2">Here we train our <kbd class="calibre12">Word2Vec</kbd> and <kbd class="calibre12">Doc2Vec</kbd> model using the following code:</p>
<div class="cdpaligncenter"><img class="alignnone97" src="../images/00109.jpeg"/></div>
<p class="calibre2">We are using <kbd class="calibre12">Doc2Vec</kbd> because we want to determine a vector for each document, not necessarily for each word in the document, because our documents are reviews and we want to see whether these reviews are positive or negative, which means it's similar to positive reviews or similar to negative reviews. <kbd class="calibre12">Doc2Vec</kbd> is provided by <kbd class="calibre12">gensim</kbd> and the library has a class called <kbd class="calibre12">TaggedDocument</kbd> that allows us to use "<kbd class="calibre12">these are the words in the document, and Doc2Vec is the model</kbd>".</p>
<p class="calibre2">Now we create a utility function that will take a sentence or a whole paragraph and lowercase it and remove all the HTML tags, apostrophes, punctuation, spaces, and repeated spaces, and then ultimately break it apart by words:</p>
<div class="cdpaligncenter"><img class="alignnone98" src="../images/00110.jpeg"/></div>
<p class="calibre2">Now it's time for our training set. We are not going to use the 3,000 Yelp, IMDb, and Amazon reviews because there's simply not enough data to train for a good <kbd class="calibre12">Doc2Vec</kbd> model. If we had millions reviews, then we could take a good portion of that to train with and use the rest for testing, but with just 3,000 reviews it's not enough. So, instead, I've gathered reviews from IMDb and other places, including Rotten Tomato. This will be enough to train a <kbd class="calibre12">Doc2Vec</kbd> model, but none of these are actually from the dataset that we're going to use for our final prediction. These are simply reviews. They're positive; they're negative. I don't know which, as I'm not keeping track of which. What matters is that we have enough text to learn how words are used in these reviews. Nothing records whether the review is positive or negative.</p>
<p class="calibre2"/>
<p class="calibre2">So, <kbd class="calibre12">Doc2Vec</kbd> and <kbd class="calibre12">Word2Vec</kbd> are actually being used for unsupervised training. That means we don't have any answers. We simply learn how words are used together. Remember the context of words, and how a word is used according to the words nearby:</p>
<div class="cdpaligncenter"><img class="alignnone99" src="../images/00111.jpeg"/></div>
<p class="calibre2">So, in each case, in each file, we simply make a <kbd class="calibre12">TaggedDocument</kbd> object with the words from that document or that review plus a tag, which is simply the filename. This is important so that it learns that all these words go together in the same document, and that these words are somehow related to each other. After loading, we have 175,000 training examples from different documents:</p>
<div class="cdpaligncenter"><img class="alignnone100" src="../images/00112.gif"/></div>
<p class="calibre2">Now let's have a look at the first 10 sentences in the following screenshot:</p>
<div class="cdpaligncenter"><img class="alignnone101" src="../images/00113.gif"/></div>
<p class="calibre2"><span class="calibre5">We shuffle these documents and then feed them into our <kbd class="calibre12">Doc2Vec</kbd> trainer, using </span><kbd class="calibre12">Doc2Vec(permuter, dm=0, hs=1, size=50)</kbd><span class="calibre5">, where we finally do the training of the <kbd class="calibre12">Doc2Vec</kbd> model and where it learns the document vectors for all the different documents. </span><kbd class="calibre12">dm=0</kbd><span class="calibre5"> and </span><kbd class="calibre12">hs=1</kbd><span class="calibre5"> are just parameters to say how to do the training. These are just things that I found were the most accurate. </span><kbd class="calibre12">dm=0</kbd><span class="calibre5"> is where we are using the model that was shown in the last section, which means it receives a filename and it predicts the words:</span></p>
<div class="cdpaligncenter"><img class="alignnone102" src="../images/00114.gif"/></div>
<p class="calibre2">Here <kbd class="calibre12">size=50</kbd> means that we found that 50-dimensional vectors for each document was best, and 300-dimensional vectors are optimal, because we don't have enough training examples. Since we don't have millions or billions of data. This is a good 300 dimensional vector, and 50 seemed to work better. Running this code uses the processor and all the cores you have, so it will takes some time to execute. You will see that it's going through all the percentages of how much it got through. Ultimately, it takes 300 seconds to get this information in my case, which is definitely not bad. That's pretty fast, but if you have millions or billions of training documents, it could take days.</p>
<p class="calibre2">Once the training is complete, we can delete some stuff to free up some memory<span class="calibre5">:</span></p>
<div class="cdpaligncenter"><img class="alignnone103" src="../images/00115.jpeg"/></div>
<p class="calibre2">We do need to keep the inference data, which is enough to bind a new document vector for new documents, but we don't need it to keep all the data about all the different words.</p>
<p class="calibre2">You can save the model and then load it later with the <kbd class="calibre12">model = Doc2Vec.Load('reviews.d2v')</kbd> command, if you want to put it in a product and deploy it, or put it on a server:</p>
<div class="cdpaligncenter"><img class="alignnone104" src="../images/00116.jpeg"/></div>
<p class="calibre2">After the model's been trained, you can infer a vector, which is regarding what the document vector is for this new document. So, let's extract the words with the utility function. Here we are using an example phrase that was found in a review. This is the 50-dimensional vector it learned for that phrase:</p>
<div class="cdpaligncenter"><img src="../images/00117.jpeg" class="calibre20"/></div>
<p class="calibre2">Now the question that rises is what about a negative phrase? And another negative phrases. Are they considered similar? Well, they're considered 48% similar, as seen in the following screenshot:</p>
<div class="cdpaligncenter"><img class="alignnone105" src="../images/00118.jpeg"/></div>
<p class="calibre2">What about different phrases? <kbd class="calibre12">Highly recommended</kbd> and <kbd class="calibre12">Service sucks</kbd>. They're less similar:</p>
<div class="cdpaligncenter"><img class="alignnone106" src="../images/00119.jpeg"/></div>
<p class="calibre2">The model learned about how words are used together in the same review and that these words go together in one way and that other words go together in a different way.</p>
<p class="calibre2">Finally, we are ready to load our real dataset for prediction:</p>
<div class="cdpaligncenter"><img class="alignnone107" src="../images/00120.jpeg"/></div>
<p class="calibre2">To summarize, we used Yelp, Amazon, and IMDb reviews. We loaded different files and in each file, each line had a review. As a result, we get the words from the line and found out what the vector was for that document. We put that in a list, shuffle, and finally built a classifier. In this case, we're going to use k-nearest neighbors, which is a really simple technique.</p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">It's just a technique that says <em class="calibre16">find all the similar documents</em>, in this case, the nine closest documents to the one that we're looking at, and count votes:</p>
<div class="cdpaligncenter"><img class="alignnone108" src="../images/00121.jpeg"/></div>
<p class="calibre2">We will be using nine reviews for the purposes of this example, and if you have a majority, let's say of positive reviews, then we will say that this is a positive review too. If the majority says negative, then this is a negative too. We don't want a tie regarding the reviews, which is why we say that there's nine instead of eight.</p>
<p class="calibre2">Now we will compare the outcome with a random forest:</p>
<div class="cdpaligncenter"><img class="alignnone109" src="../images/00122.jpeg"/></div>
<p class="calibre2">Now we need to perform cross-validation with the 9 nearest neighbors; we get 76% accuracy for detecting positive/negative reviews with <kbd class="calibre12">Doc2Vec</kbd>. For experimental purposes, if we use a random forest without really trying to choose an amount of trees, we just get an accuracy of 70%:</p>
<div class="cdpaligncenter"><img class="alignnone110" src="../images/00123.jpeg"/></div>
<p class="calibre2">In such cases, k-nearest neighbors is both simpler and more accurate. Ultimately, is it all worth it? Well, let's comparing it to the bag of words model. Let's make a little pipeline with <kbd class="calibre12">CountVectorizer</kbd>, TF-IDF, and random forest, and at the end, do cross-validation on the same data, which in this case is the reviews. Here, we get 74%, as seen in the following screenshot:</p>
<div class="cdpaligncenter"><img class="alignnone111" src="../images/00124.jpeg"/></div>
<p class="calibre2">The outcome that we found after executing the model build we found <kbd class="calibre12">Doc2Vec</kbd> was better. <kbd class="calibre12">Doc2Vec</kbd> can be a lot more accurate than bag of words if we add a lot of training examples that are of the same style as the testing set. Hence, in our case, the testing set was pretty much the Yelp, Amazon, and IMDb reviews, which are all one sentence or one line of text and are pretty short. However, the training set that we found came from different reviews from different places, and we got about 175,000 examples. Those were often like paragraphs or just written in different ways.</p>
<p class="calibre2">Ideally, we will train a <kbd class="calibre12">Doc2Vec</kbd> or <kbd class="calibre12">Word2Vec</kbd> model on examples that are similar to what we're going to predict on later, but it can be difficult to find enough examples, as it was here so we did our best. Even so, it still turned out better than bag of words.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we introduced text processing and the bag of words technique. We then used this technique to build a spam detector for YouTube comments. Next, we learned about the sophisticated Word2Vec model and put it to task with a coding project that detects positive and negative product, restaurant, and movie reviews. That's the end of this chapter about text.</p>
<p class="calibre2">In the next chapter, we're going to look at deep learning, which is a popular technique that's used in neural networks.</p>


            </article>

            
        </section>
    </body></html>