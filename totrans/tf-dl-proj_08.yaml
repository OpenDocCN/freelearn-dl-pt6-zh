- en: Detecting Duplicate Quora Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quora ([www.quora.com](http://www.quora.com)) is a community-driven question
    and answer website where users, either anonymously or publicly, ask and answer
    questions. In January 2017, Quora first released a public dataset consisting of
    question pairs, either duplicate or not. A duplicate pair of questions is semantically
    similar; in other words, two questions being duplicated means that they carry
    the same meaning, although they use a different set of words to express the exact same
    intent. For Quora, it is paramount to have a single question page for each distinct
    question, in order to offer a better service to users consulting its repository
    of answers, so they won't have to look for any more sources before finding all
    they need to know. Moderators can be helpful in avoiding duplicated content on
    the site, but that won't easily scale, given the increasing number of questions
    answered each day and a growing historical repository. In this case, an automation
    project based on **Natural Language Processing** (**NLP**) and deep learning could
    be the right solution for the task.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will deal with understanding how to build a project based on TensorFlow
    that explicates the semantic similarity between sentences using the Quora dataset.
    The chapter is based on the work of Abhishek Thakur ([https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur/](https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur/)),
    who originally developed a solution based on the Keras package. The presented
    techniques can also easily be applied to other problems that deal with semantic
    similarity. In this project, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering on text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-IDF and SVD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2vec and GloVe based features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional machine learning models such as logistic regression and gradient
    boosting using `xgboost`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning models including LSTM, GRU, and 1D-CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the chapter, you will be able to train your own deep learning
    model on similar problems. To start with, let's have a quick look at the Quora
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Presenting the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data, made available for non-commercial purposes ([https://www.quora.com/about/tos](https://www.quora.com/about/tos))
    in a Kaggle competition ([https://www.kaggle.com/c/quora-question-pairs](https://www.kaggle.com/c/quora-question-pairs))
    and on Quora's blog ([https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs)),
    consists of 404,351 question pairs with 255,045 negative samples (non-duplicates)
    and 149,306 positive samples (duplicates). There are approximately 40% positive
    samples, a slight imbalance that won't need particular corrections. Actually,
    as reported on the Quora blog, given their original sampling strategy, the number
    of duplicated examples in the dataset was much higher than the non-duplicated
    ones. In order to set up a more balanced dataset, the negative examples were upsampled
    by using pairs of related questions, that is, questions about the same topic that
    are actually not similar.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting work on this project, you can simply directly download the data,
    which is about 55 MB, from its Amazon S3 repository at this link: [http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv](http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv) into
    our working directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading it, we can start diving directly into the data by picking some
    example rows and examining them. The following diagram shows an actual snapshot
    of the few first rows from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3062b7c-9d99-40fa-b022-cf9a7c14e202.png)'
  prefs: []
  type: TYPE_IMG
- en: First few rows of the Quora dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploring further into the data, we can find some examples of  question pairs
    that mean the same thing, that is, duplicates, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  How does Quora quickly mark  questions as needing improvement? | Why does
    Quora mark my questions as needing improvement/clarification'
  prefs: []
  type: TYPE_NORMAL
- en: before I have time to give it details?
  prefs: []
  type: TYPE_NORMAL
- en: Literally within seconds… |
  prefs: []
  type: TYPE_NORMAL
- en: '| Why did Trump win the Presidency? | How did Donald Trump win the 2016 Presidential
    Election? |'
  prefs: []
  type: TYPE_TB
- en: '| What practical applications might evolve from the discovery of the Higgs
    Boson? | What are some practical benefits of the discovery of the Higgs Boson?
    |'
  prefs: []
  type: TYPE_TB
- en: At first sight, duplicated questions have quite a few words in common, but they
    could be very different in length.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, examples of non-duplicate questions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Who should I address my cover letter to if I''m applying to a big company
    like Mozilla? | Which car is better from a safety persepctive? swift or grand
    i10\. My first priority is safety? |'
  prefs: []
  type: TYPE_TB
- en: '| Mr. Robot (TV series): Is Mr. Robot a good representation of real-life hacking
    and hacking culture? Is the depiction of hacker societies realistic? | What mistakes
    are made when depicting hacking in Mr. Robot compared to real-life cyber security
    breaches or just a regular use of technologies? |'
  prefs: []
  type: TYPE_TB
- en: '| How can I start an online shopping (e-commerce) website? | Which web technology
    is best suited for building a big e-commerce website? |'
  prefs: []
  type: TYPE_TB
- en: Some questions from these examples are clearly not duplicated and have few words
    in common, but some others are more difficult to detect as unrelated. For instance,
    the second pair in the example might turn being appealing to some and leave even
    a human judge uncertain. The two questions might mean different things: *why* versus
    *how*, or they could be intended as the same from a superficial examination. Looking
    deeper, we may even find more doubtful examples and even some clear data mistakes;
    we surely have some anomalies in the dataset (as the Quota post on the dataset
    warned) but, given that the data is derived from a real-world problem, we can't
    do anything but deal with this kind of imperfection and strive to find a robust
    solution that works.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, our exploration becomes more quantitative than qualitative and
    some statistics on the question pairs are provided here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Average number of characters in question1 | 59.57 |'
  prefs: []
  type: TYPE_TB
- en: '| Minimum number of characters in question1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Maximum number of characters in question1 | 623 |'
  prefs: []
  type: TYPE_TB
- en: '| Average number of characters in question2 | 60.14 |'
  prefs: []
  type: TYPE_TB
- en: '| Minimum number of characters in question2 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Maximum number of characters in question2 | 1169 |'
  prefs: []
  type: TYPE_TB
- en: Question 1 and question 2 are roughly the same average characters, though we
    have more extremes in question 2\. There also must be some trash in the data,
    since we cannot figure out a question made up of a single character.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can even get a completely different vision of our data by plotting it into
    a word cloud and highlighting the most common words present in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7334f53b-df7f-41a3-8937-657d184d1165.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A word cloud made up of the most frequent words to be found in the
    Quora dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The presence of word sequences such as Hillary Clinton and Donald Trump reminds
    us that the data was gathered at a certain historical moment, and that many questions
    we can find inside it are clearly ephemeral, reasonable only at the very time
    the dataset was collected. Other topics, such as programming language, World War,
    or earn money could be longer lasting, both in terms of interest and in the validity
    of the answers provided.
  prefs: []
  type: TYPE_NORMAL
- en: After exploring the data a bit, it is now time to decide what target metric
    we will strive to optimize in our project. Throughout the chapter, we will be
    using accuracy as a metric to evaluate the performance of our models. Accuracy
    as a measure is simply focused on the effectiveness of the prediction, and it
    may miss some important differences between alternative models, such as discrimination
    power (is the model more able to detect duplicates or not?) or the exactness of
    probability scores (how much margin is there between being a duplicate and not
    being one?). We chose accuracy based on the fact that this metric was the one decided
    on by Quora's engineering team to create a benchmark for this dataset (as stated
    in this blog post of theirs: [https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning](https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning)).
    Using accuracy as the metric makes it easier for us to evaluate and compare our
    models with the one from Quora's engineering team, and also several other research
    papers. In addition, in a real-world application, our work may simply be evaluated
    on the basis of how many times it is just right or wrong, regardless of other
    considerations.
  prefs: []
  type: TYPE_NORMAL
- en: We can now proceed furthermore in our projects with some very basic feature
    engineering to start with.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with basic feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before starting to code, we have to load the dataset in Python and also provide
    Python with all the necessary packages for our project. We will need to have these
    packages installed on our system (the latest versions should suffice, no need
    for any specific package version):'
  prefs: []
  type: TYPE_NORMAL
- en: '`Numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fuzzywuzzy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`python-Levenshtein`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gensim`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pyemd`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NLTK`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we will be using each one of these packages in the project, we will provide
    specific instructions and tips to install them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For all dataset operations, we will be using pandas (and Numpy will come in
    handy, too). To install `numpy` and `pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset can be loaded into memory easily by using pandas and a specialized
    data structure, the pandas dataframe (we expect the dataset to be in the same
    directory as your script or Jupyter notebook):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will be using the pandas dataframe denoted by `data` throughout this chapter,
    and also when we work with our TensorFlow model and provide input to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now start by creating some very basic features. These basic features
    include length-based features and string-based features:'
  prefs: []
  type: TYPE_NORMAL
- en: Length of question1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Length of question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Difference between the two lengths
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Character length of question1 without spaces
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Character length of question2 without spaces
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of words in question1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of words in question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of common words in question1 and question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These features are dealt with one-liners transforming the original input using
    the pandas package in Python and its method `apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For future reference, we will mark this set of features as feature set-1 or
    `fs_1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This simple approach will help you to easily recall and combine a different
    set of features in the machine learning models we are going to build, turning
    comparing different models run by different feature sets into a piece of cake.
  prefs: []
  type: TYPE_NORMAL
- en: Creating fuzzy features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next set of features are based on fuzzy string matching. Fuzzy string matching
    is also known as approximate string matching and is the process of finding strings
    that approximately match a given pattern. The closeness of a match is defined
    by the number of primitive operations necessary to convert the string into an
    exact match. These primitive operations include insertion (to insert a character
    at a given position), deletion (to delete a particular character), and substitution
    (to replace a character with a new one).
  prefs: []
  type: TYPE_NORMAL
- en: Fuzzy string matching is typically used for spell checking, plagiarism detection,
    DNA sequence matching, spam filtering, and so on and it is part of the larger
    family of edit distances, distances based on the idea that a string can be transformed
    into another one. It is frequently used in natural language processing and other
    applications in order to ascertain the grade of difference between two strings
    of characters.
  prefs: []
  type: TYPE_NORMAL
- en: It is also known as Levenshtein distance, from the name of the Russian scientist,
    Vladimir Levenshtein, who introduced it in 1965.
  prefs: []
  type: TYPE_NORMAL
- en: These features were created using the `fuzzywuzzy` package available for Python
    ([https://pypi.python.org/pypi/fuzzywuzzy](https://pypi.python.org/pypi/fuzzywuzzy)).
    This package uses Levenshtein distance to calculate the differences in two sequences,
    which in our case are the pair of questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fuzzywuzzy` package can be installed using pip3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As an important dependency, `fuzzywuzzy` requires the `Python-Levenshtein`
    package ([https://github.com/ztane/python-Levenshtein/](https://github.com/ztane/python-Levenshtein/)),
    which is a blazingly fast implementation of this classic algorithm, powered by
    compiled C code. To make the calculations much faster using `fuzzywuzzy`, we also
    need to install the `Python-Levenshtein` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fuzzywuzzy` package offers many different types of ratio, but we will
    be using only the following:'
  prefs: []
  type: TYPE_NORMAL
- en: QRatio
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: WRatio
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partial ratio
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partial token set ratio
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partial token sort ratio
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Token set ratio
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Token sort ratio
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Examples of `fuzzywuzzy` features on Quora data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This code snippet will result in the value of 67 being returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this comparison, the returned value will be 60\. Given these examples, we
    notice that although the values of `QRatio` are close to each other, the value
    for the similar question pair from the dataset is higher than the pair with no
    similarity. Let''s take a look at another feature from fuzzywuzzy for these same
    pairs of questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the returned value is 73:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now the returned value is 57.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `partial_ratio` method, we can observe how the difference in scores
    for these two pairs of questions increases notably, allowing an easier discrimination
    between being a duplicate pair or not. We assume that these features might add
    value to our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using pandas and the `fuzzywuzzy` package in Python, we can again apply
    these features as simple one-liners:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This set of features are henceforth denoted as feature set-2 or `fs_2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Again, we will store our work and save it for later use when modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Resorting to TF-IDF and SVD features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next few sets of features are based on TF-IDF and SVD. **Term Frequency-Inverse
    Document Frequency** (**TF-IDF**). Is one of the algorithms at the foundation
    of information retrieval. Here, the algorithm is explained using a formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00c867d8-7952-404f-902f-526fb4b64e24.png)![](img/7aed5e97-3d58-4d51-9c8f-82108a7c1b94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can understand the formula using this notation: *C(t)* is the number of
    times a term *t* appears in a document, *N* is the total number of terms in the
    document, this results in the **Term Frequency** (**TF**).  ND is the total number
    of documents and *ND[t]* is the number of documents containing the term *t*, this
    provides the **Inverse Document Frequency** (**IDF**).  TF-IDF for a term *t* is
    a multiplication of Term Frequency and Inverse Document Frequency for the given
    term *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a050f4da-3d8c-4dd6-bd2c-ddc3329d7fe3.png)'
  prefs: []
  type: TYPE_IMG
- en: Without any prior knowledge, other than about the documents themselves, such
    a score will highlight all the terms that could easily discriminate a document
    from the others, down-weighting the common words that won't tell you much, such
    as the common parts of speech (such as articles, for instance).
  prefs: []
  type: TYPE_NORMAL
- en: If you need a more hands-on explanation of TFIDF, this great online tutorial
    will help you try coding the algorithm yourself and testing it on some text data: [https://stevenloria.com/tf-idf/](https://stevenloria.com/tf-idf/)
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience and speed of execution, we resorted to the `scikit-learn` implementation
    of TFIDF.  If you don''t already have `scikit-learn` installed, you can install
    it using pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We create TFIDF features for both question1 and question2 separately (in order
    to type less, we just deep copy the question1 `TfidfVectorizer`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It must be noted that the parameters shown here have been selected after quite
    a lot of experiments. These parameters generally work pretty well with all other
    problems concerning natural language processing, specifically text classification.
    One might need to change the stop word list to the language in question.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now obtain the TFIDF matrices for question1 and question2 separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In our TFIDF processing, we computed the TFIDF matrices based on all the data
    available (we used the `fit_transform` method). This is quite a common approach
    in Kaggle competitions because it helps to score higher on the leaderboard. However,
    if you are working in a real setting, you may want to exclude a part of the data
    as a training or validation set in order to be sure that your TFIDF processing
    helps your model to generalize to a new, unseen dataset.
  prefs: []
  type: TYPE_NORMAL
- en: After we have the TFIDF features, we move to SVD features. SVD is a feature
    decomposition method and it stands for singular value decomposition. It is largely
    used in NLP because of a technique called Latent Semantic Analysis (LSA).
  prefs: []
  type: TYPE_NORMAL
- en: A detailed discussion of SVD and LSA is beyond the scope of this chapter, but
    you can get an idea of their workings by trying these two approachable and clear
    online tutorials: [https://alyssaq.github.io/2015/singular-value-decomposition-visualisation/](https://alyssaq.github.io/2015/singular-value-decomposition-visualisation/)
    and [https://technowiki.wordpress.com/2011/08/27/latent-semantic-analysis-lsa-tutorial/](https://technowiki.wordpress.com/2011/08/27/latent-semantic-analysis-lsa-tutorial/)
  prefs: []
  type: TYPE_NORMAL
- en: To create the SVD features, we again use `scikit-learn` implementation. This
    implementation is a variation of traditional SVD and is known as `TruncatedSVD`.
  prefs: []
  type: TYPE_NORMAL
- en: A `TruncatedSVD` is an approximate SVD method that can provide you with reliable
    yet computationally fast SVD matrix decomposition. You can find more hints about
    how this technique works and it can be applied by consulting this web page: [http://langvillea.people.cofc.edu/DISSECTION-LAB/Emmie'sLSI-SVDModule/p5module.html](http://langvillea.people.cofc.edu/DISSECTION-LAB/Emmie'sLSI-SVDModule/p5module.html)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We chose 180 components for SVD decomposition and these features are calculated
    on a TF-IDF matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Feature set-3 is derived from a combination of these TF-IDF and SVD features.
    For example, we can have only the TF-IDF features for the two questions separately
    going into the model, or we can have the TF-IDF of the two questions combined
    with an SVD on top of them, and then the model kicks in, and so on. These features
    are explained as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature set-3(1) or `fs3_1` is created using two different TF-IDFs for the
    two questions, which are then stacked together horizontally and passed to a machine
    learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11f9f437-007c-421d-ae50-b7b76898dfce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be coded as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Feature set-3(2), or `fs3_2`, is created by combining the two questions and
    using a single TF-IDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d77c673e-f76f-4638-9794-156f099d9116.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The next subset of features in this feature set, feature set-3(3) or `fs3_3`,
    consists of separate TF-IDFs and SVDs for both questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d853fdf8-eb11-44d2-a33c-fdd9f67927d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be coded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We can similarly create a couple more combinations using TF-IDF and SVD, and
    call them `fs3-4` and `fs3-5`, respectively. These are depicted in the following
    diagrams, but the code is left as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature set-3(4) or `fs3-4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7aa20c6-daf9-4107-b9d8-da3963587e68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Feature set-3(5) or `fs3-5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76064d3b-aa91-4bb5-b004-a5f06bbb6d41.png)'
  prefs: []
  type: TYPE_IMG
- en: After the basic feature set and some TF-IDF and SVD features, we can now move
    to more complicated features before diving into the machine learning and deep
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping with Word2vec embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Very broadly, Word2vec models are two-layer neural networks that take a text
    corpus as input and output a vector for every word in that corpus. After fitting,
    the words with similar meaning have their vectors close to each other, that is,
    the distance between them is small compared to the distance between the vectors
    for words that have very different meanings.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, Word2vec has become a standard in natural language processing problems
    and often it provides very useful insights into information retrieval tasks. For
    this particular problem, we will be using the Google news vectors. This is a pretrained
    Word2vec model trained on the Google News corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every word, when represented by its Word2vec vector, gets a position in space,
    as depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7324f13f-f14f-443e-b8e6-0cd4f1c58e96.png)'
  prefs: []
  type: TYPE_IMG
- en: All the words in this example, such as Germany, Berlin, France, and Paris, can
    be represented by a 300-dimensional vector, if we are using the pretrained vectors
    from the Google news corpus. When we use Word2vec representations for these words
    and we subtract the vector of Germany from the vector of Berlin and add the vector
    of France to it, we will get a vector that is very similar to the vector of Paris.
    The Word2vec model thus carries the meaning of words in the vectors. The information
    carried by these vectors constitutes a very useful feature for our task.
  prefs: []
  type: TYPE_NORMAL
- en: For a user-friendly, yet more in-depth, explanation and description of possible
    applications of Word2vec, we suggest reading [https://www.distilled.net/resources/a-beginners-guide-to-Word2vec-aka-whats-the-opposite-of-canada/](https://www.distilled.net/resources/a-beginners-guide-to-word2vec-aka-whats-the-opposite-of-canada/),
    or if you need a more mathematically defined explanation, we recommend reading
    this paper: [http://www.1-4-5.net/~dmm/ml/how_does_Word2vec_work.pdf](http://www.1-4-5.net/~dmm/ml/how_does_word2vec_work.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'To load the Word2vec features, we will be using Gensim. If you don''t have
    Gensim, you can install it easily using pip. At this time, it is suggested you
    also install the pyemd package, which will be used by the WMD distance function,
    a function that will help us to relate two Word2vec vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To load the Word2vec model, we download the `GoogleNews-vectors-negative300.bin.gz`
    binary and use Gensim''s `load_Word2vec_format` function to load it into memory.
    You can easily download the binary from an Amazon AWS repository using the `wget`
    command from a shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After downloading and decompressing the file, you can use it with the Gensim
    `KeyedVectors` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can easily get the vector of a word by calling model[word]. However,
    a problem arises when we are dealing with sentences instead of individual words.
    In our case, we need vectors for all of question1 and question2 in order to come
    up with some kind of comparison. For this, we can use the following code snippet.
    The snippet basically adds the vectors for all words in a sentence that are available
    in the Google news vectors and gives a normalized vector at the end. We can call
    this sentence to vector, or Sent2Vec.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that you have **Natural Language Tool Kit** (**NLTK**) installed
    before running the preceding function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also suggested that you download the `punkt` and `stopwords` packages,
    as they are part of NLTK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If NLTK is now available, you just have to run the following snippet and define
    the `sent2vec` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: When the phrase is null, we arbitrarily decide to give back a standard vector
    of zero values.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the similarity between the questions, another feature that we created
    was word mover's distance. Word mover's distance uses Word2vec embeddings and
    works on a principle similar to that of earth mover's distance to give a distance
    between two text documents. Simply put, word mover's distance provides the minimum
    distance needed to move all the words from one document to an other document.
  prefs: []
  type: TYPE_NORMAL
- en: The WMD has been introduced by this paper: <q>KUSNER, Matt, et al. From word
    embeddings to document distances. In: International Conference on Machine Learning.
    2015\. p. 957-966</q> which can be found at [http://proceedings.mlr.press/v37/kusnerb15.pdf](http://proceedings.mlr.press/v37/kusnerb15.pdf).
    For a hands-on tutorial on the distance, you can also refer to this tutorial based
    on the Gensim implementation of the distance: [https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html](https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'Final **Word2vec** (**w2v**) features also include other distances, more usual
    ones such as the Euclidean or cosine distance. We complete the sequence of features
    with some measurement of the distribution of the two document vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: Word mover distance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalized word mover distance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cosine distance between vectors of question1 and question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manhattan distance between vectors of question1 and question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Jaccard similarity between vectors of question1 and question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Canberra distance between vectors of question1 and question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Euclidean distance between vectors of question1 and question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minkowski distance between vectors of question1 and question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Braycurtis distance between vectors of question1 and question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The skew of the vector for question1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The skew of the vector for question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The kurtosis of the vector for question1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The kurtosis of the vector for question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the Word2vec features are denoted by **fs4**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A separate set of w2v features consists in the matrices of Word2vec vectors
    themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec vector for question1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Word2vec vector for question2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These will be represented by **fs5**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to easily implement all the different distance measures between the
    vectors of the Word2vec embeddings of the Quora questions, we use the implementations
    found in the `scipy.spatial.distance module`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'All the features names related to distances are gathered under the list `fs4_1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The Word2vec matrices for the two questions are instead horizontally stacked
    and stored away in the `w2v` variable for later usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The Word Mover''s Distance is implemented using a function that returns the
    distance between two questions, after having transformed them into lowercase and
    after removing any stopwords. Moreover, we also calculate a normalized version
    of the distance, after transforming all the Word2vec vectors into L2-normalized
    vectors (each vector is transformed to the unit norm, that is, if we squared each
    element in the vector and summed all of them, the result would be equal to one)
    using the `init_sims` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'After these last computations, we now have most of the important features that
    are needed to create some basic machine learning models, which will serve as a
    benchmark for our deep learning models. The following table displays a snapshot
    of the available features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/daae71d5-e910-4ad6-994b-bca6afd52af9.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's train some machine learning models on these and other Word2vec based features.
  prefs: []
  type: TYPE_NORMAL
- en: Testing machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before proceeding, depending on your system, you may need to clean up the memory
    a bit and free space for machine learning models from previously used data structures.
    This is done using `gc.collect`, after deleting any past variables not required
    anymore, and then checking the available memory by exact reporting from the `psutil.virtualmemory`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we simply recap the different features created up to now, and
    their meaning in terms of generated features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fs_1`: List of basic features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs_2`: List of fuzzy features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs3_1`: Sparse data matrix of TFIDF for separated questions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs3_2`: Sparse data matrix of TFIDF for combined questions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs3_3`: Sparse data matrix of SVD'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs3_4`: List of SVD statistics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs4_1`: List of w2vec distances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs4_2`: List of wmd distances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`w2v`: A matrix of transformed phrase''s Word2vec vectors by means of the `Sent2Vec`
    function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We evaluate two basic and very popular models in machine learning, namely logistic
    regression and gradient boosting using the `xgboost` package in Python. The following
    table provides the performance of the logistic regression and `xgboost` algorithms
    on different sets of features created earlier, as obtained during the Kaggle competition:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature set** | **Logistic regression accuracy** | **xgboost accuracy**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Basic features (fs1) | 0.658 | 0.721 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic features + fuzzy features (fs1 + fs2) | 0.660 | 0.738 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic features + fuzzy features + w2v features (fs1 + fs2 + fs4) | 0.676
    | 0.766 |'
  prefs: []
  type: TYPE_TB
- en: '| W2v vector features (fs5) | * | 0.78 |'
  prefs: []
  type: TYPE_TB
- en: '| Basic features + fuzzy features + w2v features + w2v vector features (fs1
    + fs2 + fs4 + fs5) | * | **0.814** |'
  prefs: []
  type: TYPE_TB
- en: '| TFIDF-SVD features (`fs3-1`) | 0.777 | 0.749 |'
  prefs: []
  type: TYPE_TB
- en: '| TFIDF-SVD features (`fs3-2`) | 0.804 | 0.748 |'
  prefs: []
  type: TYPE_TB
- en: '| TFIDF-SVD features (`fs3-3`) | 0.706 | 0.763 |'
  prefs: []
  type: TYPE_TB
- en: '| TFIDF-SVD features (`fs3-4`) | 0.700 | 0.753 |'
  prefs: []
  type: TYPE_TB
- en: '| TFIDF-SVD features (`fs3-5`) | 0.714 | 0.759 |'
  prefs: []
  type: TYPE_TB
- en: '* = These models were not trained due to high memory requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: We can treat the performances achieved as benchmarks or baseline numbers before
    starting with deep learning models, but we won't limit ourselves to that and we
    will be trying to replicate some of them.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to start by importing all the necessary packages. As for as the
    logistic regression, we will be using the scikit-learn implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The xgboost is a scalable, portable, and distributed gradient boosting library
    (a tree ensemble machine learning algorithm). Initially created by Tianqi Chen
    from Washington University, it has been enriched with a Python wrapper by Bing
    Xu, and an R interface by Tong He (you can read the story behind xgboost directly
    from its principal creator at [homes.cs.washington.edu/~tqchen/2016/03/10/story-and-lessons-behind-the-evolution-of-xgboost.html](http://homes.cs.washington.edu/~tqchen/2016/03/10/story-and-lessons-behind-the-evolution-of-xgboost.html)
    ). The xgboost is available for Python, R, Java, Scala, Julia, and C++, and it
    can work both on a single machine (leveraging multithreading) and in Hadoop and
    Spark clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Detailed instruction for installing xgboost on your system can be found on
    this page: [github.com/dmlc/xgboost/blob/master/doc/build.md](https://github.com/dmlc/xgboost/blob/master/doc/build.md)'
  prefs: []
  type: TYPE_NORMAL
- en: The installation of xgboost on both Linux and macOS is quite straightforward,
    whereas it is a little bit trickier for Windows users.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, we provide specific installation steps for having xgboost
    working on Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, download and install Git for Windows ([git-for-windows.github.io](https://git-for-windows.github.io/))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, you need a MINGW compiler present on your system. You can download it
    from [www.mingw.org](http://www.mingw.org/) according to the characteristics of
    your system
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the command line, execute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`$> git clone --recursive [https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`$> cd xgboost`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`$> git submodule init`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`$> git submodule update`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, always from the command line, you copy the configuration for 64-byte
    systems to be the default one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`$> copy make\mingw64.mk config.mk`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Alternatively, you just copy the plain 32-byte version:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`$> copy make\mingw.mk config.mk`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After copying the configuration file, you can run the compiler, setting it
    to use four threads in order to speed up the compiling process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`$> mingw32-make -j4`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In MinGW, the `make` command comes with the name `mingw32-make`; if you are
    using a different compiler, the previous command may not work, but you can simply
    try:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`$> make -j4`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, if the compiler completed its work without errors, you can install
    the package in Python with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`$> cd python-package`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`$> python setup.py install`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If xgboost has also been properly installed on your system, you can proceed
    with importing both machine learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we will be using a logistic regression solver that is sensitive to the
    scale of the features (it is the `sag` solver from [https://github.com/EpistasisLab/tpot/issues/292](https://github.com/EpistasisLab/tpot/issues/292),
    which requires a linear computational time in respect to the size of the data),
    we will start by standardizing the data using the `scaler` function in scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We also select the data for the training by first filtering the `fs_1`, `fs_2`,
    `fs3_4`, `fs4_1`, and `fs4_2` set of variables, and then stacking the `fs3_3` sparse
    SVD data matrix. We also provide a random split, separating 1/10 of the data for
    validation purposes (in order to effectively assess the quality of the created
    model):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'As a first model, we try logistic regression, setting the regularization l2
    parameter C to 0.1 (modest regularization). Once the model is ready, we test its
    efficacy on the validation set (`x_val` for the training matrix, `y_val` for the
    correct answers). The results are assessed on accuracy, that is the proportion
    of exact guesses on the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: After a while (the solver has a maximum of 1,000 iterations before giving up
    converging the results), the resulting accuracy on the validation set will be
    0.743, which will be our starting baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we try to predict using the `xgboost` algorithm. Being a gradient boosting
    algorithm, this learning algorithm has more variance (ability to fit complex predictive
    functions, but also to overfit) than a simple logistic regression afflicted by
    greater bias (in the end, it is a summation of coefficients) and so we expect
    much better results. We fix the max depth of its decision trees to 4 (a shallow
    number, which should prevent overfitting) and we use an eta of 0.02 (it will need
    to grow many trees because the learning is a bit slow). We also set up a watchlist,
    keeping an eye on the validation set for an early stop if the expected error on
    the validation doesn't decrease for over 50 steps.
  prefs: []
  type: TYPE_NORMAL
- en: It is not best practice to stop early on the same set (the validation set in
    our case) we use for reporting the final results. In a real-world setting, ideally,
    we should set up a validation set for tuning operations, such as early stopping,
    and a test set for reporting the expected results when generalizing to new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting all this, we run the algorithm. This time, we will have to wait
    for longer than we when running the logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The final result reported by `xgboost` is `0.803` accuracy on the validation
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Building a TensorFlow model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The deep learning models in this chapter are built using TensorFlow, based on
    the original script written by Abhishek Thakur using Keras (you can read the original
    code at [https://github.com/abhishekkrthakur/is_that_a_duplicate_quora_question](https://github.com/abhishekkrthakur/is_that_a_duplicate_quora_question)).
    Keras is a Python library that provides an easy interface to TensorFlow. Tensorflow
    has official support for Keras, and the models trained using Keras can easily
    be converted to TensorFlow models. Keras enables the very fast prototyping and
    testing of deep learning models. In our project, we rewrote the solution entirely
    in TensorFlow from scratch anyway.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, let''s import the necessary libraries, in particular TensorFlow,
    and let''s check its version by printing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we simply load the data into the `df` pandas dataframe or we
    load it from disk. We replace the missing values with an empty string and we set
    the `y` variable containing the target answer encoded as 1 (duplicated) or 0 (not
    duplicated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We can now dive into deep neural network models for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Processing before deep neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before feeding data into any neural network, we must first tokenize the data
    and then convert the data to sequences. For this purpose, we use the Keras `Tokenizer`
    provided with TensorFlow, setting it using a maximum number of words limit of
    200,000 and a maximum sequence length of 40\. Any sentence with more than 40 words
    is consequently cut off to its first 40 words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'After setting the `Tokenizer`, `tk`, this is fitted on the concatenated list
    of the first and second questions, thus learning all the possible word terms present
    in the learning corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In order to keep track of the work of the tokenizer, `word_index` is a dictionary
    containing all the tokenized words paired with an index assigned to them.
  prefs: []
  type: TYPE_NORMAL
- en: Using the GloVe embeddings, we must load them into memory, as previously seen
    when discussing how to get the Word2vec embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GloVe embeddings can be easily recovered using this command from a shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The GloVe embeddings are similar to Word2vec in the sense that they encode
    words into a complex multidimensional space based on their co-occurrence. However,
    as explained by the paper [http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf) 
    —<q>BARONI, Marco; DINU, Georgiana; KRUSZEWSKI, Germán. Don''t count, predict!
    A systematic comparison of context-counting vs. context-predicting semantic vectors.
    In: Proceedings of the 52nd Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)</q><q>. 2014\. p. 238-247</q>.'
  prefs: []
  type: TYPE_NORMAL
- en: GloVe is not derived from a neural network optimization that strives to predict
    a word from its context, as Word2vec is. Instead, GloVe is generated starting
    from a co-occurrence count matrix (where we count how many times a word in a row
    co-occurs with the words in the columns) that underwent a dimensionality reduction
    (a factorization just like SVD, as we mentioned before when preparing our data).
  prefs: []
  type: TYPE_NORMAL
- en: Why are we now using GloVe instead of Word2vec? In practice, the main difference
    between the two simply boils down to the empirical fact that GloVe embeddings
    work better on some problems, whereas Word2vec embeddings perform better on others.
    In our case, after experimenting, we found GloVe embeddings working better with
    deep learning algorithms. You can read more information about GloVe and its uses
    from its official page at Stanford University: [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
  prefs: []
  type: TYPE_NORMAL
- en: Having got a hold of the GloVe embeddings, we can now proceed to create an `embedding_matrix`
    by filling the rows of the `embedding_matrix` array with the embedding vectors
    (sized at 300 elements each) extracted from the GloVe file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code reads the glove embeddings file and stores them into our
    embedding matrix, which in the end will consist of all the tokenized words in
    the dataset with their respective vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Starting from an empty `embedding_matrix`, each row vector is placed on the
    precise row number of the matrix that is expected to represent its corresponding
    wording. Such correspondence between words and rows has previously been defined
    by the encoding process completed by the tokenizer and is now available for consultation
    in the `word_index` dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: After the `embedding_matrix` has completed loading the embeddings, it is time
    to start building some deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks building blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to present the key functions that will allow our
    deep learning project to work. Starting from batch feeding (providing chunks of
    data to learn to the deep neural network) we will prepare the building blocks
    of a complex LSTM architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM architecture is presented in a hands-on and detailed way in [Chapter
    7](95c80f98-b5f4-4e07-9054-0c968dae1e76.xhtml), *Stock Price Prediction with LSTM*,
    inside the *Long short-term memory – LSTM 101* section
  prefs: []
  type: TYPE_NORMAL
- en: 'The first function we start working with is the `prepare_batches` one. This
    function takes the question sequences and based on a step value (the batch size),
    returns a list of lists, where the internal lists are the sequence batches to
    be learned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The dense function will create a dense layer of neurons based on the provided
    size and activate and initialize them with random normally distributed numbers
    that have a mean of zero, and as a standard deviation, the square root of 2 divided
    by the number of input features.
  prefs: []
  type: TYPE_NORMAL
- en: 'A proper initialization helps back-propagating the input derivative deep inside
    the network. In fact:'
  prefs: []
  type: TYPE_NORMAL
- en: If you initialize the weights in a network too small, then the derivative shrinks
    as it passes through each layer until it's too faint to trigger the activation
    functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the weights in a network are initialized too large, then the derivative simply
    grows (the so-called exploding gradient problem) as it traverses through each
    layer, the network won't converge to a proper solution and it will break because
    of handling numbers that are too large.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The initialization procedure makes sure the weights are just right by setting
    a reasonable starting point where the derivative can propagate through many layers.
    There are quite a few initialization procedures for deep learning networks, such
    as Xavier by Glorot and Bengio (Xavier is Glorot's first name), and the one proposed
    by He, Rang, Zhen, and Sun, and built on the Glorot and Bengio one, which is commonly
    referred to as He.
  prefs: []
  type: TYPE_NORMAL
- en: Weight initialization is quite a technical aspect of building a neural network
    architecture, yet a relevant one. If you want to know more about it, you can start
    by consulting this post, which also delves into more mathematical explanations
    of the topic: [http://deepdish.io/2015/02/24/network-initialization/](http://deepdish.io/2015/02/24/network-initialization/)
  prefs: []
  type: TYPE_NORMAL
- en: 'In this project, we opted for the He initialization, since it works quite well
    for rectified units. Rectified units, or ReLu, are the powerhouse of deep learning
    because they allow signals to propagate and avoid the exploding or vanishing gradient
    problems, yet neurons activated by the ReLU,  from a practical point of view,
    are actually most of the time just firing a zero value. Keeping the variance large
    enough in order to have a constant variance of the input and output gradient passing
    through the layer really helps this kind of activation to work best, as explained
    in this paper: <q>HE, Kaiming, et al. Delving deep into rectifiers: Surpassing
    human-level performance on imagenet classification. In: Proceedings of the IEEE
    international conference on computer vision.</q> 2015\. p. 1026-1034 which can
    be found and read at [https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Next, we work on another kind of layer, the time distributed dense layer.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of layer is used on recurrent neural networks in order to keep a one-to-one
    relationship between the input and the output. An RNN (with a certain number of
    cells providing channel outputs), fed by a standard dense layer, receives matrices
    whose dimensions are rows (examples) by columns (sequences) and it produces as
    output a matrix whose dimensions are rows by the number of channels (cells). If
    you feed it using the time distributed dense layer, its output will instead be
    dimensionality shaped as rows by columns by channels. In fact, it happens that
    a dense neural network is applied to timestamp (each column).
  prefs: []
  type: TYPE_NORMAL
- en: A time distributed dense layer is commonly used when you have, for instance,
    a sequence of inputs and you want to label each one of them, taking into account
    the sequence that arrived. This is a common scenario for tagging tasks, such as
    multilabel classification or Part-Of-Speech tagging. In our project, we will be
    using it just after the GloVe embedding in order to process how each GloVe vector
    changes by passing from a word to another in the question sequence.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let's say you have a sequence of two cases (a couple of question
    examples), and each one has three sequences (some words), each of which is made
    of four elements (their embeddings). If we have such a dataset passed through
    the time distributed dense layer with five hidden units, we will obtain a tensor
    of size (2, 3, 5). In fact, passing through the time distributed layer, each example
    retains the sequences, but the embeddings are replaced by the result of the five
    hidden units. Passing them through a reduction on the 1 axis, we will simply have
    a tensor of size (2,5), that is a result vector for each since example.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to replicate the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`print("Tensor''s shape:", X.shape)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`tensor = tf.convert_to_tensor(X, dtype=tf.float32)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`dense_size = 5 i = time_distributed_dense(tensor, dense_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`print("Shape of time distributed output:", i)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`j = tf.reduce_sum(i, axis=1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`print("Shape of reduced output:", j)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of a time distributed dense layer could be a bit trickier to grasp
    than others and there is much discussion online about it. You can also read this
    thread from the Keras issues to get more insight into the topic: [https://github.com/keras-team/keras/issues/1029](https://github.com/keras-team/keras/issues/1029)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The `conv1d` and `maxpool1d_global` functions are in the end wrappers of the
    TensorFlow functions `tf.layers.conv1d` ([https://www.tensorflow.org/api_docs/python/tf/layers/conv1d](https://www.tensorflow.org/api_docs/python/tf/layers/conv1d)),
    which is a convolution layer, and `tf.reduce_max` ([https://www.tensorflow.org/api_docs/python/tf/reduce_max](https://www.tensorflow.org/api_docs/python/tf/reduce_max)),
    which computes the maximum value of elements across the dimensions of an input
    tensor. In natural language processing, this kind of pooling (called global max
    pooling) is more frequently used than the standard max pooling that is commonly
    found in deep learning applications for computer vision. As explained by a Q&A
    on cross-validated ([https://stats.stackexchange.com/a/257325/49130](https://stats.stackexchange.com/a/257325/49130))
    global max pooling simply takes the maximum value of an input vector, whereas
    standard max pooling returns a new vector made of the maximum values found in
    different pools of the input vector given a certain pool size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Our core `lstm` function is initialized by a different scope at every run due
    to a random integer number generator, initialized by He initialization (as seen
    before), and it is a wrapper of the TensorFlow `tf.contrib.rnn.BasicLSTMCell`
    for the layer of Basic LSTM recurrent network cells ([https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell))
    and `tf.contrib.rnn.static_rnn` for creating a recurrent neural network specified
    by the layer of cells ([https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn](https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn)).
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of the Basic LSTM recurrent network cells is based on the
    paper <q>ZAREMBA, Wojciech; SUTSKEVER, Ilya; VINYALS, Oriol. Recurrent neural
    network regularization. arXiv preprint arXiv</q>:1409.2329, 2014 found at [https://arxiv.org/abs/1409.2329](https://arxiv.org/abs/1409.2329).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: At this stage of our project, we have gathered all the building blocks necessary
    to define the architecture of the neural network that will be learning to distinguish
    duplicated questions.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the learning architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start defining our architecture by fixing some parameters such as the number
    of features considered by the GloVe embeddings, the number and length of filters,
    the length of maxpools, and the learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Managing to grasp the different semantic meanings of less or more different
    phrases in order to spot possible duplicated questions is indeed a hard task that
    requires a complex architecture. For this purpose, after various experimentation,
    we create a deeper model consisting of LSTM, time-distributed dense layers, and
    1d-cnn. Such a model has six heads, which are merged into one by concatenation.
    After concatenation, the architecture is completed by five dense layers and an
    output layer with sigmoid activation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full model is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fc7c1e2-3bd3-4d71-a9e9-ffd0dda59a73.png)'
  prefs: []
  type: TYPE_IMG
- en: The first head consists of an embedding layer initialized by GloVe embeddings,
    followed by a time-distributed dense layer. The second head consists of 1D convolutional
    layers on top of embeddings initialized by the GloVe model, and the third head
    is an LSTM model on the embeddings learned from scratch. The other three heads
    follow the same pattern for the other question in the pair of questions.
  prefs: []
  type: TYPE_NORMAL
- en: We start defining the six models and concatenating them. In the end, the models
    are merged by concatenation, that is, the vectors from the six models are stacked
    together horizontally.
  prefs: []
  type: TYPE_NORMAL
- en: Even if the following code chunk is quite long, following it is straightforward.
    Everything starts at the three input placeholders, `place_q1`, `place_q2`, and `place_y`,
    which feed all six models with the first questions, the second questions, and
    the target response respectively. The questions are embedded using GloVe (`q1_glove_lookup`
    and `q2_glove_lookup`) and a random uniform embedding. Both embeddings have 300
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The first two models, `model_1` and `model_2`, acquire the GloVe embeddings
    and they apply a time distributed dense layer.
  prefs: []
  type: TYPE_NORMAL
- en: The following two models, `model_3` and `model_4`, acquire the GloVe embeddings
    and process them by a series of convolutions, dropouts, and maxpools. The final
    output vector is batch normalized in order to keep stable variance between the
    produced batches.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know about the nuts and bolts of batch normalization, this Quora
    answer by Abhishek Shivkumar clearly provides all the key points you need to know
    about what batch normalization is and why it is effective in neural network architecture: [https://www.quora.com/In-layman%E2%80%99s-terms-what-is-batch-normalisation-what-does-it-do-and-why-does-it-work-so-well/answer/Abhishek-Shivkumar](https://www.quora.com/In-layman%E2%80%99s-terms-what-is-batch-normalisation-what-does-it-do-and-why-does-it-work-so-well/answer/Abhishek-Shivkumar)
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, `model_5` and `model_6` acquire the uniform random embedding and process
    it with an LSTM. The results of all six models are concatenated together and batch
    normalized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We then complete the architecture by adding five dense layers with dropout
    and batch normalization. Then, there is an output layer with sigmoid activation.
    The model is optimized using an `AdamOptimizer` based on log-loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'After defining the architecture, we initialize the sessions and we are ready
    for learning. As a good practice, we split the available data into a training
    part (9/10) and a testing one (1/10). Fixing a random seed allows replicability
    of the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the following code snippet, the training will start and you will
    notice that the model accuracy increases with the increase in the number of epochs.
    However, the model will take a lot of time to train, depending on the number of
    batches you decide to iterate through. On an NVIDIA Titan X, the model takes over
    300 seconds per epoch. As a good balance between obtained accuracy and training
    time, we opt for running 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Trained for 10 epochs, the model produces an accuracy of  82.5%. This is much
    higher than the benchmarks we had before. Of course, the model could be improved
    further by using better preprocessing and tokenization. More epochs (up to 200)
    could also help raise the accuracy a bit more. Stemming and lemmatization may
    also definitely help to get near the state-of-the-art accuracy of 88% reported
    by Quora on its blog.
  prefs: []
  type: TYPE_NORMAL
- en: Having completed the training, we can use the in-memory session to test some
    question evaluations. We try with two questions about the duplicated questions
    on Quora, but the procedure works with any pair of questions you would like to
    test the algorithm on.
  prefs: []
  type: TYPE_NORMAL
- en: As with many machine learning algorithms, this one depends on the distribution
    that it has learned. Questions completely different from the ones it has been
    trained on could prove difficult for the algorithm to guess.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the code, the answer should reveal that the questions are duplicated
    (answer: 1.0).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we built a very deep neural network with the help of TensorFlow
    in order to detect duplicated questions from the Quora dataset. The project allowed
    us to discuss, revise, and practice plenty of different topics previously seen
    in other chapters: TF-IDF, SVD, classic machine learning algorithms,  Word2vec
    and GloVe embeddings, and LSTM models.'
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we obtained a model whose achieved accuracy is about 82.5%, a figure
    that is higher than traditional machine learning approaches and is also near other
    state-of-the-art deep learning solutions, as reported by the Quora blog.
  prefs: []
  type: TYPE_NORMAL
- en: It should also be noted that the models and approaches discussed in this chapter
    can easily be applied to any semantic matching problem.
  prefs: []
  type: TYPE_NORMAL
