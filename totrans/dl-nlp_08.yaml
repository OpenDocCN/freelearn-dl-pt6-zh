- en: '*Chapter 8*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: State-of-the-Art Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate vanishing gradients in long sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe an attention mechanism model as a state-of-the-art NLP domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess one specific attention mechanism architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop a neural machine translation model using an attention mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop a text summarization model using an attention mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter aims to acquaint you with the current practices and technologies
    in the NLP domain.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the last chapter, we studied Long Short Term Memory units (LSTMs), which
    help combat the vanishing gradient problem. We also studied GRU in detail, which
    has its own way of handling vanishing gradients. Although LSTM and GRU reduce
    this problem in comparison to simple recurrent neural networks, the vanishing
    gradient problem still manages to prevail in many practical cases. The issue essentially
    remains the same: longer sentences with complex structural dependences are challenging
    for deep learning algorithms to encapsulate. Therefore, one of the most prevalent
    research areas represents the community''s attempts to mitigate the effects of
    the vanishing gradient problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanisms, in the last few years, have attempted to provide a solution
    to the vanishing gradient problem. The basic concept of an attention mechanism
    relies on having access to all parts of the input sentence when arriving at an
    output. This allows the model to lay varying amounts of weight (attention) to
    different parts of the sentence, which allows dependencies to be deduced. Due
    to their uncanny ability to learn such dependencies, attention mechanism-based
    architectures represent the current state of the art in the NLP domain.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about attention mechanisms and solve a neural
    machine translation task using a specific architecture based on an attention mechanism.
    We will also mention some other related architectures that are being used in the
    industry today.
  prefs: []
  type: TYPE_NORMAL
- en: Attention Mechanisms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the last chapter, we solved a *Neural Language Translation* task. The architecture
    for the translation model adopted by us consists of two parts: *Encoder and Decoder*.
    Refer to the following diagram for the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1: Neural language translation model](img/C13783_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Neural language translation model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For a neural machine translation task, a sentence is passed into an encoder
    word by word, which produces a single *thought* vector (represented in the preceding
    image as '**S**'), which embeds the meaning of the entire sentence into a single
    representation. The decoder then uses this vector to initialize the hidden states
    and produce a translation word by word.
  prefs: []
  type: TYPE_NORMAL
- en: In the simple encoder-decoder regime, only 1 vector (the thought vector) contains
    the representation of the entire sentence. The longer the sentence, the more difficult
    it becomes for the single thought vector to retain long-term dependencies. The
    use of LSTM units reduces the problem only to some extent. A new concept was developed
    to mitigate the vanishing gradient problem further, and this concept is called
    **Attention mechanisms**.
  prefs: []
  type: TYPE_NORMAL
- en: 'An attention mechanism aims to mimic a human''s way of learning dependencies.
    Let''s illustrate this with an example sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '"There have been many incidents of thefts lately in our neighborhood, which
    has forced me to consider hiring a security agency to install a burglar-detection
    system in my house so that I can keep myself and my family safe."'
  prefs: []
  type: TYPE_NORMAL
- en: Note the use of the words 'my', 'I', 'me', 'myself,' and 'our'. These occur
    at distant positions within the sentence but are tightly coupled to each other
    to represent the meaning of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'When trying to translate the previous sentence, a traditional encoder-decoder
    functions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pass the sentence word by word to the encoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoder produces a single thought vector, which represents the entire sentence
    encoding. For a long sentence, such as the previous one, even with the use of
    LSTMs, it would be difficult for the encoder to embed all the dependencies. Therefore,
    the earlier part of the sentence is not as strongly encoded as the later part
    of the sentence, which means the later part of the sentence ends up having a dominant
    influence over the encodings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder uses the thought vector to initialize the hidden state vector to
    generate the output translation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A more intuitive way to translate the sentence would be to pay attention to
    the correct positions of words in the input sentence when determining a particular
    word in the target language. As an example, consider the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '''*The animal could not walk on the street because it was badly injured.*'''
  prefs: []
  type: TYPE_NORMAL
- en: 'In this sentence, whom does the word ''it'' refer to? Is it the animal or the
    street? An answer to this question would be possible if the entire sentence were
    considered together and different parts of the sentence were weighed differently
    to determine the answer to the question. An attention mechanism accomplishes this,
    as depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2: An example of an attention mechanism](img/C13783_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: An example of an attention mechanism'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The diagram shows how much weight each word receives in understanding every
    word in a sentence. As can be seen, the word '**it_**' receives a very strong
    weighting from '**animal_**' and a relatively weaker weighting from '**street_**'.
    Thus, the model can now answer the question of which entity 'it' refers to in
    the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a translation encoder-decoder model, while generating word-by-word output,
    at a given point in time, not all the words in the input sentence are important
    for the determination of the output word. An attention mechanism implements a
    scheme that does exactly that: weighs different parts of the input sentence with
    all of the input words at each point in the determination of the output. A well-trained
    network with an attention mechanism would learn to apply an appropriate amount
    of weighting to different parts of the sentence. This regime allows the entire
    part of the input sentence to be always available for use at every point of determining
    the output. Thus, instead of one thought vector, the decoder has access to the
    "thought" vector specific for the determination of each word in the output sentence.
    This ability of an attention mechanism is in stark contrast to a traditional LSTM/GRU/RNN-based
    encoder-decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: An attention mechanism is a general concept. It can be realized in several architectural
    flavors, which are discussed in the later part of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: An Attention Mechanism Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s see how an encoder-decoder architecture could look with an attention
    mechanism in place:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: An attention mechanism model](img/C13783_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: An attention mechanism model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The preceding diagram depicts the training phase of a language translation
    model with an attention mechanism. We can note a few differences compared to a
    basic encoder-decoder regime, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The initial states of the decoder get initialized with the encoder output state
    from the last encoder cell. An initial **NULL** word is used to start the translation,
    and the first word is produced as '**Er**'. This is the same as the previous encoder-decoder
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the second word, in addition to the input from the previous word and the
    hidden state of the preceding decoder timestep, another vector is fed as input
    to the cell. This vector, generally regarded as '**Context vector**', is a function
    of all the encoder hidden states. From the preceding diagram, it is a weighted
    summation of the hidden states of the encoder for all the timesteps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the training phase, since the output of each decoder timestep is known,
    we can learn all the parameters of the network. In addition to the usual parameters,
    corresponding to whichever RNN flavor is being used, the parameters specific to
    the attention function are also learned. If the attention function is just a simple
    summation of the hidden state encoder vectors, the weights of the hidden states
    at each encoder timestep can be learned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At inference time, at every timestep, the decoder cell can take as input the
    predicted word from the last timestep, the hidden states from the previous decoder
    cell, and the context vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at one specific realization of an attention mechanism for neural
    machine translation. In the previous chapter, we built a neural language translation
    model, which is a subproblem area of a more general area of NLP called neural
    machine translation. In the following section, we attempt to solve a date-normalization
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Data Normalization Using an Attention Mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's say you're maintaining a database that has a table containing a column
    for date. The input for the date is taken from your customers, who fill in a form
    and enter the date in a **date** field. The frontend engineer somehow forgot to
    enforce a scheme upon the field, such that only dates in a "YYYY-MM-DD" format
    are accepted. You are now tasked with normalizing the **date** column of database
    table, such that the user inputs in several formats get converted to a standard
    "YYYY-MM-DD" format.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, the user inputs for date and the corresponding correct normalization
    are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: Table for date normalization](img/C13783_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Table for date normalization'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can see that there is a lot of variation in the way a user can input a date.
    There are many more ways in which the date could be specified apart from the examples
    in the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'This problem is a good candidate to be solved by a neural machine translation
    model as the input has a sequential structure, wherein the meanings of the different
    components in the input need to be learned. This model will have the following
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is a bidirectional LSTM that takes each character of the date as input.
    Thus, at each timestep, the input to the encoder is a single character of the
    input date. Apart from this, the hidden state and memory state is also taken as
    an input from the previous encoder cell. Since this is a bidirectional architecture,
    there are two sets of parameters pertaining to the LSTM: one in the forward direction
    and the other in the backward direction.'
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a unidirectional LSTM. It takes as input the context vector for this
    timestep. Since each output character is not strictly dependent upon the last
    output character in the case of date normalization, we don't need to feed the
    previous timestep output as an input to the current timestep. Additionally, since
    it is an LSTM unit, the hidden states and memory state from the previous decoder
    timestep are also fed to the current timestep unit for the determination of the
    decoder output at this timestep.
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanisms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Attention mechanisms are explained in this section. For determination of a
    decoder input at a given timestep, a context vector is calculated. A context vector
    is a weighted summation of all the hidden state of an encoder from all timesteps.
    This is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: Expression for the context vector](img/C13783_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: Expression for the context vector'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The dot operation is a dot product operation that multiplies weights (represented
    by **alpha**) with the corresponding hidden state vector for all timesteps and
    sums them up. The value of the alpha vector is calculated separately for each
    decoder output timestep. The alphas encapsulate the essence of an attention mechanism,
    that is, determining how much ''attention'' to be given to which part of the input
    to figure out the current timestep output. This can be realized in a diagram,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6: Determination of attention to inputs](img/C13783_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: Determination of attention to inputs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As an example, let's say that the encoder input has a fixed length of 30 characters,
    and the decoder output has a fixed output length of 10 characters. For the date
    normalization problem, this means that the user input is fixed to be a maximum
    of 30 characters, while the model output is fixed at 10 characters (the number
    of characters in the YYYY-MM-DD format, including the hyphens).
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that we wish to determine the decoder output at the output timestep=4
    (an arbitrary number chosen to explain the concept; it just needs to be <=10,
    which is the output timestep count). At this step, the weight vector alpha is
    computed. This vector has a dimensionality equal to the number of timesteps of
    the encoder input (as a weight needs to be computed for every encoder input timestep).
    So, in our case, alpha has a dimensionality of 30.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we already have the hidden state vector from each of the encoder timesteps,
    so there are a total of 30 hidden state vectors available. The dimensionality
    of the hidden state vector accounts for both the forward and backward components
    of the bidirectional encoder LSTM. For a given timestep, we combine the forward
    hidden state and backward hidden state into a single vector. So, if the dimensionality
    of forward and backward hidden states is 32 each, we put them in a single vector
    of 64 dimensions as [**h_forward**, **h_backward**]. This is a simple concatenation
    function. Let's call this the encoder hidden state vector.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a single 30-dimensional weight vector alpha, and 30 vectors of 64-dimensional
    hidden states. So, we can now multiply each of the 30 hidden state vectors with
    a corresponding entry in the alpha vector. Furthermore, we can sum up these scaled
    representations of hidden states to receive a single 64-dimensional context vector.
    This is essentially the operation performed by the dot operator.
  prefs: []
  type: TYPE_NORMAL
- en: The Calculation of Alpha
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The weights can be modeled by a multilayer perceptron (MLP), which is a simple
    neural network consisting of multiple hidden layers. We choose to have two dense
    layers with a **softmax** output. The number of dense layers and units can be
    treated as hyperparameters. The input to this MLP consists of two components:
    these are the hidden state vectors for all timesteps from the encoder bidirectional
    LSTM, as explained in the last point, and the hidden states from the previous
    timestep of the decoder. These are concatenated to form a single vector. So, the
    input to the MLP is: [*encoder hidden state vector*, *previous state vector from
    decoder*]. This is a concatenation operation of tensors: [**H**, **S_prev**].
    **S_prev** refers to the decoder''s hidden state output from the previous timestep.
    If the dimensionality of **S_prev** is 64 (denoting a hidden state dimensionality
    of 64 for the decoder LSTM) and the dimensionality of the encoder''s hidden state
    vector is 64 (from the last point), a concatenation of these two vectors produces
    a vector of size 128.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the MLP receives a 128-dimension input for a single encoder timestep.
    As we have fixed the encoder input length to 30 characters, we will have a matrix
    (more accurately, a tensor) of size [30, 128]. The parameters of this MLP are
    learned using the same BPTT regime that is used to learn all the other parameters
    of the model. So, all the parameters of the entire model (encoder + decoder +
    attention function MLP) are learned together. This can be seen in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7: The calculation of alpha](img/C13783_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: The calculation of alpha'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the previous step, we learned the weights (alpha vector) for determining
    only one step of the decoder output (we had assumed this timestep to be 4 in an
    earlier point). So, the determination of a single step decoder output requires
    the inputs: **S_prev** and encoder hidden states for calculating the context vector,
    decoder hidden states, and decoder previous timestep memory, which goes as input
    to the decoder unidirectional LSTM. Proceeding to the next decoder timestep requires
    a calculation of a new alpha vector since, for this next step, various parts of
    the input sequence will most likely be weighted differently compared to the previous
    timestep.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the architecture of the model, the training and inference steps are the
    same. The only difference is that, during training, we know the output for each
    decoder timestep and use that to train the model parameters (this technique is
    referred to as 'Teacher Forcing').
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, during inference time, we predict the output character. Note that
    both during training and inference, we do not feed the previous timestep decoder
    output character as input to the current timestep decoder cell. It should be noted
    that the architecture proposed here is specific to this problem. There are a lot
    of architectures and ways to define an attention function. We will take a brief
    look at some of these in later sections of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 28: Build a Date Normalization Model for a Database Column'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A database column accepts date inputs from various users in multiple formats.
    In this exercise, we aim to normalize the date column of the database table such
    that the user inputs in several formats get converted to a standard "YYYY-MM-DD"
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Python requirements for running the code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Babel==2.6.0
  prefs: []
  type: TYPE_NORMAL
- en: Faker==1.0.2
  prefs: []
  type: TYPE_NORMAL
- en: Keras==2.2.4
  prefs: []
  type: TYPE_NORMAL
- en: numpy==1.16.1
  prefs: []
  type: TYPE_NORMAL
- en: pandas==0.24.1
  prefs: []
  type: TYPE_NORMAL
- en: scipy==1.2.1
  prefs: []
  type: TYPE_NORMAL
- en: tensorflow==1.12.0
  prefs: []
  type: TYPE_NORMAL
- en: tqdm==4.31.1
  prefs: []
  type: TYPE_NORMAL
- en: Faker==1.0.2
  prefs: []
  type: TYPE_NORMAL
- en: 'We import all the necessary modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define some helper functions. We first use the ''`faker`'' and `babel`
    modules to generate data for training. The `format_date` function from `babel`
    generates date in a specific format (using `FORMATS`). Additionally, dates are
    also returned in a human-readable format that emulates the informal user input
    date that we wish to normalize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the format of the data we would like to generate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we generate and write a function to load the dataset. In this function,
    examples are created using the `load_date()` function defined earlier. In addition
    to this dataset, the function also returns dictionaries for mapping human-readable
    and machine-readable tokens along with the inverse machine vocabulary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The previous helper functions are used to generate a dataset using the `babel`
    Python package. Additionally, it returns the input and output vocab dictionaries,
    as we have been doing in past exercises.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we generate a dataset having 10,000 samples using these helper functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The variables hold values, as depicted:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.8: Screenshot displaying variable values](img/C13783_08_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.8: Screenshot displaying variable values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `human_vocab` is a dictionary that maps input characters to integers. The
    following is the mapping of values for `human_vocab`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.9: Screenshot for human_vocab dictionary](img/C13783_08_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.9: Screenshot for human_vocab dictionary'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The `machine_vocab` dictionary contains the mapping of the output character
    to integers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.10: Screenshot for the machine_vocab dictionary](img/C13783_08_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.10: Screenshot for the machine_vocab dictionary'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '`inv_machine_vocab` is an inverse mapping of `machine_vocab` to map predicted
    integers back to characters:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.11: Screenshot for the inv_machine_vocab dictionary](img/C13783_08_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.11: Screenshot for the inv_machine_vocab dictionary'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, we preprocess data such that the input sequences have shape (`10000`,
    `30`, `len(human_vocab)`). Thus, every row in this matrix represents 30 timesteps
    and the one-coded vector, having a value of 1 corresponding to the character at
    a given timestep. Similarly, the Y output gets the shape (`10000`, `10`, `len(machine_vocab)`).
    This corresponds to 10 output timesteps and the corresponding one-hot-coded output
    vector. We first define a function named ''`string_to_int`'' that takes as input
    a single user date and returns a sequence of integers that can be fed to the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Change the case to lowercase to standardize the text
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now utilize this helper function to generate input and output integer
    sequences, as explained previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Print the shape of the matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this step is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13783_08_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.12: Screenshot for the shape of matrices'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can further inspect the shapes of the `X`,`Y`, `Xoh`, and `Yoh` vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13783_08_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.13: Screenshot for the shape of matrices after processing'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We now start defining some functions that we need to build the model. First,
    we define a function that calculates a softmax value given a tensor as input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we can start to put the model together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`RepeatVector` serves the purpose of repeating a given tensor multiple times.
    In our case, this is done `Tx` times, which is 30 input timesteps. The repeator
    is used to repeat `S_prev` 30 times. Recall that to calculate the context vector
    for determining one timestep decoder output, `S_prev` needs to be concatenated
    with each of the input encoder timesteps. The `Concatenate` `keras` function accomplishes
    the next step, that is, concatenating the repeated `S_prev` and encoder hidden
    state vector for each timestep. We have also defined MLP layers, which are two
    dense layers (`densor1`, `densor2`). Next, the output of MLP is passed through
    a `softmax` layer. This `softmax` distribution is an alpha vector with each entry
    corresponding to the weight for each concatenated vector. In the end, a `dotor`
    function is defined, which is responsible for calculating the context vector.
    The entire flow corresponds to one step attention (since it is for one decoder
    output timestep):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `repeator` to repeat `s_prev` to be of shape (`m`, `Tx`, `n_s`) so that
    you can concatenate it with all hidden states, ''`h`'':'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `concatenator` to concatenate `a` and `s_prev` on the last axis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `densor1` to propagate `concat` through a small fully-connected neural
    network to compute the intermediate energies variable, `e`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `densor2` to propagate `e` through a small fully-connected neural network
    to compute the variable energies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `activator` on `energies` to compute the attention weights `alphas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `dotor` along with `alphas` and `a` to compute the context vector to be
    given to the next (post-attention) LSTM-cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Up to this point, we still haven''t defined the number of hidden state units
    for the encoder and decoder LSTMs. We also need to define the decoder LSTM, which
    is a unidirectional LSTM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now define the encoder and decoder model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the inputs of your model with a shape (`Tx,`). Define `s0` and `c0`,
    and the initial hidden state for the decoder LSTM of shape (`n_s,`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize an empty list of `outputs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define your pre-attention Bi-LSTM. Remember to use `return_sequences=True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Iterate for `Ty` steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform one step of the attention mechanism to get back the context vector
    at step `t`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the post-attention LSTM cell to the `context` vector. Also, pass `initial_state`
    `= [hidden state, cell state]`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the `Dense` layer to the hidden state output of the post-attention LSTM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a model instance by taking three inputs and returning the list of outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output could be as shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.14: Screenshot for model summary](img/C13783_08_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.14: Screenshot for model summary'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We will now compile the model with `categorical_crossentropy` as the loss function
    and `Adam` optimizer as the optimization strategy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to initialize the hidden state vector and memory state for decoder
    LSTM before fitting the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This starts the training:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.15: Screenshot for epoch training](img/C13783_08_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.15: Screenshot for epoch training'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model is now trained and can be called for inference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.16: Screenshot for normalized date output](img/C13783_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.16: Screenshot for normalized date output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other Architectures and Developments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The attention mechanism architecture described in the last section is only a
    way of building attention mechanism. In recent times, several other architectures
    have been proposed, which constitute a state of the art in the deep learning NLP
    world. In this section, we will briefly mention some of these architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In late 2017, Google came up with an attention mechanism architecture in their
    seminal paper titled "Attention is all you need." This architecture is considered
    state-of-the-art in the NLP community. The transformer architecture makes use
    of a special multi-head attention mechanism to generate attention at various levels.
    Additionally, it is also employs residual connections to further ensure that the
    vanishing gradient problem has a minimal impact on learning. The special architecture
    of transformers also allows a massive speed up of the training phase while providing
    better quality results.
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used package with transformer architecture is **tensor2tensor**.
    The Keras code for transformer tends to be very bulky and untenable, while **tensor2tensor**
    allows the use of both a Python package and a simple command-line utility that
    can be used to train a transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For more information on tensor2tensor, refer to https://github.com/tensorflow/tensor2tensor/#t2t-overview
  prefs: []
  type: TYPE_NORMAL
- en: 'Readers interested in learning more about the architecture should read the
    mentioned paper and the associated Google blogpost at this link: https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html'
  prefs: []
  type: TYPE_NORMAL
- en: BERT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In late 2018, Google open sourced yet another groundbreaking architecture, called
    **BERT** (**Bidirectional Encoder Representations from Transformers**). The deep
    learning community for NLP has been missing the transfer-learning regime for training
    models for a long time. The transfer learning approach to deep learning has been
    state-of-the-art with image-related tasks such as image classification. Images
    are universal in their basic structure, as they do not differ regardless of geographical
    locations. This allows the training of deep learning models on generic images.
    These pre-trained models can then be fine-tuned for a specific task. This saves
    training time and the need for massive amounts of data to achieve a respectable
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Languages, unfortunately, vary a lot depending upon geographical locations and
    tend to not share basic structures. Hence, transfer learning is not a viable option
    when it comes to NLP tasks. BERT has now made it possible with its new attention
    mechanism architecture, which builds on top of the basic transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For more information on BERT, refer to https://github.com/google-research/bert
  prefs: []
  type: TYPE_NORMAL
- en: Readers interested in learning more about BERT should take a look at the Google
    blog on it at https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html.
  prefs: []
  type: TYPE_NORMAL
- en: Open AI GPT-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Open AI also open sourced an architecture called **GPT-2**, which builds upon
    their previous architecture called GPT. The mainstay of the GPT-2 architecture
    is its ability to perform well on text-generation tasks. The GPT-2 model is also
    a transformer-based model containing around 1.5 billion parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Readers interested in learning more can refer to the blogpost by OpenAI at https://blog.openai.com/better-language-models/.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 11: Build a Text Summarization Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the attention mechanism model architecture we built for neural machine
    translation to build a text summarization model. The goal of text summarization
    is to write a summary of a given large text corpus. You can imagine using text
    summarizers for the summarization of books or the generation of headlines for
    news articles.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, use the given input text:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Celebrating its 25th year, Mercedes-Benz India is set to redefine India''s
    luxury space in the automotive segment by launching the new V-Class. The V-Class
    is powered by a 2.1-litre BS VI diesel engine that generates 120kW power, 380Nm
    torque, and can go from 0-100km/h in 10.9 seconds. It features LED headlamps,
    a multi-functional steering wheel, and 17-inch alloy wheels."'
  prefs: []
  type: TYPE_NORMAL
- en: 'A good text summarization model should be able to produce a meaningful summary,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Mercedes-Benz India launches the new V-Class"'
  prefs: []
  type: TYPE_NORMAL
- en: From an architectural viewpoint, a text summarization model is exactly the same
    as a translation model. The input to the model is text that is fed character by
    character (or word by word) to an encoder, while the decoder produces output characters
    in the same language as the source text.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The input text can be found at https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2008.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you with the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required Python packages and make the human and machine vocab dictionaries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the length of the input and output characters and the model functions
    (*Repeator*, *Concatenate*, *Densors*, and *Dotor*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a one-step-attention function and the number of hidden states for the
    decoder and encoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the model architecture and run it to obtain a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define model loss functions and other hyperparameters. Also, initialize the
    decoder state vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model to our data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the inference step for the new text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expected Output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.17: Output for text summarization](img/C13783_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.17: Output for text summarization'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for the activity can be found on page 333.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we learned about the concept of attention mechanisms. Based
    on attention mechanisms, several architectures have been proposed that constitute
    the state of the art in the NLP world. We learned about one specific model architecture
    to perform a neural machine translation task. We also briefly mentioned other
    state-of-the-art architectures such as transformers and BERT.
  prefs: []
  type: TYPE_NORMAL
- en: Up to now, we have seen many different NLP models. In the next chapter, we will
    look at the flow of a practical NLP project in an organization and related technology.
  prefs: []
  type: TYPE_NORMAL
