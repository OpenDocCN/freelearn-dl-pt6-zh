<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building a Machine Vision Mobile App to Classify Flower Species</h1>
                </header>
            
            <article>
                
<p>In this chapter, we are going to use the theoretical knowledge we have learned in previous chapters to create a mobile application that will classify a specific species of flower. By utilizing use your mobile camera and pointing it at a flower, the application will analyze the image and make its best educated guess as to the species of that flower. This is where we put to work the understanding we have gained about the workings of a <strong>convolutional neural network</strong> (<strong>CNN</strong>). We will also learn a bit more about using TensorFlow as well as some tools such as TensorBoard. But before we dive in too deep, let’s talk about a few things first.</p>
<p>Throughout this chapter we use terms that may not be familiar to all, so let’s make sure we’re all on the same page as to what they mean.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>CoreML versus TensorFlow Lite</li>
<li>What is MobileNet</li>
<li>Datasets for image classification</li>
<li>Creating your own image dataset</li>
<li>Using TensorFlow to build the model</li>
<li>Running TensorBoard</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CoreML versus TensorFlow Lite</h1>
                </header>
            
            <article>
                
<p>In the machine learning world, there are two efforts (as of the time of this writing) taking place in order to improve the mobile AI experience. Instead of offloading AI or ML processing to the cloud and a data center, the faster option would be to process data on the device itself. In order to do this, the model must already be pre-trained, which means that there is a chance that it is not trained for exactly what you are going to use it for.</p>
<p>In this space, Apple’s effort (iOS) is called <strong>Core ML</strong>, and Google’s (Android) is called <strong>TensorFlow Lite</strong>. Let’s talk briefly about both.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CoreML</h1>
                </header>
            
            <article>
                
<p>The CoreML framework from Apple provides a large selection of neural network types. This allows developers to be able to experiment with different designs when developing their apps. Camera and microphone data are just two area which can be leveraged for things like image recognition, natural language processing, and more. There are several pre-trained models that developers can use straight out of the box, and tweak as necessary for their application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow Lite</h1>
                </header>
            
            <article>
                
<p>TensorFlow Lite is what is known as a local-device version of TensorFlow, meaning it is designed to run on your mobile device itself. As of the time of this writing it is still in pre-release status, so an exact comparison to CoreML is difficult. We will have to wait and see what the final offering provides. For now, simply be aware there are two options for mobile device-local AI and machine learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is MobileNet?</h1>
                </header>
            
            <article>
                
<p>Before we dive in too deep, let us first talk about a term you will hear used quite a bit in this chapter, <strong>MobileNets</strong>. What is a<span> </span>MobileNet<span> </span>you might ask? Simply put, it’s an architecture which is designed specifically for mobile and embedded vision-based applications. On such devices there is a lack of computing power available for such processing, which therefore increases the need for a better solution that one used on a desktop environment.</p>
<p>The<span> </span><strong>MobileNet</strong><span> </span>architecture was proposed by Google, and briefly:</p>
<ol>
<li>Uses depth-wise separable convolutions. This significantly reduces the number of parameters when compared to a neural network using normal convolutions with the same depth. The result is what is known as a <strong>light-weight deep neural network</strong>.</li>
<li><strong>Depth-wise convolution</strong>, followed by <strong>Pointwise convolution</strong>, replaces the normal convolution process.</li>
</ol>
<p>In order to simplify things, we are going to break down this chapter into the following two sections:</p>
<ul>
<li><strong>Datasets for Image Classification</strong>: In this section we will explore the various datasets (all of which are available online) that can be used for image classification. We will also address the issue of how to create our own datasets, if necessary.</li>
</ul>
<ul>
<li><strong>Using TensorFlow to Build the Model</strong>: In this section we will use TensorFlow to train our classification model. We do this by using a pretrained model called <strong>MobileNet</strong>. MobileNets are a family of mobile-first computer vision models for TensorFlow, designed to maximize accuracy while considering the restricted resources available for an on-device or embedded application.</li>
<li>In addition, we will look at converting the output model into a <kbd>.tflite</kbd> format, which can be used within other mobile or embedded devices. TFLite stands for TensorFlow Lite. You can learn more about TensorFlow Lite via any internet search engine.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Datasets for image classification</h1>
                </header>
            
            <article>
                
<p>For our flower classification example, we will be using the University of Oxford's <strong>Visual Geometry Group</strong> (<strong>VGG</strong>) image dataset collection. The collection can be accessed at <span class="MsoHyperlink"><a href="http://www.robots.ox.ac.uk/~vgg/data/">http://www.robots.ox.ac.uk/~vgg/data/</a>.<a href="http://www.robots.ox.ac.uk/~vgg/data/"><br/></a></span></p>
<p>The VGG is the same department that won previous ImageNet competitions. The pretrained models, such as VGG14 and VGG16, were built by this department and they won in 2014 and 2016, respectively. These datasets are used by the VGG to train and evaluate the models that they build.</p>
<p>The flower dataset can be found in the <span class="packt_screen">Fine-Grain Recognition Datasets</span> section of the page, along with textures and pet datasets. Click on <span class="packt_screen">Flower Category Datasets</span>, or use the following link to access the flower datasets from VGG, <span class="MsoHyperlink"><a href="http://www.robots.ox.ac.uk/~vgg/data/flowers/">http://www.robots.ox.ac.uk/~vgg/data/flowers/</a>.<a href="http://www.robots.ox.ac.uk/~vgg/data/flowers/"><br/></a></span></p>
<p>Here, you can find two datasets, one with 17 different species of flowers, and the other with 102 different species of flowers. You can choose either one based on their ease of use for the tutorial, or based on the kind of processing that is available at your disposal.</p>
<div class="packt_infobox">Using a larger dataset means that the training will take longer, and so will the data processing before training; therefore, we recommend that you choose wisely.</div>
<p>Here is a subset of the images you will find here. As you will see, the folder names match up identically with those we will use a bit later on in this chapter:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-751 image-border" src="assets/390e5a15-74f2-47e8-801f-91e7d469466f.png" style="width:34.75em;height:33.00em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Aside from the images we talked about above, here are several additional links that you can use to get image data for similar classification use cases should you ever desire to use them:</p>
<ul>
<li><strong>CVonline datasets</strong>: <span class="MsoHyperlink"><a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm">http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm</a></span></li>
<li><strong>CVpapers datasets</strong>: <span class="MsoHyperlink"><a href="http://www.cvpapers.com/datasets.html">http://www.cvpapers.com/datasets.html</a></span></li>
<li><strong>Image datasets</strong>: <span class="MsoHyperlink"><a href="http://wiki.fast.ai/index.php/Image_Datasets">http://wiki.fast.ai/index.php/Image_Datasets</a></span></li>
<li><strong>Deep learning datasets</strong>: <span class="MsoHyperlink"><a href="http://deeplearning.net/datasets/">http://deeplearning.net/datasets/</a></span></li>
<li><strong>COCO datsets</strong>: <span class="MsoHyperlink"><a href="http://cocodataset.org/#home">http://cocodataset.org/#home</a></span></li>
<li><strong>ImageNet datasets</strong>: <span class="MsoHyperlink"><a href="http://www.image-net.org/">http://www.image-net.org/</a></span></li>
<li><strong>Open Images datasets</strong>: <span class="MsoHyperlink"><a href="https://storage.googleapis.com/openimages/web/index.html">https://storage.googleapis.com/openimages/web/index.html</a></span></li>
<li><strong>Kaggle datasets</strong>: <span class="MsoHyperlink"><a href="https://www.kaggle.com/datasets?sortBy=relevance&amp;group=featured&amp;search=image">https://www.kaggle.com/datasets?sortBy=relevance&amp;group=featured&amp;search=image</a></span></li>
<li><strong>Open datasets</strong>: <span class="MsoHyperlink"><a href="https://skymind.ai/wiki/open-datasets">https://skymind.ai/wiki/open-datasets</a></span></li>
<li><strong>Wikipedia</strong>: <span class="MsoHyperlink"><a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#Object_detection_and_recognition">https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#Object_detection_and_recognition</a></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating your own image dataset using Google images</h1>
                </header>
            
            <article>
                
<p>Let’s say, for whatever reason, we need to determine what kind of dog a picture is of, but we do not have any pictures readily available on our computer. What can we do? Well, perhaps the easiest approach is to open Google Chrome and search for the images online.</p>
<p>As an example, let’s say we are interested in Doberman dogs. Just open Google Chrome and search for <strong>doberman</strong> pictures as shown below:</p>
<ol>
<li><strong>Perform a search for Doberman pictures</strong>: On searching, following the result<span> were obtained:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-753 image-border" src="assets/49739228-8671-4818-9de8-7aaadbb7b33a.png" style="width:160.00em;height:90.00em;"/></p>
<ol start="2">
<li><strong>Open the JavaScript console</strong>: You can find the JavaScript Console in Chrome <span>in the top-right</span> menu<span>:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-855 image-border" src="assets/95960d28-7610-4dc0-9354-dd2383ef520a.png" style="width:33.00em;height:26.83em;"/></p>
<p style="padding-left: 60px">Click on <span class="packt_screen">More tools</span> and then <span class="packt_screen">Developer tools</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-754 image-border" src="assets/21a212a0-9ecb-4c29-b2c0-c3ffff13b4c6.png" style="width:50.83em;height:28.58em;"/></p>
<p style="padding-left: 60px">Make sure that you select the <span class="packt_screen">Console</span> tab, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-755 image-border" src="assets/741be916-0f66-4f48-b82f-2c6a18247ca8.png" style="width:46.50em;height:26.17em;"/></p>
<ol start="3">
<li><strong>Using JavaScript</strong>: Continue to scroll down until you think you have enough images for your use case. Once this is done, go back to the <span class="packt_screen">Console</span> tab in <span class="packt_screen">Developer tools</span>, and then copy and paste the following script:</li>
</ol>
<pre style="padding-left: 60px">//the jquery  is pulled down in the JavaScript console<br/>var script = document.createElement('script');<br/>script.src = "https://ajax.googleapis.com/ajax/libs/jquery/2.2.0/jquery.min.js";<br/>document.getElementsByTagName('head')[0].appendChild(script);<br/>//Let us get the URLs<br/>var urls = $('.rg_di .rg_meta').map(function() { return JSON.parse($(this).text()).ou; });<br/>// Now, we will write the URls one per line to file<br/>var textToSave = urls.toArray().join('\n');<br/>var hiddenElement = document.createElement('a');<br/>hiddenElement.href = 'data:attachment/text,' + encodeURI(textToSave);<br/>hiddenElement.target = '_blank';<br/>hiddenElement.download = 'urls.txt';<br/>hiddenElement.click();</pre>
<p style="padding-left: 60px">This code snippet collects all the image URLs and saves them to a file called <kbd>urls.txt</kbd> in your default <kbd>Downloads</kbd> directory.</p>
<ol start="4">
<li><strong>Use Python to download the images</strong>: Now, we will use Python to read the URLs of the images from <kbd>urls.txt</kbd> and download all the images into a folder:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-705 image-border" src="assets/bfeb9d0e-f904-4a4b-9ab9-f98a96ce1561.png" style="width:17.42em;height:6.50em;"/></p>
<p>This can be done easily by following the following steps:</p>
<ol>
<li>Open a Python notebook and copy and paste the following code to download the images:</li>
</ol>
<pre style="padding-left: 60px"># We will start by importing the required pacages<br/>from imutils import paths<br/>import argparse<br/>import requests<br/>import cv2<br/>import os</pre>
<ol start="2">
<li>After importing, start constructing the arguments, and after constructing parsing the arguments is important:</li>
</ol>
<pre style="padding-left: 60px">ap = argparse.ArgumentParser()<br/>ap.add_argument("-u", "--urls", required=True,<br/>help="path to file containing image URLs")<br/>ap.add_argument("-o", "--output", required=True,<br/>help="path to output directory of images")<br/>args = vars(ap.parse_args())</pre>
<ol start="3">
<li>The next step includes grabbing the list of URLs from the input file counting total number of images downloaded:</li>
</ol>
<pre style="padding-left: 60px">rows = open(args["urls"]).read().strip().split("\n")<br/>total = 0<br/># URLs are looped in<br/>for url in rows:<br/>try:<br/># Try downloading the image<br/>r = requests.get(url, timeout=60)<br/>#The image is then saved to the disk<br/>p = os.path.sep.join([args["output"], "{}.jpg".format(<br/>str(total).zfill(8))])<br/>f = open(p, "wb")<br/>f.write(r.content)<br/>f.close()<br/>#The counter is updated<br/>print("[INFO] downloaded: {}".format(p))<br/>total += 1</pre>
<ol start="4">
<li>During the download process, the exceptions that are thrown need to be handled:</li>
</ol>
<pre style="padding-left: 60px">print("[INFO] error downloading {}...skipping".format(p))</pre>
<ol start="5">
<li>The image paths that are downloaded need to be looped over:</li>
</ol>
<pre style="padding-left: 90px">for imagePath in paths.list_images(args["output"])</pre>
<ol start="6">
<li>Now, decide whether the image should be deleted or not and accordingly initialize:</li>
</ol>
<pre style="padding-left: 60px">delete = False</pre>
<ol start="7">
<li>The image needs to be loaded. Let's try to do that:</li>
</ol>
<pre style="padding-left: 60px">image = cv2.imread(imagePath)</pre>
<ol start="8">
<li>If we weren't able to load the image properly, since the image is <kbd>None</kbd>, then it should be deleted from the disk:</li>
</ol>
<pre style="padding-left: 60px">if image is None:<br/>delete = True</pre>
<ol start="9">
<li>Also, if OpenCV was unable to load the image, it means the image is corrupt and should be deleted:</li>
</ol>
<pre style="padding-left: 60px">except:<br/>print("Except")<br/>delete = True</pre>
<ol start="10">
<li>Give a final check and see whether the image was deleted:</li>
</ol>
<pre style="padding-left: 60px">if delete:<br/>print("[INFO] deleting {}".format(imagePath))<br/>os.remove(imagePath)</pre>
<ol start="11">
<li class="mce-root">
<p>With that complete, let’s download this notebook as a Python file and name it <kbd>image_download.py</kbd>. Make sure that you place the <kbd>urls.txt</kbd> file in the same folder as the Python file that you just created. This is very important.</p>
</li>
<li class="mce-root">
<p>Next, we need to execute the Python file we just created. We will do so by using the command line as shown here (make sure your <kbd>path</kbd> variable points to your Python location):</p>
</li>
</ol>
<pre style="padding-left: 60px">Image_download.py --urls urls.txt --output Doberman</pre>
<p><span>By executing this command, the images will be downloaded to the folder named Doberman. Once this has been completed, you should see all the images of the Doberman that you viewed in Google Chrome, like what is shown in the following image:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-756 image-border" src="assets/06057770-7f65-4faf-9b99-c9c6f2bdbb0c.png" style="width:55.25em;height:31.08em;"/></p>
<p>Select the required folder for saving the images as shown: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-757 image-border" src="assets/8e2f30d2-cac4-4cba-8228-62723d73a617.png" style="width:16.67em;height:6.08em;"/></p>
<p>That's it we now have a folder full of Doberman images. The same method can be applied to create a folder of any other type of category that we may need.</p>
<p><span>There may be a number of images that are part of the Google image results that are not desirable. Ensure that you browse through the images and remove any unwanted images.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Alternate approach of creating custom datasets from videos</h1>
                </header>
            
            <article>
                
<p>There may be occasions when the images we find via the Internet do not satisfy our requirements or, we may find no images at all. This could be because of the uniqueness of the data, the use case at hand, copyright restrictions, the required resolution, etc. In this case, an alternative approach would be to record a video of the object you need, extract the frames of that video that meet your requirements, and save each frame as an individual image. How would we go about doing that?</p>
<p>Let's say that we have a skin condition that we are unable to find information about online. We need to somehow classify what this skin condition might be. However, in order to do this, we need to have an image of this condition. Accordingly, we could take a video of that skin condition and save the video file to a file. For the purposes of discussion, let’s say that we save the video with the filename <kbd>myvideo.mp4</kbd>.</p>
<p>Once this is complete, we could use the following Python script to break the video into images and save it into a folder. This function will take the path of the video file, break it into frames based on frequency, and save the corresponding images to a specified output location. Here is that function in its entirety:</p>
<pre>import sys<br/>import argparse<br/>import os<br/>import cv2<br/>import numpy as np<br/>print(cv2.__version__)</pre>
<p>This function takes the path of the video file, breaks it into frames based on frequency, and saves the corresponding images to a specified output location:</p>
<pre>def extractImages(pathIn, pathOut):<br/>count = 0<br/>vidcap = cv2.VideoCapture(pathIn)<br/>success,image = vidcap.read()<br/>success = True<br/>while success:<br/>vidcap.set(cv2.CAP_PROP_POS_MSEC,(count*10)) # Adjust frequency of frames here<br/>success,image = vidcap.read()<br/>print ('Read a new frame: ', success)<br/>#Once we identify the last frame, stop there<br/>image_last = cv2.imread("frame{}.png".format(count-1))<br/>if np.array_equal(image,image_last):<br/>break<br/>cv2.imwrite( os.path.join("frames","frame{:d}.jpg".format(count)), image) # save frame as JPEG file<br/>count = count + 1<br/>pathIn = "myvideo.mp4"<br/>pathOut = ""<br/>extractImages(pathIn, pathOut)</pre>
<p><span>As mentioned above, this will save every frame of the video in the current folder based on the frequency set. After running this script, you now will have created your image dataset and be able to use the images you need.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building your model using TensorFlow</h1>
                </header>
            
            <article>
                
<p><span>Now that we have seen several methods of obtaining the images we need, or, in the absence of any, creating our own, we will now use TensorFlow to create the classification model for our flower use case:</span></p>
<ol>
<li><strong>Creating the folder structure</strong>: To start with, let's create the folder structure that's required for our flower classification use case. First, create a main folder called <kbd>image_classification</kbd>. Within the <kbd>image_classification</kbd> folder, create two folders: <kbd>images</kbd> and <kbd>tf_files</kbd>. The <kbd>images</kbd> folder will contain the images that are required for model training, and the <kbd>tf_files</kbd> folder will hold all the generated TensorFlow-specific files during runtime.</li>
<li><strong>Downloading the images</strong>: Next, we need to download the images that are specific to our use case. Using the example of <strong>Flowers</strong><span>, our images will come from the VGG datasets page we discussed earlier.</span></li>
</ol>
<div class="packt_tip">Please feel free to use your own datasets, but make sure that each category is in its own separate folder. Place the downloaded image dataset within the <kbd>images</kbd> folder.</div>
<p style="padding-left: 60px">For example, the complete folder structure will look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-758 image-border" src="assets/2f389c5e-de53-43c8-b807-d68aacb9c7aa.png" style="width:17.33em;height:43.42em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li><strong>Creating the Python script</strong>: In this step, we will create the TensorFlow code that is required <span>to build our model. Create a Python file named</span> <kbd>retrain.py</kbd> <span>within the main</span> <kbd>image_classification</kbd> <span>folder.</span></li>
</ol>
<p>Once this is complete, the following code block should be copied and used. Below we have broken out the process into several steps in order to describe what is taking place:</p>
<ol>
<li>The following code block is the complete script that goes into <kbd>retrain.py</kbd>:</li>
</ol>
<pre style="padding-left: 60px">from __future__ import absolute_import<br/>from __future__ import division<br/>from __future__ import print_function<br/>import argparse<br/>import collections<br/>from datetime import datetime<br/>import hashlib<br/>import os.path<br/>import random<br/>import re<br/>import sys<br/>import tarfile<br/>import numpy as np<br/>from six.moves import urllib<br/>import tensorflow as tf<br/>from tensorflow.python.framework import graph_util<br/>from tensorflow.python.framework import tensor_shape<br/>from tensorflow.python.platform import gfile<br/>from tensorflow.python.util import compat<br/>FLAGS = None<br/>MAX_NUM_IMAGES_PER_CLASS = 2 ** 27 - 1 # ~134M</pre>
<ol start="2">
<li>Next, we need to prepare the images so that they can be trained, validated, and tested:</li>
</ol>
<pre style="padding-left: 60px">result = collections.OrderedDict()<br/>sub_dirs = [<br/>os.path.join(image_dir,item)<br/>for item in gfile.ListDirectory(image_dir)]<br/>sub_dirs = sorted(item for item in sub_dirs<br/>if gfile.IsDirectory(item))<br/>for sub_dir in sub_dirs:</pre>
<p>The first thing we are going to do is to retrieve the images from the directory path where they are stored. We will use the images to create the model graph using the model that you previously downloaded and installed.</p>
<p>The next step is to bottleneck the array initialization by creating what is known as <strong>bottleneck files</strong>. <strong>Bottleneck</strong> is an informal term used for the layer just before the final output layer that does the actual classification. (TensorFlow Hub calls this an <strong>image feature vector</strong>.) This layer has been trained to output a set of values that's good enough for the classifier to use in order to distinguish between all the classes it's been asked to recognize. This means that it must be a meaningful and compact summary of the images, since it must contain enough information for the classifier to make a good choice in a very small set of values.</p>
<p>It's important that we have bottleneck values for each image. If the bottleneck values aren't available for each image, we will have to create them manually because these values will be required in the future when training the images. It is highly recommended to cache these values in order to speed up processing time later. Because every image is reused multiple times during training, and calculating each bottleneck takes a significant amount of time, it speeds things up to cache these bottleneck values on disk to avoid repeated recalculated. By default, bottlenecks are stored in the <kbd>/tmp/bottleneck</kbd> directory (unless a new directory was specified as an argument).</p>
<p>When we retrieve the bottleneck values, we will do so based upon the filenames of images that are stored in the cache. If distortions were applied to images, there might be difficulty in retrieving the bottleneck values. The biggest disadvantage of enabling distortions in our script is that the bottleneck caching is no longer useful, since input images are never reused exactly. This directly correlates to a longer training process time, so it is highly recommended this happens once you have a model that you are reasonably happy with. Should you experience problems, we have supplied a method of getting the values for images which have distortions supplied as a part of the GitHub repository for this book.</p>
<div class="packt_infobox">
<p>Please note that we materialize the distorted image data as a NumPy array first.</p>
</div>
<p>Next, we need to send the running inference on the image. This requires a trained object detection model and is done by using two memory copies.</p>
<p>Our next step will be to apply distortion to the images. Distortions such as cropping, scaling and brightness are supplied as percentage values which control how much of each distortion is applied to each image. It's reasonable to start with values of 5 or 10 for each of them and then experiment from there to see which/what helps and what does not.</p>
<p>We next need to summarize our model based on accuracy and loss. We will use TensorBoard visualizations to analyze it. If you do not already know, TensorFlow offers a suite of visualization tools called TensorBoard which allows you to visualize your TensorFlow graph, plot variables about the execution, and show additional data like images that pass through it. The following is an example TensorBoard dashboard:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-759 image-border" src="assets/be6eec08-7056-441f-b2e3-5263c485e200.png" style="width:77.33em;height:63.00em;"/></p>
<p>Our next step will be to save the model to a file, as well as setting up a directory path to write summaries for the TensorBoard.</p>
<p>At this point we should point out the <kbd>create_model_info</kbd> function, that will return the model information. In our example below, we handle both MobileNet and Inception_v3 architectures. You will see later how we handle any other architecture but these:</p>
<pre>def create_model_info(architecture):<br/>architecture = architecture.lower()<br/>if architecture == 'inception_v3':<br/># pylint: disable=line-too-long<br/>data_url = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'<br/># pylint: enable=line-too-long<br/>bottleneck_tensor_name = 'pool_3/_reshape:0'<br/>bottleneck_tensor_size = 2048<br/>input_width = 299<br/>input_height = 299<br/>input_depth = 3<br/>resized_input_tensor_name = 'Mul:0'<br/>model_file_name = 'classify_image_graph_def.pb'<br/>input_mean = 128<br/>input_std = 128<br/>elif architecture.startswith('mobilenet_'):<br/>parts = architecture.split('_')<br/>if len(parts) != 3 and len(parts) != 4:<br/>tf.logging.error("Couldn't understand architecture name '%s'",<br/>architecture)<br/>return None<br/>version_string = parts[1]<br/>if (version_string != '1.0' and version_string != '0.75' and<br/>version_string != '0.50' and version_string != '0.25'):<br/>tf.logging.error(<br/>""""The Mobilenet version should be '1.0', '0.75', '0.50', or '0.25',<br/>but found '%s' for architecture '%s'""",<br/>version_string, architecture)<br/>return None<br/>size_string = parts[2]<br/>if (size_string != '224' and size_string != '192' and<br/>size_string != '160' and size_string != '128'):<br/>tf.logging.error(<br/>"""The Mobilenet input size should be '224', '192', '160', or '128',<br/>but found '%s' for architecture '%s'""",<br/>size_string, architecture)<br/>return None<br/>if len(parts) == 3:<br/>is_quantized = False</pre>
<p><span>If the above argument turns out to be false, this means that we encountered an architecture which we were not expecting. If this happens, we will need to execute the following code block to obtain the result. In this instance we are not dealing with either MobileNet or Inception_V3 and will default to using version 1 of MobileNet:</span></p>
<pre>else:<br/>if parts[3] != 'quantized':<br/>tf.logging.error(<br/>"Couldn't understand architecture suffix '%s' for '%s'", parts[3],<br/>architecture)<br/>return None<br/>is_quantized = True<br/>data_url = 'http://download.tensorflow.org/models/mobilenet_v1_'<br/>data_url += version_string + '_' + size_string + '_frozen.tgz'<br/>bottleneck_tensor_name = 'MobilenetV1/Predictions/Reshape:0'<br/>bottleneck_tensor_size = 1001<br/>input_width = int(size_string)<br/>input_height = int(size_string)<br/>input_depth = 3<br/>resized_input_tensor_name = 'input:0'<br/>if is_quantized:<br/>model_base_name = 'quantized_graph.pb'<br/>else:<br/>model_base_name = 'frozen_graph.pb'<br/>model_dir_name = 'mobilenet_v1_' + version_string + '_' + size_string<br/>model_file_name = os.path.join(model_dir_name, model_base_name)<br/>input_mean = 127.5<br/>input_std = 127.5<br/>else:<br/>tf.logging.error("Couldn't understand architecture name '%s'", architecture)<br/>raise ValueError('Unknown architecture', architecture)<br/>return {<br/>'data_url': data_url,<br/>'bottleneck_tensor_name': bottleneck_tensor_name,<br/>'bottleneck_tensor_size': bottleneck_tensor_size,<br/>'input_width': input_width,<br/>'input_height': input_height,<br/>'input_depth': input_depth,<br/>'resized_input_tensor_name': resized_input_tensor_name,<br/>'model_file_name': model_file_name,<br/>'input_mean': input_mean,<br/>'input_std': input_std,<br/>}<br/>==============================================================</pre>
<p class="mce-root"/>
<p><span>Another important point we should note is that we will need to decode the image JPEG data after processing. The following function, </span><kbd>add_jpeg_decoding</kbd><span>, is a complete code snippet which does this by calling the</span> <kbd>tf.image.decode_jpeg</kbd><span> function:</span></p>
<pre>def add_jpeg_decoding(input_width, input_height, input_depth, input_mean,<br/>input_std):<br/>jpeg_data = tf.placeholder(tf.string, name='DecodeJPGInput')<br/>decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)<br/>decoded_image_as_float = tf.cast(decoded_image, dtype=tf.float32)<br/>decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)<br/>resize_shape = tf.stack([input_height, input_width])<br/>resize_shape_as_int = tf.cast(resize_shape, dtype=tf.int32)<br/>resized_image = tf.image.resize_bilinear(decoded_image_4d,<br/>resize_shape_as_int)<br/>offset_image = tf.subtract(resized_image, input_mean)<br/>mul_image = tf.multiply(offset_image, 1.0 / input_std)<br/>return jpeg_data, mul_image</pre>
<p>And here, in all its glory is our <kbd>main</kbd> function. Basically we do the following:</p>
<ul>
<li>Set our logging level to <kbd>INFO</kbd></li>
<li>Prepare the file system for usage</li>
<li>Create our model information</li>
<li>Download and extract our data</li>
</ul>
<pre>def main(_):<br/>tf.logging.set_verbosity(tf.logging.INFO)<br/>prepare_file_system()<br/>model_info = create_model_info(FLAGS.architecture)<br/>if not model_info:<br/>tf.logging.error('Did not recognize architecture flag')<br/>return -1<br/>maybe_download_and_extract(model_info['data_url'])<br/>graph, bottleneck_tensor, resized_image_tensor = (<br/>create_model_graph(model_info))<br/>image_lists = create_image_lists(FLAGS.image_dir, FLAGS.testing_percentage,<br/>FLAGS.validation_percentage)</pre>
<p>The preceding <kbd>retrain.py</kbd> file is available for download as part of the assets within this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running TensorBoard</h1>
                </header>
            
            <article>
                
<p>To run TensorBoard, use the following command:</p>
<div>
<pre>tensorboard --logdir=path/to/log-directory</pre></div>
<p>Where <kbd>logdir</kbd> points to the directory where serialized data is contained. If this directory contains subdirectories which also contain serialized data, TensorBoard will visualize the data from all of those runs. Once TensorBoard is running, navigate your web browser to <kbd>localhost:6006</kbd> to view the TensorBoard and its associated data.</p>
<p>For those wanting or needing to learn more about TensorBoard, please check out the following tutorial at <a href="https://www.tensorflow.org/tensorboard/r1/summaries">https://www.tensorflow.org/tensorboard/r1/summaries</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter we have accomplished a lot in this small chapter. We began the chapter with understanding the various datasets that are available for image classification, as well as how we could obtain or create images if we could not find any that met our requirements. Next, then divided the chapter into two distinct sections. In the first section we learned about creating our own image dataset. In the second section we learned how to use TensorFlow to build the model.</p>
<p>In the next chapter, we are going to extend our TensorFlow knowledge even further by using various TensorFlow libraries to build a machine learning model which will predict body damage done to a car.</p>


            </article>

            
        </section>
    </body></html>