- en: Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll cover some of the basics of deep learning. Deep learning
    refers to neural networks with lots of layers. It's kind of a buzzword, but the
    technology behind it is real and quite sophisticated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The term has been rising in popularity along with machine learning and artificial
    intelligence, as shown in this Google trend chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00161.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As stated by some of the inventors of deep learning methods, the primary advantage
    of deep learning is that adding more data and more computing power often produces
    more accurate results, without the significant effort required for engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to be looking at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying handwritten mathematical symbols with CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisiting the bird species identifier to use images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning refers to several methods which may be used in a particular application.
    These methods include convolutional layers and pooling. Simpler and faster activation
    functions, such as ReLU, return the neuron's weighted sum if it's positive and
    zero if negative. Regularization techniques, such as dropout, randomly ignore
    weights during the weight update base to prevent overfitting. GPUs are used for
    faster training with the order that is 50 times faster. This is because they're
    optimized for matrix calculations that are used extensively in neural networks
    and memory units for applications such as speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Several factors have contributed to deep learning's dramatic growth in the last
    five years. Large public datasets, such as ImageNet, that holds millions of labeled
    images covering a thousand categories and Mozilla's Common Voice Project, that
    contain speech samples are now available. Such datasets have satisfied the basic
    requirement for deep learning-lot of training data. GPUs have transitioned to
    deep learning and clusters while also focusing on gaming. This helps make large-scale
    deep learning possible.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced software frameworks that were released open source and are undergoing
    rapid improvement are also available to everyone. These include TensorFlow, Keras,
    Torch, and Caffe. Deep architectures that achieve state-of-the-art results, such
    as Inception-v3 are being used for the ImageNet dataset. This network actually
    has an approximate of 24 million parameters, and a large community of researchers
    and software engineers quickly translating research prototypes into open source
    software that anyone can download, evaluate, and extend.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions and pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This sections takes a closer look at two fundamental deep learning technologies,
    namely, convolution and pooling. Throughout this section, we will be using images
    to understand these concepts. Nevertheless, what we''ll be studying can also be
    applied to other data, such as, audio signals. Let''s take a look at the following
    photo and begin by zooming in to observe the pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00162.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Convolutions occur per channel. An input image would generally consist of three
    channels; red, green, and blue. The next step would be to separate these three
    colors. The following diagram depicts this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00163.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A convolution is a kernel. In this image, we apply a 3 x 3 kernel. Every kernel
    contains a number of weights. The kernel slides around the image and computes
    the weighted sum of the pixels on the kernel, each multiplied by their corresponding
    kernel weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00164.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'A bias term is also added. A single number, the weighted sum, is produced for
    each position that the kernel slides over. The kernel''s weights start off with
    any random value and change during the training phase. The following diagram shows
    three examples of kernels with different weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00165.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see how the image transforms differently depending on the weights.
    The rightmost image highlights the edges, which is often useful for identifying
    objects. The stride helps us understand how the kernel slides across the image.
    The following diagram is an example of a 1 x 1 stride:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00166.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The kernel moves by one pixel to the right and then down. Throughout this process,
    the center of the kernel will hit every pixel of the image whilst overlapping
    the other kernels. It is also observed that some pixels are missed by the center
    of the kernel. The following image depicts a 2 x 2 stride:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00167.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In certain cases, it is observed that no overlapping takes place. To prove
    this, the following diagram contains a 3 x 3 stride:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00168.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In such cases, no overlap takes place because the kernel is the same size as
    the stride.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the borders of the image need to be handled differently. To affect
    this, we can use padding. This helps avoid extending the kernel across the border.
    Padding consists of extra pixels, which are always zero. They don''t contribute
    to the weighted sum. The padding allows the kernel''s weights to cover every region
    of the image while still letting the kernels assume the stride is 1\. The kernel
    produces one output for every region it covers. Hence, if we have a stride that
    is greater than 1, we''ll have fewer outputs than there were original pixels.
    In other words, the convolution helped reduce the image''s dimensions. The formula
    shown here tells us the dimensions of the output of a convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00169.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is a general practice to use square images. Kernels and strides are used
    for simplicity. This helps us focus on only one dimension, which will be the same
    for the width and height. In the following diagram, a 3 x 3 kernel with a (3,
    3) stride is depicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00170.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding calculation gives the result of 85 width and 85 height. The image's
    width and height have effectively been reduced by a factor of three from the original
    256\. Rather than use a large stride, we shall let the convolution hit every pixel
    by using a stride of 1\. This will help us attain a more practical result. We
    also need to make sure that there is sufficient padding. However, it is beneficial
    to reduce the image dimensions as we move through the network. This helps the
    network train faster as there will be fewer parameters. Fewer parameters imply
    a smaller chance of over-fitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'We often use max or average pooling between convolution dimensionality instead
    of varying the stride length. Pooling looks at a region, which, let us assume, is
    2 x 2, and keeps only the largest or average value. The following image depicts
    a 2 x 2 matrix that depicts pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00171.gif)'
  prefs: []
  type: TYPE_IMG
- en: A pooling region always has the same-sized stride as the pool size. This helps
    avoid overlapping.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling doesn't use any weights, which means there is nothing to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a relatively shallow **convolutional neural networks** (**CNNs**) representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00172.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Source: cs231.github.io, MIT License
  prefs: []
  type: TYPE_NORMAL
- en: We observe that the input image is subjected to various convolutions and pooling
    layers with ReLU activations between them before finally arriving at a traditionally
    fully connected network. The fully connected network, though not depicted in the
    diagram, is ultimately predicting the class. In this example, as in most CNNs,
    we will have multiple convolutions at each layer. Here, we will observe 10, which
    are depicted as rows. Each of these 10 convolutions have their own kernels in
    each column so that different convolutions can be learned at each resolution.
    The fully connected layers on the right will determine which convolutions best
    identify the car or the truck, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying handwritten mathematical symbols with CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This sections deals with building a CNN to identify handwritten mathematical
    symbols. We're going to use the `HASYv2` dataset. This contains 168,000 images
    from 369 different classes where each represents a different symbol. This dataset
    is a more complex analog compared to the popular MNIST dataset, which contains
    handwritten numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts the kind of images that are available in this
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00173.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And here, we can see a graph showing how many symbols have different numbers
    of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00174.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is observed that many symbols have few images and there are a few that have
    lots of images. The code to import any image is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00175.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We begin by importing the `Image` class from the `IPython` library. This allows
    us to show images inside Jupyter Notebook. Here''s one image from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00176.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is an image of the alphabet **A**. Each image is 30 x 30 pixels. This
    image is in the RGB format even though it doesn''t really need to be RGB. The
    different channels are predominately black and white or grayscale. We''re going
    to use these three channels. We then proceed to import CSV, which allows us to
    load the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00177.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This CSV file states all the different filenames and the class names. We import
    the image class from `pil`, which allows us to load the image. We import `preprocessing.image`, which
    then allows us to convert the images into `numpy` arrays. Let''s us then go through
    the data file, taking a closer look at every filename and loading it, while recording
    which class it belongs to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00178.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The immediate next step would be to save the images and the classes and use
    the CSV reader. We need to set a counter to make sure we skip the first row, which
    is the header of the CSV file. Only after this, we proceed to open the image,
    which is in the first column of each row. This is converted into an array. The
    achieved result will have dimensions of 30 x 30 x 3, which is interpreted as 30
    width, 30 height, and 3 channels (RGB).
  prefs: []
  type: TYPE_NORMAL
- en: 'These three channels will have numbers between 0 and 255\. These are typical
    pixel values, which are not good for a neural network. We need values that lie
    between 0 and 1 or -1 and 1\. To do this, we divide each pixel value by 255\.
    To make things easier, we''re going to collect the filename, the class name, and
    the image matrix and put them into our images list. We will also make a note of
    the name of the class. The following snippet will make us understand the concept
    to a greater depth:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00179.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The file is named `hasy-data/v2-00000.png`. `A` is the name of the class followed
    by the array. The array has dimensions 30 x 30 x 3\. The innermost and last dimension,
    is 3\. Each 1.0 depicts the color white. We understand this because we divided
    everything by 255 as mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have 168,000 images in the `HASYv2` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00180.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We then proceed to shuffle and then split the data on an 80% train, 20% test
    basis. As seen in the following codeblock, we first shuffle, then proceed to split
    the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00181.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Because we use these tuples with three different values, we''re going to need
    to ultimately collect all that into a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00182.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We need to collect the images as well as the labels. To collect the images,
    we go through each row and take each third element. This element is the image
    matrix. We stick it all together into a `numpy` array. The same is done for the
    train and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For the outputs, we need to go and pick out the second value. These are still
    strings, such as `a` and `=`. We need to convert the second value into one-hot
    encoding before it can be used for a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'We proceed to use scikit-learn''s preprocessing label encoder and one-hot encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00183.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We''re going to make a `LabelEncoder` object and we''re going to both fit and
    transform on the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00184.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The `fit` function learns which classes exist. It learns that there are 369
    different class names. The `tranform` function turns them into integers. This
    is done by sorting the classes and giving each class an integer ID. `integer_encoded`
    helps to reproduce the list of classes as integer IDs. The one-hot encoder takes
    these integers and fits on them; this too learns how many different integers are
    represented. Just as `LabelEncoder` learned about the class names, `onehot_encoder`
    is going to learn that there are 369 different integers.
  prefs: []
  type: TYPE_NORMAL
- en: The code then moves to `LabelEncoder` which transforms `train_output` into integers.
    These integers are then transformed into one-hot encoding. The one-hot encoding
    returns a 369-dimension with the first dimension of 369 values and a vector of
    369 values. All values are zeros except for a single 1\. The position of this
    1 depends on which class it is. `test_output` undergoes the same process. When
    the training data for input and output is ready, we proceed to build a neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we are going to use `Sequential` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00185.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Sequential is a feed-forward network. Even though there are convolutions that
    still feed forward and are not recurrent, there are no cycles. Dense layers are
    used at the end of the network. We also use `Dropout` to try to prevent overfitting.
    When we switch from convolutions to dense layers, we need to use the `flatten`
    command, since convolutions are two-dimensional and dense layers are not. We also
    need to use `Conv2D` and `MaxPooling2D`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block is our network design:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00186.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is modeled after MNIST design, which handles handwritten numbers. We start
    by making a sequential model. We need to add a convolution layer that has 32 different
    convolutions. The kernel size will be 3 x 3 and the activation will be ReLU. Since
    this is the first layer, we need to mention the input shape. If you recall, the
    dimensions were 30 x 30 x 3.
  prefs: []
  type: TYPE_NORMAL
- en: We use the kernel size of 3 x 3 and the stride as 1 as it is the default value.
    Having the stride as 1 will require padding. This is going to produce a 30 x 30
    x 32 shape because there are 32 convolutions. The 30 x 30 dimensions remain constant.
    WE now observe that we haven't really reduced dimensions just by doing this convolution.
  prefs: []
  type: TYPE_NORMAL
- en: '`MaxPooling` is used to reduce the dimensions by half. This is possible because
    it has a 2 x 2 pool size. We then follow with another convolution layer, which
    is another dimensionality reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: After all the convolutions have taken place, we flatten everything. This converts
    a two-dimensional representation into a one-dimensional representation. This is
    then fed into a dense layer with more than 1,000 neurons.
  prefs: []
  type: TYPE_NORMAL
- en: This dense layer will then have a `tanh` activation. This is then fed into another
    dense layer of neurons. This time around, there are 369 of them for the class
    outputs. This is the `onehot_encoding` output. We're not going to do any particular
    activation except for softmax. So, the original values will be rescaled to be
    between 0 and 1\. This means that the sum of all the values across the 369 different
    neurons is 1.0\. Softmax basically turns the output into a probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Proceeding to compiling `categorical _crossentropy` again helps us predict
    one of multiple classes. You would want to do this on the `adam` optimizer and
    observe it''s accuracy. Here''s the model''s summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00187.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It is observed that the convolution layer doesn't change the dimensions, but
    the pooling does. It reduces it by half because of the odd dimension size, that
    is, 15\. The next layer is at 13 output, which also gets reduced by half. The
    `conv2d_1 (Conv2D)` parameters are used for learning the convolutions. The `dense_1
    (Dense)` parameters are used for learning the weights connected to the prior layer.
    In a similiar fashion, the `dense_2 (Dense)` parameters are for the weights for
    the prior layer. Ultimately, we have about 1.6 million parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to visualize the performance''s accuracy and validation''s accuracy
    with TensorBoard. We''re going to save all the results into a directory called
    `mnist-style` because that''s the style of the network we built earlier. The following
    is a callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00188.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Keras supports callbacks of various types. The callback is used in the `fit`
    method, so after every epoch, it calls the callback. It passes information to
    the callback, such as the validation loss and the training loss. We use 10 epochs
    and a batch size of 32, with a 0.2, 20%, validation split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the result of the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00189.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now, there are a lot of choices, but ultimately we need to check them. We got
    about 76% validation accuracy, and when we test this out on the test set, we get
    the same 76% accuracy. Now, there were a lot of decisions in this design, including
    how many convolution layers to have and what size they should be, what kernel
    should be used or what size of kernel, what kind of stride, what the activation
    was for the convolutions, where the max pooling showed up, if it ever did, what
    the pooling size was, how many dense layers we have, when do they appear, what
    is the activation, and so on and so forth. A lot of decisions. It's quite difficult
    to know how to choose these different designs. These are actually called **hyperparameters**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights that can be learned during the fit procedure are just called parameters,
    but the decisions you have to make about how to design the network and the activation
    functions and so forth we call hyperparameters, because they can''t be learned
    by the network. In order to try different parameters, we can just do some loops:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00190.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will time how long it takes to train each of these. We will collect the results,
    which would be the accuracy numbers. Then, we will try a convolution 2D, which
    will have one or two such layers. We're going to try a dense layer with 128 neurons.
    We will try a dropout as `for dropout in [0.0, 0.25, 0.50, 0.7`, which will be
    either yes or no, and means 0-25%, 50%, 75%. So, for each of these combinations,
    we make a model depending on how many convolutions we're going to have, with convolution
    layers either one or two. We're going to add a convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'If it''s the first layer, we need to put in the input shape, otherwise we''ll
    just add the layer. Then, after adding the convolution layer, we''re going to
    do the same with max pooling. Then, we''re going to flatten and add a dense layer
    of whatever size that comes from `for dense_size in [128, 256, 512, 1024, 2048]:
    loop`. It will always be `tanh`, though.'
  prefs: []
  type: TYPE_NORMAL
- en: If `Dropout` is used, we're going to add a dropout layer. Calling this dropout
    means, say it's 50%, that every time it goes to update the weights after each
    batch, there's a 50% chance for each weight that it won't be updated, but we put
    this between the two dense layers to kind of protect it from overfitting. The
    last layer will always be the number of classes because it has to be, and we'll
    use softmax. It gets compiled in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up a different log directory for TensorBoard so that we can distinguish
    the different configurations. Start the timer and run fit. Do the evaluation and
    get the score, stop the timer, and print the results. So, here it is running on
    all of these different configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00191.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 0.74 is the actual test set accuracy. So, you can see that there are a lot of
    different numbers for accuracy. They go down to low point sevens up to the high
    point sevens, and the time differs depending on how many parameters there are
    in the network. We can visualize these results because we are using the callback
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the accuracy and loss, which are from the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00192.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And here''s the validation accuracy and validation loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00193.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Zoom out a bit so that we can see the configurations on the side, and then
    we can turn them all off. Turn `mnist-style` back on. This was the first one we
    tried:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00194.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the accuracy goes up and the loss goes down. That's pretty
    normal. Validation accuracy goes up and loss goes down, and it mostly stays consistent.
    What we don't want to see is validation loss skyrocketing after a while, even
    though the accuracy is going way up. That's pretty much by-definition overfitting.
    It's learning the training examples really well, but it's getting much worse on
    the examples it didn't see. We really don't want that to happen. So, let's compare
    a few things. First, we'll compare different dropouts. Let's go to `conv2d_1`-`dense_128`
    but with different dropouts.
  prefs: []
  type: TYPE_NORMAL
- en: 'As far as loss goes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00195.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that with a very low dropout, such as 0 or 0.25, the loss is minimized.
    That's because if you want to really learn that training set, don't refuse to
    update weights. Instead, update all of them all the time. With that same run,
    by looking at the dark blue line, we can see that it definitely overfit after
    just two epochs because the validation loss, the examples it did not see, started
    to get much worse. So, that's where the overfitting started. It's pretty clear
    that dropout reduces overfitting. Look at the 0.75 dropout. That's where the validation
    loss just got better and better, which means lower and lower.
  prefs: []
  type: TYPE_NORMAL
- en: 'It doesn''t make it the most accurate, though, because we can see that the
    accuracy is not necessarily the best for our training set or the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00196.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Actually, about 0.5 seems pretty good for a validation set. Now, let's just
    make sure it's the same for other layers. Again, with no dropouts (0.0), we get
    the lowest training loss but the highest validation loss. Likewise, we get a 0.75
    dropout for the lowest validation loss but not necessarily the best training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s compare how many dense layers they have. We''re just going to stick
    with dropout 0.5, so we''ll use `conv2d_1`. So, we have one convolution layer,
    `dense_*`, and a dropout of 0.50:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00197.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So the choice here is, does the dense layer have 128, 256, 512, 1,024, or 2,048?
    In the previous graph, we can see that there are some clear cases of overfitting.
    Pretty much anything that''s not the 128 starts to suffer from overfitting. So,
    a dense layer of 128 is probably the best choice. Now, let''s compare one convolution
    layer to two convolution layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00198.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Not a big difference, actually. For validation, we get two convolution layers
    and receive the lowest loss, which is usually the same as the highest accuracy.
    This means that we''ve narrowed down. This is called model selection, which is
    all about figuring out what the best model is, as well as the best hyperparameters.
    We''ve narrowed this down to the two-dimensional convolution, two layers of that,
    128 dense in the first dense layer, and 50% dropout. Given that, let''s retrain
    on all the data so that we have the best trained model we could possibly have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00199.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We get our two convolution layers, we do our dense 128 dropout 0.5, and in this
    case we take all the data we have, the entire dataset trained and tested, and
    stick it all together. Now, we can't really evaluate this model because we just
    lost our testing set, so what we're going to do instead is use this model to predict
    other images. Actually, we're going to save the model after it's fit and we're
    going to show how to load in a minute. If you're going to load this in another
    file, you're also going to want to know what those labels were called because
    all we know is the one-hot encoding. From the one-hot encoding, we can get back
    the integer number, but still that's not the same as the actual name of the symbol.
    So, we have to save the classes from `LabelEncoder` and we're just going to use
    a `numpy` file to save that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00200.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This could actually be all in a separate file. You can load everything again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00201.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Import `keras.models` and you can use the `load _model` feature. The model file
    there actually saves the structure as well as the weights. That's all you need
    to do to recover the network. You can print the summary again. For `LabelEncoder`,
    we need to call the constructor again and give it the classes that we saved ahead
    of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can make a function called predict takes an image. We do a little bit
    of preprocessing to turn the image into an array, we divide it by 255, and we
    predict. If you have a whole set of images, you won''t need to do this reshape,
    but since we just have one, we can put it in an array that has a single row. We
    will get the prediction out of this, and using `LabelEncoder`, we can reverse
    the prediction to the actual name of the class, the name of the symbol, and which
    prediction? Well, it''s one-hot encoding, so you can figure out the position of
    the highest number. This takes all the neuron outputs, the 369, figures out what
    the largest confidence number is, and says that''s the one that was predicted.
    Therefore, one-hot encoding would tell you this particular symbol, and then we
    can print it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00202.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here''s how we can use that function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00203.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We're actually using the training images for this purpose instead of making
    new ones, but you get the idea. You take an image that says that's an `A`, and
    I'm 87% confident about it. For pi prediction, we're 58% confident and for alpha
    prediction, we're 88% confident. Next, we'll look at the bird species example
    we used previously, and instead of using all of the attributes that humans created,
    we're going to use the images themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the bird species identifier to use images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we're going to revisit the bird species identifier from before.
    This time, we're going to update it to use neural networks and deep learning.
    Can you recall the birds dataset? It has 200 different species of birds across
    12,000 images. Unlike last time, we won't be using the human-labeled attributes,
    and instead we'll use the actual images without any pre-processing. In our first
    attempt, we're going to build a custom convolutional neural network, just like
    we did for the mathematical symbols classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go to the code. We will start with the typical imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00204.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll make some convenience variables, the rows and columns of the image,
    the width and height, and the number of channels, RGB, though every bird image
    will be equal. Even though they''re not all necessarily the same size, we''re
    going to resize them to this size so that they''re all consistent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00205.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, this project introduces an interesting feature on Keras called an **image
    data generator**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00206.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The data generator can produce new images from the existing training set and
    these new images can have various differences; for example, they can be rotated,
    they can be flipped  horizontally or vertically, and so forth. Then, we can generate
    more examples than we actually started with. This is a great thing to do when
    you have a small number of training examples. We have, in our case, about 6,000
    training sets. That's relatively small in deep learning, so we want to be able
    to generate more; the data generator will just keep generating them as long as
    we keep asking for them. For the training images, we want to also generate versions
    with the horizontal flip. We don't want to do a vertical flip because I don't
    expect any bird images to be upside down. We also want to support rotations of
    up to 45 degrees, and we want to rescale all the pixel values to divide by 255\.
    Actually, `ImageDataGenerator` just calls the constructor, so nothing's actually
    happened yet. What you want to do next is use `flow_from_directory`, so that your
    images can be organized into directories or subdirectories.
  prefs: []
  type: TYPE_NORMAL
- en: We have a `train` directory, and inside that there's going to be a folder for
    each bird class. So, there's 200 different folders inside train and inside those
    folders are the images for that particular bird. We want all the images to be
    resized to 256 x 256 and we can indicate that instead of using binary, we want
    to use categorical classes, meaning that we will have lots of different classes
    (200, in this case). We're going to use the data generator for the test set too,
    just because `flow_from_directory` is a convenient function. We don't want to
    do any flips, though, or rotations. We just want to use the testing set as is
    so we can compare it with other people. The other really convenient thing about
    `flow_from _directory` is that it's automatically going to produce a `numpy` matrix
    with the image data, and it's also going to give the class values in one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: So, what was several steps before is now being done all at once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, I don''t really need to do a reset, but since these are technically iterators,
    if you''re constantly fixing the model and trying to retrain, then you might want
    to do a reset so that you get all the same images in the same order. In any event,
    it''s an iterator, so you can call next, reset, and so forth:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00207.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will build a sequential model, which is going to be a convolutional
    model. We have a convolution kernel of 3 x 3, 64 of this. We also have a `relu`
    and another convolution built by `relu`, which we can do a max pooling with, and
    just from experimentation, I discovered that this works relatively well: 3 x 3
    followed by 3 x 3, each 64\. By having a pretty dramatic max point of 4 x 4, so
    we repeat this process and then we flatten. We have a dropout of 50% just to reduce
    overfitting, a dense of 400 neurons, another dropout, and then 200 for the output
    because there are 200 different classes, and because it''s categorical one-hot
    encoding, we want to use softmax so that only one of those 200 has the highest
    value. We also want to ensure that they all add up to 1.0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the summary of the model. Ultimately, we have about 5 million parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00208.jpeg)![](img/00209.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The different variations I did that had far more parameters, such as, say, 100
    million performed worse because there were just too many parameters. There's either
    too many parameters, meaning it's really hard to train it to learn anything because
    obviously all the parameters start random, so it's really hard to make those parameters
    trend toward the right values, or there are so few that it's not going to learn
    anything either. There's kind of a balance that you have to find, and 5 million,
    I think, is somewhere near that balance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you use a generator, you don''t have all the data for the training
    prepared ahead of time; it''s going to produce those images as it goes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00210.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: That makes it actually quite memory-efficient. You don't have to load the whole
    dataset ahead of time. It'll just make it as needed, but you have to call `fit
    _generator` instead of just using fit. What you give instead of the train input
    and train output is the generator. The generator knows how to produce the image
    matrices and it knows how to produce one-hot encoding. So, again, that's extremely
    convenient when you have images. There's other kinds of generators, too. Look
    at the Keras documentation for these. `steps_per_epoch` shows how many images
    to produce per epoch, or how many batches to produce. The generator, by default,
    produces batches of 32 images. Regarding the number of epochs, and if you want
    to do some statistics on TensorBoard, you can set up a callback and verbose 2
    so that we can see some output here.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00211.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the training accuracy is on the images that is training on.
    It's not very accurate for what the accuracy is going to be on the test set, so
    we do this separately. The test images are also in a generator. You don't just
    evaluate—you use `evaluate_generator` and you say, *how many images do you want
    to evaluate?* We'll just do 1,000, and we'll get 22% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: That's not so bad. Random guessing would yield 0.5%, so 22% is pretty good,
    and that's just from a handcrafted model starting from scratch that had to learn
    everything from those bird images. The reason I'm saying things like this is because
    the next thing we're going to do is extend a pre-trained model to get a good boost
    in accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'This model was built by hand, but it would be even better to extend something
    such as `Inceptionv3`, which is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00212.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It's quite deep; it has a lot of convolutional layers and, like most CNNs, it
    ends with a fully-connected layer or perhaps multiple fully connected layers.
    The `Inceptionv3` model was designed for ImageNet. Well, it's the dataset, and
    there's competitions associated with it where there are millions of images and
    1,000 different classes, such as insects, houses, cars, and so on. The `Inceptionv3`
    model is state-of-the-art, or it was at one point. It was ImageNet's competition
    to combat other databases. We're going to use most of this network all the way
    up until the fully-connected layers. We don't want the final fully-connected or
    dense layers because those are designed for ImageNet. Specifically, there are
    1,000 outputs and that's not good for us. We don't need to recognize the ImageNet
    images. We do need to recognize our bird images however, and there's only 200
    different classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we just chop off the front of that and replace it with our own fully-connected
    layer, or multiple layers. We''re going to use all the convolutions that it learned,
    and all of the kernels that it learned based on those ImageNet images. Let''s
    go to the code. To do this, import `Inceptionv3` from Keras''s applications. There''s
    other models that you can choose from that Keras has available as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00213.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We're going to use the data generator just like we did previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where it starts to become different:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00214.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: First, load the `InceptionV3` model using the ImageNet weights. `include_top
    =False` means to drop off the dense fully connected layers at the top. That's
    what they call the top. That's where it finally produces 1,000 different outputs.
    We don't want that. We want just the convolutions. This would be called the `base_model`.
    Call `x`, which is the output of the base model, add a `GlobalAveragePooling`,
    which means that it's computing the average across the whole convolution, and
    then put in some dense layers, with 1,024 dense neurons and another layer of 200\.
    Of course, the 200 is because we have 200 different bird species, and the 1,024
    is just to learn how the convolutions can match the bird species and then produce
    a model with those layers. The input of the model is the input of `Inceptionv3`
    and the output is `out_layer = Dense(200, activation='softmax')(x)`.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you can call regular model functions such as compile, but before
    we compile, we want to mark all of the base model layers and all of the convolutions
    as not trainable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to perform two steps here. When we attached our new two dense
    layers, the 1,024 dense and the 200 dense, those have random weights, so they''re
    pretty much useless so far. The convolutions have been learned on ImageNet, so
    they''re good. We don''t want to change the convolutions below all those kernels
    by training on our bird images until we get that new pair of dense layers in the
    right order. So, we''re first going to mark those layers from the inception model
    as not trainable; just keep those numbers as they are—we''re only going to train
    our two new layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00215.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: That happens next on the fit generator, just like before.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will do 100 epochs to start off:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00216.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And we''ll do an evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00217.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, now, we're up to 44% accuracy. So just by using the inception v3 weights
    and structure or ImageNet but replacing the top two layers with our own fully-connected
    network, we get a 20% boost from what we had with our own custom convolutional
    neural network. But we can do even better.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use what we just got so that the model has now trained the top two layers
    and marked everything as trainable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00218.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, now that the top two layers are kind of massaged into a form that is reasonable,
    with 44% accuracy, we''re going to let the entire network update all of our bird
    images. We''re going to do it very slowly using stochastic gradient descent with
    a very slow learning rate and a high momentum. Going through 100 epochs, we now
    have 64%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00219.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, we basically did a 20% boost each time. With the custom CNN, we got 22%
    accuracy just by starting from scratch. Now, of course, this is not as big of
    a network as the inception model, but it kind of shows what happens if you just
    start from scratch. Then, we started with inception, all the kernels, but then
    added our own random 2 layers on top, with random weights, trained those weights
    but did not change the kernels, and we got 44% accuracy. Finally, we went through
    and updated all the weights, kernels, and the top layer, and we got 64% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, this is far far better than what random guessing would be, which is 0.5%,
    and it''s been an increasing gain in accuracy each time we''ve improved the model.
    You can save the result and then you can load it into a separate file, perhaps
    by loading the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00220.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You also want to know what the class names are if you want to print the name
    of the bird to the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00221.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, we can just list the subdirectories in a sorted form because
    that''s going to match the one -hot encoding, and we can define a function called
    `predict` where you give it a filename with an image in it and it loads that image.
    Make sure it resizes it and converts it into an array, divides it by 255, and
    then runs the predictor. All this was done for us before with the image generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00222.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'But now, because we''re doing this one at a time, we''re just going to do it
    by hand instead. Run the prediction, find out what the best score was, the position,
    and retrieve the class name and then print it, plus the confidence. There''s a
    couple of examples of just birds that I found on the internet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00223.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: I can't guarantee that these were not part of the training set, but who knows.
    In the case of the hummingbird, they got it right. The house wren was also predicted
    correctly. However, the goose was not predicted correctly. This is an example
    of letting the user type in filenames. So if you have your own images that are
    relatively close to photography type images, you should consider using a pre-trained
    model like `InceptionV3` to get a major gain in accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed deep learning and CNNs. We practiced with convolutional
    neural networks and deep learning with two projects. First, we built a system
    that can read handwritten mathematical symbols and then revisited the bird species
    identifier form and changed the implementation to use a deep convolutional neural
    network that is significantly more accurate. This concludes the Python AI projects
    for beginners.
  prefs: []
  type: TYPE_NORMAL
