- en: Chapter 9. Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reinforcement Learning** (**RL**) is a framework that is used by an agent
    for decision-making. The agent is not necessarily a software entity such as in
    video games. Instead, it could be embodied in hardware such as a robot or an autonomous
    car. An embodied agent is probably the best way to fully appreciate and utilize
    reinforcement learning since a physical entity interacts with the real-world and
    receives responses.'
  prefs: []
  type: TYPE_NORMAL
- en: The agent is situated within an **environment**. The environment has a **state**
    that can be partially or fully observable. The agent has a set of **actions**
    that it can use to interact with its environment. The result of an action transitions
    the environment to a new state. A corresponding scalar **reward** is received
    after executing an action. The goal of the agent is to maximize the accumulated
    future reward by learning a **policy** that will decide which action to take given
    a state.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning has a strong similarity to human psychology. Humans learn
    by experiencing the world. Wrong actions result in a certain form of penalty and
    should be avoided in the future, whilst actions which are right are rewarded and
    should be encouraged. This strong similarity to human psychology has convinced
    many researchers to believe that reinforcement learning can lead us towards **Artificial
    Intelligence** (**AI**).
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning has been around for decades. However, beyond simple world
    models, RL has struggled to scale. This is where **Deep Learning** (**DL**), came into
    play. It solved this scalability problem which opened up the era of **Deep Reinforcement
    Learning** (**DRL**), which is what we are going to focus on in this chapter.
    One of the notable examples in DRL is the work of DeepMind on agents that were
    able to surpass the best human performance on different video games. In this chapter,
    we discuss both RL and DRL.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the goal of this chapter is to present:'
  prefs: []
  type: TYPE_NORMAL
- en: The principles of RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Reinforcement Learning technique, Q-Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced topics including **Deep Q-Network** (**DQN**), and **Double Q-Learning**
    (**DDQN**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructions on how to implement RL on Python and DRL within Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principles of reinforcement learning (RL)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Figure 9.1.1* shows the perception-action-learning loop that is used to describe
    RL. The environment is a soda can sitting on the floor. The agent is a mobile
    robot whose goal is to pick up the soda can. It observes the environment around
    it and tracks the location of the soda can through an onboard camera. The observation
    is summarized in a form of state which the robot will use to decide which action
    to take. The actions it takes may pertain to low-level control such as the rotation
    angle/speed of each wheel, rotation angle/speed of each joint of the arm, and
    whether the gripper is open or close.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, the actions may be high-level control moves such as moving the
    robot forward/backward, steering with a certain angle, and grab/release. Any action
    that moves the gripper away from the soda receives a negative reward. Any action
    that closes the gap between the gripper location and the soda receives a positive
    reward. When the robot arm successfully picks up the soda can, it receives a big
    positive reward. The goal of RL is to learn the optimal policy that helps the
    robot to decide which action to take given a state to maximize the accumulated
    discounted reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of reinforcement learning (RL)](img/B08956_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1.1: The perception-action-learning loop in reinforcement learning'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, the RL problem can be described as a **Markov Decision Process**
    (**MDP**). For simplicity, we''ll assume a *deterministic* environment where a
    certain action in a given state will consistently result in a known next state
    and reward. In a later section of this chapter, we''ll look at how to consider
    stochasticity. At timestep *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: The environment is in a state *s*[t] from the state space ![Principles of reinforcement
    learning (RL)](img/B08956_09_001.jpg) which may be discrete or continuous. The
    starting state is *s*[0] while the terminal state is *s*[t].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent takes action *a*[t] from the action space ![Principles of reinforcement
    learning (RL)](img/B08956_09_002.jpg) by obeying the policy, ![Principles of reinforcement
    learning (RL)](img/B08956_09_003.jpg). ![Principles of reinforcement learning
    (RL)](img/B08956_09_004.jpg) may be discrete or continuous.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment transitions to a new state *s*[t+1] using the state transition
    dynamics ![Principles of reinforcement learning (RL)](img/B08956_09_005.jpg).
    The next state is only dependent on the current state and action. ![Principles
    of reinforcement learning (RL)](img/B08956_09_006.jpg) is not known to the agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent receives a scalar reward using a reward function, r*[t+1]* = *R*(*s*[t],*a*[t])
    with ![Principles of reinforcement learning (RL)](img/B08956_09_007.jpg). The
    reward is only dependent on the current state and action. *R* is not known to
    the agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future rewards are discounted by ![Principles of reinforcement learning (RL)](img/B08956_09_008.jpg)
    where ![Principles of reinforcement learning (RL)](img/B08956_09_009.jpg) and
    *k* is the future timestep.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Horizon*, *H*, is the number of timesteps, *T*, needed to complete one episode
    from *s*[0] to *s*[t].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment may be fully or partially observable. The latter is also known
    as a **partially observable MDP** or **POMDP**. Most of the time, it's unrealistic
    to fully observe the environment. To improve the observability, past observations
    are also taken into consideration with the current observation. The state comprises
    the sufficient observations about the environment for the policy to decide on
    which action to take. In *Figure 9.1.1*, this could be the 3D position of the
    soda can with respect to the robot gripper as estimated by the robot camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every time the environment transitions to a new state, the agent receives a
    scalar reward, r*[t+1]*. In *Figure 9.1.1*, the reward could be +1 whenever the
    robot gets closer to the soda can, -1 whenever it gets farther, and +100 when
    it closes the gripper and successfully picks up the soda can. The goal of the
    agent is to learn an optimal policy ![Principles of reinforcement learning (RL)](img/B08956_09_010.jpg)
    that maximizes the *return* from all states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of reinforcement learning (RL)](img/B08956_09_011.jpg) (Equation
    9.1.1)'
  prefs: []
  type: TYPE_IMG
- en: The return is defined as the discounted cumulative reward, ![Principles of reinforcement
    learning (RL)](img/B08956_09_012.jpg). It can be observed from *Equation 9.1.1*
    that future rewards have lower weights when compared to the immediate rewards
    since generally ![Principles of reinforcement learning (RL)](img/B08956_09_013.jpg)
    where ![Principles of reinforcement learning (RL)](img/B08956_09_014.jpg). At the
    extremes, when ![Principles of reinforcement learning (RL)](img/B08956_09_015.jpg),
    only the immediate reward matters. When ![Principles of reinforcement learning
    (RL)](img/B08956_09_016.jpg) future rewards have the same weight as the immediate
    reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return can be interpreted as a measure of the *value* of a given state by following
    an arbitrary policy, ![Principles of reinforcement learning (RL)](img/B08956_09_017.jpg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of reinforcement learning (RL)](img/B08956_09_018.jpg) (Equation
    9.1.2)'
  prefs: []
  type: TYPE_IMG
- en: 'To put the RL problem in another way, the goal of the agent is to learn the
    optimal policy that maximizes ![Principles of reinforcement learning (RL)](img/B08956_09_019.jpg)
    for all states *s*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of reinforcement learning (RL)](img/B08956_09_020.jpg) (Equation
    9.1.3)'
  prefs: []
  type: TYPE_IMG
- en: The value function of the optimal policy is simply *V**. In *Figure 9.1.1*,
    the optimal policy is the one that generates the shortest sequence of actions
    that brings the robot closer and closer to the soda can until it has been fetched.
    The closer the state is to the goal state, the higher its value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sequence of events leading to the goal (or terminal state) can be modeled
    as the *trajectory* or *rollout* of the policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Trajectory* = (s0a0r1s1,s1a1r2s2,...,s*T*-1a*T*-1*r* *T* *s*[t]) (Equation
    9.1.4)'
  prefs: []
  type: TYPE_NORMAL
- en: If the MDP is *episodic* when the agent reaches the terminal state, *s*[T'],
    the state is reset to *s*[0]. If *T* is finite, we have a finite *horizon*. Otherwise,
    the horizon is infinite. In *Figure 9.1.1*, if the MDP is episodic, after collecting
    the soda can, the robot may look for another soda can to pick up and the RL problem
    repeats.
  prefs: []
  type: TYPE_NORMAL
- en: The Q value
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An important question is that if the RL problem is to find ![The Q value](img/B08956_09_022.jpg),
    how does the agent learn by interacting with the environment? *Equation * *9.1.3*
    does not explicitly indicate the action to try and the succeeding state to compute
    the return. In RL, we find that it''s easier to learn ![The Q value](img/B08956_09_023.jpg)
    by using the *Q* value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Q value](img/B08956_09_024.jpg) (Equation 9.2.1)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Q value](img/B08956_09_025.jpg) (Equation 9.2.2)'
  prefs: []
  type: TYPE_IMG
- en: In other words, instead of finding the policy that maximizes the value for all
    states, *Equation 9.2.1* looks for the action that maximizes the quality (*Q*)
    value for all states. After finding the *Q* value function, *V** and hence ![The
    Q value](img/B08956_09_026.jpg) are determined by *Equation 9.2.2* and *9.1.3*
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'If for every action, the reward and the next state can be observed, we can
    formulate the following iterative or trial and error algorithm to learn the *Q*
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Q value](img/B08956_09_027.jpg) (Equation 9.2.3)'
  prefs: []
  type: TYPE_IMG
- en: For notational simplicity, both *s* *'* and *a* *'* are the next state and action
    respectively. *Equation 9.2.3* is known as the **Bellman Equation** which is the
    core of the Q-Learning algorithm. Q-Learning attempts to approximate the first-order
    expansion of return or value (*Equation 9.1.2*) as a function of both current
    state and action.
  prefs: []
  type: TYPE_NORMAL
- en: From zero knowledge of the dynamics of the environment, the agent tries an action
    *a*, observes what happens in the form of reward, *r*, and next state, *s* *'*.
    ![The Q value](img/B08956_09_028.jpg) chooses the next logical action that will
    give the maximum *Q* value for the next state. With all terms in *Equation* *9.2.3*
    known, the *Q* value for that current state-action pair is updated. Doing the
    update iteratively will eventually learn the *Q* value function.
  prefs: []
  type: TYPE_NORMAL
- en: Q-Learning is an *off-policy* RL algorithm. It learns to improve the policy
    by not directly sampling experiences from that policy. In other words, the *Q*
    values are learned independently of the underlying policy being used by the agent.
    When the *Q* value function has converged, only then is the optimal policy determined
    using *Equation* *9.2.1*.
  prefs: []
  type: TYPE_NORMAL
- en: Before giving an example on how to use Q-Learning, we should note that the agent
    must continually explore its environment while gradually taking advantage of what
    it has learned so far. This is one of the issues in RL – finding the right balance
    between *Exploration* and *Exploitation*. Generally, during the start of learning,
    the action is random (exploration). As the learning progresses, the agent takes
    advantage of the *Q* value (exploitation). For example, at the start, 90% of the
    action is random and 10% from *Q* value function, and by the end of each episode,
    this is gradually decreased. Eventually, the action is 10% random and 90% from
    *Q* value function.
  prefs: []
  type: TYPE_NORMAL
- en: Q-Learning example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To illustrate the Q-Learning algorithm, we need to consider a simple deterministic
    environment, as shown in the following figure. The environment has six states.
    The rewards for allowed transitions are shown. The reward is non-zero in two cases.
    Transition to the **Goal** (**G**) state has +100 reward while moving into **Hole**
    (**H**) state has -100 reward. These two states are terminal states and constitute
    the end of one episode from the **Start** state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning example](img/B08956_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3.1: Rewards in a simple deterministic world'
  prefs: []
  type: TYPE_NORMAL
- en: 'To formalize the identity of each state, we need to use a (*row*, *column*)
    identifier as shown in the following figure. Since the agent has not learned anything
    yet about its environment, the Q-Table also shown in the following figure has
    zero initial values. In this example, the discount factor, ![Q-Learning example](img/B08956_09_029.jpg).
    Recall that in the estimate of current *Q* value, the discount factor determines
    the weight of future *Q* values as a function of the number of steps, ![Q-Learning
    example](img/B08956_09_030.jpg). In *Equation* *9.2.3*, we only consider the immediate
    future *Q* value, *k* = 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning example](img/B08956_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3.2: States in the simple deterministic environment and the agent''s
    initial Q-Table'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, the agent assumes a policy that selects a random action 90% of the
    time and exploits the Q-Table 10% of the time. Suppose the first action is randomly
    chosen and indicates a move in the right direction. *Figure 9.3.3* illustrates
    the computation of the new *Q* value of state (0, 0) for a move to the right action.
    The next state is (0, 1). The reward is 0, and the maximum of all the next state's
    *Q* values is zero. Therefore, the *Q* value of state (0, 0) for a move to the
    right action remains 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'To easily track the initial state and next state, we use different shades of
    gray on both the environment and the Q-Table–lighter gray for initial state and
    darker gray for the next state. In choosing the next action for the next state,
    the candidate actions are in the thicker border:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning example](img/B08956_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3.3: Assuming the action taken by the agent is a move to the right,
    the update on Q value of state (0, 0) is shown'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning example](img/B08956_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3.4: Assuming the action chosen by the agent is move down, the update
    on Q value of state (0, 1) is shown'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning example](img/B08956_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3.5: Assuming the action chosen by the agent is a move to the right,
    the update on Q value of state (1, 1) is shown'
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose that the next randomly chosen action is move down. *Figure 9.3.4*
    shows no change in the *Q* value of state (0, 1) for the move down action. In
    *Figure 9.3.5*, the agent's third random action is a move to the right. It encountered
    the **H** and received a -100 reward. This time, the update is non-zero. The new
    *Q* value for the state (1, 1) is -100 for the move to the right direction. One
    episode has just finished, and the agent returns to the **Start** state.
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning example](img/B08956_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3.6: Assuming the actions chosen by the agent are two successive moves
    to the right, the update on Q value of state (0, 1) is shown'
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose the agent is still in the exploration mode as shown in *Figure
    9.3.6*. The first step it took for the second episode is a move to the right.
    As expected, the update is 0\. However, the second random action it chose is also
    move to the right. The agent reached the **G** state and received a big +100 reward.
    The *Q* value for the state (0, 1) move to the right becomes 100\. The second
    episode is done, and the agent goes back to the **Start** state.
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning example](img/B08956_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3.7: Assuming the action chosen by the agent is a move to the right,
    the update on Q value of state (0, 0) is shown'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning example](img/B08956_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3.8: In this instance, the agent''s policy decided to exploit the
    Q-Table to determine the action at states (0, 0) and (0, 1). The Q-Table suggests
    to move to the right for both states.'
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of the third episode, the random action taken by the agent
    is a move to the right. The *Q* value of state (0, 0) is now updated with a non-zero
    value because the next state's possible actions have 100 as the maximum *Q* value.
    *Figure 9.3.7* shows the computation involved. The *Q* value of the next state
    (0, 1) ripples back to the earlier state (0, 0). It is like giving credit to the
    earlier states that helped in finding the **G** state.
  prefs: []
  type: TYPE_NORMAL
- en: The progress in Q-Table has been substantial. In fact, in the next episode,
    if for some reason the policy decided to exploit the Q-Table instead of randomly
    exploring the environment, the first action is to move to the right according
    to the computation in *Figure 9.3.8*. In the first row of the Q-Table, the action
    that results in maximum *Q* value is a move to the right. For the next state (0,
    1), the second row of Q-Table suggests that the next action is still to move to
    the right. The agent has successfully reached the goal. The policy guided the
    agent on the right set of actions to achieve its goal.
  prefs: []
  type: TYPE_NORMAL
- en: If the Q-Learning algorithm continues to run indefinitely, the Q-Table will
    converge. The assumptions for convergence are the RL problem must be deterministic
    MDP with bounded rewards and all states are visited infinitely often.
  prefs: []
  type: TYPE_NORMAL
- en: Q-Learning in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The environment and the Q-Learning discussed in the previous section can be
    implemented in Python. Since the policy is just a simple table, there is, at this
    point in time no need for Keras. *Listing* *9.3.1* shows `q-learning-9.3.1.py`,
    the implementation of the simple deterministic world (environment, agent, action,
    and Q-Table algorithms) using the `QWorld` class. For conciseness, the functions
    dealing with the user interface are not shown.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the environment dynamics is represented by `self.transition_table`.
    At every action, `self.transition_table` determines the next state. The reward
    for executing an action is stored in `self.reward_table`. The two tables are consulted
    every time an action is executed by the `step()` function. The Q-Learning algorithm
    is implemented by `update_q_table()` function. Every time the agent needs to decide
    which action to take, it calls the `act()` function. The action may be randomly
    drawn or decided by the policy using the Q-Table. The percent chance that the
    action chosen is random is stored in the `self.epsilon` variable which is updated
    by `update_epsilon()` function using a fixed `epsilon_decay`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before executing the code in *Listing* *9.3.1*, we need to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To install `termcolor` package. This package helps in visualizing text outputs
    on the Terminal.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The complete code can be found on GitHub at: [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 9.3.1, `q-learning-9.3.1.py`. A simple deterministic MDP with six states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9.3.2, `q-learning-9.3.1.py`. The main Q-Learning loop. The agent''s
    Q-Table is updated every state, action, reward, and next state iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The perception-action-learning loop is illustrated in *Listing* *9.3.2*. At
    every episode, the environment resets to the *Start* state. The action to execute
    is chosen and applied to the environment. The reward and next state are observed
    and used to update the Q-Table. The episode is completed (`done = True`) upon
    reaching the *Goal* or *Hole* state. For this example, the Q-Learning runs for
    100 episodes or 10 wins, whichever comes first. Due to the decrease in the value
    of the `self.epsilon` variable at every episode, the agent starts to favor exploitation
    of Q-Table to determine the action to perform given a state. To see the Q-Learning
    simulation we simply need to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Q-Learning in Python](img/B08956_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3.9: A screenshot showing the Q-Table after 2000 wins of the agent'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure shows the screenshot if `maxwins = 2000` (2000*x* *Goal*
    state is reached) and `delay = 0` (to see the final Q-Table only) by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The Q-Table has converged and shows the logical action that the agent can take
    given a state. For example, in the first row or state (0, 0), the policy advises
    move to the right. The same for the state (0, 1) on the second row. The second
    action reaches the *Goal* state. The `scores` variable dump shows that the minimum
    number of steps taken decreases as the agent gets correct actions from the policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'From *Figure 9.3.9*, we can compute the value of each state from *Equation
    9.2.2*, ![Q-Learning in Python](img/B08956_09_031.jpg). For example, for state
    (0, 0), *V**(*s*) = max(81.0,72.9,90.0,81.0) = 90.0\. Following figure shows the
    value for each state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning in Python](img/B08956_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3.10: The value for each state from Figure 9.3.9 and Equation9.2.2'
  prefs: []
  type: TYPE_NORMAL
- en: Nondeterministic environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the event that the environment is nondeterministic, both the reward and
    action are probabilistic. The new system is a stochastic MDP. To reflect the nondeterministic
    reward the new value function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Nondeterministic environment](img/B08956_09_032.jpg) (Equation 9.4.1)'
  prefs: []
  type: TYPE_IMG
- en: 'The Bellman equation is modified as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Nondeterministic environment](img/B08956_09_033.jpg) (Equation 9.4.2)'
  prefs: []
  type: TYPE_IMG
- en: Temporal-difference learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Q-Learning is a special case of a more generalized **Temporal-Difference Learning**
    or **TD-Learning** ![Temporal-difference learning](img/B08956_09_034.jpg). More
    specifically, it''s a special case of one-step TD-Learning *TD*(0):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Temporal-difference learning](img/B08956_09_035.jpg) (Equation 9.5.1)'
  prefs: []
  type: TYPE_IMG
- en: In the equation ![Temporal-difference learning](img/B08956_09_036.jpg) is the
    learning rate. We should note that when ![Temporal-difference learning](img/B08956_09_037.jpg),
    *Equation* *9.5.1* is similar to the Bellman equation. For simplicity, we'll refer
    to *Equation* *9.5.1* as Q-Learning or generalized Q-Learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we referred to Q-Learning as an off-policy RL algorithm since it
    learns the Q value function without directly using the policy that it is trying
    to optimize. An example of an *on-policy* one-step TD-learning algorithm is SARSA
    which similar to *Equation* *9.5.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Temporal-difference learning](img/B08956_09_038.jpg) (Equation 9.5.2)'
  prefs: []
  type: TYPE_IMG
- en: The main difference is the use of the policy that is being optimized to determine
    *a**'*. The terms *s*, *a*, *r*, *s**'* and *a**'* (thus the name SARSA) must
    be known to update the *Q* value function at every iteration. Both Q-Learning
    and SARSA use existing estimates in the *Q* value iteration, a process known as
    **bootstrapping**. In bootstrapping, we update the current *Q* value estimate
    from the reward and the subsequent *Q* value estimate(s).
  prefs: []
  type: TYPE_NORMAL
- en: Q-Learning on OpenAI gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before presenting another example, there appears to be a need for a suitable
    RL simulation environment. Otherwise, we can only run RL simulations on very simple
    problems like in the previous example. Fortunately, OpenAI created **Gym**, [https://gym.openai.com](https://gym.openai.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'The gym is a toolkit for developing and comparing RL algorithms. It works with
    most deep learning libraries, including Keras. The gym can be installed by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The gym has several environments where an RL algorithm can be tested against
    such as toy text, classic control, algorithmic, Atari, and 2D/3D robots. For example,
    `FrozenLake-v0` (*Figure 9.5.1*) is a toy text environment similar to the simple
    deterministic world used in the Q-Learning in Python example. `FrozenLake-v0`
    has 12 states. The state marked **S** is the starting state, **F** is the frozen
    part of the lake which is safe, **H** is the Hole state that should be avoided,
    and **G** is the Goal state where the frisbee is. The reward is +1 for transitioning
    to the Goal state. For all other states, the reward is zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `FrozenLake-v0`, there are also four available actions (Left, Down, Right,
    Up) known as action space. However, unlike the simple deterministic world earlier,
    the actual movement direction is only partially dependent on the chosen action.
    There are two variations of the `FrozenLake-v0` environment, slippery and non-slippery.
    As expected, the slippery mode is more challenging:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning on OpenAI gym](img/B08956_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5.1: Frozen lake environment in OpenAI Gym'
  prefs: []
  type: TYPE_NORMAL
- en: An action applied on `FrozenLake-v0` returns the observation (equivalent to
    the next state), reward, done (whether the episode is finished), and a dictionary
    of debugging information. The observable attributes of the environment, known
    as observation space, are captured by the returned observation object.
  prefs: []
  type: TYPE_NORMAL
- en: The generalized Q-Learning can be applied to the `FrozenLake-v0` environment.
    *Table 9.5.1* shows the improvement in performance of both slippery and non-slippery environments.
    A method of measuring the performance of the policy is the percent of episodes
    executed that resulted in reaching the Goal state. The higher is the percentage,
    the better. From the baseline of pure exploration (random action) of about 1.5%,
    the policy can achieve ~76% Goal state for non-slippery and ~71% for the slippery
    environment. As expected, it is harder to control the slippery environment.
  prefs: []
  type: TYPE_NORMAL
- en: The code can still be implemented in Python and NumPy since it only requires
    a Q-Table. *Listing* *9.5.1* shows the implementation of the `QAgent` class while
    listing *9.5.2* demonstrates the agent's perception-action-learning loop. Apart
    from using `FrozenLake-v0` environment from OpenAI Gym, the most important change
    is the implementation of the generalized Q-Learning as defined by *Equation* *9.5.1*
    in the `update_q_table()` function.
  prefs: []
  type: TYPE_NORMAL
- en: The `qagent` object can operate in either slippery or non-slippery mode. The
    agent is trained for 40,000 iterations. After training, the agent can exploit
    the Q-Table to choose the action to execute given any policy as shown in the test
    mode of *Table 9.5.1*. There is a huge performance boost in using the learned
    policy as demonstrated in *Table 9.5.1*. With the use of the gym, a lot of the
    code in constructing the environment is gone.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will help us to focus on building a working RL algorithm. To run the code
    in slow motion or delay of 1 sec per action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '| Mode | Run | Approx % Goal |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Train non-slippery |'
  prefs: []
  type: TYPE_TB
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '| 26.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Test non-slippery |'
  prefs: []
  type: TYPE_TB
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '| 76.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Pure random action non-slippery |'
  prefs: []
  type: TYPE_TB
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '| 1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Train slippery |'
  prefs: []
  type: TYPE_TB
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '| 26 |'
  prefs: []
  type: TYPE_TB
- en: '| Test slippery |'
  prefs: []
  type: TYPE_TB
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '| 71.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Pure random slippery |'
  prefs: []
  type: TYPE_TB
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '| 1.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9.5.1: Baseline and performance of generalized Q-Learning on the FrozenLake-v0
    environment with learning rate = 0.5'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Listing 9.5.1, `q-frozenlake-9.5.1.py` shows the implementation of Q-Learning
    on `FrozenLake-v0` environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9.5.2, `q-frozenlake-9.5.1.py`. The main Q-Learning loop for the `FrozenLake-v0`
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Deep Q-Network (DQN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the Q-Table to implement Q-Learning is fine in small discrete environments.
    However, when the environment has numerous states or continuous as in most cases,
    a Q-Table is not feasible or practical. For example, if we are observing a state
    made of four continuous variables, the size of the table is infinite. Even if
    we attempt to discretize the four variables into 1000 values each, the total number
    of rows in the table is a staggering 1000⁴ = 1*e*^(12). Even after training, the
    table is sparse - most of the cells in this table are zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'A solution to this problem is called DQN [2] which uses a deep neural network
    to approximate the Q-Table. As shown in *Figure 9.6.1*. There are two approaches
    to build the Q-network:'
  prefs: []
  type: TYPE_NORMAL
- en: The input is the state-action pair, and the prediction is the *Q* value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input is the state, and the prediction is the *Q* value for each action
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first option is not optimal since the network will be called a number of
    times equal to the number of actions. The second is the preferred method. The
    Q-Network is called only once.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most desirable action is simply the action with the biggest *Q* value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Q-Network (DQN)](img/B08956_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6.1: A Deep Q-Network'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data required to train the Q-Network come from the agent''s experiences:
    ![Deep Q-Network (DQN)](img/B08956_09_039.jpg). Each training sample is a unit
    of experience ![Deep Q-Network (DQN)](img/B08956_09_040.jpg). At a given state
    at timestep *t*, *s* = *s*[t], the action, *a* = a[t], is determined using the
    Q-Learning algorithm similar to the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Q-Network (DQN)](img/B08956_09_041.jpg) (Equation 9.6.1)'
  prefs: []
  type: TYPE_IMG
- en: 'For notational simplicity, we omit the subscript and the use of the bold letter.
    We need to note that *Q*(*s*,*a*) is the Q-Network. Strictly speaking, it is *Q*(*a*|*s*)
    since the action is moved to the prediction as shown on the right of *Figure 9.6.1*.
    The action with the highest *Q* value is the action that is applied on the environment
    to get the reward, *r* = *r* [t+1], the next state, *s* *''* = *s*[t+1] and a
    Boolean `done` indicating if the next state is terminal. From *Equation* *9.5.1*
    on generalized Q-Learning, an MSE loss function can be determined by applying
    the chosen action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Q-Network (DQN)](img/B08956_09_042.jpg) (Equation 9.6.2)'
  prefs: []
  type: TYPE_IMG
- en: Where all terms are familiar from the previous discussion on Q-Learning and
    *Q*(*a*|*s*) → *Q*(*s*,*a*). The term ![Deep Q-Network (DQN)](img/B08956_09_043.jpg).
    In other words, using the Q-Network, predict the *Q* value of each action given
    next state and get the maximum among them. Note that at the terminal state *s'*,
    ![Deep Q-Network (DQN)](img/B08956_09_044.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm 9.6.1, DQN algorithm:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: Initialize replay memory *D* to capacity *N*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: Initialize action-value function *Q* with random weights ![Deep
    Q-Network (DQN)](img/B08956_09_045.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: Initialize target action-value function *Q*[*target*] with weights
    ![Deep Q-Network (DQN)](img/B08956_09_046.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: Exploration rate, ![Deep Q-Network (DQN)](img/B08956_09_047.jpg)
    and discount factor, ![Deep Q-Network (DQN)](img/B08956_09_048.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: '`for` *episode* = 1, …,*M* `do:`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given initial state *s*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`for` *step* = 1,…, *T* `do`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose action ![Deep Q-Network (DQN)](img/B08956_09_049.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute action *a*, observe reward *r* and next state *s'*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store transition (*s*, *a*, *r*, *s**'*) in *D*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the state, *s* = *s**'*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: //experience replay
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a mini batch of episode experiences (*s*[*j*], *a*[*j*], *r*[*j+1*],
    *s*[*j+1*]) from *D*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Deep Q-Network (DQN)](img/B08956_09_050.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: Perform gradient descent step on ![Deep Q-Network (DQN)](img/B08956_09_051.jpg)with
    respect to parameters ![Deep Q-Network (DQN)](img/B08956_09_052.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: // periodic update of the target network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Every *C* steps *Q*[*target*] = *Q*, that is set ![Deep Q-Network (DQN)](img/B08956_09_053.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: End
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'However, it turns out that training the Q-Network is unstable. There are two
    problems causing the instability:'
  prefs: []
  type: TYPE_NORMAL
- en: A high correlation between samples
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A non-stationary target
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A high correlation is due to the sequential nature of sampling experiences.
    DQN addressed this issue by creating a buffer of experiences. The training data
    are randomly sampled from this buffer. This process is known as **experience replay**.
  prefs: []
  type: TYPE_NORMAL
- en: The issue of the non-stationary target is due to the target network *Q*(*s*
    *'*,*a* *'*) that is modified after every mini batch of training. A small change
    in the target network can create a significant change in the policy, the data
    distribution, and the correlation between the current *Q* value and target *Q*
    value. This is resolved by freezing the weights of the target network for *C*
    training steps. In other words, two identical Q-Networks are created. The target
    Q-Network parameters are copied from the Q-Network under training every *C* training
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: The DQN algorithm is summarized in *Algorithm* *9.6.1*.
  prefs: []
  type: TYPE_NORMAL
- en: DQN on Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To illustrate DQN, the `CartPole-v0` environment of the OpenAI Gym is used.
    `CartPole-v0` is a pole balancing problem. The goal is to keep the pole from falling
    over. The environment is 2D. The action space is made of two discrete actions
    (left and right movements). However, the state space is continuous and is made
    of four variables:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear position
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear velocity
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Angle of rotation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Angular velocity
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `CartPole-v0` is shown in *Figure 9.6.1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, the pole is upright. A reward of +1 is provided for every timestep
    that the pole remains upright. The episode ends when the pole exceeds 15 degrees
    from the vertical or 2.4 units from the center. The `CartPole-v0` problem is considered
    solved if the average reward is 195.0 in 100 consecutive trials:'
  prefs: []
  type: TYPE_NORMAL
- en: '![DQN on Keras](img/B08956_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6.1: The CartPole-v0 environment'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing* *9.6.1* shows us the DQN implementation for `CartPole-v0`. The `DQNAgent`
    class represents the agent using DQN. Two Q-Networks are created:'
  prefs: []
  type: TYPE_NORMAL
- en: Q-Network or *Q* in *Algorithm* *9.6.1*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Target Q-Network or *Q*[target] in *Algorithm* *9.6.1*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both networks are MLP with three hidden layers of 256 units each. The Q-Network
    is trained during experience replay, `replay()`. At a regular interval of *C*
    = 10 training steps, the Q-Network parameters are copied to the Target Q-Network
    by `update_weights()`. This implements line *13*, *Q*[target] = *Q*, in algorithm
    *9.6.1*. After every episode, the ratio of exploration-exploitation is decreased
    by `update_epsilon()` to take advantage of the learned policy.
  prefs: []
  type: TYPE_NORMAL
- en: To implement line *10* in *Algorithm* *9.6.1* during experience replay, `replay()`,
    for each experience unit, (*s*[j], *a*[j], *r*[j+1], *s*[j+1]), the *Q* value
    for the action *a*[j] is set to *Q*[*max*]. All other actions have their *Q* values
    unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is implemented by the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Only the action *a*[j] has a non-zero loss equal to ![DQN on Keras](img/B08956_09_054.jpg)
    as shown by line *11* of *Algorithm 9.6.1*. Note that the experience replay is
    called by the perception-action-learning loop in *Listing* *9.6.2* after the end
    of each episode assuming that there is sufficient data in the buffer (that is,
    buffer size, is greater or equal to batch size). During the experience replay,
    one batch of experience units is randomly sampled and used to train the Q-Network.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the Q-Table, `act()` implements the ![DQN on Keras](img/B08956_09_055.jpg)-greedy
    policy, *Equation* *9.6.1*. Experiences are stored by `remember()` in the replay
    buffer. The computation of *Q* is done by the `get_target_q_value()` function.
    On the average of 10 runs, `CartPole-v0` is solved by DQN within 822 episodes.
    We need to take note that the results may vary every time the training runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 9.6.1, `dqn-cartpole-9.6.1.py` shows us the DQN implementation within
    Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 9.6.2, `dqn-cartpole-9.6.1.py`. Training loop of DQN implementation
    in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Double Q-Learning (DDQN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In DQN, the target Q-Network selects and evaluates every action resulting in
    an overestimation of *Q* value. To resolve this issue, DDQN [3] proposes to use
    the Q-Network to choose the action and use the target Q-Network to evaluate the
    action.
  prefs: []
  type: TYPE_NORMAL
- en: 'In DQN as summarized by *Algorithm 9.6.1*, the estimate of the *Q* value in
    line *10* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Double Q-Learning (DDQN)](img/B08956_09_056.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Q*[target] chooses and evaluates the action *a* [j+1].'
  prefs: []
  type: TYPE_NORMAL
- en: 'DDQN proposes to change line *10* to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Double Q-Learning (DDQN)](img/B08956_09_057.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The term ![Double Q-Learning (DDQN)](img/B08956_09_058.jpg) lets *Q* to choose
    the action. Then this action is evaluated by *Q*[target].
  prefs: []
  type: TYPE_NORMAL
- en: 'In Listing 9.6.1, both DQN and DDQN are implemented. Specifically, for DDQN,
    the modification on the *Q* value computation performed by `get_target_q_value()`
    function is highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'For comparison, on the average of 10 runs, the `CartPole-v0` is solved by DDQN
    within 971 episodes. To use DDQN, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've been introduced to DRL. A powerful technique believed
    by many researchers as the most promising lead towards artificial intelligence.
    Together, we've gone over the principles of RL. RL is able to solve many toy problems,
    but the Q-Table is unable to scale to more complex real-world problems. The solution
    is to learn the Q-Table using a deep neural network. However, training deep neural
    networks on RL is highly unstable due to sample correlation and non-stationarity
    of the target Q-Network.
  prefs: []
  type: TYPE_NORMAL
- en: DQN proposed a solution to these problems using experience replay and separating
    the target network from the Q-Network under training. DDQN suggested further improvement
    of the algorithm by separating the action selection from action evaluation to
    minimize the overestimation of *Q* value. There are other improvements proposed
    for the DQN. Prioritized experience replay [6] argues that that experience buffer
    should not be sampled uniformly. Instead, experiences that are more important
    based on TD errors should be sampled more frequently to accomplish more efficient
    training. [7] proposes a dueling network architecture to estimate the state value
    function and the advantage function. Both functions are used to estimate the *Q*
    value for faster learning.
  prefs: []
  type: TYPE_NORMAL
- en: The approach presented in this chapter is value iteration/fitting. The policy
    is learned indirectly by finding an optimal value function. In the next chapter,
    the approach will be to learn the optimal policy directly by using a family of
    algorithms called policy gradient methods. Learning the policy has many advantages.
    In particular, policy gradient methods can deal with both discrete and continuous
    action spaces.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sutton and Barto. *Reinforcement Learning: An Introduction*, 2017 ([http://incompleteideas.net/book/bookdraft2017nov5.pdf](http://incompleteideas.net/book/bookdraft2017nov5.pdf)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Volodymyr Mnih and others, *Human-level control through deep reinforcement
    learning*. Nature 518.7540, 2015: 529 ([http://www.davidqiu.com:8888/research/nature14236.pdf](http://www.davidqiu.com:8888/research/nature14236.pdf))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hado Van Hasselt, Arthur Guez, and David Silver *Deep Reinforcement Learning
    with Double Q-Learning*. AAAI. Vol. 16, 2016 ([http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kai Arulkumaran and others *A Brief Survey of Deep Reinforcement Learning*.
    arXiv preprint arXiv:1708.05866, 2017 ([https://arxiv.org/pdf/1708.05866.pdf](https://arxiv.org/pdf/1708.05866.pdf)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: David Silver *Lecture Notes on Reinforcement Learning*, ([http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tom Schaul and others. *Prioritized experience replay*. arXiv preprint arXiv:1511.05952,
    2015 ([https://arxiv.org/pdf/1511.05952.pdf](https://arxiv.org/pdf/1511.05952.pdf)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ziyu Wang and others. *Dueling Network Architectures for Deep Reinforcement
    Learning*. arXiv preprint arXiv:1511.06581, 2015 ([https://arxiv.org/pdf/1511.06581.pdf](https://arxiv.org/pdf/1511.06581.pdf)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
