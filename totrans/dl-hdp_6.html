<html><head></head><body><div id="book-columns"><div id="book-inner"><div class="chapter" title="Chapter 6.  Autoencoders"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/><span class="koboSpan" id="kobo.1.1">Chapter 6.  Autoencoders </span></h1></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"><span class="koboSpan" id="kobo.2.1"> </span></td><td valign="top"><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.3.1">"People worry that computers will get too smart and take over the world, but the real problem is that they're too stupid and they've already taken over the world."</span></em></span>
</p></td><td valign="top"><span class="koboSpan" id="kobo.4.1"> </span></td></tr><tr><td valign="top"><span class="koboSpan" id="kobo.5.1"> </span></td><td colspan="2" align="right" valign="top" style="text-align: center"><span class="koboSpan" id="kobo.6.1">--</span><span class="attribution"><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">Pedro Domingos</span></em></span></span></td></tr></table></div><p><span class="koboSpan" id="kobo.8.1">In the last chapter, we discussed a generative model called Restricted Boltzmann machine. </span><span class="koboSpan" id="kobo.8.2">In this chapter, we will introduce one more generative model called </span><span class="strong"><strong><span class="koboSpan" id="kobo.9.1">autoencoder</span></strong></span><span class="koboSpan" id="kobo.10.1">. </span><span class="koboSpan" id="kobo.10.2">Autoencoder, a type of artificial neural network, is generally used for dimensionality reduction, feature learning, or extraction.</span></p><p><span class="koboSpan" id="kobo.11.1">As we move on with this chapter, we will discuss the concept of autoencoder and its various forms in detail. </span><span class="koboSpan" id="kobo.11.2">We will also explain the terms </span><span class="emphasis"><em><span class="koboSpan" id="kobo.12.1">regularized autoencoder</span></em></span><span class="koboSpan" id="kobo.13.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.14.1">sparse autoencoder</span></em></span><span class="koboSpan" id="kobo.15.1">. </span><span class="koboSpan" id="kobo.15.2">The concept of sparse coding, and selection criteria of the sparse factor in a sparse autoencoder will be taken up. </span><span class="koboSpan" id="kobo.15.3">Later, we will talk about the deep learning model, deep autoencoder, and its implementation using Deeplearning4j. </span><span class="koboSpan" id="kobo.15.4">Denoising autoencoder is one more form of a traditional autoencoder, which will be discussed in the end part of the chapter.</span></p><p><span class="koboSpan" id="kobo.16.1">Overall, this chapter is broken into a few subsections, which are listed as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.17.1">Autoencoder</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.18.1">Sparse autoencoder</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.19.1">Deep autoencoder</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.20.1">Denoising autoencoder</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.21.1">Applications of autoencoders</span></li></ul></div><div class="section" title="Autoencoder"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec40"/><span class="koboSpan" id="kobo.22.1">Autoencoder</span></h1></div></div></div><p><span class="koboSpan" id="kobo.23.1">An autoencoder is a neural network with one hidden layer, which is trained to learn an identity function that attempts to reconstruct its input to its output. </span><span class="koboSpan" id="kobo.23.2">In other words, the autoencoder tries to copy the input data by projecting onto a lower dimensional subspace defined by the hidden nodes. </span><span class="koboSpan" id="kobo.23.3">The hidden layer, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.24.1">h</span></em></span><span class="koboSpan" id="kobo.25.1">, describes a code, which is used to represent the input data and its structure. </span><span class="koboSpan" id="kobo.25.2">This hidden layer is thus forced to learn the structure from its input training dataset so that it can copy the input at the output layer.</span></p><p><span class="koboSpan" id="kobo.26.1">The network of an autoencoder can be split into two parts: encoder and decoder. </span><span class="koboSpan" id="kobo.26.2">The encoder is described by the function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.27.1">h=f (k)</span></em></span><span class="koboSpan" id="kobo.28.1">, and a decoder that tries to reconstruct or copy is defined by </span><span class="emphasis"><em><span class="koboSpan" id="kobo.29.1">r = g (h)</span></em></span><span class="koboSpan" id="kobo.30.1">. </span><span class="koboSpan" id="kobo.30.2">The basic idea of autoencoder should be to copy only those aspects of the inputs which are prioritized, and not to create an exact replica of the input. </span><span class="koboSpan" id="kobo.30.3">They are designed in such a way so as to restrict the hidden layer to copy only approximately, and not everything from the input data. </span><span class="koboSpan" id="kobo.30.4">Therefore, an autoencoder will not be termed as useful if it learns to completely set </span><span class="emphasis"><em><span class="koboSpan" id="kobo.31.1">g(f(k) = k</span></em></span><span class="koboSpan" id="kobo.32.1"> for all the values of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.33.1">k</span></em></span><span class="koboSpan" id="kobo.34.1">. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.35.1">Figure 6.1</span></em></span><span class="koboSpan" id="kobo.36.1"> represents the general structure of an autoencoder, mapping an input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.37.1">k</span></em></span><span class="koboSpan" id="kobo.38.1"> to an output </span><span class="emphasis"><em><span class="koboSpan" id="kobo.39.1">r</span></em></span><span class="koboSpan" id="kobo.40.1"> through an internal hidden layer of code </span><span class="emphasis"><em><span class="koboSpan" id="kobo.41.1">h</span></em></span><span class="koboSpan" id="kobo.42.1">:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.43.1"><img src="graphics/image_06_001.jpg" alt="Autoencoder"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.44.1">Figure 6.1: General block diagram of an autoencoder. </span><span class="koboSpan" id="kobo.44.2">Here, input k is mapped to an output r through a hidden state or internal representation h. </span><span class="koboSpan" id="kobo.44.3">An encoder f maps the input k to the hidden state h, and decoder g performs the mapping of h to the output r.</span></p><p><span class="koboSpan" id="kobo.45.1">To provide one more example, let us consider </span><span class="emphasis"><em><span class="koboSpan" id="kobo.46.1">Figure 6.2</span></em></span><span class="koboSpan" id="kobo.47.1">. </span><span class="koboSpan" id="kobo.47.2">The figure shows a practical representation of an autoencoder for input image patches </span><span class="emphasis"><em><span class="koboSpan" id="kobo.48.1">k</span></em></span><span class="koboSpan" id="kobo.49.1">, which learns the hidden layer </span><span class="emphasis"><em><span class="koboSpan" id="kobo.50.1">h</span></em></span><span class="koboSpan" id="kobo.51.1"> to output </span><span class="emphasis"><em><span class="koboSpan" id="kobo.52.1">r</span></em></span><span class="koboSpan" id="kobo.53.1">. </span><span class="koboSpan" id="kobo.53.2">The input layer </span><span class="emphasis"><em><span class="koboSpan" id="kobo.54.1">k</span></em></span><span class="koboSpan" id="kobo.55.1"> is a combination of intensity values from the image patches. </span><span class="koboSpan" id="kobo.55.2">The hidden layer nodes help to project the high-dimensional input layer into lower-dimensional activation values of the hidden nodes. </span><span class="koboSpan" id="kobo.55.3">These activation values of the hidden node are merged together to generate the output layer </span><span class="emphasis"><em><span class="koboSpan" id="kobo.56.1">r</span></em></span><span class="koboSpan" id="kobo.57.1">, which is an approximation of the input pixel. </span><span class="koboSpan" id="kobo.57.2">In ideal cases, hidden layers generally have a smaller number of nodes as compared to the input layer nodes. </span><span class="koboSpan" id="kobo.57.3">For this reason, they are forced to diminish the information in such a way that the output layer can still be generated.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.58.1"><img src="graphics/B05883_06_02-1.jpg" alt="Autoencoder"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.59.1">Figure 6.2: Figure shows a practical example of how an autoencoder learns output structure from the approximation of the input pixels.</span></p><p><span class="koboSpan" id="kobo.60.1">Replicating the structure of the input to the output might sound inefficacious, however, practically, the final result of an autoencoder is not exactly dependent on the output of the decoder. </span><span class="koboSpan" id="kobo.60.2">Instead, the main idea behind training an autoencoder is to copy the useful properties of the input task, which will reflect in the hidden layer.</span></p><p><span class="koboSpan" id="kobo.61.1">One of the common ways to extract desired features or information from the autoencoder is to limit the hidden layer, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.62.1">h</span></em></span><span class="koboSpan" id="kobo.63.1">, to have smaller dimension </span><span class="emphasis"><em><span class="koboSpan" id="kobo.64.1">(d</span><sup><span class="koboSpan" id="kobo.65.1">/</span></sup><span class="koboSpan" id="kobo.66.1">)</span></em></span><span class="koboSpan" id="kobo.67.1"> than the input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.68.1">k</span></em></span><span class="koboSpan" id="kobo.69.1"> with a dimension </span><span class="emphasis"><em><span class="koboSpan" id="kobo.70.1">d,</span></em></span><span class="koboSpan" id="kobo.71.1"> that is </span><span class="emphasis"><em><span class="koboSpan" id="kobo.72.1">d</span><sup><span class="koboSpan" id="kobo.73.1">/</span></sup><span class="koboSpan" id="kobo.74.1">&lt;d</span></em></span><span class="koboSpan" id="kobo.75.1">. </span><span class="koboSpan" id="kobo.75.2">This resulting smaller dimensional layer can thus be called a loss compressed representation of the input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.76.1">k</span></em></span><span class="koboSpan" id="kobo.77.1">. </span><span class="koboSpan" id="kobo.77.2">An autoencoder whose hidden layer's dimension is less than the input's dimension is termed as </span><span class="emphasis"><em><span class="koboSpan" id="kobo.78.1">undercomplete</span></em></span><span class="koboSpan" id="kobo.79.1">.</span></p><p><span class="koboSpan" id="kobo.80.1">The learning process described can be mathematically represented as minimizing the loss function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.81.1">L</span></em></span><span class="koboSpan" id="kobo.82.1">, which is given as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.83.1"><img src="graphics/image_06_003.jpg" alt="Autoencoder"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.84.1">In simple words, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.85.1">L</span></em></span><span class="koboSpan" id="kobo.86.1"> can be defined as a loss function that penalized </span><span class="emphasis"><em><span class="koboSpan" id="kobo.87.1">g (f (k))</span></em></span><span class="koboSpan" id="kobo.88.1"> for being different from the input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.89.1">k</span></em></span><span class="koboSpan" id="kobo.90.1">.</span></p><p><span class="koboSpan" id="kobo.91.1">With a linear decoder function, an autoencoder learns to form the basis for space as similar to the </span><span class="strong"><strong><span class="koboSpan" id="kobo.92.1">Principal component analysis</span></strong></span><span class="koboSpan" id="kobo.93.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.94.1">PCA</span></strong></span><span class="koboSpan" id="kobo.95.1">) procedure. </span><span class="koboSpan" id="kobo.95.2">Upon convergence, the hidden layer will form a basis for the space spanned by the principal subspace of the training dataset given as the input. </span><span class="koboSpan" id="kobo.95.3">However, unlike PCA, these procedures need not necessarily generate orthogonal vectors. </span><span class="koboSpan" id="kobo.95.4">For this reason, autoencoders with non-linear encoder functions </span><span class="emphasis"><em><span class="koboSpan" id="kobo.96.1">f</span></em></span><span class="koboSpan" id="kobo.97.1"> and non-linear decoder function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.98.1">g</span></em></span><span class="koboSpan" id="kobo.99.1"> can learn more powerful non-linear generalization of the PCA. </span><span class="koboSpan" id="kobo.99.2">This will eventually increase the capacity of the encoder and decoder to a large extent. </span><span class="koboSpan" id="kobo.99.3">With this increase in capacity, however, the autoencoder starts showing unwanted behavior.</span></p><p><span class="koboSpan" id="kobo.100.1">It can then learn to undergo copying the whole input without giving attention to extract the desired information. </span><span class="koboSpan" id="kobo.100.2">In a theoretical sense, an autoencoder might be a one-dimensional code, but practically, a very powerful nonlinear encoder can learn to represent each training example </span><span class="emphasis"><em><span class="koboSpan" id="kobo.101.1">k(i)</span></em></span><span class="koboSpan" id="kobo.102.1"> with code </span><span class="emphasis"><em><span class="koboSpan" id="kobo.103.1">i</span></em></span><span class="koboSpan" id="kobo.104.1">. </span><span class="koboSpan" id="kobo.104.2">The decoder then maps those integers </span><span class="emphasis"><em><span class="koboSpan" id="kobo.105.1">(i)</span></em></span><span class="koboSpan" id="kobo.106.1"> to the values of specific training examples. </span><span class="koboSpan" id="kobo.106.2">Hence, copying of only the useful features from the input dataset fails completely with an autoencoder with higher capacity.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note23"/><span class="koboSpan" id="kobo.107.1">Note</span></h3><p><span class="koboSpan" id="kobo.108.1">PCA is a statistical method which applies orthogonal transformation to convert a set of possibly correlated observed variables into a set of linearly correlated set of variables termed as principal components. </span><span class="koboSpan" id="kobo.108.2">The number of principal components in the PCA method is less than or equal to the number of original input variables.</span></p></div></div><p><span class="koboSpan" id="kobo.109.1">Similar to the edge case problem mentioned for an undercomplete autoencoder, where the dimension of the hidden layer is less than that of the input, autoencoder, where the hidden layer or code is allowed to have an equal dimension of input, often faces the same problem.</span></p><p><span class="koboSpan" id="kobo.110.1">An autoencoder, where the hidden code has greater dimension than the dimension of the input, is termed as an overcomplete autoencoder. </span><span class="koboSpan" id="kobo.110.2">This type of autoencoder is even more vulnerable to the aforementioned problems. </span><span class="koboSpan" id="kobo.110.3">Even a linear encoder and decoder can perform learning a copy of input to output without learning any desired attributes of the input dataset.</span></p><div class="section" title="Regularized autoencoders"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec55"/><span class="koboSpan" id="kobo.111.1">Regularized autoencoders</span></h2></div></div></div><p><span class="koboSpan" id="kobo.112.1">By choosing a proper dimension for the hidden layer, and the capacity of the encoder and decoder in accordance with the complexity of the model distribution, autoencoders of any kind of architecture can be built successfully. </span><span class="koboSpan" id="kobo.112.2">The autoencoder which has the ability to provide the same is termed as a regularized autoencoder.</span></p><p><span class="koboSpan" id="kobo.113.1">Besides the ability to copy the input to output, a regularized autoencoder has a loss function, which helps the model to possess other properties too. </span><span class="koboSpan" id="kobo.113.2">These include robustness to missing inputs, sparsity of the representation of data, smallness of the derivative of the representation, and so on. </span><span class="koboSpan" id="kobo.113.3">Even a nonlinear and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.114.1">overcomplete</span></em></span><span class="koboSpan" id="kobo.115.1"> regularized autoencoder is able to learn at least something about the data distribution, irrespective of the capacity of the model. </span><span class="koboSpan" id="kobo.115.2">Regularized autoencoders [131] are able to capture the structure of the training distribution with the help of productive opposition between a restructuring error and a regularizer.</span></p></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Sparse autoencoders"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec41"/><span class="koboSpan" id="kobo.1.1">Sparse autoencoders</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Distributed sparse representation is one of the primary keys to learn useful features in deep learning algorithms. </span><span class="koboSpan" id="kobo.2.2">Not only is it a coherent mode of data representation, but it also helps to capture the generation process of most of the real world dataset. </span><span class="koboSpan" id="kobo.2.3">In this section, we will explain how autoencoders encourage sparsity of data. </span><span class="koboSpan" id="kobo.2.4">We will start with introducing sparse coding. </span><span class="koboSpan" id="kobo.2.5">A code is termed as sparse when an input provokes the activation of a relatively small number of nodes of a neural network, which combine to represent it in a sparse way. </span><span class="koboSpan" id="kobo.2.6">In deep learning technology, a similar constraint is used to generate the sparse code models to implement regular autoencoders, which are trained with sparsity constants called sparse autoencoders.</span></p><div class="section" title="Sparse coding"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec56"/><span class="koboSpan" id="kobo.3.1">Sparse coding</span></h2></div></div></div><p><span class="koboSpan" id="kobo.4.1">Sparse coding is a type of unsupervised method to learn sets of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.5.1">overcomplete</span></em></span><span class="koboSpan" id="kobo.6.1"> bases in order to represent the data in a coherent and efficient way. </span><span class="koboSpan" id="kobo.6.2">The primary goal of sparse coding is to determine a set of vectors </span><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">(n) v</span><sub><span class="koboSpan" id="kobo.8.1">i</span></sub><span class="koboSpan" id="kobo.9.1"> </span></em></span><span class="koboSpan" id="kobo.10.1">such that the input vector </span><span class="emphasis"><em><span class="koboSpan" id="kobo.11.1">k</span></em></span><span class="koboSpan" id="kobo.12.1"> can be represented as a linear combination of these vectors.</span></p><p><span class="koboSpan" id="kobo.13.1">Mathematically, this can be represented as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.14.1"><img src="graphics/image_06_004.jpg" alt="Sparse coding"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.15.1">Here </span><span class="emphasis"><em><span class="koboSpan" id="kobo.16.1">a</span><sub><span class="koboSpan" id="kobo.17.1">i</span></sub></em></span><span class="koboSpan" id="kobo.18.1"> is the coefficient associated with each vector </span><span class="emphasis"><em><span class="koboSpan" id="kobo.19.1">v</span><sub><span class="koboSpan" id="kobo.20.1">i</span></sub></em></span>
<sub><span class="koboSpan" id="kobo.21.1">. </span></sub></p><p><span class="koboSpan" id="kobo.22.1">With the help of PCA, we can learn a complete set of basis vectors in a coherent way; however, we want to learn an </span><span class="emphasis"><em><span class="koboSpan" id="kobo.23.1">overcomplete</span></em></span><span class="koboSpan" id="kobo.24.1"> set of basis vectors to represent the input vector </span><span class="emphasis"><em><span class="koboSpan" id="kobo.25.1">k</span></em></span>
<span class="inlinemediaobject"><span class="koboSpan" id="kobo.26.1"><img src="graphics/equation.jpg" alt="Sparse coding"/></span></span><span class="koboSpan" id="kobo.27.1">
 where </span><span class="emphasis"><em><span class="koboSpan" id="kobo.28.1">n&gt;m</span></em></span><span class="koboSpan" id="kobo.29.1">. </span><span class="koboSpan" id="kobo.29.2">The reason to have the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.30.1">overcomplete</span></em></span><span class="koboSpan" id="kobo.31.1"> basis is that the basis vectors are generally able to catch the pattern and structure that are inherent to the input data. </span><span class="koboSpan" id="kobo.31.2">However, overcompleteness sometime raises a degeneracy that, with its basis, the coefficient </span><span class="emphasis"><em><span class="koboSpan" id="kobo.32.1">a</span><sub><span class="koboSpan" id="kobo.33.1">i </span></sub></em></span><span class="koboSpan" id="kobo.34.1">cannot uniquely identify the input vector </span><span class="emphasis"><em><span class="koboSpan" id="kobo.35.1">k</span></em></span><span class="koboSpan" id="kobo.36.1">. </span><span class="koboSpan" id="kobo.36.2">For this reason, an additional criterion called sparsity is introduced in sparse coding.</span></p><p><span class="koboSpan" id="kobo.37.1">In a simple way, sparsity can be defined as having few non-zero components or having few components that are not close to zero. </span><span class="koboSpan" id="kobo.37.2">The set of coefficients </span><span class="emphasis"><em><span class="koboSpan" id="kobo.38.1">a</span><sub><span class="koboSpan" id="kobo.39.1">i</span></sub></em></span><span class="koboSpan" id="kobo.40.1"> is termed as sparse if, for a given input vector, the number of non-zero coefficients, or the number of coefficients that are way far from zero, should be a few.</span></p><p><span class="koboSpan" id="kobo.41.1">With this basic understanding of sparse coding, we can now move to the next part to discover how the sparse coding concept is used for autoencoders to generate sparse autoencoders.</span></p></div><div class="section" title="Sparse autoencoders"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec57"/><span class="koboSpan" id="kobo.42.1">Sparse autoencoders</span></h2></div></div></div><p><span class="koboSpan" id="kobo.43.1">When the input dataset maintains some structure, and if the input features are correlated, then even a simple autoencoder algorithm can discover those correlations. </span><span class="koboSpan" id="kobo.43.2">Moreover, in such cases, a simple autoencoder will end up learning a low-dimensional representation, which is similar to PCA.</span></p><p><span class="koboSpan" id="kobo.44.1">This perception is based on the fact that the number of hidden layers is relatively small. </span><span class="koboSpan" id="kobo.44.2">However, by imposing other constraints on the network, even with a large number of hidden layers, the network can still discover desired features from the input vectors.</span></p><p><span class="koboSpan" id="kobo.45.1">Sparse autoencoders are generally used to learn features to perform other tasks such as classification. </span><span class="koboSpan" id="kobo.45.2">Autoencoders for which the sparsity constraints have been added must respond to the unique statistical features of the input dataset with which it is training on, rather than simply acting as an identity function.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.46.1"><img src="graphics/B05883_06_03.jpg" alt="Sparse autoencoders"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.47.1">Figure 6.3: Figure shows a typical example of a sparse autoencoder</span></p><p><span class="koboSpan" id="kobo.48.1">Sparse autoencoders are a type of autoencoder with a sparsity enforcer, which helps to direct a single layer network to learn the hidden layer code. </span><span class="koboSpan" id="kobo.48.2">This approach minimizes the reconstruction errors along with restricting the number of code words needed to restructure the output. </span><span class="koboSpan" id="kobo.48.3">This kind of sparsifying algorithm can be considered as a classification problem that restricts the input to a single class value, which helps to reduce the prediction errors.</span></p><p><span class="koboSpan" id="kobo.49.1">In this part, we will explain sparse autoencoder with a simple architecture. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.50.1">Figure 6.3</span></em></span><span class="koboSpan" id="kobo.51.1"> shows the simplest form of a sparse autoencoder, consisting of a single hidden layer </span><span class="emphasis"><em><span class="koboSpan" id="kobo.52.1">h</span></em></span><span class="koboSpan" id="kobo.53.1">. </span><span class="koboSpan" id="kobo.53.2">The hidden layer </span><span class="emphasis"><em><span class="koboSpan" id="kobo.54.1">h</span></em></span><span class="koboSpan" id="kobo.55.1">, is connected to the input vector </span><span class="emphasis"><em><span class="koboSpan" id="kobo.56.1">K</span></em></span><span class="koboSpan" id="kobo.57.1"> by a weight matrix, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.58.1">W</span></em></span><span class="koboSpan" id="kobo.59.1">, which forms the encoding step. </span><span class="koboSpan" id="kobo.59.2">In the decoder step, the hidden layer h outputs to a reconstruction vector </span><span class="emphasis"><em><span class="koboSpan" id="kobo.60.1">K` </span></em></span><span class="koboSpan" id="kobo.61.1">with the help of the tied weight matrix </span><span class="emphasis"><em><span class="koboSpan" id="kobo.62.1">W</span><sup><span class="koboSpan" id="kobo.63.1">T</span></sup></em></span><span class="koboSpan" id="kobo.64.1">. </span><span class="koboSpan" id="kobo.64.2">In the network, the activation function is denoted as </span><span class="emphasis"><em><span class="koboSpan" id="kobo.65.1">f</span></em></span><span class="koboSpan" id="kobo.66.1"> and the bias term as </span><span class="emphasis"><em><span class="koboSpan" id="kobo.67.1">b</span></em></span><span class="koboSpan" id="kobo.68.1">. </span><span class="koboSpan" id="kobo.68.2">The activation function could be anything: linear, sigmoidal, or ReLU.</span></p><p><span class="koboSpan" id="kobo.69.1">The equation to compute the sparse representation of the hidden code </span><span class="emphasis"><em><span class="koboSpan" id="kobo.70.1">l</span></em></span><span class="koboSpan" id="kobo.71.1"> is written as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.72.1"><img src="graphics/Capture-10.jpg" alt="Sparse autoencoders"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.73.1">The reconstructed output is the hidden representation, mapped linearly to the output using this:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.74.1"><img src="graphics/Capture-11.jpg" alt="Sparse autoencoders"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.75.1">Learning occurs via backpropagation on the reconstruction error. </span><span class="koboSpan" id="kobo.75.2">All the parameters are optimized to minimize the mean square error, given as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.76.1"><img src="graphics/image_06_008.jpg" alt="Sparse autoencoders"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.77.1">As we have the network setup now, we can add the sparsifying component, which drives the vector </span><span class="emphasis"><em><span class="koboSpan" id="kobo.78.1">L</span></em></span><span class="koboSpan" id="kobo.79.1"> towards a sparse representation. </span><span class="koboSpan" id="kobo.79.2">Here, we will use k-Sparse autoencoders to implement the sparse representation of the layer. </span><span class="koboSpan" id="kobo.79.3">(Don't get confused between the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.80.1">k</span></em></span><span class="koboSpan" id="kobo.81.1"> of k-Sparse representation and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.82.1">K</span></em></span><span class="koboSpan" id="kobo.83.1"> input vector. </span><span class="koboSpan" id="kobo.83.2">To distinguish between both of them, we have denoted these two with a small </span><span class="emphasis"><em><span class="koboSpan" id="kobo.84.1">k</span></em></span><span class="koboSpan" id="kobo.85.1"> and capital </span><span class="emphasis"><em><span class="koboSpan" id="kobo.86.1">K</span></em></span><span class="koboSpan" id="kobo.87.1"> respectively.)</span></p><div class="section" title="The k-Sparse autoencoder"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec18"/><span class="koboSpan" id="kobo.88.1">The k-Sparse autoencoder</span></h3></div></div></div><p><span class="koboSpan" id="kobo.89.1">The k-Sparse autoencoder [132] is based on an autoencoder with tied weights and linear activation functions. </span><span class="koboSpan" id="kobo.89.2">The basic idea of a k-Sparse autoencoder is very simple. </span><span class="koboSpan" id="kobo.89.3">In the feed-forward phase of the autoencoder, once we compute the hidden code </span><span class="emphasis"><em><span class="koboSpan" id="kobo.90.1">l = WK + b</span></em></span><span class="koboSpan" id="kobo.91.1">, rather than reconstructing the input from all the hidden units, the method searches for the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.92.1">k</span></em></span><span class="koboSpan" id="kobo.93.1"> largest hidden units and sets the remaining hidden units' values as zero.</span></p><p><span class="koboSpan" id="kobo.94.1">There are alternative methods to determine the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.95.1">k</span></em></span><span class="koboSpan" id="kobo.96.1"> largest hidden units. </span><span class="koboSpan" id="kobo.96.2">By sorting the activities of the hidden units or using ReLU, hidden units with thresholds are adjusted until we determine the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.97.1">k</span></em></span><span class="koboSpan" id="kobo.98.1"> largest activities. </span><span class="koboSpan" id="kobo.98.2">This selection step to find the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.99.1">k</span></em></span><span class="koboSpan" id="kobo.100.1"> largest activities is non-linear. </span><span class="koboSpan" id="kobo.100.2">The selection step behaves like a regularizer, which helps to prevent the use of large numbers of hidden units while building the output by reconstructing the input.</span></p><div class="section" title="How to select the sparsity level k"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec4"/><span class="koboSpan" id="kobo.101.1">How to select the sparsity level k</span></h4></div></div></div><p><span class="koboSpan" id="kobo.102.1">An issue might arise during the training of a k-Sparse autoencoder if we enforce a low sparsity level, say </span><span class="emphasis"><em><span class="koboSpan" id="kobo.103.1">k=10</span></em></span><span class="koboSpan" id="kobo.104.1">. </span><span class="koboSpan" id="kobo.104.2">One common problem is that in the first few epochs, the algorithm will aggressively start assigning the individual hidden units to groups of training cases. </span><span class="koboSpan" id="kobo.104.3">The phenomena can be compared with the k-means clustering approach. </span><span class="koboSpan" id="kobo.104.4">In the successive epochs, these hidden units will be selected and re-enforced, but the other hidden units would not be adjusted.</span></p><p><span class="koboSpan" id="kobo.105.1">This issue can be addressed by scheduling the sparsity level in a proper way. </span><span class="koboSpan" id="kobo.105.2">Let us assume we are aiming for a sparsity level of 10. </span><span class="koboSpan" id="kobo.105.3">In such cases, we can start with a large sparsity level of say </span><span class="emphasis"><em><span class="koboSpan" id="kobo.106.1">k=100</span></em></span><span class="koboSpan" id="kobo.107.1"> or </span><span class="emphasis"><em><span class="koboSpan" id="kobo.108.1">k=200</span></em></span><span class="koboSpan" id="kobo.109.1">. </span><span class="koboSpan" id="kobo.109.2">Hence, the k-Sparse autoencoder can train all the hidden units present. </span><span class="koboSpan" id="kobo.109.3">Gradually, over half of the epoch, we can linearly decrease the sparsity level of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.110.1">k=100</span></em></span><span class="koboSpan" id="kobo.111.1"> to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.112.1">k=10</span></em></span><span class="koboSpan" id="kobo.113.1">. </span><span class="koboSpan" id="kobo.113.2">This greatly increases the chances of all the hidden units being picked. </span><span class="koboSpan" id="kobo.113.3">Then, we will keep </span><span class="emphasis"><em><span class="koboSpan" id="kobo.114.1">k=10</span></em></span><span class="koboSpan" id="kobo.115.1"> for the next half of the epoch. </span><span class="koboSpan" id="kobo.115.2">In this way, this kind of scheduling will guarantee that even with a low sparsity level, all of the filters will be trained.</span></p></div><div class="section" title="Effect of sparsity level"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec5"/><span class="koboSpan" id="kobo.116.1">Effect of sparsity level</span></h4></div></div></div><p><span class="koboSpan" id="kobo.117.1">The choice of value of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.118.1">k</span></em></span><span class="koboSpan" id="kobo.119.1"> is very much crucial while designing or implementing a k-Sparse autoencoder. </span><span class="koboSpan" id="kobo.119.2">The value of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.120.1">k</span></em></span><span class="koboSpan" id="kobo.121.1"> determines the desirable sparsity level, which helps to make the algorithm ideal for a wide variety of datasets. </span><span class="koboSpan" id="kobo.121.2">For example, one application could be used to pre-train a deep discriminative neural network or a shallow network.</span></p><p><span class="koboSpan" id="kobo.122.1">If we take a large value for </span><span class="emphasis"><em><span class="koboSpan" id="kobo.123.1">k</span></em></span><span class="koboSpan" id="kobo.124.1"> (say </span><span class="emphasis"><em><span class="koboSpan" id="kobo.125.1">k=200</span></em></span><span class="koboSpan" id="kobo.126.1"> on an MNIST dataset), the algorithm will tend to identify and learn very local features of the dataset. </span><span class="koboSpan" id="kobo.126.2">These features sometimes behave too prematurely to be used for the classification of a shallow architecture. </span><span class="koboSpan" id="kobo.126.3">A shallow architecture generally has a naive linear classifier, which does not really have enough architectural strength to merge all of these features and achieve a substantial classification rate. </span><span class="koboSpan" id="kobo.126.4">However, similar local features are very much desirable to pre-train a deep neural network.</span></p><p><span class="koboSpan" id="kobo.127.1">For a smaller value of the sparsity level (say </span><span class="emphasis"><em><span class="koboSpan" id="kobo.128.1">k=10</span></em></span><span class="koboSpan" id="kobo.129.1"> on an MNIST dataset), the output is reconstructed from the input using a smaller set of hidden units. </span><span class="koboSpan" id="kobo.129.2">This eventually results in detecting of global features from the datasets, instead of local features as in the earlier case. </span><span class="koboSpan" id="kobo.129.3">These less local features are suitable for shallow architecture for the classification tasks. </span><span class="koboSpan" id="kobo.129.4">On the contrary, these types of situations are not ideal for deep neural networks.</span></p></div></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Deep autoencoders"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec42"/><span class="koboSpan" id="kobo.1.1">Deep autoencoders</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">So far, we have talked only about single-layer encoders and single-layer decoders for a simple autoencoder. </span><span class="koboSpan" id="kobo.2.2">However, a deep autoencoder with more than one encoder and decoder brings more advantages.</span></p><p><span class="koboSpan" id="kobo.3.1">Feed-forward networks perform better when they are deep. </span><span class="koboSpan" id="kobo.3.2">Autoencoders are basically feed-forward networks; hence, the advantages of a basic feed-forward network can also be applied to autoencoders. </span><span class="koboSpan" id="kobo.3.3">The encoders and decoders are autoencoders, which also work like a feed-forward network. </span><span class="koboSpan" id="kobo.3.4">Hence, we can deploy the advantages of the depth of a feed-forward network in these components also.</span></p><p><span class="koboSpan" id="kobo.4.1">In this context, we can also talk about the universal approximator theorem, which ensures that a feed-forward neural network with at least one hidden layer, and with enough hidden units, can produce an approximation of any arbitrary function to any degree of accuracy. </span><span class="koboSpan" id="kobo.4.2">Following this concept, a deep autoencoder having at least one hidden layer, and containing sufficient hidden units, can approximate any mapping from input to code arbitrarily well.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note24"/><span class="koboSpan" id="kobo.5.1">Note</span></h3><p><span class="koboSpan" id="kobo.6.1">One can approximate any continuous function to any degree of accuracy with a two-layer network. </span><span class="koboSpan" id="kobo.6.2">In the mathematical theory of artificial neural networks, the universal approximation function states that a feed-forward network can approximate any continuous function of a compact subset of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">R</span><sup><span class="koboSpan" id="kobo.8.1">n</span></sup></em></span><span class="koboSpan" id="kobo.9.1">, if it has at least one hidden layer with a finite number of neurons.</span></p></div></div><p><span class="koboSpan" id="kobo.10.1">Deep autoencoder provides many advantages as compared to shallow architecture. </span><span class="koboSpan" id="kobo.10.2">The non-trivial depth of an autoencoder suppresses the computation of representing a few functions. </span><span class="koboSpan" id="kobo.10.3">Also, the depth of autoencoders drastically reduces the amount of training data required to learn the functions. </span><span class="koboSpan" id="kobo.10.4">Even experimentally, it has been found that deep autoencoders provide better compression when compared to shallow autoencoders.</span></p><p><span class="koboSpan" id="kobo.11.1">To train a deep autoencoder, the common practice is to train a stack of shallow autoencoders. </span><span class="koboSpan" id="kobo.11.2">Therefore, to train a deep autoencoder, a series of shallow autoencoders are encountered frequently. </span><span class="koboSpan" id="kobo.11.3">In the next subsections, we will discuss the concept of deep autoencoders in depth.</span></p><div class="section" title="Training of deep autoencoders"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec58"/><span class="koboSpan" id="kobo.12.1">Training of deep autoencoders</span></h2></div></div></div><p><span class="koboSpan" id="kobo.13.1">The design of a deep autoencoder explained here is based on MNIST handwritten digit databases. </span><span class="koboSpan" id="kobo.13.2">In the paper [133],a well-structured procedure of building and training of a deep autoencoder is explained. </span><span class="koboSpan" id="kobo.13.3">The fundamentals of training a deep autoencoder is through three phases, that is: Pre-training, Unrolling, and Fine-tuning.</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong><span class="koboSpan" id="kobo.14.1">Pre-training</span></strong></span><span class="koboSpan" id="kobo.15.1">: The first phase of training a deep autoencoder is 'pre-training'. </span><span class="koboSpan" id="kobo.15.2">The main purpose of this phase is to work on binary data, generalize in to a real-valued data, and then to conclude that it works well for various datasets.</span><p><span class="koboSpan" id="kobo.16.1">We already have enough insights that a single layer of hidden units is not the proper way to model the structure in a large set of images. </span><span class="koboSpan" id="kobo.16.2">A deep autoencoder is composed of multiple layers of a Restricted Boltzmann machine. </span><span class="koboSpan" id="kobo.16.3">In </span><a class="link" href="ch05.html" title="Chapter 5.  Restricted Boltzmann Machines"><span class="koboSpan" id="kobo.17.1">Chapter 5</span></a><span class="koboSpan" id="kobo.18.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.19.1">Restricted Boltzmann Machines</span></em></span><span class="koboSpan" id="kobo.20.1"> we gave enough information on how a Restricted Boltzmann machine works. </span><span class="koboSpan" id="kobo.20.2">Using the same concept, we can proceed to build the structure for a deep autoencoder:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.21.1"><img src="graphics/image_06_009.jpg" alt="Training of deep autoencoders"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.22.1">Figure 6.4: Pre-training a deep autoencoder involves learning a stack of Restricted Boltzmann machines (RBMs) where each RBM possesses a single layer of feature detectors. </span><span class="koboSpan" id="kobo.22.2">The learned features of one Restricted Boltzmann machine is used as the 'input data' to train the next RBM of the stack. </span><span class="koboSpan" id="kobo.22.3">After the pre-training phase, all the RBMs are unfolded or unrolled to build a deep autoencoder. </span><span class="koboSpan" id="kobo.22.4">This deep autoencoder is then fine-tuned using the backpropagation approach of error derivatives.</span></p><p><span class="koboSpan" id="kobo.23.1">When the first layer of the RBM is driven by a stream of data, the layer starts to learn the feature detectors. </span><span class="koboSpan" id="kobo.23.2">This learning can be treated as input data for learning for the next layer. </span><span class="koboSpan" id="kobo.23.3">In this way, feature detectors of the first layer become the visible units for learning the next layer of the Restricted Boltzmann machine. </span><span class="koboSpan" id="kobo.23.4">This procedure of learning layer-by-layer can be iterated as many times as desired. </span><span class="koboSpan" id="kobo.23.5">This procedure is indeed very much effectual in pre-training the weights of a deep autoencoder. </span><span class="koboSpan" id="kobo.23.6">The features captured after each layer have a string of high-order correlations between the activities of the hidden units below. </span><span class="koboSpan" id="kobo.23.7">The first part of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.24.1">Figure 6.4</span></em></span><span class="koboSpan" id="kobo.25.1"> gives a flow diagram of this procedure. </span><span class="koboSpan" id="kobo.25.2">Processing the benchmark dataset MNIST, a deep autoencoder would use binary transformations after each RBM. </span><span class="koboSpan" id="kobo.25.3">To process real-valued data, deep autoencoders use Gaussian rectified transformations after each Restricted Boltzmann machine layer.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.26.1"><img src="graphics/B05883_06_05-1.jpg" alt="Training of deep autoencoders"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.27.1">Figure 6.5: Pictorial representation of how the number or vectors of encoder and decoder varies during the phases.</span></p></li><li class="listitem"><span class="strong"><strong><span class="koboSpan" id="kobo.28.1">Unrolling</span></strong></span><span class="koboSpan" id="kobo.29.1">: Once the multiple layers of feature detectors of the deep autoencoders are pre-trained, the whole model is unrolled to generate the encoder and decoder networks, which at first use the same weights. </span><span class="koboSpan" id="kobo.29.2">We will explain each of the designs of each part given in the second part of the image separately to have a better understanding of this phase.</span><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.30.1">Encoder</span></strong></span><span class="koboSpan" id="kobo.31.1">: For an MNIST dataset of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.32.1">28x28</span></em></span><span class="koboSpan" id="kobo.33.1"> pixel images, the input that the network will get is that of 784 pixels. </span><span class="koboSpan" id="kobo.33.2">As per the rule of thumb, the number of parameters of the first layer of the deep autoencoder should be slightly larger. </span><span class="koboSpan" id="kobo.33.3">As shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.34.1">Figure 6.4</span></em></span><span class="koboSpan" id="kobo.35.1">, </span><span class="strong"><strong><span class="koboSpan" id="kobo.36.1">2000</span></strong></span><span class="koboSpan" id="kobo.37.1"> parameters are taken for the first layer of the network. </span><span class="koboSpan" id="kobo.37.2">This might sound unreasonable, as taking more parameters as inputs increase the chance of overfitting the network. </span><span class="koboSpan" id="kobo.37.3">However, in this case, increasing the number of parameters will eventually increase the features of the input, which, in turn, make the decoding of the autoencoder data possible.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.38.1">As shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.39.1">Figure 6.4</span></em></span><span class="koboSpan" id="kobo.40.1">, the layers would be </span><span class="strong"><strong><span class="koboSpan" id="kobo.41.1">2000</span></strong></span><span class="koboSpan" id="kobo.42.1">, </span><span class="strong"><strong><span class="koboSpan" id="kobo.43.1">1000</span></strong></span><span class="koboSpan" id="kobo.44.1">, </span><span class="strong"><strong><span class="koboSpan" id="kobo.45.1">500</span></strong></span><span class="koboSpan" id="kobo.46.1">, and </span><span class="strong"><strong><span class="koboSpan" id="kobo.47.1">30</span></strong></span><span class="koboSpan" id="kobo.48.1">-nodes wide respectively. </span><span class="koboSpan" id="kobo.48.2">A snapshot of this phenomenon is depicted in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.49.1">Figure 6.5</span></em></span><span class="koboSpan" id="kobo.50.1">. </span><span class="koboSpan" id="kobo.50.2">In the end, the encoder will produce a vector </span><span class="strong"><strong><span class="koboSpan" id="kobo.51.1">30</span></strong></span><span class="koboSpan" id="kobo.52.1"> numbers long. </span><span class="koboSpan" id="kobo.52.2">This </span><span class="strong"><strong><span class="koboSpan" id="kobo.53.1">30</span></strong></span><span class="koboSpan" id="kobo.54.1"> number vector is the last layer of the encoder of the deep autoencoder. </span><span class="koboSpan" id="kobo.54.2">A rough outline for this encoder will be as follows:</span><p>
</p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.55.1"><img src="graphics/image_06_011.jpg" alt="Training of deep autoencoders"/></span></div><p>
</p><p>
</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.56.1">Decoder</span></strong></span><span class="koboSpan" id="kobo.57.1">: The </span><span class="strong"><strong><span class="koboSpan" id="kobo.58.1">30</span></strong></span><span class="koboSpan" id="kobo.59.1"> number vectors found at the end of the encoding phase are the encoded version of the 28x28 pixel images. </span><span class="koboSpan" id="kobo.59.2">The second part of the deep autoencoder is the decoder phase, where it basically learns how to decode the condensed vector. </span><span class="koboSpan" id="kobo.59.3">Hence, the output of the encoder phase (</span><span class="strong"><strong><span class="koboSpan" id="kobo.60.1">30</span></strong></span><span class="koboSpan" id="kobo.61.1">-number vectors) becomes the input of the decoder phase. </span><span class="koboSpan" id="kobo.61.2">This half of the deep autoencoder is a feed-forward network, where the encoded condensed vector proceeds towards the reconstructed input after each layer. </span><span class="koboSpan" id="kobo.61.3">The layers shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.62.1">Figure 6.4</span></em></span><span class="koboSpan" id="kobo.63.1"> are </span><span class="strong"><strong><span class="koboSpan" id="kobo.64.1">30</span></strong></span><span class="koboSpan" id="kobo.65.1">, </span><span class="strong"><strong><span class="koboSpan" id="kobo.66.1">500</span></strong></span><span class="koboSpan" id="kobo.67.1">, </span><span class="strong"><strong><span class="koboSpan" id="kobo.68.1">1000</span></strong></span><span class="koboSpan" id="kobo.69.1">, and </span><span class="strong"><strong><span class="koboSpan" id="kobo.70.1">2000</span></strong></span><span class="koboSpan" id="kobo.71.1">. </span><span class="koboSpan" id="kobo.71.2">The layers initially possess the same weights as their counterparts in the pre-training network; it is just that the weights are transposed as shown in the figure. </span><span class="koboSpan" id="kobo.71.3">A rough outline for this encoder will be as follows:</span><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.72.1"><img src="graphics/image_06_012.jpg" alt="Training of deep autoencoders"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.73.1">So, the main purpose of decoding half of a deep autoencoder is to learn how to reconstruct the image. </span><span class="koboSpan" id="kobo.73.2">The operation is carried out in the second feed-forward network that also performs back propagation, which happens through reconstruction entropy.</span></p></li></ul></div><p>
</p></li><li class="listitem"><span class="strong"><strong><span class="koboSpan" id="kobo.74.1">Fine-tuning</span></strong></span><span class="koboSpan" id="kobo.75.1">: In the fine-tuning phase, the stochastic activities are replaced by the deterministic, real-valued probabilities. </span><span class="koboSpan" id="kobo.75.2">The weights associated with each layer of the whole deep autoencoder are fine-tuned for optimal reconstruction by using the backpropagation method.</span></li></ol></div></div><div class="section" title="Implementation of deep autoencoders using Deeplearning4j"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec59"/><span class="koboSpan" id="kobo.76.1">Implementation of deep autoencoders using Deeplearning4j</span></h2></div></div></div><p><span class="koboSpan" id="kobo.77.1">So, you now have sufficient idea of how to build a deep autoencoder using a number of Restricted Boltzmann machines. </span><span class="koboSpan" id="kobo.77.2">In this section, we will explain how to design a deep autoencoder with the help of Deeplearning4j.</span></p><p><span class="koboSpan" id="kobo.78.1">We will use the same MNIST dataset as in the previous section, and keep the design of the deep autoencoder similar to what we explained earlier.</span></p><p><span class="koboSpan" id="kobo.79.1">As already explained in earlier examples, a small batch size of 1024 number of examples is used from the raw MNIST datasets, which can be split into </span><span class="emphasis"><em><span class="koboSpan" id="kobo.80.1">N</span></em></span><span class="koboSpan" id="kobo.81.1"> multiple blocks of Hadoop. </span><span class="koboSpan" id="kobo.81.2">These </span><span class="emphasis"><em><span class="koboSpan" id="kobo.82.1">N</span></em></span><span class="koboSpan" id="kobo.83.1"> multiple blocks will run on the Hadoop Distributed File System by each worker in parallel. </span><span class="koboSpan" id="kobo.83.2">The flow of code to implement the deep autoencoder is simple and straightforward.</span></p><p><span class="koboSpan" id="kobo.84.1">The steps are shown as follows:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.85.1">Batch-wise loading of the MNIST dataset in HDFS. </span><span class="koboSpan" id="kobo.85.2">Each batch will contain </span><code class="literal"><span class="koboSpan" id="kobo.86.1">1024</span></code><span class="koboSpan" id="kobo.87.1"> number of examples.</span></li><li class="listitem"><span class="koboSpan" id="kobo.88.1">Start building the model.</span></li><li class="listitem"><span class="koboSpan" id="kobo.89.1">Perform the encoding operation.</span></li><li class="listitem"><span class="koboSpan" id="kobo.90.1">Perform the decoding operation.</span></li><li class="listitem"><span class="koboSpan" id="kobo.91.1">Train the model by calling the </span><code class="literal"><span class="koboSpan" id="kobo.92.1">fit()</span></code><span class="koboSpan" id="kobo.93.1"> method.</span></li></ol></div><pre class="programlisting"><span class="koboSpan" id="kobo.94.1">final int numRows = 28;</span></pre><p><span class="koboSpan" id="kobo.95.1">Initial configuration needed to set the Hadoop environment. </span><span class="koboSpan" id="kobo.95.2">The </span><code class="literal"><span class="koboSpan" id="kobo.96.1">batchsize</span></code><span class="koboSpan" id="kobo.97.1"> is set to </span><code class="literal"><span class="koboSpan" id="kobo.98.1">1024</span></code><span class="koboSpan" id="kobo.99.1">.</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.100.1">final int numColumns = 28; 
int seed = 123; 
int numSamples = MnistDataFetcher.NUM_EXAMPLES; 
int batchSize = 1024; 
int iterations = 1; 
int listenerFreq = iterations/5; 
</span></pre><p><span class="koboSpan" id="kobo.101.1">Load the data into the HDFS:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.102.1">log.info("Load data...."); 
DataSetIterator iter = new  MnistDataSetIterator(batchSize,numSamples,true); 
</span></pre><p><span class="koboSpan" id="kobo.103.1">We are now all set to build the model to add the number of layers of the Restricted Boltzmann machine to build the deep autoencoder:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.104.1">log.info("Build model...."); 
MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder() 
  .seed(seed) 
  .iterations(iterations) 
  .optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT) 
</span></pre><p><span class="koboSpan" id="kobo.105.1">To a create a ListBuilder with the specified layers (here it is eight), we call the .</span><code class="literal"><span class="koboSpan" id="kobo.106.1">list()</span></code><span class="koboSpan" id="kobo.107.1"> method:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.108.1">  .list(8) 
</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.109.1">The next step now is to build the encoding phase of the model. </span><span class="koboSpan" id="kobo.109.2">This can be done by the subsequent addition of the Restricted Boltzmann machine into the model. </span><span class="koboSpan" id="kobo.109.3">The encoding phase has four layers of the restricted Boltzmann machine in which each layer would have </span><code class="literal"><span class="koboSpan" id="kobo.110.1">2000</span></code><span class="koboSpan" id="kobo.111.1">, </span><code class="literal"><span class="koboSpan" id="kobo.112.1">1000</span></code><span class="koboSpan" id="kobo.113.1">, </span><code class="literal"><span class="koboSpan" id="kobo.114.1">500</span></code><span class="koboSpan" id="kobo.115.1">, and </span><code class="literal"><span class="koboSpan" id="kobo.116.1">30</span></code><span class="koboSpan" id="kobo.117.1"> nodes respectively:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.118.1">  .layer(0, new RBM.Builder().nIn(numRows *    
  numColumns).nOut(2000).lossFunction(LossFunctions.LossFunction
  .RMSE_XENT).build()) 
  .layer(1, new RBM.Builder().nIn(2000).nOut(1000)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) 
  .layer(2, new RBM.Builder().nIn(1000).nOut(500)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) 
  .layer(3, new RBM.Builder().nIn(500).nOut(30)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) 
                
</span></pre><p><span class="koboSpan" id="kobo.119.1">The next phase after encoder is the decoder phase, where we will use four more Restricted Boltzmann machines in a similar manner:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.120.1">  .layer(4, new RBM.Builder().nIn(30).nOut(500)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())  
  .layer(5, new RBM.Builder().nIn(500).nOut(1000)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) 
  .layer(6, new RBM.Builder().nIn(1000).nOut(2000)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) 
  .layer(7, new OutputLayer.Builder(LossFunctions.LossFunction.MSE)
  .activation("sigmoid").nIn(2000).nOut(numRows*numColumns).build()) 
</span></pre><p><span class="koboSpan" id="kobo.121.1">As all the intermediate layers are now built, we can build the model by calling the </span><code class="literal"><span class="koboSpan" id="kobo.122.1">build()</span></code><span class="koboSpan" id="kobo.123.1"> method:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.124.1">  .pretrain(true).backprop(true) 
  .build(); 
</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.125.1">The last phase of the implementation is to train the deep autoencoder. </span><span class="koboSpan" id="kobo.125.2">It can be done by calling the </span><code class="literal"><span class="koboSpan" id="kobo.126.1">fit ()</span></code><span class="koboSpan" id="kobo.127.1"> method:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.128.1">MultiLayerNetwork model = new MultiLayerNetwork(conf); 
model.init(); 
 
model.setListeners(new ScoreIterationListener(listenerFreq)); 
 
log.info("Train model...."); 
while(iter.hasNext())
  { 
   DataSet next = iter.next(); 
  </span><span class="strong"><strong><span class="koboSpan" id="kobo.129.1"> model.fit(new DataSet(next.getFeatureMatrix(),next
   .getFeatureMatrix()));</span></strong></span><span class="koboSpan" id="kobo.130.1"> 
  } 
</span></pre></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Denoising autoencoder"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec43"/><span class="koboSpan" id="kobo.1.1">Denoising autoencoder</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">The reconstruction of output from input does not always guarantee the desired output, and can sometimes end up in simply copying the input. </span><span class="koboSpan" id="kobo.2.2">To prevent such a situation, in [134], a different strategy has been proposed. </span><span class="koboSpan" id="kobo.2.3">In that proposed architecture, rather than putting some constraints in the representation of the input data, the reconstruction criteria is built, based on cleaning the partially corrupted input.</span></p><div class="blockquote"><blockquote class="blockquote"><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.3.1">"A good representation is one that can be obtained robustly from a corrupted input and that will be useful for recovering the corresponding clean input."</span></em></span>
</p></blockquote></div><p><span class="koboSpan" id="kobo.4.1">A denoising autoencoder is a type of autoencoder which takes corrupted data as input, and the model is trained to predict the original, clean, and uncorrupted data as its output. </span><span class="koboSpan" id="kobo.4.2">In this section, we will explain the basic idea behind designing a denoising autoencoder.</span></p><div class="section" title="Architecture of a Denoising autoencoder"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec60"/><span class="koboSpan" id="kobo.5.1">Architecture of a Denoising autoencoder</span></h2></div></div></div><p><span class="koboSpan" id="kobo.6.1">The primary idea behind a denoising autoencoder is to introduce a corruption process, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">Q (k</span><sup><span class="koboSpan" id="kobo.8.1">/</span></sup><span class="koboSpan" id="kobo.9.1"> | k)</span></em></span><span class="koboSpan" id="kobo.10.1">, and reconstruct the output </span><span class="emphasis"><em><span class="koboSpan" id="kobo.11.1">r</span></em></span><span class="koboSpan" id="kobo.12.1"> from the corrupted input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.13.1">k</span><sup><span class="koboSpan" id="kobo.14.1">/</span></sup></em></span><span class="koboSpan" id="kobo.15.1">. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.16.1">Figure 6.6</span></em></span><span class="koboSpan" id="kobo.17.1"> shows the overall representation of a denoising autoencoder. </span><span class="koboSpan" id="kobo.17.2">In a denoising autoencoder, for every minibatch of training data </span><span class="emphasis"><em><span class="koboSpan" id="kobo.18.1">k</span></em></span><span class="koboSpan" id="kobo.19.1">, the corresponding corrupted </span><span class="emphasis"><em><span class="koboSpan" id="kobo.20.1">k</span><sup><span class="koboSpan" id="kobo.21.1">/</span></sup></em></span><span class="koboSpan" id="kobo.22.1"> should be generated using </span><span class="emphasis"><em><span class="koboSpan" id="kobo.23.1">Q (k</span><sup><span class="koboSpan" id="kobo.24.1">/</span></sup><span class="koboSpan" id="kobo.25.1"> | k)</span></em></span><span class="koboSpan" id="kobo.26.1">. </span><span class="koboSpan" id="kobo.26.2">From there, if we consider the initial input as the corrupted input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.27.1">k</span><sup><span class="koboSpan" id="kobo.28.1">/</span></sup></em></span><span class="koboSpan" id="kobo.29.1">, then the whole model can be considered as a form of a basic encoder. </span><span class="koboSpan" id="kobo.29.2">The corrupted input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.30.1">k</span><sup><span class="koboSpan" id="kobo.31.1">/ </span></sup></em></span><span class="koboSpan" id="kobo.32.1">is mapped to generate the hidden representation </span><span class="emphasis"><em><span class="koboSpan" id="kobo.33.1">h</span></em></span><span class="koboSpan" id="kobo.34.1">.</span></p><p><span class="koboSpan" id="kobo.35.1">Therefore, we get the following:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.36.1"><img src="graphics/image_06_013.jpg" alt="Architecture of a Denoising autoencoder"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.37.1">From this hidden representation, the reconstructed output, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.38.1">r</span></em></span><span class="koboSpan" id="kobo.39.1">, can be derived using </span><span class="emphasis"><em><span class="koboSpan" id="kobo.40.1">r = g (h)</span></em></span><span class="koboSpan" id="kobo.41.1">. </span><span class="koboSpan" id="kobo.41.2">Denoising autoencoder reorganizes the data, and then tries to learn about the data for the reconstruction of the output. </span><span class="koboSpan" id="kobo.41.3">This reorganization of the data or shuffling of the data generates the noise, and the model learns the features from the noise, which allows categorizing the input. </span><span class="koboSpan" id="kobo.41.4">During training of the network, it produces a model, which computes the distance between that model and the benchmark through a loss function. </span><span class="koboSpan" id="kobo.41.5">The idea is to minimize the average reconstruction error over a training set to make the output r as close as possible to the original uncorrupted input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.42.1">k</span></em></span><span class="koboSpan" id="kobo.43.1">.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.44.1"><img src="graphics/B05883_06_06.jpg" alt="Architecture of a Denoising autoencoder"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.45.1">Figure 6.6: The steps involved in designing a denoising autoencoder. </span><span class="koboSpan" id="kobo.45.2">The original input is k; corrupted input derived from k is denoted as k</span><sup><span class="koboSpan" id="kobo.46.1">/</span></sup><span class="koboSpan" id="kobo.47.1">. </span><span class="koboSpan" id="kobo.47.2">The final output is denoted as r.</span></p></div><div class="section" title="Stacked denoising autoencoders"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec61"/><span class="koboSpan" id="kobo.48.1">Stacked denoising autoencoders</span></h2></div></div></div><p><span class="koboSpan" id="kobo.49.1">The basic concept of building a stacked denoising autoencoder to initialize a deep neural network is similar to stacking a number of Restricted Boltzmann machines to build a Deep Belief network or a traditional deep autoencoder. </span><span class="koboSpan" id="kobo.49.2">The generation of corrupted input is only needed for the initial denoising training of each of the individual layers to help in learning the useful features extraction.</span></p><p><span class="koboSpan" id="kobo.50.1">Once we know the encoding function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.51.1">f</span></em></span><span class="koboSpan" id="kobo.52.1"> to reach the hidden state, it is used on the original, uncorrupted data to reach the next level. </span><span class="koboSpan" id="kobo.52.2">In general, no corruption or noise is put to generate the representation, which will act as an uncorrupted input for training the next layer. </span><span class="koboSpan" id="kobo.52.3">A key function of a stacked denoising autoencoder is its layer-by-layer unsupervised pre-training as the input is fed through. </span><span class="koboSpan" id="kobo.52.4">Once a layer is pre-trained to perform the feature selection and extraction on the input from the preceding layer, the next stages of supervised fine tuning can follow, just as in case of the traditional deep autoencoders.</span></p><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.53.1">Figure 6.7</span></em></span><span class="koboSpan" id="kobo.54.1"> shows the detailed representation to design a stacked denoising autoencoder. </span><span class="koboSpan" id="kobo.54.2">The overall procedure for learning and stacking multiple layers of a denoising autoencoder is shown in the following figure:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.55.1"><img src="graphics/B05883_06_07-1.jpg" alt="Stacked denoising autoencoders"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.56.1">Figure 6.7: The representation of a stacked denoising autoencoder</span></p></div><div class="section" title="Implementation of a stacked denoising autoencoder using Deeplearning4j"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec62"/><span class="koboSpan" id="kobo.57.1">Implementation of a stacked denoising autoencoder using Deeplearning4j</span></h2></div></div></div><p><span class="koboSpan" id="kobo.58.1">Stacked denoising autoencoders can be built using Deeplearning4j by creating a </span><code class="literal"><span class="koboSpan" id="kobo.59.1">MultiLayerNetwork</span></code><span class="koboSpan" id="kobo.60.1"> that possesses autoencoders as its hidden layers. </span><span class="koboSpan" id="kobo.60.2">The autoencoders have some </span><code class="literal"><span class="koboSpan" id="kobo.61.1">corruptionLevel</span></code><span class="koboSpan" id="kobo.62.1">, which is denoted as noise.</span></p><p><span class="koboSpan" id="kobo.63.1">Here we set the initial configuration needed to set up the model. </span><span class="koboSpan" id="kobo.63.2">For illustration purposes, a </span><code class="literal"><span class="koboSpan" id="kobo.64.1">batchSize</span></code><span class="koboSpan" id="kobo.65.1"> of </span><code class="literal"><span class="koboSpan" id="kobo.66.1">1024</span></code><span class="koboSpan" id="kobo.67.1"> numbers of examples is taken. </span><span class="koboSpan" id="kobo.67.2">The input number and output number is taken as </span><code class="literal"><span class="koboSpan" id="kobo.68.1">1000</span></code><span class="koboSpan" id="kobo.69.1"> and </span><code class="literal"><span class="koboSpan" id="kobo.70.1">2</span></code><span class="koboSpan" id="kobo.71.1"> respectively.</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.72.1">int outputNum = 2;
int inputNum = 1000;
int iterations = 10;
int seed = 123;
int batchSize = 1024;
</span></pre><p><span class="koboSpan" id="kobo.73.1">The loading of the input dataset is the same as explained in the deep autoencoder section. </span><span class="koboSpan" id="kobo.73.2">Therefore, we will directly jump to how to build the stack denoising autoencoder. </span><span class="koboSpan" id="kobo.73.3">We have taken a five-hidden-layer deep model to illustrate the method:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.74.1">log.info ("Build model....");</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.75.1">MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder ()</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.76.1">.seed(seed)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.77.1">.gradientNormalization(GradientNormalization
  .ClipElementWiseAbsoluteValue)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.78.1">.gradientNormalizationThreshold (1.0)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.79.1">.iterations(iterations)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.80.1">.updater(Updater.NESTEROVS)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.81.1">.momentum(0.5)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.82.1">.momentumAfter(Collections.singletonMap(3, 0.9))</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.83.1">.optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.84.1">.list()</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.85.1">.layer(0, new AutoEncoder.Builder()</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.86.1">.nIn(inputNum)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.87.1">.nOut(500)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.88.1">.weightInit(WeightInit.XAVIER)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.89.1">.lossFunction(LossFunction.RMSE_XENT)</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.90.1">The following code denotes how much input data is to be corrupted:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.91.1">.corruptionLevel (0.3)</span></strong></span><span class="koboSpan" id="kobo.92.1">
    .build())
  .layer(1, new AutoEncoder.Builder()
    .nIn(500)
    .nOut(250)
    .weightInit(WeightInit.XAVIER).lossFunction
    (LossFunction.RMSE_XENT)
    .corruptionLevel(0.3)
    .build())
  .layer(2, new AutoEncoder.Builder()
    .nIn(250)
    .nOut(125)
    .weightInit(WeightInit.XAVIER).lossFunction         
    (LossFunction.RMSE_XENT)
    .corruptionLevel(0.3)
    .build())
  .layer(3, new AutoEncoder.Builder()
     .nIn(125)
     .nOut(50)
     .weightInit(WeightInit.XAVIER).lossFunction
     (LossFunction.RMSE_XENT)
     .corruptionLevel(0.3)
     .build())
   .layer(4, new OutputLayer.Builder   
   (LossFunction.NEGATIVELOGLIKELIHOOD)
    </span><span class="strong"><strong><span class="koboSpan" id="kobo.93.1"> .activation("softmax")</span></strong></span><span class="koboSpan" id="kobo.94.1">
     .nIn(75)
     .nOut(outputNum)
     .build())
   </span><span class="strong"><strong><span class="koboSpan" id="kobo.95.1">.pretrain(true)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.96.1">.backprop(false)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.97.1">.build();</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.98.1">Once the model is built, it is trained by calling the </span><code class="literal"><span class="koboSpan" id="kobo.99.1">fit()</span></code><span class="koboSpan" id="kobo.100.1"> method:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.101.1">try {
    </span><span class="strong"><strong><span class="koboSpan" id="kobo.102.1"> model.fit(iter);</span></strong></span><span class="koboSpan" id="kobo.103.1">
    } 
catch(Exception ex)
   {
     ex.printStackTrace();
   }
</span></pre></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Applications of autoencoders"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec44"/><span class="koboSpan" id="kobo.1.1">Applications of autoencoders</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Autoencoders can be successfully applied in many use cases, and hence, have gained much popularity in the world of deep learning. </span><span class="koboSpan" id="kobo.2.2">In this section, we will discuss the important applications and uses of autoencoders:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.3.1">Dimensionality reduction</span></strong></span><span class="koboSpan" id="kobo.4.1">: If you remember, in </span><a class="link" href="ch01.html" title="Chapter 1. Introduction to Deep Learning"><span class="koboSpan" id="kobo.5.1">Chapter 1</span></a><span class="koboSpan" id="kobo.6.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">Introduction to Deep Learning</span></em></span><span class="koboSpan" id="kobo.8.1">, we introduced the concept of the 'curse of dimensionality'. </span><span class="koboSpan" id="kobo.8.2">Dimensionality reduction was one of the first applications of deep learning. </span><span class="koboSpan" id="kobo.8.3">Autoencoders were initially studied to overcome the issues with the curse of dimensionality. </span><span class="koboSpan" id="kobo.8.4">We have already got a fair idea from this chapter how deep autoencoders work on higher-dimensional data to reduce the dimensionality in the final output.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.9.1">Information Retrieval</span></strong></span><span class="koboSpan" id="kobo.10.1">: One more important application of autoencoders is in information retrieval. </span><span class="koboSpan" id="kobo.10.2">Information retrieval basically means to search for some entries, which match with an entered query, in a database. </span><span class="koboSpan" id="kobo.10.3">Searching in high-dimensional data is generally a cumbersome task; however, with reduced dimensionality of a dataset, the search can become extremely efficient in certain kinds of lower dimensional data. </span><span class="koboSpan" id="kobo.10.4">The dimensionality reduction obtained from the autoencoder can generate codes that are low dimensional and binary in nature. </span><span class="koboSpan" id="kobo.10.5">These can be stored in a key values stored data structure, where keys are binary code vectors and values are the corresponding entries. </span><span class="koboSpan" id="kobo.10.6">Such key value stores help us to perform information retrieval by returning all the database entries that match some binary code with the query. </span><span class="koboSpan" id="kobo.10.7">This approach to retrieving information through dimensionality reduction and binary code is called semantic hashing [135].</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.11.1">Image Search</span></strong></span><span class="koboSpan" id="kobo.12.1">: As explained in the deep autoencoder section, deep autoencoders are capable of compressing image datasets of higher dimensions to a very small number of vectors, say 30. </span><span class="koboSpan" id="kobo.12.2">Therefore, this has made image searching easier for high-dimensional images. </span><span class="koboSpan" id="kobo.12.3">Once an image is uploaded, the search engine will compress it into small vectors, and then compare that vector to all the others in its index. </span><span class="koboSpan" id="kobo.12.4">For a search query, the vectors that contain similar numbers will be returned and translated into the mapped image.</span></li></ul></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec45"/><span class="koboSpan" id="kobo.1.1">Summary</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Autoencoders, one of the most popular and widely applicable generative models, have been discussed in this chapter. </span><span class="koboSpan" id="kobo.2.2">Autoencoders basically help two phases: one is the encoder phase and the other is the decoder phase. </span><span class="koboSpan" id="kobo.2.3">In this chapter, we elaborated on both of these phases with suitable mathematical explanations. </span><span class="koboSpan" id="kobo.2.4">Going forward, we explained a special kind of autoencoder called the sparse autoencoder. </span><span class="koboSpan" id="kobo.2.5">We also discussed how autoencoders can be used in the world of deep neural networks by explaining deep autoencoders. </span><span class="koboSpan" id="kobo.2.6">Deep autoencoders consist of layers of Restricted Boltzmann machines, which take part in the encoder and decoder phases of the network. </span><span class="koboSpan" id="kobo.2.7">We explained how to deploy deep autoencoders using Deeplearning4j, by loading chunks of the input dataset into a Hadoop Distributed File System. </span><span class="koboSpan" id="kobo.2.8">Later in this chapter, we introduced the most popular form of autoencoder called the denoising autoencoder and its deep network version known as the stacked denoising autoencoder. </span><span class="koboSpan" id="kobo.2.9">The implementation of a stacked denoising autoencoder using Deeplearning4j was also shown. </span><span class="koboSpan" id="kobo.2.10">We concluded this chapter by outlining the common applications of autoencoders.</span></p><p><span class="koboSpan" id="kobo.3.1">In the next chapter, we will discuss some common useful applications of deep learning with the help of Hadoop.</span></p></div></div></div></body></html>