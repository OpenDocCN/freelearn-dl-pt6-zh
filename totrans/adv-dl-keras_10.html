<html><head></head><body><div class="chapter" title="Chapter&#xA0;10.&#xA0;Policy Gradient Methods"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Policy Gradient Methods</h1></div></div></div><p>In the final chapter of this book, we're going to introduce algorithms that directly optimize the policy network in reinforcement learning. These algorithms are collectively referred to as <span class="emphasis"><em>policy gradient methods</em></span>. Since the policy network is directly optimized during training, the policy gradient methods belong to the family of <span class="emphasis"><em>on-policy</em></span> reinforcement learning algorithms. Like value-based methods that we discussed in <a class="link" href="ch09.html" title="Chapter 9. Deep Reinforcement Learning">Chapter 9</a>, <span class="emphasis"><em>Deep Reinforcement Learning</em></span>, policy gradient methods can also be implemented as deep reinforcement learning algorithms.</p><p>A fundamental motivation in studying the policy gradient methods is addressing the limitations of Q-Learning. We'll recall that Q-Learning is about selecting the action that maximizes the value of the state. With Q function, we're able to determine the policy that enables the agent to decide on which action to take for a given state. The chosen action is simply the one that gives the agent the maximum value. In this respect, Q-Learning is limited to a finite number of discrete actions. It's not able to deal with continuous action space environments. Furthermore, Q-Learning is not directly optimizing the policy. In the end, reinforcement learning is about finding that optimal policy that the agent will be able to use to decide on which action it should take in order to maximize the return.</p><p>In contrast, policy gradient methods are applicable to environments with discrete or continuous action spaces. In addition, the four policy gradient methods that we will be presenting in this chapter are directly optimizing the performance measure of the policy network. This results in a trained policy network that the agent can use to act in its environment optimally.</p><p>In summary, the goal of this chapter is to present:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The policy gradient theorem</li><li class="listitem" style="list-style-type: disc">Four policy gradient methods: <span class="strong"><strong>REINFORCE</strong></span>, <span class="strong"><strong>REINFORCE with baseline</strong></span>, <span class="strong"><strong>Actor-Critic</strong></span>, and <span class="strong"><strong>Advantage Actor-Critic</strong></span>(<span class="strong"><strong>A2C</strong></span>)</li><li class="listitem" style="list-style-type: disc">A guide on how to implement the policy gradient methods in Keras in a continuous action space environment</li></ul></div><div class="section" title="Policy gradient theorem"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec55"/>Policy gradient theorem</h1></div></div></div><p>As discussed in <a class="link" href="ch09.html" title="Chapter 9. Deep Reinforcement Learning">Chapter 9</a>, <span class="emphasis"><em>Deep Reinforcement Learning</em></span>, in Reinforcement Learning the agent is situated<a id="id423" class="indexterm"/> in an environment that is in state <span class="emphasis"><em>s</em></span><sub>t'</sub>, an element of state space <span class="inlinemediaobject"><img src="graphics/B08956_10_001.jpg" alt="Policy gradient theorem"/></span>. The state space <span class="inlinemediaobject"><img src="graphics/B08956_10_001.jpg" alt="Policy gradient theorem"/></span> may be discrete or continuous. The agent takes an action <span class="emphasis"><em>a</em></span><sub>t</sub> from the action space <span class="inlinemediaobject"><img src="graphics/B08956_10_002.jpg" alt="Policy gradient theorem"/></span> by obeying the policy, <span class="inlinemediaobject"><img src="graphics/B08956_10_003.jpg" alt="Policy gradient theorem"/></span>. <span class="inlinemediaobject"><img src="graphics/B08956_10_002.jpg" alt="Policy gradient theorem"/></span> may be discrete or continuous. Because of executing the action <span class="emphasis"><em>a</em></span><sub>t</sub>, the agent receives a reward <span class="emphasis"><em>r</em></span>
<sub>t+1</sub> and the environment transitions to a new state <span class="emphasis"><em>s</em></span>
<sub>t+1</sub>. The new state is dependent only on the current state and action. The goal of the agent is to learn an optimal policy <span class="inlinemediaobject"><img src="graphics/B08956_10_004.jpg" alt="Policy gradient theorem"/></span> that maximizes the <span class="emphasis"><em>return</em></span> from all the states:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_17.jpg" alt="Policy gradient theorem"/></span>     (Equation 9.1.1)</p><p>The return, <span class="inlinemediaobject"><img src="graphics/B08956_10_095.jpg" alt="Policy gradient theorem"/></span>, is defined as the discounted cumulative reward from time <span class="emphasis"><em>t</em></span> until the end of the episode or when the terminal state is reached:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_19.jpg" alt="Policy gradient theorem"/></span>     (Equation 9.1.2)</p><p>From <span class="emphasis"><em>Equation 9.1.2</em></span>, the return can also be interpreted as a value of a given state by following the policy <span class="inlinemediaobject"><img src="graphics/B08956_10_005.jpg" alt="Policy gradient theorem"/></span>. It can be observed from <span class="emphasis"><em>Equation 9.1.1</em></span> that future rewards have lower weights compared to immediate rewards since generally <span class="inlinemediaobject"><img src="graphics/B08956_10_006.jpg" alt="Policy gradient theorem"/></span> where <span class="inlinemediaobject"><img src="graphics/B08956_10_007.jpg" alt="Policy gradient theorem"/></span>.</p><p>So far, we have only considered learning the policy by optimizing a value-based function, <span class="emphasis"><em>Q(s,a)</em></span>. Our goal in this chapter is to directly learn the policy by parameterizing <span class="inlinemediaobject"><img src="graphics/B08956_10_008.jpg" alt="Policy gradient theorem"/></span>. By parameterization, we can use a neural network to learn the policy function. Learning the policy means that we are going to maximize a certain objective function, <span class="inlinemediaobject"><img src="graphics/B08956_10_009.jpg" alt="Policy gradient theorem"/></span> which is a performance measure with respect to parameter <span class="inlinemediaobject"><img src="graphics/B08956_10_010.jpg" alt="Policy gradient theorem"/></span>. In episodic reinforcement learning, the performance measure is the value of the start state. In the continuous case, the objective function is the average reward rate.</p><p>Maximizing the objective function <span class="inlinemediaobject"><img src="graphics/B08956_10_009.jpg" alt="Policy gradient theorem"/></span> is achieved by performing <span class="emphasis"><em>gradient ascent</em></span>. In gradient ascent, the gradient update is in the direction of the derivative of the function being optimized. So far, all our loss functions are optimized by minimization or by performing <span class="emphasis"><em>gradient descent</em></span>. Later, in the Keras implementation, we're able to see that the gradient ascent can be performed by simply negating the objective function and performing gradient descent.</p><p>The advantage of learning the policy directly is that it can be applied to both discrete and continuous action spaces. For discrete action spaces:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_026.jpg" alt="Policy gradient theorem"/></span>     (Equation 10.1.1)</p><p>In that formula, <span class="emphasis"><em>a</em></span><sub><span class="emphasis"><em>i</em></span></sub> is the <span class="emphasis"><em>i</em></span>-th action. <span class="emphasis"><em>a</em></span><sub><span class="emphasis"><em>i</em></span></sub> can be the prediction of a neural network or a linear function<a id="id424" class="indexterm"/> of state-action features:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_011.jpg" alt="Policy gradient theorem"/></span>     (Equation 10.1.2)</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_012.jpg" alt="Policy gradient theorem"/></span> is any function such as an encoder that converts the state-action to features.</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_013.jpg" alt="Policy gradient theorem"/></span> determines the probability of each <span class="emphasis"><em>a</em></span><sub><span class="emphasis"><em>i</em></span></sub>. For example, in the cartpole balancing problem in the previous chapter, the goal is to keep the pole upright by moving the cart along the 2D axis to the left or to the right. In this case, <span class="emphasis"><em>a</em></span><sub><span class="emphasis"><em>0</em></span></sub> and <span class="emphasis"><em>a</em></span><sub><span class="emphasis"><em>1</em></span></sub> are the probabilities of the left and right movements respectively. In general, the agent takes the action with the highest probability, <span class="inlinemediaobject"><img src="graphics/B08956_10_014.jpg" alt="Policy gradient theorem"/></span>.</p><p>For continuous action spaces, <span class="inlinemediaobject"><img src="graphics/B08956_10_015.jpg" alt="Policy gradient theorem"/></span> samples an action from a probability distribution given the state. For example, if the continuous action space is the range <span class="inlinemediaobject"><img src="graphics/B08956_10_016.jpg" alt="Policy gradient theorem"/></span>, then <span class="inlinemediaobject"><img src="graphics/B08956_10_17.jpg" alt="Policy gradient theorem"/></span> is usually a Gaussian distribution whose mean and standard deviation are predicted by the policy network. The predicted action is a sample from this Gaussian distribution. To ensure that no invalid prediction is generated, the action is clipped between <span class="emphasis"><em>-1.0</em></span> and <span class="emphasis"><em>1.0</em></span>.</p><p>Formally, for continuous action spaces, the policy is a sample from a Gaussian distribution:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_018.jpg" alt="Policy gradient theorem"/></span>     (Equation 10.1.3)</p><p>The mean, <span class="inlinemediaobject"><img src="graphics/B08956_10_019.jpg" alt="Policy gradient theorem"/></span>, and standard deviation, <span class="inlinemediaobject"><img src="graphics/B08956_10_020.jpg" alt="Policy gradient theorem"/></span>, are both functions of the state features:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_021.jpg" alt="Policy gradient theorem"/></span>     (Equation 10.1.4)</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_022.jpg" alt="Policy gradient theorem"/></span>     (Equation 10.1.5)</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_023.jpg" alt="Policy gradient theorem"/></span> is any function that converts the state to its features. <span class="inlinemediaobject"><img src="graphics/B08956_10_024.jpg" alt="Policy gradient theorem"/></span> is the <span class="emphasis"><em>softplus</em></span> function that ensures positive values of standard deviation. One way of implementing the state feature function, <span class="inlinemediaobject"><img src="graphics/B08956_10_023.jpg" alt="Policy gradient theorem"/></span>, is using the encoder of an autoencoder network. At the end of this chapter, we will train an autoencoder and use the encoder part as the state feature function. Training a policy network is therefore a matter of optimizing the parameters <span class="inlinemediaobject"><img src="graphics/B08956_10_025.jpg" alt="Policy gradient theorem"/></span>.</p><p>Given a continuously differentiable policy function, <span class="inlinemediaobject"><img src="graphics/B08956_10_015.jpg" alt="Policy gradient theorem"/></span>, the policy gradient can be computed as:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_027.jpg" alt="Policy gradient theorem"/></span>     (Equation 10.1.6)</p><p>
<span class="emphasis"><em>Equation 10.1.6</em></span> is also known as the <span class="emphasis"><em>policy gradient theorem</em></span>. It is applicable to both discrete and<a id="id425" class="indexterm"/> continuous action spaces. The gradient with respect to the parameter <span class="inlinemediaobject"><img src="graphics/B08956_10_010.jpg" alt="Policy gradient theorem"/></span> is computed from the natural logarithm of the policy action sampling scaled by the Q value. <span class="emphasis"><em>Equation 10.1.6</em></span> takes advantage of the property of the natural logarithm, <span class="inlinemediaobject"><img src="graphics/B08956_10_33.jpg" alt="Policy gradient theorem"/></span>.</p><p>Policy gradient theorem is intuitive in the sense that the performance gradient is estimated from the target policy samples and proportional to the policy gradient. The policy gradient is scaled by the Q value to encourage actions that positively contribute to the state value. The gradient is also inversely proportional to the action probability to penalize frequently occurring actions that do not contribute to the increase of performance measure.</p><p>In the next section, we will demonstrate the different methods of estimating the policy gradient.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>For the proof<a id="id426" class="indexterm"/> of policy gradient theorem, please see [2] and lecture notes from David Silver on Reinforcement Learning, <a class="ulink" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf</a>
</p></div></div><p>There are subtle advantages of policy gradient methods. For example, in some card-based games, value-based methods have no straightforward procedure in handling stochasticity, unlike policy-based methods. In policy-based methods, the action probability changes smoothly with the parameters. Meanwhile, value-based actions may suffer from drastic changes with respect to small changes in parameters. Lastly, the dependence of policy-based methods on parameters leads us to different formulations on how to perform gradient ascent on the performance measure. These are the four policy gradient methods to be presented in the succeeding sections.</p><p>Policy-based methods have their own disadvantages as well. They are generally harder to train because of the tendency to converge on a local optimum instead of the global optimum. In the experiments to be presented at the end of this chapter, it is easy for an agent to become comfortable and to choose actions that do not necessarily give the highest value. Policy gradient is also characterized by high variance.</p><p>The gradient updates are frequently overestimated. Furthermore, training policy-based methods are time-consuming. The training requires thousands of episodes (that is, not sample efficient). Each episode only provides a small number of samples. Typical training in the implementation provided at the end of the chapter would take about an hour for 1,000 episodes on a GTX 1060 GPU.</p><p>In the following sections, we discuss the four policy gradient methods. While the discussion focuses<a id="id427" class="indexterm"/> on continuous action spaces, the concept is generally applicable to discrete action spaces. Due to similarities in the implementation of the policy and value networks of the four policy gradient methods, we will wait until the end of this chapter to illustrate the implementation into Keras.</p></div></div>
<div class="section" title="Monte Carlo policy gradient (REINFORCE) method"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec56"/>Monte Carlo policy gradient (REINFORCE) method</h1></div></div></div><p>The simplest<a id="id428" class="indexterm"/> policy gradient method is called REINFORCE [5], this is a Monte Carlo policy gradient method:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_028.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span>     (Equation 10.2.1)</p><p>where <span class="emphasis"><em>R</em></span><sub><span class="emphasis"><em>t</em></span></sub> is the return as defined in <span class="emphasis"><em>Equation 9.1.2</em></span>. <span class="emphasis"><em>R</em></span><sub><span class="emphasis"><em>t</em></span></sub> is an unbiased sample of <span class="inlinemediaobject"><img src="graphics/B08956_10_029.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span> in the policy gradient theorem.</p><p>
<span class="emphasis"><em>Algorithm</em></span> <span class="emphasis"><em>10.2.1</em></span> summarizes the REINFORCE algorithm [2]. REINFORCE is a Monte Carlo algorithm. It does not require knowledge of the dynamics of the environment (that is, model-free). Only experience samples, <span class="inlinemediaobject"><img src="graphics/B08956_10_030.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span>, are needed to optimally tune the parameters of the policy network, <span class="inlinemediaobject"><img src="graphics/B08956_10_015.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span>. The discount factor, <span class="inlinemediaobject"><img src="graphics/B08956_10_031.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span>, takes into consideration that rewards decrease in value as the number of steps increases. The gradient is discounted by <span class="inlinemediaobject"><img src="graphics/B08956_10_032.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span>. Gradients taken at later steps have smaller contributions. The learning rate, <span class="inlinemediaobject"><img src="graphics/B08956_10_033.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span>, is a scaling factor of the gradient update.</p><p>The parameters are updated by performing gradient ascent using the discounted gradient and learning rate. As a Monte Carlo algorithm, REINFORCE requires that the agent completes an episode before processing the gradient updates. Due to its Monte Carlo nature, the gradient update of REINFORCE is characterized by high variance. At the end of this chapter, we will implement the REINFORCE algorithm into Keras.</p><p>
<span class="strong"><strong>Algorithm 10.2.1 REINFORCE</strong></span>
</p><p>
<span class="emphasis"><em>Require</em></span>: A differentiable parameterized target policy network, <span class="inlinemediaobject"><img src="graphics/B08956_10_015.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span>.</p><p>
<span class="emphasis"><em>Require</em></span>: Discount factor, <span class="inlinemediaobject"><img src="graphics/B08956_10_035.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span> and learning rate <span class="inlinemediaobject"><img src="graphics/B08956_10_033.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span>. For example, <span class="inlinemediaobject"><img src="graphics/B08956_10_036.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span> and <span class="inlinemediaobject"><img src="graphics/B08956_10_037.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span>.</p><p>
<span class="emphasis"><em>Require</em></span>: <span class="inlinemediaobject"><img src="graphics/B08956_10_038.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span>, initial policy network parameters (for example, <span class="inlinemediaobject"><img src="graphics/B08956_10_039.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span>).</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Repeat</li><li class="listitem">
    Generate an episode <span class="inlinemediaobject"><img src="graphics/B08956_10_36.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span> by following <span class="inlinemediaobject"><img src="graphics/B08956_10_041.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span></li><li class="listitem">
    for steps <span class="inlinemediaobject"><img src="graphics/B08956_10_042.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span> do
</li><li class="listitem">
        Compute return, <span class="inlinemediaobject"><img src="graphics/B08956_10_37.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span></li><li class="listitem">
Compute discounted<a id="id429" class="indexterm"/> performance gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_38.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span></li><li class="listitem">
       Perform gradient ascent, <span class="inlinemediaobject"><img src="graphics/B08956_10_39.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span></li></ol></div><div class="mediaobject"><img src="graphics/B08956_10_01.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/><div class="caption"><p>Figure 10.2.1: Policy network</p></div></div><p>In REINFORCE, the parameterized policy can be modeled by a neural network as shown in <span class="emphasis"><em>Figure 10.2.1</em></span>. As discussed in the previous section, for the case of continuous action spaces, the state input is converted into features. The state features are the inputs of the policy network. The Gaussian distribution representing the policy function has a mean and standard deviation that are both functions of the state features. The policy network, <span class="inlinemediaobject"><img src="graphics/B08956_10_043.jpg" alt="Monte Carlo policy gradient (REINFORCE) method"/></span>, could be an MLP, CNN, or an RNN depending on the nature of the state inputs. The predicted action is simply a sample from the policy function.</p><div class="section" title="REINFORCE with baseline method"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec43"/>REINFORCE with baseline method</h2></div></div></div><p>The REINFORCE algorithm<a id="id430" class="indexterm"/> can be generalized by subtracting a baseline from the return, <span class="inlinemediaobject"><img src="graphics/B08956_10_044.jpg" alt="REINFORCE with baseline method"/></span>. The baseline function, <span class="emphasis"><em>B(s</em></span>
<span class="emphasis"><em>t</em></span>
<span class="emphasis"><em>)</em></span> can be any function as long as it does not depend on <span class="emphasis"><em>a</em></span><sub>t</sub> The baseline does not alter the expectation of the performance gradient:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_046.jpg" alt="REINFORCE with baseline method"/></span>     (Equation 10.3.1)</p><p>
<span class="emphasis"><em>Equation 10.3.1</em></span> implies that <span class="inlinemediaobject"><img src="graphics/B08956_10_047.jpg" alt="REINFORCE with baseline method"/></span> since <span class="inlinemediaobject"><img src="graphics/B08956_10_048.jpg" alt="REINFORCE with baseline method"/></span> is not a function of <span class="inlinemediaobject"><img src="graphics/B08956_10_049.jpg" alt="REINFORCE with baseline method"/></span>.</p><p>While the<a id="id431" class="indexterm"/> introduction of baseline does not change the expectation, it reduces the variance of the gradient updates. The reduction in variance generally accelerates learning. In most cases, we use the value function, <span class="inlinemediaobject"><img src="graphics/B08956_10_44.jpg" alt="REINFORCE with baseline method"/></span> as the baseline. If the return is overestimated, the scaling factor is proportionally reduced by the value function resulting to a lower variance. The value function is also parameterized, <span class="inlinemediaobject"><img src="graphics/B08956_10_050.jpg" alt="REINFORCE with baseline method"/></span> and is jointly trained with the policy network. In continuous action spaces, the state value can be a linear function of state features:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_051.jpg" alt="REINFORCE with baseline method"/></span>     (Equation 10.3.2)</p><p>
<span class="emphasis"><em>Algorithm</em></span> <span class="emphasis"><em>10.3.1</em></span> summarizes the REINFORCE with baseline method [1]. This is similar to REINFORCE except that the return is replaced by <span class="inlinemediaobject"><img src="graphics/B08956_10_001.jpg" alt="REINFORCE with baseline method"/></span>. The difference is we are now training two neural networks. As shown in <span class="emphasis"><em>Figure 10.3.1</em></span>, in addition to the policy network, <span class="inlinemediaobject"><img src="graphics/B08956_10_034.jpg" alt="REINFORCE with baseline method"/></span>, the value network, <span class="inlinemediaobject"><img src="graphics/B08956_10_052.jpg" alt="REINFORCE with baseline method"/></span>, is also trained at the same time. The policy network parameters are updated by the performance gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_053.jpg" alt="REINFORCE with baseline method"/></span>, while the value network parameters are adjusted by the value gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_054.jpg" alt="REINFORCE with baseline method"/></span>. Since REINFORCE is a Monte Carlo algorithm, it follows that the value function training is also a Monte Carlo algorithm.</p><p>The learning rates are not necessarily the same. Note that the value network is also performing gradient ascent. We illustrate how to implement REINFORCE with baseline using Keras at the end of this chapter.</p><p>
<span class="strong"><strong>Algorithm 10.3.1 REINFORCE with baseline</strong></span>
</p><p>
<span class="emphasis"><em>Require</em></span>: A differentiable parameterized target policy network, <span class="inlinemediaobject"><img src="graphics/B08956_10_015.jpg" alt="REINFORCE with baseline method"/></span>.</p><p>
<span class="emphasis"><em>Require</em></span>: A differentiable parameterized value network, <span class="inlinemediaobject"><img src="graphics/B08956_10_055.jpg" alt="REINFORCE with baseline method"/></span>.</p><p>
<span class="emphasis"><em>Require</em></span>: Discount factor, <span class="inlinemediaobject"><img src="graphics/B08956_10_035.jpg" alt="REINFORCE with baseline method"/></span>, the learning rate <span class="inlinemediaobject"><img src="graphics/B08956_10_033.jpg" alt="REINFORCE with baseline method"/></span> for the performance gradient and learning rate for the value gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_056.jpg" alt="REINFORCE with baseline method"/></span>.</p><p>
<span class="emphasis"><em>Require</em></span>: <span class="inlinemediaobject"><img src="graphics/B08956_10_057.jpg" alt="REINFORCE with baseline method"/></span>, initial policy network parameters (for example, <span class="inlinemediaobject"><img src="graphics/B08956_10_058.jpg" alt="REINFORCE with baseline method"/></span>). <span class="inlinemediaobject"><img src="graphics/B08956_10_059.jpg" alt="REINFORCE with baseline method"/></span>, initial value network parameters (for example, <span class="inlinemediaobject"><img src="graphics/B08956_10_060.jpg" alt="REINFORCE with baseline method"/></span>).</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Repeat</li><li class="listitem">
   Generate an episode <span class="inlinemediaobject"><img src="graphics/B08956_10_061.jpg" alt="REINFORCE with baseline method"/></span> by following <span class="inlinemediaobject"><img src="graphics/B08956_10_015.jpg" alt="REINFORCE with baseline method"/></span></li><li class="listitem">
   for steps <span class="inlinemediaobject"><img src="graphics/B08956_10_042.jpg" alt="REINFORCE with baseline method"/></span> do
</li><li class="listitem">
       Compute return, <span class="inlinemediaobject"><img src="graphics/B08956_10_063.jpg" alt="REINFORCE with baseline method"/></span></li><li class="listitem">
        Subtract baseline, <span class="inlinemediaobject"><img src="graphics/B08956_10_064.jpg" alt="REINFORCE with baseline method"/></span></li><li class="listitem">
        Compute discounted value gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_065.jpg" alt="REINFORCE with baseline method"/></span></li><li class="listitem">
        Perform gradient ascent, <span class="inlinemediaobject"><img src="graphics/B08956_10_066.jpg" alt="REINFORCE with baseline method"/></span></li><li class="listitem">
       Compute discounted performance gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_067.jpg" alt="REINFORCE with baseline method"/></span></li><li class="listitem">
Perform gradient ascent, <span class="inlinemediaobject"><img src="graphics/B08956_10_068.jpg" alt="REINFORCE with baseline method"/></span></li></ol></div><div class="mediaobject"><img src="graphics/B08956_10_02.jpg" alt="REINFORCE with baseline method"/><div class="caption"><p>Figure 10.3.1: Policy and value networks</p></div></div></div><div class="section" title="Actor-Critic method"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec44"/>Actor-Critic method</h2></div></div></div><p>In REINFORCE with baseline method, the value is used as a baseline. It is not used to train the<a id="id432" class="indexterm"/> value function. In this section, we'll introduce a variation of REINFORCE with baseline called the<a id="id433" class="indexterm"/> Actor-Critic method. The policy and value networks played the roles of actor and critic networks. The policy network is the actor deciding which action to take given the state. Meanwhile, the value network evaluates the decision made by the actor or the policy network. The value network acts as a critic which quantifies how good or bad the chosen action made by the actor is. The value network evaluates the state value, <span class="inlinemediaobject"><img src="graphics/B08956_10_069.jpg" alt="Actor-Critic method"/></span>, by comparing it with the sum of the received reward, <span class="inlinemediaobject"><img src="graphics/B08956_10_070.jpg" alt="Actor-Critic method"/></span>, and the discounted value of the observed next state, <span class="inlinemediaobject"><img src="graphics/B08956_10_071.jpg" alt="Actor-Critic method"/></span>. The difference, <span class="inlinemediaobject"><img src="graphics/B08956_10_072.jpg" alt="Actor-Critic method"/></span>, is expressed as:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_073.jpg" alt="Actor-Critic method"/></span>     (Equation 10.4.1)</p><p>where we dropped the subscripts of <span class="inlinemediaobject"><img src="graphics/B08956_10_070.jpg" alt="Actor-Critic method"/></span> and <span class="inlinemediaobject"><img src="graphics/B08956_10_074.jpg" alt="Actor-Critic method"/></span> for simplicity. <span class="emphasis"><em>Equation 10.4.1</em></span> is similar to the temporal<a id="id434" class="indexterm"/> differencing in Q-Learning discussed in <a class="link" href="ch09.html" title="Chapter 9. Deep Reinforcement Learning">Chapter 9</a>, <span class="emphasis"><em>Deep Reinforcement Learning</em></span>. The next state value is discounted by <span class="inlinemediaobject"><img src="graphics/B08956_10_075.jpg" alt="Actor-Critic method"/></span> Estimating distant future rewards is difficult. Therefore, our estimate is based only on the immediate future, <span class="inlinemediaobject"><img src="graphics/B08956_10_076.jpg" alt="Actor-Critic method"/></span>. This has been known as <span class="emphasis"><em>bootstrapping</em></span> technique. The bootstrapping technique and the dependence on state representation in <span class="emphasis"><em>Equation 10.4.1</em></span> often accelerates learning and reduces variance. From <span class="emphasis"><em>Equation 10.4.1</em></span>, we notice that the value network evaluates the current state, <span class="inlinemediaobject"><img src="graphics/B08956_10_077.jpg" alt="Actor-Critic method"/></span>, which is due to the previous action, <span class="inlinemediaobject"><img src="graphics/B08956_10_078.jpg" alt="Actor-Critic method"/></span>, of the policy network. Meanwhile, the policy gradient is based on the current action, <span class="inlinemediaobject"><img src="graphics/B08956_10_079.jpg" alt="Actor-Critic method"/></span>. In a sense, the evaluation is delayed by one step.</p><p>
<span class="emphasis"><em>Algorithm</em></span> <span class="emphasis"><em>10.4.1</em></span> summarizes the Actor-Critic method [2]. Apart from the evaluation of the state value which is used to train both the policy and value networks, the training is done online. At every step, both networks are trained. This is unlike REINFORCE and REINFORCE with baseline where the agent completes an episode before the training is performed. The value network is consulted twice. Firstly, during the value estimate of the current state and secondly for the value of the next state. Both values are used in the computation of gradients. <span class="emphasis"><em>Figure 10.4.1</em></span> shows the Actor-Critic network. We will implement the Actor-Critic method in Keras at the end of this chapter.</p><p>
<span class="strong"><strong>Algorithm 10.4.1 Actor-Critic</strong></span>
</p><p>
<span class="emphasis"><em>Require</em></span>: A differentiable<a id="id435" class="indexterm"/> parameterized target policy network, <span class="inlinemediaobject"><img src="graphics/B08956_10_080.jpg" alt="Actor-Critic method"/></span>.</p><p>
<span class="emphasis"><em>Require</em></span>: A differentiable parameterized value network, <span class="inlinemediaobject"><img src="graphics/B08956_10_081.jpg" alt="Actor-Critic method"/></span>.</p><p>
<span class="emphasis"><em>Require</em></span>: Discount factor, <span class="inlinemediaobject"><img src="graphics/B08956_10_007.jpg" alt="Actor-Critic method"/></span>, the learning rate <span class="inlinemediaobject"><img src="graphics/B08956_10_033.jpg" alt="Actor-Critic method"/></span> for the performance gradient, and the learning rate for the value gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_056.jpg" alt="Actor-Critic method"/></span>.</p><p>
<span class="emphasis"><em>Require</em></span>: <span class="inlinemediaobject"><img src="graphics/B08956_10_038.jpg" alt="Actor-Critic method"/></span>, initial policy network parameters (for example, <span class="inlinemediaobject"><img src="graphics/B08956_10_039.jpg" alt="Actor-Critic method"/></span>). <span class="inlinemediaobject"><img src="graphics/B08956_10_082.jpg" alt="Actor-Critic method"/></span>, initial value network parameters (for example, <span class="inlinemediaobject"><img src="graphics/B08956_10_083.jpg" alt="Actor-Critic method"/></span>).</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Repeat</li><li class="listitem">
    for steps <span class="inlinemediaobject"><img src="graphics/B08956_10_042.jpg" alt="Actor-Critic method"/></span> do
</li><li class="listitem">
       Sample an action <span class="inlinemediaobject"><img src="graphics/B08956_10_087.jpg" alt="Actor-Critic method"/></span></li><li class="listitem">
       Execute the action and observe reward <span class="inlinemediaobject"><img src="graphics/B08956_10_070.jpg" alt="Actor-Critic method"/></span> and next state <span class="inlinemediaobject"><img src="graphics/B08956_10_088.jpg" alt="Actor-Critic method"/></span></li><li class="listitem">
       Evaluate state value estimate, <span class="inlinemediaobject"><img src="graphics/B08956_10_089.jpg" alt="Actor-Critic method"/></span></li><li class="listitem">
       Compute discounted value gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_090.jpg" alt="Actor-Critic method"/></span></li><li class="listitem">
        Perform gradient ascent, <span class="inlinemediaobject"><img src="graphics/B08956_10_091.jpg" alt="Actor-Critic method"/></span></li><li class="listitem">
       Compute discounted performance gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_092.jpg" alt="Actor-Critic method"/></span></li><li class="listitem">
        Perform gradient ascent, <span class="inlinemediaobject"><img src="graphics/B08956_10_093.jpg" alt="Actor-Critic method"/></span></li><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B08956_10_094.jpg" alt="Actor-Critic method"/></span></li></ol></div><div class="mediaobject"><img src="graphics/B08956_10_03.jpg" alt="Actor-Critic method"/><div class="caption"><p>Figure 10.4.1: Actor-critic network</p></div></div></div><div class="section" title="Advantage Actor-Critic (A2C) method"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec45"/>Advantage Actor-Critic (A2C) method</h2></div></div></div><p>In the Actor-Critic method from the previous section, the objective is for the value function to evaluate<a id="id436" class="indexterm"/> the state value correctly. There are other techniques to train the value network. One obvious<a id="id437" class="indexterm"/> method is to use <span class="strong"><strong>MSE</strong></span> (<span class="strong"><strong>mean squared error</strong></span>) in the value function optimization, similar to the algorithm in Q-Learning. The new<a id="id438" class="indexterm"/> value gradient is equal to the partial derivative of the MSE between the return, <span class="inlinemediaobject"><img src="graphics/B08956_10_095.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>, and the state value:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_61.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>     (Equation 10.5.1)</p><p>As <span class="inlinemediaobject"><img src="graphics/B08956_10_62.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>, the value network prediction gets more accurate. We call this variation of the Actor-Critic algorithm as A2C. A2C is a single threaded or synchronous version of the <span class="strong"><strong>Asynchronous Advantage Actor-Critic</strong></span> (<span class="strong"><strong>A3C</strong></span>) by [3]. The quantity <span class="inlinemediaobject"><img src="graphics/B08956_10_63.jpg" alt="Advantage Actor-Critic (A2C) method"/></span> is called <span class="emphasis"><em>Advantage</em></span>.</p><p>
<span class="emphasis"><em>Algorithm</em></span> <span class="emphasis"><em>10.5.1</em></span> summarizes<a id="id439" class="indexterm"/> the A2C method. There are some differences between A2C and Actor-Critic. Actor-Critic is online or<a id="id440" class="indexterm"/> is trained on per experience sample. A2C is similar to Monte Carlo algorithms REINFORCE and REINFORCE with baseline. It is trained after one episode has been completed. Actor-Critic is trained from the first state to the last state. A2C training starts from the last state and ends on the first state. In addition, the A2C policy and value gradients are no longer discounted by <span class="inlinemediaobject"><img src="graphics/B08956_10_032.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>.</p><p>The corresponding network for A2C is similar to <span class="emphasis"><em>Figure 10.4.1</em></span> since we only changed the method of gradient computation. To encourage agent exploration during training, A3C algorithm [3] suggests that the gradient of the weighted entropy value of the policy function is added to the gradient function, <span class="inlinemediaobject"><img src="graphics/B08956_10_69.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>. Recall that entropy is a measure of information or uncertainty of an event.</p><p>
<span class="strong"><strong>Algorithm 10.5.1 Advantage Actor-Critic (A2C)</strong></span>
</p><p>
<span class="emphasis"><em>Require</em></span>: A differentiable<a id="id441" class="indexterm"/> parameterized target policy network, <span class="inlinemediaobject"><img src="graphics/B08956_10_015.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>.</p><p>
<span class="emphasis"><em>Require</em></span>: A differentiable parameterized value network, <span class="inlinemediaobject"><img src="graphics/B08956_10_096.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>.</p><p>
<span class="emphasis"><em>Require</em></span>: Discount factor, <span class="inlinemediaobject"><img src="graphics/B08956_10_035.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>, the learning rate <span class="inlinemediaobject"><img src="graphics/B08956_10_033.jpg" alt="Advantage Actor-Critic (A2C) method"/></span> for the performance gradient, the learning rate for the value gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_056.jpg" alt="Advantage Actor-Critic (A2C) method"/></span> and entropy weight, <span class="inlinemediaobject"><img src="graphics/B08956_10_097.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>.</p><p>
<span class="emphasis"><em>Require</em></span>: <span class="inlinemediaobject"><img src="graphics/B08956_10_038.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>, initial policy network parameters (for example, <span class="inlinemediaobject"><img src="graphics/B08956_10_039.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>). <span class="inlinemediaobject"><img src="graphics/B08956_10_098.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>, initial value network parameters (for example, <span class="inlinemediaobject"><img src="graphics/B08956_10_099.jpg" alt="Advantage Actor-Critic (A2C) method"/></span>).</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Repeat</li><li class="listitem">   Generate an episode <span class="inlinemediaobject"><img src="graphics/B08956_10_061.jpg" alt="Advantage Actor-Critic (A2C) method"/></span> by following <span class="inlinemediaobject"><img src="graphics/B08956_10_015.jpg" alt="Advantage Actor-Critic (A2C) method"/></span></li><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B08956_10_100.jpg" alt="Advantage Actor-Critic (A2C) method"/></span></li><li class="listitem">
    for steps <span class="inlinemediaobject"><img src="graphics/B08956_10_101.jpg" alt="Advantage Actor-Critic (A2C) method"/></span> do
</li><li class="listitem">
        Compute return, <span class="inlinemediaobject"><img src="graphics/B08956_10_66.jpg" alt="Advantage Actor-Critic (A2C) method"/></span></li><li class="listitem">
        Compute value gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_102.jpg" alt="Advantage Actor-Critic (A2C) method"/></span></li><li class="listitem">
        Accumulate gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_103.jpg" alt="Advantage Actor-Critic (A2C) method"/></span></li><li class="listitem">
        Compute performance gradient, <span class="inlinemediaobject"><img src="graphics/B08956_10_104.jpg" alt="Advantage Actor-Critic (A2C) method"/></span></li><li class="listitem">
        Perform gradient ascent, <span class="inlinemediaobject"><img src="graphics/B08956_10_105.jpg" alt="Advantage Actor-Critic (A2C) method"/></span></li></ol></div></div><div class="section" title="Policy Gradient methods with Keras"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec46"/>Policy Gradient methods with Keras</h2></div></div></div><p>The four policy<a id="id442" class="indexterm"/> gradient methods (<span class="emphasis"><em>Algorithms</em></span> <span class="emphasis"><em>10.2.1</em></span> to <span class="emphasis"><em>10.5.1</em></span>) discussed in the previous sections use identical policy and value network models. The policy<a id="id443" class="indexterm"/> and value networks in <span class="emphasis"><em>Figures 10.2.1</em></span> to <span class="emphasis"><em>10.4.1</em></span> have the same configurations. The four policy gradient methods differ only in:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Performance and value gradients formula</li><li class="listitem" style="list-style-type: disc">Training strategy</li></ul></div><p>In this section, we discuss the implementation in Keras of <span class="emphasis"><em>Algorithms</em></span> <span class="emphasis"><em>10.2.1</em></span> to <span class="emphasis"><em>10.5.1</em></span> in one code, since they share many common routines.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>The complete code can be found on <a class="ulink" href="https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras">https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras</a>.</p></div></div><p>But before discussing the implementation, let's briefly explore the training environment.</p><div class="mediaobject"><img src="graphics/B08956_10_04.jpg" alt="Policy Gradient methods with Keras"/><div class="caption"><p>Figure 10.6.1 MountainCarContinuous-v0 OpenAI gym environment</p></div></div><p>Unlike Q-Learning, policy gradient methods are applicable to both discrete and continuous action spaces. In our example, we'll demonstrate the four policy gradient methods on a continuous action<a id="id444" class="indexterm"/> space case example, <code class="literal">MountainCarContinuous-v0</code> of OpenAI gym, <a class="ulink" href="https://gym.openai.com">https://gym.openai.com</a>. In case you are not familiar<a id="id445" class="indexterm"/> with OpenAI gym, please see <a class="link" href="ch09.html" title="Chapter 9. Deep Reinforcement Learning">Chapter 9</a>, <span class="emphasis"><em>Deep Reinforcement Learning</em></span>.</p><p>A snapshot of <code class="literal">MountainCarContinuous-v0</code> 2D environment is shown in <span class="emphasis"><em>Figure 10.6.1</em></span>. In this 2D environment, a car with a not too powerful engine is between two mountains. In order to reach the yellow flag on top of the mountain on the right, it must drive back and forth to gain enough momentum. The more energy (that is, the greater the absolute value of action) that is applied to the car, the smaller (or, the more negative) is the reward. The reward is always negative, and it is only positive upon reaching the flag. In that case, the car receives a reward of +100. However, every action is penalized by the following code:</p><div class="informalexample"><pre class="programlisting">reward-= math.pow(action[0],2)*0.1</pre></div><p>The continuous range of valid action values is [-1.0, 1.0]. Beyond the range, the action is clipped to its minimum or maximum value. Therefore, it makes no sense to apply an action value that is greater than 1.0 or less than -1.0. The <code class="literal">MountainCarContinuous-v0</code> environment state has two elements: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Car position</li><li class="listitem" style="list-style-type: disc">Car velocity</li></ul></div><p>The state is converted to state features by an encoder. The predicted action is the output of the policy model given the state. The output of the value function is the predicted value of the state:</p><div class="mediaobject"><img src="graphics/B08956_10_05.jpg" alt="Policy Gradient methods with Keras"/><div class="caption"><p>Figure 10.6.2 Autoencoder model</p></div></div><div class="mediaobject"><img src="graphics/B08956_10_06.jpg" alt="Policy Gradient methods with Keras"/><div class="caption"><p>Figure 10.6.3 Encoder model</p></div></div><div class="mediaobject"><img src="graphics/B08956_10_07.jpg" alt="Policy Gradient methods with Keras"/><div class="caption"><p>Figure 10.6.4 Decoder model</p></div></div><p>As shown in <span class="emphasis"><em>Figures 10.2.1</em></span> to <span class="emphasis"><em>10.4.1</em></span>, before<a id="id446" class="indexterm"/> building the<a id="id447" class="indexterm"/> policy and value networks, we must first create a function that converts the state to features. This function is implemented by an encoder<a id="id448" class="indexterm"/> of an autoencoder similar to the ones implemented in <a class="link" href="ch03.html" title="Chapter 3. Autoencoders">Chapter 3</a>, <span class="emphasis"><em>Autoencoders</em></span>. <span class="emphasis"><em>Figure 10.6.2</em></span> shows an autoencoder made of an encoder and a decoder. In <span class="emphasis"><em>Figure 10.6.3</em></span>, the encoder is an MLP made of <code class="literal">Input(2)-Dense(256, activation='relu')-Dense(128, activation='relu')-Dense(32)</code>. Every state is converted into a 32-dim feature vector. In <span class="emphasis"><em>Figure</em></span>
<span class="emphasis"><em> 10.6.4</em></span>, the decoder is also<a id="id449" class="indexterm"/> an MLP but made of <code class="literal">Input(32)-Dense(128, activation='relu')-Dense(256, activation='relu')-Dense(2)</code>. The autoencoder is trained for 10 epochs with an <span class="strong"><strong>MSE</strong></span>, loss function, and Keras default Adam optimizer. We sampled 220,000 random states for the train and test dataset and applied 200k/20k train-test split. After training, the encoder weights are saved for future use in the policy and value networks training. <span class="emphasis"><em>Listing 10.6.1</em></span> shows the methods for building and training the autoencoder.</p><p>Listing 10.6.1, <code class="literal">policygradient-car-10.1.1.py</code> shows us the methods for building and training the autoencoder:</p><div class="informalexample"><pre class="programlisting"># autoencoder to convert states into features
def build_autoencoder(self):
    # first build the encoder model
    inputs = Input(shape=(self.state_dim, ), name='state')
    feature_size = 32
    x = Dense(256, activation='relu')(inputs)
    x = Dense(128, activation='relu')(x)
    feature = Dense(feature_size, name='feature_vector')(x)

    # instantiate encoder model
   self.encoder = Model(inputs, feature, name='encoder')
    self.encoder.summary()
    plot_model(self.encoder, to_file='encoder.png', show_shapes=True)

    # build the decoder model
    feature_inputs = Input(shape=(feature_size,), name='decoder_input')
    x = Dense(128, activation='relu')(feature_inputs)
    x = Dense(256, activation='relu')(x)
    outputs = Dense(self.state_dim, activation='linear')(x)

    # instantiate decoder model
    self.decoder = Model(feature_inputs, outputs, name='decoder')
    self.decoder.summary()
    plot_model(self.decoder, to_file='decoder.png', show_shapes=True)

    # autoencoder = encoder + decoder
    # instantiate autoencoder model
    self.autoencoder = Model(inputs, self.decoder(self.encoder(inputs)), name='autoencoder')
    self.autoencoder.summary()
    plot_model(self.autoencoder, to_file='autoencoder.png', show_shapes=True)

    # Mean Square Error (MSE) loss function, Adam optimizer
    self.autoencoder.compile(loss='mse', optimizer='adam')


# training the autoencoder using randomly sampled
# states from the environment
def train_autoencoder(self, x_train, x_test):
    # train the autoencoder
    batch_size = 32
    self.autoencoder.fit(x_train,
                         x_train,
                         validation_data=(x_test, x_test),
                         epochs=10,
                         batch_size=batch_size)</pre></div><div class="mediaobject"><img src="graphics/B08956_10_08.jpg" alt="Policy Gradient methods with Keras"/><div class="caption"><p>Figure 10.6.5: Policy model (actor model)</p></div></div><p>Given the <code class="literal">MountainCarContinuous-v0</code> environment, the policy (or actor) model predicts the action<a id="id450" class="indexterm"/> that must be applied on the car. As discussed in<a id="id451" class="indexterm"/> the first section of this chapter on policy gradient methods, for continuous action spaces the policy model samples an action from a Gaussian distribution, <span class="inlinemediaobject"><img src="graphics/B08956_10_106.jpg" alt="Policy Gradient methods with Keras"/></span>. In Keras, this is implemented as:</p><div class="informalexample"><pre class="programlisting">    # given mean and stddev, sample an action, clip and return
    # we assume Gaussian distribution of probability of selecting an
    # action given a state
    def action(self, args):
        mean, stddev = args
        dist = tf.distributions.Normal(loc=mean, scale=stddev)
        action = dist.sample(1)
        action = K.clip(action,
                        self.env.action_space.low[0],
                        self.env.action_space.high[0])
        return action</pre></div><p>The action is clipped between its minimum and maximum possible values.</p><p>The role of the policy network is to predict the mean and standard deviation of the Gaussian distribution. <span class="emphasis"><em>Figure 10.6.5</em></span> shows the policy network to model <span class="inlinemediaobject"><img src="graphics/B08956_10_015.jpg" alt="Policy Gradient methods with Keras"/></span>. It's worth noting that the encoder model has pretrained weights that are frozen. Only the mean and standard deviation weights receive the performance gradient updates.</p><p>The policy network is basically the implementation of <span class="emphasis"><em>Equations</em></span> <span class="emphasis"><em>10.1.4</em></span> and <span class="emphasis"><em>10.1.5</em></span> that are repeated here for convenience:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_108.jpg" alt="Policy Gradient methods with Keras"/></span>     (Equation 10.1.4)</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_109.jpg" alt="Policy Gradient methods with Keras"/></span>     (Equation 10.1.5)</p><p>where <span class="inlinemediaobject"><img src="graphics/B08956_10_023.jpg" alt="Policy Gradient methods with Keras"/></span> is the encoder, <span class="inlinemediaobject"><img src="graphics/B08956_10_110.jpg" alt="Policy Gradient methods with Keras"/></span> are the weights of the mean's <code class="literal">Dense(1)</code> layer, and <span class="inlinemediaobject"><img src="graphics/B08956_10_111.jpg" alt="Policy Gradient methods with Keras"/></span> are the weights of the standard deviation's <code class="literal">Dense(1)</code> layer. We used a modified <span class="emphasis"><em>softplus</em></span> function, <span class="inlinemediaobject"><img src="graphics/B08956_10_112.jpg" alt="Policy Gradient methods with Keras"/></span>, to avoid zero standard deviation:</p><div class="informalexample"><pre class="programlisting"># some implementations use a modified softplus to ensure that
# the stddev is never zero
def softplusk(x):
    return K.softplus(x) + 1e-10</pre></div><p>The policy model builder is shown in the following listing. Also included in this listing are the log probability, entropy, and value models which we will discuss next.</p><p>Listing 10.6.2, <code class="literal">policygradient-car-10.1.1.py</code> shows<a id="id452" class="indexterm"/> us the method<a id="id453" class="indexterm"/> for building the policy (actor), <code class="literal">logp</code>, entropy, and value models from the encoded state features:</p><div class="informalexample"><pre class="programlisting">def build_actor_critic(self):
    inputs = Input(shape=(self.state_dim, ), name='state')
    self.encoder.trainable = False
    x = self.encoder(inputs)
    mean = Dense(1,
                 activation='linear',
                 kernel_initializer='zero',
                 name='mean')(x)
    stddev = Dense(1,
                   kernel_initializer='zero',
                   name='stddev')(x)
    # use of softplusk avoids stddev = 0
    stddev = Activation('softplusk', name='softplus')(stddev)
    action = Lambda(self.action,
                    output_shape=(1,),
                    name='action')([mean, stddev])
    self.actor_model = Model(inputs, action, name='action')
    self.actor_model.summary()
    plot_model(self.actor_model, to_file='actor_model.png', show_shapes=True)

    logp = Lambda(self.logp,
                  output_shape=(1,),
                  name='logp')([mean, stddev, action])
    self.logp_model = Model(inputs, logp, name='logp')
    self.logp_model.summary()
    plot_model(self.logp_model, to_file='logp_model.png', show_shapes=True)

    entropy = Lambda(self.entropy,
                     output_shape=(1,),
                     name='entropy')([mean, stddev])
    self.entropy_model = Model(inputs, entropy, name='entropy')
    self.entropy_model.summary()
    plot_model(self.entropy_model, to_file='entropy_model.png', show_shapes=True)
    value = Dense(1,
                  activation='linear',
                  kernel_initializer='zero',
                  name='value')(x)
    self.value_model = Model(inputs, value, name='value')
    self.value_model.summary()</pre></div><div class="mediaobject"><img src="graphics/B08956_10_09.jpg" alt="Policy Gradient methods with Keras"/><div class="caption"><p>Figure 10.6.6: Gaussian log probability model of the policy</p></div></div><div class="mediaobject"><img src="graphics/B08956_10_10.jpg" alt="Policy Gradient methods with Keras"/><div class="caption"><p>Figure 10.6.7: Entropy model</p></div></div><p>Apart from<a id="id454" class="indexterm"/> the policy network, <span class="inlinemediaobject"><img src="graphics/B08956_10_015.jpg" alt="Policy Gradient methods with Keras"/></span>, we must also<a id="id455" class="indexterm"/> have the action log probability (<code class="literal">logp</code>) network <span class="inlinemediaobject"><img src="graphics/B08956_10_70.jpg" alt="Policy Gradient methods with Keras"/></span> since this is actually what calculates the gradient. As shown in <span class="emphasis"><em>Figure 10.6.6</em></span>, the <code class="literal">logp</code> network is simply the policy network where an additional <code class="literal">Lambda(1)</code> layer computes the log probability of the Gaussian distribution given action, mean, and standard deviation. The <code class="literal">logp</code> network and actor (policy) model share the same set of parameters. The <code class="literal">Lambda</code> layer does not have any parameter. It is implemented by the following function:</p><div class="informalexample"><pre class="programlisting">    # given mean, stddev, and action compute
    # the log probability of the Gaussian distribution
    def logp(self, args):
        mean, stddev, action = args
        dist = tf.distributions.Normal(loc=mean, scale=stddev)
        logp = dist.log_prob(action)
        return logp</pre></div><p>Training the <code class="literal">logp</code> network trains the actor model as well. In the training methods that are discussed in this section, only the <code class="literal">logp</code> network is trained.</p><p>As shown in <span class="emphasis"><em>Figure 10.6.7</em></span>, the entropy<a id="id456" class="indexterm"/> model also shares<a id="id457" class="indexterm"/> parameters with the policy network. The output <code class="literal">Lambda(1)</code> layer computes the entropy of the Gaussian distribution given the mean and standard deviation using the following function:</p><div class="informalexample"><pre class="programlisting">    # given the mean and stddev compute the Gaussian dist entropy
    def entropy(self, args):
        mean, stddev = args
        dist = tf.distributions.Normal(loc=mean, scale=stddev)
        entropy = dist.entropy()
        return entropy</pre></div><p>The entropy model is only used by the A2C method:</p><div class="mediaobject"><img src="graphics/B08956_10_11.jpg" alt="Policy Gradient methods with Keras"/><div class="caption"><p>Figure 10.6.8: A value model</p></div></div><p>Preceding figure shows the value model. The model also uses the pre-trained encoder with frozen weights to implement following equation which is repeated here for convenience:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_113.jpg" alt="Policy Gradient methods with Keras"/></span>      (Equation 10.3.2)</p><p><span class="inlinemediaobject"><img src="graphics/B08956_10_114.jpg" alt="Policy Gradient methods with Keras"/></span> are the weights of the <code class="literal">Dense(1)</code> layer, the only layer that receives value gradient updates. <span class="emphasis"><em>Figure 10.6.8</em></span> represents <span class="inlinemediaobject"><img src="graphics/B08956_10_096.jpg" alt="Policy Gradient methods with Keras"/></span> in <span class="emphasis"><em>Algorithms 10.3.1</em></span> to <span class="emphasis"><em>10.5.1</em></span>. The value model can be built in a few lines:</p><div class="informalexample"><pre class="programlisting">inputs = Input(shape=(self.state_dim, ), name='state')
self.encoder.trainable = False
x = self.encoder(inputs)

value = Dense(1,
              activation='linear',
              kernel_initializer='zero',
              name='value')(x)
self.value_model = Model(inputs, value, name='value')</pre></div><p>These lines are also implemented in method <code class="literal">build_actor_critic()</code>, which is shown in <span class="emphasis"><em>Listing 10.6.2</em></span>.</p><p>After building<a id="id458" class="indexterm"/> the network models, the next step is training. In <span class="emphasis"><em>Algorithms</em></span> <span class="emphasis"><em>10.2.1</em></span> to <span class="emphasis"><em>10.5.1</em></span>, we perform objective function maximization by gradient ascent. In Keras, we perform loss function minimization by gradient descent. The loss<a id="id459" class="indexterm"/> function is simply the negative of the objective function being maximized. The gradient descent is the negative of gradient ascent. <span class="emphasis"><em>Listing 10.6.3</em></span> shows the <code class="literal">logp</code> and value loss functions.</p><p>We can take advantage of the common structure of the loss functions to unify the loss functions in <span class="emphasis"><em>Algorithms</em></span> <span class="emphasis"><em>10.2.1</em></span> to <span class="emphasis"><em>10.5.1</em></span>. The performance and value gradients differ only in their constant factors. All performance gradients have the common term, <span class="inlinemediaobject"><img src="graphics/B08956_10_74.jpg" alt="Policy Gradient methods with Keras"/></span>. This is represented by <code class="literal">y_pred</code> in the policy log probability loss function, <code class="literal">logp_loss()</code>. The factor to the common term, <span class="inlinemediaobject"><img src="graphics/B08956_10_74.jpg" alt="Policy Gradient methods with Keras"/></span>, depends on which algorithm and is implemented as <code class="literal">y_true</code>. Table <span class="emphasis"><em>10.6.1</em></span> shows the values of <code class="literal">y_true</code>. The remaining term is the weighted gradient of entropy, <span class="inlinemediaobject"><img src="graphics/B08956_10_69.jpg" alt="Policy Gradient methods with Keras"/></span>. It is implemented as the product of <code class="literal">beta</code> and <code class="literal">entropy</code> in the <code class="literal">logp_loss()</code> function. Only A2C uses this term, so by default, <code class="literal">beta=0.0</code>. For A2C, <code class="literal">beta=0.9</code>.</p><p>Listing 10.6.3, <code class="literal">policygradient-car-10.1.1.py</code>: The loss functions of <code class="literal">logp</code> and value networks.</p><div class="informalexample"><pre class="programlisting"># logp loss, the 3rd and 4th variables (entropy and beta) are needed
# by A2C so we have a different loss function structure
def logp_loss(self, entropy, beta=0.0):
    def loss(y_true, y_pred):
        return -K.mean((y_pred * y_true) + (beta * entropy), axis=-1)

    return loss


# typical loss function structure that accepts 2 arguments only
# this will be used by value loss of all methods except A2C
def value_loss(self, y_true, y_pred):
    return -K.mean(y_pred * y_true, axis=-1)</pre></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>
<code class="literal">y_true of logp_loss</code>
</p>
</th><th style="text-align: left" valign="bottom">
<p>
<code class="literal">y_true of value_loss</code>
</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>10.2.1 REINFORCE</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_10_115.jpg" alt="Policy Gradient methods with Keras"/></div>
</td><td style="text-align: left" valign="top">
<p>Not applicable</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10.3.1 REINFORCE with baseline</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_10_117.jpg" alt="Policy Gradient methods with Keras"/></div>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_10_117.jpg" alt="Policy Gradient methods with Keras"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10.4.1 Actor-Critic</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_10_117.jpg" alt="Policy Gradient methods with Keras"/></div>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_10_117.jpg" alt="Policy Gradient methods with Keras"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10.5.1 A2C</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_10_63.jpg" alt="Policy Gradient methods with Keras"/></div>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_10_095.jpg" alt="Policy Gradient methods with Keras"/></div>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p>Table 10.6.1: y_true value of logp_loss and value_loss</p></blockquote></div><p>Similarly, the value<a id="id460" class="indexterm"/> loss functions of <span class="emphasis"><em>Algorithms</em></span> <span class="emphasis"><em>10.3.1</em></span> and <span class="emphasis"><em>10.4.1</em></span> have the<a id="id461" class="indexterm"/> same structure. The value loss functions are implemented in Keras as <code class="literal">value_loss()</code> as shown in <span class="emphasis"><em>Listing 10.6.3</em></span>. The common gradient factor <span class="inlinemediaobject"><img src="graphics/B08956_10_75.jpg" alt="Policy Gradient methods with Keras"/></span> is represented by the tensor <code class="literal">y_pred</code>. The remaining factor is represented by <code class="literal">y_true</code>. The <code class="literal">y_true</code> values are also shown in <span class="emphasis"><em>Table 10.6.1</em></span>. REINFORCE does not use a value function. A2C uses the MSE loss function to learn the value function. In A2C, <code class="literal">y_true</code> represents the target value or ground truth.</p><p>Listing 10.6.4, <code class="literal">policygradient-car-10.1.1.py</code> shows us, REINFORCE, REINFORCE with baseline, and A2C are trained by episode. The appropriate return is computed first before calling the main train routine in <span class="emphasis"><em>Listing 10.6.5</em></span>:</p><div class="informalexample"><pre class="programlisting"># train by episode (REINFORCE, REINFORCE with baseline
# and A2C use this routine to prepare the dataset before
# the step by step training)
def train_by_episode(self, last_value=0):
    if self.args.actor_critic:
        print("Actor-Critic must be trained per step")
        return
    elif self.args.a2c:
        # implements A2C training from the last state
        # to the first state
        # discount factor
        gamma = 0.95
        r = last_value
        # the memory is visited in reverse as shown
        # in Algorithm 10.5.1
        for item in self.memory[::-1]:
            [step, state, next_state, reward, done] = item
            # compute the return
            r = reward + gamma*r
            item = [step, state, next_state, r, done]
            # train per step
            # a2c reward has been discounted
            self.train(item)

        return

    # only REINFORCE and REINFORCE with baseline
    # use the ff codes
    # convert the rewards to returns
    rewards = []
    gamma = 0.99
    for item in self.memory:
       [_, _, _, reward, _] = item
        rewards.append(reward)
    
    # compute return per step
    # return is the sum of rewards from t til end of episode
    # return replaces reward in the list
    for i in range(len(rewards)):
        reward = rewards[i:]
        horizon = len(reward)
        discount =  [math.pow(gamma, t) for t in range(horizon)]
        return_ = np.dot(reward, discount)
        self.memory[i][3] = return_

    # train every step
    for item in self.memory:
        self.train(item, gamma=gamma)</pre></div><p>Listing 10.6.5, <code class="literal">policygradient-car-10.1.1.py</code> shows us the main <code class="literal">train</code> routine used by all the policy<a id="id462" class="indexterm"/> gradient algorithms. Actor-critic calls<a id="id463" class="indexterm"/> this every experience sample while the rest call this during train per episode routine in <span class="emphasis"><em>Listing 10.6.4</em></span>:</p><div class="informalexample"><pre class="programlisting"># main routine for training as used by all 4 policy gradient
# methods
def train(self, item, gamma=1.0):
    [step, state, next_state, reward, done] = item

    # must save state for entropy computation
    self.state = state

    discount_factor = gamma**step

    # reinforce-baseline: delta = return - value
    # actor-critic: delta = reward - value + discounted_next_value
    # a2c: delta = discounted_reward - value
    delta = reward - self.value(state)[0]

    # only REINFORCE does not use a critic (value network)
    critic = False
    if self.args.baseline:
        critic = True
    elif self.args.actor_critic:
        # since this function is called by Actor-Critic
        # directly, evaluate the value function here
        critic = True
        if not done:
            next_value = self.value(next_state)[0]
            # add  the discounted next value
            delta += gamma*next_value
    elif self.args.a2c:
        critic = True
    else:
        delta = reward

    # apply the discount factor as shown in Algortihms
    # 10.2.1, 10.3.1 and 10.4.1
    discounted_delta = delta * discount_factor
    discounted_delta = np.reshape(discounted_delta, [-1, 1]) 
    verbose = 1 if done else 0

    # train the logp model (implies training of actor model
    # as well) since they share exactly the same set of
    # parameters
    self.logp_model.fit(np.array(state),
                        discounted_delta,
                        batch_size=1,
                        epochs=1,
                        verbose=verbose)
        
    # in A2C, the target value is the return (reward
    # replaced by return in the train_by_episode function)
    if self.args.a2c:
        discounted_delta = reward
        discounted_delta = np.reshape(discounted_delta, [-1, 1])

    # train the value network (critic)
    if critic:
        self.value_model.fit(np.array(state),
                             discounted_delta,
                             batch_size=1,
                             epochs=1,
                             verbose=verbose)</pre></div><p>With all network models and loss functions in place, the last part is the training strategy, which is different for<a id="id464" class="indexterm"/> each algorithm. Two train functions are<a id="id465" class="indexterm"/> used as shown in <span class="emphasis"><em>Listings 10.6.4</em></span> and <span class="emphasis"><em>10.6.5</em></span>. <span class="emphasis"><em>Algorithms</em></span> <span class="emphasis"><em>10.2.1</em></span>, <span class="emphasis"><em>10.3.1</em></span>, and <span class="emphasis"><em>10.5.1</em></span> wait for a complete episode to finish before training, so it runs both <code class="literal">train_by_episode()</code> and <code class="literal">train()</code>. The complete episode is saved in <code class="literal">self.memory</code>. Actor-Critic <span class="emphasis"><em>Algorithm</em></span> <span class="emphasis"><em>10.4.1</em></span> trains per step and only runs <code class="literal">train()</code>.</p><p>Each algorithm processes its episode trajectory in a different way.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>
<code class="literal">y_true</code> formula</p>
</th><th style="text-align: left" valign="bottom">
<p>
<code class="literal">y_true</code> in Keras</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>10.2.1 REINFORCE</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_10_115.jpg" alt="Policy Gradient methods with Keras"/></div>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">reward * discount_factor</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10.3.1 REINFORCE with baseline</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_10_117.jpg" alt="Policy Gradient methods with Keras"/></div>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(reward - self.value(state)[0]) * discount_factor</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10.4.1 Actor-Critic</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_10_117.jpg" alt="Policy Gradient methods with Keras"/></div>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(reward - self.value(state)[0] +  gamma*next_value) * discount_factor</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10.5.1 A2C</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_10_63.jpg" alt="Policy Gradient methods with Keras"/></div>
<p>and <span class="inlinemediaobject"><img src="graphics/B08956_10_095.jpg" alt="Policy Gradient methods with Keras"/></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">(reward - self.value(state)[0])</code>
</p>
<p>and</p>
<p>
<code class="literal">reward</code>
</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p>Table 10.6.2: y_true value in Table 10.6.1</p></blockquote></div><p>For REINFORCE methods and A2C, the <code class="literal">reward</code> is actually the return as computed in <code class="literal">train_by_episode()</code>. <code class="literal">discount_factor = gamma**step</code>.</p><p>Both REINFORCE methods compute the return, <span class="inlinemediaobject"><img src="graphics/B08956_10_37.jpg" alt="Policy Gradient methods with Keras"/></span>, by replacing the reward value in the memory as:</p><div class="informalexample"><pre class="programlisting">    # only REINFORCE and REINFORCE with baseline
    # use the ff codes
    # convert the rewards to returns
    rewards = []
    gamma = 0.99
    for item in self.memory:
        [_, _, _, reward, _] = item
        rewards.append(reward)
    
    # compute return per step
    # return is the sum of rewards from t til end of episode
    # return replaces reward in the list
    for i in range(len(rewards)):
        reward = rewards[i:]
        horizon = len(reward)
        discount =  [math.pow(gamma, t) for t in range(horizon)]
        return_ = np.dot(reward, discount)
        self.memory[i][3] = return_</pre></div><p>This then trains<a id="id466" class="indexterm"/> the policy (actor) and value models (with baseline only) for each step beginning with the first step.</p><p>The training<a id="id467" class="indexterm"/> strategy of A2C is different in the sense that it computes gradients from the last step to the first step. Hence, the return accumulates beginning from the last step reward or the last next state value:</p><div class="informalexample"><pre class="programlisting">        # the memory is visited in reverse as shown
        # in Algorithm 10.5.1
        for item in self.memory[::-1]:
            [step, state, next_state, reward, done] = item
            # compute the return
            r = reward + gamma*r
            item = [step, state, next_state, r, done]
            # train per step
            # a2c reward has been discounted
            self.train(item)</pre></div><p>The <code class="literal">reward</code> variable in the list is also replaced by return. It is initialized by <code class="literal">reward</code> if the terminal state is reached (that is, the car touches the flag) or the next state value for non-terminal states:</p><div class="informalexample"><pre class="programlisting">v = 0 if reward &gt; 0 else agent.value(next_state)[0]</pre></div><p>In the Keras implementation, all the routines that we mentioned are implemented as methods in the <code class="literal">PolicyAgent</code> class. The role of the <code class="literal">PolicyAgent</code> is to represent the agent implementing policy gradient methods including building and training the network models and predicting the action, log probability, entropy, and state value.</p><p>Following listing shows how one episode unfolds when the agent executes and trains the policy and value models. The <code class="literal">for</code> loop is executed for 1000 episodes. An episode terminates upon reaching 1000 steps or when the car touches the flag. The agent executes the action predicted by the policy at every step. After each episode or step, the training routine is called.</p><p>Listing 10.6.6, <code class="literal">policygradient-car-10.1.1.py</code>: The agent runs for 1000 episodes to execute the action predicted by the policy at every step and perform training:</p><div class="informalexample"><pre class="programlisting"># sampling and fitting
for episode in range(episode_count):
    state = env.reset()
    # state is car [position, speed]
    state = np.reshape(state, [1, state_dim])
    # reset all variables and memory before the start of
    # every episode
    step = 0 
    total_reward = 0 
    done = False
    agent.reset_memory()
   while not done:
        # [min, max] action = [-1.0, 1.0]
        # for baseline, random choice of action will not move
        # the car pass the flag pole
        if args.random:
            action = env.action_space.sample()
        else:
            action = agent.act(state)
        env.render()
        # after executing the action, get s', r, done
        next_state, reward, done, _ = env.step(action)
        next_state = np.reshape(next_state, [1, state_dim])
        # save the experience unit in memory for training
        # Actor-Critic does not need this but we keep it anyway.
        item = [step, state, next_state, reward, done]
        agent.remember(item)

        if args.actor_critic and train:
            # only actor-critic performs online training
            # train at every step as it happens
            agent.train(item, gamma=0.99)
        elif not args.random and done and train:
            # for REINFORCE, REINFORCE with baseline, and A2C
            # we wait for the completion of the episode before 
            # training the network(s)
            # last value as used by A2C
            v = 0 if reward &gt; 0 else agent.value(next_state)[0]
            agent.train_by_episode(last_value=v)

        # accumulate reward
        total_reward += reward
        # next state is the new state
        state = next_state
        step += 1</pre></div></div><div class="section" title="Performance evaluation of policy gradient methods"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec47"/>Performance evaluation of policy gradient methods</h2></div></div></div><p>The four<a id="id468" class="indexterm"/> policy gradients methods were evaluated by training the agent for 1,000 episodes. We define 1 training session as 1,000 episodes of training. The first performance metric is measured by accumulating the number of times the car reached the flag in 1,000 episodes. <span class="emphasis"><em>Figures 10.7.1</em></span> to <span class="emphasis"><em>10.7.4</em></span> shows five training sessions per method.</p><p>In this metric, A2C reached the flag with the greatest number<a id="id469" class="indexterm"/> of times followed by REINFORCE with baseline, Actor-Critic, and REINFORCE. The use of baseline or critic accelerates the learning. Note that these are training sessions with the agent continuously improving its performance. There were cases in the experiments where the agent's performance did not improve with time.</p><p>The second performance metric is based on the requirement that the <code class="literal">MountainCarContinuous-v0</code> is considered solved if the total reward per episode is at least 90.0. From the five training sessions per method, we selected one training session with the highest total reward for the last 100 episodes (episodes 900 to 999). <span class="emphasis"><em>Figures 10.7.5</em></span> to <span class="emphasis"><em>10.7.8</em></span> show the results of the four policy gradient methods. REINFORCE with baseline is the only method that was able to consistently achieve a total reward of about 90 after 1,000 episodes of training. A2C has the second-best performance but could not consistently reach at least 90 for the total rewards.</p><div class="mediaobject"><img src="graphics/B08956_10_12.jpg" alt="Performance evaluation of policy gradient methods"/><div class="caption"><p>Figure 10.7.1: The number of times the mountain car reached the flag using REINFORCE method</p></div></div><div class="mediaobject"><img src="graphics/B08956_10_13.jpg" alt="Performance evaluation of policy gradient methods"/><div class="caption"><p>Figure 10.7.2: The number of times the mountain car reached the flag using REINFORCE with baseline method</p></div></div><div class="mediaobject"><img src="graphics/B08956_10_14.jpg" alt="Performance evaluation of policy gradient methods"/><div class="caption"><p>Figure 10.7.3: The number of times the mountain car reached the flag using the Actor-Critic method</p></div></div><div class="mediaobject"><img src="graphics/B08956_10_15.jpg" alt="Performance evaluation of policy gradient methods"/><div class="caption"><p>Figure 10.7.4: The number of times the mountain car reached the flag using the A2C method</p></div></div><div class="mediaobject"><img src="graphics/B08956_10_16.jpg" alt="Performance evaluation of policy gradient methods"/><div class="caption"><p>Figure 10.7.5: Total rewards received per episode using REINFORCE method</p></div></div><div class="mediaobject"><img src="graphics/B08956_10_17_a.jpg" alt="Performance evaluation of policy gradient methods"/><div class="caption"><p>Figure 10.7.6: Total rewards received per episode using REINFORCE with baseline method.</p></div></div><div class="mediaobject"><img src="graphics/B08956_10_18.jpg" alt="Performance evaluation of policy gradient methods"/><div class="caption"><p>Figure 10.7.7: Total rewards received per episode using the Actor-Critic method</p></div></div><div class="mediaobject"><img src="graphics/B08956_10_19.jpg" alt="Performance evaluation of policy gradient methods"/><div class="caption"><p>Figure 10.7.8: The total rewards received per episode using the A2C method</p></div></div><p>In the experiments conducted, we used the same learning rate, <code class="literal">1e-3</code>, for log probability and value<a id="id470" class="indexterm"/> networks optimization. The discount factor is set to 0.99, except for A2C which is easier to train at a 0.95 discount factor.</p><p>The reader is encouraged to run the trained network by executing:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python3 policygradient-car-10.1.1.py</strong></span>
<span class="strong"><strong>--encoder_weights=encoder_weights.h5 --actor_weights=actor_weights.h5</strong></span>
</pre></div><p>Following table shows other modes of running <code class="literal">policygradient-car-10.1.1.py</code>. The weights file (that is, <code class="literal">*.h5</code>) can be replaced by your own pre-trained weights file. Please consult the code to see the other potential options:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Purpose</p>
</th><th style="text-align: left" valign="bottom">
<p>Run</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Train REINFORCE from scratch</p>
</td><td style="text-align: left" valign="top">
<p>
</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 policygradient-car-10.1.1.py</strong></span>
<span class="strong"><strong>--encoder_weights=encoder_weights.h5</strong></span>
</pre></div><p>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Train REINFORCE with baseline from scratch</p>
</td><td style="text-align: left" valign="top">
<p>
</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 policygradient-car-10.1.1.py</strong></span>
<span class="strong"><strong>--encoder_weights=encoder_weights.h5 -b</strong></span>
</pre></div><p>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Train Actor-Critic from scratch</p>
</td><td style="text-align: left" valign="top">
<p>
</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 policygradient-car-10.1.1.py</strong></span>
<span class="strong"><strong>--encoder_weights=encoder_weights.h5 -a</strong></span>
</pre></div><p>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Train A2C from scratch</p>
</td><td style="text-align: left" valign="top">
<p>
</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 policygradient-car-10.1.1.py</strong></span>
<span class="strong"><strong>--encoder_weights=encoder_weights.h5 -c</strong></span>
</pre></div><p>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Train REINFORCE from previously saved weights</p>
</td><td style="text-align: left" valign="top">
<p>
</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 policygradient-car-10.1.1.py</strong></span>
<span class="strong"><strong>--encoder_weights=encoder_weights.h5</strong></span>
<span class="strong"><strong>--actor_weights=actor_weights.h5 --train</strong></span>
</pre></div><p>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Train REINFORCE with baseline from previously saved weights</p>
</td><td style="text-align: left" valign="top">
<p>
</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 policygradient-car-10.1.1.py</strong></span>
<span class="strong"><strong>--encoder_weights=encoder_weights.h5</strong></span>
<span class="strong"><strong>--actor_weights=actor_weights.h5</strong></span>
<span class="strong"><strong>--value_weights=value_weights.h5 -b --train</strong></span>
</pre></div><p>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Train Actor-Critic from previously saved weights</p>
</td><td style="text-align: left" valign="top">
<p>
</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 policygradient-car-10.1.1.py</strong></span>
<span class="strong"><strong>--encoder_weights=encoder_weights.h5</strong></span>
<span class="strong"><strong>--actor_weights=actor_weights.h5</strong></span>
<span class="strong"><strong>--value_weights=value_weights.h5 -a --train</strong></span>
</pre></div><p>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Train A2C from previously saved weights</p>
</td><td style="text-align: left" valign="top">
<p>
</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 policygradient-car-10.1.1.py</strong></span>
<span class="strong"><strong>--encoder_weights=encoder_weights.h5</strong></span>
<span class="strong"><strong>--actor_weights=actor_weights.h5</strong></span>
<span class="strong"><strong>--value_weights=value_weights.h5 -c --train</strong></span>
</pre></div><p>
</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p>Table 10.7.1: Different options in running policygradient-car-10.1.1.py</p></blockquote></div><p>As a final note, the implementation of the policy gradient methods in Keras has some limitations. For example, training the actor model requires resampling the action. The action is<a id="id471" class="indexterm"/> first sampled and applied to the environment to observe the reward and next state. Then, another sample is taken for training the log probability model. The second sample is not necessarily the same as the first one, but the reward that is used for training comes from the first sampled action, which can introduce stochastic error in the computation of gradients.</p><p>The good news is Keras is gaining a lot of support from TensorFlow in the form of <code class="literal">tf.keras</code>. Transitioning from Keras to a more flexible and powerful machine learning library, like TensorFlow, has been made a lot easier. If you started with Keras and wanted to build low-level custom machine learning routines, the APIs of Keras and <code class="literal">tf.keras</code> share strong similarities.</p><p>There is a small learning curve in using Keras in TensorFlow. Furthermore, in <code class="literal">tf.keras</code>, you're able to take advantage of the new easy to use Dataset and Estimators APIs of TensorFlow. This simplifies a lot of the code and model reuse that ends up with a clean pipeline. With the new eager execution mode of TensorFlow, it becomes even easier to implement and debug Python codes in <code class="literal">tf.keras</code> and TensorFlow. Eager execution allows the execution of codes without building a computational graph as we did in this book. It also allows code structures similar to a typical Python program.</p></div></div>
<div class="section" title="Conclusion"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec57"/>Conclusion</h1></div></div></div><p>In this chapter, we've covered the policy gradient methods. Starting with the policy gradient theorem, we formulated four methods to train the policy network. The four methods, REINFORCE, REINFORCE with baseline, Actor-Critic, and A2C algorithms were discussed in detail. We explored how the four methods could be implemented in Keras. We then validated the algorithms by examining the number of times the agent successfully reached its goal and in terms of the total rewards received per episode.</p><p>Similar to Deep Q-Network [3] that we discussed in the previous chapter, there are several improvements that can be done on the fundamental policy gradient algorithms. For example, the most prominent one is the A3C [4] which is a multi-threaded version of A2C. This enables the agent to get exposed to different experiences simultaneously and to optimize the policy and value networks asynchronously. However, in the<a id="id472" class="indexterm"/> experiments conducted by OpenAI, <a class="ulink" href="https://blog.openai.com/baselines-acktr-a2c/">https://blog.openai.com/baselines-acktr-a2c/</a>, there is no strong advantage of A3C over A2C since the former could not take advantage of the strong GPUs available nowadays.</p><p>Given that this is the end of the book, it's worth noting that the area of deep learning is huge, and to cover all the advances in one book like this is impossible. What we've done is carefully selected the advanced topics that I believe will be useful in a wide range of applications and that you, the reader will be able to easily build on. The implementations in Keras that have been illustrated throughout this book will allow you to carry on and apply the techniques in your own work and research.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec58"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Sutton and Barto.<span class="emphasis"><em> Reinforcement Learning: An Introduction</em></span>, <a class="ulink" href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">http://incompleteideas.net/book/bookdraft2017nov5.pdf</a>, (2017).</li><li class="listitem">Mnih, Volodymyr, and others. <span class="emphasis"><em>Human-level control through deep reinforcement learning</em></span>, <span class="emphasis"><em>Nature</em></span> 518.7540 (2015): 529.</li><li class="listitem">Mnih, Volodymyr, and others. <span class="emphasis"><em>Asynchronous methods for deep reinforcement learning</em></span>, <span class="emphasis"><em>International conference on machine learning</em></span>, 2016.</li><li class="listitem">Williams and Ronald J. <span class="emphasis"><em>Simple statistical gradient-following algorithms for connectionist reinforcement learning</em></span>, <span class="emphasis"><em>Machine learning</em></span> 8.3-4 (1992): 229-256.</li></ol></div></div></body></html>