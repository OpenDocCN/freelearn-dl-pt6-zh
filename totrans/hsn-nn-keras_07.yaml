- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we marveled over the visual cortex and leveraged some
    insights from the way it processes visual signals to inform the architecture of
    **Convolutional Neural Networks** (**CNNs**), which form the base of many state-of-the-art
    computer vision systems. However, we do not understand the world around us with
    vision alone. Sound, for one, also plays a very important role. More specifically,
    we humans love to communicate and express intricate thoughts and ideas through
    sequences of symbolic reductions and abstract representations. Our built-in hardware
    allows us to interpret vocalizations or demarcations thereof, composing the base
    of human thought and collective understandings, upon which more complex representations
    (such as human languages, for instance) may be composed. In essence, these sequences
    of symbols are reduced representations of the world around us, through our own
    lenses, which we use to navigate our surroundings and effectively express ourselves.
    It stands to reason that we would want machines to understand this manner of processing
    sequential information, as it could help us to resolve many problems we face with
    such sequential tasks in the real world. But what kind of problems?
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are the topics that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Modeling sequences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing different types of sequence processing tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting an output per time step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation through time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploding and vanishing gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GRUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building character-level language models in keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistics of character modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The purpose of controlling stochastically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing different RNN models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a SimpleRNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building GRUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On processing reality sequentially
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bi-directional layer in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing output values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps you want to get the right translation for your order in a restaurant
    while visiting a foreign country. Maybe you want your car to perform a sequence
    of movements automatically so that it is able to park by itself. Or maybe you
    want to understand how different sequences of adenine, guanine, thymine, and cytosine
    molecules in the human genome lead to differences in biological processes occurring
    in the human body. What's the commonality between these examples? Well, these
    are all sequence modeling tasks. In such tasks, the training examples (being vectors
    of words, a set of car movements generated by on-board controls, or configuration
    of *A*, *G*, *T*, and *C* molecules) are essentially multiple time-dependent data
    points of a possibly varied length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentences, for example, are composed of words, and the spatial configuration
    of these words allude not only to what has been said, but also to what is yet
    to come. Try and fill in the following blank:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Don''t judge a book by its ___.*'
  prefs: []
  type: TYPE_NORMAL
- en: How did you know that the next word would be *cover*? You simply look at the
    words and their relative positions and performed some sort of Bayesian inference,
    leveraging the sentences you have previously seen and their apparent similarity
    to the example at hand. In essence, you used your internal model of the English
    language to predict the most probable word to follow. Here, *language model* simply
    refers to the probability of a particular configuration of words occurring together
    in a given sequence. Such models are the fundamental components of modern speech
    recognition and machine translation systems, and simply rely on modeling the likelihood
    of sequences of words.
  prefs: []
  type: TYPE_NORMAL
- en: Using RNNs for sequential modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The field of natural language understanding is a common area where **recurrent
    neural networks** (**RNNs**) tend to excel. You may imagine tasks such as recognizing
    named entities and classifying the predominant sentiment in a given piece of text.
    However, as we mentioned, RNNs are applicable to a broad spectrum of tasks that
    involve modeling time-dependent sequences of data. Generating music is also a
    sequence modeling task as we tend to distinguish music from a cacophony by modeling
    the sequence of notes that are played in a given tempo.
  prefs: []
  type: TYPE_NORMAL
- en: RNN architectures are even applicable for some visual intelligence tasks, such
    as video activity recognition. Recognizing whether a person is cooking, running,
    or robbing a bank in a given video is essentially modeling sequences of human
    movements and matching them to specific classes. In fact, RNNs have been deployed
    for some very interesting use cases, including generating text in Shakespearean
    style, creating realistic (but incorrect) algebraic papers, and even producing
    source code for the Linux operating system with proper formatting.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what makes these networks so versatile at performing these seemingly diverse
    tasks? Well, before we answer this, let''s refresh our memory on some of the difficulties
    we faced using neural nets so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3ef793d-f2f1-4923-a5b9-0e1680930241.png)'
  prefs: []
  type: TYPE_IMG
- en: Fake algebraic geometry, generated by RNN, courtesy of Andrej Karpathy
  prefs: []
  type: TYPE_NORMAL
- en: 'Which means:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee7a9951-c981-4b4f-b1d8-9b3e02bf45eb.png)'
  prefs: []
  type: TYPE_IMG
- en: What's the catch?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A problem with all of the networks we have built so far is that they only accepted
    inputs and outputs of fixed sizes for given training examples. We have always
    had to specify our input shape, defining the dimensions of the tensor entering
    our network, which in turn returns a fixed size output in terms of a class probability
    score, for example. Moreover, the hidden layers in our networks each had their
    own weights and activations, which behaved somewhat independently of each other,
    without identifying relationships between successive input values. This holds
    true for both the feedforward and the CNNs that we have familiarized ourselves
    with in previous chapters. For each network we built, we used non-sequential training
    vectors, which would propagate through a constant number of layers and produce
    a single output.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we did see some multi-output models to visualize the intermediate layers
    of CNNs, we never really modified our architecture to operate over a sequence
    of vectors. This basically prohibited us from sharing any time-dependent information
    that may affect the likelihood of our predictions. Discarding time-dependent information
    has got us by so far for the tasks we dealt with. In the case of image classification,
    the fact that your neural network saw the image of a cat at the last iteration
    does not really help it classify the current image it is viewing because the class
    probabilities of these two instances are not temporally related. However, this
    approach already caused us some trouble for the use case of sentiment analysis.
    Recall in [Chapter 3](46e25614-bb5a-4cca-ac3e-b6dfbe29eea5.xhtml), *Signal Processing
    - Data Analysis with Neural Networks*, that we classified movie reviews by treating
    each review as a bag of undirected words (that is, not in their sequential order).
    This approach entailed transforming each review into a fixed-length vector that''s
    defined by the size of our vocabulary (that is, the number of unique words in
    the corpus, which we had chosen to be 12,000 words). While useful, this is certainly
    not the most efficient or scalable form of representing information, as a sentence
    of any given length must be represented by a 12,000-dimensional vector. The simple
    feedforward network we trained (attaining an accuracy just above 88 %) incorrectly
    classified the sentiment of one of the reviews, which has been reproduced here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d2f4a36-47d1-46d5-a154-564f4aff2342.png)'
  prefs: []
  type: TYPE_IMG
- en: Our network seemed to have gotten confused due to the (unnecessarily) complex
    sentence with several long-term dependencies and contextual valence shifters.
    In retrospect, we noted unclear double negatives referring to various entities
    such as the director, actor, and the movie itself; yet we were able to make out
    that the overall sentiment of the review was clearly positive. Why? Simply because
    we are able to track concepts that are relevant to the general sentiment of the
    review, as we read it word for word. In our minds, we are able to assess how each
    new word of the review we see affects the general meaning of the statement we
    have read so far. In this manner, we adjust our sentiment score for a review as
    we read along and come across new information (such as adjectives or negations)
    that may affect this score at a given time step.
  prefs: []
  type: TYPE_NORMAL
- en: Just like in CNNs, we want our network to be able to use representations that
    have been learned on a certain segment of the input, which are then usable later
    on in other segments and examples. In other words, we need to be able to share
    the weights of our network from previous time steps to connect bits of information
    as we sequentially go over our input review. This is pretty much what RNNs allow
    us to do. These layers leverage the additional information that's encoded in successive
    events, which it does by looping over a sequence of input values. Depending on
    the architectural implementation, RNNs can save relevant information in its memory
    (also referred to as its state) and use this information to perform predictions
    at subsequent time steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'This mechanism is notably different from the networks we saw earlier, which
    processed each training iteration independently and did not maintain any state
    between predictions. There are several different implementations of recurrent
    neural networks, ranging from **Gated Recurrent Units** (**GRUs**), stateful and
    stateless **Long Short-Term Memory** (**LSTM**) networks, bi-directional units,
    and many more. As we will soon discover, each of these architectures help to address
    a certain type of problem, building on the shortcomings of each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d9ac677-7a6d-493d-8769-4c6a1737b07c.png)'
  prefs: []
  type: TYPE_IMG
- en: Basic RNN architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s have a look at how the RNN architecture differentiates itself from
    the other networks that we have seen so far by unrolling it through time. Let''s
    consider a new time series problem: speech recognition. This task can be performed
    by computers to identify the flow of words during a segment of human speech. This
    can be used to transcribe the speech itself, translate it, or use it as input
    for instructions, similar to the manner in which we instruct each other. Such
    applications form the base of systems such as Siri or Alexa and perhaps more complex
    and cognitive virtual assistants of the future. So, how can an RNN decode the
    sequence of decomposed vibrations that are recorded by the microphone on your
    computer into a string variable corresponding to the input speech?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a simplified theoretical example. Imagine that our training
    data maps a sequence of human vocalizations to a set of human-readable words.
    In other words, you show your network an audio clip and it spits out a transcript
    of whatever was said within. We task an RNN to go over a segment of speech by
    treating it as sequences of vectors (representing sound bytes). The network can
    then try to predict what words of the English language these sound bytes may represent
    at each time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d74466a5-cfac-4d7a-b9d7-a03245b7de68.png)'
  prefs: []
  type: TYPE_IMG
- en: Consider the set of vectors that represent the sound byte for the words *today
    is a nice day*. A recurrent layer will unfold this sequence in several time steps.
    In the first time step, it will take the vector representing vocalization for
    the first word in the sequence as input (that is, *today*), compute a dot product
    with the layer weights, and pass the product through a non-linear activation function
    (commonly tanh for RNNs) to output a prediction. This prediction corresponds to
    a word that the network thinks it has heard. At the second time step, the recurrent
    layer receives the next sound byte (that is, for the word *is*) in the sequence,
    along with the activation values from the first time step. Both of these values
    are then squashed through the activation function to produce a prediction for
    the current time step. This basically allows the layer to leverage information
    from the previous time steps to inform the prediction at the current time step.
    This process is repeated as the recurrent layer receives each vocalization in
    a given sequence, along with the activation values from previous vocalizations.
    The layer may compute a Softmax probability score for each word in our dictionary,
    picking the one with the highest value as output for the given layer. This word
    corresponds to what our network thinks it has heard at this time.
  prefs: []
  type: TYPE_NORMAL
- en: Temporarily shared weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why is it useful to temporally connect activations? Well, as we pointed out
    earlier, each word affects the probability distribution of the next word to come.
    If our sentence began with the word *Yesterday*, it is much more likely to be
    followed by the word *was*, than the word *is*, reflecting the use of the past
    tense. Such syntactic information can be passed along through recurrent layers
    to inform the predictions of the network at each step by using what the network
    has output in previous time steps. As our network trains on given segments of
    speech, it will adjust its layer weights to minimize the difference between what
    it predicts and the true value of each output by (hopefully) learning such grammatical
    and syntactic rules, among other things. Importantly, the recurrent layer's weights
    are temporally shared, allowing activations from previous time steps to have influence
    over predictions of subsequent time steps. Doing so, we no longer treat each prediction
    in isolation, but as a function of the network's activations at previous time
    steps, along with some input at the current time step.
  prefs: []
  type: TYPE_NORMAL
- en: The actual workflow of speech recognition models may be a bit more complex than
    what we described previously, which involves data normalization techniques such
    as the Fourier transformation, which lets us decompose audio signals into their
    constituent frequencies. In essence, we always try to normalize our input data
    with the goal to better represent data to our neural networks, as this helps it
    to converge faster to encode useful predictive rules. The key take-away from this
    example is that recurrent layers can leverage earlier temporal information to
    inform its predictions at the current time step. As we progress through this chapter,
    we will see how these architectures can be adopted for modeling different lengths
    of sequence input and output data.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence modeling variations in RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The speech recognition example consists of modeling a synchronized many-to-many
    sequence, where we predicted many sets of vocalizations to many words that correspond
    to these vocalizations. We can use a similar architecture for the task of video
    captioning, where we would want to sequentially label each frame of the video
    with the dominant object within. This is yet another synchronized many-to-many
    sequence, as we output a prediction at each time step that corresponds to the
    input frame of the video.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding many-to-many representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also have a semi-synchronized many-to-many sequence in the case of machine
    translation. This use case is semi-synchronized, as we do not immediately output
    a prediction at each time step. Instead, we use the encoder section of our RNN
    to capture the entire phrase so that it can be translated before we proceed and
    actually translate it. This lets us find better representations of the input data
    in the output language, instead of just translating each word at a time. The latter
    approach is not very robust and often leads to inaccurate translations. In the
    following example, an RNN translates the French phrase *C''est pas mal!* into
    the equivalent term in English, *It''s nice!*, which is a much more accurate translation
    than the literal, *It''s not bad!*. Hence, RNNs can help us to decipher the peculiar
    rules that are applied to complementing a person in the French language. This
    may help avoid quite a few misunderstandings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0dc153e8-dbf9-4326-8147-380991d5df01.png)'
  prefs: []
  type: TYPE_IMG
- en: Many-to-one
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similarly, you can also have a many-to-one architecture to address tasks such
    as attributing many sequences of words forming a sentence to one corresponding
    sentiment score. This is just like what we had to do in our previous exercise
    with the IMDb dataset. Last time, our approach involved representing each review
    as an undirected bag-of-words. With RNNs, we can approach this problem by modeling
    a review as a directed sequence of individual words, in their correct order, hence
    leveraging the spatial information from the arrangement of words to inform our
    sentiment score. Here is a simplified example of a many-to-one RNN architecture
    for sentiment classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5504705-2713-4b8a-bc3c-3854837869a7.png)'
  prefs: []
  type: TYPE_IMG
- en: One-to-many
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, different variations of sequential tasks may demand different architectures.
    Another commonly used architecture is the one-to-many RNN model, which we would
    use for the use case of music generation or image captioning. For music generation,
    we essentially feed our network one input note, make it predict the next note
    in the sequence, and then leverage its very own prediction as input for the next
    time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8f6a4c0-d250-4103-9ede-c2ba360e3e4b.png)'
  prefs: []
  type: TYPE_IMG
- en: One-to-many for image captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another novel example of a one-to-many architecture is what is commonly used
    for the task of image captioning. This is when we show our network an image and
    ask it to describe what is going on with a small caption. To do this, we essentially
    feed our network one image at a time to output many words corresponding to what
    is going on in the image. Commonly, you may stack a recurrent layer on top of
    a CNN that has already been trained on some entities (objects, animals, people,
    and so on). Doing so, you could use the recurrent layer to intake the output values
    of the convolutional network all together, and sequentially go over the image
    to output meaningful words corresponding to a description of the input image.
    This is a more complex setup that we will elaborate on in later chapters. For
    now, it is useful to know that LSTM networks (shown as follows) are a type of
    RNN that's inspired by semantic and episodic divisions of the human memory structure
    and will be the prime topic of discussion in [Chapter 6](62bc2e63-11f3-43ab-a3ae-967c6603c306.xhtml), *Long-Short
    Term Memory Networks*. In the following diagram, we can see how the network is
    able to pick up on the fact that there are several giraffes standing about, leveraging
    the output it receives from the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing different types of sequence processing tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we have familiarized ourselves with the basic idea of what a recurrent
    layer does and have gone over some specific examples of use cases (from speech
    recognition, machine translation, and image captioning) where variations of such
    time-dependent models may be used. The following diagram provides a visual summary
    of some of the sequential tasks we discussed, along with the type of RNN that''s
    suited for the job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bd4c08a-3dbf-4d48-9fb8-d7800c486b63.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we will dive deeper into the governing equations, as well as the learning
    mechanism behind RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: How do RNNs learn?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw previously, for virtually all neural nets, you can break down the
    learning mechanism into two separate parts. The forward propagation equations
    govern the rules that allow data to propagate forward in our neural network, all
    of the way to the network predictions. The backpropagation of errors are defined
    by equations (such as the loss function and the optimizer), which allow our model's
    prediction errors to move backward through our model's layers, adjusting the weights
    on each layer toward the correct prediction values.
  prefs: []
  type: TYPE_NORMAL
- en: This is essentially the same for RNNs, yet with a few architectural variations
    to account for time-dependent information flows. To do this, RNNs can leverage
    an internal state, or *memory*, to encode useful time-dependent representations.
    First, let's have a look at the forward pass of data in a recurrent layer. A recurrent
    layer basically combines the input vector that's entering the layer with a state
    vector to produce a new output vector at each time step. Soon, we will see how
    iteratively updating these state vectors can be leveraged to preserve temporally
    relevant information in a given sequence.
  prefs: []
  type: TYPE_NORMAL
- en: A generic RNN layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram hopefully familiarizes this process. On the left, the
    gray arrow in the diagram illustrates how activations from current time steps
    are sent forward to future time steps. This holds true for all RNNs, forming a
    distinct signature of their architecture. On the right-hand side, you will notice
    a reduced representation of the RNN unit. This is one of the most common demarcations
    of RNNs that you will find in countless computer science research papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30b9a91b-026c-4fe1-a6c4-93fa6eb8a63f.png)'
  prefs: []
  type: TYPE_IMG
- en: To sequence, or not to sequence?
  prefs: []
  type: TYPE_NORMAL
- en: The RNN layer essentially processes its input values in a time-dependent and
    sequential manner. It employs a state (or memory), which allows us to address
    sequence modeling tasks in a novel way. However, there are quite a few examples
    where approaching non-sequential data in a sequential manner has allowed us to
    address standard use cases in more efficient ways. Take the example of the research
    conducted by DeepMind on steering the attention of a network on images.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of simply applying a computation-heavy CNN for image classification,
    DeepMind researchers showed how RNNs that have been trained through reinforcement
    learning can be used to perform the same function and achieve even better accuracy
    at more complex tasks such as classifying cluttered images, along with other dynamic
    visual control problems. One of the main architectural take backs from their work
    was that their RNN effectively extracted information from images or videos by
    adaptively selecting sequence or regions to process at a high resolution, thereby
    reducing the redundant computational complexity of processing an entire image
    at a high resolution. This is pretty neat, as we don't necessarily need to process
    all of the parts of an image to perform classification. Most of what we need is
    usually centered around a local area of the image in question: [https://deepmind.com/research/publications/recurrent-models-visual-attention/](https://deepmind.com/research/publications/recurrent-models-visual-attention/).
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, how does information actually flow through this-here RNN architecture?
    Let''s use a demonstrative example to introduce the forward pass operations in
    RNNs. Imagine the simple task of predicting the next word in a phrase. Suppose
    our phrase is: *to be or not to be*. As the words enter the network, we can break
    down the computations that are performed at each time step into two conceptual
    categories. In the following diagram, you can visualize each arrow as performing
    a computation (or dot product operation) on a given set of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecf33ff7-1e17-4e70-b5cd-a5c3983031d3.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that, in a recurrent cell, computations occur both vertically and
    horizontally as data propagates through it. It is important to remember that all
    parameters (or weight matrices) of the layer are temporally shared, meaning that
    the same parameters are used for computations at every time step. At the first
    time step, our layer will use these sets of parameters to compute two output values.
    One of these is the layer's activation value at the current time step, whereas
    the other represents the predicted value at the current time step. Let's start
    with the first one.
  prefs: []
  type: TYPE_NORMAL
- en: Computing activations per time step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following equation denotes the activation of a recurrent layer at time, *t*.
    The term *g* denotes the non-linear activation function that''s chosen for the
    recurrent layer, which is conventionally a tanh function. Inside the brackets,
    we find two matrix-level multiplications being performed and then being added
    up along with a bias term (*ba*):'
  prefs: []
  type: TYPE_NORMAL
- en: '*at = g [ (W^(ax) x x^t ) + (Waa x a(t-1)) + ba ]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The term (*W^(ax)*) governs the transformation of our input vector, ![](img/ebdcf4e7-33d9-4f4e-b3f1-6e2a1b117f3d.png),
    at time, *t*, as it enters the recurrent layer. This matrix of weights is temporally
    shared, meaning that we use the same weight matrix at each time step. Then, we
    get to the term (*Waa*), which refers to the temporally shared weight matrix governing
    the activations from the previous time step. At the first time step, (*Waa*) is
    randomly initialized with very small values (or zeros), since we don''t actually
    have any activation weights to compute with just yet. The same holds for the value
    (*a<0>*), which is also initialized as a zeroed vector. Hence, at time step one,
    we our equation will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a1 = tanH [ (W^(ax) x x1 ) + (Waa x a(0)) + ba ]*'
  prefs: []
  type: TYPE_NORMAL
- en: Simplifying the activation equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can further simplify this equation by stacking the two weight matrices (Wax
    and Waa) horizontally into a single matrix (W[a]) that defines all of the weights
    (or the state) of a recurrent layer. We will also vertically stack the two vectors
    representing the activations from the previous time step (*a(t-1)*) and the input
    at the current time ( ![](img/d3b3e016-18de-4d98-be08-c5d9a8826797.png) t ) to
    form a new matrix that we denote as *[a(t-1), ![](img/45557d34-7a42-4cc5-b4ea-eb504c46f4eb.png)
    t ]* . This lets us simplify our previous activation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*at = tanH [ (W![](img/e76fecd2-fe5a-4716-a1da-048104147a2e.png) x ![](img/b7d3a7c5-8985-4076-9db2-0ab3f5c850ca.png)
    t ) + (Waa x a(t-1)) + ba ] or at = tanH (W![](img/5f597f64-f660-4896-be74-16289a777546.png)a(t-1),
    ![](img/71373feb-6c48-4dcb-9caa-3bd3750e0dd8.png) t ] + ba )*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptually, since the height of the two matrices (W![](img/7ee6ef33-8386-4d3a-a8eb-31f499fa003f.png))
    remains constant, we are able to stack them horizontally in the manner we did.
    The same goes for the length of the input (![](img/22c27f03-9e5f-401d-80ba-870c948b2871.png) t)
    and activation vector (*a(t-1)*), which also remains constant as data propagates
    through an RNN. Now, the computation step can be denoted as the weight matrix
    (W[a]) is multiplied both with the activation from the previous time step, as
    well as with the input from the current time step, after which a bias term is
    added and the whole term is passed through a non-linear activation function. We
    can visualize this process unfolding through time with the new weight matrix,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e5e7f94-b73d-42bb-9ed6-7a9c089fb868.png)'
  prefs: []
  type: TYPE_IMG
- en: In essence, the use of the temporally shared weight parameters (such as *Wa*
    and *Wya*) allows us to leverage information from earlier on in the sequence to
    inform predictions at later time- teps. Now, you know how activations are iteratively
    computed for each time step as data flows through a recurrent layer.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting an output per time step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will look at the equation that leverages the activation value that
    we just calculated to produce a prediction (![](img/94bdb4d8-346d-4392-8547-4550156203ea.png)
    at the given time step (*t*). This is represented like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/c2da3b60-fd3d-41a0-b36f-2deb2800f3f5.png) = g [ (Way x at) + by ]*'
  prefs: []
  type: TYPE_NORMAL
- en: This tells us is that our layer's prediction at a time step is determined by
    computing a dot product of yet another temporally shared output matrix of weights,
    along with the activation output (*at*) we just computed using the earlier equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the sharing of the weight parameters, information from previous time
    steps is preserved and passed through the recurrent layer to inform the current
    prediction. For example, the prediction at time step three leverages information
    from the previous time steps, as shown by the green arrow here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/020d9e3e-b7ad-4fed-b41b-db712bab664b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To formalize these computations, we mathematically show the relation between
    the predicted output at the third time step with respect to the activations at
    previous time steps, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/c003db51-5aab-473c-9e57-5bea6611fa35.png) = sigmoid [ (Way x a3*)*
    + by* ]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Where *a(3)* is define by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a3 = sigmoid (W![](img/49ef1f9f-c46d-4975-92fa-0cf0963c59d0.png)a(2), ![](img/da73e7b9-513b-4b84-a764-8c01ef19d543.png)**3
    ] + ba )*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Where *a**(2)* is defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a2 =* *sigmoid* *(W**![](img/89282f8e-619a-418d-9988-d949c010a5aa.png)a(1),
    ![](img/01d42088-c76b-475a-83bd-c1d6d5c4cfcf.png)**2 ] + ba )*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Where *a(1)* is defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a1 = sigmoid (W![](img/3eca3c48-eacc-4485-8d76-da83a1a9fc8c.png)a(0), ![](img/23fc8ac4-b910-41d8-a11e-7dffeba43a8c.png)**1
    ] + ba )*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, *a(0)* is commonly initialized as a vector of zeros. The main concept
    to understand here is that the RNN layer recursively processes a sequence through
    many time steps before passing the activations forward. Now, you are completely
    familiar with all the equations that govern the forward propagation of information
    in RNNs at a high level. This method, while powerful at modeling many temporal
    sequences, does have its limitations.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of unidirectional information flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A primary limitation is that we are only able to inform our prediction at current
    time steps with activation values from previous time steps, but not from future
    time steps. Why would we want to do this? Well, consider the problem of named
    entity recognition, where we may employ a synchronized many-to-many RNN to predict
    whether each word in our sentence is a named entity (such as the name of a person,
    a place, a product, and so on). We may run into some problems, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The Spartan marched forward, despite the obstacles thrown at him.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Spartan lifestyle that these people face is unimaginable to many.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we can see, by looking as the first two words only, we ourselves would not
    be able to tell whether the word Spartan refers to a noun (and hence is a named
    entity) or refers to an adjective. It is only later on, when we read the rest
    of the sentence, that we are able to attribute the correct label on the word.
    Similarly, our network will not be able to accurately predict that the word Spartan
    in the first sentence is a named entity unless we let it leverage activation values
    from future time steps. Since RNNs can learn sequential grammar rules from an
    annotated dataset, it will be able to learn the fact that named entities are often
    followed by verbs (such as marched) rather than nouns (such as lifestyle), and
    hence will be able to accurately predict that the word *Spartan* refers to a named
    entity on the first sentence only. This becomes possible with a specific type
    of RNN known as a bi-directional RNN, which we will look at later on in this chapter.
    It is also noteworthy that an annotated dataset with part of speech tags (tags
    referring to whether a word is a noun, adjective, and so on) will greatly increase
    your network''s ability to learn useful sequential representations, like we want
    it to do here. We can visualize the first part of both our sentences, annotated
    with part of speech tags, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spartan marched...à:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/081b15e7-8f55-49de-a32e-235c6cfc7ff0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Spartan lifestyle... à:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3b55564c-c0b7-40bf-af27-76eb9b8b0bf4.png)'
  prefs: []
  type: TYPE_IMG
- en: The sequences of words that precede this provide us with more information on
    the current word than the words that come before it. We will soon see how bi-directional
    RNNs may leverage information from future time steps as well as past time steps
    to compute predictions at the present time.
  prefs: []
  type: TYPE_NORMAL
- en: The problems of long-term dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another common problem we face with simple recurrent layers is their weakness
    in modeling long-term sequence dependencies. To clarify what we mean by this,
    consider the following examples, which we feed to an RNN word by word to predict
    the next words to come:'
  prefs: []
  type: TYPE_NORMAL
- en: The monkey had enjoyed eating bananas for a while and was eager to have more,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The monkeys had enjoyed eating bananas for a while and were eager to have more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To predict the word at the 11^(th) time step each sequence, the network must
    remember whether the subject of the sentence (monkey), seen at time-step 2, is
    a singular or plural entity. However, as the model trains and the errors are backpropagated
    through time, the weights for the time steps that are closer to the current time
    step are affected to a larger extent than the weights of earlier time steps. Mathematically
    speaking, this is the problem of vanishing gradients, which is associated with
    extremely small values of the chain rule-based partial derivatives of our loss
    function. The weights in our recurrent layer, which are normally updated in proportion
    to these partial derivatives at each time step, are not *nudged* enough in the
    right direction, prohibiting our network to learn any further. In this manner,
    the model is unable to update the layer weights to reflect long-term grammatical
    dependencies from earlier time steps, like the one reflected in our example. This
    is an especially cumbersome problem, since it significantly affects the backpropagation
    of errors in recurrent layers. Soon, we will see how to partially address this
    problem with more complex architectures such as the GRU and the LSTM networks.
    First, let's understand the process of backpropagation in RNNs, which gives birth
    to this problem.
  prefs: []
  type: TYPE_NORMAL
- en: You may well have wondered how exactly an RNN backpropagates its errors to adjust
    the temporarily shared weights of the layer as it goes over a sequence of inputs.
    This process is even described by an interesting name. Unlike other neural networks
    we have come across, RNNS are known to perform backpropagation through time.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Essentially, we are backpropagating our errors through several time steps,
    reflecting the length of a sequence. As we know, the first thing we need to have
    to be able to backpropagate our errors is a loss function. We can use any variation
    of the cross-entropy loss, depending on whether we are performing a binary task
    per sequence (that is, entity or not, per word à binary cross-entropy) or a categorical
    one (that is, the next word out of the category of words in our vocabulary à categorical
    cross entropy). The loss function here computes the cross-entropy loss between
    a predicted output ![](img/6ebd4a7c-8d96-4cd2-9c2d-2fb746fb420d.png) and actual
    value *(y)*, at time step, *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/46ec09e1-ff3f-49fc-a4e7-4d2cab07b46a.png)( ![](img/aa137547-d065-4e5f-946c-fcb135fc277b.png)
    ![](img/59cd5142-6f5b-4cb8-b930-f2cb8c7b63ae.png) log ![](img/0f3b1dbe-0f85-4cfb-abdf-c011330710ed.png)
    - [ (1-![](img/15d6e47b-8878-42b1-b060-0af7fabf429c.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This function essentially lets us perform an element-wise loss computation
    of each predicted and actual output, at each time step for our recurrent layer.
    Hence, we generate a loss value at each prediction the network makes, for each
    word (or sequence) it sees. We can then sum up each individual loss value to define
    the overall loss of our recurrent layer, operating over *ty* number of time steps.
    Hence, the overall loss of our network can be denoted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eae7767c-e6d4-4719-94ee-5b37bb3d6eee.png)'
  prefs: []
  type: TYPE_IMG
- en: Using this denotation of the overall loss of our network, we can differentiate
    it with respect to the layer weights at each time step to compute the model's
    errors. We can visualize this process by referring back to our recurrent layer
    diagram. The arrows demarcate the backpropagation of errors through time.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing backpropagation through time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we backpropagate the errors in our model with respect to the layer weights
    at each time step and adjust the weight matrices, *Way* and *Wa*, as the model
    trains. We are still essentially computing the gradient of the loss function with
    respect to all of the network parameters, and proportionally nudging both the
    weight matrices in the opposite direction for each time step in our sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca8eeab8-a691-4bd8-baa3-eb46419af982.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we know how RNNs overate over a sequence of vectors and leverage time-dependent
    contingencies to inform predictions at each step.
  prefs: []
  type: TYPE_NORMAL
- en: Exploding and vanishing gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Backpropagating the model''s errors in a deep neural network, however, comes
    with its own complexities. This holds equally true for RNNs, facing their own
    versions of the vanishing and exploding gradient problem. As we discussed earlier,
    the activation of neurons in a given time step is dependent on the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*at = tanH [ (W![](img/49479bca-1721-40de-9026-bc9268c48c87.png) x ![](img/3175fbaa-c555-4dde-8aa9-6641b48527ea.png)
    t ) + (Waa x a(t-1)) + ba ]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw how *Wax* and *Waa* are two separate weight matrices that the RNN layers
    share through time. These matrices are multiplied to the input matrix at current
    time, and the activation from the previous time step, respectively. The dot products
    are then summed up, along with a bias term, and passed through a tanh activation
    function to compute the activation of neurons at current time (*t*). We then used
    this activation matrix to compute the predicted output at current time (![](img/0d6d9ba5-a6ca-4868-b7fc-5db15a607871.png)),
    before passing the activation forward to the next time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/fcfd4173-60dc-4147-ad63-3e8ba6eaf87c.png) = softmax [ (Way x at) +
    by ]*'
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the weight matrices (*Wax*, *Waa*, and *Way*) represent the trainable
    parameters of a given layer. During backpropagation through time, we first compute
    the product of gradients, which represent the changes in the layer weights of
    each time step with respect to the changes in the predicted and actual output.
    Then, we use these products to update the respective layer weights in the opposite
    direction of the change. However, when backpropagating across multiple time steps,
    these products may become infinitesimally small (hence not shifting the layer
    weights significantly), or gargantuanly big (hence overshooting from ideal weights).
    This is mainly true for the activation matrix (*Waa*). It represents the memory
    of our RNN layer since it encodes time-dependent information from previous time
    steps. Let's clarify this notion with a conceptual example to see how updating
    the activation matrix at earlier time steps becomes increasingly hard when dealing
    with long sequences. Suppose you wanted to calculate the gradient of your loss
    at time step three with respect to your layer weights, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking on the gradient level
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The activation matrix at a given time step is a function of the activation
    matrix from the previous time step. Hence, we are forced to recursively define
    the loss at time step three as a product of the sub-gradients of layer weights
    from previous time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fa444c5-b97f-42f8-ae0d-3746cb3a7c0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, (*L*) represents the loss, (*W*) represents the weight matrices of a
    time step, and the *x* values are the inputs at a given time steps. Mathematically,
    this is equivalent to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90b4c49d-fe88-4e31-841d-4879da3ea31b.png)'
  prefs: []
  type: TYPE_IMG
- en: The derivatives of these functions are stored in a Jacobean matrix, representing
    point-wise derivations of the weight and loss vectors. Mathematically, the derivatives
    of these functions are bound by an absolute value of 1\. However, small derivative
    values (close to 0), over several time-steps of matrix-wise multiplications, degrade
    exponentially, almost vanishing, which in turn prohibits the model from converging.
    The same holds true for large values (larger than 1) in the activation matrix,
    where the gradients will become increasingly large until they are attributed a
    NaN value (not a number), abruptly terminating the training process. How can we
    address these problems?
  prefs: []
  type: TYPE_NORMAL
- en: You can find more information on vanishing gradients at: [http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/).
  prefs: []
  type: TYPE_NORMAL
- en: Preventing exploding gradients through clipping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of exploding gradients, the problem is much more evident. Your
    model simply stops training, returning a value error of NaN, corresponding to
    the exploded gradient values. A simple solution to this is to clip your gradients
    by defining an arbitrary upper bound or threshold value to prevent the gradients
    from getting too big. Keras lets us implement this with ease as you can define
    this threshold by manually initiating an optimizer and passing it a `clipvalue`
    or `clipnorm` argument, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43b2da26-4987-453f-a13e-2fe6ee04da70.png)'
  prefs: []
  type: TYPE_IMG
- en: You can then pass the `optimizers` variable to your model when compiling it.
    This idea of clipping gradients is extensively discussed, along with other problems
    that are associated with training RNNs, in the paper titled: *On the difficulty
    of training recurrent neural networks*, which is available at [http://proceedings.mlr.press/v28/pascanu13.pdf](http://proceedings.mlr.press/v28/pascanu13.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Preventing vanishing gradients with memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the case of vanishing gradients, our network stops learning anything new
    as the weights are insignificantly nudged at each update. The problem is particularly
    cumbersome for the case of RNNs, as they attempt to model long sequences over
    potentially many time steps, and so the model has a very hard time backpropagating
    the errors to nudge the layer weights of earlier time steps. We saw how this can
    affect language modeling tasks such as learning grammar rules and entity-based
    dependencies (with the monkey example). Thankfully, several solutions have been
    devised to address this problem. Some have ventured along the lines of careful
    initialization of the activation matrix, *Waa*, using a ReLU activation function
    to pre-train the layer weights in an unsupervised manner. More commonly, however,
    others have addressed this problem by designing more sophisticated architectures
    that are capable of storing long-term information based on its statistical relevance
    to current events in the sequence. This is essentially the base intuition behind
    more complex RNN variations such as the **Gated Recurrent Units** (**GRUs**) and
    **Long Short-Term Memory** (**LSTM**) networks. Let's see how GRUs address the
    problem of long-term dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: GRUs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The GRU can be considered the younger sibling of the LSTM, which we will look
    at [Chapter 6](62bc2e63-11f3-43ab-a3ae-967c6603c306.xhtml), *Long-Short Term Memory
    Networks*. In essence, both leverage similar concepts to modeling long-term dependencies,
    such as remembering whether the subject of the sentence is plural, when generating
    following sequences. Soon, we will see how memory cells and flow gates can be
    used to address the vanishing gradient problem, while better modeling long term
    dependencies in sequence data. The underlying difference between GRUs and LSTMs
    is in the computational complexity they represent. Simply put, LSTMs are more
    complex architectures that, while computationally expensive and time-consuming
    to train, perform very well at breaking down the training data into meaningful
    and generalizable representations. GRUs, on the other hand, while computationally
    less intensive, are limited in their representational abilities compared to LSTM.
    However, not all tasks require heavyset 10-layer LSTMs (like the ones used by
    Siri, Cortana, Alexia, and so on). As we will soon see, character-level language
    modeling can be achieved with quite simple architectures to begin with, producing
    increasingly interesting results with relatively lightweight models such as GRUs.
    The following diagram shows the basic architectural difference between the SimpleRNN
    we have been discussing so far and the GRU.
  prefs: []
  type: TYPE_NORMAL
- en: The memory cell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Again, we have two input values entering the unit, namely the sequence input
    at the current time and the layer activations from the preceding time step. One
    of the main differences in the GRU is the addition of a memory cell (*c*), which
    lets us store some relevant information at a given time step to inform later predictions.
    In practice, this changes how we calculate the activations at a given time step
    (*c^t*, which here is the same as *a^t*) in GRUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d45b808-83ca-40ac-ba99-bb81fc3b79b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Going back to the monkey example, a word-level GRU model has the potential
    to better represent the fact that there are several entities in the second sentence
    that''s given here, and hence will remember to use the word **were** instead of
    **was** to complete the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3adae2d4-7633-4586-9ad5-15cc5dd89532.png)'
  prefs: []
  type: TYPE_IMG
- en: 'How does this memory cell actually work? Well, the value of (*c^t*) stores
    the activation values (*a^t*) at a given time step (time step 2) and is passed
    forward to subsequent time steps if deemed relevant to the sequence at hand. Once
    the relevance of this activation is lost (that is, a new dependency has been detected
    in the sequence), the memory cell can be updated with a new value of (*c^t*),
    reflecting time-dependent information that may be more relevant:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4f4ffc2-9b3f-44df-ab6c-1fe7306206bf.png)'
  prefs: []
  type: TYPE_IMG
- en: A closer look at the GRU cell
  prefs: []
  type: TYPE_NORMAL
- en: Representing the memory cell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When processing our example sentences, a word-level RNN model may store the
    activations at time step 2 (for the words *monkey* and *monkeys*), and save it
    until time step 11, where it is used to predict the output words *was* and *were*,
    respectively. At each time step, a contender value (*c ^(̴t)*) is generated, which
    attempts to replace the value of the memory cell, (*c^t*). However, as long as
    (*c^t*) remains statistically relevant to the sequence, it is conserved, only
    to be discarded later on for more relevant representation. Let's see how this
    is mathematically implemented, starting with the contender value, (*c ^(̴t)*).
    To implement this parameter, we will initialize a new weight matrix, (*Wc*). Then,
    we will compute the dot product of (*Wc*) with the previous activation (*c^(t-1)*)
    and the input at the current time (![](img/005b1e37-203a-4ae1-b717-95d0ed56f958.png)
    t) and pass the resulting vector through a non-linear activation function such
    as tanh. This operation is strikingly similar to the standard forward propagation
    operation we saw previously.
  prefs: []
  type: TYPE_NORMAL
- en: Updating the memory value
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mathematically, we can represent this computation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*c ^(̴t) = tanh ( Wc [ c^(t-1), ![](img/d30f337b-9a65-4151-82d8-203b210d2aee.png)
    t ] + bc)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'More importantly, the GRU also implements a gate denoted by the Greek alphabet
    gamma (Γu ), which basically computes a dot product of inputs and previous activations
    through yet another non-linear function:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Γu = sigmoid ( Wu [ c^(t-1), ![](img/8d63a839-9c80-49b7-80cf-5789501a6bb9.png)
    t ] + bu)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of this gate is to determine whether we should update our current
    value (*c^t*) with a candidate value (*c ^(̴t)*). The value of the gate (Γu) can
    be thought of as a binary value. In practice, we know that the sigmoid activation
    function is known for squishing values between zero and one. In fact, the vast
    majority of input values entering a sigmoid activation function will come out
    as either zero or one, hence it is practical to think of the gamma variable as
    a binary value that decides whether to replace (*c^t*) with (*c ^(̴t)*) or not
    at every time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68fa8246-5c30-4b86-ab85-939a6ff30292.png)'
  prefs: []
  type: TYPE_IMG
- en: Mathematics of the update equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how this would work out in practice. We will use our earlier example
    once more, which has been extended here to theoretically demonstrate when a world-level
    GRU model may work:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The monkey had enjoyed eating bananas for a while and was eager to have more.
    The bananas themselves were the best one could find on this side of the island...*'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a GRU layer goes over this sequence, it may store the activation values
    at the second time step as (c^t), detecting the presence of a singular entity
    (that is, *monkey*). It will carry forward this representation until it reaches
    a new concept in the sequence (*The bananas*), at which point the update gate
    (Γu) will allow the new candidate activation value (c ̴t) to replace the old value
    in the memory cell (c), reflecting the new plural entity, *bananas*. Mathematically,
    we can tie all of this up by defining how the activation value (ct) is calculated
    in a GRU:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ct = ( Γu x c ̴t ) + [ ( 1- Γu ) x ct-1 ]*'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see here, the activation values at a given time step are defined by
    a sum of two terms. The first term reflects the product of the gate value and
    the candidate value. The second term denotes the inverse of the gate value, multiplied
    by the activation from the previous time step. Intuitively, the first term simply
    controls whether to let the update term be included in the equation by being either
    one or zero. The second term controls the potential neutralization of the activation
    of the previous time step (ct-1). Let's have a look at how these two terms work
    together to decide whether or not an update is performed at a given time step.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the no-update scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case where the value (Γu) is zero, the first term reduces to zero altogether,
    removing the effect of (c ̴t), while the second term simply takes the activation
    value from the previous time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this scenario, no update is performed, and the previous activations (`ct`)
    are preserved and passed forward to the next time step.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the update scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On the other hand, if the gate holds a `1`, the equation allows c ̴t to become
    the new value of `ct`, since the second term reduces to zero `(( 1-1) x ct-1)`.
    This is what allows us to effectively perform an update to our memory cell, hence
    conserving useful time-dependent representations. The update scenario can be denoted
    mathematically like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Preserving relevance between time steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The nature of the two terms that are used to perform our memory update helps
    us to preserve relevant information across multiple time steps. Hence, this implementation
    potentially provides a solution to the vanishing gradient issue by modeling long-term
    dependencies with the use of a memory cell. You may wonder, however, how exactly
    does the GRU assess the relevance of an activation? The update gate simply allows
    the replacement of the activation vector (*c^t*) with the new candidate (*c ^(̴t)*),
    but how do we know how relevant the previous activation (*c^(t-1)*) is to the
    current time step? Well, earlier, we presented a simplified equation for governing
    the GRU unit. A last addition to its implementation is the relevance gate (Γr),
    which helps us to do exactly what it suggests. Hence, we calculate the candidate
    value (*c ^(̴t)*) using this relevance gate (Γr) to incorporate the relevance
    of activation values from the previous time step (*c*^(**t*-1*)) to the current
    one (*c^t*). This helps us to assess how relevant the activations from the previous
    time steps are to the current input sequence at hand and is implemented in a very
    familiar way, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88c3f787-2091-4af8-8fb0-f1baebc84124.png)'
  prefs: []
  type: TYPE_IMG
- en: Formalizing the relevance gate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following equations show the full spectrum of the GRU equations, including
    the relevance gate term, which is now included in the computation we performed
    earlier to get the contender memory value, (c ̴t ):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Earlier**: *c ̴t = tanh ( Wc [ ct-1, ![](img/1666159b-52ff-45a0-a1d6-2ed0792818e5.png)
    t ] + bc)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Now**: *c ̴t = tanh ( Wc [ Γr , ct-1, ![](img/8a195fb9-f845-4c0c-b258-dbe27c76fcac.png)
    t ] + bc)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Where**: *Γr = sigmoid ( Wr [ ct-1, ![](img/8b555231-db98-4e95-926a-b4ed29399edf.png)
    t ] + br)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Not surprisingly, (Γr) is computed by initializing yet another weight matrix
    (*Wr*) and computing its dot product with past activations (*c^(t-1)*) and current
    inputs (![](img/288f2225-c922-4231-9fb6-8a3474d237fa.png) t) before summing them
    through a sigmoid activation function. The equation that''s computing the current
    activation (*c^t*) remains the same, except for the (*c ^(̴t)*) term within it,
    which is now incorporating the relevance gate (Γr) in its calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ct = ( Γu x c ̴t ) + [ ( 1- Γu ) x ct-1 ]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The predicted output at a given time step is calculated in the same manner
    as it was for the SimpleRNN layer. The only difference is that the term (*a^t*)
    is replaced by the term (*c^t*), which denotes the activations of neurons in a
    GRU layer at time step (*t*):'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/f0e96174-8b99-4ab8-8570-59e838271286.png) = softmax [ (Wcy x ct) +
    by ]*'
  prefs: []
  type: TYPE_NORMAL
- en: Practically speaking, both terms (*a^t* and *c^t*) can be thought of as synonymous
    in the case of GRUs, but we will later see architectures where this no longer
    applies, such as in LSTMs. For the time being, we have covered the basic equations
    that govern the forward propagation of data in a GRU unit. You've seen how we
    can compute the activations and the output values at each time step and use different
    gates (such as the update and relevance gates) to control the flow of information,
    allowing us to assess and store long-term dependencies. What we saw here is a
    very common implementation that addresses the vanishing gradients problem. However,
    it is but one of potentially many more. Researchers have found this particular
    formulaic implementation to be a successful way to gauge relevance and model sequential
    dependencies for an array of different problems since their introduction in 2014
    by Kyunghyun Cho et al.
  prefs: []
  type: TYPE_NORMAL
- en: Building character-level language models in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we have a good command over the basic learning mechanism of different types
    of RNNs, both simple and complex. We also know a bit about different sequence
    processing use cases, as well as different RNN architectures that permit us to
    model these sequences. Let's combine all of this knowledge and put it to use.
    Next up, we will test these different models on a hands-on task and see how each
    of them do.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will explore the simple use case of building a character level language
    model, much like the autocorrect model almost everybody is familiar with, which
    is implemented on word processor applications for almost all devices. A key difference
    will be that we will train our RNN to derive a language model from Shakespeare''s
    Hamlet. Hence, our network will take a sequence of characters from Shakespeare''s
    *Hamlet* as input and iteratively compute the probability distribution of the
    next character to come in the sequence. Let''s make some imports and load in the
    necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Loading in Shakespeare's Hamlet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the **Natural Language Toolkit **(**NLTK**) package in Python to
    import and preprocess the play, which can be found in the `gutenberg` corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The string variable (`text`) contains the entire sequence of characters that
    make up the play Hamlet. We will now break it up into shorter sequences that we
    can feed to our recurrent network at successive time steps. To forge the input
    sequences, we will define an arbitrary length of characters that the network sees
    at each time step. We will sample these characters from the text string by iteratively
    sliding over them and collecting sequences of characters (denoting our training
    features), along with the next character of the given sequence (as our training
    labels). Naturally, taking samples over longer sequences allows the network to
    compute more accurate probability distributions, hence reflecting contextual information
    on the character to follow. As a result, however, this is also computationally
    more intensive, both for training the model and to generate predictions during
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: Each of our input sequences (`x`) will correspond to 40 characters, and one
    output character (`y1`) that corresponds to the next character in the sequence.
    We can create this data structure of 11 characters per row by using the range
    function to span by segments characters of our entire string (text) at a time,
    and saving them in a list, as shown here. We can see that we have broken up the
    entire play into about 55, 575 sequences of characters.
  prefs: []
  type: TYPE_NORMAL
- en: Building a dictionary of characters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will proceed and create a vocabulary, or dictionary of characters,
    for mapping each character to a specific integer. This is a necessary step for
    us to be able to represent these integers as vectors, which we can sequentially
    feed into our network at each time step. We will create two versions of our dictionary:
    one with characters mapped to indices, and the other with indices mapped to characters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is just for the sake of practicality, as we will need both lists for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can always check how large your vocabulary is by checking the length of
    the mapping dictionary. In our case, it appears that we have `66` unique characters
    that make up the sequences forming the play Hamlet.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing training sequences of characters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After constructing our dictionary of characters, we will break up the text
    making up Hamlet into a set of sequences that can be fed to our network, with
    a corresponding output character for each sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We created two lists and looped over our text string to append a sequence of
    40 characters at a time. One list holds the training sequences, while the other
    holds the next character to come, following the 40 characters of the sequence.
    We have implemented an arbitrary sequence length of 40, but you are free to experiment
    with it. Keep in mind that setting too small a sequence will not allow you network
    to look far back enough to inform predictions, whereas setting to big a sequence
    may give your network a hard time converging, as it won't be able to find the
    most efficient representations. Just like the story of Goldilocks and the three
    bears, you will aim for a sequence length that is *just right*, as informed by
    experiments and/or domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Printing out example sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similarly, we also arbitrarily choose to stride through our text file with
    a window of one character at a time. This simply means that we can potentially
    sample each character multiple times, just like how our convolutional filter progressively
    sampled an entire image by striding through it in fixed steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The difference here is that we embed this sequentially in the training data
    itself, instead of letting a layer perform the striding operation while training.
    This is an easier (and more logical) approach with text data, which can be easily
    manipulated to produce the sequences of characters, at desired strides, from our
    entire text of Hamlet. As we can see, each of our lists now stores sequences of
    strings that are sampled at a stride of three steps, from the original text string.
    We printed out the first and second sequences and labels of our training data,
    which demonstrates the sequential nature of its arrangement.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorizing the training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step is one that your already quite familiar with. We will simply
    vectorize our data by transforming our list of training sequences into a three-dimensional
    tensor representing one-hot encoded training features, with their corresponding
    labels (that is, the next word to come in the sequence). The dimensions of the
    feature matrix can be represented as (*time steps x sequence length x number of
    characters*). In our case, this amounts to 55,575 sequences, each of a length
    of 40\. Hence, our tensor will be composed of 55,575 matrices, each with `40`
    vectors of `66` dimensions, stacked on top of each other. Here, each vector represents
    a single character, in a sequence of 40 characters. It has 66 dimensions, as we
    have one-hot-encoded each character as a vector of zeros, with `1` in the index
    position of that character from our dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Statistics of character modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We often distinguish words and numbers as being in different realms. As it happens,
    they are not so far apart. Everything can be deconstructed using the universal
    language of mathematics. This is quite a fortunate property of our reality, not
    just for the pleasure of modeling statistical distributions over sequences of
    characters. However, since we are on the topic, we will go ahead and define the
    concept of language models. In essence, language models follow Bayesian logic
    that relates the probability of posterior events (or tokens to come) as a function
    of prior occurrences (tokens that came). With such an assumption, we are able
    to construct a feature space corresponding to the statistical distribution of
    words over a period of time. The RNNs we will build shortly will each construct
    a unique feature space of probability distributions. Then, we are able to feed
    it a sequence of characters and recursively generate the next character to come
    using the distribution schemes.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling character-level probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In **natural language processing** (**NLP**), the unit of a string is denoted
    as a token. Depending on how you wish to preprocess your string data, you can
    have word tokens or character tokens. We will be working with character tokens
    for the purpose of this example, as our training data is set up to make our network
    predict a single character at a time. Hence, given a sequence of characters, our
    network will output a Softmax probability score for each of the characters in
    our vocabulary of characters. In our case, we initially had 66 total characters
    in Shakespeare''s Hamlet. These included uppercase and lowercase letters, which
    are quite redundant for the task at hand. Hence, to increase our efficiency and
    keep track of less Softmax scores, we will reduce our training vocabulary by converting
    the Hamlet text into lowercase, leaving us with 44 characters. This means that,
    at each network prediction, it will generate a 44-way Softmax output. We can take
    the character with the maximum score (that is, do some greedy sampling) and add
    it to the input sequence, then ask our network what it thinks should come next.
    RNNs are able to learn the general structure of words in the English language,
    as well as punctuation and grammar rules, and even have a flair for inventing
    novel sequences, ranging from cool sounding names to possibly life-saving molecular
    compounds, depending on what sequence you decide to feed it. In fact, RNNs have
    been shown to capture the syntax of molecular representations and can be fine-tuned
    to generate specific molecular targets. This helps researchers considerably in
    tasks such as drug discovery and is a vivid area of scientific research. For further
    reading, check out the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.ncbi.nlm.nih.gov/pubmed/29095571](https://www.ncbi.nlm.nih.gov/pubmed/29095571)'
  prefs: []
  type: TYPE_NORMAL
- en: Sampling thresholds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be able to generate sequences of Shakespeare-like sentences, we need to devise
    a mannerism to sample our probability distributions. These probability distributions
    are represented by our model's weights and continuously change at successive time
    steps during the training process. Sampling these distributions is akin to peeking
    into the network's idea of Shakespearean text at the end of each training epoch.
    We are essentially using the probability distributions that have been learned
    by our model to generate a sequence of characters. Moreover, depending on the
    sampling strategy we choose, we could potentially introduce some controlled randomness
    in our generated text to force our model to come up with some novel sequences.
    This can result in interesting formulations and is quite entertaining in practice.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of controlling stochasticity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main concept behind sampling is how you choose control stochasticity (or
    randomness) in selecting the next character from the probability distributions
    for possible characters to come. Various applications may ask for different approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Greedy sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are trying to train an RNN for automatic text completion and correction,
    you will probably be better off going with a greedy sampling strategy. This simply
    means that, at each sampling step, you will choose the next character in the sequence
    based on the character that was attributed the highest probability distribution
    by our Softmax output. This ensures that your network will output predictions
    that likely correspond to words you most commonly use. On the other hand, you
    may want to try a more stratified approach when training an RNN to generate cool
    names, handwriting in a particular person's style, or even producing undiscovered
    molecular compounds. In this case, you wouldn't want to choose the most likely
    characters to come, as this is simply boring. We can instead introduce some controlled
    randomness (or stochasticity) by picking out the next character in a probabilistic
    manner, rather than a fixed one.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One approach could be, instead of simply choosing the next character based
    on the Softmax output values, to reweight the probability distribution of these
    output values at a given time step. This lets us do things such as assign a proportional
    probability score for any of the characters of our vocabulary to be chosen next.
    As an example, suppose a given character has an assigned probability of 0.25 to
    be the next character in the sequence. We will then choose it one out of four
    times as the next character. In this manner, we are able to systematically introduce
    a little randomness, which gives rise to creative and realistic, albeit artificial
    words and sequences. Playing around by introducing randomness can often be usefully
    informative in the realm of generative modeling, as we will see in later chapters.
    For now, we will implement the controlled introduction of randomness in our sampling
    strategy by introducing a sampling threshold, which lets us redistribute the Softmax
    prediction probabilities of our model, [https://arxiv.org/pdf/1308.0850.pdf](https://arxiv.org/pdf/1308.0850.pdf):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This threshold denotes the entropy of the probability distribution we will be
    using, to sample a given generation from our model. A higher threshold will correspond
    to higher entropy distributions, leading to seemingly unreal and less structured
    sequences. Lower thresholds, on the other hand, will plainly encode English language
    representations and morphology, generating familiar words and terms.
  prefs: []
  type: TYPE_NORMAL
- en: Testing different RNN models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our training data preprocessed and ready in tensor format,
    we can try a slightly different approach than previous chapters. Normally, we
    would go ahead and build a single model and then proceed to train it. Instead,
    we will construct several models, each reflecting a different RNN architecture,
    and train them successively to see how each of them do at the task of generating
    character-level sequences. In essence, each of these models will leverage a different
    learning mechanism and induct its proper language model, based on sequences of
    characters it sees. Then, we can sample the language models that are learned by
    each network. In fact, we can even sample our networks in-between training epochs
    to see how our network is doing at generating Shakespearean phrases at the level
    of each epoch. Before we continue to build our networks, we must go over some
    basic strategies to inform our task of language modeling and sampling. Then, we
    will build some Keras callbacks that let us interact with and sample our model
    while it trains.
  prefs: []
  type: TYPE_NORMAL
- en: Using custom callbacks to generate text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will construct a custom Keras callback that will allow us to use the
    sample function we just constructed to iteratively probe our model at the end
    of each training epoch. As you will recall, callbacks are a class of functions
    that allow operations to be performed on our model (such as saving and testing)
    during the training process. These are very useful functions to visualize how
    a model performs throughout the training process. Essentially, this function will
    take a random sequence of characters from the Hamlet text and then generate 400
    characters to follow on, starting from the given input. It does this for each
    of the five sampling thresholds chosen and prints out the generated results at
    the end of each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Testing multiple models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last task on our list is to build a helper function that will train, sample,
    and save a list of RNN models. This function also saves the history objects of
    the model that we used earlier to plot out the loss and accuracy values per epoch,
    which can be useful in case you want to explore different models and their relative
    performances at a later time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can finally proceed to the task of constructing several types of RNNs
    and training them with the helper function to see how different types of RNNs
    perform at generating Shakespeare-like texts.
  prefs: []
  type: TYPE_NORMAL
- en: Building a SimpleRNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The SimpleRNN model in Keras is a basic RNN layer, like the ones we discussed
    earlier. While it has many parameters, most of them are set with excellent defaults
    that will get you by for many different use cases. Since we have initialized the
    RNN layer as the first layer of our model, we must pass it an input shape, corresponding
    to the length of each sequence (which we chose to be 40 characters earlier) and
    the number of unique characters in our dataset (which was 44). While this model
    is computationally compact to run, it gravely suffers from the vanishing gradients
    problem we spoke of. As a result, it has some trouble modeling long-term dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that this two-layered model has a final dense layer with a number of neurons
    corresponding to each of the 44 unique characters in our dataset. We equip it
    with a Softmax activation function, which will output a 44-way probability score
    at each time step, corresponding to the likelihood of each character to follow.
    All of the models we build for this experiment will have this final dense layer
    in common. Finally, all RNNs have the ability to remain stateful. This simply
    refers to passing on the layer weights for computations on the subsequent sequences
    of our training data. This feature can be explicitly set in all RNNs, with the
    `stateful` argument, which takes a Boolean value and can be provided when initializing
    a layer.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking RNN layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Why have one, when you can have two? All of the recurrent layers in Keras can
    return two different types of tensors, depending on what you wish to accomplish.
    You could either receive a 3D tensor of dimensions as output (`batch_size`, `time_steps`,
    `output_features`) or simply a 2D one, with dimensions of (`time_steps`, `output_features`).
    We query the 3D tensor if we want our model to return entire sequences of successive
    output values on each time step. This is useful if we want to stack an RNN layer
    on top of another, and then ask the first layer to return all of the activations
    to the second layer that''s stacked. Returning all activations essentially means
    returning the activation for each specific time step. These values can be subsequently
    fed into yet another recurrent layer, which aims to encode higher level abstract
    representations from the same input sequence. The following diagram shows the
    mathematical consequences of setting the Boolean argument to **True** or **False**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03ea7c75-466e-4209-b81f-49a95778f42d.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting it to true will simply return a tensor with predictions for each time
    step, instead of the prediction from the last time step only. The stacking of
    recurrent layers is quite useful. By stacking RNN layers on top on one another,
    we potentially increase the time-dependent representational value of our network,
    allowing it to memorize more abstract patterns that are potentially present in
    our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if we want it to only return the output at the last time
    step for each input sequence, we can ask it to return a 2D tensor. This is necessary
    when we want to go ahead and actually predict which of the characters from our
    vocabulary is most likely to be next. We can control this implementation with
    the `return_sequences` argument, which is passed when we add a recurrent layer.
    Alternatively, we can set it to false, making our model return the activation
    values from the last time step only, which can be propagated forward for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `return_sequences` argument can only be invoked for the penultimate
    hidden layers, and not for the hidden layer preceding the densely connected output
    layer, since the output layer is only tasked with classifying the next sequence
    to come.
  prefs: []
  type: TYPE_NORMAL
- en: Building GRUs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Excellent at mitigating the vanishing gradients problem, the GRU is a good
    choice for modeling long-term dependencies such as grammar, punctuation, and word
    morphology:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like the SimpleRNN, we define the dimensions of the input at the first
    layer and return a 3D tensor output to the second GRU layer, which will help retain
    more complex time-dependent representations that are present in our training data.
    We also stack two GRU layers on top of each other to see what the increased representational
    power of our model produces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc2f701b-6f1c-424b-a062-39703e28e64e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hopefully, this architecture results in realistic albeit novel sequences of
    text that even a Shakespeare expert couldn''t tell apart from the real deal. Let''s
    visualize the model we built here through the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have also included the line `model.summary()` in our trainer function
    we built earlier to visually depict the structure of the model after it is fit.
  prefs: []
  type: TYPE_NORMAL
- en: Building bi-directional GRUs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next in our models to test is yet another GRU unit, but this time with a twist.
    We nest it within a bi-directional layer, which allows us to feed our model each
    sequence in both the normal and the reverse order. In this manner, our model is
    able to *see* what is yet to come, leveraging future sequence data to inform predictions
    at the current time step. The nature of processing a sequence in a bi-directional
    manner greatly enhances the extracted representations from our data. In fact,
    the order of processing a sequence can have a significant effect on the type of
    representations that are learned after.
  prefs: []
  type: TYPE_NORMAL
- en: On processing reality sequentially
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The notion of changing the order of processing a sequence is quite an intriguing
    one. We humans certainly seem to prefer a certain order of learning things over
    another. The second sentence that''s been reproduced in the following image simply
    makes no sense to us, even though we know exactly what each individual word within
    the sentence means. Similarly, many of us have a hard time reciting the letters
    of the alphabet backward, even though we are extremely familiar with each letter,
    and compose much more complex concepts with them, such as words, ideas, and even
    Keras code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a56a47a-a34a-4dc3-9911-acdd412050a6.png)'
  prefs: []
  type: TYPE_IMG
- en: It is very likely that our sequential preferences have to do with the nature
    of our reality, which is sequential and forward-moving by definition. At the end
    of the day, the configuration of the 10^(11) neurons in our brain has been engineered
    by time and natural forces to best encode and represent the deluge of time-dependent
    sensory signals we come across, every living second of our lives. It stands to
    reason that our own neural architecture efficiently implements a mechanism that
    tends to prefer processing signals in a specific order. However, that is not to
    say that we cannot part ways with a learned order, as many pre-school kids take
    up the challenge of reciting the alphabet backward and do so quite successfully.
    Other sequential tasks such as listening to natural language or rhythmic music,
    however, may be harder to process in reverse order. But don't take my word for
    it. Try listening to your favorite song in reverse and see whether you still like
    it as much.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of re-ordering sequential data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In some manner, it seems that bi-directional networks are able to potentially
    overcome our own biases in processing information. As you will see, they can learn
    equally useful representations that we would have otherwise not thought to include
    to inform and enhance our predictions. It all depends on how important it is to
    process a given signal, in its sequential order, for the task at hand. In the
    case of our earlier natural language example, this was quite crucial to determine
    the **part-of-speech** (**POS**) tag for the word *Spartan*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/259ac156-568d-4eb2-938c-498308c58d7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Bi-directional layer in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Therefore, the bi-directional layer in Keras processes a sequence of data in
    both the normal and reverse sequence, which allows us to pick up on words that
    come later on in the sequence to inform our prediction at the current time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, the bi-directional layer duplicates any layer that''s fed to it
    and uses one copy to process information in the normal sequential order, while
    the other processes data in the reverse order. Pretty neat, no? We can intuitively
    visualize what a bi-directional layer actually does by going through a simple
    example. Suppose you were modeling the two-word sequence **Whats up**, with a
    bi-directional GRU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/179d4b85-f770-4bd3-bfb2-bfeb4ba5e7b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To do this, you will nest the GRU in a bi-directional layer, which allows Keras
    to generates two versions of the bi-directional model. In the preceding image,
    we stacked two bi-directional layers on top of each other before connecting them
    to a dense output layer, as we did previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The model that processes the sequence in the normal order is shown in red.
    Similarly, the blue model processes the same sequence in reverse order. Both of
    these models collaborate at each time step to produce a predicted output, with
    respect to the current time step. We can see how these two models receive input
    values and work together to produce the predicted output (![](img/5a07aa4f-cdcc-4d84-96af-8024ce385aa7.png),
    which corresponds to the two respective time steps of our input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/353bfcae-f247-43a6-a7bb-b9699b84c1fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The equations governing the forward propagation of information can be altered
    slightly to account for data entering our RNN, from both the forward and reverse
    sequence layers, at each time step. The backward propagation of errors through
    time is still achieved in the same manner and is done for each orientation of
    GRU layer (red and blue). In the following formulation, we can see how activations
    from both the forward and reverse sequence layers are used to compute a predicted
    output (![](img/5c108a8b-2fe1-40e2-a65a-75cbbc5f1ba6.png)) for a given time step
    (*t*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d977fa08-9d31-4a09-a9f0-43130a7c0a7c.png)'
  prefs: []
  type: TYPE_IMG
- en: The activation and weight matrices here are simply defined by the model nested
    within the bi-directional layer. As we saw earlier, they will be initialized at
    the first time step and updated by backpropagating errors through time. Hence,
    these are the implemented processes that let us generate a bi-directional network,
    which is an acyclical network where predictions are informed by information flowing
    both forward and backward, corresponding to the ordering of the sequence. One
    key disadvantage with implementing the bi-directional layer is that our network
    needs to see the entire sequence of data before it is able to make a prediction.
    In use cases such as speech recognition, this becomes problematic, since we must
    ensure that the target has ceased to speak before we perform our predictions to
    classify each sound byte as a word. One way to solve this is to keep performing
    predictions on an input sequence iteratively, and update the previous prediction
    iteratively as new information flows in.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing recurrent dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In earlier chapters, we saw how we can drop out the prediction of a few neurons
    randomly to better distribute representations over our network and avoid the problem
    of overfitting. While our current task at hand does not have much of a negative
    consequence in regards to overfitting, we could not help but briefly introduce
    the specific case of mitigating overfitting in RNNs. This will help our model
    better generate novel sequences, instead of copy-pasting segments from the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, adding a normal dropout layer here just doesn''t do the trick. It
    introduces too much randomness. This often prohibits our model from converging
    to ideal loss values and encoding useful representations. Instead, we may find
    a confused model that fails to keep track of relevant time-dependent data. What
    does seem to work, on the other hand, is the notion of applying the same dropout
    scheme (or mask) at each time step. This is different from the classic dropout
    operation, which drops neurons on a random basis for each time step. We can use
    this recurrent dropout technique to capture regularized representations, since
    a constant dropout mask is maintained through time. This is one of the most significant
    techniques that helps to prevent overfitting in recurrent layers and is known
    as a **recurrent dropout strategy**. Doing so essentially permits our model to
    representatively encode sequential data without losing valuable information via
    the randomized dropout process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The designers of Keras have kindly implemented two dropout-related arguments
    that may be passed when constructing a recurrent layer. The `recurrent_dropout`
    argument accepts a float value that refers to the fraction of neurons upon which
    the same dropout mask will be applied. You can also specify the fraction of input
    values entering a recurrent layer to be randomly dropped to control random noise
    in the data. This can be achieved by passing a float value to the dropout argument
    (different from `recurrent_dropout`), while defining the RNN layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'For reference you can read the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A Theoretically Grounded Application of Dropout in Recurrent Neural Networks**: [https://arxiv.org/pdf/1512.05287](https://arxiv.org/pdf/1512.05287.pdf)[.pdf](https://arxiv.org/pdf/1512.05287.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://mlg.eng.cam.ac.uk/yarin/blog_2248.html](http://mlg.eng.cam.ac.uk/yarin/blog_2248.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing output values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the sake of entertainment, we will display some of the more interesting
    results from our own training experiments to conclude this chapter. The first
    screenshot shows the output that's generated by our SimpleRNN model at the end
    of the first epoch (note that the output prints out the first epoch as epoch 0).
    This is simply an implementational issue, denoting the first index position in
    range of *n* epochs. As we can see, even after the very first epoch, the SimpleRNN
    seems to have picked up on word morphology and generates real English words at
    low sampling thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is just as we expected. Similarly, higher entropy samples (with a threshold
    of 1.2, for example) produce more stochastic results and generate (from a subjective
    perspective) interesting sounding words (such as *eresdoin*, *harereus*, and *nimhte*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76592355-55cd-4b81-ab60-f20f772b6127.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing the output of heavier GRU models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following screenshot, we present the output from our heavier GRU model,
    which started to produce pretty Shakespeare-sounding strings only after two training
    epochs. It even throws in Hamlet''s name here and there. Note that the loss of
    your network is not the best assessment metric for the purpose of our illustration.
    The models that are shown here had a loss of 1.3, which is still pretty far from
    what we would normally require. You may, of course, keep training your model to
    produce even more comprehensible bits of Shakespeare. However, comparing the performance
    of any model with the loss metric is akin to judging apples and oranges in this
    use case. Intuitively, having reached a loss close to zero simply means that the
    model has memorized Shakespeare''s Hamlet, and won''t really generate novel sequences
    as we want it to do. At the end of the day, you shall remain the best judge of
    its performance for generative tasks such as this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7fa7810-3b8f-4212-a42a-c23aae919ec4.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about recurren6t neural networks and their aptness
    at processing sequential time-dependent data. The concepts that you have learned
    can now be applied to any time-series dataset that you may stumble upon. While
    this holds true for use cases such as stock market data and time-series in nature,
    it would be unreasonable to expect fantastic results from feeding your network
    real time price changes only. This is simply because the elements that affect
    the market price of stocks (such as investor perception, information networks,
    and available resources) are not nearly reflected to the level that would allow
    proper statistical modeling. The key is representing all relevant information
    in the most *learnable* manner possible for your network to successfully encode
    valuable representations therefrom.
  prefs: []
  type: TYPE_NORMAL
- en: While we did extensively explore the learning mechanisms behind several types
    of RNNs, we also implemented a generative modeling use case in Keras and learned
    to construct custom callbacks that let us generate sequences of data at the end
    of each epoch. Due to spatial limitations, we were forced to leave out some concepts
    from this chapter on RNNs. However, rest assured, these are yet to be elaborated
    upon in the upcoming chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will learn more about a very popular RNN architecture
    known as the **LSTM network**, and implement it for other exciting use cases.
    These networks are as versatile as RNNs get, and allow us to produce very detailed
    statistical models of languages for use cases such as speech and entity recognition,
    translation, and machine question-answering. For natural language understanding,
    LSTMs (and other RNNs) are often implemented by leveraging concepts such as word
    embeddings, which are dense word vectors that are capable of encoding their semantic
    meaning. LSTMs also tend to do much better at generating novel sequences such
    as pieces of music, but hopefully you will be able to listen for yourself. We
    will also explore the intuition behind attention models briefly and revisit this
    concept in more detail, in a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, before concluding this chapter, we will note a similarity between RNNs
    and a type of CNN we mentioned in earlier chapters. RNNs are a popular choice
    when modeling time series data, yet **one-dimensional convolutional layers** (**Conv1D**)
    also do the trick. The drawback here comes from the fact that CNNs process input
    values independently, not sequentially. As we will see, we can even overcome this
    by combining both convolutional and recurrent layers. This lets the former perform
    a sort of preprocessing on the input sequence before reduced representations are
    passed forward to the RNN layer for sequential processing. But more on that later.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GRUs**: [https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neural machine translation**: [https://arxiv.org/abs/1409.1259](https://arxiv.org/abs/1409.1259)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Train each model on the Hamlet text and use their history objects to compare
    their relative losses. Which one converges faster? What do they learn?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examine the samples that are generated at different entropy distributions, at
    each epoch, to see how each RNN improves upon its language model through time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
