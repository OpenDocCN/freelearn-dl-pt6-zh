<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deploying Models to Production</h1>
                </header>
            
            <article>
                
<p>In the previous chapters of this book, we've worked on our skills for developing, testing, and using various deep learning models. We haven't talked much about the role of deep learning within the broader context of software engineering. In this last chapter, we will use the time to talk about continuous delivery, and the role of machine learning within this context. We will then look at how you can deploy models to production with a continuous delivery mindset. Finally, we will look at Azure Machine Learning service to properly manage the models you develop.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Using machine learning in a DevOps environment</li>
<li>Storing models</li>
<li>Using Azure Machine Learning service to manage models</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>We assume that you have a recent version of Anaconda installed on your computer, and have followed the steps in <a href="9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml">Chapter 1</a>, <em>Getting Started with CNTK</em>, to install CNTK on your computer. The sample code for this chapter can be found in our</span><span> G</span>itHub<span> </span><span>repository at: <a href="https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch7">https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch7</a></span>.</p>
<p>In this chapter, we'll work on a few examples stored in Jupyter notebooks. To access the sample code, run the following commands inside an Anaconda prompt in the directory where you've downloaded the code:</p>
<pre><strong>cd ch7</strong><br/><strong>jupyter notebook</strong></pre>
<p>This chapter also contains a C# code sample to demonstrate how to load models in the open source ONNX format. If you want to run the C# code you will need to have .NET Core 2.2 installed on your machine. You can download the latest version of .NET core from: <a href="https://dotnet.microsoft.com/download">https://dotnet.microsoft.com/download</a>.</p>
<p class="mce-root">Check out the following video to see the code in action:</p>
<p><a href="http://bit.ly/2U8YkZf">http://bit.ly/2U8YkZf</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using machine learning in a DevOps environment</h1>
                </header>
            
            <article>
                
<p>Most modern software development happens in an agile fashion, in an environment where developers and IT-pros work on the same project. The software we're building often is deployed to production through continuous integration and continuous deployment pipelines. How are we going to integrate machine learning in this modern environment? And does it mean we have to change a lot when we start building AI solutions? These are some of the frequently asked questions you can run into when you introduce AI and machine learning to the workflow.</p>
<p>Luckily, you don't have to change your whole build environment or deployment tool stack to integrate machine learning into your software. Most of the things that we'll talk about will fit right into your existing environment.</p>
<p>Let's take a look at a typical continuous delivery scenario that you may encounter in a regular agile software project:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-632 image-border" src="assets/31eff2da-92c2-470c-8e45-8b98931548be.png" style=""/></div>
<p>This overview will look familiar if you've worked in a DevOps environment before. It starts with source control, which is connected to a continuous integration pipeline. The continuous integration pipeline produces artifacts that can be deployed to production. These artifacts are typically stored somewhere for backup and rollback purposes. This artifact repository is connected to a release pipeline that deploys the software to a test, acceptance, and, finally, a production environment.</p>
<p>You don't need to change much of this standard setup to integrate machine learning into it. There are, however, a few key things that are important to get right when you start to use machine learning. Let's focus on four stages and explore how to extend the standard continuous delivery setup:</p>
<ul>
<li>How to keep track of the data you use for machine learning.</li>
<li>Training models in a continuous integration pipeline.</li>
<li>Deploying models to production.</li>
<li>Gathering feedback on production.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keeping track of your data</h1>
                </header>
            
            <article>
                
<p>Let's start where it all begins with machine learning: the data with which you are going to train your models. It's difficult to get good data for machine learning. Almost 80% of your effort will be on data management and data processing. It would be really sad if you had to redo all your work every time you want to train a model.</p>
<p>That's why it is important to have some form of data management in place. This can be a central server where you store datasets that you know are good to use for training models. It could also be a data warehouse, if you have more than a few gigabytes of data. Some companies choose to use tools such as Hadoop or Azure Data Lake to manage their data. Whatever you use, the most important thing is to keep your dataset clean and in a format that is ready to use for training. </p>
<p>To create a data pipeline for your solution you can use traditional <strong>Extract</strong> <strong>Transform </strong><strong>Load </strong>(<strong>ETL</strong>) tools, such as SQL server integration services, or you can build custom scripts in Python and execute them as part of a dedicated continuous integration pipeline in Jenkins, Azure DevOps, or Team Foundation Server.</p>
<p>The data pipeline will be your tool to gather data from various business sources, and process it so that you get a dataset that is of sufficient quality to be stored as the master dataset for your model. It's important to note here that, although you can reuse datasets across different models, it is best not to start out with this goal in mind. You will quickly find that your master dataset will turn dirty and unmanageable when you try to use the dataset across too many usage scenarios.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training models in a continuous integration pipeline</h1>
                </header>
            
            <article>
                
<p>Once you have a basic data pipeline running, it's time to look at integrating the training of AI models in your continuous integration environment. Up until now, we've only used Python notebooks to create our models. Sadly, Python notebooks don't deploy well to production. You can't automatically run them during a build.</p>
<p>In a continuous delivery environment, you can still use Python notebooks to perform initial experiments in order to discover patterns in the data and to build an initial version of your model. Once you have a candidate model, you will have to move your code away from a notebook and into a proper Python program.</p>
<p>You can run your Python training code as part of a continuous integration pipeline. For example, if you're using Azure DevOps, Team Foundation Server, or Jenkins, you already have all the tools to run your training code as a continuous integration pipeline. </p>
<p>We recommend running the training code as a separate pipeline from the rest of your software. Training a deep learning model often takes a very long time, and you don't want to lock your build infrastructure on that. Often, you will see people build training pipelines for their machine learning models using dedicated virtual machines, or even dedicated hardware, because of the amount of computation power it takes to train a model.</p>
<p>The continuous integration pipeline will produce a model based on a dataset you produced using your data pipeline. Just like code, you should also version your models and the settings you've used to train them.</p>
<p>Keeping track of your models and the settings you used to train them is important, as this allows you to experiment with different versions of the same model in production and gather feedback. Keeping a backup of your trained models also helps to get back in production fast after a disaster, such as a crashed production server.</p>
<p>Since models are binary files and can get quite large, it's best you treat your models as binary artifacts, much like NuGet packages in .NET, or Maven artifacts in Java.</p>
<p>Tools like Nexus or Artifactory are great for storing models. Publishing your models in Nexus or Artifactory takes only a few lines of code, and will save you up to hundreds of hours of work retraining your model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying models to production</h1>
                </header>
            
            <article>
                
<p>Once you have a model, you need to be able to deploy it to production. If you've stored your models in a repository, such as Artifactory or Nexus, this becomes easier. You can create dedicated release pipelines in the same way that you would create a continuous integration pipeline. In Azure DevOps and Team Foundation Server, there's a dedicated feature for this. In Jenkins, you can use a separate pipeline to deploy models to a server. </p>
<p>In the release pipeline, you can download your model from the artifact repository and deploy it to production. There are two main deployment methods for machine learning models. Either you can deploy it as an extra file with your application, or you can deploy it as a dedicated service component.</p>
<p>If you're deploying your model as part of an application, you will typically store just the model in your artifact repository. The model now becomes an extra artifact that needs to be downloaded in an existing release pipeline that deploys your solution.</p>
<p>If you're deploying a dedicated service component for your model, you will typically store the model, the scripts that use the model to make a prediction, and other files needed by the model, in the artifact repository and deploy that to production.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gathering feedback on your models</h1>
                </header>
            
            <article>
                
<p>There's one last point that is important to keep in mind when working with deep learning or machine learning models in production. You've trained the models with a certain dataset. You hope this dataset contains a good representation of what is really happening in your production environment. But it doesn't have to be that way, because the world changes around you as you build your models.</p>
<p>That's why it is important to ask for feedback from your users and update your model accordingly. Although not officially part of a continuous deployment environment, it's still an important aspect to set up correctly if you want to be successful with your machine learning solution.</p>
<p>Setting up a feedback loop doesn't have to be very complicated. For example, when you're classifying transactions for fraud detection, you can set up a feedback loop by asking an employee to validate the outcome of the model. You can then store the validation result of the employee with the input that was classified. By doing this, you make sure your model doesn't falsely accuse customers of fraud, and it helps you gather new observations to improve your model. Later, when you want to improve the model, you can use the newly gathered observations to extend your training set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storing your models</h1>
                </header>
            
            <article>
                
<p>In order to be able to deploy your models to production, you need to be able to store a trained model on disk. CNTK offers two ways to store models on disk. You can either store checkpoints to continue training at a later time, or you can store a portable version of your model. Each of these storage methods has its own use. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storing model checkpoints to continue training at a later point</h1>
                </header>
            
            <article>
                
<p>Some models take a long time to train, sometimes up to weeks at a time. You don't want to lose all your progress when your machine crashes during training, or if there's a power outage. </p>
<p>This is where checkpointing becomes useful. You can create a checkpoint during training using a <kbd>CheckpointConfig</kbd> object. You can add this additional callback to your training code by modifying the callbacks list as follows:</p>
<pre>checkpoint_config = CheckpointConfig('solar.dnn', frequency=100, restore=True, preserve_all=False)<br/><br/>history = loss.train(<br/>    train_datasource, <br/>    epoch_size=EPOCH_SIZE,<br/>    parameter_learners=[learner], <br/>    model_inputs_to_streams=input_map,<br/>    callbacks=[progress_writer, test_config, checkpoint_config],<br/>    minibatch_size=BATCH_SIZE,<br/>    max_epochs=EPOCHS)</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, create a new <kbd>CheckpointConfig</kbd> and provide a filename for the checkpointed model file, the number of minibatches before a new checkpoint should be created as the <kbd>frequency</kbd> and set the <kbd>preserve_all</kbd> setting to <kbd>False</kbd>.</li>
<li>Next, use the train method on the <kbd>loss</kbd> and provide the <kbd>checkpoint_config</kbd> in the <kbd>callbacks</kbd> keyword argument to use checkpointing.</li>
</ol>
<p>When you use checkpointing during training, you will start to see additional files on disk named <kbd>solar.dnn</kbd><span>, and</span> <kbd>solar.dnn.ckp</kbd><span>. The</span> <kbd>solar.dnn</kbd><span> file contains the trained model stored in a binary format. The</span> <kbd>solar.dnn.ckp</kbd><span> file contains the checkpoint information for the minibatch source used during training.</span></p>
<p>The most recent checkpoint is automatically restored for you when you set the restore parameter of the <kbd>CheckpointConfig</kbd> object to <kbd>True</kbd>. This makes it easy to integrate checkpointing in your training code. </p>
<p>Having a checkpointed model is not only useful in case you run into a computer problem during training. A checkpoint is also useful if you want to continue training after you've gathered additional data from production. You can simply restore the latest checkpoint and start feeding new samples into the model from there.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storing portable models for use in other applications</h1>
                </header>
            
            <article>
                
<p class="mce-root">Although you can use a checkpointed model in production, it's not very smart to do so. Checkpointed models are stored in a format that is only understood by CNTK. For now, it's fine to use the binary format, since CNTK is around and the model format will remain compatible for quite a long time. But, as with all software, CNTK isn't made to last for an eternity.</p>
<p>That's exactly why ONNX was invented. ONNX is the open neural network exchange format. When you use ONNX, you store your model in a protobuf compatible format that is understood by many other frameworks. There's even a native ONNX runtime available for Java and C#, which allows you to use models created in CNTK from your .NET or Java application.</p>
<p>ONNX is supported by a number of large companies, such as Facebook, Intel, NVIDIA, Microsoft, AMD, IBM, and Hewlett-Packard. Some of these companies offer converters for ONNX, while others even support running ONNX models directly on their hardware without using additional software. NVIDIA has a number of chips available now that can read ONNX files directly and execute these models.</p>
<p>As an example, we'll first explore how to store a model in the ONNX format and use C# to load it from disk again to make predictions. First, we'll look at how to save a model in the ONNX format and after that we'll explore how to load ONNX models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storing a model in ONNX format</h1>
                </header>
            
            <article>
                
<p>To store a model in the ONNX format you can use the <kbd>save</kbd> method on the <kbd>model</kbd> function. When you don't provide any additional parameters, it will store the model in the same format as is used for checkpointing. You can, however, provide an additional parameter to specify the model format as follows:</p>
<pre>from cntk import ModelFormat<br/><br/>model.save('solar.onnx', format=ModelFormat.ONNX)</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, import the <kbd>ModelFormat</kbd> enumeration from the <kbd>cntk</kbd> package.</li>
<li>Next, invoke the <kbd>save</kbd> method on the trained model with the output filename and specify <kbd>ModelFormat.ONNX</kbd> as the <kbd>format</kbd> keyword argument. </li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using ONNX models in C#</h1>
                </header>
            
            <article>
                
<p>Once the model is stored on disk, we can use C# to load and use it. CNTK version 2.6 includes a pretty complete API for C#, which you can use for training and evaluating models. </p>
<p>To use a CNTK model in C# you need to use a library called <kbd>CNTK.GPU</kbd> or <kbd>CNTK.CPUOnly</kbd>, which can be retrieved from NuGet, a package manager for .NET. The CPU-only version of CNTK includes a version of the CNTK binaries that have been compiled to run models on the CPU, while the GPU version can use the GPU as well as the CPU. </p>
<p>Loading a CNTK model in C# is done by using the following snippet of code:</p>
<div>
<pre><span>var</span><span> </span><span>deviceDescriptor</span><span> </span><span>=</span><span> </span><span>DeviceDescriptor</span><span>.</span><span>CPUDevice</span><span>;<br/></span><span>var</span><span> </span><span>function</span><span> </span><span>=</span><span> </span><span>Function</span><span>.</span><span>Load</span><span>(</span><span>"model.onnx"</span><span>, </span><span>deviceDescriptor</span><span>, </span><span>ModelFormat</span><span>.</span><span>ONNX</span><span>);</span></pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li><span>First, create a device descriptor so the model is executed against the CPU.</span></li>
<li><span>Next, use </span><span>the </span><kbd>Function.Load</kbd><span> method to load the previously stored model. Provide the <kbd>deviceDescriptor</kbd> and use the <kbd>ModelFormat.ONNX</kbd> to load the file as ONNX model. </span></li>
</ol>
</div>
<p>Now that we have loaded <span>the model</span>, let's make a prediction with it. For, this we need to write another fragment of code:</p>
<pre>public IList&lt;float&gt; Predict(float petalWidth, float petalLength, float sepalWidth, float sepalLength)<br/>{<br/>    var features = _modelFunction.Inputs[0];<br/>    var output = _modelFunction.Outputs[0];<br/><br/>    var inputMapping = new Dictionary&lt;Variable, Value&gt;();<br/>    var outputMapping = new Dictionary&lt;Variable, Value&gt;();<br/><br/>    var batch = Value.CreateBatch(<br/>        features.Shape,<br/>        new float[] { sepalLength, sepalWidth, petalLength, petalWidth },<br/>        _deviceDescriptor);<br/><br/>    inputMapping.Add(features, batch);<br/>    outputMapping.Add(output, null);<br/><br/>    _modelFunction.Evaluate(inputMapping, outputMapping, _deviceDescriptor);<br/><br/>    var outputValues = outputMapping[output].GetDenseData&lt;float&gt;(output);<br/>    return outputValues[0];<br/>}</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>Create a new method  <kbd>Predict</kbd> that accepts the input features for the model. </li>
<li>Within the <kbd>Predict</kbd> method, store the input and output variable of the model in two separate variables for easy access.</li>
<li>Next, create a dictionary to map data to the input and output variables of the model.</li>
<li>Then, create a new batch, containing one sample with the input features for the model.</li>
<li>Add a new entry to the input mapping to map the batch to the input variable.</li>
<li>Next, add a new entry to the output mapping for the output variable.</li>
<li>Now, invoke the <kbd>Evaluate</kbd> method on the loaded model with the input, output mapping, and a device descriptor. </li>
<li>Finally, extract the output variable from the output mapping and retrieve the data. </li>
</ol>
<p>The sample code for this chapter includes a basic C# project in .NET core that demonstrates the use of CNTK from a .NET Core project. You can find the sample code in the <kbd>csharp-client</kbd> folder in the code examples directory for this chapter.</p>
<p>Working with models stored in the ONNX format makes it possible to use Python for training models and C# or another language to run models on production. This is especially useful since the runtime performance of a language like C# is much better than that of Python.</p>
<p>In the next section we'll look at using Azure Machine Learning service to manage the process of training and storing models so we have a much more structured way of working with models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Azure Machine Learning service to manage models</h1>
                </header>
            
            <article>
                
<p>While you can completely hand-build a continuous integration pipeline, it's still quite a bit of work. You need to get dedicated hardware to run deep learning training jobs, and that can bring up the costs. There are great alternatives available in the cloud. Google has a TensorFlow serving offer. Microsoft offers Azure Machine Learning service as a way to manage models. Both are great tools that we can highly recommend.</p>
<p>Let's take a look at Azure Machine Learning service to get a sense of what it can do for you when you want to set up a complete machine learning pipeline:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-633 image-border" src="assets/931612d7-2dae-4252-b2ba-0ea9d6bf7d19.png" style=""/></div>
<p>Azure Machine Learning service is a cloud service that offers a complete solution for every phase of your machine learning project. It has the concept of experiments, and runs that allow you to manage experiments. It features a model registry that allows you to store trained models and Docker images for those models. You can use the Azure Machine Learning service tools to deploy these models to production in a matter of minutes. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying Azure Machine Learning service</h1>
                </header>
            
            <article>
                
<p>In order to use this service, you need to have an active account on Azure. You can use a trial account by going to: <a href="https://azure.microsoft.com/en-gb/free/">https://azure.microsoft.com/en-gb/free/</a>, if you haven't got an account yet. This will give you a free account for 12 months with 150 dollars, worth of credits to explore the different Azure services.</p>
<p class="mce-root"/>
<p><span>There are many ways in which you can deploy Azure Machine Learning service. You can create a new instance through the portal, but you can also use the cloud shell to create an instance of the service. Let's take a look at how you can create a new Azure Machine Learning service instance through the portal.</span></p>
<p>With your favorite browser, navigate to the URL at: <a href="https://portal.azure.com/">https://portal.azure.com/</a>. Log in with your credentials, and you will be greeted with a portal that shows you all your available Azure resources and a dashboard resembling the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-634 image-border" src="assets/d6be4d9a-532b-43d3-873e-7f298c5fd304.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span><span>Azure resources and a dashboard</span></span></div>
<p>From this portal you can create new Azure resources, such as the Azure Machine Learning Workspace. Click the large <span class="packt_screen">+</span> button at the top left of the screen to get started. This will show the following page, allowing you to create a new resource: </p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-635 image-border" src="assets/d473560b-00c5-4ba2-88ef-2cb881565369.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span><span>Creating a new resource</span></span></div>
<p>You can search for different types of resources in this search bar. Search for <span class="packt_screen">Azure Machine Learning</span> and select the <span class="packt_screen">Azure Machine Learning Workspace</span> resource type from the list. This will show the following details panel that allows you to start the creation wizard:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-636 image-border" src="assets/ea5b88f6-7e13-4d81-8726-59f65d9c68c3.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Starting the creation wizard</div>
<p>This details panel will explain what the resource does, and point towards the documentation and other important information about this resource, such as the pricing details. To create a new instance of this resource type, click the <span class="packt_screen">create</span> button. This will start the wizard to create a new instance of the Azure Machine Learning workspace as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-637 image-border" src="assets/c36e7609-d67b-4cd8-8700-c6d3da70200d.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Creating a new instance of the Azure Machine Learning workspace</span></div>
<p>In the creation wizard, you can configure the name of the workspace, the resource group it belongs to, and the data center it should create. Azure resources are created as part of resource groups. These resource groups help you organize things, and keep related infrastructure together in one place. If you want to remove a set of resources, you can just delete the resource group instead of every resource separately. This is especially useful if you want to remove everything after you're done testing the machine learning workspace.</p>
<p>It's a good idea to use a dedicated resource group for the machine learning workspace, since it will contain more than one resource. Mixing this with other resources will make it harder to clean up after you're done or need to move resources for some reason.</p>
<p class="mce-root"/>
<p>Once you have clicked the <span class="packt_screen">create</span> button at the bottom of the screen, the machine learning workspace is created. This will take a few minutes. In the background, the Azure Resource Manager will create a number of resources based on the selection in the creation wizard. You will receive a notification in the portal when the deployment is completed.</p>
<p>When the machine learning workspace is created, you can navigate to the workspace through the portal by first going to the <span class="packt_screen">R</span><span class="packt_screen">esource groups</span> on the portal in the navigation bar on the left of the screen. Next, click the resource group you just created to get an overview of the machine learning workspace and related resources, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-639 image-border" src="assets/d9c88065-5451-4366-9750-fb73570dbb4b.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Getting an overview of the machine learning workspace and related resources</span></div>
<p>There's the workspace itself, with a dashboard that allows you to explore experiments and manage some aspects of your machine learning solution. The workspace also includes a Docker registry to store models as Docker images, together with the scripts needed to make a prediction using a model. When you check out the workspace on Azure Portal, you'll also find a storage account that you can use to store datasets and data generated by your experiments.</p>
<p>One of the nice things that's included in an Azure Machine Learning service environment is an Application Insights instance. You can use Application Insights to monitor your models in production and gather valuable feedback to improve your models later on. This is included by default, so you don't have to manually create a monitoring solution for your machine learning solution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the machine learning workspace</h1>
                </header>
            
            <article>
                
<p>The Azure Machine Learning workspace contains a number of elements. Let's explore them to get a feel of what's available to you when you start working with it:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-640 image-border" src="assets/560a062f-5309-48c3-a1b7-0f4bdef7b38e.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Machine learning workspace</div>
<p>To get to the machine learning workspace, click the <span class="packt_screen">resource groups</span> item in the navigation bar on the left of the screen. Select the <span class="packt_screen">resource group</span> containing the <span class="packt_screen">machine learning workspace</span> item and click on <span class="packt_screen">machine learning workspace</span>. It will have the name you've configured in the creation wizard earlier.</p>
<p>In the workspace, there's a dedicated section for experiments. This section will provide access to experiments that you've run in the workspace, as well as details about the runs executed as part of the experiments. </p>
<p>Another useful element of the machine learning workspace is the models section. When you've trained a model, you can store it in the model registry so you can deploy it to production at a later time. A model automatically connects to the experiment run that produced it, so you can always trace back what code was used to produce a model, and which settings were used to train it.</p>
<p>Below the model section is the images section. This section shows you the Docker images created from your models. You can package models in Docker images together with a scoring script to make deployment to production easier and more predictable.</p>
<p>Finally, there's the deployment sections that contain all the deployments based on the images. You can use Azure Machine Learning service to deploy models to production using single container instances, a virtual machine, or even a Kubernetes cluster, should you need to scale your model deployment.</p>
<p>Azure Machine Learning service also offers a technique that allows you to build a pipeline to prepare data, train a model, and deploy it to production. This feature can be useful should you want to build a single process that contains both preprocessing steps and training steps. It's especially powerful in cases where you need to execute many steps to obtain a trained model. For now, we'll limit ourselves to running basic experiments and deploying the resulting model to a production Docker container instance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running your first experiment</h1>
                </header>
            
            <article>
                
<p>Now that you have a workspace, let's take a look at how to use it from a Python notebook. We'll modify some deep learning code so we save the trained model to the Azure Machine Learning service workspace as the output of an experiment, and track metrics for the model.</p>
<p>First, we need to install the <kbd>azureml</kbd> package as follows:</p>
<pre><strong><span>pip install --upgrade azureml-sdk[notebooks]</span></strong></pre>
<p>The <kbd>azureml</kbd> package contains the necessary components to run experiments. In order for it to work, you'll need to create a file called <kbd>config.json</kbd> in the root of your machine learning project. If you're working with the sample code for this chapter, you can modify the <kbd>config.json</kbd> file in the <kbd>azure-ml-service</kbd> folder. It contains the following content:</p>
<pre>{<br/>    "workspace_name": "&lt;workspace name&gt;",<br/>    "resource_group": "&lt;resource group&gt;",<br/>    "subscription_id": "&lt;your subscription id&gt;"<br/>}</pre>
<p>This file contains the workspace your Python code will work with, the resource group that contains the workspace you're working with, and the subscription that the workspace was created in. The workspace name should match the name you've chosen in the wizard to create the workspace earlier. The resource group should match the one that contains the workspace. Finally, you will need to find the subscription ID. </p>
<p>When you navigate to <span class="packt_screen">R</span><span class="packt_screen">esource groups</span> for the machine learning workspace on the portal, you'll see the <span class="packt_screen">Subscription ID</span> at the top of the resource group details panel, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-641 image-border" src="assets/e46ec086-f4c5-4d0b-b3f5-dffbfabc7860.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Subscription ID at the top of the resource group details panel</div>
<p>When you hover over the value for the <span class="packt_screen">Subscription ID</span>, the portal will show a button to copy the value to your clipboard. Paste this value into the <span class="packt_screen">subscriptionId</span> field of the config file and save it. You can now connect to your workspace from any Python notebook or Python program by using the following small snippet of code:</p>
<pre>from azureml.core import Workspace, Experiment<br/><br/>ws = Workspace.from_config()<br/>experiment = Experiment(name='classify-flowers', workspace=ws)</pre>
<p class="mce-root"/>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, we create a new workspace based on the configuration file we just created. This connects to the workspace in Azure. Once you're connected, you can create a new experiment with a name of your choice and connect it to the workspace.</li>
<li>Next, create a new experiment and connect it to the workspace.</li>
</ol>
<p>An experiment in Azure Machine Learning service can be used to keep track of an architecture you're testing with CNTK. For example, you could create an experiment for a convolutional neural network, and a second experiment to try solving the same problem with a recurrent neural network.</p>
<p>Let's explore how to track metrics and other output from experiments. We'll use the iris flower classification model from previous chapters and extend the training logic to track metrics as follows: </p>
<pre>from cntk import default_options, input_variable<br/>from cntk.layers import Dense, Sequential<br/>from cntk.ops import log_softmax, sigmoid<br/><br/>model = Sequential([<br/>    Dense(4, activation=sigmoid),<br/>    Dense(3, activation=log_softmax)<br/>])<br/><br/>features = input_variable(4)<br/>z = model(features)</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, import the <kbd>default_options</kbd> and <kbd>input_variable</kbd> function.</li>
<li>Next, import the layer types for the model from the <kbd>cntk.layers</kbd> module.</li>
<li>After that, import the <kbd>log_softmax</kbd> and <kbd>sigmoid</kbd> activation function from the <kbd>cntk.ops</kbd> module.</li>
<li>Create a new <kbd>Sequential</kbd> layer set.</li>
<li>Add a new <kbd>Dense</kbd> layer to the <kbd>Sequential</kbd> layer set with 4 neurons and the <kbd>sigmoid</kbd> activation function.</li>
<li>Add another <kbd>Dense</kbd> layer with 3 outputs and a <kbd>log_softmax</kbd> activation function.</li>
<li>Create a new <kbd>input_variable</kbd> with size 4.</li>
<li>Invoke the model with the <kbd>features</kbd> variable to complete the model.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To train the model, we're going to use a manual minibatch loop. First, we'll have to load and preprocess the iris dataset so that it matches the format that our model expects, as demonstrated in the following code snippet:</p>
<pre>import pandas as pd<br/>import numpy as np<br/><br/>df_source = pd.read_csv('iris.csv', <br/>    names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], <br/>    index_col=False)<br/><br/>X = df_source.iloc[:, :4].values<br/>y = df_source['species'].values</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>Import the <kbd>pandas</kbd> and <kbd>numpy</kbd> modules to load the CSV file containing the training samples.</li>
<li>Use the read_csv function to load the input file containing the training data.</li>
<li>Next, extract the first 4 columns as the input features</li>
<li>Finally, extract the species column as the labels </li>
</ol>
<p>The labels are stored as a string, so we'll have to convert those to a set of one-hot vectors in order to match the model as follows:</p>
<pre>label_mapping = {<br/>    'Iris-setosa': 0,<br/>    'Iris-versicolor': 1,<br/>    'Iris-virginica': 2<br/>}<br/><br/>def one_hot(index, length):<br/>    result = np.zeros(length)<br/>    result[index] = 1.<br/>    <br/>y = [one_hot(label_mapping[v], 3) for v in y]</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>Create a mapping from labels to their numeric representation.</li>
<li>Next, define a new utility function <kbd>one_hot</kbd> to encode a numeric value to a one-hot vector.</li>
<li>Finally, use a python list comprehension, to iterate over the values in the labels collection and turn them into one-hot encoded vectors.</li>
</ol>
<p class="mce-root"/>
<p>We need to execute one more step to prepare the dataset for training. In order to be able to verify that the model did indeed get optimized correctly, we want to create a hold-out set, against which we will run a test:</p>
<pre>from sklearn.model_selection import train_test_split<br/><br/>X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y)</pre>
<p>Using the <kbd>train_test_split</kbd> method, create a small hold-out set containing 20% of training samples. Use the <kbd>stratify</kbd> keyword and provide the labels to balance the split.</p>
<p>Once we have the data prepared, we can focus on training the model. First, we'll need to set up a <kbd>loss</kbd> function, <kbd>learner</kbd>, and <kbd>trainer</kbd> as follows:</p>
<pre>from cntk.losses import cross_entropy_with_softmax<br/>from cntk.metrics import classification_error<br/>from cntk.learners import sgd<br/>from cntk.train.trainer import Trainer<br/><br/>label = input_variable(3)<br/><br/>loss = cross_entropy_with_softmax(z, label)<br/>error_rate = classification_error(z, label)<br/><br/>learner = sgd(z.parameters, 0.001)<br/>trainer = Trainer(z, (loss, error_rate), [learner])</pre>
<ol>
<li>Import the cross_entropy_with_softmax function from the <kbd>cntk.losses</kbd> module.</li>
<li>Next, import the classificatin_error function from the <kbd>cnkt.metrics</kbd> module.</li>
<li>Then, import the <kbd>sgd</kbd> learner from the <kbd>cntk.learners</kbd> module.</li>
<li>Create a new <kbd>input_variable</kbd> with shape 3 to store the labels</li>
<li>Next, create a new instance of the cross_entropy_with_softmax loss and provide it the model variable <kbd>z</kbd> and the <kbd>label</kbd> variable.</li>
<li>Then, create a new metric using the classification_error function and provide it the network and <kbd>label</kbd> variable.</li>
<li>Now, initialize the <kbd>sgd</kbd> learner with the parameters of the network and set its learning rate to 0.001.</li>
<li>Finally, initialize the <kbd>Trainer</kbd> with the network, <kbd>loss</kbd>, <kbd>metric</kbd> , and <kbd>learner</kbd>.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Normally, we could just use the <kbd>train</kbd> method on the <kbd>loss</kbd> function to optimize the parameters in our model. This time, however, we want to have control over the training process so we can inject logic to track metrics in the Azure Machine Learning workspace, as demonstrated in the following code snippet:</p>
<pre>import os<br/>from cntk import ModelFormat<br/><br/>with experiment.start_logging() as run:<br/>    for _ in range(10):<br/>        trainer.train_minibatch({ features: X_train, label: y_train })<br/><br/>        run.log('average_loss', trainer.previous_minibatch_loss_average)<br/>        run.log('average_metric', trainer.previous_minibatch_evaluation_average)<br/><br/>    test_metric = trainer.test_minibatch( {features: X_test, label: y_test })</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>To start a new run, invoke the <kbd>start_logging</kbd> method on the experiment. This will create a new <kbd>run</kbd>. Within the scope of the run, we can execute the training logic.</li>
<li>Create a new for-loop to train for 10 epochs.</li>
<li>Within the for-loop, call the <kbd>train_minibatch</kbd> method on the <kbd>trainer</kbd> to train the model. Provide it a mapping between the input variables and the data to train with. </li>
<li>After this, log the <kbd>average_loss</kbd> metric for the run using the <kbd>previous_minibatch_loss_average</kbd> value from the trainer object.</li>
<li>In addition to the average loss, log the average metric in the run using the <kbd>previous_minibatch_evaluation_average</kbd> property on the trainer object.</li>
</ol>
<p>Once we have trained the model, we can execute a test against the test set using the <kbd>test_minibatch</kbd> method. This method returns the output of the <kbd>metric</kbd> function that we created earlier. We will log this to the machine learning workspace as well.</p>
<p>A run allows us to keep track of data related to a single training session for a model. We can log metrics using the <kbd>log</kbd> method on the <kbd>run</kbd> object. This method accepts the name of the metric and a value for the metric. You can use this method to record things such as the output of the <kbd>loss</kbd> function to monitor how your model is converging to an optimal set of parameters.</p>
<p>Other things can also be logged, such as the number of epochs used to train the model, the random seed used in the program, and other useful settings that you may need in order to reproduce the experiment at a later time.</p>
<p><span>Metrics recorded during a run automatically show up on the portal when you navigate to the experiment in the machine learning workspace under the experiments tab as </span>is shown in the image below.</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-642 image-border" src="assets/2cbb7274-466a-4831-943b-ab5c0d1ad1e0.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Navigating to the experiment in the machine learning workspace under the experiments tab</div>
<p>Aside from the <kbd>log</kbd> method, there's an <kbd>upload_file</kbd> method to upload files generating during training, as demonstrated in the following code snippet. You can use this method to store model files that you've saved after training is completed:</p>
<pre>z.save('outputs/model.onnx') # The z variable is the trained model<br/>run.upload_file('model.onnx', 'outputs/model.onnx')</pre>
<p>The <kbd>upload_file</kbd> method needs a name for the file, as it can be found in the workspace and a local path where the source file can be found. Please be aware of the location of the file. Due to a limitation in the Azure Machine Learning workspace, it will only pick up files from the outputs folder. This limitation will likely be lifted in the future.</p>
<p>Make sure you execute the <kbd>upload_file</kbd> method within the scope of the run, so that the AzureML library links the model to your experiment run as to make it traceable.</p>
<p>After you've uploaded the file to the workspace, you can find it in the portal under the outputs section of a run. To get to the run details, open up the machine learning workspace in Azure Portal, navigate to the experiment, and then select the run you want to see the details for as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-643 image-border" src="assets/663a6299-7df9-4aac-b503-0005bb600bc0.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Selecting the run</span></div>
<p>Finally, when you're done with the run and want to publish the model, you can register it in the model registry as follows:</p>
<pre>stored_model = run.register_model(model_name='classify_flowers', model_path='model.onnx')</pre>
<p>The <kbd>register_model</kbd> method stores the model in the model registry so you can deploy it to production. When the model was previously stored in the registry, it will automatically be stored as a new version. Now you can always go back to a previous version should you need to, as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-644 image-border" src="assets/1747e79d-4e26-473d-abbf-941254425c2f.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Model stored as a new version</span></div>
<p>You can find the model in the model registry in your workspace by going to the Machine Learning workspace on Azure Portal and clicking on the <span class="packt_screen">Models</span> item in the bar in the navigation menu of the workspace.</p>
<p>Models are automatically related to experiment runs, so you can always find the settings that you used to train the model. This is important, as it increases the chance that you can reproduce the results, should you need to.</p>
<p>We've limited ourselves to running experiments locally. You can use Azure Machine Learning to run experiments on dedicated hardware, should you want to. You can read more about this on the Azure Machine Learning documentation website at: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets">https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets</a>.</p>
<p>Once you have completed a run for an experiment you can deploy the trained model to production. In the next section we'll explore how to do this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying your model to production</h1>
                </header>
            
            <article>
                
<p>The final interesting piece of Azure Machine Learning is the deployment tooling that is included with it. The deployment tooling allows you to take a model from the model registry and deploy it to production.</p>
<p>Before you can deploy a model to production, you need to have an image that includes the model and a scoring script. The image is a Docker image that includes a web server, which will invoke the scoring script when a request is made against it. The scoring script accepts input in the form of a JSON payload, and uses it to make a prediction using the model. The scoring script for our iris classification model looks like this:</p>
<pre>import os<br/>import json<br/>import numpy as np<br/>from azureml.core.model import Model<br/>import onnxruntime<br/><br/>model = None<br/><br/>def init():<br/>    global model<br/>    <br/>    model_path = Model.get_model_path('classify_flowers')<br/>    model = onnxruntime.InferenceSession(model_path)<br/>    <br/><br/>def run(raw_data):<br/>    data = json.loads(raw_data)<br/>    data = np.array(data).astype(np.float32)<br/>    <br/>    input_name = model.get_inputs()[0].name<br/>    output_name = model.get_outputs()[0].name<br/><br/>    prediction = model.run([output_name], { input_name: data})<br/>    <br/>    # Select the first output from the ONNX model.<br/>    # Then select the first row from the returned numpy array.<br/>    prediction = prediction[0][0]<br/><br/>    return json.dumps({'scores': prediction.tolist() })</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, import the components needed to build the script.</li>
<li>Next, define a global model variable that will contain the loaded model.</li>
<li>After that, define the init function to initialize the model in the script.</li>
<li>Within the init function, retrieve the path for the model using the <kbd>Model.get_model_path</kbd> function. This automatically locates the model file in the Docker image.</li>
<li>Next, load the model by initializing a new instance of the <kbd>onnxruntime.InferenceSession</kbd> class.</li>
<li>Define another function, <kbd>run</kbd> that accepts a single parameter <kbd>raw_data</kbd>.</li>
<li>Within the <kbd>run</kbd> function, convert the contents of <kbd>raw_data</kbd> variable from JSON to a Python array.</li>
<li>Next, convert the <kbd>data</kbd> array into a Numpy array so we can use it to make a prediction.</li>
<li>After that, use the <kbd>run</kbd> method on the loaded model and feed it the input features. Include a dictionary that tells the ONNX runtime how to map the input data to the input variable of the model.</li>
<li>The model returns an array of outputs with 1 element for the output of the model. This output contains one row of data. Select the first element from the output array and the first row from the selected output variable and store it in the <kbd>prediction</kbd> variable.</li>
<li>Finally, return the predicted output as a JSON object.</li>
</ol>
<p>Azure Machine Learning service will automatically include any model files that you registered for a particular model when you create a container image. So, <kbd>get_model_path</kbd> will also work inside deployed images and resolve to a directory in the container that hosts the model and scoring script.</p>
<p> </p>
<p>Now that we have a scoring script, let's create an image and deploy the image as a web service in the cloud. To deploy a web service, you can explicitly create an image. Or, you can let Azure Machine Learning service create one based on the configuration you provided, as follows:</p>
<pre>from azureml.core.image import ContainerImage<br/><br/>image_config = ContainerImage.image_configuration(<br/>    execution_script="score.py", <br/>    runtime="python", <br/>    conda_file="conda_env.yml")</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, import the ContainerImage class from the <kbd>azureml.core.image</kbd> module.</li>
<li>Next, create a new image configuration using the <kbd>ContainerImage.image_configuration</kbd> method. Provide it with the score.py as the <kbd>execution_script</kbd> argument, the python <kbd>runtime</kbd> and finally provide conda_env.yml as the <kbd>conda_file</kbd> for the image.</li>
</ol>
<p>We configure the container image to use Python as the runtime. We're also configuring a special environment file for Anaconda so that we can configure custom modules like CNTK as follows: </p>
<pre>name: project_environment<br/>dependencies:<br/>  # The python interpreter version.<br/>  # Currently Azure ML only supports 3.5.2 and later.<br/>- python=3.6.2<br/><br/>- pip:<br/>    # Required packages for AzureML execution, history, and data preparation.<br/><br/>  - azureml-defaults<br/>  - onnxruntime</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, give the environment a name. This optional, but can be useful when you create an environment from this file locally for testing.</li>
<li>Next, Provide the python version 3.6.2 for your scoring script.</li>
<li>Finally, add a pip dependency to the list with a sublist containing <kbd>azureml-default</kbd> and <kbd>onnxruntime</kbd>.</li>
</ol>
<p>The <kbd>azureml-default</kbd> package contains everything you need to work with experiments and models in the docker container image. It includes standard packages like Numpy and Pandas as well for easier installation. The <kbd>onnxruntime</kbd> package is required so we can load the model inside the scoring script that we're using.</p>
<p>One more step is needed to deploy the trained model as a web service. We'll need to set up a web service configuration and deploy the model as a service. Machine Learning service supports deploying to virtual machines, <span>Kubernetes </span>clusters, and Azure Container Instances, which are basic Docker containers running in the cloud. This is how to deploy the model to an Azure Container Instance:</p>
<pre>from azureml.core.webservice import AciWebservice, Webservice<br/><br/>aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)<br/><br/>service = Webservice.deploy_from_model(workspace=ws,<br/>                                       name='classify-flowers-svc',<br/>                                       deployment_config=aciconfig,<br/>                                       models=[stored_model],<br/>                                       image_config=image_config)</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, Import the AciWebservice and Webservice classes from the azureml.core.webservice module.</li>
<li>Then, create a new <kbd>AciWebservice</kbd> configuration using the deploy_configuration method on the AziWebservice class. Provide it with a set of resource limits for the software. One CPU and 1GB of memory.</li>
<li>Once you have a configuration for the web service, deploy the model the registered model to production by calling <kbd>deploy_from_model</kbd> with the workspace to deploy from, a service name and the models that you want to deploy. Provide the image configuration you created earlier.</li>
</ol>
<p>Once the container image is created, it will get deployed as a container instance on Azure. This will create a new resource in the resource group for your machine learning workspace. </p>
<p>Once the new service is started, you will see a new deployment on Azure Portal in your machine learning workspace, as seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-646 image-border" src="assets/8c048b5c-208c-4e6a-917c-b4043faf8d70.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>New deployment on Azure Portal in your machine learning workspace</span></div>
<p>The deployment includes a scoring URL that you can invoke from your application to use the model. Because you're using REST to invoke the model, you're isolated from the fact that it runs CNTK underneath the covers. You also have something that can be used from any programming language you can possibly think of, as long as it can execute HTTP requests.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For example, in Python, we can use the <kbd>requests</kbd> package as a basic REST client to make predictions using the service you just created. Let's start by installing the <kbd>requests</kbd> module first, as follows:</p>
<pre><strong>pip install --upgrade requests</strong></pre>
<p>With the <kbd>requests</kbd> package installed, we can write a small script to execute a request against the deployed service as follows:</p>
<pre>import requests<br/>import json<br/><br/>service_url = "&lt;service-url&gt;"<br/>data = [[1.4, 0.2, 4.9, 3.0]]<br/><br/>response = requests.post(service_url, json=data)<br/><br/>print(response.json())</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, import the requests and json package.</li>
<li>Next, create a new variable for the service_url and fill it with the URL for the webservice.</li>
<li>Then, create another variable, to store the data you want to make a prediction for.</li>
<li>After that, use the requests.post function to post the data to the deployed service and store the response.</li>
<li>Finally, read the JSON data returned in the response to obtain the predicted values.</li>
</ol>
<p>The service_url can be obtained by performing the following steps:</p>
<ol>
<li>First, navigate to the resource group that contains the machine learning workspace.</li>
<li>Then, select the workspace and choose the <span class="packt_screen">Deployments</span> section on the left of the details panel.</li>
<li>Select the deployment you want to view the details of and copy the URL from the details page.</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-647 image-border" src="assets/98ea4d64-d656-4c2d-a74e-7a76b9ace6fc.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Selecting the deployment</div>
<p>When you run the script you just created, you'll receive a response with the predicted classes for the input sample. The output will look similar to this:</p>
<pre>{"scores": [-2.27234148979187, -2.486853837966919, -0.20609207451343536]}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we've looked at what it takes to bring deep learning and machine learning models to production. We've explored some of the basic principles that will help you to be successful with deep learning in a continuous delivery environment.</p>
<p>We've taken a look at exporting models to ONNX to make it easier to deploy your trained models to production and keep them running for years, thanks to the portable nature of the ONNX format. We then explored how you can use the CNTK API in other languages, such as C#, to make predictions.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Finally, we've looked at using Azure Machine Learning service to level-up your DevOps experience with experiment management, model management, and deployment tools. Although you don't need a tool like this to get started, it really helps to have something like Azure Machine Learning service in your arsenal when you're planning on running a bigger project on production.</p>
<p>With this chapter, we've reached the end of this book. In the first chapter, we started exploring CNTK. We then looked at how to build models, feed them with data, and measure their performance. With the basics covered, we explored two interesting use cases looking at images and time series data. Finally, we ended with taking models to production. You should now have enough information get started with building your own models with CNTK!</p>


            </article>

            
        </section>
    </body></html>