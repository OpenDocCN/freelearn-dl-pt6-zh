- en: Building Neural Networks with CNTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we talked about what deep learning is, and how neural
    networks work on a conceptual level. Finally, we talked about CNTK, and how to
    get it installed on your machine. In this chapter, we will build our first neural
    network with CNTK and train it.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at building a neural network using the different functions and
    classes from the CNTK library. We will do this with a basic classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a neural network for our classification problem, we will train
    it with sample data obtained from an open dataset. After our neural network is
    trained, we will look at how to use it to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, we will spend some time talking about ways to improve
    your model once you've trained it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic neural network concepts in CNTK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building your first neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making predictions with a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will work on a sample model, built using Python in a Jupyter
    notebook. Jupyter is an open source technology that allows you to create interactive
    web pages that contain sections of Python code, Markdown, and HTML. It makes it
    much easier to document your code and assumptions you made while building your
    deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''ve installed Anaconda using the steps defined in [Chapter 1](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml),
    *Getting Started with CNTK*, you already have Jupyter installed on your machine.
    Should you not have Anaconda yet, you can download it from: [https://anacondacloud.com/download](https://www.anaconda.com/download/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the sample code for this chapter from: [https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch2](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch2).
    To run the sample code, run the following commands inside a Terminal in the directory
    where you downloaded the sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Look for the `Train your first model.ipynb` notebook, and click it to open up
    the sample code. You can execute all the code in one step by choosing Cell | Run
    All. This will execute all the steps in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2YoyNKY](http://bit.ly/2YoyNKY)'
  prefs: []
  type: TYPE_NORMAL
- en: Basic neural network concepts in CNTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at the basic concepts of a neural network.
    Let's map the concepts we've learned to components in the CNTK library, and discover
    how you can use these concepts to build your own model.
  prefs: []
  type: TYPE_NORMAL
- en: Building neural networks using layer functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks are made using several layers of neurons. In CNTK, we can model
    the layers of a neural network using layer functions defined in the layers module.
    A `layer` function in CNTK looks like a regular function. For example, you can
    create the most basic layer type, `Dense`, with one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To Create the most basic layer type following the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, import the `Dense` layer function from the layers package
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, import the `input_variable` function from the `cntk` root package
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new input variable with the name features using the `input_variable`
    function and give it a size of `100`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new layer using the `Dense` function providing it with the number of
    neurons you want
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Invoke the configured `Dense` layer function providing the features variable
    to connect the `Dense` layer to the input
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Working with layers in CNTK has a distinct functional programming feel to it.
    When we look at the previous chapter, we can understand why CNTK has gone down
    this route. Ultimately, every layer in a neural network is a mathematical function.
    All the layer functions in CNTK produce a mathematical function with a set of
    predefined parameters. Invoke the function again, and you bind the last missing
    parameter, the input, to the layer.
  prefs: []
  type: TYPE_NORMAL
- en: You will typically build neural networks with this style of programming when
    you want to create a neural network with a complex architecture. But, for most
    starting developers, the functional style feels unfamiliar. CNTK provides an easier
    API for when you want to build a basic neural network through the `Sequential`
    layer function.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `Sequential` layer function to chain several layers together,
    without having to use the functional programming style, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To do so, follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, import the layer functions you want to use from the `layers` package
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the the `input_variable` function to create an input variable used to
    feed data into the neural network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new input variable to feed data into the neural network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new sequential layer block by invoking the `Sequential` function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide the list of layers that you want to chain together to the `Sequential`
    function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Invoke the configured `Sequential` function object providing the features input
    variable to complete the network structure
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By combining the `Sequential` function and other layer functions you can create
    any neural network structure. In the next section, we'll take a look at how to
    customize layers with settings to configure things like the `activation` function.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing layer settings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNTK provides a pretty good set of defaults for building neural networks. But
    you'll find yourself experimenting with those settings a lot. The behavior and
    performance of the neural network will be different based on the `activation`
    function and other settings you choose. Because of this, it is good to understand
    what you can configure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each layer has its own unique configuration options, some of which you will
    use a lot, and others you will use less. When we look at the `Dense` layer, there
    are a few important settings that you want to define:'
  prefs: []
  type: TYPE_NORMAL
- en: '`shape`:The output shape of the layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation`: The `activation` function for the layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init`: The `initialization` function of the layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output shape of a layer determines the number of neurons in that layer.
    Each neuron needs to have an `activation` function defined so it can transform
    the input data. Finally, we need a function that will initialize the parameters
    of the layer when we start training the neural network. The output shape is the
    first parameter in each `layer` function. The `activation` and `init` arguments
    are supplied as keyword arguments. These parameters have default values for them,
    so you can omit them should you not need a custom setting. The next sample demonstrates
    how to configure a `Dense` layer with a custom `initializer` and `activation`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To configure a Dense layer follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, import the `Dense` layer from the `layers` package
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, import the `sigmoid` operator from the `ops` package so we can use it
    to configure as an `activation` function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then import the `glorot_uniform` initializer from the `initializer` package
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, create a new layer using the `Dense` layer providing the number of
    neurons as the first argument and provide the `sigmoid` operator as the `activation`
    function and the `glorot_uniform` function as the `init` function for the layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are several `activation` functions to choose from; for example, you can
    use **Rectified Linear Unit** (**ReLU**),or `sigmoid`, as an `activation` function.
    All `activation` functions can be found in the `cntk.ops` package.
  prefs: []
  type: TYPE_NORMAL
- en: Each `activation` function will have a different effect on the performance of
    your neural network. We will go into more detail regarding `activation` functions
    when we build a neural network later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Initializers determine how the parameters in the layer are initialized when
    we start training our neural network. You can choose from various initializers
    in CNTK. `Normal`, `uniform`, and `glorot_uniform` are some of the more widely
    used initializers in the `cntk.initializer` package. We will get into more detail
    about which initializer to use when we start to solve our first deep learning
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever initializer function you're using from CNTK, it's important to realize
    that they use random number generators to generate the initial values for the
    parameters in the layer. This is an important technique, because it allows the
    neural network to learn the right parameters effectively. All initializer functions
    in CNTK support an extra seed setting. When you set this parameter to a fixed
    value, you will get the same initial values every time you train your neural network.
    This can be useful when you're trying to reproduce a problem, or are experimenting
    with different settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you are building a neural network, you typically have to specify the same
    set of settings for several layers in your neural network. This can become problematic
    when you are experimenting with your model. To solve this, CNTK includes a `utility`
    function called `default_options`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'By using the `default_options` function, we''ve configured the `sigmoid` activation
    function for all three layers, with just one line of code. The `default_options`
    function accepts a standard set of settings that get applied to all layers in
    the scope of this function. Using the `default_options` function makes configuring
    the same options for a set of layers much more comfortable. You can configure
    quite a lot of settings this way; for example, with the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`activation`:The `activation` function to use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init`: The `initialization` function for the layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias`: Whether the layers should have a `bias` term included'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_bias`:The `initialization` function for the bias term'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using learners and trainers to optimize the parameters in a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections we've seen how to create the structure for a neural
    network and how to configure various settings. Now let's look at how to use `learners`
    and `trainers` to optimize the parameters of a neural network. In CNTK, a neural
    network is trained using a combination of two components. The first component
    is the `trainer` component, which implements the backpropagation process. The
    second component is the `learner`. It is responsible for performing the gradient
    descent algorithm that we've seen in [Chapter 1](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml),
    *Getting Started with CNTK.*
  prefs: []
  type: TYPE_NORMAL
- en: The `trainer` passes the data through the neural network to obtain a prediction.
    It then uses the `learner` to obtain the new values for the parameters in the
    neural network. It then applies these new values, and repeats the process. This
    goes on until an exit criterion is met. The training process is stopped when a
    configured number of iterations is reached. This can be enhanced using custom
    callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: We've discussed a very basic form of gradient descent in [Chapter 1](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml),
    *Getting Started with CNTK*. But, in reality, there are many variations on this
    basic algorithm. The basic gradient descent doesn't work very well for complex
    cases. Often, it gets stuck in a local optimum (a bump in the hillside, if you
    will), so it doesn't reach a globally optimal value for the parameters in the
    neural network. Other algorithms, such as **Stochastic Gradient Descent** (**SGD**)
    with momentum, account for local optima, and use concepts such as momentum to
    get past bumps in the slope of the loss curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are few interesting `learners` that are included in the CNTK library:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SGD**: The basic stochastic gradient descent, without any extras'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MomentumSGD**: Applies momentum to overcome local optima'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RMSProp**: Uses decaying learning rates to control the rate of descent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adam**: Uses decaying momentum to decrease the rate of descent over time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adagrad**: Uses different learning rates for frequently, and infrequently,
    occurring features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's important to know that you can choose different `learners`, depending on
    the problem you want to solve. We will learn more about choosing the right optimizer
    when we start to solve our first machine learning problem with a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order for the `trainer` and `learner` to be able to optimize the parameters
    of the neural network, we need to define a function that measures the loss in
    the neural network. The `loss` function calculates how big the difference is between
    the predicted output of the neural network, and the expected output that we know
    beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: CNTK contains a number of `loss` functions in the `cntk.losses` module. Each
    `loss` function has its own use and specific characteristics. For example, when
    you want to measure the loss in a model that predicts a continuous value, you're
    going to need the `squared_error` loss function. It measures the distance between
    the predicted value generated by the model, and the real value that you provided
    when training the model.
  prefs: []
  type: TYPE_NORMAL
- en: For classification models, you will need a different set of `loss` functions.
    The `binary_cross_entropy` loss function can be used to measure the loss in a
    model that is used for binary classification jobs, such as a fraud detection model.
    The `cross_entropy_with_softmax` loss function is more suitable for classification
    models that predict multiple classes.
  prefs: []
  type: TYPE_NORMAL
- en: Model metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Combining a `learner` and `loss` function with a `trainer` allows us to optimize
    the parameters in the neural network. This should produce a good model, but in
    order to know that for sure we need metrics to measure model performance. A metric
    is a single value that tells us, for example, what percentage of samples was predicted
    correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Because the `loss` function measures the difference between the actual value
    and the predicted value, you might think that it's a good measure of how well
    our model is doing. Depending on the model, it may provide some value, but often
    you will need to use a separate `metric` function to measure your model's performance
    in a meaningful way.
  prefs: []
  type: TYPE_NORMAL
- en: CNTK offers a number of different `metric` functions in the `cntk.metrics` package.
    For example, if you want to measure the performance of a classification model,
    you can use the `classification_error` function. This is used to measure the percentage
    of samples that were predicted correctly.
  prefs: []
  type: TYPE_NORMAL
- en: The `classification_error` function is just one example of a metric. One other
    important `metric` function is the `ndcg_at_1` metric. If you're working with
    a ranking model, then you are interested in how closely your model ranked the
    samples according to a predefined ranking. This is what the `ndcg_at_1` metric
    gives you.
  prefs: []
  type: TYPE_NORMAL
- en: Building your first neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've learned what concepts CNTK offers to build a neural network,
    we can start to apply these concepts to a real machine learning problem. In this
    section, we'll explore how to use a neural network to classify species of iris
    flowers.
  prefs: []
  type: TYPE_NORMAL
- en: This is not a typical task where you want to use a neural network. But, as you
    will soon discover, the dataset is simple enough to get a good grasp of the process
    of building a deep learning model. Yet it contains enough data to ensure that
    the model works reasonably well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The iris dataset describes the physical properties of different varieties of
    iris flowers:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class (iris setosa, iris versicolor, iris virginica)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this chapter includes the iris dataset, on which you need to train
    the deep learning model. If you''re interested, you can find the original files
    online at: [http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris).
    It is also included with the sample code for this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: We are going to build a deep learning model that is going to classify a flower
    based on the physical properties of sepal width and length, and petal width and
    length. We can predict three different classes as output for the model.
  prefs: []
  type: TYPE_NORMAL
- en: We have a total of 150 different samples to train on, which should be enough
    to get reasonable performance when we try to use the model to classify a flower.
  prefs: []
  type: TYPE_NORMAL
- en: Building the network structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we need to determine what architecture to use for our neural network.
    We will be building a regular neural network, which is often called a feedforward
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: We need to define the number of neurons on the input and output layers first.
    Then, we need to define the shape of the hidden layer in our neural network. Because
    the task that we're solving is a simple one, we don't need more than one layer.
  prefs: []
  type: TYPE_NORMAL
- en: When we look at our dataset, we can see it has four features and one label.
    Because we have four features, we need to make sure our neural network has an
    input layer with four neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to define the output layer for our neural network. For this, we
    look at the number of classes that we need to be able to predict with our model.
    In our case, we have three different species of flowers to choose from, so we
    need three neurons in the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the necessary components from the CNTK library, which are
    our layer types, `activation` functions, and a function that allows us to define
    an input variable for our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create our model using the `Sequential` function, and feed it the layers
    that we want. We create two distinct layers in our network—first, one with four
    neurons, and then, another one with three neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we bind the network to the input variable, which will compile the
    neural network so it has an input layer with four neurons, and an output layer
    with three neurons, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's go back to our layer structure. Notice that we didn't model an input
    layer when we invoked the `Sequential` layer function. This is because the `input_variable`
    we created in our code is the input layer for the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The first layer in the sequential call is the hidden layer in the network. As
    a general rule of thumb, you want hidden layers that are no bigger than two times
    the number of neurons in the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: You will want to experiment with this setup in order to get the best results.
    Picking the right numbers of layers and neurons in your neural network requires
    some experience and experimentation. There are no hard rules that determine how
    many hidden layers you should include.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an activation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we chose the `sigmoid` activation function for our
    neural network. Choosing the right activation makes a big difference to how well
    your deep learning model will perform.
  prefs: []
  type: TYPE_NORMAL
- en: You will find a lot of opinions about choosing an `activation` function. That's
    because there's a lot to choose from, and not enough hard proof for any of the
    choices made by experts in the field. So, how do you pick one for your neural
    network?
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an activation function for the output layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we need to define what kind of problem we're solving. This determines
    the `activation` function for the output layer of your network. For regression
    problems, you want to use a `linear` activation function on the output layer.
    For a classification problem, you will want to use `sigmoid` for binary classification,
    and the `softmax` function for multi-class classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the model that we're building, we need to predict one of three classes, which
    means we need to use the `softmax` activation function on the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an activation function for the hidden layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's look at the hidden layers. Choosing an `activation` function for
    the hidden layers in our model is much harder. We will need to run some experiments
    and monitor the performance to see which `activation` function works best.
  prefs: []
  type: TYPE_NORMAL
- en: For classification problems, like our flower classification model, we need something
    that gives us probabilistic values. We need this because we need to predict the
    probability a sample belongs to a specific class. The `sigmoid` function helps
    us reach this goal. Its output is a probability, measured as a value between 0
    and 1\.
  prefs: []
  type: TYPE_NORMAL
- en: There are some problems that we have to account for with a `sigmoid` activation
    function. When you create larger networks, you may run into a problem called the
    **vanishing gradient**.
  prefs: []
  type: TYPE_NORMAL
- en: Very large input values given to a `sigmoid` function will converge to either
    zero or one, depending on whether they are negative or positive. This means that,
    when we work with large input values for our model, we won't see a lot of difference
    in the output of the `sigmoid` function. A change in an already large input value
    will result in only a very small change in the output. The gradient that is derived
    from this by the optimizer during training is also very small. Sometimes, it is
    so small that your computer will round it to zero, which means the optimizer can't
    detect which way to go with the values for the parameters. When the optimizer
    can't calculate gradients because of rounding problems in the CPU, we're dealing
    with a vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, scientists have come up with a new activation function,
    `ReLU`. This activation function converts all negative values to zero, and works
    as a pass-through filter for positive values. It helps solve the vanishing gradient
    problem, because it doesn't limit the output value.
  prefs: []
  type: TYPE_NORMAL
- en: There are, however, two problems with the `ReLU` function. First, it converts
    negative input to zero. In some cases, this can lead to a situation where the
    optimizer sets the weight of some parameters to zero as well. This causes your
    network to have dead neurons. That, of course, limits what your network can do.
  prefs: []
  type: TYPE_NORMAL
- en: The second problem is that the `ReLU` function suffers from exploding gradients.
    Because the upper bound of the output of this function isn't limited, it can amplify
    signals in such a way that the optimizer will calculate gradients that are close
    to infinity. When you apply this gradient to parameters in your network, your
    network will start to output NaN values.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the correct activation function for hidden layers requires some experimentation.
    Again, there is no hard rule that says which activation function to use. In the
    example code of this chapter, we choose the `sigmoid` function, after experimenting
    a bit with the model.
  prefs: []
  type: TYPE_NORMAL
- en: Picking a loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we have the structure for the model, it is time to take a look at how to
    optimize it. For this, we need a `loss` function to minimize. There are quite
    a few `loss` functions to choose from.
  prefs: []
  type: TYPE_NORMAL
- en: 'The right `loss` function depends on what kind of problem you are solving.
    For example, in a classification model like ours, we need a `loss` function that
    can measure the difference between a predicted class and an actual class. It needs
    to do so for three classes. The `categorical cross entropy` function is a good
    candidate. In CNTK, this `loss` function is implemented as `cross_entropy_with_softmax`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We need to import the `cross_entropy_with_softmax` function from the `cntk.losses`
    package first. After we've imported the `loss` function, we create a new input
    variable so we can feed the expected label into the `loss` function. Then we create
    a new `loss` variable that will hold a reference to the `loss` function. Any `loss`
    function in CNTK requires the output of the model and an input variable for the
    expected label.
  prefs: []
  type: TYPE_NORMAL
- en: Recording metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the structure in place and a `loss` function, we have all the ingredients
    we need to start optimizing our deep learning model. But before we start to look
    at how to train the model, let's take a look at metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for us to see how our network is doing, we need to record some metrics.
    Since we''re building a classification model, we''re going to use a `classification_error`
    metric. This metric produces a number between 0 and 1, which indicates the percentage
    of samples correctly predicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Let's import `classification_error` from the `cntk.metrics` package. We then
    create a new `error_rate` variable and bind the `classification_error` function
    to it. The function needs the output of the network and the expected label as
    input. We already have those available from defining our model and `loss` function.
  prefs: []
  type: TYPE_NORMAL
- en: Training the neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have all the components for the deep learning defined, let's train
    it. You can train a model in CNTK using a combination of a `learner` and `trainer`.
    We're going to need to define those and then feed data through the trainer to
    train the model. Let's see how that works.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a learner and setting up training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several `learners` to choose from. For our first model, we are going
    to use the `stochastic gradient descent` learner. Let''s configure the `learner`
    and `trainer` to train the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To configure the `learner` and `trainer` to train the neural network, follow
    the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, import the `sgd` function from the `learners` package
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, import the `Trainer` from the `trainer` package which is part of the `train`
    package
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now create a `learner` by invoking the `sgd` function providing the parameters
    of the model and a value for the learning rate
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, initialize the `trainer` and provide it the network, the combination
    of the `loss` and `metric` and the `learner`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The learning rate that we provide to the `sgd` function controls the speed of
    optimization and should be a small number somewhere in the area of 0.1 to 0.001\.
  prefs: []
  type: TYPE_NORMAL
- en: Note that every `learner` has its own parameters, so be sure to check the documentation
    to find out what parameters you need to configure when using a specific `learner`
    from the `cntk.learners` package.
  prefs: []
  type: TYPE_NORMAL
- en: Feeding data into the trainer to optimize the neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We spent quite a bit of time defining our model, configuring the `loss`, `metrics`,
    and, finally, the `learner`. Now it is time to train it on our dataset. Before
    we can train our model, however, we need to load the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset in the example code is stored as a CSV file. In order to load this
    dataset, we need to use a data wrangling package such as `pandas`. This package
    is included by default in your Anaconda installation. The following sample demonstrates
    how to use `pandas` to load the dataset into memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To load the dataset into memory using `pandas` follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, import the `pandas` package under the alias `pd`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, invoke the `read_csv` function to load the `iris.csv` file from disk
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because the CSV file doesn't include column headers, we need to define them
    ourselves. It will make it easier to refer to specific columns later on.
  prefs: []
  type: TYPE_NORMAL
- en: Normally, `pandas` will use the first column in the input file as the index
    of the dataset. The index will serve as a key by which you can identify records.
    We don't have an index in our dataset, so we disable its use through the `index_col`
    keyword argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have loaded the dataset, let''s split it into a set of features and
    a label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To split the dataset into a set of features and label, follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, use the `iloc` function to select all rows and the first four columns
    from the dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, select the species column from the dataset and use the values property
    to access the underlying `numpy` array
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our model requires numeric input values. But the species column is a string
    value, indicating the type of flower. We can fix this by encoding the species
    column to a numeric vector representation. The vector representation we''re creating
    matches the number of output neurons of the neural network. Each element in the
    vector represents a species of flowers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To create one-hot vector representations for the species, we will use a small
    `utility` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `one_hot` function performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, initialize a new array filled with zeros with the required `length`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, select the element at the specified `index` and set it to `1`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we have a dictionary mapping the species to the index, and a way to
    create one-hot vectors, we can turn the string values into their vector representation
    using one additional line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, create a list expression to iterate over all elements in the array
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each value in the array perform a look up in the `label_mapping` dictionary
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, take this converted numeric value and apply the `one_hot` function to
    convert it to a one-hot encoded vector
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, take the converted list and turn it into a `numpy` array
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you are training a deep learning model, or any machine learning model for
    that matter, you need to keep in mind that the computer will try to remember all
    the samples that you've used for training the model. At the same time, it will
    try to learn general rules. When the model remembers samples, but isn't able to
    deduce rules from the training samples, it is overfitting on your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To detect overfitting, you want to keep a small portion of your dataset separate
    from the training set. The training set is then used to train the model, while
    the test set is used to measure the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can split our dataset into training and test sets using a `utility` function
    from the `scikit-learn` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, import the `train_test_split` function from the `model_selection` module
    in the `sklearn` package
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, invoke the `train_test_split` function with the features `X` and the labels
    `y`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify a `test_size` of `0.2` to set aside 20% of the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `stratify` keyword argument with the values from the labels array `y`
    so that we get an equal amount of samples in the training and test set for each
    of the species in the dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you don't use the `stratify` argument, you end up with a dataset that might
    not contain any samples for one class, while it has too many of another class.
    The model then doesn't learn how to classify the class that is missing in the
    training set, while it overfits on the other class, which has too many samples
    available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a training set and validation set, let''s see how to feed
    them to our model to train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To train the model, invoke the `train_minibatch` method on the `trainer` and
    give it a dictionary that maps the input data to the input variables that you
    used to define the neural network and its associated `loss` function.
  prefs: []
  type: TYPE_NORMAL
- en: We're using the `train_minibatch` method as a convenient way to feed data into
    the trainer. In the next chapter, we'll discuss other ways to feed data. We'll
    also look at what the `train_minibatch` method does in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that you will have to call `train_minibatch` a number of times to get
    the network decently trained. So we''ll have to write a short loop around this
    method call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, create a new loop using the `for` statement and give it a range of `10`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within the loop invoke the `train_minibatch` method with a mapping between the
    input variables and the associated data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, print the `previous_minibatch_loss_average` and `previous_minibatch_evaluation_average`
    to monitor the training progress.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you invoke the `train_minibatch` method, the `trainer` will update the
    output of the `loss` function and the value for the `metric` function that we
    provided to the `trainer` and store it in the `previous_minibatch_evaluation_average`.
  prefs: []
  type: TYPE_NORMAL
- en: Each time the loop completes, and we've run the whole dataset through the `trainer`,
    we've completed one epoch of training. As we have seen in the previous chapter,
    it is normal to run several epochs before a model works well enough. As an added
    bonus, we're also printing the progress of our `trainer` after each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the performance of the neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every time we pass data through the trainer to optimize our model, it measures
    the performance of the model through the metric that we configured for the trainer.
    The model performance measured during training is on the training set. It is useful
    to measure the accuracy on the training set, because it will tell you whether
    the model is actually learning anything from the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a full analysis of the model performance, you need to measure the performance
    of the model using the test set. This can be done by invoking the `test_minibatch`
    method on the `trainer` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This method accepts a dictionary with a mapping between the input variables
    and the data for the variables. The output of this method is the output of the
    `metric` function you've configured earlier. In our case, it's the accuracy of
    our model based on the data we've given as input.
  prefs: []
  type: TYPE_NORMAL
- en: When the accuracy on the test set is higher than the accuracy on the training
    set, we will have a model that is underfitting. We're dealing with overfitting
    when the accuracy on the test set is lower than the accuracy on the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Both underfitting and overfitting are bad if you take them too far. The best
    performance is achieved when the accuracy on both test set and training set are
    almost the same. We'll talk more about model performance in [Chapter 4](e39df191-73e4-414f-b44b-efca6f0ad4cd.xhtml),
    *Validating Model Performance*.
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions with a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most satisfying things after training a deep learning model is to
    actually use it in an application. For now, we'll limit ourselves to using the
    model with a sample that we randomly pick from our test set. But, later on, in
    [Chapter 7](8db9f932-5716-4a33-82a7-0c5ce5fe2ed4.xhtml), *Deploying Models to*
    *Production*, we'll look at how to save the model to disk and use it in C# or
    .NET to build applications with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write the code to make a prediction with the neural network that we
    trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, pick a random item from the test set using the `np.random.choice` function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then select the sample data from the test set using the generated `sample_index`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, create an inverted mapping so you can convert the numeric output of the
    neural network to an actual label
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, use the selected `sample` data and make a prediction by invoking the neural
    network `z` as a function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the predicted output, take the index of the neuron that has the highest
    value as the predicted value using the `np.argmax` function from the `numpy` package
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `inverted_mapping` to convert the index value into the real label
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you execute the code sample, you will get output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Improving the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will quickly learn that building and training neural networks takes more
    than one attempt. Usually, the first version of your model will not work as well
    as you hope. It requires quite a bit of experimentation to come up with a great
    model.
  prefs: []
  type: TYPE_NORMAL
- en: A good neural network starts with a great dataset. In nearly all cases, better
    performance is achieved by using a proper dataset. Many data scientists will tell
    you that they spend about 80% of their time working on a good dataset. As with
    all computer software, if you put garbage in, you will get garbage out.
  prefs: []
  type: TYPE_NORMAL
- en: Even with a good dataset, you still need to spend quite some time to build and
    train different models before you get the performance you're after. So, let's
    see what you can do to improve your model after you've built it for the first
    time.
  prefs: []
  type: TYPE_NORMAL
- en: After you've trained the model for the first time, you have a couple of options
    to choose from in order to improve your model.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the accuracy of your training and validation sets. Is the accuracy
    on the training set lower? Try to train the model for more epochs. Usually, this
    will help improve the model.
  prefs: []
  type: TYPE_NORMAL
- en: Does the training accuracy not improve even, if you train the model for longer?
    Then your model is probably unable to learn the complex relationships in your
    dataset. Try to change the model structure, and train the model again to see if
    that improves the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: For example, try to change the activation function or the number of neurons
    in your hidden layers. This will usually help the model to learn the more complex
    relationships in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can take a look at the number of layers in your model. Adding
    one more layer can have quite a large effect on the ability of your model to learn
    rules from the data you feed it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when that doesn't help, take a look at the initialization of the layers
    in your model. In some cases, choosing a different initialization function helps
    the model during the initial learning steps.
  prefs: []
  type: TYPE_NORMAL
- en: The key to the process of experimentation is to change one thing at a time and
    keep track of your experiments. Using a source control solution such as Git can
    help you keep track of different versions of your training code.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've built our first neural network and trained it to recognize
    iris flowers. While this sample is really basic, it shows how to use CNTK to build
    and train neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: We've seen how to use the layer library in CNTK to our advantage to quickly
    define the structure for our neural network. In this chapter, we've talked about
    a few basic building blocks, such as the `Dense` layer and the `Sequential` layer,
    to chain several other layers together. In the coming chapters, we will learn
    other layer functions to build other types of neural networks such as convolutional
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we've also discussed how to use `learner` and `trainer` to
    build a basic algorithm to train our neural network. We've used the `train_minibatch`
    method, together with a basic loop, to construct our own training process. This
    is a pretty simple and powerful way to train our model. In the next chapter, we'll
    discuss other methods of training and the `train_minibatch` method in much more
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: After we trained the model, we made use of the functional properties of CNTK
    to make a prediction with our trained model. The fact that a model is a function
    is quite powerful, and makes it really intuitive to use trained models in your
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we've seen how to measure model performance using the `test_minibatch`
    method, and how to use performance metrics to check whether our model is overfitting.
    We later discussed how to use metrics to determine how to improve the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at different ways to access and feed data
    to CNTK models. We'll also explore each method of data access in CNTK, and which
    are the most appropriate to use in different circumstances.
  prefs: []
  type: TYPE_NORMAL
