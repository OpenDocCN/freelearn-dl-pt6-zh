<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Variational Autoencoders (VAEs)"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Variational Autoencoders (VAEs)</h1></div></div></div><p>Similar to <span class="strong"><strong>Generative Adversarial Networks</strong></span> (<span class="strong"><strong>GANs</strong></span>) that we've discussed in the previous chapters, <span class="strong"><strong>Variational Autoencoders</strong></span> (<span class="strong"><strong>VAEs</strong></span>) [1] belong to the family of generative models. The generator of VAE is able to produce meaningful outputs while navigating its<a id="id343" class="indexterm"/> continuous latent space. The possible attributes of the decoder outputs are explored through the latent vector.</p><p>In GANs, the focus is on how to arrive at a model that approximates the input distribution. VAEs attempt to model the input distribution from a decodable continuous latent space. This is one of the possible underlying reasons why GANs are able to generate more realistic signals when compared to VAEs. For example, in image generation, GANs are able to produce more realistic looking images while VAEs in comparison generate images that are less sharp.</p><p>Within VAEs, the focus is on the variational inference of latent codes. Therefore, VAEs provide a suitable framework for both learning and efficient Bayesian inference with latent variables. For example, VAEs with disentangled representations enable latent code reuse for transfer learning.</p><p>In terms of structure, VAEs bear a resemblance to an autoencoder. They are also made up of an encoder (also known as recognition or inference model) and a decoder (also known as a generative model). Both VAEs and autoencoders attempt to reconstruct the input data while learning the latent vector. However, unlike autoencoders, the latent space of VAEs is continuous, and the decoder itself is used as a generative model.</p><p>In the same line of discussions on GANs that we discussed in the previous chapters, the VAEs decoder can also be conditioned. For example, in the MNIST dataset, we're able to specify the digit to produce given a one-hot vector. This class of conditional VAE is called CVAE [2]. VAE latent vectors can also be disentangled by including a regularizing hyperparameter on the loss function. This is called </p><div class="mediaobject"><img src="graphics/B08956_08_001.jpg" alt="Variational Autoencoders (VAEs)"/></div><p>-VAE [5]. For example, within MNIST, we're able to isolate the latent vector that determines the thickness or tilt angle of each digit.</p><p>The goal of this chapter is to present:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The principles of VAEs</li><li class="listitem" style="list-style-type: disc">An understanding of the reparameterization trick that facilitates the use of stochastic gradient descent on VAE optimization</li><li class="listitem" style="list-style-type: disc">The principles of conditional VAE (CVAE) and <div class="mediaobject"><img src="graphics/B08956_08_002.jpg" alt="Variational Autoencoders (VAEs)"/></div><p>-VAE</p></li><li class="listitem" style="list-style-type: disc">An understanding of how to implement VAEs within the Keras library</li></ul></div><div class="section" title="Principles of VAEs"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec47"/>Principles of VAEs</h1></div></div></div><p>In a generative model, we're often interested in approximating the true distribution of our inputs<a id="id344" class="indexterm"/> using neural networks:</p><div class="mediaobject"><img src="graphics/B08956_08_003.jpg" alt="Principles of VAEs"/></div><p>          (Equation 8.1.1)</p><p>In the preceding equation, </p><div class="mediaobject"><img src="graphics/B08956_08_004.jpg" alt="Principles of VAEs"/></div><p> are the parameters determined during training. For example, in the context of the celebrity faces dataset, this is equivalent to finding a distribution that can draw faces. Similarly, in the MNIST dataset, this distribution can generate recognizable handwritten digits.</p><p>In machine learning, to perform a certain level of inference, we're interested in finding </p><div class="mediaobject"><img src="graphics/B08956_08_005.jpg" alt="Principles of VAEs"/></div><p>, a joint distribution between inputs, <span class="emphasis"><em>x</em></span>, and the latent variables, <span class="emphasis"><em>z</em></span>. The latent variables are not part of the dataset but instead encode certain properties observable from inputs. In the context of celebrity faces, these might be facial expressions, hairstyles, hair color, gender, and so on. In the MNIST dataset, the latent variables may represent the digit and writing styles.</p><div class="mediaobject"><img src="graphics/B08956_08_006.jpg" alt="Principles of VAEs"/></div><p> is practically a distribution of input data points and their attributes. <span class="emphasis"><em>P</em></span>
<span class="emphasis"><em>θ</em></span>(<span class="emphasis"><em>x</em></span>) can be computed from the marginal distribution:</p><div class="mediaobject"><img src="graphics/B08956_08_007.jpg" alt="Principles of VAEs"/></div><p>          (Equation 8.1.2)</p><p>In other words, considering all of the possible attributes, we end up with the distribution that describes the inputs. In celebrity faces, if we consider all the facial expressions, hairstyles, hair colors, gender, the distribution describing the celebrity faces is recovered. In the MNIST dataset, if we<a id="id345" class="indexterm"/> consider all of the possible digits, writing styles, and so on, we end up with the distribution of handwritten digits.</p><p>The problem is <span class="emphasis"><em>Equation 8.1.2</em></span> is <span class="emphasis"><em>intractable</em></span>. the equation does not have an analytic form or an efficient estimator. It cannot be differentiated with respect to its parameters. Therefore, optimization by a neural network is not feasible.</p><p>Using Bayes theorem, we can find an alternative expression for <span class="emphasis"><em>Equation 8.1.2</em></span>:</p><div class="mediaobject"><img src="graphics/B08956_08_008.jpg" alt="Principles of VAEs"/></div><p>          (Equation 8.1.3)</p><p>
<span class="emphasis"><em>P</em></span>(<span class="emphasis"><em>z</em></span>) is a prior distribution over <span class="emphasis"><em>z</em></span>. It is not conditioned on any observations. If <span class="emphasis"><em>z</em></span> is discrete and </p><div class="mediaobject"><img src="graphics/B08956_08_009.jpg" alt="Principles of VAEs"/></div><p> is a Gaussian distribution, then </p><div class="mediaobject"><img src="graphics/B08956_08_010.jpg" alt="Principles of VAEs"/></div><p> is a mixture of Gaussians. If <span class="emphasis"><em>z</em></span> is continuous, </p><div class="mediaobject"><img src="graphics/B08956_08_011.jpg" alt="Principles of VAEs"/></div><p> is an infinite mixture of Gaussians.</p><p>In practice, if we try to build a neural network to approximate </p><div class="mediaobject"><img src="graphics/B08956_08_012.jpg" alt="Principles of VAEs"/></div><p> without a suitable loss function, it will just ignore <span class="emphasis"><em>z</em></span> and arrive at a trivial solution </p><div class="mediaobject"><img src="graphics/B08956_08_013.jpg" alt="Principles of VAEs"/></div><p> = </p><div class="mediaobject"><img src="graphics/B08956_08_014.jpg" alt="Principles of VAEs"/></div><p>. Therefore, <span class="emphasis"><em>Equation 8.1.3</em></span> does not provide us with a good estimate of </p><div class="mediaobject"><img src="graphics/B08956_08_015.jpg" alt="Principles of VAEs"/></div><p>.</p><p>Alternatively, <span class="emphasis"><em>Equation 8.1.2</em></span> can also be expressed as:</p><div class="mediaobject"><img src="graphics/B08956_08_016.jpg" alt="Principles of VAEs"/></div><p>          (Equation 8.1.4)</p><p>However, </p><div class="mediaobject"><img src="graphics/B08956_08_017.jpg" alt="Principles of VAEs"/></div><p> is also intractable. The goal of a VAEs is to find a tractable distribution that closely<a id="id346" class="indexterm"/> estimates </p><div class="mediaobject"><img src="graphics/B08956_08_018.jpg" alt="Principles of VAEs"/></div><p>.</p><div class="section" title="Variational inference"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec28"/>Variational inference</h2></div></div></div><p>In order<a id="id347" class="indexterm"/> to make </p><div class="mediaobject"><img src="graphics/B08956_08_019.jpg" alt="Variational inference"/></div><p> tractable, VAE introduces the variational inference model (an encoder):</p><div class="mediaobject"><img src="graphics/B08956_08_020.jpg" alt="Variational inference"/></div><p>          (Equation 8.1.5)</p><div class="mediaobject"><img src="graphics/B08956_08_021.jpg" alt="Variational inference"/></div><p> provides a good estimate of </p><div class="mediaobject"><img src="graphics/B08956_08_022.jpg" alt="Variational inference"/></div><p>. It is both parametric and tractable. </p><div class="mediaobject"><img src="graphics/B08956_08_023.jpg" alt="Variational inference"/></div><p> can be approximated by deep neural networks by optimizing the parameters </p><div class="mediaobject"><img src="graphics/B08956_08_024.jpg" alt="Variational inference"/></div><p>.</p><p>Typically, </p><div class="mediaobject"><img src="graphics/B08956_08_025.jpg" alt="Variational inference"/></div><p> is chosen to be a multivariate Gaussian:</p><div class="mediaobject"><img src="graphics/B08956_08_026.jpg" alt="Variational inference"/></div><p>          (Equation 8.1.6)</p><p>Both mean, </p><div class="mediaobject"><img src="graphics/B08956_08_027.jpg" alt="Variational inference"/></div><p>, and standard deviation, </p><div class="mediaobject"><img src="graphics/B08956_08_028.jpg" alt="Variational inference"/></div><p>, are computed by the encoder neural network using the input data points. The diagonal matrix implies that the elements of <span class="emphasis"><em>z</em></span> are independent.</p></div><div class="section" title="Core equation"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec29"/>Core equation</h2></div></div></div><p>The inference model </p><div class="mediaobject"><img src="graphics/B08956_08_029.jpg" alt="Core equation"/></div><p> generates latent vector <span class="emphasis"><em>z</em></span> from input <span class="emphasis"><em>x</em></span>. </p><div class="mediaobject"><img src="graphics/B08956_08_030.jpg" alt="Core equation"/></div><p> is like the encoder in<a id="id348" class="indexterm"/> an autoencoder model. On the other hand, </p><div class="mediaobject"><img src="graphics/B08956_08_031.jpg" alt="Core equation"/></div><p> reconstructs the input from the latent code <span class="emphasis"><em>z</em></span>. </p><div class="mediaobject"><img src="graphics/B08956_08_032.jpg" alt="Core equation"/></div><p> acts like the decoder in an autoencoder model. To estimate </p><div class="mediaobject"><img src="graphics/B08956_08_033.jpg" alt="Core equation"/></div><p>, we must identify its relationship with </p><div class="mediaobject"><img src="graphics/B08956_08_034.jpg" alt="Core equation"/></div><p> and </p><div class="mediaobject"><img src="graphics/B08956_08_035.jpg" alt="Core equation"/></div><p>.</p><p>If </p><div class="mediaobject"><img src="graphics/B08956_08_036.jpg" alt="Core equation"/></div><p> is an estimate of </p><div class="mediaobject"><img src="graphics/B08956_08_037.jpg" alt="Core equation"/></div><p>, the <span class="strong"><strong>Kullback-Leibler</strong></span> (<span class="strong"><strong>KL</strong></span>) divergence determines the distance<a id="id349" class="indexterm"/> between these two conditional densities:</p><div class="mediaobject"><img src="graphics/B08956_08_038.jpg" alt="Core equation"/></div><p>          (Equation 8.1.7)</p><p>Using Bayes theorem,</p><div class="mediaobject"><img src="graphics/B08956_08_039_.jpg" alt="Core equation"/></div><p>          (Equation 8.1.8)</p><p>in <span class="emphasis"><em>Equation 8.1.7</em></span>, </p><div class="mediaobject"><img src="graphics/B08956_08_040.jpg" alt="Core equation"/></div><p>   (Equation 8.1.9)</p><div class="mediaobject"><img src="graphics/B08956_08_041.jpg" alt="Core equation"/></div><p> can be<a id="id350" class="indexterm"/> taken out the expectation since it is not dependent on </p><div class="mediaobject"><img src="graphics/B08956_08_042.jpg" alt="Core equation"/></div><p>. Rearranging the preceding equation and recognizing that </p><div class="mediaobject"><img src="graphics/B08956_08_043.jpg" alt="Core equation"/></div><p>:</p><div class="mediaobject"><img src="graphics/B08956_08_044.jpg" alt="Core equation"/></div><p>   (Equation 8.1.10)</p><p>
<span class="emphasis"><em>Equation 8.1.10</em></span> is the core of VAEs. The left-hand side is the term </p><div class="mediaobject"><img src="graphics/B08956_08_045.jpg" alt="Core equation"/></div><p> that we are maximizing less the error due to the distance of </p><div class="mediaobject"><img src="graphics/B08956_08_046.jpg" alt="Core equation"/></div><p> from the true </p><div class="mediaobject"><img src="graphics/B08956_08_047.jpg" alt="Core equation"/></div><p>. We can recall that the logarithm does not change the location of maxima (or minima). Given an inference model that provides a good estimate of </p><div class="mediaobject"><img src="graphics/B08956_08_048.jpg" alt="Core equation"/></div><p>, </p><div class="mediaobject"><img src="graphics/B08956_08_049.jpg" alt="Core equation"/></div><p> is approximately zero. The first term, </p><div class="mediaobject"><img src="graphics/B08956_08_050.jpg" alt="Core equation"/></div><p>, on the right-hand side resembles a decoder that takes samples from the inference model to reconstruct the input. The second term is another distance. This time it's between </p><div class="mediaobject"><img src="graphics/B08956_08_051.jpg" alt="Core equation"/></div><p> and the prior </p><div class="mediaobject"><img src="graphics/B08956_08_052.jpg" alt="Core equation"/></div><p>.</p><p>The left side of <span class="emphasis"><em>Equation 8.1.10</em></span> is also<a id="id351" class="indexterm"/> known as the <span class="strong"><strong>variational lower bound</strong></span> or <span class="strong"><strong>evidence lower bound</strong></span> (<span class="strong"><strong>ELBO</strong></span>). Since the KL is always positive, ELBO is<a id="id352" class="indexterm"/> the lower bound of </p><div class="mediaobject"><img src="graphics/B08956_08_053.jpg" alt="Core equation"/></div><p>. Maximizing ELBO by optimizing the parameters </p><div class="mediaobject"><img src="graphics/B08956_08_054.jpg" alt="Core equation"/></div><p> and </p><div class="mediaobject"><img src="graphics/B08956_08_055.jpg" alt="Core equation"/></div><p> of the neural network means that:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="inlinemediaobject"><img src="graphics/B08956_08_056.jpg" alt="Core equation"/></span>
 or the inference model is getting better in encoding the attributes of <span class="emphasis"><em>x</em></span> in <span class="emphasis"><em>z</em></span></li><li class="listitem" style="list-style-type: disc"><span class="inlinemediaobject"><img src="graphics/B08956_08_057.jpg" alt="Core equation"/></span>
 on the right-hand side of <span class="emphasis"><em>Equation 8.1.10</em></span> is maximized or the decoder model is getting better in reconstructing <span class="emphasis"><em>x</em></span> from the latent vector <span class="emphasis"><em>z</em></span></li></ul></div></div><div class="section" title="Optimization"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec30"/>Optimization</h2></div></div></div><p>The right-hand side<a id="id353" class="indexterm"/> of <span class="emphasis"><em>Equation 8.1.10</em></span> has two important bits of information about the loss function of VAEs. The decoder term </p><div class="mediaobject"><img src="graphics/B08956_08_058.jpg" alt="Optimization"/></div><p> means that the generator takes <span class="emphasis"><em>z</em></span> samples from the output of the inference model to<a id="id354" class="indexterm"/> reconstruct the inputs. Maximizing this term implies that we minimize the <span class="strong"><strong>Reconstruction Loss</strong></span>, </p><div class="mediaobject"><img src="graphics/B08956_08_059.jpg" alt="Optimization"/></div><p>. If the image (data) distribution is assumed to be Gaussian, then MSE can be used. If every pixel (data) is considered a Bernoulli distribution, then the loss function is a binary cross entropy.</p><p>The second term, </p><div class="mediaobject"><img src="graphics/B08956_08_060.jpg" alt="Optimization"/></div><p>, turns out to be straightforward to evaluate. From <span class="emphasis"><em>Equation 8.1.6</em></span>, </p><div class="mediaobject"><img src="graphics/B08956_08_061.jpg" alt="Optimization"/></div><p> is a Gaussian distribution. Typically, </p><div class="mediaobject"><img src="graphics/B08956_08_062.jpg" alt="Optimization"/></div><p> is also a Gaussian<a id="id355" class="indexterm"/> with zero mean and standard deviation equal to 1.0. The KL term simplifies to:</p><div class="mediaobject"><img src="graphics/B08956_08_063.jpg" alt="Optimization"/></div><p>         (Equation 8.1.11)</p><p>Where </p><div class="mediaobject"><img src="graphics/B08956_08_064.jpg" alt="Optimization"/></div><p> is the dimensionality of <span class="emphasis"><em>z</em></span>. Both </p><div class="mediaobject"><img src="graphics/B08956_08_065.jpg" alt="Optimization"/></div><p> and </p><div class="mediaobject"><img src="graphics/B08956_08_066.jpg" alt="Optimization"/></div><p> are functions of <span class="emphasis"><em>x</em></span> computed through the inference model. To maximize </p><div class="mediaobject"><img src="graphics/B08956_08_067.jpg" alt="Optimization"/></div><p>, </p><div class="mediaobject"><img src="graphics/B08956_08_068.jpg" alt="Optimization"/></div><p> and </p><div class="mediaobject"><img src="graphics/B08956_08_069.jpg" alt="Optimization"/></div><p>. The choice of </p><div class="mediaobject"><img src="graphics/B08956_08_070.jpg" alt="Optimization"/></div><p> stems from the property of isotropic unit Gaussian which can be morphed to an arbitrary distribution given a suitable function. From <span class="emphasis"><em>Equation 8.1.11</em></span>, the <span class="strong"><strong>KL Loss</strong></span> </p><div class="mediaobject"><img src="graphics/B08956_08_071.jpg" alt="Optimization"/></div><p> is simply </p><div class="mediaobject"><img src="graphics/B08956_08_072.jpg" alt="Optimization"/></div><p>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>For example, it was previously [6] demonstrated that an isotropic Gaussian could be morphed into a ring-shaped distribution using the function </p><div class="mediaobject"><img src="graphics/B08956_08_073.jpg" alt="Optimization"/></div><p>.</p><p>Readers can further explore the theory as presented in Luc Devroye's, <span class="emphasis"><em>Sample-Based Non-Uniform Random Variate Generation </em></span>[7].</p></div></div><p>In summary, the VAE loss function is defined as:</p><div class="mediaobject"><img src="graphics/B08956_08_074.jpg" alt="Optimization"/></div><p>          (Equation 8.1.12)</p></div><div class="section" title="Reparameterization trick"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec31"/>Reparameterization trick</h2></div></div></div><div class="mediaobject"><img src="graphics/B08956_08_01.jpg" alt="Reparameterization trick"/><div class="caption"><p>Figure 8.1.1: A VAE network with and without the reparameterization trick</p></div></div><p>On the left side of the preceding figure shows the VAE network. The encoder takes the input <span class="emphasis"><em>x</em></span>, and estimates the mean, </p><div class="mediaobject"><img src="graphics/B08956_08_075.jpg" alt="Reparameterization trick"/></div><p>, and the standard deviation, </p><div class="mediaobject"><img src="graphics/B08956_08_076.jpg" alt="Reparameterization trick"/></div><p>, of the multivariate Gaussian distribution of<a id="id356" class="indexterm"/> the latent vector <span class="emphasis"><em>z</em></span>. The decoder takes samples from the latent vector <span class="emphasis"><em>z</em></span> to reconstruct the input as </p><div class="mediaobject"><img src="graphics/B08956_08_077.jpg" alt="Reparameterization trick"/></div><p>. This seems straightforward until the gradient updates happen during backpropagation.</p><p>Backpropagation gradients will not pass through the stochastic <span class="strong"><strong>Sampling</strong></span> block. While it's fine<a id="id357" class="indexterm"/> to have stochastic inputs for neural networks, it's not possible for the gradients to go through a stochastic layer.</p><p>The solution to this problem is to push out the <span class="strong"><strong>Sampling</strong></span> process as the input as shown on the right side of <span class="emphasis"><em>Figure 8.1.1</em></span>. Then, compute the sample as:</p><div class="mediaobject"><img src="graphics/B08956_08_078.jpg" alt="Reparameterization trick"/></div><p>          (Equation 8.1.13)</p><p>If </p><div class="mediaobject"><img src="graphics/B08956_08_079.jpg" alt="Reparameterization trick"/></div><p> and </p><div class="mediaobject"><img src="graphics/B08956_08_080.jpg" alt="Reparameterization trick"/></div><p> are expressed in vector format, then </p><div class="mediaobject"><img src="graphics/B08956_08_081.jpg" alt="Reparameterization trick"/></div><p> is element-wise multiplication. Using <span class="emphasis"><em>Equation 8.1.13</em></span>, it appears<a id="id358" class="indexterm"/> as if sampling is directly coming from the latent space as originally intended. This technique is better known as the <span class="strong"><strong>Reparameterization Trick</strong></span>.</p><p>With <span class="emphasis"><em>Sampling</em></span> now happening at the input, the VAE network can be trained using the familiar optimization algorithms such as SGD, Adam, or RMSProp.</p></div><div class="section" title="Decoder testing"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec32"/>Decoder testing</h2></div></div></div><p>After training the VAE network, the inference model including the addition and multiplication operator can be discarded. To generate new meaningful outputs, samples are taken<a id="id359" class="indexterm"/> from the Gaussian distribution used in generating </p><div class="mediaobject"><img src="graphics/B08956_08_082.jpg" alt="Decoder testing"/></div><p>. Following figure shows us how to test the decoder:</p><div class="mediaobject"><img src="graphics/B08956_08_02.jpg" alt="Decoder testing"/><div class="caption"><p>Figure 8.1.2: Decoder testing setup</p></div></div></div><div class="section" title="VAEs in Keras"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec33"/>VAEs in Keras</h2></div></div></div><p>The structure<a id="id360" class="indexterm"/> of VAE bears a resemblance to a typical autoencoder. The difference is mainly on the sampling of the Gaussian random variables in the reparameterization trick. <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>8.1.1</em></span> shows the encoder, decoder, and VAE which are implemented using MLP. This code has also been contributed to the official Keras GitHub repository. For simplicity of the discussion, the latent vector <span class="emphasis"><em>z</em></span> is 2-dim.</p><p>The encoder is just a two-layer MLP with the second layer generating the mean and log variance. The use of log variance is for simplicity in the computation of <span class="emphasis"><em>KL Loss</em></span> and reparameterization trick. The third output of the encoder is the sampling of <span class="emphasis"><em>z</em></span> using the reparameterization trick. We should note that in the sampling function, </p><div class="mediaobject"><img src="graphics/B08956_08_083.jpg" alt="VAEs in Keras"/></div><p> since </p><div class="mediaobject"><img src="graphics/B08956_08_084.jpg" alt="VAEs in Keras"/></div><p> given that it's the standard deviation of the Gaussian distribution.</p><p>The decoder is also a two-layer MLP that takes samples of <span class="emphasis"><em>z</em></span> to approximate the inputs. Both the encoder and the decoder use an intermediate dimension with a size of 512.</p><p>The VAE network<a id="id361" class="indexterm"/> is simply both the encoder and the decoder joined together. <span class="emphasis"><em>Figures 8.1.3</em></span> to <span class="emphasis"><em>8.1.5</em></span> show the encoder, decoder, and VAE models. The loss function is the sum of both the <span class="emphasis"><em>Reconstruction Loss</em></span> and <span class="emphasis"><em>KL Loss</em></span>. The VAE network has good results on the default Adam optimizer. The total number of parameters of the VAE network is 807,700.</p><p>The Keras code for VAE MLP has pretrained weights. To test, we need to run:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python3 vae-mlp-mnist-8.1.1.py --weights=vae_mlp_mnist.h5</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>The complete code can be found on the following link: <a class="ulink" href="https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras">https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras</a>.</p></div></div><p>Listing 8.1.1, <code class="literal">vae-mlp-mnist-8.1.1.py</code> shows us the Keras code of VAE using MLP layers:</p><div class="informalexample"><pre class="programlisting"># reparameterization trick
# instead of sampling from Q(z|X), sample eps = N(0,I)
# z = z_mean + sqrt(var)*eps
def sampling(args):
    z_mean, z_log_var = args
    batch = K.shape(z_mean)[0]
    # K is the keras backend
    dim = K.int_shape(z_mean)[1]
    # by default, random_normal has mean=0 and std=1.0
    epsilon = K.random_normal(shape=(batch, dim))
    return z_mean + K.exp(0.5 * z_log_var) * epsilon

# MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

image_size = x_train.shape[1]
original_dim = image_size * image_size
x_train = np.reshape(x_train, [-1, original_dim])
x_test = np.reshape(x_test, [-1, original_dim])
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# network parameters
input_shape = (original_dim, )
intermediate_dim = 512
batch_size = 128
latent_dim = 2
epochs = 50

# VAE model = encoder + decoder
# build encoder model
inputs = Input(shape=input_shape, name='encoder_input')
x = Dense(intermediate_dim, activation='relu')(inputs)
z_mean = Dense(latent_dim, name='z_mean')(x)
z_log_var = Dense(latent_dim, name='z_log_var')(x)

# use reparameterization trick to push the sampling out as input
z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])
# instantiate encoder model
encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')
encoder.summary()
plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)

# build decoder model
latent_inputs = Input(shape=(latent_dim,), name='z_sampling')
x = Dense(intermediate_dim, activation='relu')(latent_inputs)
outputs = Dense(original_dim, activation='sigmoid')(x)

# instantiate decoder model
decoder = Model(latent_inputs, outputs, name='decoder')
decoder.summary()
plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)

# instantiate vae model
outputs = decoder(encoder(inputs)[2])
vae = Model(inputs, outputs, name='vae_mlp')

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    help_ = "Load h5 model trained weights"
    parser.add_argument("-w", "--weights", help=help_)
    help_ = "Use mse loss instead of binary cross entropy (default)"
    parser.add_argument("-m",
                        "--mse",
                        help=help_, action='store_true')
    args = parser.parse_args()
    models = (encoder, decoder)
    data = (x_test, y_test)
    # VAE loss = mse_loss or xent_loss + kl_loss
    if args.mse:
        reconstruction_loss = mse(inputs, outputs)
    else:
        reconstruction_loss = binary_crossentropy(inputs,
                                                  outputs)
    reconstruction_loss *= original_dim
    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
    kl_loss = K.sum(kl_loss, axis=-1)
    kl_loss *= -0.5
    vae_loss = K.mean(reconstruction_loss + kl_loss)
    vae.add_loss(vae_loss)
    vae.compile(optimizer='adam')
    vae.summary()
    plot_model(vae,
               to_file='vae_mlp.png',
               show_shapes=True)

    if args.weights:
        vae = vae.load_weights(args.weights)
    else:
        # train the autoencoder
        vae.fit(x_train,
                epochs=epochs,
                batch_size=batch_size,
                validation_data=(x_test, None))
        vae.save_weights('vae_mlp_mnist.h5')

    plot_results(models,
                 data,
                 batch_size=batch_size,
                 model_name="vae_mlp")</pre></div><div class="mediaobject"><img src="graphics/B08956_08_03.jpg" alt="VAEs in Keras"/><div class="caption"><p>Figure 8.1.3: The encoder models of VAE MLP</p></div></div><div class="mediaobject"><img src="graphics/B08956_08_04.jpg" alt="VAEs in Keras"/><div class="caption"><p>Figure 8.1.4: The decoder model of VAE MLP</p></div></div><div class="mediaobject"><img src="graphics/B08956_08_05.jpg" alt="VAEs in Keras"/><div class="caption"><p>Figure 8.1.5: The VAE model using MLP</p></div></div><p>
<span class="emphasis"><em>Figure 8.1.6</em></span> shows the continuous space of latent vector after 50 epochs using <code class="literal">plot_results()</code>. For simplicity, the function is not shown here but can be found in the rest of the<a id="id362" class="indexterm"/> code of <code class="literal">vae-mlp-mnist-8.1.1.py</code>. The function plots two images, the test dataset labels (<span class="emphasis"><em>Figure 8.1.6</em></span>) and the sample generated digits (<span class="emphasis"><em>Figure 8.1.7</em></span>) both as a function of <span class="emphasis"><em>z</em></span>. Both plots demonstrate how the latent vector determines the attributes of the generated digits.</p><p>Navigating through the continuous space will always result in an output that bears a resemblance to the MNIST digits. For example, the region of digit 9 is close to the region of digit 7. Moving from 9 near the center to the left morphs the digit to 7. Moving from the center downward changes the generated digits from 3 to 8 and finally to 1. The morphing of the digits is more evident in <span class="emphasis"><em>Figure 8.1.7</em></span> which is another way of interpreting <span class="emphasis"><em>Figure 8.1.6</em></span>.</p><p>In <span class="emphasis"><em>Figure 8.1.7</em></span>, instead of colorbar, the generator output is displayed. The distribution of digits in the latent space is shown. It can be observed that all the digits are represented. Since the distribution is dense near the center, the change is rapid in the middle and slow as the mean values get bigger. We need to remember that <span class="emphasis"><em>Figure 8.1.7</em></span> is a reflection of <span class="emphasis"><em>Figure 8.1.6</em></span>. For example, digit 0 is on the top right quadrant on both figures while digit 1 is on the lower right quadrant.</p><p>There are some unrecognizable digits in <span class="emphasis"><em>Figure 8.1.7</em></span>, especially on the top left quadrant. From the following figure, it can be observed that this region is mostly empty and far away from the center:</p><div class="mediaobject"><img src="graphics/B08956_08_06.jpg" alt="VAEs in Keras"/><div class="caption"><p>Figure 8.1.6: The latent vector mean values for the test dataset (VAE MLP). The colorbar shows the corresponding MNIST digit as a function of z. Color images can be found on the book GitHub repository: https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/tree/master/chapter8-vae.</p></div></div><div class="mediaobject"><img src="graphics/B08956_08_07.jpg" alt="VAEs in Keras"/><div class="caption"><p>Figure 8.1.7: The digits generated as a function of latent vector mean values (VAE MLP). For ease of interpretation, the range of values for the mean is similar to Figure 8.1.6.</p></div></div></div><div class="section" title="Using CNNs for VAEs"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec34"/>Using CNNs for VAEs</h2></div></div></div><p>In the<a id="id363" class="indexterm"/> original paper <span class="emphasis"><em>Auto-encoding Variational Bayes</em></span> [1], the VAE network was implemented using MLP, which is similar to what we covered in the previous section. In this section, we'll demonstrate that using a CNN will result in a significant improvement in the quality of the digits produced and a remarkable reduction in the number of parameters down to 134,165.</p><p>
<span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>8.1.3</em></span> shows the encoder, decoder, and VAE network. This code was also contributed to<a id="id364" class="indexterm"/> the official Keras GitHub repository. For conciseness, some lines of code that are similar to the MLP are no longer shown. The encoder is made of two layers of CNNs and two layers of MLPs in order to generate the latent code. The encoder output structure is similar to the MLP implementation seen in the previous section. The decoder is made up of one layer of MLP and three layers of transposed CNNs. <span class="emphasis"><em>Figures 8.1.8</em></span> to <span class="emphasis"><em>8.1.10</em></span> show the encoder, decoder, and VAE models. For VAE CNN, RMSprop will result in a lower loss than Adam.</p><p>The Keras code for VAE CNN has pre-trained weights. To test, we need to run:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python3 vae-cnn-mnist-8.1.2.py --weights=vae_cnn_mnist.h5</strong></span>
</pre></div><p>Listing 8.1.3, <code class="literal">vae-cnn-mnist-8.1.2.py</code> shows us the Keras code of VAE using CNN layers:</p><div class="informalexample"><pre class="programlisting"># network parameters
input_shape = (image_size, image_size, 1)
batch_size = 128
kernel_size = 3
filters = 16
latent_dim = 2
epochs = 30

# VAE mode = encoder + decoder
# build encoder model
inputs = Input(shape=input_shape, name='encoder_input')
x = inputs
for i in range(2):
    filters *= 2
    x = Conv2D(filters=filters,
               kernel_size=kernel_size,
               activation='relu',
               strides=2,
               padding='same')(x)

# shape info needed to build decoder model
shape = K.int_shape(x)

# generate latent vector Q(z|X)
x = Flatten()(x)
x = Dense(16, activation='relu')(x)
z_mean = Dense(latent_dim, name='z_mean')(x)
z_log_var = Dense(latent_dim, name='z_log_var')(x)

# use reparameterization trick to push the sampling out as input
# note that "output_shape" isn't necessary with the TensorFlow backend
z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])

# instantiate encoder model
encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')
encoder.summary()
plot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True)

# build decoder model
latent_inputs = Input(shape=(latent_dim,), name='z_sampling')
x = Dense(shape[1]*shape[2]*shape[3], activation='relu')(latent_inputs)
x = Reshape((shape[1], shape[2], shape[3]))(x)

for i in range(2): 
    x = Conv2DTranspose(filters=filters,
                        kernel_size=kernel_size,
                        activation='relu',
                        strides=2,
                        padding='same')(x)
    filters //= 2

outputs = Conv2DTranspose(filters=1,
                    kernel_size=kernel_size,
                    activation='sigmoid',
                    padding='same',
                    name='decoder_output')(x)

# instantiate decoder model
decoder = Model(latent_inputs, outputs, name='decoder')
decoder.summary()
plot_model(decoder, to_file='vae_cnn_decoder.png', show_shapes=True)

# instantiate vae model
outputs = decoder(encoder(inputs)[2])
vae = Model(inputs, outputs, name='vae')</pre></div><div class="mediaobject"><img src="graphics/B08956_08_08.jpg" alt="Using CNNs for VAEs"/><div class="caption"><p>Figure 8.1.8: The encoder of VAE CNN</p></div></div><div class="mediaobject"><img src="graphics/B08956_08_09.jpg" alt="Using CNNs for VAEs"/><div class="caption"><p>Figure 8.1.9: The decoder of VAE CNN</p></div></div><div class="mediaobject"><img src="graphics/B08956_08_10.jpg" alt="Using CNNs for VAEs"/><div class="caption"><p>Figure 8.1.10: The VAE model using CNNs</p></div></div><div class="mediaobject"><img src="graphics/B08956_08_11.jpg" alt="Using CNNs for VAEs"/><div class="caption"><p>Figure 8.1.11: The latent vector mean values for the test dataset (VAE CNN). The colorbar shows the corresponding MNIST digit as a function of z. Color images can be found on the book GitHub repository: https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/tree/master/chapter8-vae.</p></div></div><p>Preceding figure shows the continuous latent space of a VAE using the CNN implementation after 30 epochs. The region where each digit is assigned may be different, but the distribution<a id="id365" class="indexterm"/> is roughly the same. Following figure shows us the output of the generative model. Qualitatively, there are fewer digits that are ambiguous as compared to <span class="emphasis"><em>Figure 8.1.7</em></span> with the MLP implementation:</p><div class="mediaobject"><img src="graphics/B08956_08_12.jpg" alt="Using CNNs for VAEs"/><div class="caption"><p>Figure 8.1.12: The digits generated as a function of latent vector mean values (VAE CNN). For ease of interpretation, the range of values for the mean is similar to Figure 8.1.11.</p></div></div></div></div></div>
<div class="section" title="Conditional VAE (CVAE)"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec48"/>Conditional VAE (CVAE)</h1></div></div></div><p>Conditional VAE [2] is similar to the idea of CGAN. In the context of the MNIST dataset, if the latent space is<a id="id366" class="indexterm"/> randomly sampled, VAE has no control over which digit will be generated. CVAE is able to address this problem by including a condition (a one-hot label) of the digit to produce. The condition is imposed on both the encoder and decoder inputs.</p><p>Formally, the core equation of VAE in <span class="emphasis"><em>Equation</em></span> <span class="emphasis"><em>8.1.10</em></span> is modified to include the condition <span class="emphasis"><em>c</em></span>:</p><div class="mediaobject"><img src="graphics/B08956_08_085.jpg" alt="Conditional VAE (CVAE)"/></div><p>     (Equation 8.2.1)</p><p>Similar to VAEs, <span class="emphasis"><em>Equation</em></span> <span class="emphasis"><em>8.2.1</em></span> means that if we want to maximize the output conditioned on <span class="emphasis"><em>c</em></span>, </p><div class="mediaobject"><img src="graphics/B08956_08_086.jpg" alt="Conditional VAE (CVAE)"/></div><p>, then the two loss terms must be minimized:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Reconstruction loss of the decoder given both the latent vector and the condition.</li><li class="listitem" style="list-style-type: disc">KL loss between the encoder given both the latent vector and the condition and the prior distribution given the condition. Similar to a VAE, we typically choose <div class="mediaobject"><img src="graphics/B08956_08_087.jpg" alt="Conditional VAE (CVAE)"/></div><p>.</p></li></ul></div><p>Listing 8.2.1, <code class="literal">cvae-cnn-mnist-8.2.1.py</code> shows us the Keras code of CVAE using CNN layers. In the code that is highlighted showcases the changes made to support CVAE:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># compute the number of labels</strong></span>
<span class="strong"><strong>num_labels = len(np.unique(y_train))</strong></span>

# network parameters
input_shape = (image_size, image_size, 1)
<span class="strong"><strong>label_shape = (num_labels, )</strong></span>
batch_size = 128
kernel_size = 3
filters = 16
latent_dim = 2
epochs = 30

# VAE model = encoder + decoder
# build encoder model
inputs = Input(shape=input_shape, name='encoder_input')
<span class="strong"><strong>y_labels = Input(shape=label_shape, name='class_labels')</strong></span>
<span class="strong"><strong>x = Dense(image_size * image_size)(y_labels)</strong></span>
<span class="strong"><strong>x = Reshape((image_size, image_size, 1))(x)</strong></span>
<span class="strong"><strong>x = keras.layers.concatenate([inputs, x])</strong></span>
for i in range(2):
    filters *= 2
    x = Conv2D(filters=filters,
               kernel_size=kernel_size,
               activation='relu',
               strides=2,
               padding='same')(x)

# shape info needed to build decoder model
shape = K.int_shape(x)

# generate latent vector Q(z|X)
x = Flatten()(x)
x = Dense(16, activation='relu')(x)
z_mean = Dense(latent_dim, name='z_mean')(x)
z_log_var = Dense(latent_dim, name='z_log_var')(x)

# use reparameterization trick to push the sampling out as input
# note that "output_shape" isn't necessary with the TensorFlow backend
z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])

# instantiate encoder model
<span class="strong"><strong>encoder = Model([inputs, y_labels], [z_mean, z_log_var, z], name='encoder')</strong></span>
<span class="strong"><strong>encoder.summary()</strong></span>
<span class="strong"><strong>plot_model(encoder, to_file='cvae_cnn_encoder.png', show_shapes=True)</strong></span>

# build decoder model
latent_inputs = Input(shape=(latent_dim,), name='z_sampling')
<span class="strong"><strong>x = keras.layers.concatenate([latent_inputs, y_labels])</strong></span>
x = Dense(shape[1]*shape[2]*shape[3], activation='relu')(x)
x = Reshape((shape[1], shape[2], shape[3]))(x)
for i in range(2):
    x = Conv2DTranspose(filters=filters,
                        kernel_size=kernel_size,
                        activation='relu',
                        strides=2,
                        padding='same')(x)
    filters //= 2

outputs = Conv2DTranspose(filters=1,
                          kernel_size=kernel_size,
                          activation='sigmoid',
                          padding='same',
                          name='decoder_output')(x)

# instantiate decoder model
<span class="strong"><strong>decoder = Model([latent_inputs, y_labels], outputs, name='decoder')</strong></span>
decoder.summary()
<span class="strong"><strong>plot_model(decoder, to_file='cvae_cnn_decoder.png', show_shapes=True)</strong></span>

# instantiate vae model
<span class="strong"><strong>outputs = decoder([encoder([inputs, y_labels])[2], y_labels])</strong></span>
<span class="strong"><strong>cvae = Model([inputs, y_labels], outputs, name='cvae')</strong></span>
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    help_ = "Load h5 model trained weights"
    parser.add_argument("-w", "--weights", help=help_)
    help_ = "Use mse loss instead of binary cross entropy (default)"
    parser.add_argument("-m", "--mse", help=help_, action='store_true')
<span class="strong"><strong>    help_ = "Specify a specific digit to generate"</strong></span>
<span class="strong"><strong>    parser.add_argument("-d", "--digit", type=int, help=help_)</strong></span>
<span class="strong"><strong>    help_ = "Beta in Beta-CVAE. Beta &gt; 1. Default is 1.0 (CVAE)"</strong></span>
<span class="strong"><strong>    parser.add_argument("-b", "--beta", type=float, help=help_)</strong></span>
    args = parser.parse_args()
    models = (encoder, decoder)
    data = (x_test, y_test)

<span class="strong"><strong>    if args.beta is None or args.beta &lt; 1.0:</strong></span>
<span class="strong"><strong>        beta = 1.0</strong></span>
<span class="strong"><strong>        print("CVAE")</strong></span>
<span class="strong"><strong>        model_name = "cvae_cnn_mnist"</strong></span>
<span class="strong"><strong>    else:</strong></span>
<span class="strong"><strong>        beta = args.beta</strong></span>
<span class="strong"><strong>        print("Beta-CVAE with beta=", beta)</strong></span>
<span class="strong"><strong>        model_name = "beta-cvae_cnn_mnist"</strong></span>

    # VAE loss = mse_loss or xent_loss + kl_loss
    if args.mse:
        reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))
    else:
        reconstruction_loss = binary_crossentropy(K.flatten(inputs),
                                                  K.flatten(outputs))

    reconstruction_loss *= image_size * image_size
    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
    kl_loss = K.sum(kl_loss, axis=-1)
<span class="strong"><strong>    kl_loss *= -0.5 * beta</strong></span>
<span class="strong"><strong>    cvae_loss = K.mean(reconstruction_loss + kl_loss)</strong></span>
<span class="strong"><strong>    cvae.add_loss(cvae_loss)</strong></span>
<span class="strong"><strong>    cvae.compile(optimizer='rmsprop')</strong></span>
<span class="strong"><strong>    cvae.summary()</strong></span>
<span class="strong"><strong>    plot_model(cvae, to_file='cvae_cnn.png', show_shapes=True)</strong></span>

<span class="strong"><strong>    if args.weights:</strong></span>
<span class="strong"><strong>        cvae = cvae.load_weights(args.weights)</strong></span>
<span class="strong"><strong>    else:</strong></span>
<span class="strong"><strong>        # train the autoencoder</strong></span>
<span class="strong"><strong>        cvae.fit([x_train, to_categorical(y_train)],</strong></span>
<span class="strong"><strong>                 epochs=epochs,</strong></span>
<span class="strong"><strong>                 batch_size=batch_size,</strong></span>
<span class="strong"><strong>                 validation_data=([x_test, to_categorical(y_test)], None))</strong></span>
<span class="strong"><strong>        cvae.save_weights(model_name + '.h5')</strong></span>

<span class="strong"><strong>    if args.digit in range(0, num_labels):</strong></span>
<span class="strong"><strong>        digit = np.array([args.digit])</strong></span>
<span class="strong"><strong>    else:</strong></span>
<span class="strong"><strong>        digit = np.random.randint(0, num_labels, 1)</strong></span>

<span class="strong"><strong>    print("CVAE for digit %d" % digit)</strong></span>
<span class="strong"><strong>    y_label = np.eye(num_labels)[digit]</strong></span>
<span class="strong"><strong>    plot_results(models,</strong></span>
<span class="strong"><strong>                 data,</strong></span>
<span class="strong"><strong>                 y_label=y_label,</strong></span>
<span class="strong"><strong>                 batch_size=batch_size,</strong></span>
<span class="strong"><strong>                 model_name=model_name)</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B08956_08_13.jpg" alt="Conditional VAE (CVAE)"/><div class="caption"><p>Figure 8.2.1: The encoder in CVAE CNN. The input is now made of the concatenation of the VAE input and a conditioning label.</p></div></div><div class="mediaobject"><img src="graphics/B08956_08_14.jpg" alt="Conditional VAE (CVAE)"/><div class="caption"><p>Figure 8.2.2: The decoder in CVAE CNN. The input is now made of the concatenation of the z sampling and a conditioning label.</p></div></div><div class="mediaobject"><img src="graphics/B08956_08_15.jpg" alt="Conditional VAE (CVAE)"/><div class="caption"><p>Figure 8.2.3: The CVAE model using a CNN. The input is now made of a VAE input and a conditioning label.</p></div></div><p>Implementing CVAE requires a few modifications in the code of the VAE. For the CVAE, the VAE CNN implementation is used. <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>8.2.1</em></span> highlights the changes made to the original code<a id="id367" class="indexterm"/> of VAE for MNIST digits. The encoder input is now a concatenation of original input image and its one-hot label. The decoder input is now a combination of the latent space sampling and the one-hot label of the image it should generate. The total number of parameters is 174, 437. The codes related to </p><div class="mediaobject"><img src="graphics/B08956_08_088.jpg" alt="Conditional VAE (CVAE)"/></div><p>-VAE will be discussed in the next section of this chapter.</p><p>There are no changes in the loss function. However, the one-hot labels are supplied during training, testing, and plotting of results. <span class="emphasis"><em>Figures 8.2.1</em></span> to <span class="emphasis"><em>8.2.3</em></span> show us the encoder, decoder, and CVAE models. The role of the conditioning label in the form of a one-hot vector is indicated.</p><div class="mediaobject"><img src="graphics/B08956_08_16.jpg" alt="Conditional VAE (CVAE)"/><div class="caption"><p>Figure 8.2.4: The latent vector mean values for the test dataset (CVAE CNN). The colorbar shows the corresponding MNIST digit as a function of z. Color images can be found on the book GitHub repository: https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/tree/master/chapter8-vae.</p></div></div><div class="mediaobject"><img src="graphics/B08956_08_17.jpg" alt="Conditional VAE (CVAE)"/><div class="caption"><p>Figure 8.2.5: Digits 0 to 5 generated as a function of latent vector mean values and one-hot label (CVAE CNN). For ease of interpretation, the range of values for the mean is similar to Figure 8.2.4.</p></div></div><div class="mediaobject"><img src="graphics/B08956_08_18.jpg" alt="Conditional VAE (CVAE)"/><div class="caption"><p>Figure 8.2.6: Digits 6 to 9 generated as a function of latent vector mean values and one-hot label (CVAE CNN). For ease of interpretation, the range of values for the mean is similar to Figure 8.2.4.</p></div></div><p>In <span class="emphasis"><em>Figure 8.2.4</em></span>, the distribution of mean per label is shown after 30 epochs. Unlike in both <span class="emphasis"><em>Figures 8.1.6</em></span> and <span class="emphasis"><em>8.1.11</em></span> in the previous sections, each label is not concentrated on a region but distributed across the plot. This is expected since every sampling in the latent space should<a id="id368" class="indexterm"/> generate a specific digit. Navigating the latent space changes the attribute of that specific digit. For example, if the digit specified is 0, then navigating the latent space will still produce a 0 but the attributes, such as tilt angle, thickness, and other writing style aspects will be different.</p><p>These changes are more clearly shown in <span class="emphasis"><em>Figures 8.2.5</em></span> and <span class="emphasis"><em>8.2.6</em></span>. For ease of comparison, the range of values for the latent vector is the same as in <span class="emphasis"><em>Figure 8.2.4</em></span>. Using the pretrained weights, a digit (for example, 0) can be generated by executing the command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python3 cvae-cnn-mnist-8.2.1.py --weights=cvae_cnn_mnist.h5 --digit=0</strong></span>
</pre></div><p>In <span class="emphasis"><em>Figures 8.2.5</em></span> and <span class="emphasis"><em>8.2.6</em></span>, it can be noticed that the width and roundness (if applicable) of each digit change as <span class="emphasis"><em>z</em></span>[0] is traced from left to right. Meanwhile, the tilt angle and roundness (if applicable) of each digit change as <span class="emphasis"><em>z</em></span>[1] is navigated from top to bottom. As we move away from the center of the distribution, the image of the digit starts to degrade. This is expected since the latent space is a circle.</p><p>Other noticeable<a id="id369" class="indexterm"/> variations in attributes may be digit specific. For example, the horizontal stroke (arm) for digit 1 becomes visible in the upper left quadrant. The horizontal stroke (crossbar) for digit 7 can be seen in the right quadrants only.</p></div>
<div class="section" title="-VAE: VAE with disentangled latent representations"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec49"/>
<span class="inlinemediaobject"><img src="graphics/B08956_08_089.jpg" alt="Conditional VAE (CVAE)"/></span>-VAE: VAE with disentangled latent representations
</h1></div></div></div><p>In <a class="link" href="ch06.html" title="Chapter 6. Disentangled Representation GANs">Chapter 6</a>, <span class="emphasis"><em>Disentangled Representation GANs</em></span>, the concept, and importance of the disentangled representation of latent codes were discussed. We can recall that a disentangled representation is where single latent units are sensitive to changes in single generative factors while being relatively invariant to changes in other factors [3]. Varying a latent code results<a id="id370" class="indexterm"/> to changes in one attribute of the generated output while the rest of the properties remain the same.</p><p>In the same chapter, InfoGANs [4] demonstrated to us that in the MNIST dataset, it is possible to control which digit to generate and the tilt and thickness of writing style. Observing the results in the previous section, it can be noticed that the VAE is intrinsically disentangling the latent vector dimensions to a certain extent. For example, looking at digit 8 in <span class="emphasis"><em>Figure 8.2.6</em></span>, navigating <span class="emphasis"><em>z</em></span>[1] from top to bottom decreases the width and roundness while rotating the digit clockwise. Increasing <span class="emphasis"><em>z</em></span>[0] from left to right also decreases the width and roundness while rotating the digit counterclockwise. In other words, <span class="emphasis"><em>z</em></span>[1] controls the clockwise rotation, <span class="emphasis"><em>z</em></span>[0] affects the counterclockwise rotation, and both of them alter the width and roundness.</p><p>In this section, we'll demonstrate that a simple modification in the loss function of VAE forces the latent codes to disentangle further. The modification is the positive constant weight, </p><div class="mediaobject"><img src="graphics/B08956_08_090.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>, acting as a regularizer on the KL loss:</p><div class="mediaobject"><img src="graphics/B08956_08_091.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>          (Equation 8.3.1)</p><p>This variation of VAE is called </p><div class="mediaobject"><img src="graphics/B08956_08_092.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>-VAE [5]. The implicit effect of </p><div class="mediaobject"><img src="graphics/B08956_08_093.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p> is a tighter standard deviation. In other words, </p><div class="mediaobject"><img src="graphics/B08956_08_094.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p> forces the latent codes in the posterior distribution, </p><div class="mediaobject"><img src="graphics/B08956_08_095.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p> to be independent.</p><p>It is straightforward to implement </p><div class="mediaobject"><img src="graphics/B08956_08_096.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>-VAE. For example, for the CVAE from the previous, the required modification is the extra <span class="strong"><strong>beta</strong></span> factor in <code class="literal">kl_loss</code>.</p><div class="informalexample"><pre class="programlisting">    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
    kl_loss = K.sum(kl_loss, axis=-1)
    kl_loss *= -0.5 * <span class="strong"><strong>beta</strong></span>
</pre></div><p>CVAE is a special case of </p><div class="mediaobject"><img src="graphics/B08956_08_097.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>-VAE with </p><div class="mediaobject"><img src="graphics/B08956_08_098.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>. Everything else is the same. However, determining the value of </p><div class="mediaobject"><img src="graphics/B08956_08_099.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p> requires some trial and error. There must be a careful balance between the<a id="id371" class="indexterm"/> reconstruction error and regularization for latent codes independence. The disentanglement is maximized at around </p><div class="mediaobject"><img src="graphics/B08956_08_100.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>. When the value of </p><div class="mediaobject"><img src="graphics/B08956_08_101.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>, the </p><div class="mediaobject"><img src="graphics/B08956_08_102.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>-VAE is forced to learn one disentangled representation only while muting the other latent dimension:</p><div class="mediaobject"><img src="graphics/B08956_08_19.jpg" alt="-VAE: VAE with disentangled latent representations"/><div class="caption"><p>Figure 8.3.1: The latent vector mean values for the test dataset (</p><div class="mediaobject"><img src="graphics/B08956_08_103.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>-VAE with </p><div class="mediaobject"><img src="graphics/B08956_08_104.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>) Color images can be found on the book GitHub repository: https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/tree/master/chapter8-vae.</p></div></div><p>
<span class="emphasis"><em>Figures 8.3.1</em></span> and <span class="emphasis"><em>8.3.2</em></span> show the latent vector means for </p><div class="mediaobject"><img src="graphics/B08956_08_105.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>-VAE with </p><div class="mediaobject"><img src="graphics/B08956_08_106.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p> and </p><div class="mediaobject"><img src="graphics/B08956_08_107.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>. With </p><div class="mediaobject"><img src="graphics/B08956_08_108.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>, the distribution has a smaller standard deviation when compared to CVAE. With </p><div class="mediaobject"><img src="graphics/B08956_08_109.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>, there is only the latent code that is learned. The distribution is practically shrunk<a id="id372" class="indexterm"/> to 1D with the first latent code <span class="emphasis"><em>z</em></span>[0] ignored by the encoder and decoder:</p><div class="mediaobject"><img src="graphics/B08956_08_20.jpg" alt="-VAE: VAE with disentangled latent representations"/><div class="caption"><p>Figure 8.3.2: The latent vector mean values for the test dataset (</p><div class="mediaobject"><img src="graphics/B08956_08_110.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>-VAE with </p><div class="mediaobject"><img src="graphics/B08956_08_111.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>)Color images can be found on the book GitHub repository: https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/tree/master/chapter8-vae.</p></div></div><p>These observations are reflected in <span class="emphasis"><em>Figure 8.3.3</em></span>. </p><div class="mediaobject"><img src="graphics/B08956_08_112.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>-VAE with </p><div class="mediaobject"><img src="graphics/B08956_08_113.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p> has two latent codes that are<a id="id373" class="indexterm"/> practically independent. <span class="emphasis"><em>z</em></span>[0] determines the tilt of the writing style. Meanwhile, <span class="emphasis"><em>z</em></span>[1] specifies the width and roundness (if applicable) of the digits. For </p><div class="mediaobject"><img src="graphics/B08956_08_114.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>-VAE with </p><div class="mediaobject"><img src="graphics/B08956_08_115.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>, <span class="emphasis"><em>z</em></span>[0] is muted. Increasing <span class="emphasis"><em>z</em></span>[0] does not alter the digit in a significant way. <span class="emphasis"><em>z</em></span>[1] determines the tilt angle and<a id="id374" class="indexterm"/> width of the writing style.</p><div class="mediaobject"><img src="graphics/B08956_08_21.jpg" alt="-VAE: VAE with disentangled latent representations"/><div class="caption"><p>Figure 8.3.3: Digits 0 to 3 generated as a function of latent vector mean values and one-hot label (</p><div class="mediaobject"><img src="graphics/B08956_08_116.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>-VAE </p><div class="mediaobject"><img src="graphics/B08956_08_117.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>). For ease of interpretation, the range of values for the mean is similar to Figure 8.3.1.</p></div></div><p>The Keras code for </p><div class="mediaobject"><img src="graphics/B08956_08_118.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>-VAE has pre-trained weights. To test </p><div class="mediaobject"><img src="graphics/B08956_08_119.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p>-VAE with </p><div class="mediaobject"><img src="graphics/B08956_08_120.jpg" alt="-VAE: VAE with disentangled latent representations"/></div><p> generating digit 0, we need to run:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python3 cvae-cnn-mnist-8.2.1.py --beta=7 --weights=beta-cvae_cnn_mnist.h5 --digit=0</strong></span>
</pre></div></div>
<div class="section" title="Conclusion"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec50"/>Conclusion </h1></div></div></div><p>In this chapter, we've covered the principles of variational autoencoders (VAEs). As we learned in the principles of VAEs, they bear a resemblance to<a id="id375" class="indexterm"/> GANs in the aspect of both attempt to create synthetic outputs from latent space. However, it can be noticed that the VAE networks are much simpler and easier to train compared to GANs. It's becoming clear how conditional VAE and </p><div class="mediaobject"><img src="graphics/B08956_08_121.jpg" alt="Conclusion"/></div><p>-VAE are similar in concept to conditional GAN and disentangled representation GAN respectively.</p><p>VAEs have an intrinsic mechanism to disentangle the latent vectors. Therefore, building a </p><div class="mediaobject"><img src="graphics/B08956_08_122.jpg" alt="Conclusion"/></div><p>-VAE is straightforward. We should note however that interpretable and disentangled codes are important in building intelligent agents.</p><p>In the next chapter, we're going to focus on Reinforcement learning. Without any prior data, an agent learns by interacting with its world. We'll discuss how the agent can be rewarded for correct actions and punished for the wrong ones.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec51"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Diederik P. Kingma and Max Welling. <span class="emphasis"><em>Auto-encoding Variational Bayes</em></span>. arXiv preprint arXiv:1312.6114, 2013(<a class="ulink" href="https://arxiv.org/pdf/1312.6114.pdf">https://arxiv.org/pdf/1312.6114.pdf</a>).</li><li class="listitem">Kihyuk Sohn, Honglak Lee, and Xinchen Yan. <span class="emphasis"><em>Learning Structured Output Representation Using Deep Conditional Generative Models</em></span>. Advances in Neural Information Processing Systems, 2015(<a class="ulink" href="http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf">http://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf</a>).</li><li class="listitem">Yoshua Bengio, Aaron Courville, and Pascal Vincent. <span class="emphasis"><em>Representation Learning: A Review and New Perspectives</em></span>. IEEE transactions on Pattern Analysis and Machine Intelligence 35.8, 2013: 1798-1828(<a class="ulink" href="https://arxiv.org/pdf/1206.5538.pdf">https://arxiv.org/pdf/1206.5538.pdf</a>).</li><li class="listitem">Xi Chen and others. <span class="emphasis"><em>Infogan: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</em></span>. Advances in Neural Information Processing Systems, 2016(<a class="ulink" href="http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf">http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf</a>).</li><li class="listitem">I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. <div class="mediaobject"><img src="graphics/B08956_08_123.jpg" alt="References"/></div><p>
<span class="emphasis"><em>-VAE: Learning basic visual concepts with a constrained variational framework</em></span>. ICLR, 2017(<a class="ulink" href="https://openreview.net/pdf?id=Sy2fzU9gl">https://openreview.net/pdf?id=Sy2fzU9gl</a>).</p></li><li class="listitem">Carl Doersch. <span class="emphasis"><em>Tutorial on variational autoencoders</em></span>. arXiv preprint arXiv:1606.05908, 2016 (<a class="ulink" href="https://arxiv.org/pdf/1606.05908.pdf">https://arxiv.org/pdf/1606.05908.pdf</a>).</li><li class="listitem">Luc Devroye. <span class="emphasis"><em>Sample-Based Non-Uniform Random Variate Generation</em></span>. Proceedings of the 18th conference on Winter simulation. ACM, 1986(<a class="ulink" href="http://www.eirene.de/Devroye.pdf">http://www.eirene.de/Devroye.pdf</a>).</li></ol></div></div></body></html>