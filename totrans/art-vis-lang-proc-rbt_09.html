<html><head></head><body>
		<div id="_idContainer227" class="Content">
			<h1 id="_idParaDest-151"><em class="italics"><a id="_idTextAnchor194"/>Chapter 9</em></h1>
		</div>
		<div id="_idContainer228" class="Content">
			<h1 id="_idParaDest-152"><a id="_idTextAnchor195"/>Computer Vision for Robotics</h1>
		</div>
		<div id="_idContainer229" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Evaluate objects using artificial vision</li>
				<li class="bullets">Combine external frameworks with ROS</li>
				<li class="bullets">Use a robot to interact with objects</li>
				<li class="bullets">Create a robot to understand natural language</li>
				<li class="bullets">Develop your own end-to-end robotics applications</li>
			</ul>
			<p>In this chapter, you'll learn how to work with Darknet and YOLO. You'll also evaluate objects using AI and integrate YOLO and ROS to enable your virtual robot to predict objects in the virtual environment.</p>
		</div>
		<div id="_idContainer245" class="Content">
			<h2 id="_idParaDest-153"><a id="_idTextAnchor196"/><a id="_idTextAnchor197"/><a id="_idTextAnchor198"/><a id="_idTextAnchor199"/><a id="_idTextAnchor200"/>Introduction</h2>
			<p>In previous chapters, you came across many technologies and techniques that may be new to you. You have learned many concepts and techniques that help solve real-world problems. Now, you are going to use all the acquired skills to complete this last chapter and build your own end-to-end robotics application.</p>
			<p>In this chapter, you'll use a deep learning framework, Darknet, to build robots that recognize objects in real time. This framework will be integrated with ROS so that the final application can be applied to any robot. Furthermore, it's important to say that object recognition can be used for building different kinds of robotics applications.</p>
			<p>The end-to-end applications you are going to build will not only have academic value but will also be useful for real-world problems and live situations. You will even be able to adapt how the application functions depending on circumstances. This will give you a lot of opportunities to solve real-world problems when working with robots.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor201"/>Darknet</h2>
			<p>Darknet is an open source neural network framework, which has been written in C and CUDA. It is very fast, as it allows GPU as well as CPU computation. It was developed by Joseph Redmon, a computer scientist focused on artificial vision.</p>
			<p>Although we are not going to study all of the functionalities in this chapter, Darknet includes a lot of interesting applications. As we mentioned earlier, we are going to use YOLO, but the following is a list of other Darknet functionalities:</p>
			<ul>
				<li><strong class="keyword">ImageNet Classification</strong>: This is an image classifier, which uses known models such as AlexNet, ResNet, and ResNeXt. After classifying some ImageNet images with all these models, a comparison between them is performed. They are based on time, accuracy, weights etc..</li>
				<li><strong class="keyword">RNN's</strong>: Recurrent neural networks are used for generating and managing natural language. They use an architecture called a vanilla RNN with three recurrent modules, which achieves good results in tasks such as speech recognition and natural language processing.</li>
				<li><strong class="keyword">Tiny Darknet</strong>: Consists of another image classifier, but this time, the generated model is much lighter. This network obtains similar results to Darknet, but the model weight is only 4 MB.<h4>Note</h4><p class="callout">Apart from the preceding, Darknet has some other applications as well. You can get more information about the framework by heading to its website: <a href="https://pjreddie.com/darknet/">https://pjreddie.com/darknet/</a>.</p></li>
			</ul>
			<h3 id="_idParaDest-155"><a id="_idTextAnchor202"/>Basic Installation of Darknet</h3>
			<p>The Darknet basic installation won't let you use the entire YOLO power, but it will be enough to check how it works and make your first few object detection predictions. It won't let you use GPU computation to make real-time predictions. For more complex tasks, go to the next section.</p>
			<h4>Note</h4>
			<p class="callout">For detailed steps regarding the basic and advanced installation of Darknet, refer to the preface, page vii.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor203"/>YOLO</h2>
			<p>YOLO is a real-time object detection system based on deep learning and is included in the Darknet framework. Its name comes from the acronym <em class="italics">You Only Look Once</em>, which references to how fast YOLO can work. </p>
			<p>On the website (<a href="https://pjreddie.com/darknet/yolo/">https://pjreddie.com/darknet/yolo/</a>), the author has added an image where this system is compared to others with the same purpose:</p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="image/C13550_09_01.jpg" alt="Figure 9.1: A comparison of object detection systems"/>
				</div>
			</div>
			<h6>Figure 9.1: A comparison of object detection systems</h6>
			<p>In the preceding graphic, the <strong class="bold">y</strong> axis represents the <strong class="keyword">mAP</strong> (mean Average Precision), and the <strong class="bold">x</strong> axis represents the time in milliseconds. So, you can see that YOLO achieves a higher mAP in lesser time than the other systems.</p>
			<p>It is also important to understand how YOLO works. It uses a neural network, which is applied to the entire image and splits it into different parts, predicting the bounding boxes. These bounding boxes are similar to rectangles marking off certain objects, which will be identified later in the process. YOLO is fast, because it is able to make predictions with only an evaluation of the neural network, while other recognition systems need several evaluations.</p>
			<p>The mentioned network has 53 convolutional layers, alternating 3x3 and 1x1 layers. Here's an image of the architecture extracted from a YOLO author's paper (<a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a>):</p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="image/C13550_09_02.jpg" alt="Figure 9.2: The YOLO architecture"/>
				</div>
			</div>
			<h6>Figure 9.2: The YOLO architecture</h6>
			<h3 id="_idParaDest-157"><a id="_idTextAnchor204"/>First Steps in Image Classification with YOLO</h3>
			<p>In this section, we are going to make our first predictions with YOLO. You are required to complete the basic installation. Let's start recognizing objects in a single image:</p>
			<ol>
				<li>We are going to use a pretrained model in order to avoid the training process, so the first step is to download the network weights in the Darknet directory:<p class="snippet">cd &lt;darknet_path&gt;</p><p class="snippet">wget https://pjreddie.com/media/files/yolov3.weights</p></li>
				<li>After that, we are going to make predictions with YOLO. In this first example, we are trying to recognize a single object, a dog. This is the sample image we are using:</li>
			</ol>
			<div>
				<div id="_idContainer232" class="IMG---Figure">
					<img src="image/C13550_09_03.jpg" alt="Figure 9.3: Sample image to predict"/>
				</div>
			</div>
			<h6>Figure 9.3: Sample image to predict</h6>
			<p>Save this image as a .jpg file in the Darknet directory and run YOLO on it:</p>
			<p class="snippet">./darknet detect cfg/yolov3.cfg yolov3.weights dog.jpg</p>
			<p>When the execution is finished, you should see an output like the following:</p>
			<div>
				<div id="_idContainer233" class="IMG---Figure">
					<img src="image/C13550_09_04.jpg" alt="Figure 9.4: The predicted output"/>
				</div>
			</div>
			<h6>Figure 9.4: The predicted output</h6>
			<p>As you can see, YOLO detects that there's a dog in the image with 100% accuracy. It also generates a new file named <strong class="inline">predictions.jpg</strong>, where it is possible to see the location of the dog in the image. You can open it from the Darknet directory:</p>
			<div>
				<div id="_idContainer234" class="IMG---Figure">
					<img src="image/Image47221.jpg" alt="Figure 9.5: Recognized objects in the image"/>
				</div>
			</div>
			<h6>Figure 9.5: Recognized objects in the image</h6>
			<p>Another possibility when using YOLO is to make predictions for several images with a single execution. To do this, you must enter the same command as before, but this time do not enter the image path:</p>
			<p class="snippet">./darknet detect cfg/yolov3.cfg yolov3.weights</p>
			<p>In this case, you will see the following output:</p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<img src="image/C13550_09_06.jpg" alt=""/>
				</div>
			</div>
			<h6>Figure 9.6: The prediction command output</h6>
			<p>As you can see, it is asking you to enter an image. You could enter, for instance, the same image as before by typing <strong class="inline">dog.jpg</strong>. You'll then be asked to enter another image path. This way, you can make predictions for all the images you want. This could be an example:</p>
			<div>
				<div id="_idContainer236" class="IMG---Figure">
					<img src="image/C13550_09_07.jpg" alt="Figure 9.7: The output after image prediction"/>
				</div>
			</div>
			<h6>Figure 9.7: The output after image prediction</h6>
			<p>If you do so, you will obtain this image:</p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="image/C13550_09_08.jpg" alt="Figure 9.8: Image prediction"/>
				</div>
			</div>
			<h6>Figure 9.8: Image prediction</h6>
			<p>There's one more interesting command to know when working with YOLO. It can be used to modify the detection threshold.</p>
			<h4>Note</h4>
			<p class="callout">The detection threshold is an accuracy limit to consider if a prediction is incorrect. For example, if you set your threshold to 0.75, objects detected with a lower accuracy won't be considered as a correct prediction.</p>
			<p>By default, YOLO includes an object in its output when it is predicted with an accuracy of 0.25 or higher. You can change the threshold value using the last flag of the following command:</p>
			<p class="snippet">./darknet detect cfg/yolov3.cfg yolov3.weights dog2.jpg -thresh 0.5</p>
			<p>As you may suppose, the preceding command sets the threshold to 0.5. Let's look at a practical example of this. Follow these steps to test the functioning of the threshold modification:</p>
			<ol>
				<li value="1">Make predictions for images until you find one where an object is predicted with less than 100% accuracy. We are going to use this as an example, where the dog is recognized with 60% accuracy:<div id="_idContainer238" class="IMG---Figure"><img src="image/C13550_09_09.jpg" alt="Figure 9.9: Example image with less than 100% accuracy"/></div><h6>Figure 9.9: Example image with less than 100% accuracy</h6></li>
				<li>Now, use the <strong class="inline">predict</strong> command modifying the detection threshold. As the dog is detected with 60% accuracy, if we change the threshold to 70%, no object should be detected:<p class="snippet">./darknet detect cfg/yolov3.cfg yolov3.weights dog2.jpg -thresh 0.7</p></li>
				<li>If we check the <strong class="inline">predictions</strong> file, we can confirm that the dog was not detected. Hence, you can see how threshold plays an important role in recognition as well:</li>
			</ol>
			<div>
				<div id="_idContainer239" class="IMG---Figure">
					<img src="image/C13550_09_10.jpg" alt="Figure 9.10: The final prediction with the modified threshold"/>
				</div>
			</div>
			<h6>Figure 9.10: The final prediction with the modified threshold</h6>
			<h3 id="_idParaDest-158"><a id="_idTextAnchor205"/>YOLO on a Webcam</h3>
			<p>Once you have made your first predictions with YOLO, it's time to try a more interesting feature of this system. You're going to detect your own real objects by connecting YOLO to your personal webcam. To do this, you must complete the advanced installation because it needs a GPU and OpenCV:</p>
			<ol>
				<li value="1">Make sure your webcam is connected and can be detected by your system.</li>
				<li>Enter the following command in the Darknet directory:<p class="snippet">./darknet detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights</p></li>
				<li>Try to recognize an object in your environment; for example, we have detected the books on our shelves:</li>
			</ol>
			<div>
				<div id="_idContainer240" class="IMG---Figure">
					<img src="image/C13550_09_11.jpg" alt="Figure 9.11: Books recognized using a webcam"/>
				</div>
			</div>
			<h6>Figure 9.11: Books recognized using a webcam</h6>
			<h3 id="_idParaDest-159"><a id="_idTextAnchor206"/>Exercise 28: Programming with YOLO</h3>
			<p>In this exercise, we are going to see how to make predictions with YOLO using Python. We will create a dataset and check how many images containing a certain object are present in the dataset. To build the dataset, check the following images:</p>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="image/C13550_09_12.jpg" alt="Figure 9.12: Images contained in the dataset"/>
				</div>
			</div>
			<h6>Figure 9.12: Images contained in the dataset</h6>
			<p>As you can see, it is a very simple dataset containing animals and landscape images. The Python program you are going to implement will have to obtain the number of images in which dogs appear.</p>
			<p>We will begin by cloning Darknet files from GitHub:</p>
			<p class="snippet">git clone https://github.com/pjreddie/darknet</p>
			<p class="snippet">cd darknet</p>
			<p class="snippet">make</p>
			<ol>
				<li value="1">Create a new folder named <strong class="inline">dataset</strong> in the Darknet directory.</li>
				<li>Place these images or others of your choice inside the new folder.<h4>Note</h4><p class="callout">The images can be found in the Chapter 9/exercise28/dataset/ folder on GitHub</p><p class="callout">URL: <a href="https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson09/Exercise28/dataset">https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson09/Exercise28/dataset</a></p></li>
				<li>Create a Python file, <strong class="inline">excercise1.py</strong>, and start the implementation.<p>Import Python itself and the required libraries:</p><p class="snippet">#!usr/bin/env python</p><p class="snippet">import sys, os</p></li>
				<li>Tell the system where to find the Darknet framework and then import it. If you have created a file inside the Darknet directory, you can do this as follows:<h4>Note</h4><p class="callout">The path set here needs to be the path where you have placed your Darknet directory.</p><p class="snippet">sys.path.append(os.path.join(os.getcwd(),'python/'))</p><p class="snippet">import darknet as dn</p></li>
				<li> Tell Darknet which GPU to use for the program execution:<h4>Note</h4><p class="callout">In Ubuntu, you can obtain an ordered list of your GPUs using the nvidia-smi command. When obtained, check the index of the GPU you want to use and type it into your Python program. If you only have one GPU, you will have to choose the number 0.</p><p class="snippet">dn.set_gpu(0)</p></li>
				<li>Configure the network you are going to use for making your predictions. In this case, we are using the same configuration as before:<p class="snippet">net = dn.load_net("cfg/yolov3.cfg", "yolov3.weights", 0)</p><p class="snippet">meta = dn.load_meta("cfg/coco.data")</p><h4>Note</h4><p class="callout">Pay attention to the paths entered here; they may change if your Python file is not inside Darknet's folder.</p></li>
				<li>Declare the variables to count the total number of images and the number of images containing dogs:<p class="snippet">dog_images = 0</p><p class="snippet">number_of_images = 0</p></li>
				<li>Implement a loop for iterating over the files in the dataset:<p class="snippet">for file in os.listdir("dataset/"):</p></li>
				<li>Use Darknet's <strong class="inline">detect</strong> method to recognize the objects of each image:<p class="snippet">    filename = "dataset/" + file</p><p class="snippet">    r = dn.detect(net, meta, filename)</p></li>
				<li>Iterate over the recognized objects and check whether any of them are dogs. If they are, add one to the dog images counter and stop checking the rest of the objects. Add one to the total counter too:<h4>Note</h4><p class="callout">YOLO returns its predictions as a list of arrays, where each of these arrays represents an object. The data they provide is distributed in the following way:- Position 0: Object name - Position 1: Accuracy - Position 2: Array with the object coordinates in the image</p><p class="snippet">    for obj in r:</p><p class="snippet">        if obj[0] == "dog":</p><p class="snippet">            dog_images += 1</p><p class="snippet">            break</p><p class="snippet">    number_of_images += 1</p></li>
				<li>Finally, print the obtained results. For example:<p class="snippet">print("There are " + str(dog_images) + "/" + str(number_of_images) + " images containing dogs")</p><h4>Note</h4><p class="callout">To execute the code successfully, you need to make changes to the darknet.py file, as follows:</p><p class="callout">Head to the darknet.py file and modify the line lib = CDLL("&lt;your path&gt;/datrknet/libdarknet.so", RTLD_GLOBAL).</p><p class="snippet">cd ..</p><p class="snippet"> wget https://pjreddie.com/media/files/yolov3.weights</p><p class="snippet">python exercise28.py</p><h4>Note</h4><p class="callout">Here the <strong class="inline">cd ..</strong> command switches to the directory where your file is located and downloads the weights and run the script. </p><p class="callout">For example <strong class="inline">cd &lt;your_script_location&gt;</strong></p></li>
			</ol>
			<p>You can test whether it works as expected by running the script. If you used the proposed dataset, the output should be as follows:</p>
			<div>
				<div id="_idContainer242" class="IMG---Figure">
					<img src="image/C13550_09_13.jpg" alt="Figure 9.13: Exercise 1 final output"/>
				</div>
			</div>
			<h6>Figure 9.13: Exercise 28 final output</h6>
			<h3 id="_idParaDest-160"><a id="_idTextAnchor207"/>ROS Integration</h3>
			<p>Now, you have already learned how to use YOLO in a common Python program. It's time to see how to integrate it with Robot Operating System (ROS) so that you can use it in real robotics problems. You can combine it with any robot camera to allow the robot to detect and recognize objects, achieving the goal of artificial vision. After the completion of the following exercise, you will be able to do it by yourself.</p>
			<h3 id="_idParaDest-161"><a id="_idTextAnchor208"/>Exercise 29: ROS and YOLO Integration</h3>
			<p>This exercise consists of a new ROS node implementation that uses YOLO to recognize objects. We will test it using TurtleBot, the ROS simulator we used in <em class="italics">Chapter 6, Robot Operating System (ROS)</em>, but it will be easily adaptable for any robot with a camera. These are the steps that must be followed:</p>
			<ol>
				<li value="1">Create a new package in your catkin workspace to contain the integration node. Do it with this command to include the correct dependencies:<p class="snippet">cd ~/catkin_ws/</p><p class="snippet">source devel/setup.bash</p><p class="snippet">roscore</p><p class="snippet">cd src</p><p class="snippet">catkin_create_pkg exercise29 rospy cv_bridge geometry_msgs image_transport sensor_msgs std_msgs</p></li>
				<li>Switch to the package folder and create a new <strong class="inline">scripts</strong> directory. Then, create the Python file and make it executable:<p class="snippet">cd exercise29</p><p class="snippet">mkdir scripts</p><p class="snippet">cd scripts</p><p class="snippet">touch exercise29.py</p><p class="snippet">chmod +x exercise29.py</p></li>
				<li>Begin with the implementation.<p>Import the libraries you will use for node implementation. You will need <strong class="inline">sys</strong> and <strong class="inline">os</strong> to import Darknet from its path, <strong class="inline">OpenCV</strong> to process images, and <strong class="inline">Image</strong> from <strong class="inline">sensor_msgs</strong> to publish them:</p><p class="snippet">import sys</p><p class="snippet">import os</p><p class="snippet">from cv_bridge import CvBridge, CvBridgeError</p><p class="snippet">from sensor_msgs.msg import Image</p><p>Tell the system where to find Darknet:</p><p class="snippet">sys.path.append(os.path.join(os.getcwd(), '/home/alvaro/Escritorio/tfg/darknet/python/'))</p><h4>Note</h4><p class="callout">The above mentioned path may change as per the directories placed in your computer.</p><p>Now, import the framework:</p><p class="snippet">import darknet as dn</p><p>Create the class where the node logic will be coded and its constructor:</p><p class="snippet">class Exercise29():</p><p class="snippet">    def __init__(self):</p><p>Code the constructor:</p><p>Now, we will initialize the node:</p><p class="snippet">        rospy.init_node('Exercise29', anonymous=True)</p><p>Create a bridge object:</p><p class="snippet">        self.bridge = CvBridge()</p><p>Subscribe to the camera topic:</p><p class="snippet">        self.image_sub = rospy.Subscriber("camera/rgb/image_raw", Image, self.imageCallback)</p><p>Create the variable to store the obtained images:</p><p class="snippet">        self.imageToProcess = None</p><p>Define the corresponding paths for YOLO configuration:</p><p class="snippet">        cfgPath =  "/home/alvaro/Escritorio/tfg/darknet/cfg/yolov3.cfg"</p><p class="snippet">        weightsPath = "/home/alvaro/Escritorio/tfg/darknet/yolov3.weights"</p><p class="snippet">        dataPath = "/home/alvaro/Escritorio/tfg/darknet/cfg/coco2.data"</p><h4>Note</h4><p class="callout">The above mentioned path may change as per the directories placed in your computer.</p><p>Create YOLO variables for making predictions:</p><p class="snippet">        self.net = dn.load_net(cfgPath, weightsPath, 0)</p><p class="snippet">        self.meta = dn.load_meta(dataPath)</p><p>Define the name that will be used for storing the images:</p><p class="snippet">        self.fileName = 'predict.jpg'</p><p>Implement the callback function to obtain the images with the OpenCV format:</p><p class="snippet">    def imageCallback(self, data):</p><p class="snippet">        self.imageToProcess = self.bridge.imgmsg_to_cv2(data, "bgr8")</p><p>Create a function for making predictions over the obtained images. The node must keep making predictions until the user stops the execution. This will be done by storing the image to the disk and making predictions over it using the detection function. Finally, the results will be constantly printed:</p><p class="snippet">    def run(self):</p><p class="snippet">        while not rospy.core.is_shutdown():</p><p class="snippet">            if(self.imageToProcess is not None):</p><p class="snippet">                cv2.imwrite(self.fileName, self.imageToProcess)</p><p class="snippet">                r = dn.detect(self.net, self.meta, self.fileName)</p><p class="snippet">                print r</p><p>Implement the main program entry. Here, you will have to initialize Darknet, make an instance of the created class, and call its main method:</p><p class="snippet">if __name__ == '__main__':</p><p class="snippet">    dn.set_gpu(0)</p><p class="snippet">    node = Exercise29()</p><p class="snippet">    try:</p><p class="snippet">        node.run()</p><p class="snippet">    except rospy.ROSInterruptException:</p><p class="snippet">        pass</p></li>
				<li>Test whether the node works as it should.<p>Open a terminal and start ROS:</p><p class="snippet">cd ../../</p><p class="snippet">cd ..</p><p class="snippet">source devel/setup.bash</p><p class="snippet">roscore</p><p>Open another terminal and run Gazebo with TurtleBot:</p><p class="snippet">cd ~/catkin_ws</p><p class="snippet">source devel/setup.bash</p><p class="snippet">roslaunch turtlebot_gazebo turtlebot_world.launch</p><p>Insert YOLO recognizable objects and make TurtleBot <strong class="bold">look at</strong> them. You can insert new objects by clicking on the <strong class="bold">insert</strong> button located in the upper-left corner. You could insert, for example, a bowl:</p><div id="_idContainer243" class="IMG---Figure"><img src="image/C13550_09_14.jpg" alt="Figure 9.14: Inserted bowl in Gazebo"/></div><h6>Figure 9.14: Inserted bowl in Gazebo</h6></li>
				<li>Open a new terminal and run the created node:<p class="snippet">cd ~/catkin_ws</p><p class="snippet">source devel/setup.bash</p><p class="snippet">rosrun exercise29 exercise29.py</p><p>If you used a bowl, check that you get an output like the one that follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer244" class="IMG---Figure">
					<img src="image/C13550_09_15.jpg" alt="Figure 9.15: Object predicted by the node"/>
				</div>
			</div>
			<h6>Figure 9.15: Object predicted by the node</h6>
			<h3 id="_idParaDest-162"><a id="_idTextAnchor209"/>Activity 9: A Robotic Security Guard</h3>
			<p>Let's suppose a scenario similar to the one in the <em class="italics">Chapter 6, Activity 6, Simulator and Sensors</em> activity: You are working for a robotics company that has recently got a new client, a shopping center. The client wants your company to provide some robots for the shopping center at night to avoid robbery. These robots must consider any person a thief and alert the client if they detect one.</p>
			<p>Use Gazebo to give the desired functionality to TurtleBot or any other simulator. You should follow these steps:</p>
			<ol>
				<li value="1">Create a catkin package for storing the required nodes.</li>
				<li>Now, implement the first node. It should obtain the images from the robot camera and run YOLO on them.</li>
				<li>Next, it should publish the list of detected objects in string format.</li>
				<li>Implement the second node. It should subscribe to the topic where the detected objects are being published and obtain them. Finally, it should check whether a person is one of these objects and print an alert message if it is.</li>
				<li>Run both nodes simultaneously.<h4>Note</h4><p class="callout">Although it's not the main goal of this activity, it would be interesting to combine the execution of these nodes with another one to move the robot (you can use the one implemented in <em class="italics">Chapter 6, Robot Operating System (ROS)</em>).</p><p class="callout">The solut<a id="_idTextAnchor210"/>ion of this activity can be found on page 330.</p></li>
			</ol>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor211"/>Summary</h2>
			<p>We have now achieved the objective of this book and built an end-to-end application for a robot. This has only been an example application; however, you could use the techniques that you learned during this book to build other applications for robotics. In this chapter, you also learned how to install and work with Darknet and YOLO. You worked through evaluating objects using AI and integrating YOLO and ROS to enable your virtual robot to predict objects.</p>
			<p>You have learned how to control the robot with natural language processing commands, along with studying various models in this book, such as Word2Vec, GloVe embedding techniques, and non-numeric data. After this, you worked with ROS and built a conversational agent to manage your virtual robot. You developed the skills needed to build a functional application that could integrate with ROS to extract useful information about your environment. You worked with tools that are not only useful for robotics; you can use artificial vision and language processing as well.</p>
			<p>We end this book by encouraging you to start your own robotics projects and practicing with the technologies you most enjoyed during the book. You can now compare different methods used to work with robots and explore computer vision, algorithms, and limits. Always remember that a robot is a machine that can possess the behavior you want it to.</p>
		</div>
	</body></html>