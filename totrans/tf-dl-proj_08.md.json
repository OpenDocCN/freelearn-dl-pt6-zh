["```py\npip install numpy\npip install pandas\n```", "```py\nimport pandas as pd\nimport numpy as np\n\ndata = pd.read_csv('quora_duplicate_questions.tsv', sep='\\t')\ndata = data.drop(['id', 'qid1', 'qid2'], axis=1)\n```", "```py\n# length based features\ndata['len_q1'] = data.question1.apply(lambda x: len(str(x)))\ndata['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n\n# difference in lengths of two questions\ndata['diff_len'] = data.len_q1 - data.len_q2\n\n# character length based features\ndata['len_char_q1'] = data.question1.apply(lambda x: \n                  len(''.join(set(str(x).replace(' ', '')))))\ndata['len_char_q2'] = data.question2.apply(lambda x: \n                  len(''.join(set(str(x).replace(' ', '')))))\n\n# word length based features\ndata['len_word_q1'] = data.question1.apply(lambda x: \n                                         len(str(x).split()))\ndata['len_word_q2'] = data.question2.apply(lambda x: \n                                         len(str(x).split()))\n\n# common words in the two questions\ndata['common_words'] = data.apply(lambda x: \n                        len(set(str(x['question1'])\n                        .lower().split())\n                        .intersection(set(str(x['question2'])\n                        .lower().split()))), axis=1)\n```", "```py\nfs_1 = ['len_q1', 'len_q2', 'diff_len', 'len_char_q1', \n        'len_char_q2', 'len_word_q1', 'len_word_q2',     \n        'common_words']\n```", "```py\npip install fuzzywuzzy\n```", "```py\npip install python-Levenshtein\n```", "```py\nfrom fuzzywuzzy import fuzz\n\nfuzz.QRatio(\"Why did Trump win the Presidency?\", \n\"How did Donald Trump win the 2016 Presidential Election\")\n```", "```py\nfuzz.QRatio(\"How can I start an online shopping (e-commerce) website?\", \"Which web technology is best suitable for building a big E-Commerce website?\")\n```", "```py\nfuzz.partial_ratio(\"Why did Trump win the Presidency?\", \n\"How did Donald Trump win the 2016 Presidential Election\")\n```", "```py\nfuzz.partial_ratio(\"How can I start an online shopping (e-commerce) website?\", \"Which web technology is best suitable for building a big E-Commerce website?\")\n```", "```py\ndata['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(\n    str(x['question1']), str(x['question2'])), axis=1)\n\ndata['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(\n    str(x['question1']), str(x['question2'])), axis=1)\n\ndata['fuzz_partial_ratio'] = data.apply(lambda x: \n                    fuzz.partial_ratio(str(x['question1']), \n                    str(x['question2'])), axis=1)\n\ndata['fuzz_partial_token_set_ratio'] = data.apply(lambda x:\n          fuzz.partial_token_set_ratio(str(x['question1']), \n          str(x['question2'])), axis=1)\n\ndata['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: \n          fuzz.partial_token_sort_ratio(str(x['question1']), \n          str(x['question2'])), axis=1)\n\ndata['fuzz_token_set_ratio'] = data.apply(lambda x: \n                   fuzz.token_set_ratio(str(x['question1']), \n                   str(x['question2'])), axis=1)\n\ndata['fuzz_token_sort_ratio'] = data.apply(lambda x: \n                   fuzz.token_sort_ratio(str(x['question1']), \n                   str(x['question2'])), axis=1)\n```", "```py\nfs_2 = ['fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio', \n       'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n       'fuzz_token_set_ratio', 'fuzz_token_sort_ratio']\n```", "```py\npip install -U scikit-learn\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom copy import deepcopy\n\ntfv_q1 = TfidfVectorizer(min_df=3, \n                         max_features=None, \n                         strip_accents='unicode', \n                         analyzer='word', \n                         token_pattern=r'\\w{1,}',\n                         ngram_range=(1, 2), \n                         use_idf=1, \n                         smooth_idf=1, \n                         sublinear_tf=1,\n                         stop_words='english')\n\ntfv_q2 = deepcopy(tfv_q1)\n```", "```py\nq1_tfidf = tfv_q1.fit_transform(data.question1.fillna(\"\"))\nq2_tfidf = tfv_q2.fit_transform(data.question2.fillna(\"\"))\n```", "```py\nfrom sklearn.decomposition import TruncatedSVD\nsvd_q1 = TruncatedSVD(n_components=180)\nsvd_q2 = TruncatedSVD(n_components=180)\n```", "```py\nquestion1_vectors = svd_q1.fit_transform(q1_tfidf)\nquestion2_vectors = svd_q2.fit_transform(q2_tfidf)\n```", "```py\nfrom scipy import sparse\n\n# obtain features by stacking the sparse matrices together\nfs3_1 = sparse.hstack((q1_tfidf, q2_tfidf))\n```", "```py\ntfv = TfidfVectorizer(min_df=3, \n                      max_features=None, \n                      strip_accents='unicode', \n                      analyzer='word', \n                      token_pattern=r'\\w{1,}',\n                      ngram_range=(1, 2), \n                      use_idf=1, \n                      smooth_idf=1, \n                      sublinear_tf=1,\n                      stop_words='english')\n\n# combine questions and calculate tf-idf\nq1q2 = data.question1.fillna(\"\") \nq1q2 += \" \" + data.question2.fillna(\"\")\nfs3_2 = tfv.fit_transform(q1q2)\n\n```", "```py\n# obtain features by stacking the matrices together\nfs3_3 = np.hstack((question1_vectors, question2_vectors))\n```", "```py\npip install gensim\npip install pyemd\n```", "```py\nwget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n```", "```py\nimport gensim\n\nmodel = gensim.models.KeyedVectors.load_word2vec_format(\n        'GoogleNews-vectors-negative300.bin.gz', binary=True)\n```", "```py\n$ pip install nltk\n```", "```py\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n```", "```py\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\n\nstop_words = set(stopwords.words('english'))\n\ndef sent2vec(s, model):  \n    M = []\n    words = word_tokenize(str(s).lower())\n    for word in words:\n        #It shouldn't be a stopword\n        if word not in stop_words:\n            #nor contain numbers\n            if word.isalpha():\n                #and be part of word2vec\n                if word in model:\n                    M.append(model[word])\n    M = np.array(M)\n    if len(M) > 0:\n        v = M.sum(axis=0)\n        return v / np.sqrt((v ** 2).sum())\n    else:\n        return np.zeros(300)\n```", "```py\nw2v_q1 = np.array([sent2vec(q, model) \n                   for q in data.question1])\nw2v_q2 = np.array([sent2vec(q, model) \n                   for q in data.question2])\n```", "```py\nfrom scipy.spatial.distance import cosine, cityblock, \n          jaccard, canberra, euclidean, minkowski, braycurtis\n\ndata['cosine_distance'] = [cosine(x,y) \n                           for (x,y) in zip(w2v_q1, w2v_q2)]\ndata['cityblock_distance'] = [cityblock(x,y) \n                           for (x,y) in zip(w2v_q1, w2v_q2)]\ndata['jaccard_distance'] = [jaccard(x,y) \n                           for (x,y) in zip(w2v_q1, w2v_q2)]\ndata['canberra_distance'] = [canberra(x,y) \n                           for (x,y) in zip(w2v_q1, w2v_q2)]\ndata['euclidean_distance'] = [euclidean(x,y) \n                           for (x,y) in zip(w2v_q1, w2v_q2)]\ndata['minkowski_distance'] = [minkowski(x,y,3) \n                           for (x,y) in zip(w2v_q1, w2v_q2)]\ndata['braycurtis_distance'] = [braycurtis(x,y) \n                           for (x,y) in zip(w2v_q1, w2v_q2)]\n```", "```py\nfs4_1 = ['cosine_distance', 'cityblock_distance', \n         'jaccard_distance', 'canberra_distance', \n         'euclidean_distance', 'minkowski_distance',\n         'braycurtis_distance']\n```", "```py\nw2v = np.hstack((w2v_q1, w2v_q2))\n```", "```py\ndef wmd(s1, s2, model):\n    s1 = str(s1).lower().split()\n    s2 = str(s2).lower().split()\n    stop_words = stopwords.words('english')\n    s1 = [w for w in s1 if w not in stop_words]\n    s2 = [w for w in s2 if w not in stop_words]\n    return model.wmdistance(s1, s2)\n\ndata['wmd'] = data.apply(lambda x: wmd(x['question1'],     \n                            x['question2'], model), axis=1)\nmodel.init_sims(replace=True) \ndata['norm_wmd'] = data.apply(lambda x: wmd(x['question1'], \n                            x['question2'], model), axis=1)\nfs4_2 = ['wmd', 'norm_wmd']\n```", "```py\nimport gc\nimport psutil\ndel([tfv_q1, tfv_q2, tfv, q1q2, \n     question1_vectors, question2_vectors, svd_q1, \n     svd_q2, q1_tfidf, q2_tfidf])\ndel([w2v_q1, w2v_q2])\ndel([model])\ngc.collect()\npsutil.virtual_memory()\n```", "```py\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\n```", "```py\nscaler = StandardScaler()\ny = data.is_duplicate.values\ny = y.astype('float32').reshape(-1, 1)\nX = data[fs_1+fs_2+fs3_4+fs4_1+fs4_2]\nX = X.replace([np.inf, -np.inf], np.nan).fillna(0).values\nX = scaler.fit_transform(X)\nX = np.hstack((X, fs3_3))\n```", "```py\nnp.random.seed(42)\nn_all, _ = y.shape\nidx = np.arange(n_all)\nnp.random.shuffle(idx)\nn_split = n_all // 10\nidx_val = idx[:n_split]\nidx_train = idx[n_split:]\nx_train = X[idx_train]\ny_train = np.ravel(y[idx_train])\nx_val = X[idx_val]\ny_val = np.ravel(y[idx_val])\n```", "```py\nlogres = linear_model.LogisticRegression(C=0.1, \n                                 solver='sag', max_iter=1000)\nlogres.fit(x_train, y_train)\nlr_preds = logres.predict(x_val)\nlog_res_accuracy = np.sum(lr_preds == y_val) / len(y_val)\nprint(\"Logistic regr accuracy: %0.3f\" % log_res_accuracy)\n```", "```py\nparams = dict()\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = ['logloss', 'error']\nparams['eta'] = 0.02\nparams['max_depth'] = 4\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_val, label=y_val)\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\nbst = xgb.train(params, d_train, 5000, watchlist, \n                early_stopping_rounds=50, verbose_eval=100)\nxgb_preds = (bst.predict(d_valid) >= 0.5).astype(int)\nxgb_accuracy = np.sum(xgb_preds == y_val) / len(y_val)\nprint(\"Xgb accuracy: %0.3f\" % xgb_accuracy)\n```", "```py\nimport zipfile\nfrom tqdm import tqdm_notebook as tqdm\nimport tensorflow as tf\nprint(\"TensorFlow version %s\" % tf.__version__)\n```", "```py\ntry:\n    df = data[['question1', 'question2', 'is_duplicate']]\nexcept:\n    df = pd.read_csv('data/quora_duplicate_questions.tsv',                                                  \n                                                    sep='\\t')\n    df = df.drop(['id', 'qid1', 'qid2'], axis=1)\n\ndf = df.fillna('')\ny = df.is_duplicate.values\ny = y.astype('float32').reshape(-1, 1)\n```", "```py\nTokenizer = tf.keras.preprocessing.text.Tokenizer pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n\ntk = Tokenizer(num_words=200000) max_len = 40\n\n```", "```py\ntk.fit_on_texts(list(df.question1) + list(df.question2))\nx1 = tk.texts_to_sequences(df.question1)\nx1 = pad_sequences(x1, maxlen=max_len)\nx2 = tk.texts_to_sequences(df.question2)\nx2 = pad_sequences(x2, maxlen=max_len) \nword_index = tk.word_index\n```", "```py\nwget http://nlp.stanford.edu/data/glove.840B.300d.zip\n```", "```py\nembedding_matrix = np.zeros((len(word_index) + 1, 300), dtype='float32')\n\nglove_zip = zipfile.ZipFile('data/glove.840B.300d.zip')\nglove_file = glove_zip.filelist[0]\n\nf_in = glove_zip.open(glove_file)\nfor line in tqdm(f_in):\n    values = line.split(b' ')\n    word = values[0].decode()\n    if word not in word_index:\n        continue\n    i = word_index[word]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embedding_matrix[i, :] = coefs\n\nf_in.close()\nglove_zip.close()\n```", "```py\ndef prepare_batches(seq, step):\n    n = len(seq)\n    res = []\n    for i in range(0, n, step):\n        res.append(seq[i:i+step])\n    return res\n\n```", "```py\ndef dense(X, size, activation=None):\n    he_std = np.sqrt(2 / int(X.shape[1]))\n    out = tf.layers.dense(X, units=size, \n                activation=activation,\n                kernel_initializer=\\\n                tf.random_normal_initializer(stddev=he_std))\n    return out\n```", "```py\ndef time_distributed_dense(X, dense_size):\n    shape = X.shape.as_list()\n    assert len(shape) == 3\n    _, w, d = shape\n    X_reshaped = tf.reshape(X, [-1, d])\n    H = dense(X_reshaped, dense_size, \n                              tf.nn.relu)\n    return tf.reshape(H, [-1, w, dense_size])\n```", "```py\ndef conv1d(inputs, num_filters, filter_size, padding='same'):\n    he_std = np.sqrt(2 / (filter_size * num_filters))\n    out = tf.layers.conv1d(\n        inputs=inputs, filters=num_filters, padding=padding,\n        kernel_size=filter_size,\n        activation=tf.nn.relu,\n        kernel_initializer=tf.random_normal_initializer(\n                                                 stddev=he_std))\n    return out\n\ndef maxpool1d_global(X):\n    out = tf.reduce_max(X, axis=1)\n    return out\n```", "```py\ndef lstm(X, size_hidden, size_out):\n    with tf.variable_scope('lstm_%d' \n                            % np.random.randint(0, 100)):\n        he_std = np.sqrt(2 / (size_hidden * size_out))\n        W = tf.Variable(tf.random_normal([size_hidden, size_out], \n                                                  stddev=he_std))\n        b = tf.Variable(tf.zeros([size_out]))\n        size_time = int(X.shape[1])\n        X = tf.unstack(X, size_time, axis=1)\n        lstm_cell = tf.contrib.rnn.BasicLSTMCell(size_hidden, \n                                                 forget_bias=1.0)\n        outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, X, \n                                                 dtype='float32')\n        out = tf.matmul(outputs[-1], W) + b\n        return out\n```", "```py\nmax_features = 200000\nfilter_length = 5\nnb_filter = 64\npool_length = 4\nlearning_rate = 0.001\n```", "```py\ngraph = tf.Graph()\ngraph.seed = 1\n\nwith graph.as_default():\n    place_q1 = tf.placeholder(tf.int32, shape=(None, max_len))\n    place_q2 = tf.placeholder(tf.int32, shape=(None, max_len))\n    place_y = tf.placeholder(tf.float32, shape=(None, 1))\n    place_training = tf.placeholder(tf.bool, shape=())\n\n    glove = tf.Variable(embedding_matrix, trainable=False)\n    q1_glove_lookup = tf.nn.embedding_lookup(glove, place_q1)\n    q2_glove_lookup = tf.nn.embedding_lookup(glove, place_q2)\n\n    emb_size = len(word_index) + 1\n    emb_dim = 300\n    emb_std = np.sqrt(2 / emb_dim)\n    emb = tf.Variable(tf.random_uniform([emb_size, emb_dim],\n                                             -emb_std, emb_std))\n    q1_emb_lookup = tf.nn.embedding_lookup(emb, place_q1)\n    q2_emb_lookup = tf.nn.embedding_lookup(emb, place_q2)\n\n    model1 = q1_glove_lookup\n    model1 = time_distributed_dense(model1, 300)\n    model1 = tf.reduce_sum(model1, axis=1)\n\n    model2 = q2_glove_lookup\n    model2 = time_distributed_dense(model2, 300)\n    model2 = tf.reduce_sum(model2, axis=1)\n\n    model3 = q1_glove_lookup\n    model3 = conv1d(model3, nb_filter, filter_length, \n                                       padding='valid')\n    model3 = tf.layers.dropout(model3, rate=0.2,                                                               training=place_training)\n    model3 = conv1d(model3, nb_filter, filter_length,                                                                               padding='valid')\n    model3 = maxpool1d_global(model3)\n    model3 = tf.layers.dropout(model3, rate=0.2,                                                               training=place_training)\n    model3 = dense(model3, 300)\n    model3 = tf.layers.dropout(model3, rate=0.2,                                                                training=place_training)\n    model3 = tf.layers.batch_normalization(model3,                                                                training=place_training)\n\n    model4 = q2_glove_lookup\n    model4 = conv1d(model4, nb_filter, filter_length,                                                                                padding='valid')\n    model4 = tf.layers.dropout(model4, rate=0.2,                                                                training=place_training)\n    model4 = conv1d(model4, nb_filter, filter_length,                                                                                padding='valid')\n    model4 = maxpool1d_global(model4)\n    model4 = tf.layers.dropout(model4, rate=0.2,                                                                training=place_training)\n    model4 = dense(model4, 300)\n    model4 = tf.layers.dropout(model4, rate=0.2,                                                                training=place_training)\n    model4 = tf.layers.batch_normalization(model4,                                                                training=place_training)\n\n    model5 = q1_emb_lookup\n    model5 = tf.layers.dropout(model5, rate=0.2,                                                                 training=place_training)\n    model5 = lstm(model5, size_hidden=300, size_out=300)\n\n    model6 = q2_emb_lookup\n    model6 = tf.layers.dropout(model6, rate=0.2,                                                                 training=place_training)\n    model6 = lstm(model6, size_hidden=300, size_out=300)\n\n    merged = tf.concat([model1, model2, model3, model4, model5,                                                                         model6], axis=1)\n\n    merged = tf.layers.batch_normalization(merged,                                                                         training=place_training)\n```", "```py\n    for i in range(5):\n        merged = dense(merged, 300, activation=tf.nn.relu)\n        merged = tf.layers.dropout(merged, rate=0.2,                                                                     training=place_training)\n        merged = tf.layers.batch_normalization(merged,                                                                     training=place_training)\n\n    merged = dense(merged, 1, activation=tf.nn.sigmoid)\n    loss = tf.losses.log_loss(place_y, merged)\n    prediction = tf.round(merged)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(place_y,                                                                      prediction), 'float32'))\n    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n\n    # for batchnorm\n    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(extra_update_ops):\n        step = opt.minimize(loss)\n\n    init = tf.global_variables_initializer()\n\nsession = tf.Session(config=None, graph=graph)\nsession.run(init)\n```", "```py\nnp.random.seed(1)\n\nn_all, _ = y.shape\nidx = np.arange(n_all)\nnp.random.shuffle(idx)\n\nn_split = n_all // 10\nidx_val = idx[:n_split]\nidx_train = idx[n_split:]\n\nx1_train = x1[idx_train]\nx2_train = x2[idx_train]\ny_train = y[idx_train]\n\nx1_val = x1[idx_val]\nx2_val = x2[idx_val]\ny_val = y[idx_val] \n```", "```py\nval_idx = np.arange(y_val.shape[0])\nval_batches = prepare_batches(val_idx, 5000)\n\nno_epochs = 10\n\n# see https://github.com/tqdm/tqdm/issues/481\ntqdm.monitor_interval = 0\n\nfor i in range(no_epochs):\n    np.random.seed(i)\n    train_idx_shuffle = np.arange(y_train.shape[0])\n    np.random.shuffle(train_idx_shuffle)\n    batches = prepare_batches(train_idx_shuffle, 384)\n\n    progress = tqdm(total=len(batches))\n    for idx in batches:\n        feed_dict = {\n            place_q1: x1_train[idx],\n            place_q2: x2_train[idx],\n            place_y: y_train[idx],\n            place_training: True,\n        }\n        _, acc, l = session.run([step, accuracy, loss],                                              feed_dict)\n        progress.update(1)\n        progress.set_description('%.3f / %.3f' % (acc, l))\n\n    y_pred = np.zeros_like(y_val)\n    for idx in val_batches:\n        feed_dict = {\n            place_q1: x1_val[idx],\n            place_q2: x2_val[idx],\n            place_y: y_val[idx],\n            place_training: False,\n        }\n        y_pred[idx, :] = session.run(prediction, feed_dict)\n\n    print('batch %02d, accuracy: %0.3f' % (i, \n                                 np.mean(y_val == y_pred))) \n```", "```py\ndef convert_text(txt, tokenizer, padder):\n    x = tokenizer.texts_to_sequences(txt)\n    x = padder(x, maxlen=max_len)\n    return x  \n\ndef evaluate_questions(a, b, tokenizer, padder, pred):\n    feed_dict = {\n            place_q1: convert_text([a], tk, pad_sequences),\n            place_q2: convert_text([b], tk, pad_sequences),\n            place_y: np.zeros((1,1)),\n            place_training: False,\n        }\n    return session.run(pred, feed_dict)\n\nisduplicated = lambda a, b: evaluate_questions(a, b, tk, pad_sequences, prediction)\n\na = \"Why are there so many duplicated questions on Quora?\"\nb = \"Why do people ask similar questions on Quora multiple times?\"\n\nprint(\"Answer: %0.2f\" % isduplicated(a, b))\n```"]