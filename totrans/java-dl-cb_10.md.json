["```py\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-core_2.11</artifactId>\n    <version>2.1.0</version>\n</dependency>\n```", "```py\n<dependency>\n    <groupId>org.datavec</groupId>\n    <artifactId>datavec-spark_2.11</artifactId>\n    <version>1.0.0-beta3_spark_2</version>\n</dependency>\n```", "```py\n<dependency>\n    <groupId>org.datavec</groupId>\n    <artifactId>datavec-spark_2.11</artifactId>\n    <version>1.0.0-beta3_spark_2</version>\n</dependency>\n```", "```py\n<dependency>\n    <groupId>org.deeplearning4j</groupId>\n    <artifactId>dl4j-spark-parameterserver_2.11</artifactId>\n    <version>1.0.0-beta3_spark_2</version>\n</dependency>\n```", "```py\n<dependency>\n    <groupId>org.nd4j</groupId>\n    <artifactId>nd4j-native-platform</artifactId>\n    <version>1.0.0-beta3</version>\n</dependency>\n```", "```py\n<dependency>\n    <groupId>org.nd4j</groupId>\n    <artifactId>nd4j-cuda-x.x</artifactId>\n    <version>1.0.0-beta3</version>\n</dependency>\n```", "```py\n<dependency>\n    <groupId>com.beust</groupId>\n    <artifactId>jcommander</artifactId>\n    <version>1.72</version>\n</dependency>\n```", "```py\nHADOOP_HOME = {PathDownloaded}/hadoop-x.x \n HADOOP_HDFS_HOME = {PathDownloaded}/hadoop-x.x \n HADOOP_MAPRED_HOME = {PathDownloaded}/hadoop-x.x \n HADOOP_YARN_HOME = {PathDownloaded}/hadoop-x.x \n```", "```py\n${HADOOP_HOME}\\bin\n```", "```py\n<configuration>\n     <property>\n      <name>dfs.replication</name>\n      <value>1</value>\n     </property>\n     <property>\n      <name>dfs.namenode.name.dir</name>\n      <value>file:/{NameNodeDirectoryPath}</value>\n     </property>\n     <property>\n      <name>dfs.datanode.data.dir</name>\n      <value>file:/{DataNodeDirectoryPath}</value>\n     </property>\n   </configuration>\n```", "```py\n<configuration>\n  <property>\n   <name>mapreduce.framework.name</name>\n   <value>yarn</value>\n  </property>\n </configuration>\n```", "```py\n<configuration>\n  <!-- Site specific YARN configuration properties -->\n  <property>\n   <name>yarn.nodemanager.aux-services</name>\n   <value>mapreduce_shuffle</value>\n  </property>\n  <property>\n   <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>\n   <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n  </property>\n </configuration>\n```", "```py\n<configuration>\n  <property>\n   <name>fs.default.name</name>\n   <value>hdfs://localhost:9000</value>\n  </property>\n </configuration> \n```", "```py\nhdfs namenode –format\n```", "```py\nSPARK_HOME = {PathDownloaded}/spark-x.x-bin-hadoopx.x\nSPARK_CONF_DIR = ${SPARK_HOME}\\conf\n```", "```py\nSPARK_MASTER_HOST=localhost\n\n```", "```py\nspark-class org.apache.spark.deploy.master.Master\n```", "```py\n<filters>\n   <filter>\n    <artifact>*:*</artifact>\n    <excludes>\n     <exclude>META-INF/*.SF</exclude>\n     <exclude>META-INF/*.DSA</exclude>\n     <exclude>META-INF/*.RSA</exclude>\n    </excludes>\n   </filter>\n </filters>\n```", "```py\nmvn package -DskipTests\n```", "```py\nException in thread “main” java.lang.SecurityException: Invalid signature file digest for Manifest main attributes.\n```", "```py\n<dependency>\n   <groupId>org.nd4j</groupId>\n   <artifactId>nd4j-cuda-x.x</artifactId>\n   <version>1.0.0-beta3</version>\n </dependency> \n```", "```py\n<dependency>\n    <groupId>org.nd4j</groupId>\n    <artifactId>nd4j-native-platform</artifactId>\n    <version>1.0.0-beta3</version>\n </dependency>\n```", "```py\n<dependency>\n  <groupId>org.deeplearning4j</groupId>\n  <artifactId>deeplearning4j-cuda-x.x</artifactId>\n  <version>1.0.0-beta3</version>\n </dependency>\n\n```", "```py\n--conf \"spark.executor.extraJavaOptions=-Dorg.bytedeco.javacpp.maxbytes=8G\"\n```", "```py\nnew SharedTrainingMaster.Builder(voidConfiguration, minibatch)\n   .workerTogglePeriodicGC(true) \n   .workerPeriodicGCFrequency(frequencyIntervalInMs) \n   .build();\n\n```", "```py\n<dependency>\n   <groupId>org.nd4j</groupId>\n   <artifactId>nd4j-kryo_2.11</artifactId>\n  <version>1.0.0-beta3</version>\n </dependency>\n```", "```py\nSparkConf conf = new SparkConf();\n conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n conf.set(\"spark.kryo.registrator\", \"org.nd4j.Nd4jRegistrator\");\n```", "```py\n--conf spark.locality.wait=0 \n```", "```py\nSPARK_DRIVER_MEMORY=8G\n\n```", "```py\nSPARK_DRIVER_OPTS=-Dorg.bytedeco.javacpp.maxbytes=8G\n\n```", "```py\nSPARK_WORKER_MEMORY=8G\n\n```", "```py\nSPARK_WORKER_OPTS=-Dorg.bytedeco.javacpp.maxbytes=8G \n\n```", "```py\nNd4j.getMemoryManager().setAutoGcWindow(frequencyIntervalInMs);\n```", "```py\nNd4j.getMemoryManager().togglePeriodicGc(false);\n```", "```py\nspark.master spark://5.6.7.8:7077\nspark.executor.memory 4g\n```", "```py\nTrainingMaster tm = new SharedTrainingMaster.Builder(voidConfiguration, minibatchSize)\n   .thresholdAlgorithm(new AdaptiveThresholdAlgorithm(gradientThreshold))\n  .build();\n```", "```py\nTrainingMaster tm = new SharedTrainingMaster.Builder(voidConfiguration, minibatch)\n .residualPostProcessor(new ResidualClippingPostProcessor(clipValue, frequency))\n .build();\n```", "```py\n* Windows: C:\\Users\\<username>\\.deeplearning4j\\data\\TINYIMAGENET_200\n * Linux: ~/.deeplearning4j/data/TINYIMAGENET_200\n```", "```py\nFile saveDirTrain = new File(batchSavedLocation, \"train\");\n SparkDataUtils.createFileBatchesLocal(dirPathDataSet, NativeImageLoader.ALLOWED_FORMATS, true, saveDirTrain, batchSize);\n```", "```py\nFile saveDirTest = new File(batchSavedLocation, \"test\");\n SparkDataUtils.createFileBatchesLocal(dirPathDataSet, NativeImageLoader.ALLOWED_FORMATS, true, saveDirTest, batchSize);\n```", "```py\nPathLabelGenerator labelMaker = new ParentPathLabelGenerator();\n ImageRecordReader rr = new ImageRecordReader(imageHeightWidth, imageHeightWidth, imageChannels, labelMaker);\n rr.setLabels(new TinyImageNetDataSetIterator(1).getLabels());\n```", "```py\nRecordReaderFileBatchLoader loader = new RecordReaderFileBatchLoader(rr, batchSize, 1, TinyImageNetFetcher.NUM_LABELS);\n loader.setPreProcessor(new ImagePreProcessingScaler()); \n```", "```py\nJCommander jcmdr = new JCommander(this);\n jcmdr.parse(args);\n```", "```py\nVoidConfiguration voidConfiguration = VoidConfiguration.builder()\n .unicastPort(portNumber)\n .networkMask(netWorkMask)\n .controllerAddress(masterNodeIPAddress)\n .build();\n```", "```py\nTrainingMaster tm = new SharedTrainingMaster.Builder(voidConfiguration, batchSize)\n .rngSeed(12345)\n .collectTrainingStats(false)\n .batchSizePerWorker(batchSize) // Minibatch size for each worker\n .thresholdAlgorithm(new AdaptiveThresholdAlgorithm(1E-3)) //Threshold algorithm determines the encoding threshold to be use.\n .workersPerNode(1) // Workers per node\n .build();\n```", "```py\nComputationGraphConfiguration.GraphBuilder builder = new NeuralNetConfiguration.Builder()\n .convolutionMode(ConvolutionMode.Same)\n .l2(1e-4)\n .updater(new AMSGrad(lrSchedule))\n .weightInit(WeightInit.RELU)\n .graphBuilder()\n .addInputs(\"input\")\n .setOutputs(\"output\");\n\n```", "```py\nDarknetHelper.addLayers(builder, 0, 3, 3, 32, 0); //64x64 out\n DarknetHelper.addLayers(builder, 1, 3, 32, 64, 2); //32x32 out\n DarknetHelper.addLayers(builder, 2, 2, 64, 128, 0); //32x32 out\n DarknetHelper.addLayers(builder, 3, 2, 128, 256, 2); //16x16 out\n DarknetHelper.addLayers(builder, 4, 2, 256, 256, 0); //16x16 out\n DarknetHelper.addLayers(builder, 5, 2, 256, 512, 2); //8x8 out\n```", "```py\nbuilder.addLayer(\"convolution2d_6\", new ConvolutionLayer.Builder(1, 1)\n .nIn(512)\n .nOut(TinyImageNetFetcher.NUM_LABELS) // number of labels (classified outputs) = 200\n .weightInit(WeightInit.XAVIER)\n .stride(1, 1)\n .activation(Activation.IDENTITY)\n .build(), \"maxpooling2d_5\")\n .addLayer(\"globalpooling\", new GlobalPoolingLayer.Builder(PoolingType.AVG).build(), \"convolution2d_6\")\n .addLayer(\"loss\", new LossLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).activation(Activation.SOFTMAX).build(), \"globalpooling\")\n .setOutputs(\"loss\");\n```", "```py\nComputationGraphConfiguration configuration = builder.build(); \n```", "```py\nSparkComputationGraph sparkNet = new SparkComputationGraph(context,configuration,tm);\n sparkNet.setListeners(new PerformanceListener(10, true));\n```", "```py\nString trainPath = dataPath + (dataPath.endsWith(\"/\") ? \"\" : \"/\") + \"train\";\n JavaRDD<String> pathsTrain = SparkUtils.listPaths(context, trainPath);\n```", "```py\nfor (int i = 0; i < numEpochs; i++) {\n   sparkNet.fitPaths(pathsTrain, loader);\n }\n\n```", "```py\nString testPath = dataPath + (dataPath.endsWith(\"/\") ? \"\" : \"/\") + \"test\";\n JavaRDD<String> pathsTest = SparkUtils.listPaths(context, testPath);\n```", "```py\nEvaluation evaluation = new Evaluation(TinyImageNetDataSetIterator.getLabels(false), 5);\n evaluation = (Evaluation) sparkNet.doEvaluation(pathsTest, loader, evaluation)[0];\n log.info(\"Evaluation statistics: {}\", evaluation.stats());\n```", "```py\nspark-submit --master spark://{sparkHostIp}:{sparkHostPort} --class {clssName} {JAR File location absolute path} --dataPath {hdfsPathToPreprocessedData} --masterIP {masterIP}\n\nExample:\n spark-submit --master spark://192.168.99.1:7077 --class com.javacookbook.app.SparkExample cookbookapp-1.0-SNAPSHOT.jar --dataPath hdfs://localhost:9000/user/hadoop/batches/imagenet-preprocessed --masterIP 192.168.99.1\n```", "```py\nTinyImageNetFetcher fetcher = new TinyImageNetFetcher();\n fetcher.downloadAndExtract();\n```", "```py\nFile baseDirTrain = DL4JResources.getDirectory(ResourceType.DATASET, f.localCacheName() + \"/train\");\n File baseDirTest = DL4JResources.getDirectory(ResourceType.DATASET, f.localCacheName() + \"/test\");\n```", "```py\nEvaluation evaluation = new Evaluation(TinyImageNetDataSetIterator.getLabels(false), 5);\n```", "```py\nMultiLayerNetwork model = sparkModel.getNetwork();\n File file = new File(\"MySparkMultiLayerNetwork.bin\");\n ModelSerializer.writeModel(model,file, saveUpdater);\n```", "```py\nMultiLayerNetwork model = sparkModel.getNetwork();\n  File locationToSave = new File(\"MySparkMultiLayerNetwork.bin);\n model.save(locationToSave, saveUpdater);\n```", "```py\nModelSerializer.restoreMultiLayerNetwork(new File(\"MySparkMultiLayerNetwork.bin\"));\n```", "```py\nMultiLayerNetwork restored = MultiLayerNetwork.load(savedModelLocation, saveUpdater);\n```", "```py\nSparkDl4jMultiLayer.feedForwardWithKey(JavaPairRDD<K, INDArray> featuresData, int batchSize);\n```", "```py\nSparkComputationGraph.feedForwardWithKey(JavaPairRDD<K, INDArray[]> featuresData, int batchSize) ;\n```"]