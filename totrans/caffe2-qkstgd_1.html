<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction and Installation</h1>
                </header>
            
            <article>
                
<p>Welcome to the Caffe2 Quick Start Guide. This book aims to provide you with a quick introduction to the Caffe2 deep learning framework and how to use it for training and deployment of deep learning models. This book uses code samples to create, train, and run inference on actual deep learning models that solve real problems. In this way, its code can be applied quickly by readers to their own applications.</p>
<p>This chapter provides a brief introduction to Caffe2 and shows you how to build and install it on your computer. In this chapter, we will cover the following topics:</p>
<ul>
<li>Introduction to deep learning and Caffe2</li>
<li>Building and installing Caffe2</li>
<li>Testing Caffe2 Python API</li>
<li>Testing Caffe2 C++ API</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to deep learning</h1>
                </header>
            
            <article>
                
<p>Terms such as <strong>artificial intelligence</strong> (<strong>AI</strong>), <strong>machine learning</strong> (<strong>ML</strong>), and <strong>d<span>eep learning (DL)</span></strong> are popular right now. This popularity can be attributed to significant improvements that deep learning techniques have brought about in the last few years in enabling computers to see, hear, read, and create. First and foremost, we'll introduce these three fields and how they intersect:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-280 image-border" src="assets/16a849e6-1ea0-4cb9-bfbd-46f406cb0f91.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1.1: Relationship between <span>deep learning</span>, ML, and AI</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AI</h1>
                </header>
            
            <article>
                
<p><strong>Artificial intelligence</strong> (<strong>AI</strong>) is a general term used to refer to the intelligence of computers, specifically their ability to reason, sense, perceive, and respond. It is used to refer to any non-biological system that has intelligence, and this intelligence is a consequence of a set of rules. It does not matter in AI if those sets of rules were created manually by a human, or if those rules were automatically learned by a computer by analyzing data. Research into AI started in 1956, and it has been through many ups and a couple of downs, called <strong>AI winters</strong>, since then.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ML</h1>
                </header>
            
            <article>
                
<p><strong>Machine learning</strong> (<strong>ML</strong>) is a subset of AI that uses statistics, data, and learning algorithms to teach computers to learn from given data. This data, called <strong>training data</strong>, is specific to the problem being solved, and contains examples of input and the expected output for each input. ML algorithms learn models or representations automatically from training data, and these models can be used to obtain predictions for new input data.</p>
<p>There are many popular types of models in ML, including <strong>artificial neural networks</strong> (<strong>ANNs</strong>), Bayesian networks, <strong>support vector machines</strong> (<strong>SVM</strong>), and random forests. The ML model that is of interest to us in this book is ANN. The structure of ANNs are inspired by the connections in the brain. These neural network models were initially popular in ML, but later fell out of favor since they required enormous computing power that was not available at that time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning</h1>
                </header>
            
            <article>
                
<p>Over the last decade, utilization of the parallel processing capability of <strong>graphics processing units</strong> (<strong>GPUs</strong>) to solve general computation problems became popular. This type of computation came to be known as <strong>general-purpose computing on GPU (GPGPU)</strong>. GPUs were quite affordable and were easy to use as accelerators by using GPGPU programming models and APIs such as <strong>Compute Unified Device Architecture</strong> (<strong>CUDA</strong>) and <strong>Open Computing Language</strong> (<strong>OpenCL</strong>). Starting in 2012, neural network researchers harnessed GPUs to train neural networks with a large number of layers and started to generate breakthroughs in solving computer vision, speech recognition, and other problems. The use of such deep neural networks with a large number of layers of neurons gave rise to the term <strong>deep learning</strong>. Deep learning algorithms form a subset of ML and use multiple layers of abstraction to learn and parameterize multi-layer neural network models of data.<br/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Caffe2</h1>
                </header>
            
            <article>
                
<p>The popularity and success of deep learning has been motivated by the creation of many popular and open source deep learning frameworks that can be used for training and inference of neural networks. <strong>Caffe</strong> was one of the first popular deep learning frameworks. It was created by <em>Yangqing Jia</em> at UC Berkeley for his PhD thesis and released to the public at the end of 2013. It was primarily written in C++ and provided a C++ API. Caffe also provided a rudimentary Python API wrapped around the C++ API. The Caffe framework created networks using layers. Users created networks by listing down and describing its layers in a text file commonly referred to as a <strong>prototxt</strong>.</p>
<p>Following the popularity of Caffe, universities, corporations, and individuals created and launched many deep learning frameworks. Some of the popular ones today are Caffe2, TensorFlow, MXNet, and PyTorch. TensorFlow is driven by Google, MXNet has the support of Amazon, and PyTorch was primarily developed by Facebook.</p>
<p>Caffe's creator, Yangqing Jia, moved to Facebook, where he created a follow-up to Caffe called Caffe2. Compared to the other deep learning frameworks, Caffe2 was designed to focus on scalability, high performance, and portability. Written in C++, it has both a C++ API and a Python API. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Caffe2 and PyTorch</h1>
                </header>
            
            <article>
                
<p>Caffe2 and PyTorch are both popular DL frameworks, maintained and driven by Facebook. PyTorch originates from the <strong>Torch</strong> DL framework. It is characterized by a Python API that is easy for designing different network structures and experimenting with training parameters and regimens on them. While PyTorch could be used for inference in production applications on the cloud and in the edge, it is not as efficient when it comes to this.</p>
<p>Caffe2 has a Python API and a C++ API. It is designed for practitioners who tinker with existing network structures and use pre-trained models from PyTorch, Caffe, and other DL frameworks, and ready them for deployment inside applications, local workstations, low-power devices at the edge, mobile devices, and in the cloud.</p>
<p>Having observed the complementary features of PyTorch and Caffe2, Facebook has a plan to merge the two projects. As we will see later, Caffe2 source code is already organized as a subdirectory under the PyTorch Git repository. In the future, expect more intermingling of these two projects, with a final goal of fusing the two together to create a single DL framework that is easy to experiment with and tinker, efficient to train and deploy, and that can scale from the cloud to the edge, from general-purpose processors to special-purpose accelerators.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hardware requirements</h1>
                </header>
            
            <article>
                
<p>Working with <span>deep learning</span> models, especially the training process, requires a lot of computing power. While you could train a popular neural network on the CPU, it could typically take many hours or days, depending on the complexity of the network. Using GPUs for training is highly recommended since they typically reduce the training time by an order of magnitude or more compared to CPUs. Caffe2 uses CUDA to access the parallel processing capabilities of NVIDIA GPUs. CUDA is an API that enables developers to use the parallel computation capabilities of an NVIDIA GPU, so you will need to use an NVIDIA GPU. You can either install an NVIDIA GPU on your local computer, or use a cloud service provider such as Amazon AWS that provides instances with NVIDIA GPUs. Please take note of the running costs of such cloud instances before you use them for extended periods of training.</p>
<p>Once you have trained a model using Caffe2, you can use CPUs, GPUs, or many other processors for inference. We will explore a few such options in <a href="91e4cdcf-24f6-4426-ac95-b6845c020d83.xhtml">Chapter 6</a>, <em>Deploying Models to Accelerators for Inference</em>, and <a href="91e4cdcf-24f6-4426-ac95-b6845c020d83.xhtml">Chapter 7</a>, <em>Caffe2 at the Edge and in the cloud</em>, later in the book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Software requirements</h1>
                </header>
            
            <article>
                
<p>A major portion of <span>deep learning</span> research and development is currently taking place on Linux computers. <strong>Ubuntu</strong> is a distribution of Linux that happens to be very popular for <span>deep learning</span> research and development. We will be using Ubuntu as the operating system of choice in this book. If you are using a different flavor of Linux, you should be able to search online for commands similar to Ubuntu commands for most of the operations described here. If you use Windows or macOS, you will need to replace the Linux commands in this book with equivalent commands. All the code samples should work on Linux, Windows, and macOS with zero or minimal changes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building and installing Caffe2</h1>
                </header>
            
            <article>
                
<p>Caffe2 can be built and installed from source code quite easily. Installing Caffe2 from source gives us more flexibility and control over our application setup. The build and install process has four stages:</p>
<ol>
<li>Installing dependencies</li>
<li>Installing acceleration libraries</li>
</ol>
<ol start="3">
<li>Building Caffe2</li>
<li>Installing Caffe2</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing dependencies</h1>
                </header>
            
            <article>
                
<p>We first need to install packages that Caffe2 is dependent on, as well as the tools and libraries required to build it.</p>
<ol>
<li>First, obtain information about the newest versions of Ubuntu packages by querying their online repositories using the <kbd>apt-get</kbd> tool:</li>
</ol>
<pre style="padding-left: 90px"><strong>$ sudo apt-get update</strong></pre>
<ol start="2">
<li>Next, using the <kbd>apt-get</kbd> tool, install the libraries that are required to build Caffe2, and that Caffe2 requires for its operation:</li>
</ol>
<pre style="padding-left: 90px"><strong>$ sudo apt-get install -y --no-install-recommends \</strong><br/><strong>      build-essential \</strong><br/><strong>      cmake \</strong><br/><strong>      git \</strong><br/><strong>      libgflags2 \</strong><br/><strong>      libgoogle-glog-dev \</strong><br/><strong>      libgtest-dev \</strong><br/><strong>      libiomp-dev \</strong><br/><strong>      libleveldb-dev \</strong><br/><strong>      liblmdb-dev \</strong><br/><strong>      libopencv-dev \</strong><br/><strong>      libopenmpi-dev \</strong><br/><strong>      libsnappy-dev \</strong><br/><strong>      libprotobuf-dev \</strong><br/><strong>      openmpi-bin \</strong><br/><strong>      openmpi-doc \</strong><br/><strong>      protobuf-compiler \</strong><br/><strong>      python-dev \</strong><br/><strong>      python-pip                          </strong></pre>
<p style="padding-left: 60px">These packages include tools required to download Caffe2 source code (Git) and to build Caffe2 (<kbd>build-essential</kbd>, <kbd>cmake</kbd>, and <kbd>python-dev</kbd>). The rest are libraries that Caffe2 is dependent on, including Google Flags (<kbd>libgflags2</kbd>), Google Log (<kbd>libgoogle-glog-dev</kbd>), Google Test (<kbd>libgtest-dev</kbd>), LevelDB (<kbd>libleveldb-dev</kbd>), LMDB (<kbd>liblmdb-dev</kbd>), OpenCV (<kbd>libopencv-dev</kbd>), OpenMP (<kbd>libiomp-dev</kbd>), OpenMPI (<kbd>openmpi-bin and openmpi-doc</kbd>), Protobuf (<kbd>libprotobuf-dev and protobuf-compiler</kbd>), and Snappy (<kbd>libsnappy-dev</kbd>).</p>
<ol start="3">
<li>Finally, install the Python Pip tool and use it to install other Python libraries such as <kbd>NumPy</kbd> and <kbd>Protobuf</kbd> Python APIs that are useful when working with Python:</li>
</ol>
<pre style="padding-left: 90px"><strong>$ </strong><strong>sudo apt-get install -y --no-install-recommends </strong><strong>python-pip </strong><strong>                  </strong><br/><br/><strong>$ pip install --user \</strong><br/><strong>      future \</strong><br/><strong>      numpy \</strong><br/><strong>      protobuf</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing acceleration libraries</h1>
                </header>
            
            <article>
                
<p>Using Caffe2 to train DL networks and using them for inference involves a lot of math computation. Using acceleration libraries of math routines and deep learning primitives helps Caffe2 users by speeding up training and inference tasks. Vendors of CPUs and GPUs typically offer such libraries, and Caffe2 has support to use such libraries if they are available on your system.</p>
<p><strong>Intel Math Kernel Library</strong> (<strong>MKL</strong>) is key to faster training and inference on Intel CPUs. This library is free for personal and community use. It can be downloaded by registering here: <a href="https://software.seek.intel.com/performance-libraries">https://software.seek.intel.com/performance-libraries</a>. Installation involves uncompressing the downloaded package and running the <kbd>install.sh</kbd> installer script as a superuser. The library files are installed by default to the <kbd>/opt/intel</kbd> directory. The Caffe2 build step, described in the next section, finds and uses the BLAS and LAPACK routines of MKL automatically, if MKL was installed at the default directory.</p>
<p><strong>CUDA</strong> and <strong>CUDA Deep Neural Network</strong> (<strong>cuDNN</strong>) libraries are essential for faster training and inference on NVIDIA GPUs. CUDA is free to download after registering here: <a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a>. cuDNN can be downloaded from here: <a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a>. Note that you need to have a modern NVIDIA GPU and an NVIDIA GPU driver already installed. As an alternative to the GPU driver, you could use the driver that is installed along with CUDA. Files of the CUDA and cuDNN libraries are typically installed in the <kbd>/usr/local/cuda</kbd> directory on Linux. The Caffe2 build step, described in the next section, finds and uses CUDA and cuDNN automatically if installed in the default directory.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building Caffe2</h1>
                </header>
            
            <article>
                
<p>Using Git, we can clone the Git repository containing Caffe2 source code and all the submodules it requires:</p>
<pre><strong>$ git clone --recursive https://github.com/pytorch/pytorch.git &amp;&amp; cd pytorch<br/></strong><br/><strong>$ git submodule update --init</strong></pre>
<p>Notice how the Caffe2 source code now exists in a subdirectory inside the PyTorch source repository. This is because of Facebook's cohabitation plan for these two popular DL frameworks as it endeavors to merge the best features of both frameworks over a period of time.</p>
<p>Caffe2 uses CMake as its build system. CMake enables Caffe2 to be easily built for a wide variety of compilers and operating systems.</p>
<p>To build Caffe2 source code using CMake, we first create a build directory and invoke CMake from within it:</p>
<pre><strong>$ mkdir build<br/>$ cd build</strong><br/><strong>$ cmake ..<br/></strong></pre>
<p>CMake checks available compilers, operating systems, libraries, and packages, and figures out which Caffe2 features to enable and compilation options to use. These options can be seen listed in the <kbd>CMakeLists.txt</kbd> file present at the root directory. Options are listed in the form of <kbd>option(USE_FOOBAR "Use Foobar library" OFF)</kbd>. You can enable or disable those options by setting them to <kbd>ON</kbd> or <kbd>OFF</kbd> in <kbd>CMakeLists.txt</kbd>.</p>
<p>These options can also be configured when invoking CMake. For example, if your Intel CPU has support for AVX/AVX2/FMA, and you would wish to use those features to speed up Caffe2 operations, then enable the <kbd>USE_NATIVE_ARCH</kbd> option as follows:</p>
<pre><strong>$ cmake -DUSE_NATIVE_ARCH=ON ..</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Caffe2</h1>
                </header>
            
            <article>
                
<p>CMake produces a <kbd>Makefile</kbd> file at the end. We can build Caffe2 and install it on our system using the following command:</p>
<pre><strong>$ sudo make install</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This step involves building a large number of CUDA files, which can be very slow. It is recommended to use the parallel execution feature of <kbd>make</kbd> to use all the cores of your CPU for a faster build. We can do this by using the following command:</p>
<pre><strong>$ sudo make -j install</strong></pre>
<p>Using the <kbd>make</kbd> install method to build and install makes it difficult to update or uninstall Caffe2 later.<br/>
Instead, I prefer to create a Debian package of Caffe2 and install it. That way, I can uninstall or update it conveniently. We can do this using the <kbd>checkinstall</kbd> tool.</p>
<p>To install <kbd>checkinstall</kbd>, and to use it to build and install Caffe2, use the following commands:</p>
<pre><strong>$ sudo apt-get install checkinstall</strong><br/><strong>$ sudo checkinstall --pkgname caffe2</strong></pre>
<p>This command also produces a Debian <kbd>.deb</kbd> package file that you can use to install on other computers or share with others. For example, on my computer, this command produced a file named <kbd>caffe2_20181207-1_amd64.deb</kbd>.</p>
<p>If you need a faster build, use the parallel execution feature of <kbd>make</kbd> along with <kbd>checkinstall</kbd>:</p>
<pre><strong>$ sudo checkinstall --pkgname caffe2 make -j install</strong></pre>
<p>If you need to uninstall Caffe2 in the future, you can now do that easily using the following command:</p>
<pre><strong>$ sudo dpkg -r caffe2</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the Caffe2 Python API</h1>
                </header>
            
            <article>
                
<p>We have now installed Caffe2, but we need to make sure it is correctly installed and that its Python API is working. An easy way to do that is to return to your home directory and check whether the Python API of Caffe2 is imported and can execute correctly. This can be done using the following commands:</p>
<pre><strong>$ cd ~</strong><br/><strong>$ python -c "from caffe2.python import core"</strong></pre>
<p>Do not run the preceding command from within the Caffe2 directories. This is to avoid the ambiguity of Python having to pick between your installed Caffe2 files and those in the source or build directories.</p>
<p>If your Caffe2 is <em>not</em> installed correctly, you may see an error of some kind, such as the one shown in the following code block, for example:</p>
<pre><strong>$ python -c "from caffe2.python import core"</strong><br/><strong>Traceback (most recent call last):</strong><br/><strong> File "&lt;string&gt;", line 1, in &lt;module&gt;</strong><br/><strong>ImportError: No module named caffe2.python</strong></pre>
<p>If your Caffe2 has been installed correctly, then you may not see an error. However, you may still get a warning if you don't have a GPU:</p>
<pre><strong>$ python -c "from caffe2.python import core"</strong><br/><strong>WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the Caffe2 C++ API</h1>
                </header>
            
            <article>
                
<p>We have now installed Caffe2, but we need to make sure it is correctly installed and that its C++ API is working. An easy way to do that is to create a small C++ program that initializes the global environment of Caffe2. This is done by calling a method named <kbd>GlobalInit</kbd> and passing it the program's arguments. This is typically the first call in a Caffe2 C++ application.</p>
<p>Create a C++ source file named <kbd>ch1.cpp</kbd> with this code:</p>
<pre>// ch1.cpp<br/>#include "caffe2/core/init.h"<br/><br/>int main(int argc, char** argv)<br/>{<br/>    caffe2::GlobalInit(&amp;argc, &amp;argv);<br/>    return 0;<br/>}</pre>
<p>We can compile this C++ source file using the following command:</p>
<pre><strong>$ g++ ch1.cpp -lcaffe2</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We ask the linker to link with the <kbd>libcaffe2.so</kbd> shared library file by using the <kbd>-lcaffe2</kbd> option. The compiler uses the default include file locations, and the linker uses the default shared library file locations, so we do not need to specify those.</p>
<p>By default, Caffe2 header files are installed to a <kbd>caffe2</kbd> subdirectory in <kbd>/usr/local/include</kbd>. This location is usually automatically included in a C++ compilation. Similarly, the Caffe2 shared library files are installed to <kbd>/usr/local/lib</kbd> by default. If you installed Caffe2 to a different location, you would need to specify the include directory location using the <kbd>-I</kbd> option and the shared library file location using the <kbd>-L</kbd> option.</p>
<p>We can now execute the compiled binary:</p>
<pre><strong>$ ./a.out</strong></pre>
<p>If it executes successfully, then your Caffe2 installation is fine. You are now ready to write Caffe2 C++ applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Congratulations! This chapter provided a brief introduction to deep learning and Caffe2. We examined the process of building and installing Caffe2 on our system. We are now ready to explore the world of deep learning by building our own networks, training our own models, and using them for inference on real-world problems.</p>
<p>In the next chapter, we will learn about Caffe2 operators and learn how to compose them to build simple computation graphs. We will then proceed to build a neural network that can recognize handwritten digits.</p>


            </article>

            
        </section>
    </body></html>