- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we saw how to perform several signal-processing tasks while
    leveraging the predictive power of feedforward neural networks. This foundational
    architecture allowed us to introduce many of the basic features that comprise
    the learning mechanisms of **Artificial Neural Networks** (**ANNs**).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we dive deeper to explore another type of ANN, namely the **Convolutional
    Neural Network** (**CNN**), famous for its adeptness at visual tasks such as image
    recognition, object detection, and semantic segmentation, to name a few. Indeed,
    the inspiration for these particular architectures also refers back to our own
    biology. Soon, we will go over the experiments and discoveries of the human race
    that led to the inspiration for these complex systems that perform so well. The
    latest iterations of this idea can be traced back to the ImageNet classification
    challenge, where AlexNet was able to outperform the state-of-the-art computer
    vision systems of the time at image classification tasks on supermassive datasets.
    However, the idea behind CNNs, as we will soon see, was a product of multidisciplinary
    scientific research, backed by millions of years of trail runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why CNNs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The birth of vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding biological vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The birth of the modern CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing a CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense versus convolutional layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolution operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preserving the spatial structure of an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction using filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why CNNs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs are very similar to ordinary neural networks. As we have seen in the previous
    chapter, neural networks are made up of neurons that have learnable weights and
    biases. Each neuron still computes the weighted sum of its inputs using dot products,
    adds a bias term, and passes it through a nonlinear equation. The network will
    show just one differentiable score function that  will be, from raw images at
    one end to the class scores at other end.
  prefs: []
  type: TYPE_NORMAL
- en: And they will also have a loss function such as the softmax, or SVM on the last
    layer. Moreover, all the techniques that we learned ti develop neural networks
    will be applicable.
  prefs: []
  type: TYPE_NORMAL
- en: But then what's different with ConvNets you may ask. So the main point to note
    is that the ConvNet architecture explicitly assumes that the inputs that are received
    are all images, this assumption actually helps us to encode other properties of
    the architecture itself. Doing so permits the network to be more efficient from
    an implementation perspective, vastly reducing the number of parameters required
    in the network. We call a network *convolutional* because of the convolutional
    layers it has, in addition to other types of layers. Soon, we will explore how
    these special layers, along with a few other mathematical operations, can help
    computers visually comprehend the world around us.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, this specific architecture of neural networks excels at a variety of
    visual-processing tasks, ranging from object detection, face recognition, video
    classification, semantic segmentation, image captioning, human pose estimation,
    and many, many more. These networks allow an array of computer vision tasks to
    be performed effectively, some critical for the advancement of our species (such
    as medical diagnoses), and others bordering on entertainment (superimposing a
    certain artistic style on a given image). Before we dive deep into its conception
    and contemporary implementation, it is quite useful to understand the broader
    scope of what we are trying to replicate, by taking a quick tour of how vision,
    something so complex, yet so innate to us humans, actually came about.
  prefs: []
  type: TYPE_NORMAL
- en: The birth of vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following is an epic tale, an epic tale that took place nearly 540 million
    years ago.
  prefs: []
  type: TYPE_NORMAL
- en: Around this time, on the pale blue cosmic dot that would later become known
    as Earth, life was quite tranquil and hassle-free. Back then, almost all of our
    ancestors were water dwellers, who would just float about in the serenity of the
    oceans, munching on sources of food only if they were to float by them. Yes, this
    was quite different than the predatory, stressful, and stimulating world of today.
  prefs: []
  type: TYPE_NORMAL
- en: Suddenly, something quite curious occurred. In a comparatively short period
    of time that followed, there was an explosion in the number and variety of animal
    species present on our planet. In only a span of about 20 million years that came
    after, the kind of creatures you could find on our watery earth drastically changed.
    They changed from the occasional single-celled organisms you would encounter,
    organized in loosely connected colonies, to complex multi-cellular organisms,
    popping up in every creek and corner.
  prefs: []
  type: TYPE_NORMAL
- en: Biologists remained baffled for a very long time, debating what caused this *big
    bang* of evolutionary acceleration. What we had actually discovered was the birth
    of the biological visual system. Studying the fossil records of the organisms
    from that time, zoologists were able to present decisive proof, connecting this
    explosion of species with the first appearance of photo-receptive cells. These
    cells allowed organisms to sense and respond to light, triggering an evolutionary
    arms race that eventually led to the sophisticated mammalian visual cortex that
    you are likely using right now to interpret this piece of text. Indeed, the gift
    of sight made life much more dynamic and proactive, as now organisms were able
    to sense and respond to their environments.
  prefs: []
  type: TYPE_NORMAL
- en: Today, vision is one of the main sensory systems in nearly all organisms, intelligent
    or not. In fact, we humans use almost half of our neuronal capacity for visual
    processing, making it the biggest sensory system we employ to orient ourselves,
    recognize people and objects, and go about our daily lives. As it turns out, vision
    is a very important component of cognitive systems, biological or otherwise. Hence,
    it is quite reasonable to go about examining the development and implementation
    of visual systems created by nature. After all, there's no point reinventing the
    wheel.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding biological vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our next insight into biological visual systems comes from a series of experiments
    conducted by scientists from Harvard University, back in the late 1950s. Nobel
    laureates David Hubel and Torstein Wiesel showed the world the inner workings
    of the mammalian visual cortex, by mapping the action of receptor cells along
    with the visual pathway of a cat, from the retina to the visual cortex. These
    scientists used electrophysiology to understand exactly how our sensory organs
    intake, process, and interpret electromagnetic radiation, to generate the reality
    we see around us. This allowed them to better appreciate the flow of stimuli and
    related responses that occur at the level of individual neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a057c3ee-63ef-4454-b8e4-1acbc968f2a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot describes how cells respond to light:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55e51fb4-6c25-4605-aec1-3b30a1ba6fba.png)'
  prefs: []
  type: TYPE_IMG
- en: Thanks to their experiments in the field of neuroscience, we are able to share
    with you several key elements of their research that directly affected the scientific
    understanding of visual signal processing, leading to a cascade of academic contributions
    that brings us to this date. These elements enlighten the mechanism of visual
    information processing leveraged by our own brain, inspiring the design of the
    CNN, a cornerstone of modern visual intelligence systems.
  prefs: []
  type: TYPE_NORMAL
- en: Conceptualizing spatial invariance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first of these notions comes from the concept of **spatial invariance**.
    The researchers noticed that the cat's neural activations to particular patterns
    would be consistent, regardless of the exact location of the patterns on the screen.
    Intuitively, the same set of neurons were noted to fire for a given pattern (that
    is, a line segment), even if the pattern appeared at the top or the bottom of
    the screen. This showed that the neurons' activations were spatially invariant,
    meaning that their activations were not dependent on the spatial location of the
    given patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Defining receptive fields of neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Secondly, they also noted that neurons were *in charge* of responding to specific
    regions of a given input. They named this property of a neuron as its **receptive
    field**. In other words, certain neurons only responded to certain regions of
    a given input, whereas others responded to different regions of the same input.
    The receptive field of a neuron simply denotes the span of input to which a neuron
    is likely to respond.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a hierarchy of neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, the researchers were able to demonstrate that a hierarchy of neurons
    exists in the visual cortex. They showed that lower-level cells are tasked to
    detect simple visual patterns, such as line segments. The output from these neurons
    is used in subsequent layers of neurons to construct more and more complex patterns,
    forming the objects and people we see and interact with. Indeed, modern neuroscience
    confirms that the structure of the visual cortex is hierarchically organized to
    perform increasingly complex inferences using the output of previous layers, as
    illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb3d45e9-1025-4e42-a472-06eef7410b21.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram means that recognizing a friend involves detecting the
    line segments making up their face (**V1**), using those segments to build shapes
    and edges (**V2**), using those shapes and edges to form complex shapes such as
    eyes and noses (**V3**), and then leveraging previous knowledge to infer who out
    of your friends does this current bundle of eyes and nose resemble the most (**IT-posterior**).
    Based upon that reasoning, even higher-level activations pertaining to notions
    of that friend's personality, attractiveness, and so on, may emerge (**IT-anterior**).
  prefs: []
  type: TYPE_NORMAL
- en: The birth of the modern CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It wasn''t until the 1980s that Heubel and Wiesel''s findings were repurposed
    in the field of computer science. The *Neurocognitron* (Fukushima, 1980: [https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf))
    leveraged the concept of simple and complex cells by sandwiching layers of one
    after the other. This ancestor of the modern neural network used the aforementioned
    alternating layers to sequentially include modifiable parameters (or simple cells),
    while using pooling layers (or complex cells) to make the network invariant to
    minor altercations from the simple cells. While intuitive, this architecture was
    still not powerful enough to capture the intricate complexities present in visual
    signals.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the major breakthroughs followed in 1998, when famed AI researchers,
    Yan Lecun and Yoshua Bengio, were able to train a CNN, leveraging gradient-based
    weight updates, to perform document recognition. This network did an excellent
    job of recognizing digits of zip codes. Similar networks were quickly adopted
    by organizations such as the US postal service, to automate the tedious task of
    sorting mail (nope, not the electronic kind). While the results were impressive
    enough for commercial interest in narrow segments, these networks were still not
    able to handle more challenging and complex data, such as faces, cars, and other
    real-world objects. However, the collective work of these researchers and many
    others led to the modern incarnation of the larger and deeper CNNs, first appearing
    at the ImageNet classification challenge. These networks have now come to dominate
    the realm of computer vision, making their appearance in today's most sophisticated
    artificial visual intelligence systems. These networks are now used for tasks
    such as medical image diagnosis, detecting celestial objects in outer space, and
    making computers play old-school Atari games, to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, armed with the intuition of biological vision, we understand how neurons
    must be organized hierarchically, to detect simple patterns and use these to progressively
    build more complex patterns corresponding to real-world objects. We also know
    that we must implement a mechanism for spatial invariance to allow neurons to
    deal with similar inputs occurring at different spatial locations of a given image.
    Finally, we are aware that implementing a receptive field for each neuron is useful
    to achieve a topographical mapping of neurons to spatial locations in the real
    world, so that nearby neurons may represent nearby regions in the field of vision.
  prefs: []
  type: TYPE_NORMAL
- en: Dense versus convolutional layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will recall from the previous chapter that we used a feedforward neural
    network, composed of fully connected dense layers of neurons, to perform the task
    of handwritten digit recognition. While constructing our network, we were forced
    to flatten our input pixels of 28 x 28 into a vector of 784 pixels, for each image.
    Doing so caused us to lose any spatially relevant information that our network
    could leverage for classifying the digits it was shown. We simply showed it a
    784-dimensional vector for each image in our dataset and expected it to recognize
    digits thereafter. While this approach was sufficient to attain a considerable
    accuracy in classifying simple handwritten digits from the nice and clean MNIST
    dataset, it quickly becomes impractical in more complex data that we may want
    to deal with, involving a multitude of local patterns of different spatial orientations.
  prefs: []
  type: TYPE_NORMAL
- en: We would ideally like to preserve spatial information and reuse neurons for
    the detection of similar patterns occurring in different spatial regions. This
    would allow our convolutional networks to be more efficient. Through reusing its
    neurons to recognize specific patterns in our data irrespective of their location,
    CNNs are advantageous to use in visual tasks. A densely connected network, on
    the other hand, would be forced to learn a pattern again, if it were to appear
    in another location of the image. Given the natural spatial hierarchies that exist
    in visual data, using convolution layers is an ideal way to detect minute local
    patterns and use them to progressively build more complex ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot illustrates the hierarchical nature of visual data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/896eb13d-29a1-4df1-baa0-657440c08e48.png)'
  prefs: []
  type: TYPE_IMG
- en: Another problem we mentioned with dense layers is to do with their weakness
    in capturing local patterns from data. The dense layer is well known for capturing
    global patterns involving all pixels in images. As the findings of Hubel and Wiesel
    suggest, however, we want to limit the receptive field of our neurons to local
    patterns present in the data and use those patterns to progressively form more
    complex ones. This would allow our network to deal with very different forms of
    visual data, each with different types of local patterns enclosed. To overcome
    these problems, among others, the core component of CNN was developed, known as
    the **convolution operation**.
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The word *convolvere* comes from Latin and translates to *convolve* or *roll
    together*. From a mathematical perspective, you may define a convolution as the
    calculus-based integral denoting the amount by which two given functions overlap,
    as one of the two is slid across the other. In other words, performing the convolution
    operation on two functions (*f* and *g*) will produce a third function that expresses
    how the shape of one is modified by the other. The term *convolution* refers to
    both the result function and the computation process, with roots in the mathematical
    subfield of signal processing, as we can here in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98d0d59a-e7b7-454c-9bba-6fe684609c99.png)'
  prefs: []
  type: TYPE_IMG
- en: So, how can we leverage this operation to our advantage?
  prefs: []
  type: TYPE_NORMAL
- en: Preserving the spatial structure of an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Firstly, we will use the inherent spatial structure of an image by simply feeding
    it as an *n*-dimensional tensor into our neural network. This means that the network
    will receive each pixel in its original matrix position, and not reduced to a
    position inside a single vector, as we did before. In the case of the MNIST example,
    a convolutional network would receive as input a 28 x 28 x 1 tensor, representing
    each image. Here, 28 x 28 represents a two-dimensional grid upon which the pixels
    are arranged to form the handwritten digits, whereas `1` at the end denotes the
    color channels of the image (that is, the number of pixel values per pixel, in
    a given image). As we know, a colored dataset may have image dimensions such as
    28 x 28 x 3, where `3` denotes the different red, green, and blue values that
    form the color of an individual pixel. Since we only deal with single grayscale
    values per pixel in the MNIST dataset, the color channel is denoted as 1\. Hence,
    it is important to understand that an image enters our network as a three-dimensional
    tensor, preserving the spatial structure of our image data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0c88e93-5c70-4ca0-a02c-cc73dbbce452.png)'
  prefs: []
  type: TYPE_IMG
- en: Receptive field
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we can leverage the properties of additional spatial information, inherent
    to the image, in the architecture of our network. In other words, we are now ready
    to perform a convolution operation. This simply means that we will use a smaller
    segment of space and slide it over our input image as a filter to detect local
    patterns. The intuition here is to connect patches of our input data space to
    corresponding neurons in the hidden layers. Doing so allows the neuron to only
    observe a local area of the image at each convolution, thereby limiting its receptive
    field. Limiting a neuron's receptive field is useful for two main reasons. Since
    we reason that nearby pixels are more likely to be related to each other in an
    image, limiting the receptive field of neurons in our network makes these neurons
    better able to distinguish local variances between pixels in a given image. Moreover,
    this practice also allows us to drastically reduce the number of learnable parameters
    (or weights) in our network. In this manner, we use a **filter**, which is essentially
    a matrix of weights and apply it iteratively over the input space starting from
    the top-left corner of an image. We move a stride to the right at each convolution,
    by centering our filter on top of each pixel, in our input space. Once having
    convolved over the image segment corresponding to the top rows of pixels, we repeat
    the operation from the left side of the image once again, this time, for the rows
    underneath. This is how we slide the filter across our entire input image, extracting
    local features at each convolution by computing the dot products of our input
    area and the so-called **filters**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram here depicts the initial step of a convolution operation. Progressively,
    the three-dimensional blue rectangle shown here will be slid across segments (in
    red) of the entire image, giving the convolution operation its name. The rectangle
    itself, is known as a filter, or a **convolutional kernel**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89364f65-c2de-43ec-ba69-99f8f3a67982.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature extraction using filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each filter can be essentially thought of as an arrangement of neurons, similar
    in spirit to the ones we encountered in [Chapter 3](46e25614-bb5a-4cca-ac3e-b6dfbe29eea5.xhtml), *Signal
    Processing - Data Analysis with Neural Networks*. Here, the filter's neurons are
    initialized with random weights and progressively updated using the backpropagation
    algorithm during training. The filter itself is in charge of detecting a particular
    type of pattern, ranging from line segments to curves and more complex shapes.
    As a filter passes over an area of the input image, the filter weights are multiplied
    (element-wise) with the pixel values at that location, generating a single output
    vector. Then, we simply reduce this vector to a scalar value using a summation
    operation, exactly as we did for our feedforward neural network previously. These
    scalar values are generated at each new position the filter is moved to, spanning
    the entire input image. The values are stored in something referred to as the
    **activation map** (also known as a **feature map** or a **response map**) for
    a given filter. The activation map itself is conventionally smaller in dimension
    than the input image, and it embodies a new representation of the input image,
    while accentuating certain patterns within.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights of the filters themselves can be thought of as the learnable parameters
    of a convolutional layer and are updated to capture patterns relevant to the task
    at hand, while the network trains:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fd980af-7e27-4b63-9338-0dbbad1f50ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation of errors in CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As our network trains to find ideal filter weights, we hope that the activation
    map of a given filter is able to capture the most informative visual patterns
    that may exist in our data. Essentially, these activation maps are generated using
    matrix-wise multiplication. These activation maps are fed into the subsequent
    layer as inputs, and thus information propagates forward, up until the final layers
    of our model, which perform the classification. At this point, our `loss` function
    assesses the difference in the network's predictions versus the actual output,
    and backpropagates the prediction errors to adjust the networks weights, for each
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is basically how a ConvNet is trained, on a very high level. The convolution
    operation consists of iteratively computing the dot product of a transposed filter
    matrix (holding the weights of our convolutional filter) with the corresponding
    input space of pixels to extract generalizable features from the training examples.
    These feature maps (or activation maps) are then first fed into pooling layers
    to reduce their dimensionality, and subsequently fed into fully connected layers
    to determine which combination of filters best represent a given output class.
    As the model weights are updated during a backward pass, new activation maps are
    generated at the next forward pass, which ideally encode more representative features
    of our data. Here, we present a brief visual summary of the ConvNet architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b288bdad-fdfc-45d1-a7a5-be63bb8a9a9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It follows the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Learns features in input image through convolution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduce non-linearity through activation function(real-world data is non-linear)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce dimensionality and preserve spatial invariance with pooling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using multiple filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since filters are pattern-specific (that is, each filter excels at picking up
    a certain type of pattern), we require more than just one filter to pick up all
    the different types of patterns that may exist in our image. This means that we
    may use multiple filters for a given convolutional layer in our network, allowing
    us to extract multiple distinct local features for a given region of input space.
    These local features, stored in the activation map of that specific filter, may
    be passed on to subsequent layers to build more complex patterns. Progressive
    convolutional layers will use different filters to convolve over the input activation
    map from the previous layer, again extracting and transforming the input map into
    a three-dimensional tensor output corresponding to the activation maps for each
    filter used. The new activation maps will again be three-dimensional tensors and
    can be similarly passed on the subsequent layers. Recall that when an image enters
    our network, it does so as a three-dimensional tensor with (width, height, and
    depth) corresponding to the input image dimensions (where depth is denoted by
    the pixel channels). In our output tensor, on the other hand, the depth axis denotes
    the number of filters used in the previous convolutional layer, where each filter
    produces its own activation map. In essence, this is how data propagates forward
    in a CNN, entering as an image and exiting as a three-dimensional activation map,
    being progressively transformed by various filters as the data propagates through
    deeper layers. The exact nature of transformations will become clearer soon, when
    we will build a ConvNet. We'll discuss theory a little bit more, and then we will
    be ready to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: Stride of a convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram depicts the convolution operation that you have now familiarized
    yourself with, in two dimensions (for simplicity). It shows a 4 x 4 filter (red
    box) being slid across a larger image (14 x 14), taking a step of two pixels at
    a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45490089-e87f-47df-8725-450ecaefce34.png)'
  prefs: []
  type: TYPE_IMG
- en: The number of pixels by which a filter shifts at each iteration is known as
    its **stride**. At each stride, a dot product is computed using the pixel matrix
    from the corresponding input region, as well as the filter weights. These dot
    products are stored as a scalar value in the activation map for that filter (shown
    as a 6 x 6 square as follows). Hence, the activation map denotes a reduced representation
    of the layer inputs, and essentially is a matrix composed of the summed-up dot
    products we get by convolving our filter over segments of the input data. On a
    higher level, the activation map represents the activation of neurons for specific
    patterns that the respective filter is detecting. Longer strides of these filters
    will lead to less detailed sampling of the corresponding input regions, whereas
    shorter strides will allow individual pixels to be sampled more often, permitting
    a higher definition activation map.
  prefs: []
  type: TYPE_NORMAL
- en: What are features?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the overall mechanism of collecting features with different types of filters
    may be clear, you may well be wondering what a feature actually looks like and
    how we go about extracting them from a filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a simple example to clarify our understanding. Suppose you
    wished to detect the letter X from a bunch of grayscale images of letters. How
    would a CNN go about doing this? Well, let''s first consider the image of an X.
    As shown as follows, we can think of the pixels with positive values form the
    lines of an X, whereas pixels with a negative value simply represent blank space
    in our image, as shown in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63093442-c5bf-41be-87b7-e09edd13968c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But then comes the problem of positional variance: What if the X appears slightly
    rotated or distorted in any other way? In the real world, the letter X comes in
    many sizes, shapes, and so on. How can we break up the image of an X using smaller
    filters that capture its underlying patterns? Well, here''s one way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba63b741-18d9-40a1-ae8c-6d7bd9d98f13.png)'
  prefs: []
  type: TYPE_IMG
- en: As you may have noticed, we can actually break down the image into smaller segments,
    where each segment (denoted by the green, orange, and purple boxes) represents
    recurring patterns within the image. In our case, we are able to use two diagonally
    oriented filters and one intersecting filter and slide each of them across our
    image to pick up on the line segments that form an X. Essentially, you may think
    of each filter as a pattern detector of sorts. As these different filters convolve
    over our input image, we are left with activation maps that start to resemble
    long horizontal lines and crosses, which the network learns to combine to recognize
    the letter X.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing feature extraction with filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider another example, to solidify our understanding of how filters
    detect patterns. Consider this depiction of the number 7, taken from the MNIST
    dataset. We use this 28 x 28 pixelated image to show how filters actually pick
    up on different patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3666371-2cdb-4ae5-975f-f3e73b6e5a56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Intuitively, we notice that this 7 is composed of two horizontal lines, as
    well as a slanted vertical line. We essentially need to initialize our filters
    with values that can pick up on these separate patterns. Next, we observe some
    3 x 3 filter matrices that a ConvNet would typically learn for the task at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08cb92c2-c8cb-466c-a60f-edfc86715ef0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While not very intuitive to visualize, these filters are actually sophisticated
    edge detectors. To see how they work, let''s picture each 0 in our filter weights
    as the color grey, whereas each value of 1 takes the color white, leaving -1 with
    the color black. As these filters convolve over an input image, element-wise multiplications
    are performed using the filter values and the pixels underneath. The product of
    this operation is yet another matrix, known as an **activation map**, representing
    particular features picked up by their respective filters, for a given input image.
    Now let''s see the effects of executing a convolution operation over the input
    image of a 7, using each of these four filters, to exactly understand what kind
    of patterns they individually pick up on. The following illustration plots out
    the activation map for each filter in the convolutional layer, once it is shown
    the image of a 7:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53e3fdc9-124c-4381-8cf4-cb75e93adafc.png)'
  prefs: []
  type: TYPE_IMG
- en: We observe that each filter is able to pick up on a specific pattern in the
    input image by simply computing the dot product summation of pixel values and
    filter weights at each spatial location. The patterns picked up can be denoted
    by the white regions in the aforementioned activation maps. We see that the first
    two filters adeptly pick up on upper and lower horizontal lines in the image of
    the 7, respectively. We also notice how the latter two filters pick up on the
    inner and outer vertical lines that form the body of the digit 7, respectively.
    While these are still examples of fairly simple pattern detections, progressive
    layers in ConvNets tend to be able to pick up on much more informative structures,
    by differentiating colors and shapes, which are essentially patterns of numbers,
    to our network.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at complex filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image shows the top nine activation maps per grid, associated
    with specific inputs, for the second layer of a ConvNet. On the left, you can
    think of the mini-grids as activations of individual neurons, for given inputs.
    The corresponding colored grids on the right relate to the inputs these neurons
    were shown. What we are visualizing here is the kind of input that *maximizes*
    the activation of these neurons. We notice that already some pretty-clear circle
    detector neurons are visible (grid 2, 2), being activated for inputs such as the
    top of lamp shades and animal eyes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bcd19b0-5d97-436d-8491-fffc1f3d477e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we notice some square-like pattern detectors (grid 4, 4) that seem
    to activate for images containing door and window frames. As we progressively
    visualize activation maps for deeper layers in CNNs, we observe even more complex
    geometric patterns being picked up, representing faces of dogs (grid 1, 1), bird
    legs (grid 2, 4), and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99378b16-e96a-4e0f-8614-46d48a6d3f34.png)'
  prefs: []
  type: TYPE_IMG
- en: Summarizing the convolution operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All that we are doing here is applying a set of weights (that is, a filter)
    to local input spaces for feature extraction. We do this iteratively, moving our
    filter across the input space in fixed steps, known as a **stride**. Moreover,
    the use of different filters allows us to capture different patterns from a given
    input. Finally, since the filters convolve over the entire image, we are able
    to spatially share parameters for a given filter. This allows us to use the *same*
    filter to detect similar patterns in different locations of the image, relating
    to the concept of spatial invariance discussed earlier. However, these activation
    maps that a convolutional layer outputs are essentially abstract high-dimensional
    representations. We need to implement a mechanism to reduce these representations
    into more manageable dimensions, before we go ahead and perform classification.
    This brings us to the **pooling layers**.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding pooling layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A final consideration when using convolutional layers is to do with the idea
    of stacking simple cells to detect local patterns and complex cells to downsample
    representations, as we saw earlier with the cat-brain experiments, and the neocognitron.
    The convolutional filters we saw behave like simple cells by focusing on specific
    locations on the input and training neurons to fire, given some stimuli from the
    local regions of our input image. Complex cells, on the other hand, are required
    to be less specific to the location of the stimuli. This is where the pooling
    layer comes in. This technique of pooling intends to reduce the output of CNN
    layers to more manageable representations. Pooling layers are periodically added
    between convolutional layers to spatially downsample the outputs of our convolutional
    layer. All this does is progressively reduce the size of the convolutional layer
    outputs, thereby leading to more efficient representations, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e3eb8a8-0a3a-4565-b004-318a4fba670b.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the depth of volume is preserved because we have pooled the
    input volume of size 224 x 224 x 64 with a filter of size 2 and a stride of 2\.
    And this gives an output volume of 112 x 112 x 64.
  prefs: []
  type: TYPE_NORMAL
- en: Types of pooling operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This similarly reduces the number of learnable parameters required in our network
    for the same task and prevents our network from overfitting. Finally, pooling
    is performed for every activation map generated by a given layer (that is, every
    slice of depth of the input tensor) and resizes its input spatially using its
    own filters. It is very common to see pooling layers with 2 x 2 filters along
    with a stride of 2, performed on every depth slice. There are many ways to perform
    this sort of downsampling. Most commonly, this is achieved through a **max pooling**
    operation, which simply means that we preserve the pixel with the highest value
    out of all the pixels in the input region under our pooling filter. The following
    diagram illustrates this max pooling operation on a given slice of our input tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc8b9aa0-1ece-47cb-9762-fbeef39a3276.png)'
  prefs: []
  type: TYPE_IMG
- en: The pooling layer downsamples the activation volume spatially, independently
    in each depth slice of the input volume. The most common downsampling operation
    is max, giving rise to max pooling, here shown with a stride of 2\. That is, each
    max is taken over four numbers (the little 2 x 2 square).
  prefs: []
  type: TYPE_NORMAL
- en: You can also downsample by taking the average value of the 2 x 2 squares as
    shown. This operation is known as **average pooling**. As we will see throughout
    later chapters, Keras comes with quite a few pooling layers that each perform
    different types of downsampling operations on the outputs from the previous layer.
    The choice will largely depend on your specific use case. In the case of image
    classification, a two-dimensional or three-dimensional max pooling layer is most
    commonly used. The dimension of the layer simply refers to the type of input it
    accepts. For example, two-dimensional layers are used to downsample when processing
    grayscale images, and three-dimensional layers are used in the case of colored
    images. You can refer to the well-maintained documentation to study it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing CNNs in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having achieved a high-level understanding of the key components of a CNN, we
    may now proceed with actually implementing one ourselves. This will allow us to
    become familiar with the key architectural considerations when building convolutional
    networks and get an overview of the implementational details that make these networks
    perform so well. Soon, we will implement the convolutional layer in Keras, and
    explore downsampling techniques such as pooling layers to see how we can leverage
    a combination of convolutional, pooling, and densely connected layers for various
    image classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we will adopt a simple use case. Let's say we wanted our CNN
    to detect human emotion, in the form of a smile or a frown. This is a simple binary
    classification task. How do we proceed? Well, firstly, we will need a labeled
    dataset of humans smiling and frowning. While there are many ways to go about
    doing this, we select the **Happy House Dataset** for this purpose. This dataset
    comes with about 750 images of people, each either smiling or frowning, stored
    in the `h5py` files. To follow along, all you need to do is to download the dataset
    hosted on the Kaggle website, which you can access freely through this link: [https://www.kaggle.com/iarunava/happy-house-dataset](https://www.kaggle.com/iarunava/happy-house-dataset)
  prefs: []
  type: TYPE_NORMAL
- en: Probing our data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by loading in and probing our dataset, to get an idea of what
    we are dealing with. We make a simple function that reads our `h5py` files, extracts
    the training and test data, and places it into the standard NumPy arrays, shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Verifying the data shape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will print out the shape of our training and test data. In the following
    code block, we notice that we are dealing with colored images of 64 x 64 pixels.
    We have 600 of these in our training set and `150` in the test set. We can also
    have a look at what an image actually looks like, as we did in previous examples
    using Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '—et voila! Ladies and gentlemen, we have a frowny face:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ab7a4ab-dbf3-4581-8103-06808ab90da1.png)'
  prefs: []
  type: TYPE_IMG
- en: Normalizing our data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will prepare our images by rescaling pixel values between 0 and 1\.
    We also transpose our label matrices, as we want them to be oriented as (600,
    1), and not (1, 600), referring to our training labels, as shown previously in
    the training labels. Finally, we print out the shape of our features and labels
    for both the training and the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we convert our NumPy arrays to floating-point arithmetic values, which
    our network prefers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Making some imports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we get to import the new layers that we will be employing for our
    emotion classification task. In the bottom section of the block of the previous
    code, we import a two-dimensional convolutional layer. The dimension of the convolutional
    layer is a property specific to the task you wish to perform. Since we are dealing
    with images, a two-dimensional convolutional layer is the best choice. If we were
    dealing with time-series sensor data (such as biomedical data—for example, EEGs
    or financial data such as the stock market), then a one-dimensional convolutional
    layer would be a more appropriate choice. Similarly, we would use three-dimensional
    convolutional layers if videos were the input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, we also imported a two-dimensional max pooling layer, along with
    a batch-wise normalizer. Batch normalization simply allows us to deal with the
    changing values of layer outputs, as data propagates through our network.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of ‘internal covariate shift’ is indeed a well noted phenomena in
  prefs: []
  type: TYPE_NORMAL
- en: CNNs as well as other ANN architectures, and refers to the change in the input’s
    statistical
  prefs: []
  type: TYPE_NORMAL
- en: distribution after a few training iterations, slowing down the convergence of
    our model to ideal weights. This problem can be avoided by simply normalizing
    our data in mini-batches, using a mean and variance reference. While we encourage
    you to further research the problem of internal covariate shift and the mathematics
    behind batch normalization, for now it suffices to know that this helps us train
    our network faster and allows higher learning rates, while making our network
    weights easier to initialize
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Two main architectural considerations are associated with the convolutional
    layer in Keras. The first is to do with the number of filters to employ in the
    given layer, whereas the second denotes the size of the filters themselves. So,
    let''s see how this is implemented by initializing a blank sequential model and
    adding our first convolutional layer to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Defining the number and size of the filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw previously, we define the layer by embodying it with 16 filters, each
    with a height and width of 5 x 5\. In actuality, the proper dimensions of our
    filters are 5 x 5 x 3\. However, the depth of all the filters spans the full depth
    of a given input tensor, and hence never needs to be specified. Since this is
    the first layer, receiving as input a tensor representation of our training image,
    the depth of our filters would be 3, for each of the red, green, and blue values
    per pixel covered.
  prefs: []
  type: TYPE_NORMAL
- en: Padding input tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thinking about the convolution operation intuitively, it becomes apparent that
    as we slide our filter across an input tensor, what ends up happening is that
    our filter passes over borders and edges less frequently than it does over other
    parts of the input tensor. This is simply because each pixel that is not located
    at the edge of an input may be resampled by the filter multiple times, as it strides
    across an image. This leaves our output representations with uneven samplings
    of borders and edges from our input, referred to as **the border effect**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can avoid this by simply padding our input tensor with zeros, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1ad6a02-499a-4aaf-8654-16637e6cc1bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this manner, the pixels that would normally appear at the edge of our input
    tensor now appear later on, allowing an even sampling to be performed on all pixels
    within the input. We specify in our first convolutional layer that we want to
    preserve the input length and width of our inputs, so as to ensure the output
    tensor has the same spatial dimensions with respect to its length and width. This
    is done by defining the padding parameter of the layer as `same`. The depth of
    a convolutional layer output, as previously stated, is denoted by the number of
    filters we choose to use. In our case, this would be 16, denoting the 16 activation
    maps produced as each of our 16 filters convolve over the input space. Finally,
    we define the input shape as the dimensions of any single input image. For us,
    this corresponds to the 64 x 64 x 3 coloured pixels that we have in our dataset,
    shown again as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Max Pooling layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The activation maps from our first convolutional layer are normalized and fed
    into the max pooling layer below. Similar to the convolution operation, pooling
    is applied one input region at a time. For the case of max pooling, we simply
    take the largest value in our grid of pixels, which represents the strongest correlating
    pixels to each feature, and combine these max values to form a lower dimensional
    representation of the input image. In this manner, we preserve more important
    values and discard the remaining values in a given grid of the respective activation
    map.
  prefs: []
  type: TYPE_NORMAL
- en: 'This downsampling operation does naturally cause a certain degree of information
    loss, yet drastically reduces the amount of storage space required in networks,
    giving it a considerable efficiency boost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Leveraging a fully connected layer for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Then, we simply add a few more layers of convolution, batch normalization,
    and dropouts, progressively building our network until we reach the final layers.
    Just like in the MNIST example, we will leverage densely connected layers to implement
    the classification mechanism in our network. Before we can do this, we must flatten
    our input from the previous layer (16 x 16 x 32) to a 1D vector of dimension (8,192).
    We do this because dense layer-based classifiers prefer to receive 1D vectors,
    unlike the output from our previous layer. We proceed by adding two densely connected
    layers, the first one with 128 neurons (an arbitrary choice) and the second one
    with just one neuron, since we are dealing with a binary classification problem.
    If everything goes according to plan, this one neuron will be supported by its
    cabinet of neurons from the previous layers and learn to fire when it sees a certain
    output class (for example, smiling faces) and abstain from this when presented
    with images from the other class (for example, frowning faces). Note that we again
    use a sigmoid activation function for our last layer, which computes the class
    probability per given input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Summarizing our model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's visualize our model to better understand what we just built. You will
    notice that the number of activation maps (denoted by the depth of subsequent
    layer outputs) progressively increases throughout the network. On the other hand,
    the length and width of the activation maps tend to decrease, from (64 x 64) to
    (16 x 16), by the time the dropout layer is reached. These two patterns are conventional
    in most, if not all, modern iterations of CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason behind the variance in input and output dimensions between layers
    can depend on how you have chosen to address *the border effects* we discussed
    earlier, or what *stride* you have implemented for the filters in your convolutional
    layer. Smaller strides will lead to higher dimensions, whereas larger strides
    will lead to lower dimensions. This is simply to do with the number of locations
    you are computing dot products at, while storing the result in an activation map.
    Larger filter strides (or steps) will reach the end of an image earlier, having
    computed fewer dot product values over the same input space of a smaller filter
    stride. The strides of a convolutional layer may be set by defining the strides
    parameter, with an integer or tuple/list of single integers for the given layer.
    The integer value(s) will refer to the length of the stride at each convolution
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Following will be the summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/219aa4f8-8c91-4f49-8679-6a85e4769a3b.png)'
  prefs: []
  type: TYPE_IMG
- en: As we noted earlier, you will notice that both the convolutional and the max
    pooling layers generate a three-dimensional tensor with dimensions corresponding
    to the output height, output width, and output depth. The depth of the layer outputs
    are essentially activation maps for each filter initialized. We implemented our
    first convolutional layer with 16 filters, and the second layer with 32 filters,
    hence the respective layers will produce that many numbers of activation maps,
    as seen previously.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the moment, we have covered all the key architectural decisions involved
    in designing a ConvNet and are now ready to compile the network we have been building.
    We choose the `adam` optimizer, and the `binary_crossentropy` loss function, as
    we previously did for the binary sentiment analysis task from the previous chapter.
    Similarly, we also import the `EarlyStopping` callback to monitor our loss on
    the validation set, to get an idea of how well our model is doing on unseen data,
    at each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d2424db-d215-464b-8371-1754e63cba37.png)'
  prefs: []
  type: TYPE_IMG
- en: Checking model accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw previously, we achieved a test accuracy of 88% at the last epoch
    of our training session. Let''s have a look at what this really means, by interpreting
    the precision and recall scores of our classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc8a4fd2-d20e-4f13-91a9-525cdde053fb.png)'
  prefs: []
  type: TYPE_IMG
- en: As we noticed previously, the ratio of correctly predicted positive observations
    to the total number of positive observations in our test set (otherwise known
    as the **precision score**) is pretty high at 0.98\. The recall score is a bit
    lower and denotes the number of correctly predicted results divided by the number
    of results that should have been returned. Finally, the F-measure simply combines
    both the precision and recall scores as a harmonic mean.
  prefs: []
  type: TYPE_NORMAL
- en: To supplement our understanding, we plot out a confusion matrix of our classifier
    on the test set, as shown as follows. This is essentially an error matrix that
    lets us visualize how our model performed. The *x* axis denotes the predicted
    classes of our classifier, whereas the *y* axis denotes the actual classes of
    our test examples. As we see, our classifier falsely detects about 17 images where
    it thinks that the person is smiling, whereas they are in fact frowning (also
    known as a **false positive**).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, our classifier only makes one mistake in classifying a smiling
    face as a frowning face (also known as a **false negative**). Thinking of false
    positives and negatives helps us evaluate the utility of our classifier in real-world
    scenarios and lets us perform a cost benefit analysis of deploying such a system.
    In our case, this would not be necessary, given the subject matter of our classification
    task; however, other scenarios (such as skin cancer detection, for example) will
    require careful consideration and evaluation before using such learning systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20d88cab-a52e-4927-9bab-8febf0d5501a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Whenever you are satisfied with your models'' accuracy, you may save a model
    shown as follows. Note that this not only constitutes best practice, as it gives
    you well-documented records of your previous attempts, steps taken, and results
    achieved, but is also useful if you want to further probe the model, by peering
    into its intermediate layers to see what it has actually learned, as we will momentarily
    do:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e06e4e2d-c845-4935-8774-122927e3916e.png)'
  prefs: []
  type: TYPE_IMG
- en: The problem with detecting smiles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We must note at this point, that the problem of external validity (that is,
    the generalizability of our model) persists with a dataset like the smile detector.
    Given the restricted manner in which data has been collected, it would be unreasonable
    to expect our CNN to generalize well on other data. Firstly, the network is trained
    with low resolution input images. Moreover, it has only seen images of one smiling
    or frowning person in the same location each time. Feeding this network an image
    of, say, the managerial board of FIFA will not cause it to detect smiles however
    large and present they may be. We would need to readapt our approach. One way
    can be through applying the same transformations to the input image as done for
    the training data, by segmenting and resizing the input image per face. A better
    approach would be to gather more varied data and augment the training set by rotating
    and contorting the training examples, as we will see in later chapters. The key
    here is to include people smiling at different poses and orientations, with different
    lighting conditions, in your dataset, to truly capture all useful visual representations
    of a smile. If it is too expensive to collect more data, generating synthetic
    images (using the Keras image generator) can also be a viable option. The quality
    of your data can drastically improve the performance of your network. For now,
    we will explore some techniques to inform ourselves of the inner workings of a
    CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the black box
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is one thing to train a **smile detector** from a narrow dataset of similar
    images. Not only can you directly verify your predictions, but each false prediction
    doesn't exactly cost you a fortune. Now, if you were using a similar system to
    monitor behavioral responses of psychiatric patients (as a hypothetical example),
    you would probably want to ensure a high accuracy by ensuring that your model
    really understands what a smile means and is not picking up some irrelevant pattern
    in your training set. In the context of high-risk industries such as healthcare
    or energy, any misunderstandings can have disastrous consequences ranging from
    the loss of lives to the loss of resources. Hence, we want to be able to ensure
    that our deployed model has indeed picked up on truly predictive trends in the
    data and has not memorized some random features with no out-of-set predictivity.
    This has happened throughout the history of the use of neural networks. In the
    following section, we have selected a few tales from neural network folklore to
    illustrate these dilemmas.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network fails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once upon a time, the US Army had gotten the idea of using neural networks
    for automated detection of camouflaged enemy tanks. Researchers were commissioned
    to design and train a neural network to detect camouflaged tank images, from aerial
    photography of enemy locations. The researchers simply fine-tuned the model weights
    to reflect the correct output labels for each training example, and then tested
    the model on their secluded test examples. Luckily (or so it appeared), their
    network was able to classify all test images adequately, confirming to the researchers
    that their task had ended. Yet, soon enough, the researchers heard back from angry
    Pentagon officials claiming that the network they had handed over did no better
    than random chance at classifying camo-tanks. Confused, the researchers probed
    their training data and compared it to what the Pentagon had tested the network
    with. They discovered that the photos of camouflaged tanks used for training the
    network were all taken on cloudy days, whereas the negatives (photos without camo-tanks)
    were all taken on sunny days. The consequence of this was that their network had
    only learned to distinguish the weather (through brightness of the pixels) rather
    than approaching the intended classification task at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4494810-16c1-4784-9bed-19f1652ea8bc.png)'
  prefs: []
  type: TYPE_IMG
- en: It is quite often that neural networks, after a deluge of training iterations,
    achieve super-human accuracy on their training sets. For instance, one researcher
    observed such a phenomenon when they attempted to train a network to classify
    different types of land and sea mammals. Having achieved a great performance,
    the researchers tried to dig in further in order to decode any classification
    rules we humans might still ignore for this task. It turned out that a large part
    of what their sophisticated network had learned to do was with the presence or
    absence of blue pixels in an image, which naturally do not occur often on pictures
    with land mammals.
  prefs: []
  type: TYPE_NORMAL
- en: The last in our short selection of neural network failure tales is the case
    of the self-driving car that self-drove off a bridge. Confused automation engineers
    attempted to probe the trained network to understand what went wrong. To their
    surprise, they discovered something very curious. Instead of detecting the road
    on the street to follow, the network was, for some reason, relying on the continuous
    patches of green grass separating the road from the sidewalk for its orientation.
    When encountering the bridge, this patch of green grass disappeared, causing the
    network to behave in the seemingly unpredictable way it did.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing ConvNet learnings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These stories motivate our need to ensure that our models do not overfit on
    random noise, but actually capture representatively predictive features. We know
    how predictive inaccuracies can be introduced through careless data considerations,
    the inherent nature of your task, or inherent randomness in modeling. The conventionally
    popularized narrative on neural networks often includes terms such as **black
    box** to describe its learning mechanism. While understanding what individual
    neurons have learned may not be intuitive for all sorts of neural nets, this is
    hardly true for CNNs. Interestingly, ConvNets allow us to, literally, visualize
    their learned features. As we saw earlier, we can visualize neural activations
    for given input images. But we can do even more. In fact, there are a multitude
    of methods that have been developed in recent years to probe a CNN, to better
    understand what it has learned. While we do not have the time to cover all of
    these, we will be able to cover the most practically useful ones.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing neural activations of intermediate layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Firstly, we can visualize how progressive layers in our CNN transform the inputs
    they are fed, by looking at their activation maps. Recall that these are simply
    the reduced representations of the inputs that the network propagates through
    its architecture as it sees data. Visualizing the intermediate layers (convolutional
    or pooling) gives us an idea of the activation of neurons in our network at each
    stage as the input is broken down by the various learned filters. Since each two-dimensional
    activation map stores features extracted by a given filter, we must visualize
    these maps as two-dimensional images, where each image corresponds to a learned
    feature. This method is often referred to as visualizing intermediate activations.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to extract the learned features of our network, we will have to make
    some minor architectural tweaks to our model. This brings us to Keras's functional
    API. Recall that, previously, we were defining a sequential model using Keras's
    sequential API, which essentially let us sequentially stack layers of neurons
    to perform our classification tasks. These models ingested input tensors of images
    or word representations and spat out class probabilities assigned to each input.
    Now we will use the functional API, which allows building multi-output models,
    directed acyclic graphs, and even models with shared layers. We will use this
    API to peer into the depths of our convolutional network.
  prefs: []
  type: TYPE_NORMAL
- en: Predictions on an input image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First and foremost, we prepare an image (although you may use several) to be
    ingested by our multi-output model, so we are able to see the intermediate layer
    activations that occur as this image propagates through our new model. We take
    a random image from our test set for this purpose and prepare it as a four-dimensional
    tensor (with batch size 1, since we are only feeding our network a single image):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffc11494-ab65-4fe9-951f-de179a99ec72.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we initialize a multi-output model to make a prediction on our input image.
    The purpose of this is to capture the intermediate activations, per each layer
    of our network, so we can visually plot out the activation maps generated by different
    filters. This helps us understand which features our model has actually learned.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Keras's functional API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How exactly will we do this? Well we start by importing the `Model` class from
    the functional API. This lets us define a new model. The key difference in our
    new model is that this one is capable of giving us back multiple outputs, pertaining
    to the outputs of intermediate layers. This is achieved by using the layer outputs
    from a trained CNN (such as our smile detector) and feed it into this new multi-output
    model. Essentially, our multi-output model will take an input image and return
    filter-wise activations for each of the eight layers in our smile detector model
    that we previously trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also limit the number of layers to visualize through the list slicing
    notation used on `model.layers`, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bd743e5-6b33-4288-989d-9e3410362c5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The last line of the preceding code defines the activations variable, by making
    our multi-output model perform inference on the input image we fed it. This operation
    returns the multiple outputs corresponding to each layer of our CNN, now stored
    as a set of NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a828fd7-0c27-498c-a448-95966af6bd45.png)'
  prefs: []
  type: TYPE_IMG
- en: As you see, the activations variable stores a list of `8` NumPy *n*-dimensional
    arrays. Each of these `8` arrays represents a tensor output of a particular layer
    in our smile detector CNN. Each layer output represents the activations from the
    multiple filters used. Hence, we observe multiple activation maps per layer. These
    activation maps are essentially two-dimensional tensors that encode different
    features from the input image.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the number of channels per layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We saw that each layer has a depth that denoted the number of activation maps.
    These are also referred to as channels, where each channel contains an activation
    map, with a height and width of (*n* x *n*). Our first layer, for example, has
    16 different maps of size 64 x 64\. Similarly, the fourth layer has 16 activation
    maps of size 32 x 32\. The eighth layer has 32 activation maps, each of size 16
    x 16\. Each of these activation maps was generated by a specific filter from its
    respective layer, and are passed forward to subsequent layers to encode higher-level
    features. This will concur with our smile detector model''s architectural build,
    which we can always verify, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef30f0c5-299c-4aa7-a50a-0ae05e5ea016.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing activation maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now the fun part! We will plot out the activation maps for different filters
    in a given layer. Let''s start with the first layer. We can plot each of the 16
    activation maps out, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'While we will not show all 16 of these activation maps, here are a few interesting
    ones we found:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3096988d-4caa-4871-9e88-e462458fd77d.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can clearly see, each filter has captured distinct features from the input
    image, relating to the horizontal and vertical edges of the face, as well as the
    background of the image. As you visualize deeper activation maps of deeper layers,
    you will note that the activations become increasingly abstract in nature and
    less interpretable to the human eye. These activations are said to be encoding
    higher-level notions pertaining to a face's location and the eyes and ears within.
    You will also notice that the more and more activation maps remain blank the deeper
    you probe into the network. This means that fewer filters were activated in the
    deeper layers because the input image did not have the patterns corresponding
    to the ones encoded by the filters. This is quite common, as we would expect activation
    patterns to increasingly relate to the class of the image shown as it propagates
    through deeper layers of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding saliency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw earlier that the intermediate layers of our ConvNet seemed to encode
    some pretty clear detectors of face edges. It is harder to distinguish, however,
    whether our network understands what a smile actually is. You will notice in our
    smiling faces dataset that all pictures have been taken on the same background
    at the same approximate angle from the camera. Moreover, you will notice that
    the individuals in our dataset tend to smile as they lift their head up high and
    clear, yet mostly tilt their head downward while frowning. That's a lot of opportunity
    for our network to overfit on some irrelevant pattern. Hence, how do we actually
    know that our network understands that a smile has more to do with the movement
    of a person’s lips than it has to do with the angle at which someone’s face is
    tilted? As we saw in our neural network fails, it can happen quite often that
    the network picks up on irrelevant patterns. In this part of our experiments,
    we will visualize the **saliency maps** for the given network inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'First introduced in a paper by the visual geometry group at Oxford University,
    the idea behind saliency maps is to simply compute the gradient of a desired output
    category with respect to changes in the input image. Put differently, we are trying
    to determine how a small change in the pixel values of our image affects our network’s
    beliefs on what it is seeing for a given image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14ff46b8-8287-417c-b26b-7b84338678f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Intuitively, let''s suppose that we have trained a convolutional network on
    images of various animals: giraffes, leopards, dogs, cats, and many more. Then,
    to test what it has learned, we show it the image of a leopard and ask it, *Where
    do you think the leopard is in this image?* Technically speaking, we are ranking
    the pixels of our input image, based on the influence each of them has on the
    **class probability score** that our network spits out for this image. Then, we
    can simply visualize the pixels that had the greatest influence in classifying
    a given image, as these would be the pixels where positive changes lead to increasing
    our network''s class probability score, or confidence, that the given image pertains
    to a certain class.'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing saliency maps with ResNet50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To keep things interesting, we will conclude our smile detector experiments
    and actually use a pre-trained, very deep CNN to demonstrate our leopard example.
    We also use the Keras `vis`, which is a great higher-level toolkit to visualize
    and debug CNNs built on Keras. You can install this package using the `pip` package
    manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8b36506-43aa-459e-883f-c5573d6c10de.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we import the ResNet50 CNN architecture with pretrained weights for the
    ImageNet dataset. We encourage you to explore other models stored in Keras as
    well, accessible through `keras.applications`. We also switch out the Softmax activation
    for the linear activation function in the last layer of this network using `utils.apply_modifications`,
    which rebuilds the network graph to help us visualize the saliency of maps better.
  prefs: []
  type: TYPE_NORMAL
- en: ResNet50 was first introduced as the ILSVRC competition and won first place
    in 2015. It does very well at avoiding the accuracy degradation problem associated
    with very deep neural networks. It was trained on about a thousand output classes
    from the ImageNet dataset. It is considered a high-performing, state-of-the-art
    CNN architecture, made available for free by its creators. While it uses some
    interesting mechanics, known as **residual blocks**, we will refrain from commenting
    further on its architecture till later chapters. For now, let's see how we can
    use the pretrained weights of this model to visualize the saliency maps of a few
    leopard pictures.
  prefs: []
  type: TYPE_NORMAL
- en: Loading pictures from a local directory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you would like to follow along, simply google some nice leopard pictures
    and store them in a local directory. You can use the image loader from the `utils`
    module in Keras `vis` to resize your images to the target size that the ResNet50
    model accepts (that is, images of 224 x 224 pixels):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c5243eb-cb45-4ffc-9411-dc00ce627250.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we wish to make the experiment considerably arduous for our network,
    we purposefully selected pictures of camouflaged leopards to see how well this
    network does at detecting some of nature''s most intricate attempts to hide these
    predatory creatures from the sight of prey, such as ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b333364-2b72-4d3b-8452-67c0986b289d.png)'
  prefs: []
  type: TYPE_IMG
- en: Using Keras's visualization module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even our biological neural networks implemented throughout our visual cortex
    seem to have some difficulty finding the leopard in each image, at first glance.
    Let''s see how well its artificial counterpart does at this the task. In the following
    segment of code, we import the saliency visualizer object from the `keras-vis`
    module, as well as a utils tool that lets us search for layers by name. Note that
    this module does not come with the standard Keras install. However, it can be
    easily installed using the `pip` package manager on Python. You can even execute
    the install through your Jupyter environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Searching through layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we perform a utility search to define our last densely connected layer
    in the model. We want this layer as it outputs the class probability scores per
    output category, which we need to be able to visualize the saliency on the input
    image. The names of the layer can be found in the summary of the model (`model.summary()`).
    We will pass four specific arguments to the `visualize_salency()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcf27d3e-44ec-4237-bd72-d6bdac2afdd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This will return the gradients of our output with respect to our input, which
    intuitively inform us what pixels have the largest effect on our model''s prediction.
    The gradient variable stores six 224 x 224 images (corresponding to the input
    size for the ResNet50 architecture), one for each of the six input images of leopards.
    As we noted, these images are generated by the `visualize_salency` function, which
    takes four arguments as input:'
  prefs: []
  type: TYPE_NORMAL
- en: A seed input image to perform prediction on (`seed_input`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Keras CNN model (`model`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An identifier for the model's output layer (`layer_idx`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The index of the output class we want to visualize (`filter_indices`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The index reference we use here (288) refers to the index of the label *leopard* on
    the ImageNet dataset. Recall that earlier we imported pretrained layer weights
    for the currently initialized model. These weights were achieved by training the
    ResNet50 model on the ImageNet dataset. If you're curious about the different
    output classes, you can find them along with their respective indices, here: [https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualizing the saliency maps for the first three images, we can actually see
    that the network is paying attention to the locations where we find the leopard
    in the image—perfect! This is indeed what we want to see, as it denotes that our
    network really understands (roughly) where the leopard is located in our image,
    despite our best attempts at showing it noisy images of camouflaged leopards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bad43907-8885-42ab-bbd1-998bdea3b0ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probe all the layers in the network. What do you notice?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient weighted class activation mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another nifty gradient-based method is the **gradient weighted class activation
    map** (**Grad-CAM**). This is useful specifically if you have input images with
    entities belonging to several output classes and you want to visualize which areas
    in the input picture your network associates most with a specific output class.
    This technique leverages the class-specific gradient information flowing into
    the final convolutional layer of a CNN to produce a coarse localization map of
    the important regions in the image. In other words, we feed our network an input
    image and take the output activation map of a convolution layer by weighing every
    channel of the output (that is, the activation maps) by the gradient of the output
    class with respect to the channel. This allows us to better utilize the spatial
    information corresponding to what our network pays most attention to, represented
    in the last convolutional layer of the network. We can take these gradient weighted
    activation maps and overlay them on top of the input image to get an idea of which
    parts of our input the network associates highly with a given output class (that
    is, leopard).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing class activations with Keras-vis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this purpose, we use the `visualize_cam` function, which essentially generates
    a Grad-CAM that maximizes the layer activations for a given input, for a specified
    output class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `visualize_cam` function takes the same four arguments we saw earlier,
    plus an additional one. We pass it the arguments corresponding to a Keras model,
    a **seed input** image, a **filter index** corresponding to our output class (ImageNet
    index for leopard), as well as two model layers. One of these layers remains the
    fully connected dense output player, whereas the other layer refers to the final
    convolutional layer in the ResNet50 model. The method essentially leverages these
    two reference points to generate the gradient weighted class activation maps,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d831d8b4-2ef4-45c1-be17-9da168645641.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we see, the network correctly identifies the leopards in both images. Moreover,
    we notice that the network relies on the leopard''s black-dotted patterns for
    identification of its class. It stands to reason that the network uses this pattern
    to identify a leopard, as this is pretty distinctive of its class. We can see
    the network''s attention through the heatmap, which focuses mostly on the clear
    dotted regions of the leopard''s body and not necessarily the leopard''s face,
    as we might do ourselves if confronted with one. Perhaps millions of years of
    biological evolution have adapted the layer weights in the fusiform gyrus region
    of our brain to distinctively pick up on faces, as this was a pattern of consequence
    for our survival:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Paper on Grad-CAM**: [https://arxiv.org/pdf/1610.02391.pdf](https://arxiv.org/pdf/1610.02391.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the pretrained model for prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By the way, you may actually run an inference on a given image using the ResNet50
    architecture on pretrained ImageNet weights, as we have initialized here. You
    can do this by first preprocessing the desired image on which you want to run
    inference into the appropriate four-dimensional tensor format, as shown here.
    The same of course applies for any dataset of images you may have, as long as
    they are resized to the appropriate format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48cf2cc8-abb6-4b70-a71a-2e3035c84f80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding code reshapes one of our leopard images into a 4D tensor by expanding
    its dimension along the 0 axis, then feeds the tensor to our initialized ResNet50
    model to get a class probability prediction. We then proceed to decode the prediction
    class into a human-readable output. For fun, we also defined the `labels` variable,
    which includes all the possible labels our network predicted for this image, in
    descending order of probability. Let''s see which other labels our network attributes
    to our input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05b3700c-fb5e-4470-889c-cb4136f98d50.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing maximal activations per output class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the final method, we simply visualize the overall activations associated
    with a particular output class, without explicitly passing our model an input
    image. This method can be very intuitive, while being quite aesthetically pleasing.
    For the purpose of our last experiment, we import yet another pretrained model,
    **the VGG16 network**. This network is another deep architecture based on the
    model that won the ImageNet classification challenge in 2014\. Similar to our
    last example, we switch out the Softmax activation of our last layer with a linear
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eefcd9de-8c03-48b9-89f6-4a65d52ac5ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we simply import the activation visualizer object from the visualization
    module implemented in `keras-vis`. We plot out the overall activations for the
    leopard class, by passing the `visualize_activation` function our model, the output
    layer, and the index corresponding to our output class, leopard. As we see here,
    the network has actually captured the general shape of leopards at different orientations
    and locations in the image. Some appear zoomed-in, others are far less distinct,
    yet cat-like ears and the dotted-black pattern are quite distinguishable throughout
    the image—neat, right? Let''s have a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a32de60-9ffb-4348-92f2-68bc963acdde.png)'
  prefs: []
  type: TYPE_IMG
- en: Converging a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, you can make the model converge on this output class to visualize what
    the model thinks is a leopard (or another output class) after many iterations
    of convergence. You can define how long you want your model to converge through
    the `max_iter` argument, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f916cf1-b7e6-4352-95e8-7ab7078977a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Using multiple filter indices to hallucinate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can also play around by passing the `filter_indices` parameter different
    indices corresponding to different output classes from the ImageNet dataset. You
    could also pass it a list of two integers, corresponding to two different output
    classes. This basically lets your neural network *imagine* visual combinations
    of two separate output classes by simultaneously visualizing the activations pertaining
    to both output classes. These can at times turn out to be very interesting, so
    let both your imaginations run wild! It is noteworthy that Google''s DeepDream
    leverages similar concepts, showing how overexcited activation maps can be superimposed
    over input images to generate artistic patterns and images. The intricacy of these
    patterns is at times remarkable and awe-inspiring:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8dc35402-607f-4839-bd40-119114172faf.png)'
  prefs: []
  type: TYPE_IMG
- en: Picture of the author of this book, taken in front of the haunted mansion in
    Disneyland, Paris. The image has been processed using the open source DeepDream
    generator, which we encourage you to play around with, not just to marvel at its
    beauty. It can also generate quite handy gifts for artistic relatives, around
    the holiday season.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many may claim that the hierarchically nested pattern recognition technique
    leveraged by CNNs very much resembles the functioning of our own visual cortex.
    This may be true at a certain level. However, the visual cortex implements a much
    more complex architecture and makes it run efficiently on about 10 watts of energy.
    Our visual cortex also does not easily get fooled by images where face-like features
    appear, (although this phenomenon occurs often enough to have secured its proper
    term in modern neuroscience. **Pareidolia** is a term associated with the human
    mind interpreting signals in a manner to generate higher-level concepts, where
    none actually exists. Scientists have shown how this phenomenon is related to
    the earlier activation of neurons located in the fusiform gyrus area of the visual
    cortex, responsible for several visual recognition and classification tasks. In
    cases of pareidolia, these neurons *jump the gun,* as it were, and cause us to
    detect faces or hear sounds, even when this is really not the case. The famous
    picture of the Martian surface illustrates this point, as most of us can clearly
    make out the outlines and features of a face, whereas all the picture actually
    contains is a pile of red dust:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8345fe93-9fd7-497e-9c8f-5ea27130aa43.png)'
  prefs: []
  type: TYPE_IMG
- en: Image Courtesy NASA/JPL-Caltech
  prefs: []
  type: TYPE_NORMAL
- en: Neural network pareidolia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This problem is naturally not unique to our biological brains. In fact, despite
    the excellent functioning of CNNs for many visual tasks, this problem of neural
    network pareidolia is one that computer vision researchers are always trying to
    solve. As we noted, CNNs learn to classify images through learning an assortment
    of filters that pick up useful features, capable of breaking down the input image
    in a probabilistic manner. However, the features learned by these filters do not
    represent all the information present in a given image. The orientation of these
    features, with respect to one another, matters just as much! The presence of two
    eyes, lips, and a nose does not inherently constitute the essence of a face. Rather,
    it''s the spatial arrangement of these elements within an image that makes the
    face in question:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7bb30e2-2ccf-4512-aec5-32d92c82758e.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, firstly, we used convolutional layers that are capable of
    decomposing a given visual input space into hierarchically nested probabilistic
    activations of convolution filters that subsequently connect to dense neurons
    that perform classification. The filters in these convolutional layers learn weights
    corresponding to useful representations that may be queried in a probabilistic
    manner to map the set of input features present in a dataset to the respective
    output classes. Furthermore, we saw how we can dive deep into our convolution
    network to understand what it has learned. We saw four specific ways to do this:
    intermediate activation-based, saliency-based, gradient weighted class activations,
    and activation maximization visualizations. Each gives a unique intuition into
    which patterns are picked up by the different layers of our network. We visualized
    these patterns for given images, as well as for entire output classes, to intuitively
    understand which elements of our network pays attention while performing inference.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, although we reviewed a lot of neuroscience-based inspirations that
    led to the development of the CNN architecture, modern CNNs in no way compete
    with the intricate mechanisms implemented throughout the mammalian visual cortex.
    In fact, many of the structural design of the layers in the visual cortex do not
    even remotely resemble what we have designed here so far. For instance, the layers
    of the visual cortex are themselves structured into subsequent cortical columns,
    containing neurons with supposedly non-overlapping receptive fields, the purpose
    of which is unbeknown to modern neuroscience. Even our retina performs a deluge
    of sensory preprocessing through the use of rod cells (receptive to low-intensity
    light), cone cells (receptive to high-intensity light), and ipRGC cells (receptive
    to time-dependent stimuli) before sending the visual signals in the form of electrical
    impulses to the lateral geniculate nucleus at the base of the thalamus, otherwise
    known as the relay center for visual signals. It is from here that the signals
    begin their journey, propagating back and forth through the six densely interconnected
    (and not convolutional) layers of the visual cortex, as we go about our lives.
    In essence, human vision is quite sequential and dynamic, much different than
    the artificial implementation of it. In summation, while we are far from endowing
    visual intelligence to machines in the manner biology has done to us, CNNs represent
    the pinnacle of modern achievements in computer vision, making it a venerably
    adaptable architecture for countless machine vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we conclude our chapter on the exploration of CNNs. We will revisit more
    complex architectures in later chapters and experiment with data augmentation
    techniques and more complex computer vision tasks. In the next chapter, we will
    explore another neural network architecture known as RNN, which is especially
    useful for capturing and modeling sequential information such as time variant
    data, common in many fields ranging from industrial engineering to natural language
    dialogue generation.
  prefs: []
  type: TYPE_NORMAL
