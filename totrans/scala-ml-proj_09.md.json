["```py\n<dependencies>\n   <dependency>\n      <groupId>ai.h2o</groupId>\n      <artifactId>sparkling-water-core_2.11</artifactId>\n      <version>2.2.2</version>\n   </dependency>\n   <dependency>\n      <groupId>org.vegas-viz</groupId>\n      <artifactId>vegas_2.11</artifactId>\n      <version>0.3.11</version>\n   </dependency>\n   <dependency>\n     <groupId>org.vegas-viz</groupId>\n     <artifactId>vegas-spark_2.11</artifactId>\n     <version>0.3.11</version>\n     </dependency>\n</dependencies>\n```", "```py\npackage com.packt.ScalaML.FraudDetection\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.h2o._\nimport _root_.hex.FrameSplitter\nimport water.Key\nimport water.fvec.Frame\nimport _root_.hex.deeplearning.DeepLearning\nimport _root_.hex.deeplearning.DeepLearningModel.DeepLearningParameters\nimport _root_.hex.deeplearning.DeepLearningModel.DeepLearningParameters.Activation\nimport java.io.File\nimport water.support.ModelSerializationSupport\nimport _root_.hex.{ ModelMetricsBinomial, ModelMetrics }\nimport org.apache.spark.h2o._\nimport scala.reflect.api.materializeTypeTag\nimport water.support.ModelSerializationSupport\nimport water.support.ModelMetricsSupport\nimport _root_.hex.deeplearning.DeepLearningModel\nimport vegas._\nimport vegas.sparkExt._\nimport org.apache.spark.sql.types._\n```", "```py\nval spark = SparkSession\n        .builder\n        .master(\"local[*]\")\n        .config(\"spark.sql.warehouse.dir\", \"tmp/\")\n        .appName(\"Fraud Detection\")\n        .getOrCreate()\n```", "```py\nimplicit val sqlContext = spark.sqlContext\nimport sqlContext.implicits._\nval h2oContext = H2OContext.getOrCreate(spark)\nimport h2oContext._\nimport h2oContext.implicits._\n```", "```py\nval inputCSV = \"data/creditcard.csv\"\n\nval transactions = spark.read.format(\"com.databricks.spark.csv\")\n        .option(\"header\", \"true\")\n        .option(\"inferSchema\", true)\n        .load(inputCSV)\n```", "```py\nval distribution = transactions.groupBy(\"Class\").count.collect\nVegas(\"Class Distribution\").withData(distribution.map(r => Map(\"class\" -> r(0), \"count\" -> r(1)))).encodeX(\"class\", Nom).encodeY(\"count\", Quant).mark(Bar).show\n>>>\n```", "```py\nval daysUDf = udf((s: Double) => \nif (s > 3600 * 24) \"day2\" \nelse \"day1\")\n\nval t1 = transactions.withColumn(\"day\", daysUDf(col(\"Time\")))\nval dayDist = t1.groupBy(\"day\").count.collect\n```", "```py\nVegas(\"Day Distribution\").withData(dayDist.map(r => Map(\"day\" -> r(0), \"count\" -> r(1)))).encodeX(\"day\", Nom).encodeY(\"count\", Quant).mark(Bar).show\n>>>\n```", "```py\nval dayTimeUDf = udf((day: String, t: Double) => if (day == \"day2\") t - 86400 else t)\nval t2 = t1.withColumn(\"dayTime\", dayTimeUDf(col(\"day\"), col(\"Time\")))\n\nt2.describe(\"dayTime\").show()\n>>>\n+-------+------------------+\n|summary| dayTime |\n+-------+------------------+\n| count| 284807|\n| mean| 52336.926072744|\n| stddev|21049.288810608432|\n| min| 0.0|\n| max| 86400.0|\n+-------+------------------+\n```", "```py\n\nval d1 = t2.filter($\"day\" === \"day1\")\nval d2 = t2.filter($\"day\" === \"day2\")\nval quantiles1 = d1.stat.approxQuantile(\"dayTime\", Array(0.25, 0.5, 0.75), 0)\n\nval quantiles2 = d2.stat.approxQuantile(\"dayTime\", Array(0.25, 0.5, 0.75), 0)\n\nval bagsUDf = udf((t: Double) => \n if (t <= (quantiles1(0) + quantiles2(0)) / 2) \"gr1\" \n elseif (t <= (quantiles1(1) + quantiles2(1)) / 2) \"gr2\" \n elseif (t <= (quantiles1(2) + quantiles2(2)) / 2) \"gr3\" \n else \"gr4\")\n\nval t3 = t2.drop(col(\"Time\")).withColumn(\"Time\", bagsUDf(col(\"dayTime\")))\n```", "```py\nval grDist = t3.groupBy(\"Time\", \"class\").count.collect\nval grDistByClass = grDist.groupBy(_(1))\n```", "```py\nVegas(\"gr Distribution\").withData(grDistByClass.get(0).get.map(r => Map(\"Time\" -> r(0), \"count\" -> r(2)))).encodeX(\"Time\", Nom).encodeY(\"count\", Quant).mark(Bar).show\n>>>\n```", "```py\nVegas(\"gr Distribution\").withData(grDistByClass.get(1).get.map(r => Map(\"Time\" -> r(0), \"count\" -> r(2)))).encodeX(\"Time\", Nom).encodeY(\"count\", Quant).mark(Bar).show\n>>>\n```", "```py\nval c0Amount = t3.filter($\"Class\" === \"0\").select(\"Amount\")\nval c1Amount = t3.filter($\"Class\" === \"1\").select(\"Amount\")\n\nprintln(c0Amount.stat.approxQuantile(\"Amount\", Array(0.25, 0.5, 0.75), 0).mkString(\",\"))\n\nVegas(\"Amounts for class 0\").withDataFrame(c0Amount).mark(Bar).encodeX(\"Amount\", Quantitative, bin = Bin(50.0)).encodeY(field = \"*\", Quantitative, aggregate = AggOps.Count).show\n>>>\n```", "```py\nVegas(\"Amounts for class 1\").withDataFrame(c1Amount).mark(Bar).encodeX(\"Amount\", Quantitative, bin = Bin(50.0)).encodeY(field = \"*\", Quantitative, aggregate = AggOps.Count).show\n>>>\n```", "```py\nval t4 = t3.drop(\"day\").drop(\"dayTime\")\n```", "```py\nval creditcard_hf: H2OFrame = h2oContext.asH2OFrame(t4.orderBy(rand()))\n```", "```py\nval sf = new FrameSplitter(creditcard_hf, Array(.4, .4), \n                Array(\"train_unsupervised\", \"train_supervised\", \"test\")\n                .map(Key.make[Frame](_)), null)\n\nwater.H2O.submitTask(sf)\nval splits = sf.getResult\nval (train_unsupervised, train_supervised, test) = (splits(0), splits(1), splits(2))\n```", "```py\ntoCategorical(train_unsupervised, 30)\ntoCategorical(train_supervised, 30)\ntoCategorical(test, 30)\n```", "```py\nval response = \"Class\"\nval features = train_unsupervised.names.filterNot(_ == response)\n```", "```py\nvar dlParams = new DeepLearningParameters()\n    dlParams._ignored_columns = Array(response))// since unsupervised, we ignore the label\n    dlParams._train = train_unsupervised._key // use the train_unsupervised frame for training\n    dlParams._autoencoder = true // use H2O built-in autoencoder    dlParams._reproducible = true // ensure reproducibility    dlParams._seed = 42 // random seed for reproducibility\n    dlParams._hidden = Array[Int](10, 2, 10)\n    dlParams._epochs = 100 // number of training epochs\n    dlParams._activation = Activation.Tanh // Tanh as an activation function\n    dlParams._force_load_balance = false var dl = new DeepLearning(dlParams)\nval model_nn = dl.trainModel.get\n```", "```py\nval uri = new File(new File(inputCSV).getParentFile, \"model_nn.bin\").toURI ModelSerializationSupport.exportH2OModel(model_nn, uri)\n```", "```py\nval model: DeepLearningModel = ModelSerializationSupport.loadH2OModel(uri)\n```", "```py\nprintln(model)\n>>>\n```", "```py\nvar train_features = model_nn.scoreDeepFeatures(train_unsupervised, 1) \ntrain_features.add(\"Class\", train_unsupervised.vec(\"Class\"))\n```", "```py\ntrain_features.setNames(train_features.names.map(_.replaceAll(\"[.]\", \"-\")))\ntrain_features._key = Key.make()\nwater.DKV.put(train_features)\n\nval tfDataFrame = asDataFrame(train_features) Vegas(\"Compressed\").withDataFrame(tfDataFrame).mark(Point).encodeX(\"DF-L2-C1\", Quantitative).encodeY(\"DF-L2-C2\", Quantitative).encodeColor(field = \"Class\", dataType = Nominal).show\n>>>\n```", "```py\ntrain_features = model_nn.scoreDeepFeatures(train_unsupervised, 2)\ntrain_features._key = Key.make()\ntrain_features.add(\"Class\", train_unsupervised.vec(\"Class\"))\nwater.DKV.put(train_features)\n\nval features_dim = train_features.names.filterNot(_ == response)\nval train_features_H2O = asH2OFrame(train_features)\n```", "```py\ndlParams = new DeepLearningParameters()\n        dlParams._ignored_columns = Array(response)\n        dlParams._train = train_features_H2O\n        dlParams._autoencoder = true\n        dlParams._reproducible = true\n        dlParams._ignore_const_cols = false\n        dlParams._seed = 42\n        dlParams._hidden = Array[Int](10, 2, 10)\n        dlParams._epochs = 100\n        dlParams._activation = Activation.Tanh\n        dlParams._force_load_balance = false dl = new DeepLearning(dlParams)\nval model_nn_dim = dl.trainModel.get\n```", "```py\nModelSerializationSupport.exportH2OModel(model_nn_dim, new File(new File(inputCSV).getParentFile, \"model_nn_dim.bin\").toURI)\n```", "```py\nval test_dim = model_nn.scoreDeepFeatures(test, 2)\nval test_dim_score = model_nn_dim.scoreAutoEncoder(test_dim, Key.make(), false)\n\nval result = confusionMat(test_dim_score, test, test_dim_score.anyVec.mean)\nprintln(result.deep.mkString(\"n\"))\n>>>\nArray(38767, 29)\nArray(18103, 64)\n```", "```py\ntest_dim_score.add(\"Class\", test.vec(\"Class\"))\nval testDF = asDataFrame(test_dim_score).rdd.zipWithIndex.map(r => Row.fromSeq(r._1.toSeq :+ r._2))\n\nval schema = StructType(Array(StructField(\"Reconstruction-MSE\", DoubleType, nullable = false), StructField(\"Class\", ByteType, nullable = false), StructField(\"idRow\", LongType, nullable = false)))\n\nval dffd = spark.createDataFrame(testDF, schema)\ndffd.show()\n>>>\n```", "```py\nVegas(\"Reduced Test\", width = 800, height = 600).withDataFrame(dffd).mark(Point).encodeX(\"idRow\", Quantitative).encodeY(\"Reconstruction-MSE\", Quantitative).encodeColor(field = \"Class\", dataType = Nominal).show\n>>>\n```", "```py\ntoCategorical(train_supervised, 29)\n```", "```py\nval train_supervised_H2O = asH2OFrame(train_supervised)\n        dlParams = new DeepLearningParameters()\n        dlParams._pretrained_autoencoder = model_nn._key\n        dlParams._train = train_supervised_H2O\n        dlParams._reproducible = true\n        dlParams._ignore_const_cols = false\n        dlParams._seed = 42\n        dlParams._hidden = Array[Int](10, 2, 10)\n        dlParams._epochs = 100\n        dlParams._activation = Activation.Tanh\n        dlParams._response_column = \"Class\"\n        dlParams._balance_classes = true dl = new DeepLearning(dlParams)\nval model_nn_2 = dl.trainModel.get\n```", "```py\nval predictions = model_nn_2.score(test, \"predict\")\ntest.add(\"predict\", predictions.vec(\"predict\"))\nasDataFrame(test).groupBy(\"Class\", \"predict\").count.show //print\n>>>\n+-----+-------+-----+\n|Class|predict|count|\n+-----+-------+-----+\n| 1| 0| 19|\n| 0| 1| 57|\n| 0| 0|56804|\n| 1| 1| 83|\n+-----+-------+-----+\n```", "```py\nVegas().withDataFrame(asDataFrame(test)).mark(Bar).encodeY(field = \"*\", dataType = Quantitative, AggOps.Count, axis = Axis(title = \"\", format = \".2f\"), hideAxis = true).encodeX(\"Class\", Ord).encodeColor(\"predict\", Nominal, scale = Scale(rangeNominals = List(\"#EA98D2\", \"#659CCA\"))).configMark(stacked = StackOffset.Normalize).show\n>>>\n```", "```py\nval trainMetrics = ModelMetricsSupport.modelMetrics[ModelMetricsBinomial](model_nn_2, test)\nval auc = trainMetrics._auc\nval metrics = auc._tps.zip(auc._fps).zipWithIndex.map(x => x match { case ((a, b), c) => (a, b, c) })\n\nval fullmetrics = metrics.map(_ match { case (a, b, c) => (a, b, auc.tn(c), auc.fn(c)) })\nval precisions = fullmetrics.map(_ match { case (tp, fp, tn, fn) => tp / (tp + fp) })\n\nval recalls = fullmetrics.map(_ match { case (tp, fp, tn, fn) => tp / (tp + fn) })\nval rows = for (i <- 0 until recalls.length) yield r(precisions(i), recalls(i))\nval precision_recall = rows.toDF()\n```", "```py\nVegas(\"ROC\", width = 800, height = 600).withDataFrame(precision_recall).mark(Line).encodeX(\"recall\", Quantitative).encodeY(\"precision\", Quantitative).show\n>>>\n```", "```py\nval sensitivity = fullmetrics.map(_ \n match { \n case (tp, fp, tn, fn) => tp / (tp + fn) })\n val specificity = fullmetrics.map(_ \n match { \n case (tp, fp, tn, fn) => tn / (tn + fp) })\n val rows2 = \n for (i <- 0 until specificity.length) \n yield r2(sensitivity(i), specificity(i))\n\nval sensitivity_specificity = rows2.toDF\nVegas(\"sensitivity_specificity\", width = 800, height = 600).withDataFrame(sensitivity_specificity).mark(Line).encodeX(\"specificity\", Quantitative).encodeY(\"sensitivity\", Quantitative).show\n>>>\n```", "```py\nval withTh = auc._tps.zip(auc._fps)\n            .zipWithIndex\n            .map(x => x match { case ((a, b), c) \n            => (a, b, auc.tn(c), auc.fn(c), auc._ths(c)) })\nval rows3 = for (i <- 0 until withTh.length) yield r3(withTh(i)._1, withTh(i)._2, withTh(i)._3, withTh(i)._4, withTh(i)._5)\n```", "```py\nVegas(\"tp\", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).encodeX(\"th\", Quantitative).encodeY(\"tp\", Quantitative).show\n>>>\n```", "```py\nVegas(\"fp\", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).encodeX(\"th\", Quantitative).encodeY(\"fp\", Quantitative).show\n>>>\n```", "```py\nVegas(\"fp\", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).filter(\"datum.th > 0.01\").encodeX(\"th\", Quantitative).encodeY(\"fp\", Quantitative).show\n>>>\n```", "```py\nVegas(\"tn\", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).encodeX(\"th\", Quantitative).encodeY(\"tn\", Quantitative).show\n>>>\n```", "```py\nVegas(\"fn\", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).encodeX(\"th\", Quantitative).encodeY(\"fn\", Quantitative).show\n>>>\n```", "```py\nh2oContext.stop(stopSparkContext = true)\nspark.stop()\n```", "```py\ndef toCategorical(f: Frame, i: Int): Unit = {\n    f.replace(i, f.vec(i).toCategoricalVec)\n    f.update()\n    }\n```", "```py\ndef confusionMat(mSEs:water.fvec.Frame,actualFrame:water.fvec.Frame,thresh: Double):Array[Array[Int]] = {\n val actualColumn = actualFrame.vec(\"Class\");\n val l2_test = mSEs.anyVec();\n val result = Array.ofDim[Int](2, 2)\n var i = 0\n var ii, jj = 0\n\n for (i <- 0 until l2_test.length().toInt) {\n        ii = if (l2_test.at(i) > thresh) 1 else 0;\n        jj = actualColumn.at(i).toInt\n        result(ii)(jj) = result(ii)(jj) + 1\n        }\n    result\n    }\n```", "```py\ncaseclass r(precision: Double, recall: Double)\ncaseclass r2(sensitivity: Double, specificity: Double)\ncaseclass r3(tp: Double, fp: Double, tn: Double, fn: Double, th: Double)\n```"]