["```py\n<properties>\n      <scala.version>2.11.8</scala.version>\n      <spark.version>2.2.1</spark.version>\n      <dl4j.version>0.9.1</dl4j.version>\n      <datavec.spark.version>0.9.1_spark_2</datavec.spark.version>\n  </properties>\n\n  <dependencies>\n    <dependency>\n        <groupId>org.scala-lang</groupId>\n        <artifactId>scala-library</artifactId>\n       <version>${scala.version}</version>\n    </dependency>\n\n    <dependency>\n        <groupId>org.apache.spark</groupId>\n        <artifactId>spark-core_2.11</artifactId>\n        <version>${spark.version}</version>\n    </dependency>\n\n    <dependency>\n        <groupId>org.datavec</groupId>\n        <artifactId>datavec-api</artifactId>\n        <version>${dl4j.version}</version>\n    </dependency>\n\n    <dependency>\n        <groupId>org.datavec</groupId>\n        <artifactId>datavec-spark_2.11</artifactId>\n        <version>${datavec.spark.version}</version>\n    </dependency>\n   </dependencies>\n```", "```py\nval inputDataSchema = new Schema.Builder()\n        .addColumnString(\"DateTimeString\")\n        .addColumnsString(\"CustomerID\", \"MerchantID\")\n        .addColumnInteger(\"NumItemsInTransaction\")\n        .addColumnCategorical(\"MerchantCountryCode\", List(\"USA\", \"CAN\", \"FR\", \"MX\").asJava)\n         .addColumnDouble(\"TransactionAmountUSD\", 0.0, null, false, false) //$0.0 or more, no maximum limit, no NaN and no Infinite values\n        .addColumnCategorical(\"FraudLabel\", List(\"Fraud\", \"Legit\").asJava)\n        .build\n```", "```py\nval tp = new TransformProcess.Builder(inputDataSchema)\n        .removeColumns(\"CustomerID\", \"MerchantID\")\n        .build \n```", "```py\n.filter(new ConditionFilter(\n          new CategoricalColumnCondition(\"MerchantCountryCode\", ConditionOp.NotInSet, new HashSet(Arrays.asList(\"USA\",\"CAN\")))))\n```", "```py\nval conf = new SparkConf\nconf.setMaster(args[0])\nconf.setAppName(\"DataVec Example\")\n\nval sc = new JavaSparkContext(conf)\n```", "```py\nval directory = new ClassPathResource(\"datavec-example-data.csv\").getFile.getAbsolutePath\nval stringData = sc.textFile(directory)\n\nval rr = new CSVRecordReader\nval parsedInputData = stringData.map(new StringToWritablesFunction(rr))\n```", "```py\nval processedData = SparkTransformExecutor.execute(parsedInputData, tp)\n```", "```py\nval processedAsString = processedData.map(new WritablesToStringFunction(\",\"))\nval processedCollected = processedAsString.collect\nval inputDataCollected = stringData.collect\n```", "```py\nmysql> DESCRIBE sparkexample;\n+-----------------------+-------------+------+-----+---------+-------+\n| Field                 | Type        | Null | Key | Default | Extra |\n+-----------------------+-------------+------+-----+---------+-------+\n| DateTimeString        | varchar(23) | YES  |     | NULL    |       |\n| CustomerID            | varchar(10) | YES  |     | NULL    |       |\n| MerchantID            | varchar(10) | YES  |     | NULL    |       |\n| NumItemsInTransaction | int(11)     | YES  |     | NULL    |       |\n| MerchantCountryCode   | varchar(3)  | YES  |     | NULL    |       |\n| TransactionAmountUSD  | float       | YES  |     | NULL    |       |\n| FraudLabel            | varchar(5)  | YES  |     | NULL    |       |\n+-----------------------+-------------+------+-----+---------+-------+\n7 rows in set (0.00 sec)\n```", "```py\nmysql> select * from sparkexample;\n+-------------------------+------------+------------+-----------------------+---------------------+----------------------+------------+\n| DateTimeString          | CustomerID | MerchantID | NumItemsInTransaction | MerchantCountryCode | TransactionAmountUSD | FraudLabel |\n+-------------------------+------------+------------+-----------------------+---------------------+----------------------+------------+\n| 2016-01-01 17:00:00.000 | 830a7u3    | u323fy8902 |                     1 | USA                 |                  100 | Legit      |\n| 2016-01-01 18:03:01.256 | 830a7u3    | 9732498oeu |                     3 | FR                  |                 73.2 | Legit      |\n|...                      |            |            |                       |                     |                      |            |\n```", "```py\nvar jdbcUsername = \"root\"\n  var jdbcPassword = \"secretpw\"\n\n  val jdbcHostname = \"mysqlhost\"\n  val jdbcPort = 3306\n  val jdbcDatabase =\"sparkdb\"\n  val jdbcUrl = s\"jdbc:mysql://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}\"\n```", "```py\nClass.forName(\"com.mysql.jdbc.Driver\")\n```", "```py\nval spark = SparkSession\n        .builder()\n        .master(\"local[*]\")\n        .appName(\"Spark MySQL basic example\")\n        .getOrCreate()\n```", "```py\nimport spark.implicits._\n```", "```py\nval jdbcDF = spark.read\n        .format(\"jdbc\")\n        .option(\"url\", jdbcUrl)\n        .option(\"dbtable\", s\"${jdbcDatabase}.sparkexample\")\n        .option(\"user\", jdbcUsername)\n        .option(\"password\", jdbcPassword)\n        .load()\n```", "```py\njdbcDF.printSchema()\n```", "```py\nroot\n |-- DateTimeString: string (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- MerchantID: string (nullable = true)\n |-- NumItemsInTransaction: integer (nullable = true)\n |-- MerchantCountryCode: string (nullable = true)\n |-- TransactionAmountUSD: double (nullable = true)\n |-- FraudLabel: string (nullable = true)\n```", "```py\njdbcDF.select(\"MerchantCountryCode\", \"TransactionAmountUSD\").groupBy(\"MerchantCountryCode\").avg(\"TransactionAmountUSD\")\n```", "```py\nval jdbcDF = spark.read\n        .format(\"jdbc\")\n        .option(\"url\", jdbcUrl)\n        .option(\"dbtable\", s\"${jdbcDatabase}.employees\")\n        .option(\"user\", jdbcUsername)\n        .option(\"password\", jdbcPassword)\n        .option(\"columnName\", \"employeeID\")\n        .option(\"lowerBound\", 1L)\n        .option(\"upperBound\", 100000L)\n        .option(\"numPartitions\", 100)\n        .load()\n```", "```py\n/* 1 */\n{\n    \"_id\" : ObjectId(\"5ae39eed144dfae14837c625\"),\n    \"DateTimeString\" : \"2016-01-01 17:00:00.000\",\n    \"CustomerID\" : \"830a7u3\",\n    \"MerchantID\" : \"u323fy8902\",\n    \"NumItemsInTransaction\" : 1,\n    \"MerchantCountryCode\" : \"USA\",\n    \"TransactionAmountUSD\" : 100.0,\n    \"FraudLabel\" : \"Legit\"\n}\n\n/* 2 */\n{\n    \"_id\" : ObjectId(\"5ae3a15d144dfae14837c671\"),\n    \"DateTimeString\" : \"2016-01-01 18:03:01.256\",\n    \"CustomerID\" : \"830a7u3\",\n    \"MerchantID\" : \"9732498oeu\",\n    \"NumItemsInTransaction\" : 3,\n    \"MerchantCountryCode\" : \"FR\",\n    \"TransactionAmountUSD\" : 73.0,\n    \"FraudLabel\" : \"Legit\"\n}\n...\n```", "```py\nval sparkSession = SparkSession.builder()\n      .master(\"local\")\n      .appName(\"MongoSparkConnectorIntro\")\n      .config(\"spark.mongodb.input.uri\", \"mongodb://mdbhost:27017/sparkmdb.sparkexample\")\n      .config(\"spark.mongodb.output.uri\", \"mongodb://mdbhost:27017/sparkmdb.sparkexample\")\n      .getOrCreate()\n```", "```py\nval df = MongoSpark.load(sparkSession)\n```", "```py\ndf.printSchema()\n```", "```py\ndf.collect.foreach { println }\n```", "```py\n[830a7u3,2016-01-01 17:00:00.000,Legit,USA,u323fy8902,1,100.0,[5ae39eed144dfae14837c625]]\n[830a7u3,2016-01-01 18:03:01.256,Legit,FR,9732498oeu,3,73.0,[5ae3a15d144dfae14837c671]]\n...\n```", "```py\ncase class Transaction(CustomerID: String,\n                      MerchantID: String,\n                      MerchantCountryCode: String,\n                      DateTimeString: String,\n                      NumItemsInTransaction: Int,\n                      TransactionAmountUSD: Double,\n                      FraudLabel: String)\n```", "```py\nval transactions = MongoSpark.load[Transaction](sparkSession)\n```", "```py\ntransactions.createOrReplaceTempView(\"transactions\")\n```", "```py\nval filteredTransactions = sparkSession.sql(\"SELECT CustomerID, MerchantID FROM transactions WHERE TransactionAmountUSD = 100\")\n```", "```py\nfilteredTransactions.show\n```", "```py\n+----------+----------+\n|CustomerID|MerchantID|\n+----------+----------+\n|   830a7u3|u323fy8902|\n+----------+----------+\n```", "```py\ngroupId: com.amazonaws\n artifactId: aws-java-sdk-core\n version1.11.234\n\n groupId: com.amazonaws\n artifactId: aws-java-sdk-s3\n version1.11.234\n\n groupId: org.apache.hadoop\n artifactId: hadoop-aws\n version: 3.1.1\n```", "```py\nval sparkSession = SparkSession\n    .builder\n    .master(master)\n    .appName(\"Spark Minio Example\")\n    .getOrCreate\n```", "```py\nsparkSession.sparkContext.setLogLevel(\"WARN\")\n```", "```py\nsparkSession.sparkContext.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"http://<host>:<port>\")\nsparkSession.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", \"access_key\")\nsparkSession.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"secret\")\nsparkSession.sparkContext.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\")\nsparkSession.sparkContext.hadoopConfiguration.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\nsparkSession.sparkContext.hadoopConfiguration.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n```", "```py\nval logDataRdd = sparkSession.sparkContext.textFile(\"s3a://dl4j-bucket/access_log\")\nprintln(\"RDD size is \" + logDataRdd.count)\n```", "```py\nimport sparkSession.implicits._\nval logDataDf = logDataRdd.toDF\nlogDataDf.show(10, false)\n```", "```py\n64.242.88.10 - - [07/Mar/2004:16:05:49 -0800] \"GET /twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables HTTP/1.1\" 401 12846\n64.242.88.10 - - [07/Mar/2004:16:06:51 -0800] \"GET /twiki/bin/rdiff/TWiki/NewUserTemplate?rev1=1.3&rev2=1.2 HTTP/1.1\" 200 4523\n64.242.88.10 - - [07/Mar/2004:16:10:02 -0800] \"GET /mailman/listinfo/hsdivision HTTP/1.1\" 200 6291\n64.242.88.10 - - [07/Mar/2004:16:11:58 -0800] \"GET /twiki/bin/view/TWiki/WikiSyntax HTTP/1.1\" 200 7352\n```", "```py\nval schema = new Schema.Builder()\n      .addColumnString(\"host\")\n      .addColumnString(\"timestamp\")\n      .addColumnString(\"request\")\n      .addColumnInteger(\"httpReplyCode\")\n      .addColumnInteger(\"replyBytes\")\n      .build\n```", "```py\nval conf = new SparkConf\n conf.setMaster(\"local[*]\")\n conf.setAppName(\"DataVec Log Analysis Example\")\n val sc = new JavaSparkContext(conf)\n```", "```py\nval directory = new ClassPathResource(\"access_log\").getFile.getAbsolutePath\n```", "```py\nvar logLines = sc.textFile(directory)\nlogLines = logLines.filter { (s: String) =>\n    s.matches(\"(\\\\S+) - - \\\\[(\\\\S+ -\\\\d{4})\\\\] \\\"(.+)\\\" (\\\\d+) (\\\\d+|-)\")\n}\n```", "```py\nval regex = \"(\\\\S+) - - \\\\[(\\\\S+ -\\\\d{4})\\\\] \\\"(.+)\\\" (\\\\d+) (\\\\d+|-)\"\n  val rr = new RegexLineRecordReader(regex, 0)\n  val parsed = logLines.map(new StringToWritablesFunction(rr))\n```", "```py\nval dqa = AnalyzeSpark.analyzeQuality(schema, parsed)\n  println(\"----- Data Quality -----\")\n  println(dqa)\n```", "```py\n----- Data Quality -----\n idx   name                 type           quality   details\n 0     \"host\"               String         ok        StringQuality(countValid=1546, countInvalid=0, countMissing=0, countTotal=1546, countEmptyString=0, countAlphabetic=0, countNumerical=0, countWordCharacter=10, countWhitespace=0, countApproxUnique=170)\n 1     \"timestamp\"          String         ok        StringQuality(countValid=1546, countInvalid=0, countMissing=0, countTotal=1546, countEmptyString=0, countAlphabetic=0, countNumerical=0, countWordCharacter=0, countWhitespace=0, countApproxUnique=1057)\n 2     \"request\"            String         ok        StringQuality(countValid=1546, countInvalid=0, countMissing=0, countTotal=1546, countEmptyString=0, countAlphabetic=0, countNumerical=0, countWordCharacter=0, countWhitespace=0, countApproxUnique=700)\n 3     \"httpReplyCode\"      Integer        ok        IntegerQuality(countValid=1546, countInvalid=0, countMissing=0, countTotal=1546, countNonInteger=0)\n 4     \"replyBytes\"         Integer        FAIL      IntegerQuality(countValid=1407, countInvalid=139, countMissing=0, countTotal=1546, countNonInteger=139)\n```", "```py\n10.0.0.153 - - [12/Mar/2004:11:01:26 -0800] \"GET / HTTP/1.1\" 304 -\n10.0.0.153 - - [12/Mar/2004:12:23:11 -0800] \"GET / HTTP/1.1\" 304 -\n```", "```py\nval tp: TransformProcess = new TransformProcess.Builder(schema)\n       .conditionalReplaceValueTransform(\"replyBytes\", new IntWritable(0), new StringRegexColumnCondition(\"replyBytes\", \"\\\\D+\"))\n```", "```py\n.reduce(new Reducer.Builder(ReduceOp.CountUnique)\n         .keyColumns(\"host\")                             \n         .countColumns(\"timestamp\")                      \n         .countUniqueColumns(\"request\", \"httpReplyCode\")\n         .sumColumns(\"replyBytes\")                       \n         .build\n       )\n```", "```py\n.renameColumn(\"count\", \"numRequests\")\n```", "```py\n.filter(new ConditionFilter(new LongColumnCondition(\"sum(replyBytes)\", ConditionOp.LessThan, 1000000)))\n  .build\n```", "```py\nval processed = SparkTransformExecutor.execute(parsed, tp)\n  processed.cache\n```", "```py\nval finalDataSchema = tp.getFinalSchema\n  val finalDataCount = processed.count\n  val sample = processed.take(10)\n  val analysis = AnalyzeSpark.analyze(finalDataSchema, processed)\n```", "```py\nidx   name                              type           meta data\n 0     \"host\"                            String         StringMetaData(name=\"host\",)\n 1     \"count(timestamp)\"                Long           LongMetaData(name=\"count(timestamp)\",minAllowed=0)\n 2     \"countunique(request)\"            Long           LongMetaData(name=\"countunique(request)\",minAllowed=0)\n 3     \"countunique(httpReplyCode)\"      Long           LongMetaData(name=\"countunique(httpReplyCode)\",minAllowed=0)\n 4     \"sum(replyBytes)\"                 Integer        IntegerMetaData(name=\"sum(replyBytes)\",)\n```", "```py\n[10.0.0.153, 270, 43, 3, 1200145]\n [64.242.88.10, 452, 451, 2, 5745035]\n```", "```py\n----- Analysis -----\n idx   name                              type           analysis\n 0     \"host\"                            String         StringAnalysis(minLen=10,maxLen=12,meanLen=11.0,sampleStDevLen=1.4142135623730951,sampleVarianceLen=2.0,count=2)\n 1     \"count(timestamp)\"                Long           LongAnalysis(min=270,max=452,mean=361.0,sampleStDev=128.69343417595164,sampleVariance=16562.0,countZero=0,countNegative=0,countPositive=2,countMinValue=1,countMaxValue=1,count=2)\n 2     \"countunique(request)\"            Long           LongAnalysis(min=43,max=451,mean=247.0,sampleStDev=288.4995667241114,sampleVariance=83232.0,countZero=0,countNegative=0,countPositive=2,countMinValue=1,countMaxValue=1,count=2)\n 3     \"countunique(httpReplyCode)\"      Long           LongAnalysis(min=2,max=3,mean=2.5,sampleStDev=0.7071067811865476,sampleVariance=0.5,countZero=0,countNegative=0,countPositive=2,countMinValue=1,countMaxValue=1,count=2)\n 4     \"sum(replyBytes)\"                 Integer        IntegerAnalysis(min=1200145,max=5745035,mean=3472590.0,sampleStDev=3213722.538746928,sampleVariance=1.032801255605E13,countZero=0,countNegative=0,countPositive=2,countMinValue=1,countMaxValue=1,count=2)\n```"]