["```py\ndef fetch_cosine_values(seq_len, frequency=0.01, noise=0.1): \n    np.random.seed(101)\n    x = np.arange(0.0, seq_len, 1.0)\n    return np.cos(2 * np.pi * frequency * x) + np.random.uniform(low=-noise, high=noise, size=seq_len)\n```", "```py\nprint(fetch_cosine_values(10, frequency=0.1))\n```", "```py\n[ 1.00327973 0.82315051 0.21471184 -0.37471266 -0.7719616 -0.93322063\n-0.84762375 -0.23029438 0.35332577 0.74700479]\n```", "```py\n$> curl \"https://www.quandl.com/api/v3/datasets/WIKI/FB/data.csv\"\nDate,Open,High,Low,Close,Volume,Ex-Dividend,Split Ratio,Adj. Open,Adj. High,Adj. Low,Adj. Close,Adj. Volume\n2017-08-18,166.84,168.67,166.21,167.41,14933261.0,0.0,1.0,166.84,168.67,166.21,167.41,14933261.0\n2017-08-17,169.34,169.86,166.85,166.91,16791591.0,0.0,1.0,169.34,169.86,166.85,166.91,16791591.0\n2017-08-16,171.25,171.38,169.24,170.0,15580549.0,0.0,1.0,171.25,171.38,169.24,170.0,15580549.0\n2017-08-15,171.49,171.5,170.01,171.0,8621787.0,0.0,1.0,171.49,171.5,170.01,171.0,8621787.0\n...\n```", "```py\ndef date_obj_to_str(date_obj):\n    return date_obj.strftime('%Y-%m-%d')\n\ndef save_pickle(something, path):\n    if not os.path.exists(os.path.dirname(path)):\n        os.makedirs(os.path.dirname(path))\n    with open(path, 'wb') as fh:\n        pickle.dump(something, fh, pickle.DEFAULT_PROTOCOL)\n\ndef load_pickle(path):\n    with open(path, 'rb') as fh:\n    return pickle.load(fh)\n\ndef fetch_stock_price(symbol,\n                      from_date,\n                      to_date,\n                      cache_path=\"./tmp/prices/\"):\n    assert(from_date <= to_date)\n    filename = \"{}_{}_{}.pk\".format(symbol, str(from_date), str(to_date))\n    price_filepath = os.path.join(cache_path, filename)\n    try:\n        prices = load_pickle(price_filepath)\n        print(\"loaded from\", price_filepath)\n    except IOError:\n        historic = quandl.get(\"WIKI/\" + symbol,\n        start_date=date_obj_to_str(from_date),\n        end_date=date_obj_to_str(to_date))\n        prices = historic[\"Adj. Close\"].tolist()\n        save_pickle(prices, price_filepath)\n        print(\"saved into\", price_filepath)\n    return prices\n```", "```py\nimport datetime\nprint(fetch_stock_price(\"GOOG\",\n      datetime.date(2017, 1, 1),\n      datetime.date(2017, 1, 31)))\n```", "```py\n[786.14, 786.9, 794.02, 806.15, 806.65, 804.79, 807.91, 806.36, 807.88, 804.61, 806.07, 802.175, 805.02, 819.31, 823.87, 835.67, 832.15, 823.31, 802.32, 796.79]\n```", "```py\ndef format_dataset(values, temporal_features):\n    feat_splits = [values[i:i + temporal_features] for i in range(len(values) - temporal_features)]\n    feats = np.vstack(feat_splits)\n    labels = np.array(values[temporal_features:])\n    return feats, labels\n```", "```py\nimport datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn\nfrom tools import fetch_cosine_values, fetch_stock_price, format_dataset\nnp.set_printoptions(precision=2)\n\ncos_values = fetch_cosine_values(20, frequency=0.1)\nseaborn.tsplot(cos_values)\nplt.xlabel(\"Days since start of the experiment\")\nplt.ylabel(\"Value of the cosine function\")\nplt.title(\"Cosine time series over time\")\nplt.show()\n```", "```py\nfeatures_size = 5\nminibatch_cos_X, minibatch_cos_y = format_dataset(cos_values, features_size)\nprint(\"minibatch_cos_X.shape=\", minibatch_cos_X.shape)\nprint(\"minibatch_cos_y.shape=\", minibatch_cos_y.shape)\n```", "```py\nsamples_to_plot = 5\nf, axarr = plt.subplots(samples_to_plot, sharex=True)\nfor i in range(samples_to_plot):\n    feats = minibatch_cos_X[i, :]\n    label = minibatch_cos_y[i]\n    print(\"Observation {}: X={} y={}\".format(i, feats, label))\n    plt.subplot(samples_to_plot, 1, i+1)\n    axarr[i].plot(range(i, features_size + i), feats, '--o')\n    axarr[i].plot([features_size + i], label, 'rx')\n    axarr[i].set_ylim([-1.1, 1.1])\nplt.xlabel(\"Days since start of the experiment\")\naxarr[2].set_ylabel(\"Value of the cosine function\")\naxarr[0].set_title(\"Visualization of some observations: Features (blue) and Labels (red)\")\nplt.show()\n```", "```py\nsymbols = [\"MSFT\", \"KO\", \"AAL\", \"MMM\", \"AXP\", \"GE\", \"GM\", \"JPM\", \"UPS\"]\nax = plt.subplot(1,1,1)\nfor sym in symbols:\n    prices = fetch_stock_price(\n    sym, datetime.date(2015, 1, 1), datetime.date(2016, 12, 31))\n    ax.plot(range(len(prices)), prices, label=sym)\n\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels)\nplt.xlabel(\"Trading days since 2015-1-1\")\nplt.ylabel(\"Stock price [$]\")\nplt.title(\"Prices of some American stocks in trading days of 2015 and 2016\")\nplt.show()\n```", "```py\ndef matrix_to_array(m):\n    return np.asarray(m).reshape(-1)\n```", "```py\nimport numpy as np\nfrom matplotlib import pylab as plt\nfrom tools import matrix_to_array\n\ndef evaluate_ts(features, y_true, y_pred):\n    print(\"Evaluation of the predictions:\")\n    print(\"MSE:\", np.mean(np.square(y_true - y_pred)))\n    print(\"mae:\", np.mean(np.abs(y_true - y_pred)))\n\n    print(\"Benchmark: if prediction == last feature\")\n    print(\"MSE:\", np.mean(np.square(features[:, -1] - y_true)))\n    print(\"mae:\", np.mean(np.abs(features[:, -1] - y_true)))\n\n    plt.plot(matrix_to_array(y_true), 'b')\n    plt.plot(matrix_to_array(y_pred), 'r--')\n    plt.xlabel(\"Days\")\n    plt.ylabel(\"Predicted and true values\")\n    plt.title(\"Predicted (Red) VS Real (Blue)\")\n    plt.show()\n\n    error = np.abs(matrix_to_array(y_pred) - matrix_to_array(y_true))\n    plt.plot(error, 'r')\n    fit = np.polyfit(range(len(error)), error, deg=1)\n    plt.plot(fit[0] * range(len(error)) + fit[1], '--')\n    plt.xlabel(\"Days\")\n    plt.ylabel(\"Prediction error L1 norm\")\n    plt.title(\"Prediction error (absolute) and trendline\")\n    plt.show()\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom evaluate_ts import evaluate_ts\nfrom tensorflow.contrib import rnn\nfrom tools import fetch_cosine_values, format_dataset\n\ntf.reset_default_graph()\ntf.set_random_seed(101)\n```", "```py\nfeat_dimension = 20\ntrain_size = 250\ntest_size = 250\n```", "```py\nlearning_rate = 0.01\noptimizer = tf.train.AdamOptimizer\nn_epochs = 10\n```", "```py\ncos_values = fetch_cosine_values(train_size + test_size + feat_dimension)\nminibatch_cos_X, minibatch_cos_y = format_dataset(cos_values, feat_dimension)\ntrain_X = minibatch_cos_X[:train_size, :].astype(np.float32)\ntrain_y = minibatch_cos_y[:train_size].reshape((-1, 1)).astype(np.float32)\ntest_X = minibatch_cos_X[train_size:, :].astype(np.float32)\ntest_y = minibatch_cos_y[train_size:].reshape((-1, 1)).astype(np.float32)\n```", "```py\nX_tf = tf.placeholder(\"float\", shape=(None, feat_dimension), name=\"X\")\ny_tf = tf.placeholder(\"float\", shape=(None, 1), name=\"y\")\n```", "```py\ndef regression_ANN(x, weights, biases):\n    return tf.add(biases, tf.matmul(x, weights))\n```", "```py\nweights = tf.Variable(tf.truncated_normal([feat_dimension, 1], mean=0.0, stddev=1.0), name=\"weights\")\nbiases = tf.Variable(tf.zeros([1, 1]), name=\"bias\")\n```", "```py\ny_pred = regression_ANN(X_tf, weights, biases)\ncost = tf.reduce_mean(tf.square(y_tf - y_pred))\ntrain_op = optimizer(learning_rate).minimize(cost)\n```", "```py\nwith tf.Session() as sess:\n   sess.run(tf.global_variables_initializer())\n   # For each epoch, the whole training set is feeded into the tensorflow graph\n\n   for i in range(n_epochs):\n       train_cost, _ = sess.run([cost, train_op], feed_dict={X_tf: train_X, y_tf: train_y})\n       print(\"Training iteration\", i, \"MSE\", train_cost)\n\n   # After the training, let's check the performance on the test set\n   test_cost, y_pr = sess.run([cost, y_pred], feed_dict={X_tf: test_X, y_tf: test_y})\n   print(\"Test dataset:\", test_cost)\n\n   # Evaluate the results\n   evaluate_ts(test_X, test_y, y_pr)\n\n   # How does the predicted look like?\n   plt.plot(range(len(cos_values)), cos_values, 'b')\n   plt.plot(range(len(cos_values)-test_size, len(cos_values)), y_pr, 'r--')\n   plt.xlabel(\"Days\")\n   plt.ylabel(\"Predicted and true values\")\n   plt.title(\"Predicted (Red) VS Real (Blue)\")\n   plt.show()\n```", "```py\nTraining iteration 0 MSE 4.39424\nTraining iteration 1 MSE 1.34261\nTraining iteration 2 MSE 1.28591\nTraining iteration 3 MSE 1.84253\nTraining iteration 4 MSE 1.66169\nTraining iteration 5 MSE 0.993168\n...\n...\nTraining iteration 998 MSE 0.00363447\nTraining iteration 999 MSE 0.00363426\nTest dataset: 0.00454513\nEvaluation of the predictions:\nMSE: 0.00454513\nmae: 0.0568501\nBenchmark: if prediction == last feature\nMSE: 0.964302\nmae: 0.793475\n```", "```py\nsymbol = \"MSFT\"\nfeat_dimension = 20\ntrain_size = 252\ntest_size = 252 - feat_dimension\n\n# Settings for tensorflow\nlearning_rate = 0.05\noptimizer = tf.train.AdamOptimizer\nn_epochs = 1000\n\n# Fetch the values, and prepare the train/test split\nstock_values = fetch_stock_price(symbol, datetime.date(2015, 1, 1), datetime.date(2016, 12, 31))\nminibatch_cos_X, minibatch_cos_y = format_dataset(stock_values, feat_dimension)\ntrain_X = minibatch_cos_X[:train_size, :].astype(np.float32)\ntrain_y = minibatch_cos_y[:train_size].reshape((-1, 1)).astype(np.float32)\ntest_X = minibatch_cos_X[train_size:, :].astype(np.float32)\ntest_y = minibatch_cos_y[train_size:].reshape((-1, 1)).astype(np.float32)\n```", "```py\nlearning_rate = 0.5\nn_epochs = 20000\noptimizer = tf.train.AdamOptimizer\n```", "```py\nTraining iteration 0 MSE 15136.7\nTraining iteration 1 MSE 106385.0\nTraining iteration 2 MSE 14307.3\nTraining iteration 3 MSE 15565.6\n...\n...\nTraining iteration 19998 MSE 0.577189\nTraining iteration 19999 MSE 0.57704\nTest dataset: 0.539493\nEvaluation of the predictions:\nMSE: 0.539493\nmae: 0.518984\nBenchmark: if prediction == last feature\nMSE: 33.7714\nmae: 4.6968\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom evaluate_ts import evaluate_ts\nfrom tensorflow.contrib import rnn\nfrom tools import fetch_cosine_values, format_dataset\ntf.reset_default_graph()\ntf.set_random_seed(101)\n```", "```py\ntime_dimension = 20\ntrain_size = 250\ntest_size = 250\n```", "```py\nlearning_rate = 0.01\noptimizer = tf.train.AdagradOptimizer\nn_epochs = 100\nn_embeddings = 64\n```", "```py\ncos_values = fetch_cosine_values(train_size + test_size + time_dimension)\nminibatch_cos_X, minibatch_cos_y = format_dataset(cos_values, time_dimension)\ntrain_X = minibatch_cos_X[:train_size, :].astype(np.float32)\ntrain_y = minibatch_cos_y[:train_size].reshape((-1, 1)).astype(np.float32)\ntest_X = minibatch_cos_X[train_size:, :].astype(np.float32)\ntest_y = minibatch_cos_y[train_size:].reshape((-1, 1)).astype(np.float32)\ntrain_X_ts = train_X[:, :, np.newaxis]\ntest_X_ts = test_X[:, :, np.newaxis]\n```", "```py\nX_tf = tf.placeholder(\"float\", shape=(None, time_dimension, 1), name=\"X\")\ny_tf = tf.placeholder(\"float\", shape=(None, 1), name=\"y\")\n```", "```py\ndef RNN(x, weights, biases):\n    x_ = tf.unstack(x, time_dimension, 1)\n    lstm_cell = rnn.BasicLSTMCell(n_embeddings)\n    outputs, _ = rnn.static_rnn(lstm_cell, x_, dtype=tf.float32)\n    return tf.add(biases, tf.matmul(outputs[-1], weights))\n```", "```py\nweights = tf.Variable(tf.truncated_normal([n_embeddings, 1], mean=0.0, stddev=1.0), name=\"weights\")\nbiases = tf.Variable(tf.zeros([1]), name=\"bias\")\ny_pred = RNN(X_tf, weights, biases)\ncost = tf.reduce_mean(tf.square(y_tf - y_pred))\ntrain_op = optimizer(learning_rate).minimize(cost)\n\n# Exactly as before, this is the main loop.\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    # For each epoch, the whole training set is feeded into the tensorflow graph\n    for i in range(n_epochs):\n        train_cost, _ = sess.run([cost, train_op], feed_dict={X_tf: train_X_ts, y_tf: train_y})\n        if i%100 == 0:\n            print(\"Training iteration\", i, \"MSE\", train_cost)\n\n    # After the training, let's check the performance on the test set\n    test_cost, y_pr = sess.run([cost, y_pred], feed_dict={X_tf: test_X_ts, y_tf: test_y})\n    print(\"Test dataset:\", test_cost)\n\n    # Evaluate the results\n    evaluate_ts(test_X, test_y, y_pr)\n\n    # How does the predicted look like?\n    plt.plot(range(len(cos_values)), cos_values, 'b')\n    plt.plot(range(len(cos_values)-test_size, len(cos_values)), y_pr, 'r--')\n    plt.xlabel(\"Days\")\n    plt.ylabel(\"Predicted and true values\")\n    plt.title(\"Predicted (Red) VS Real (Blue)\")\n    plt.show()\n```", "```py\nTraining iteration 0 MSE 0.0603129\nTraining iteration 100 MSE 0.0054377\nTraining iteration 200 MSE 0.00502512\nTraining iteration 300 MSE 0.00483701\n...\nTraining iteration 9700 MSE 0.0032881\nTraining iteration 9800 MSE 0.00327899\nTraining iteration 9900 MSE 0.00327195\nTest dataset: 0.00416444\nEvaluation of the predictions:\nMSE: 0.00416444\nmae: 0.0545878\n```", "```py\nstock_values = fetch_stock_price(symbol, datetime.date(2015, 1, 1), datetime.date(2016, 12, 31))\nminibatch_cos_X, minibatch_cos_y = format_dataset(stock_values, time_dimension)\ntrain_X = minibatch_cos_X[:train_size, :].astype(np.float32)\ntrain_y = minibatch_cos_y[:train_size].reshape((-1, 1)).astype(np.float32)\ntest_X = minibatch_cos_X[train_size:, :].astype(np.float32)\ntest_y = minibatch_cos_y[train_size:].reshape((-1, 1)).astype(np.float32)\ntrain_X_ts = train_X[:, :, np.newaxis]\ntest_X_ts = test_X[:, :, np.newaxis]\n```", "```py\nweights = tf.Variable(tf.truncated_normal([n_embeddings, 1], mean=0.0, stddev=10.0), name=\"weights\")\n```", "```py\nlearning_rate = 0.1\nn_epochs = 5000\nn_embeddings = 256\n```", "```py\nTraining iteration 200 MSE 2.39028\nTraining iteration 300 MSE 1.39495\nTraining iteration 400 MSE 1.00994\n...\nTraining iteration 4800 MSE 0.593951\nTraining iteration 4900 MSE 0.593773\nTest dataset: 0.497867\nEvaluation of the predictions:\nMSE: 0.497867\nmae: 0.494975\n```", "```py\nimport os\ntf_logdir = \"./logs/tf/stock_price_lstm\"\nos.makedirs(tf_logdir, exist_ok=1)\n```", "```py\ndef RNN(x, weights, biases):\n    with tf.name_scope(\"LSTM\"):\n        x_ = tf.unstack(x, time_dimension, 1)\n        lstm_cell = rnn.BasicLSTMCell(n_embeddings)\n        outputs, _ = rnn.static_rnn(lstm_cell, x_, dtype=tf.float32)\n        return tf.add(biases, tf.matmul(outputs[-1], weights))\n```", "```py\ny_pred = RNN(X_tf, weights, biases)\nwith tf.name_scope(\"cost\"):\n    cost = tf.reduce_mean(tf.square(y_tf - y_pred))\n    train_op = optimizer(learning_rate).minimize(cost)\n    tf.summary.scalar(\"MSE\", cost)\n        with tf.name_scope(\"mae\"):\n        mae_cost = tf.reduce_mean(tf.abs(y_tf - y_pred))\n        tf.summary.scalar(\"mae\", mae_cost)\n```", "```py\nwith tf.Session() as sess:\n    writer = tf.summary.FileWriter(tf_logdir, sess.graph)\n    merged = tf.summary.merge_all()\n    sess.run(tf.global_variables_initializer())\n\n    # For each epoch, the whole training set is feeded into the tensorflow graph\n    for i in range(n_epochs):\n        summary, train_cost, _ = sess.run([merged, cost, train_op], feed_dict={X_tf: \n                                                  train_X_ts, y_tf: train_y})\n        writer.add_summary(summary, i)\n        if i%100 == 0:\n            print(\"Training iteration\", i, \"MSE\", train_cost)\n    # After the training, let's check the performance on the test set\n    test_cost, y_pr = sess.run([cost, y_pred], feed_dict={X_tf: test_X_ts, y_tf: \n           test_y})\n    print(\"Test dataset:\", test_cost)\n```", "```py\n$> tensorboard --logdir=./logs/tf/stock_price_lstm\n```"]