- en: Imitation and Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing, a new AI called AlphaStar, a **deep reinforcement learning**
    (**DRL**) agent, used **imitation learning** (**IL**) to beat a human opponent
    five-nil playing the real-time strategy game StarCraft II. AlphaStar was the continuation
    of David Silver and Google DeepMind's work to build a smarter and more intelligent
    AI. The specific techniques AlphaStar used to win could fill a book, and IL and
    the use of learning to copy human play is now of keen interest. Fortunately, Unity
    has already implemented IL in the form of offline and online training scenarios.
    While we won't make it to the level of AlphaStar in this chapter, we still will
    learn about the underlying technologies of IL and other forms of transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the implementation of IL in ML-Agents and
    then look to other applications of transfer learning. We will cover the following
    topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: IL or behavioral cloning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Online training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offline training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imitation Transfer Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While AlphaStar performed a stunning tactical victory against a human pro player
    in an RTS game, it has still come under scrutiny for the type of play and actions
    it used. Many human players stated that the AI's tactical abilities were clearly
    superior, but the overall strategy and planning were abysmal. It should be interesting
    to see how Google DeepMind approaches this criticism.
  prefs: []
  type: TYPE_NORMAL
- en: This will be an exciting chapter, and will provide you with plenty of training
    possibilities for your future developments, which all starts in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: IL, or behavioral cloning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IL, or behavioral cloning, is the process by which observations and actions
    are captured from a human, or perhaps another AI, and used as input into training
    an agent. The agent essentially becomes guided by the human and learns by their
    actions and observations. A set of learning observations can be received by real-time
    play (online) or be extracted from saved games (offline). This provides the ability
    to capture play from multiple agents and train them in tandem or individually.
    IL provides the ability to train or, in effect, program agents for tasks you may
    find impossible to train for using regular RL, and because of this, it will likely
    become a key RL technique that we use for most tasks in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is hard to gauge the value something gives you until you see what things
    are like without it. With that in mind, we will first start by looking at an example
    that uses no IL, but certainly could benefit from it. Open up the Unity editor
    and follow this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open up the Tennis scene from the Assets | ML-Agents | Examples | Tennis | Scenes
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select and disable the extra agent training areas, TennisArea(1) to TennisArea(17).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select AgentA and make sure Tennis Agent | Brain is set to TennisLearning. We
    want each agent to be against the other agent in this example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select AgentB and make sure Tennis Agent | Brain is set to TennisLearning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this example, for a brief instance, we are training multiple agents in the
    same environment. We will cover more scenarios where agents play other agents
    as a way of learning in [Chapter 11](15e7adeb-8b67-4b93-81d4-5f129772cd97.xhtml),
    *Building Multi-Agent Environments*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select Academy and make sure that Tennis Academy | Brains is set to TennisLearning
    and the Control option is enabled, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cd61436d-3edf-44b8-9966-13f9ab48b633.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting Control to enabled on Academy
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Python/Anaconda window and prepare it for training. We will launch training
    with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Watch the training for several thousand iterations, enough to convince yourself
    the agents are not going to learn this task easily. When you are convinced, stop
    the training and move on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can see by just looking at this first example that ordinary training and
    the other advanced methods we looked at, such as Curriculum and Curiosity Learning,
    would be difficult to implement, and in this case could be counterproductive.
    In the next section, we look at how to run this example with IL in online training
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: Online training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Online Imitation Learning is where you teach the agent to learn the observations
    of a player or another agent in real time. It also is one of the most fun and
    engaging ways to train agents or bots. Let''s jump in and set up the tennis environment
    for online Imitation Learning in the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the TennisArea | AgentA object and set Tennis Agent | Brain to TennisPlayer.
    In this IL scenario, we have one brain acting as a teacher, the player, and a
    second brain acting as the student, the learner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the AgentB object and make sure Tennis Agent | Brain is set to TennisLearning.
    This will be the student brain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `online_bc_config.yaml` file from the `ML-Agents/ml-agents/config`
    folder. IL does not use the same configuration as PPO so the parameters will have
    similar names but may not respond to what you have become used to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll down in the file to the **`TennisLearning`** brain configuration as
    shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking over the hyperparameters, we can see there are two new parameters of
    interest. A summary of those parameters is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`trainer`: `online_` *or* `offline_bc`—using online or offline Behavioral Cloning.
    In this case, we are performing online.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`brain_to_imitate`: `TennisPlayer`—this sets the brain that the learning brain
    should attempt to imitate.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*We won''t make any changes to the file at this point.*'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open your prepared Python/Anaconda window and launch training with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After you press Play in the editor, you will be able to control the left paddle
    with the *W*, *A*, *S*, *D* keys. Play the game, and you may be surprised at how
    quickly the agent learns and can get quite good. The following is an example of
    the game being played:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/77df8273-6f46-4876-aa07-4564b8092696.png)'
  prefs: []
  type: TYPE_IMG
- en: Playing and teaching the agent with IL
  prefs: []
  type: TYPE_NORMAL
- en: Keep playing the example until completion if you like. It can also be interesting
    to switch players during a game, or even train the brain and use the trained model
    to play against later. You do remember how to run a trained model, right?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At some point while playing through the last exercise, you may have wondered
    why we don't we train all RL agents this way. A good question, but as you can
    imagine, it depends. While IL is very powerful, and quite a capable learner, it
    doesn't always do what we expect it to do. Also, an IL agent is only going to
    learn the search space (observations) it is shown and remain within those limitations.
    In the case of AlphaStar, IL was the main input for training, but the team also
    mentioned that the AI did have plenty of time to self-play, which likely accounted
    for many of its winning strategies. So, while IL is cool and powerful, it is not
    the golden goose that will solve all our RL problems. However, you are likely
    to have a new and greater appreciation for RL, and in particular IL, after this
    exercise. In the next section, we explore using offline IL.
  prefs: []
  type: TYPE_NORMAL
- en: Offline training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Offline training is where a recorded gameplay file is generated from a player
    or agent playing a game or performing a task, and is then fed back as training
    observations to help an agent learn later on. While online learning certainly
    is more fun, and in some ways more applicable to the Tennis scene or other multiplayer
    games, it is less practical. After all, you generally need to play an agent in
    real time for several hours before an agent will become good. Likewise, in online
    training scenarios, you are typically limited to single agent training, whereas
    in offline training a demo playback can be fed to multiple agents for better overall
    learning. This also allows us to perform interesting training scenarios, similar
    to AlphaStar training, where we can teach an agent so that it can teach other
    agents.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn more about multi-agent gameplay in [Chapter 11](15e7adeb-8b67-4b93-81d4-5f129772cd97.xhtml), *Building
    Multi-Agent Environments*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this next exercise, we are going to revisit our old friend the Hallway/VisualHallway
    example. Again, we are doing this so we can compare our results to the previous
    sample exercises we ran with this environment. Follow this exercise to set up
    a new offline training session:'
  prefs: []
  type: TYPE_NORMAL
- en: Clone and download the ML-Agents code to a new folder, perhaps choosing `ml-agents_b`, `ml-agents_c`,
    or some other name. The reason we do this is to make sure that we run these new
    exercises with a clean environment. Also, it can sometimes help to go back to
    old environments and recall settings or configuration that you may forget to update.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch Unity and open the **UnitySDK** project and the Hallway or VisualHallway
    scene, your choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The scene should be set to run in Player mode. Just confirm this. If you need
    to change it, then do so.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Disable any additional agent training environments in the scene if others are
    active.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select HallwayArea | Agent in the Hierarchy window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click the Add Component button at the bottom of the Inspector window, type
    `demo`, and select the Demonstration Recorder component as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/76cb16eb-4858-4a5c-b6a5-a95d32b15a5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding a Demonstration Recorder
  prefs: []
  type: TYPE_NORMAL
- en: Click Record on the new Demonstration Recorder component, as shown in the preceding
    screenshot, check throughout. Also, fill in the Demonstration Name property of
    the recording, which is also shown.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the scene and project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press Play and play the scene for a fair amount of time, more than a few minutes
    but perhaps less than hours. Of course, how well you play will also determine
    how well the agent learns. If you play poorly, so will the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After you think enough time has passed, and you have played as well as you could,
    stop the game.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After playing the game, you should see a new folder called Demonstrations created
    in the Assets root folder in your Project window. Inside the folder will be your
    demonstration recording. This is the recording we will feed the agent in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up for training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have our demonstration recording, we can do more on the training
    part. This time, however, we will play back our observation file to multiple agents
    in multiple environments. Open the Hallway/VisualHallway sample scene and follow
    the next exercise to set up for training:'
  prefs: []
  type: TYPE_NORMAL
- en: Select and enable all the HallwayArea training environments HallwayArea(1) to
    HallwayArea(15)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select HallwayArea | Agent in the Hierarchy and then switch Hallway Agent |
    Brain to HallwayLearning, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/984fc851-9110-491a-83b8-37da2b240c42.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the agent components
  prefs: []
  type: TYPE_NORMAL
- en: Also, select and disable the Demonstration Recording component as shown in the
    preceding screen excerpt
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure all the agents in the scene are using HallwayLearning brains
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select Academy in the Hierarchy and then enable the Hallway Academy | Brains
    | Control option as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/85138784-78f7-4990-ab15-0c0b38439ae5.png)'
  prefs: []
  type: TYPE_IMG
- en: Enabling Academy to Control the Brains
  prefs: []
  type: TYPE_NORMAL
- en: Save the scene and project
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have the scene configured for agent learning, we can move on to
    feeding the agent in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Feeding the agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we performed online IL, we only fed one agent at a time in the tennis scene.
    This time, however, we are going to train multiple agents from the same demonstration
    recording in order to improve training performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already set up for training, so let''s start feeding the agent in the
    following exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a Python/Anaconda window and set it up for training from the new `ML-Agents`
    folder. You did reclone the source, right?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the `offline_bc_config.yaml` file from the `ML-Agents/ml-agents_b/config`
    folder. The contents of the file are as follows for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the last line of the `HallwayLearning` or `VisualHallwayLearning` brain
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that if you are using the `VisualHallwayLearning` brain, you will need
    to also change the name in the preceding config script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save your changes when you are done editing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go back to your Python/Anaconda window and launch training with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When prompted, press Play in the editor and watch the training unfold. You will
    see the agent play using very similar moves to yourself, and if you played well,
    the agent will quickly start learning and you should see some impressive training,
    all thanks to IL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RL can be thought of as the brute-force approach to learning, while the refinement
    of Imitation Learning and training by observation will clearly dominate the future
    of agent training. Of course, is it really any wonder? After all, we simple humans
    learn that way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at another exciting area of deep learning, transfer
    learning, and how it applies to games and DRL.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imitation Learning, by definition, falls into a category of **Transfer Learning**
    (**TL**). We can define Transfer Learning as the process by which an agent or
    DL network is trained by transference of experiences from one to the other. This
    could be as simple as the observation training we just performed, or as complex
    as swapping layers/layer weights in an agent's brain, or just training an agent
    on a similar task.
  prefs: []
  type: TYPE_NORMAL
- en: Intransfer learningwe need to make sure the experiences or previous weights
    we use are generalized. Through the foundational chapters in this book (chapters
    1-3), we learned the value of generalization using techniques such as dropout
    and batch normalization. We learned that these techniques are important for more
    general training; the form of training that allows the agent/network better inference
    on test data. This is no different than if we were to use an agent trained on
    one task to learn on another task. A more general agent will, in effect, be able
    to transfer knowledge more readily than a specialist agent could, if at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can demonstrate this in a quick example starting with training the following
    simple exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open up the VisualHallway scene in the Unity editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Disable any additional training areas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Confirm that Academy is in Control of the Brain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the VisualHallwayLearning brain from the Hallway/Brains folder and set Vector
    Action | Branches Size | Branch 0 Size to `7`, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4d4cb9a9-503a-43d5-8c7f-bdb2c1970824.png)'
  prefs: []
  type: TYPE_IMG
- en: Increasing the vector action space of the agent
  prefs: []
  type: TYPE_NORMAL
- en: We increase the action space for the brain so that it is compatible with the
    required action space for our transfer learning environment, which we will get
    to later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the scene and project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open a Python/Anaconda window that is prepared for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Launch a training session with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have introduced a new parameter that controls the frequency at which
    model checkpoints are created. The default is currently set to 50,000, but we
    just don't want to wait that long.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the agent in training in the editor for at least one model checkpoint save,
    as shown in the following screen excerpt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/32d4ca69-78e1-4a52-b2a4-f386d69cf508.png)'
  prefs: []
  type: TYPE_IMG
- en: The ML-Agents trainer creating a checkpoint
  prefs: []
  type: TYPE_NORMAL
- en: Checkpoints are a way of taking snapshots of a brain and saving them for later.
    This allows you to go back and continue training where you left off.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let the agent train to a checkpoint and then terminate training by pressing
    *Ctrl *+ *C *or c*ommand *+ *C* on Mac in the Python/Anaconda window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you have terminated training, it is time to try this saved brain on another
    learning environment in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Transferring a brain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now want to take the brain we have just been training and reuse it in a new,
    but similar, environment. Since our agent uses visual observations, this makes
    our task easier, but you could try and perform this example with other agents
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s open Unity and navigate to the VisualPushBlock example scene and follow
    this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Select Academy and enable it for Control of the Brains.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the Agent and set it to use the VisualPushBlockLearning brain. You should
    also confirm that this brain is configured in the same way as the VisualHallwayLearning
    brain we just ran, meaning that the Visual Observation and Vector Action spaces
    match.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `ML-Agents/ml-agents_b/models/vishall-0` folder in File Explorer or
    another file explorer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change the name of the file and folder from `VisualHallwayLearning` to `VisualPushBlockLearning` as
    shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d71f90f1-f500-4065-bc70-994600137b07.png)'
  prefs: []
  type: TYPE_IMG
- en: Changing the model path manually
  prefs: []
  type: TYPE_NORMAL
- en: By changing the name of the folder, we are essentially telling the model loading
    system to restore our VisualHallway brain as VisualPushBlockBrain. The trick here
    is making sure that both brains have all the same hyperparameters and configuration
    settings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Speaking of hyperparameters, open the `trainer_config.yaml` file and make sure
    that the VisualHallwayLearning and VisualPushBlockLearning parameters are the
    same. The configuration for both is shown in the following code snippet for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Save the configuration file when you are done editing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open your Python/Anaconda window and launch training with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The previous code is not a misprint; it is the exact same command we used to
    run the VisualHallway example, except with `--load` appended on the end. This
    should launch the training and prompt you to run the editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel free to run the training for as long as you like, but keep in mind that
    we barely trained the original agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, in this example, even if we had trained the agent to complete VisualHallway,
    this likely would not have been very effective in transferring that knowledge
    to VisualPushBlock. For the purposes of this example, we chose both since they
    are quite similar, and transferring one trained brain to the other was less complicated.
    For your own purposes, being able to transfer trained brains may be more about
    retraining agents on new or modified levels, perhaps even allowing the agents
    to train on progressively more difficult levels.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your version of ML-Agents, this example may or may not work so
    well. The particular problem is the complexity of the model, number of hyperparameters,
    input space, and reward system that we are running. Keeping all of these factors
    the same also requires keen attention to detail. In the next section, we will
    take a short diversion to explore how complex these models are.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring TensorFlow checkpoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow is quickly becoming the underlying graph calculation engine that
    is powering most deep learning infrastructure. While we haven''t covered how these
    graph engines are constructed in much detail, it can be helpful to review these
    TensorFlow models visually. Not only can we start to appreciate the complexity
    of these systems better, but a good visual is often worth a thousand words. Let''s
    open up a web browser and follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Search for the phrase `netron tensorflow` in your browser with your favorite
    search engine. Netron is an OpenSource TensorFlow model viewer that is perfect
    for our needs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find a link to the GitHub page and on the page the links to download the binary
    installers. Select the installer for your platform and click Download. This will
    take you to another download page where you can select the file for download.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the installer for your platform to install the Netron application. On Windows,
    this is as simple as downloading the exe installer and running it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the Netron application, and after it launches, you will see the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/95bc6be0-a29e-4677-9a94-c3686ce9f21f.png)'
  prefs: []
  type: TYPE_IMG
- en: The Netron application
  prefs: []
  type: TYPE_NORMAL
- en: Click the Open Model... button in the middle of the window
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use File Explorer to locate the `ML-Agents/ml-agents/models/vishall-0\VisualHallwayLearning`
    folder, and locate the `raw_graph.def` file as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ef7efa51-c1a1-4b3d-b189-619ab4a4f0c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting the model graph definition to load
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the graph, use the - button in the top-right to zoom the view
    as far out as you can, similar to the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/091ce181-d2c3-428c-bc00-a6ce885bb62c.png)'
  prefs: []
  type: TYPE_IMG
- en: The TensorFlow graph model of our agent's brain
  prefs: []
  type: TYPE_NORMAL
- en: As the inset shows, this graph is beyond complex, and not something we would
    be easily able to make sense of. However, it can be interesting to look through
    and see how the model/graph is constructed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll to the top of the graph and find a node called advantages, then select
    the node and note the Graph and Inputs, model properties as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/366e58fa-35bf-4cb4-be05-4b6c39f238fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Properties of the advantages graph model
  prefs: []
  type: TYPE_NORMAL
- en: Within the properties view of this model, you should be able to see some very
    familiar terms and settings, such as visual_observation_0, for instance, which
    shows the model input is a tensor of shape [84,84,3].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you are done, feel free to look over other models, and perhaps even explore
    with other models even outside Unity. While this tool isn't quite capable of summarizing
    a complex model like we have, it does show how powerful these types of tools are
    becoming. What's more, if you can find your way around, you can even export variables
    for later inspection or use.
  prefs: []
  type: TYPE_NORMAL
- en: Imitation Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the problems with Imitation Learning is that it often focuses the agent
    down a path that limits its possible future moves. This isn't unlike you being
    shown the improper way to perform a task and then doing it that way, perhaps without
    thinking, only to find out later that there was a better way. Humanity, in fact,
    has been prone to this type of problem over and over again throughout history.
    Perhaps you learned as a child that swimming right after eating was dangerous,
    only to learn later in life through your own experimentation, or just common knowledge,
    that that was just a myth, a myth that was taken as fact for a very long time.
    Training an agent through observation is no different you limit the agent's vision in
    many ways to a narrow focus that is limited by what it was taught. However, there
    is a way to allow an agent to revert back to the partial brute-force or trial-and
    error exploration in order to expand its training.
  prefs: []
  type: TYPE_NORMAL
- en: 'With ML-Agents we can combine IL with a form oftransfer learningin order to
    allow an agent to learn first from observation, then by furthering its training
    by learning from the once student. This form of IL chaining, if you will, allows
    you to train an agent to auto-train multiple agents. Let''s open up Unity to the
    TennisIL scene and follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the TennisArea | Agent object and in the Inspector, disable the BC Teacher
    Helper component, and then add a new Demonstration Recorder as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/65d19e63-93e6-436c-85c1-02a528d4c683.png)'
  prefs: []
  type: TYPE_IMG
- en: Checking that the BC Teacher is attached to the Agent
  prefs: []
  type: TYPE_NORMAL
- en: BC Teacher Helper is a recorder that works just like the Demonstration Recorder.
    The BC recorder allows you to turn the recording on and off as the agent runs,
    which is perfect for online training, but at the time of writing, the component
    was not working.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure Academy is set to Control the TennisLearning brain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the scene and project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open a Python/Anaconda window and launch training with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Press Play when prompted to run the game in the editor. Control the blue paddle
    with the *W*, *A*, *S*, *D* keys and play for a few seconds to warm up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After you are warmed up, press the *R* key to begin recording a demo observation.
    Play the game for several minutes and let the agent become capable. After the
    agent is able to return the ball, stop the training session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will not only train the agent, which is fine, but it will also create a
    demo recording playback we can use to further train the agents to learn how to
    play each other in a similar way to how AlphaStar was trained. We will set up
    our tennis scene to now run in offline training mode with multiple agents in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training multiple agents with one demonstration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, with the recording of us playing tennis, we can use this to feed into
    the training of multiple agents all feeding back into one policy. Open Unity to
    the tennis scene, the one with the multiple environments, and follow the next
    exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type `agent` into the Filter bar at the top of the Hierarchy window as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/88535449-155c-425c-911b-f57a448b2b5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Searching for all the agents in the scene
  prefs: []
  type: TYPE_NORMAL
- en: Select all the agent objects in the scene and bulk change their Brain to use
    TennisLearning and not TennisPlayer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Academy and make sure to enable it to control the brains.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `config/offline_bc_config.yaml` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following new section for the `TennisLearning` brain at the bottom:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Save the scene and the project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the Python/Anaconda window and run training with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You may want to add the `--slow` switch in order to watch the training, but
    it should not be required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let the agents train for some time and notice its improved progress. Even with
    a short observation recording input, the agent becomes a capable player rather
    quickly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are multiple ways to perform this type of IL andtransfer learningchaining
    that will allow your agent some flexibility in training. You could even use the
    trained model's checkpoint without IL and run the agents with transfer learning
    as we did earlier. The possibilities are limitless, and it remains to be seen
    what will emerge as best practices.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll provide some exercises that you can use for your
    own personal learning.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The exercises at the end of this chapter could likely provide several hours
    of fun. Try and only complete one or two exercises, as we still need to finish
    the book:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up and run the PyramidsIL scene to run online IL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up and run the PushBlockIL scene to run online IL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up and run the WallJump scene to run with online IL. This requires you to
    modify the scene.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up and run the VisualPyramids scene to use offline recording. Record a training
    session then train an agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up and run the VisualPushBlock scene to use offline recording. Use offline
    IL to train the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the PushBlockIL scene to record an observation demo. Then use this offline
    training to train multiple agents in the regular PushBlock scene.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the PyramidsIL scene to record a demo recording. Then use this for offline
    training to train multiple agents in the regular Pyramids scene.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train an agent in the VisualHallway scene using any form of learning you like.
    After training, modify the VisualHallway scene to use different materials on the
    walls and floor. Changing materials on Unity objects is quite easy. Then, use
    the technique of swapping model checkpoints as a way of transfer learning the
    previously trained brain into a new environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do exercise eight, but using the VisualPyramids scene. You could also add other
    objects or blocks in this scene.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do exercise eight, but using the VisualPushBlock scene. Try adding other blocks
    or other objects that the agent may have to work around.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Just remember that, if you are attempting any of the Transfer Learning exercises,
    attention to detail is important when matching the complex graphs. In the next
    section, we summarize what we have covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered an emerging technique in RL called Imitation Learning
    or Behavioral Cloning. This technique, as we learned, takes the captured observations
    of a player playing a game and then uses those observations in an online or offline
    setting to further train the agent. We further learned that IL is just a form
    of Transfer Learning. We then covered a technique with ML-Agents that will allow
    you to transfer brains across environments. Finally, we looked at how to chain
    IL andtransfer learningas a way of stimulating the agent's training into developing
    new strategies on its own.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will further our understanding of DRL in games by looking
    at multiple agent training scenarios.
  prefs: []
  type: TYPE_NORMAL
