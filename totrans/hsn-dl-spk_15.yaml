- en: What's Next for Deep Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This final chapter will try to give an overview of what's in store for the future
    of **deep learning** (**DL**) and, more generally, for AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: DL and AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hot topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark and **Reinforcement Learning** (**RL**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for **Generative Adversarial Networks** (**GANs**) in DL4J
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rapid advancement of technology not only speeds up the implementation of
    existing AI ideas, it creates new opportunities in this space that would have
    been unthinkable one or two years ago. Day by day, AI is finding new practical
    applications in diverse areas and is radically transforming the way we do business
    in them. Therefore, it would be impossible to cover all of the new scenarios,
    so we are going to focus on some particular contexts/areas where we have been
    directly or indirectly involved.
  prefs: []
  type: TYPE_NORMAL
- en: What to expect next for deep learning and AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, there are daily advances in technology and there's
    a growing availability of greater, but at the same time cheaper, computational
    power, along with a greater availability of data, which is driving toward the
    implementation of deeper and more complex models. So, at the same time, the limit
    for both DL and AI seems to be the sky. Trying to understand what we have to expect
    for these fields is speculation that could help us clearly understand what would
    happen in a short period of time (2-3 years), but what can happen next could be
    less predictable, as it has been observed that any new idea in this space is bringing
    up other ideas and is contributing to radically transforming ways of doing business
    in several areas. So, what I am going to describe in this section is related to
    the immediate future rather than a long-term period.
  prefs: []
  type: TYPE_NORMAL
- en: DL has played a key role in shaping the future of AI. In some areas, such as,
    for example, image classification and recognition, object detection, and NLP,
    DL has outperformed ML, but this doesn't mean that ML algorithms became obsolete.
    For some particular problems, DL is probably overkill, so ML would still be enough.
    In some other more complex cases, a combination of algorithms (DL and non-DL)
    have led to significant results; a perfect example is the AlphaGo system ([https://deepmind.com/research/alphago/](https://deepmind.com/research/alphago/))
    by the DeepMind team, which uses a combination of the **Monte Carlo tree search**
    (**MCTS**): [http://mcts.ai/about/](http://mcts.ai/about/), with a DL network
    to quickly search for winning moves. This huge progress in DL has also led to
    other more complex and advanced techniques such as RL and GANs, which are discussed
    in the last two sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, while algorithms and models are making incredibly fast progress, there
    are still plenty of obstacles that require significant human intervention (and
    extra time) to remove them before data can be taken and turned into machine intelligence.
    As discussed in the paper *Hidden Technical Debt in Machine Learning Systems*
    ([https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf))
    by a research group at Google, in DL and ML systems the cost of data dependencies
    is difficult to detect and it could easily become higher than the cost of code
    dependencies. The following diagram, which has been taken from the same Google
    research paper, shows the proportion of the dependencies in ML or DL code versus
    the rest of the dependencies in an ML or DL system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b3e64a5-dd2b-48ef-b401-925ae338744e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.1: Only a small fraction (the black rectangle at the center of the
    image) of real-world ML/DL systems are composed of ML/DL code'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding diagram, things such as data collection and
    the setup and maintenance of the serving infrastructure are more time and money
    consuming than the model's implementation and training. Therefore, I would expect
    significant improvements when automating these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Topics to watch for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past few months, a new debate started about the so-called explainable
    AI, an AI which isn't a sort of black box (where we understand only the underlying
    mathematical principles) and whose actions or decisions can be easily understood
    by humans. Criticism has been also been made (in general for AI, but in particular
    DL) about the generated results from models not being compliant with **GDPR**
    (short for **General Data Protection Regulation**): [https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/data-protection/2018-reform-eu-data-protection-rules_en](https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/data-protection/2018-reform-eu-data-protection-rules_en) for
    data related to EU citizens, or other data regulations that will probably be defined
    next in other parts of the world, which require the right to an explanation to
    prevent discriminatory effects based on different factors.
  prefs: []
  type: TYPE_NORMAL
- en: While this is a real hot and not negligible topic, and since several interesting
    analyze and proposals (such as [https://www.academia.edu/18088836/Defeasible_Reasoning_and_Argument-Based_Systems_in_Medical_Fields_An_Informal_Overview](https://www.academia.edu/18088836/Defeasible_Reasoning_and_Argument-Based_Systems_in_Medical_Fields_An_Informal_Overview) from
    Dr. Luca Longo ([https://ie.linkedin.com/in/drlucalongo](https://ie.linkedin.com/in/drlucalongo))
    from the Dublin Institute of Technology) have been done, I (and the readers of
    this book most probably too) have had the chance to listen to a few others' opinions
    and points of view in predicting a bad future for DL in particular, where DL applications
    will be restricted to non-business apps and games. In this section, I am not going
    to make comments on that point of view, which is often based more on opinions
    than facts, and is sometimes done by people who are not fully involved in production
    or research projects in the DL or ML spaces. Instead, I would prefer to present
    a list of practical DL applications that should still stay valid for a while.
  prefs: []
  type: TYPE_NORMAL
- en: Healthcare is one of the sectors that has a higher number of practical applications
    of AI and DL. Optum ([https://www.optum.com/](https://www.optum.com/)), a tech
    company that's part of the UnitedHealth Group, has achieved, as part of its overall
    strategy to transform healthcare operations, significant results when applying
    NLP to several of its business use cases. The ability of AI to understand both
    structured and unstructured data plays a critical role in medical record review
    (where most parts of the data are unstructured). Optum's so-called clinically
    intelligent NLP unlocks the unstructured content to get structured data elements,
    such as diagnoses, procedures, drugs, labs, and more that make up complete and
    accurate clinical documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Data from unstructured sources is automatically retrieved through NLP technology
    that complements the structured data coming through more *traditional* clinical
    models and rules engines. This level of automation can accurately identify diagnoses,
    along with related conditions and procedures to implement the care that's provided,
    but it is also necessary to define the appropriate reimbursement, quality initiatives,
    and other critical healthcare operations. But understanding what has been documented
    in a record is only a part of what makes NLP so valuable in healthcare. Clinically
    intelligent NLP technology can also identify documentation gaps; it can understand
    not only what is in a record, but also what is missing. This way, clinicians can
    get valuable feedback so that they can improve documentation. Other remarkable
    applications of AI in Optum have been related to payment integrity, simplified
    population analysis, and call centers.
  prefs: []
  type: TYPE_NORMAL
- en: Another hot topic in AI is robotics. While, technically speaking, it is a separate
    branch, it has a lot of overlap with AI. Advances in DL and RL provide answers
    to several questions in robotics. Robots have being defined by first being able
    to sense, then compute the inputs of their sensors, and finally take action based
    on the results of those computations. AI comes into play to move them away from
    an industrial step-and-repeat model and make them smarter.
  prefs: []
  type: TYPE_NORMAL
- en: A perfect example of a successful user story in this direction is the German
    startup Kewazo ([https://www.kewazo.com/](https://www.kewazo.com/)). They have
    implemented a smart robotic scaffolding transportation system that addresses several
    problems such as understaffing, efficiency, high costs, time-consuming activities,
    and worker's safety. AI has made it possible for them to implement a robotic system
    that, through data about the overall scaffolding assembly process delivered in
    real time, allows constant control and significant optimization or tuning. AI
    has also helped Kewazo engineers to identify other use cases, such as roofing
    or solar panel installations, where their robots can work and help achieve the
    same results as a scaffolding assembly.
  prefs: []
  type: TYPE_NORMAL
- en: The **Internet of Things** (**IoT**) is another area where AI is becoming more
    pervasive every day. IoT is based on the concept that daily use physical devices
    are connected to the internet and can communicate with each other to exchange
    data. The data that's collected could be processed intelligently to make devices
    smarter. The number of AI and IoT use cases is constantly growing due to the rapidly
    increasing number of connected devices (and the data that's generated by them).
  prefs: []
  type: TYPE_NORMAL
- en: 'Among these use cases, I would like to mention the potential of AI for smart
    buildings. The rapid growth of the Irish economy in the past 5 years, which has
    been driven by industries such as IT, banking, finance, and pharma, has led to
    a radical transformation of the area where I work at the present time, the Dublin
    city center between the Docklands and the Grand Canal Dock. To address the constant
    increasing need for office space from new or expanding companies, hundreds of
    new buildings have been built (and many more are coming). All of these recent
    buildings use some AI, combined with IoT, to become smarter. Significant results
    have been achieved in the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Making buildings more comfortable for humans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making building safer for humans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving energy savings (and helping the environment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional controllers (for temperature, lights, doors, and so on) use a limited
    number of sensors to automatically adjust the devices to a constant end result.
    This paradigm used to leave out an important thing: buildings are occupied by
    humans, but they are controlled the same, regardless of whether occupants are
    present or not. This means that things like making the people comfortable or saving
    energy, just to mention a couple issues, weren't taken into account. IoT combined
    with AI can add this critical missing piece. Therefore, buildings can have priorities
    and not simply follow a rigid programming paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting real-world use case for IoT and AI is farming. The farming
    sector (dairy, in particular) is a significant part of the Irish GDP and not a
    negligible voice in Irish exports. Farming has new and old challenges (such as
    producing more food on the same acres, meeting strict emissions requirements,
    protecting plantations from pests, taking the climate into account and global
    climate change, controlling water flow, monitoring extensive orchards, fighting
    fires, monitoring soil quality, monitoring the health of animals, and so on).
    This means that farmers can't rely just on traditional practices. AI, IoT, and
    IoT-enabled sensors are now helping them in solving the challenges we mentioned
    previously, and many others. Lots of practical applications of smart farming are
    in place in Ireland (some of them were presented at the Predict 2018 conference: [https://www.tssg.org/projects/precision-dairy/](https://www.tssg.org/projects/precision-dairy/))
    and more are to be expected across 2019.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking about AI and IoT, edge analytics is another hot topic. Edge analytics,
    which is an alternative to traditional big data analytics that is performed in
    centralized ways, is the analysis of data from some non-central point in a system,
    such as a connected device or sensor. Several real-world applications of edge
    analytics are currently in place, but are not restricted to it, in the industry
    4.0 space ([https://en.wikipedia.org/wiki/Industry_4.0](https://en.wikipedia.org/wiki/Industry_4.0)).
    Analyzing data as it is generated can decrease latency in the decision-making
    process on connected devices.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine, for example, a situation where sensor data from a manufacturing system
    points to the probable failure of a specific part; rules built into a ML or DL
    algorithm interpreting the data at the network edge can automatically shut down
    the machine and send an alert to maintenance managers so that that part can be
    promptly replaced. This can save lot of time compared to transmitting the data
    to a centralize data location for processing and analysis and reduce, if not avoid,
    the risk of unplanned machinery downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Edge analytics also brings benefits in terms of scalability. In those cases
    where the number of connected devices in an organization increases (and the amount
    of generated and collected data too), by pushing algorithms to sensors and network
    devices, it is possible to alleviate the processing strain on enterprise data
    management and centralized analytics systems. There are some promising open source
    projects in this space to keep an eye on. One is DL4J itself; its mobile features
    allow multi-layer neural network model definition, training, and inference on
    Android devices (there's no support for other mobile platforms, since Android
    is the natural choice as it's a DL4J a framework for the JVM). TensorFlow Lite
    ([https://www.tensorflow.org/lite/](https://www.tensorflow.org/lite/)) enables
    on‑device ML inference with low latency and a small binary size on several mobile
    operating systems (Android, iOS, and others) and embedded devices. The latest
    releases of the StreamSets data collector edge ([https://streamsets.com/products/sdc-edge](https://streamsets.com/products/sdc-edge))
    allow you to trigger advanced analytics and ML (TensorFlow) in devices (Linux,
    Android, iOS, Windows, and MacOS are the supported operating systems for it).
    I would expect much more to come from the open source world on this front.
  prefs: []
  type: TYPE_NORMAL
- en: The rise of DL has led researchers to develop hardware chips that can directly
    implement neural network architectures. They are designed to mimic the human brain
    at the hardware level. In a traditional chip, the data needs to be transferred
    between CPUs and storage blocks, while in an neuromorphic chip, data is both processed
    and stored in the chip and can generate synapses when required. This second approach
    results in no time overhead and an energy saving. Therefore, the future of AI
    would most probably be more neuromorphic than based on CPUs or GPUs. With about
    100 billion neurons densely packed into a small volume, the human brain can handle complex
    computations at lightning speed using very little energy. These past few years
    saw brain-inspired algorithms that can do things like identify faces, mimic voices,
    play games, and more. But software is only part of the bigger picture. Our state-of-the-art
    computers can't really run these powerful algorithms. That's where neuromorphic
    computing comes into the game.
  prefs: []
  type: TYPE_NORMAL
- en: The scenarios that have been presented in this section definitely confirm that,
    when considering GDPR or other data regulations, DL and AI definitely wouldn't
    be restricted to useless applications.
  prefs: []
  type: TYPE_NORMAL
- en: Is Spark ready for RL?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we have understood how DL can address several problems
    in computer vision, natural language processing, and time series forecasting.
    This combination of DL with RL can lead to more astonishing applications to solve
    more complex problems. But what is RL? It is a specific area of ML, where agents
    have to take action to maximize the reward in a given environment. The term reinforcement
    comes from the similarity of this learning process to what happens when children
    are incentivized by sweets; the RL algorithms are rewarded when making the right
    decision and penalized when making a wrong one. RL differs from supervised learning,
    where the training data brings the answer key with it and a model is then trained
    with the correct answer itself. In RL, the agents decide what to do to perform
    the given task and, if no training dataset is available, they are tied to learn
    only from their experience.
  prefs: []
  type: TYPE_NORMAL
- en: One of the principal practical applications of RL is in computer gaming (one
    of the best and most popular results is from AlphaGo ([https://deepmind.com/research/alphago/](https://deepmind.com/research/alphago/)),
    from Alphabet's DeepMind team), but it can also be used in other areas such as
    robotics, industrial automation, chatbot systems, autonomous cars, data processing,
    and many others.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the basics of RL before we understand what the availability of
    support for it in Apache Spark is and what it could become.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the main concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agent**: It is the algorithm that takes actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action**: It is one of the possible moves that an agent can make.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discount factor**: It quantifies the difference, in terms of importance,
    between immediate and future rewards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment**: It is the world through which agents move. The environment
    takes the agent''s current state and action as input. It returns the agent reward
    and next state as output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State**: It is a concrete situation in which an agent finds itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward**: It is the feedback by which the success or failure of an agent''s
    action (which makes a transition from one state to another) can be measured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy**: It is the strategy that an agent follows to determine its next
    action, based on the current state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value**: It is the expected long-term return of the current state under a
    given policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Q-value**: It is similar to value, but it also takes into account the current
    action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trajectory**: It is a sequence of states and actions that influence them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can summarize RL as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebe1cc81-df0a-459d-bb4a-6b96cfe1df3c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.2: RL feedback loop'
  prefs: []
  type: TYPE_NORMAL
- en: 'A good example to explain these concepts is the popular Pac-Man video game
    ([https://en.wikipedia.org/wiki/Pac-Man](https://en.wikipedia.org/wiki/Pac-Man)); see
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6280cd9e-4e53-44ba-b03f-eeb290ac771b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3: The Pac-Man video game'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the agent is the Pac-Man character, whose goal is to eat all of the food
    items in a maze, while avoiding some ghosts that try to kill it. The maze is the
    environment for the agent. It receives a reward for eating food and punishment
    (game over) when it gets killed by a ghost. The states are the locations of the
    agent in the maze. The total cumulative reward is the agent winning the game and
    moving to the next level. After starting its exploration, Pac-Man (agent) might
    find one of the four power pellets (which make it invulnerable to the ghosts)
    near the four corners of the maze and decide to spend all its time exploiting
    that discovery by continually going around that small portion of the overall maze
    and never going further into the rest of the environment to pursue the bigger
    prize. To build an optimal policy, the agent faces the dilemma of exploring new
    states while maximizing its reward at the same time. This way, it would then miss
    out on the ultimate reward (moving to the next level). This is called an exploration
    versus exploitation trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular algorithms for RL are the **Markov decision process** (**MDP**): [https://en.wikipedia.org/wiki/Markov_decision_process](https://en.wikipedia.org/wiki/Markov_decision_process),
    **Q-learning** ([https://en.wikipedia.org/wiki/Q-learning](https://en.wikipedia.org/wiki/Q-learning)),
    and **A3C** ([https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Q-learning is widely use in gaming (or gaming-like) spaces. It can be summarized
    with the following equation (the source code is from the Wikipedia page for Q-learning):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f16df61e-ff74-4da3-aba9-809885b22226.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *s[t]* is the state at time *t*, *a[t]* is the action taken by the agent,
    *r[t]* is the reward at time *t*, *s[t+1]* is the new state (time *t+1*), ![](img/645aec1f-4361-4b81-ac15-021c0fae6d8e.png)
    is the learning rate (![](img/653aec36-6dd8-4058-b906-55647168e2d6.png)), and
    ![](img/d2d3c192-68f8-46c1-afc4-962f9294af48.png) is the discount factor. This
    last one determines the importance of future rewards. If it is zero, it will make
    the agent short-sighted because it means that it will only consider current rewards.
    If its value is close to one, the agent will work hard to achieve a long-term
    high reward. If the discount factor value is or exceeds one, then the action values
    could diverge.
  prefs: []
  type: TYPE_NORMAL
- en: The MLLib component of Apache Spark currently doesn't have any facility for
    RL and it seems that there is no plan, at the time of writing this book, to implement
    support for it in future Spark releases. However, there are some open source stable
    initiatives for RL that integrate with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: The DL4J framework provides a specific module for RL, RL4J, which was originally
    a separate project. As for all of the other DL4J components, it is fully integrated
    with Apache Spark. It implements the DQN (Deep Q Learning with double DQN) and
    AC3 RL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Interesting implementations have been done by Yuhao Yang ([https://www.linkedin.com/in/yuhao-yang-8a150232](https://www.linkedin.com/in/yuhao-yang-8a150232))
    from Intel, which led to the analytics zoo initiative ([https://github.com/intel-analytics/analytics-zoo](https://github.com/intel-analytics/analytics-zoo)).
    Here's the link to the presentation he did at the Spark-AI summit 2018 ([https://databricks.com/session/building-deep-reinforcement-learning-applications-on-apache-spark-using-bigdl](https://databricks.com/session/building-deep-reinforcement-learning-applications-on-apache-spark-using-bigdl)). Analytics
    zoo provides a unified analytics and AI platform that seamlessly puts the Spark,
    TensorFlow, Keras, and BigDL programs into an integrated pipeline that can scale
    out to a large Spark cluster for distributed training or inference.
  prefs: []
  type: TYPE_NORMAL
- en: While RL4J, as part of DL4J, provides APIs for the JVM languages (including
    Scala) and BigDL provides APIs for both Python and Scala, a Python-only, end-to-end,
    open source platform for large-scale RL is available from Facebook. The name of
    this platform is Horizon ([https://github.com/facebookresearch/Horizon](https://github.com/facebookresearch/Horizon)).
    It is used by Facebook itself in production to optimize systems in large-scale
    environments. It supports the discrete-action DQN, parametric-action DQN, double
    DQN, DDPG ([https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971)),
    and SAC ([https://arxiv.org/abs/1801.01290](https://arxiv.org/abs/1801.01290))
    algorithms. The workflows and algorithms included in this platform have been built
    on open source frameworks (PyTorch 1.0, Caffe2, and Apache Spark). There's currently
    no support for their use with other popular Python ML frameworks such as TensorFlow
    and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: The Ray framework ([https://ray-project.github.io/](https://ray-project.github.io/))
    by RISELab ([https://rise.cs.berkeley.edu/](https://rise.cs.berkeley.edu/)) deserves
    a special mention. While DL4J and the other frameworks that we mentioned previously
    work in a distributed mode on top of Apache Spark, in the mind of the Berkley
    researchers, Ray is a replacement for Spark itself, which is seen by them as more
    general purpose and not the perfect fit for some real-world AI applications. Ray
    has been implemented in Python; it is fully compatible with the most popular Python
    DL frameworks, including TensorFlow and PyTorch; and it allows us to use a combination
    of more than one of them in the same application.
  prefs: []
  type: TYPE_NORMAL
- en: In the specific case of RL, the Ray framework also provides a dedicated library,
    RLLib ([https://ray.readthedocs.io/en/latest/rllib.html](https://ray.readthedocs.io/en/latest/rllib.html)),
    which implements the AC3, DQN, evolution strategy ([https://en.wikipedia.org/wiki/Evolution_strategy](https://en.wikipedia.org/wiki/Evolution_strategy)),
    and PPO ([https://blog.openai.com/openai-baselines-ppo/](https://blog.openai.com/openai-baselines-ppo/))
    algorithms. At the time of writing this book, I am not aware of any real-world
    AI applications that are using this framework, but I believe it is worth following
    how it is going to evolve and its level of adoption by the industry.
  prefs: []
  type: TYPE_NORMAL
- en: DeepLearning4J future support for GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative Adversarial Networks** (**GANs**) are deep neural network architectures
    that include two nets that are pitted against each other (that''s the reason for
    the *adversarial* adjective in the name). GAN algorithms are used in unsupervised
    machine learning. The main focus for GANs is to generate data from scratch. Among
    the most popular use cases of GANs, there''s image generation from text, image-to-image-translation,
    increasing image resolution to make more realistic pictures, and doing predictions
    on the next frames of videos.'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, a GAN is made up of two deep networks, the **generator**
    and the **discriminator**; the first one generates candidates, while the second
    one evaluates them. Let's see how generative and discriminative algorithms work
    at a very high level. Discriminative algorithms try to classify the input data.
    Therefore, they predict a label or category to which that input data belongs.
    Their only concern is to map features to labels. Generative algorithms, instead
    of predicting a label when given certain features, attempt to predict features
    when given a certain label. Essentially, they do the opposite thing from what
    the discriminative algorithms do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how a GAN works. The generator generates new data instances, while
    the discriminator evaluates them to assess their authenticity. Using the same
    MNIST dataset ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/))
    that has been considered to illustrate more than one code example throughout this
    book, let''s think of a scenario to make it clear what happens in GANs. Suppose
    we have the generator generating an MNIST dataset like hand-written numerals and
    then we''re passing them to the discriminator. The goal of the generator is to
    generate passable hand-written digits without being caught, while the goal of
    the discriminator is to identify those images coming from the generator as fake
    hand-written digits. With reference to the following diagram, these are the steps
    that this GAN takes:'
  prefs: []
  type: TYPE_NORMAL
- en: The generator net takes some random numbers as input and then returns an image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The generated image is used to feed the discriminator net alongside a stream
    of other images that have been taken from the training dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While taking in both real and fake images, the discriminator returns probabilities,
    which are numbers between zero and one. Zero represents a prediction of fake,
    while one represents a prediction of authenticity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ab766101-1770-4c23-a226-532ce41bf581.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.4: The typical flow of the MNIST example GAN'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of implementation, the **Discriminator Net** is a standard CNN that
    can categorize the images fed to it, while the **Generator Net** is an inverse
    CNN. Both nets try to optimize a different and opposing loss function in a zero-sum
    game. This model is essentially an actor-critic model ([https://cs.wmich.edu/~trenary/files/cs5300/RLBook/node66.html](https://cs.wmich.edu/~trenary/files/cs5300/RLBook/node66.html)),
    whereas the **Discriminator Net** changes its behavior, so does the generator
    net, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book, DL4J doesn't provide any direct API for GANs,
    but it allows you to import existing Keras (like those you can find it at [https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN),
    which is our GitHub repository) or TensorFlow (like this one: [https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/gan.py](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/gan.py))
    GAN models and then retrain them and/or make predictions using the DL4J API in
    a JVM environment (which can include Spark), as explained in [Chapter 10](1066b0d4-c2f3-44f9-9cc4-d38469d72c3f.xhtml), *Deploying
    on a Distributed System*, and [Chapter 14](0b58f375-cfc1-4b9e-89d1-437ce6eff839.xhtml),
    *Image Classification*. No direct capabilities for GANs are in the immediate plan
    for DL4J, but the Python model's import is a valid way to train and make inference
    with them.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter wraps up this book. In this book, we got familiar with Apache Spark
    and its components, and then we moved on to discover the fundamentals of DL before
    getting practical. We started our Scala hands-on journey with the DL4J framework
    by understanding how to ingest training and testing data from diverse data sources
    (in both batch and streaming modes) and transform it into vectors through the
    DataVec library. The journey then moved on to exploring the details of CNNs and
    RNNs the implementation of those network models through DL4J, how to train them
    in a distributed and Spark-based environment, how to get useful insights by monitoring
    them using the visual facilities of DL4J, and how to evaluate their efficiency
    and do inference.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned some tips and best practices that we should use when configuring
    a production environment for training, and how it is possible to import Python
    models that have been implemented in Keras and/or TensorFlow and make them run
    (or be retrained) in a JVM-based environment. In the last part of this book, we
    applied what we learned previously to implementing NLP use cases with DL first
    and then an end-to-end image classification application.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that all of the readers who went through all of the chapters of this
    book have reached my initial goal: they have all of the building blocks to start
    tackling their own specific DL use case scenarios in Scala and/or Python, in a
    distributed system such as Apache Spark.
  prefs: []
  type: TYPE_NORMAL
