<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Data Extraction, Transformation, and Loading</h1>
                </header>
            
            <article>
                
<p>Let's discuss the most important part of any machine learning puzzle: data preprocessing and normalization. <em>Garbage in, garbage out</em> would be the most appropriate statement for this situation. The more noise we let pass through, the more undesirable outputs we will receive. Therefore, you need to remove noise and keep signals at the same time.</p>
<p>Another challenge is handling various types of data. We need to convert raw datasets into a suitable format that a neural network can understand and perform scientific computations on. We need to convert data into a numeric vector so that it is understandable to the network and so that computations can be applied with ease. Remember that neural networks are constrained to only one type of data: vectors. </p>
<p>There has to be an approach regarding how data is loaded into a neural network. We cannot put 1 million data records onto a neural network at once <span>– t</span>hat would bring performance down. We are referring to training time when we mention performance here. To increase performance, we need to make use of data pipelines, batch training, and other sampling techniques.</p>
<p><strong>DataVec</strong><span> </span>is an input/output format system that can manage everything that we just mentioned. It solves the biggest headaches that every deep learning puzzle causes.<span> </span>DataVec<span> </span>supports all types of input data, such as text, images, CSV files, and videos.<span> The </span>DataVec library<span> manages the</span> data pipeline in DL4J.</p>
<p>In this chapter, we will learn how to perform ETL operations using DataVec. This is the first step in building a neural network in DL4J.</p>
<p class="mce-root">In this chapter, we will cover the following recipes:</p>
<ul>
<li>Reading and iterating through data</li>
<li>Performing schema transformations</li>
<li>Serializing transforms</li>
<li>Building a transform process</li>
<li>Executing a transform process</li>
<li>Normalizing data for network efficiency</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>Concrete implementations of the use cases that will be discussed in this chapter can be found at </span><span><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app</a>.<br/>
<br/>
After cloning our GitHub repository, navigate to the </span><kbd>Java-Deep-Learning-Cookbook/02_Data_Extraction_Transform_and_Loading/sourceCode</kbd><strong> </strong>directory. Then, import the <kbd>cookbook-app</kbd> proj<span>ect as a Maven project</span><strong> </strong><span>by importing the </span><kbd>pom.xml</kbd><em> </em>file inside the <kbd>cookbook-app</kbd> directory.</p>
<p><span>The datasets that are required for this chapter are located in the <kbd>Chapter02</kbd> root directory (</span><kbd>Java-Deep-Learning-Cookbook/02_Data_Extraction_Transform_and_Loading/</kbd><span>). You may keep it in a different location, for example, your local directory, and then refer to it in the source code accordingly. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading and iterating through data</h1>
                </header>
            
            <article>
                
<p>ETL is an important stage in neural network training since it involves data. Data extraction, transformation, and loading needs to be addressed before we proceed with neural network design. Bad data is a much worse situation than a less efficient neural network. We need to have a basic understanding of the following aspects as well:</p>
<ul>
<li>The type of data you are trying to process</li>
<li>File-handling strategies</li>
</ul>
<p>In this recipe, we will demonstrate how to read and iterate data using DataVec.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>As a prerequisite, make sure that the required Maven dependencies have been added for DataVec in your <kbd>pom.xml</kbd> file, as we mentioned in previous chapter, <em>Configuring Maven for DL4J</em> recipe. </p>
<p>The following is the sample <kbd>pom.xml</kbd> file: <a href="https://github.com/rahul-raj/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/pom.xml">https://github.com/rahul-raj/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/pom.xml</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li><span><span>Manage a range of records using <strong><kbd>FileSplit</kbd></strong></span></span>:</li>
</ol>
<pre style="padding-left: 60px">String[] allowedFormats=new String[]{".JPEG"};<br/> FileSplit fileSplit = new FileSplit(new File("temp"), allowedFormats,true)<br/> </pre>
<div class="packt_infobox" style="padding-left: 60px"><span>You can find the <kbd>FileSplit</kbd> example at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java">https://</a><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java">g</a><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java">ithu</a><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java">b</a><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java">.com</a><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java">/P</a><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java">acktPublishing/Java-Deep-Learnin</a><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java">g-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java</a></span>.</div>
<ol start="2">
<li>Manage the URI collection from a file using<strong> </strong><span><strong><kbd>CollectionInputSplit</kbd></strong>:</span></li>
</ol>
<pre style="padding-left: 60px">FileSplit fileSplit = new FileSplit(new File("temp"));<br/> CollectionInputSplit collectionInputSplit = new CollectionInputSplit(fileSplit.locations());</pre>
<div class="packt_infobox" style="padding-left: 60px"><span>You can find the <kbd>CollectionInputSplit</kbd> example at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/CollectionInputSplitExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/CollectionInputSplitExample.java</a>.<br/></span></div>
<ol start="3">
<li>Use<strong> <span><kbd>NumberedFileInputSplit</kbd></span></strong> <span>to manage data with numbered file</span> <span>formats:</span></li>
</ol>
<pre style="padding-left: 60px">NumberedFileInputSplit numberedFileInputSplit = new NumberedFileInputSplit("numberedfiles/file%d.txt",1,4);<br/> numberedFileInputSplit.locationsIterator().forEachRemaining(System.out::println);</pre>
<div class="packt_infobox" style="padding-left: 60px"><span>You can find the <kbd>NumberedFileInputSplit</kbd> example at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/NumberedFileInputSplitExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/NumberedFileInputSplitExample.java</a>.<br/>
<br/></span></div>
<ol start="4">
<li>Use<strong> <span><kbd>TransformSplit</kbd></span></strong> to map the input URIs to the different output URIs:<strong><span> <br/></span></strong></li>
</ol>
<pre style="padding-left: 60px">TransformSplit.URITransform uriTransform = URI::normalize;<br/> <br/> List&lt;URI&gt; uriList = Arrays.asList(new URI("file://storage/examples/./cats.txt"),<br/> new URI("file://storage/examples//dogs.txt"),<br/> new URI("file://storage/./examples/bear.txt"));<br/> <br/> TransformSplit transformSplit = new TransformSplit(new CollectionInputSplit(uriList),uriTransform);</pre>
<div class="packt_infobox" style="padding-left: 60px">You can find the <kbd>TransformSplit</kbd><span> example at </span><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/TransformSplitExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/TransformSplitExample.java</a>.</div>
<ol start="5">
<li>Perform URI string replacement using <kbd>TransformSplit</kbd>:</li>
</ol>
<pre style="padding-left: 60px">InputSplit transformSplit = TransformSplit.ofSearchReplace(new CollectionInputSplit(inputFiles),"-in.csv","-out.csv");      </pre>
<ol start="6">
<li>Extract the CSV data for the neural network using <kbd>CSVRecordReader</kbd>:</li>
</ol>
<pre style="padding-left: 60px">RecordReader reader = new CSVRecordReader(numOfRowsToSkip,deLimiter);<br/> recordReader.initialize(new FileSplit(file));</pre>
<div class="packt_infobox" style="padding-left: 90px"><span>You can find the <kbd>CSVRecordReader</kbd> example at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/CSVRecordReaderExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/CSVRecordReaderExample.java</a>.<br/>
<br/>
The dataset for this can be found at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/titanic.csv">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/titanic.csv</a>.<br/>
<br/></span></div>
<ol start="7">
<li class="mce-root">Extract image data f<span>or the neural network using </span><kbd>ImageRecordReader</kbd><span>:</span></li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><span>ImageRecordReader imageRecordReader = new ImageRecordReader(imageHeight,imageWidth,channels,parentPathLabelGenerator);<br/></span>imageRecordReader.initialize(trainData,transform);</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 90px">You can find the <kbd>ImageRecordReader</kbd><span> example at </span><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/ImageRecordReaderExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/ImageRecordReaderExample.java</a>.</p>
<ol start="8">
<li>Transform and extract the data using <kbd>TransformProcessRecordReader</kbd>:</li>
</ol>
<pre style="padding-left: 60px">RecordReader recordReader = new TransformProcessRecordReader(recordReader,transformProcess);</pre>
<div class="packt_infobox" style="padding-left: 60px">You can find the <kbd>TransformProcessRecordReader</kbd> example at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/TransformProcessRecordReaderExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/TransformProcessRecordReaderExample.java<br/>
<br/></a> The dataset for this can be found at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/transform-data.csv">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/transform-data.csv</a>.</div>
<ol start="9">
<li>Extract the sequence data using <kbd>SequenceRecordReader</kbd> and <kbd>CodecRecordReader</kbd>:</li>
</ol>
<pre style="padding-left: 60px">RecordReader codecReader = new CodecRecordReader();<br/> codecReader.initialize(conf,split);</pre>
<div class="packt_infobox" style="padding-left: 90px">You can find the <kbd>CodecRecordReader</kbd> example at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/CodecReaderExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/CodecReaderExample.java</a><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/CodecReaderExample.java">.</a></div>
<p style="padding-left: 90px">The following code shows how to use <kbd>RegexSequenceRecordReader</kbd>:</p>
<pre style="padding-left: 60px">RecordReader recordReader = new RegexSequenceRecordReader((\d{2}/\d{2}/\d{2}) (\d{2}:\d{2}:\d{2}) ([A-Z]) (.*)",skipNumLines);<br/> recordReader.initialize(new NumberedFileInputSplit(path/log%d.txt));</pre>
<div class="packt_infobox" style="padding-left: 60px">You can find the <kbd>RegexSequenceRecordReader</kbd> example at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/RegexSequenceRecordReaderExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/RegexSequenceRecordReaderExample.java</a>.<br/>
<br/>
<span>The dataset for this can be found at </span><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/logdata.zip">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/logdata.zip</a><span>.</span></div>
<p style="padding-left: 60px">The following code shows how to use <kbd>CSVSequenceRecordReader</kbd>:</p>
<pre style="padding-left: 60px">CSVSequenceRecordReader seqReader = new CSVSequenceRecordReader(skipNumLines, delimiter);<br/> seqReader.initialize(new FileSplit(file));</pre>
<div class="packt_infobox" style="padding-left: 60px">You can find the <kbd>CSVSequenceRecordReader</kbd> example at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/SequenceRecordReaderExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/SequenceRecordReaderExample.java</a>.<br/>
<br/>
<span>The dataset for this can be found at </span><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/dataset.zip">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/dataset.zip</a><span>.</span></div>
<ol start="10">
<li>Extract the JSON/XML/YAML data using <strong><kbd><span>JacksonLineRecordReader<kbd>:</kbd></span></kbd></strong></li>
</ol>
<pre style="padding-left: 60px">RecordReader recordReader = new JacksonLineRecordReader(fieldSelection, new ObjectMapper(new JsonFactory()));<br/> recordReader.initialize(new FileSplit(new File("json_file.txt")));</pre>
<div class="packt_infobox" style="padding-left: 60px">You can find the <kbd>JacksonLineRecordReader</kbd> example at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/JacksonLineRecordReaderExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/JacksonLineRecordReaderExample.java</a>.<br/>
<br/>
The dataset for this can be found at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/irisdata.txt">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/irisdata.txt</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Data can be spread across multiple files, subdirectories, or multiple clusters. We need a mechanism to extract and handle data in different ways due to various constraints, such as size. In distributed environments, a large amount of data can be stored as chunks in multiple clusters. DataVec uses <kbd>InputSplit</kbd> for this purpose.</p>
<p>In step 1, we looked at <kbd>FileSplit</kbd>, an <kbd>InputSplit</kbd> implementation that splits the root directory into files. <span><kbd>FileSplit</kbd> </span>will recursively look for files inside the specified directory location. You can also pass an array of strings as a parameter to denote the allowed extensions:</p>
<ul>
<li><span><strong>Sample input</strong></span>:<span><strong> </strong>A directory location with files:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1198 image-border" src="assets/9d46081e-a227-496a-b7a1-0a42637fd175.png" style="width:16.75em;height:15.00em;"/></p>
<ul>
<li><span><strong>Sample output</strong>: A list of URIs with the filter applied:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1199 image-border" src="assets/1e782ca9-2cda-437e-a4ef-8e439d440b05.png" style="width:82.17em;height:8.00em;"/></p>
<p><span>In the sample output, we removed any file paths that are not in </span><kbd>.jpeg</kbd> format. <kbd>CollectionInputSplit</kbd> would be useful here if you want to extract data from a list of URIs, like we did in step 2. In step 2, the <span><kbd>temp</kbd> directory has a list of files in it. We used <kbd>CollectionInputSplit</kbd> to generate a list of URIs from the files. While <kbd>FileSplit</kbd> is specifically for splitting the directory into files (a list of URIs), <kbd>CollectionInputSplit</kbd> is a simple <kbd>InputSplit</kbd> implementation that handles a collection of URI inputs. If we already have a list of URIs to process, then we can simply use <kbd>CollectionInputSplit</kbd> instead of <kbd>FileSplit</kbd>. </span></p>
<ul>
<li><span><strong>Sample input</strong></span><span>:</span> <span>A directory location with files. Refer to the following screenshot (directory with image files as input):</span><span><br/></span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1200 image-border" src="assets/0c1f8ed8-7480-4aca-b2ae-bf2acc32172f.png" style="width:14.50em;height:14.83em;"/></p>
<ul>
<li class="CDPAlignLeft CDPAlign"><span><strong>Sample output</strong></span>:<span> A list of URIs. Refer to the following list of URIs generated by <kbd>CollectionInputSplit</kbd> from the earlier mentioned input.</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1201 image-border" src="assets/8de37b76-2610-4b79-b76f-c5c30b1c21fc.png" style="width:91.67em;height:13.25em;"/></p>
<p>In step 3, <kbd>NumberedFileInputSplit</kbd><span> </span>generates URIs based on the specified numbering format. </p>
<p class="mce-root"/>
<p>Note that we need to pass an appropriate regular expression pattern to generate filenames in a sequential format. Otherwise, it will throw runtime errors. <span>A regular expression allows us to accept inputs in various numbered formats.</span> <kbd>NumberedFileInputSplit</kbd> will generate a list of URIs that you can pass down the level in order to extract and process data. We added the <kbd>%d</kbd> regular expression at the end of file name to specify that numbering is present at the trailing end. </p>
<ul>
<li><strong>Sample input</strong><span>:</span><span> </span><span>A directory location with files</span> <span>in a numbered naming format, for example, </span><kbd>file1.txt</kbd>, <kbd>file2.txt</kbd>, and <kbd>file3.txt</kbd><span>.</span></li>
<li><strong>Sample output</strong><span>: </span><span>A list of URIs:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1202 image-border" src="assets/7615f0ca-4974-470f-a2e6-04b1ac481993.png" style="width:36.83em;height:12.58em;"/></p>
<p>If you need to map input URIs to different output URIs, then you will need <kbd>TransformSplit</kbd>. We used it in step 4 to normalize/transform the data URI into the required format. It will be especially helpful if features and labels are kept at different locations. When step 4 is executed, t<span>he</span> <kbd>"."</kbd><span> string will be stripped from the URIs, which results in the following URIs:</span></p>
<ul>
<li><strong>Sample input</strong>: A collection of URIs, just like what we saw in <span><kbd>CollectionInputSplit</kbd>. However, <kbd>TransformSplit</kbd> can accept erroneous URIs:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1203 image-border" src="assets/9b26e276-e149-4fc0-ad68-2d53fe43a441.png" style="width:51.33em;height:6.17em;"/></p>
<ul>
<li><strong>Sample output</strong>: A list of URIs after formatting them:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1204 image-border" src="assets/c44c2da0-a1dc-4526-851e-53c06731b8eb.png" style="width:36.33em;height:11.50em;"/></p>
<p>After executing step 5, the <kbd>-in.csv</kbd> substrings in the <span>URIs will</span><span> </span><span>be</span><span> replaced with <kbd>-out.csv</kbd>.</span></p>
<p><span><kbd>CSVRecordReader</kbd> is a simple CSV record reader for streaming CSV data. We can form data stream objects based on the delimiters and specify various other parameters, such as the number of lines to skip from the beginning. In step 6, we used <kbd>CSVRecordReader</kbd> for the same. <br/></span></p>
<div class="packt_infobox">For the <kbd>CSVRecordReader</kbd> example, use the <kbd>titanic.csv</kbd> file that's included in this chapter's GitHub repository. You need to update the directory path in the code to be able to use it.</div>
<p><span><kbd>ImageRecordReader</kbd> is an image record reader that's used for streaming image data.</span></p>
<p><span>In step 7, we read images from a local filesystem. Then, we scaled them and converted them according to a given height, width, and channels. We can also specify the labels that are to be tagged for the image data. In order to specify the labels for the image set, create a separate subdirectory under the root. Each of them represents a label. </span></p>
<p><span>In step 7, the first two parameters from the <kbd>ImageRecordReader</kbd> constructor represent the height and width to which images are to be scaled. We usually give a value of 3 for channels representing R, G, and B. <kbd>parentPathLabelGenerator</kbd> will define how to tag labels in images. <kbd>trainData</kbd> is the <kbd>inputSplit</kbd> we need in order to specify the range of records to load, while <kbd>transform</kbd> is the image transformation to be applied while loading images.</span></p>
<div class="packt_infobox"><span>For the</span> <kbd>ImageRecordReader</kbd><span> </span><span>example, you can download some sample images from</span> <kbd>ImageNet</kbd><span>. Each category of images will be represented by a subdirectory. For example, you can download dog images and put them under a subdirectory named "dog". You will need to provide the parent directory path where all the possible categories will be included. <br/>
<br/></span> The ImageNet website can be found at<span> <a href="http://www.image-net.org/">http://www.image-net.org/</a>.</span></div>
<p><span><kbd>TransformProcessRecordReader</kbd> requires a bit of explanation when it's used in the schema transformation process. <kbd>TransformProcessRecordReader</kbd> is the end product of applying schema transformation to a record reader. This will ensure that a defined transformation process is applied before it is fed to the training data.</span></p>
<p><span>In step 8, <kbd>transformProcess</kbd> defines an ordered list of transformations to be applied to the given dataset. This can be the removal of unwanted features, feature data type conversions, and so on. The intent is to make the data suitable for the neural network to process further. You will learn how to create a transformation process in the upcoming recipes in this chapter. <br/></span></p>
<div class="packt_infobox"><span>For the</span> <kbd>TransformProcessRecordReader</kbd><span> example,</span> use the <kbd>transform-data.csv</kbd><span> file that's included in this chapter's GitHub repository. You need to update the file path in code to be able to use it.</span></div>
<p><span>In step 9, we looked at some of the implementations of <kbd>SequenceRecordReader</kbd>. We use this record reader if we have a sequence of records to process. This record reader can be used locally as well as in distributed environments (such as Spark).<br/></span></p>
<div class="packt_infobox">For the <kbd>SequenceRecordReader</kbd> <span>example, you need to extract the </span><kbd>dataset.zip</kbd> file <span>from this chapter's GitHub repository. After the extraction, you will see two subdirectories underneath:</span> <kbd>features</kbd> <span>and</span> <kbd>labels</kbd><span>. In each of them, there is a sequence of files. You need to provide the absolute path to these two directories in the code. </span></div>
<p><span><kbd>CodecRecordReader</kbd> is a record reader that handle multimedia datasets and can be used for the following purposes:<br/></span></p>
<ul>
<li><span>H.264 (AVC) main profile decoder</span></li>
<li><span>MP3 decoder/encoder</span></li>
<li><span><span>Apple ProRes decoder and encoder</span></span></li>
<li><span>H264 Baseline profile encoder</span></li>
<li><span>Matroska (MKV) demuxer and muxer</span></li>
<li><span>MP4 (ISO BMF, QuickTime) demuxer/muxer and tools</span></li>
<li><span>MPEG 1/2 decoder</span></li>
<li><span>MPEG PS/TS demuxer</span></li>
<li><span>Java player applet parsing</span></li>
<li><span>VP8 encoder</span></li>
<li><span>MXF demuxer</span></li>
</ul>
<p><kbd>CodecRecordReader</kbd> makes use of jcodec as the underlying media parser.</p>
<div class="packt_infobox">For the <kbd>CodecRecordReader</kbd> <span>example, you need to provide the directory location of a short video file in the code. This video file will be the input for</span> the <kbd>CodecRecordReader</kbd> <span>example. </span></div>
<p><kbd>RegexSequenceRecordReader</kbd> will consider the entire file as a single sequence and will read it one line at a time. Then, it will split each of them using the specified regular expression. We can combine <span><kbd>RegexSequenceRecordReader</kbd> with <kbd>NumberedFileInputSplit</kbd> to read file sequences. In step 9, we used <kbd>RegexSequenceRecordReader</kbd> to read the transactional logs that were recorded over the time steps (time series data). In our dataset (</span><kbd>logdata.zip</kbd><span>), transactional logs are unsupervised data with no specification for features or labels.<br/></span></p>
<div class="packt_infobox"><span>For the</span> <kbd>RegexSequenceRecordReader</kbd><span> example, you need to extract the </span><kbd>logdata.zip</kbd><span> file from this chapter's GitHub repository. After the extraction, you will see a sequence of transactional logs with a numbered file naming format. You need to provide the absolute path to the extracted directory in the code. </span></div>
<p><kbd>CSVSequenceRecordReader</kbd> reads the sequences of data in CSV format. Each sequence represents a separate CSV file. Each line represents one time step.</p>
<p>In step 10, <kbd>JacksonLineRecordReader</kbd> will read the JSON/XML/YAML data line by line. It expects a valid JSON entry for each of the lines without a separator at the end. This follows the Hadoop convention of ensuring that the split works properly in a cluster environment. If the record spans multiple lines, the split won't work as expected and may result in calculation errors. Unlike <kbd>JacksonRecordReader</kbd>, <kbd>JacksonLineRecordReader</kbd> doesn't create the labels automatically and will require you to mention the configuration during training.</p>
<p class="mce-root"/>
<div class="packt_infobox">For the <kbd>JacksonLineRecordReader</kbd><strong> </strong>example, you need to provide the directory location of <kbd>irisdata.txt</kbd>, which is located in <span>this chapter's GitHub</span> repository. In the <kbd>irisdata.txt</kbd><strong> </strong>file, each line represents a JSON object. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span><kbd>JacksonRecordReader</kbd> is a record reader that uses the</span> Jackson API. Just <span>like <kbd>JacksonLineRecordReader</kbd>, it also supports JSON, XML, and YAML formats. For <kbd>JacksonRecordReader</kbd>, the user needs to provide a list of fields to read from the JSON/XML/YAML file. This may look complicated, but it allows us to parse the files under the following conditions:<br/></span></p>
<ul>
<li>There is no consistent schema for the JSON/XML/YAML data. The order of output fields can be provided using the <kbd>FieldSelection</kbd> object.</li>
<li>There are fields that are missing in some files but that can be provided using the <kbd>FieldSelection</kbd><span> object.</span></li>
</ul>
<p> <span><kbd>JacksonRecordReader</kbd> </span>can also be used with <span><kbd>PathLabelGenerator</kbd> to append the label based on the file path.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing schema transformations</h1>
                </header>
            
            <article>
                
<p class="mce-root">Data transformation is an important data normalization process. It's a possibility that bad data occurs, such as duplicates, missing values, non-numeric features, and so on. We need to normalize them by applying schema transformation so that data can be processed in a neural network. A neural network can only process numeric features. In this recipe, we will demonstrate the schema creation process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li><strong>Identify the outliers in the data</strong>:<strong> </strong>For a small dataset with just a few features, we can spot outliers/noise via manual inspection. For a dataset with a large number of features, we can perform <strong>Principal Component Analysis</strong> (<strong>PCA</strong>), as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">INDArray factor = org.nd4j.linalg.dimensionalityreduction.PCA.pca_factor(inputFeatures, projectedDimension, normalize);<br/> INDArray reduced = inputFeatures.mmul(factor);</pre>
<ol start="2">
<li><strong>Use a schema to define the structure of the data</strong>: The following is an example of a basic schema for a customer churn dataset. You can download the dataset from <a href="https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling/downloads/bank-customer-churn-modeling.zip/1">https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling/downloads/bank-customer-churn-modeling.zip/1</a>:<a href="https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling/downloads/bank-customer-churn-modeling.zip/1"/></li>
</ol>
<pre style="padding-left: 60px">  Schema schema = new Schema.Builder()<br/> .addColumnString("RowNumber")<br/> .addColumnInteger("CustomerId")<br/> .addColumnString("Surname")<br/> .addColumnInteger("CreditScore")<br/> .addColumnCategorical("Geography",  <br/>  Arrays.asList("France","Germany","Spain"))<br/> .addColumnCategorical("Gender", Arrays.asList("Male","Female"))<br/> .addColumnsInteger("Age", "Tenure")<br/> .addColumnDouble("Balance")<br/> .addColumnsInteger("NumOfProducts","HasCrCard","IsActiveMember")<br/> .addColumnDouble("EstimatedSalary")<br/> .build();</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">Before we start schema creation, we need to examine all the features in our dataset. Then, we need to clear all the noisy features, such as n</span><span class="s2">ame</span><span class="s1">, where it is fair to assume that they have no effect on the produced outcome. If some features are unclear to you, just keep them as such and include them in the schema. If you remove a feature that happens to be a signal unknowingly, then you'll degrade the efficiency of the neural network. This process of removing outliers and keeping signals (valid features) is referred to in step 1. <span><strong>Principal Component Analysis</strong> (<strong>PCA</strong>) </span>would be an ideal choice, and the same has been implemented in ND4J. The <strong>PCA</strong> class can perform dimensionality reduction in the case of a dataset with a large number of features where you want to reduce the number of features to reduce the complexity. Reducing the features just means removing irrelevant features (outliers/noise). In step 1, we generated a PCA factor matrix by calling <span><kbd>pca_factor()</kbd> with the following arguments:</span></span></p>
<ul>
<li class="p1"><span class="s1"><span><kbd>inputFeatures</kbd>: Input features as a matrix</span></span></li>
<li class="p1"><span class="s1"><span><kbd>projectedDimension</kbd>: The number of features to project from the actual set of features (for example, 100 important features out of 1,000)</span></span></li>
<li class="p1"><span class="s1"><span><kbd>normalize</kbd>: A Boolean varia</span></span>ble (true/false) ind<span class="s1"><span>icating whether the features are to be normalized (zero mean) </span></span></li>
</ul>
<p class="mce-root"/>
<p><span class="s1"><span>Matrix multiplication is performed by calling the <kbd>mmul()</kbd> method and the end result. <kbd>reduced</kbd> is the feature matrix that we use after performing the dimensionality reduction based on the PCA factor. Note that you may need to perform multiple training sessions using input features (which are generated using the PCA factor) to understand signals.</span></span></p>
<p class="p1"><span class="s1">In step 2, we used the customer churn dataset (the simple dataset that we used in the next chapter) to demonstrate the <kbd>Schema</kbd> creation process. The data types that are mentioned in the schema are for the respective features or labels. For example, if you want to add a schema definition for an integer feature, then it would be </span><kbd><span class="s2">addColumnInteger()</span></kbd><span class="s1">. Similarly, there are other <kbd>Schema</kbd> methods available that we can use to manage other data types.</span></p>
<p class="p1"><span class="s1">Categorical variables can be added using </span><kbd><span class="s2">addColumnCategorical()</span></kbd><span class="s1">, as we mentioned in step 2. Here, we marked the categorical variables and the possible values were supplied. Even if we get a masked set of features, we can still construct their schema if the features are arranged in numbered format (</span>for example, <kbd>column1</kbd>, <kbd>column2</kbd>, and similar).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>In a nutshell, here is what you need to do to build the schema for your datasets:</p>
<ul>
<li>Understand your data well. Identify the noise and signals.</li>
<li>Capture features and labels. Identify categorical variables.</li>
<li>Identify categorical features that one-hot encoding can be applied to.</li>
<li>Pay attention to missing or bad data.</li>
<li>Add features using type-specific methods such as <kbd>addColumnInteger()</kbd> and <kbd>addColumnsInteger()</kbd>, where the feature type is an integer. Apply the respective <kbd>Builder</kbd> method to other data types.</li>
<li>Add categorical variables using <kbd>addColumnCategorical()</kbd>.</li>
<li>Call the <kbd>build()</kbd> method to build the schema.</li>
</ul>
<p class="CDPAlignLeft CDPAlign">Note that you cannot skip/ignore any features from the dataset without specifying them in the s<span><span>chema</span></span>. You need to remove the outlying features from the dataset, create a schema from the remaining features, and then move on to the transformation process for further processing. Alternatively, you can keep all the features aside, keep all the features in the schema, and then define the outliers during the transformation process.</p>
<p class="CDPAlignLeft CDPAlign">When it comes to feature engineering/data analysis, DataVec comes up with its own analytic engine to perform data analysis on feature/target variables. For local executions, we can make use of <kbd>AnalyzeLocal</kbd> to return a data analysis object that holds information about each column in the dataset. Here is how you can create a data analysis object from a record reader object:</p>
<pre>DataAnalysis analysis = AnalyzeLocal.analyze(mySchema, csvRecordReader);<br/> System.out.println(analysis);</pre>
<p>You can also analyze your dataset for missing values and check whether it is schema-compliant by calling <kbd>analyzeQuality()</kbd>:</p>
<pre>DataQualityAnalysis quality = AnalyzeLocal.analyzeQuality(mySchema, csvRecordReader);<br/> System.out.println(quality);</pre>
<p>For sequence data, you need to use <kbd>analyzeQualitySequence()</kbd> instead of <span><kbd>analyzeQuality()</kbd>. For data analysis on Spark, you can make use of the <kbd>AnalyzeSpark</kbd> utility class in place of <kbd>AnalyzeLocal</kbd>. <br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a transformation process</h1>
                </header>
            
            <article>
                
<p>The next step after schema creation is to define a data transformation process by adding all the required transformations. We can manage an ordered list of transformations using <kbd>TransformProcess</kbd>. During the schema creation process, we only defined a structure for the data with all its existing features and didn't really perform transformation. <span>Let's look at how we can transform the features in the datasets from a non-numeric format into a numeric format. Neural networks cannot understand raw data unless it is mapped to numeric vectors. </span>In this recipe, we will build a transformation process from the given schema.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Add a list of transformations to <kbd>TransformProcess</kbd>. Consider the following example:</li>
</ol>
<pre style="padding-left: 60px">TransformProcess transformProcess = new TransformProcess.Builder(schema)<br/> .removeColumns("RowNumber","CustomerId","Surname")<br/> .categoricalToInteger("Gender")<br/> .categoricalToOneHot("Geography")<br/> .removeColumns("Geography[France]")<br/> .build();</pre>
<ol start="2">
<li>Create a record reader using <kbd>TransformProcessRecordReader</kbd> to extract and transform the data:<strong><br/></strong></li>
</ol>
<pre style="padding-left: 60px">TransformProcessRecordReader transformProcessRecordReader = new TransformProcessRecordReader(recordReader,transformProcess);<strong><br/></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In step 1, we added all the transformations that are needed for the dataset. <kbd>TransformProcess</kbd> defines an unordered list of all the transformations that we want to apply to the dataset. We removed any unnecessary features by calling <kbd><span>removeColumns()</span></kbd>. During schema creation, we marked the categorical features in the <kbd>Schema</kbd>. Now, we can actually decide on what kind of transformation is required for a particular categorical variable. Categorical variables can be converted into integers by calling <span><kbd>categoricalToInteger()</kbd>. Categorical variables can undergo one-hot encoding if we call <kbd>categoricalToOneHot()</kbd>. Note that the schema needs to be created prior to the transformation process. We need the schema to create a <strong><kbd>TransformProcess</kbd></strong>.<br/>
In step 2, we apply the transformations that were added before with the help of <strong><kbd><span class="pl-smi">TransformProcessRecordReader</span></kbd></strong>. All we need to do is create the basic record reader object with the raw data and pass it to <strong><kbd><span class="pl-smi">TransformProcessRecordReader</span></kbd></strong>, along with the defined transformation process. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>DataVec allows us to do much more within the transformation stage.<span> Here are some of the other important transformation features that are available within</span><span> </span><kbd>TransformProcess</kbd><span>:</span></p>
<ul>
<li><span><kbd>addConstantColumn()</kbd>: Adds a new column in a dataset, where all the values in the column are identical and are as they were specified by the value. This method accepts three attributes:</span><span> the </span>new column name, the new column type, and the value.</li>
<li><span><kbd>appendStringColumnTransform()</kbd>: Appends a string to the specified column. This method accepts two attributes:</span><span> the </span>column to append to and the string value to append.</li>
<li><span><kbd>conditionalCopyValueTransform()</kbd>: Replaces the value in a column with the value specified in another column if a condition is satisfied. This method accepts three attributes:</span><span> the </span>column to replace the values, the column to refer to the values, and the condition to be used.</li>
<li><span><kbd>conditionalReplaceValueTransform()</kbd>: Replaces the value in a column with the specified value if a condition is satisfied. This method accepts three attributes: the </span>column to replace the values, the value to be used as a replacement, and the condition to be used.</li>
<li><span><kbd>conditionalReplaceValueTransformWithDefault()</kbd>: Replaces the value in a column with the specified value if a condition is satisfied. Otherwise, it fills the column with another value. This method accepts four attributes: the </span>column to replace the values, the value to be used if the condition is satisfied, the value to be used if the condition is not satisfied, and the condition to be used.<span><br/>
We can use built-in conditions that have been written in DataVec with the transformation process or data cleaning process. We can use <kbd>NaNColumnCondition</kbd> to replace</span><span> </span><kbd>NaN</kbd><span> </span><span>values and <kbd>NullWritableColumnCondition</kbd> to replace null values, respectively.</span></li>
<li><span><kbd>stringToTimeTransform()</kbd>: Converts a string column into a time column. This targets date columns that are saved as a string/object in the dataset. This method accepts three attributes: the </span>name of the column to be used, the time format to be followed, and the time zone.</li>
<li><span><kbd>reorderColumns()</kbd>: Reorders the columns using the newly defined order. We can provide the column names in the specified order as attributes to this method.</span></li>
<li><span><kbd>filter ()</kbd>: Defines a filter process based on the specified condition. If the condition is satisfied, remove the example or sequence; otherwise, keep the examples or sequence. This method accepts only a single attribute, which is the condition/filter to be applied. The </span><kbd>filter()</kbd><span> </span><span>method is very useful for the data cleaning process. If we want to remove <kbd>NaN</kbd> values from a specified column, we can create a filter, as follows:</span></li>
</ul>
<pre style="padding-left: 60px">Filter filter = new ConditionFilter(new NaNColumnCondition("columnName"));<span><br/></span></pre>
<p style="padding-left: 90px"><span>If we want to remove null values from a specified column, we can create a filter, as follows:</span></p>
<pre style="padding-left: 90px">Filter filter =  new ConditionFilter(new NullWritableColumnCondition("columnName")); <span> </span><span><br/></span></pre>
<p class="mce-root"/>
<ul>
<li><span><kbd>stringRemoveWhitespaceTransform()</kbd>: This method removes whitespace characters from the value of a column. This method accepts only a single attribute, which is the column from which whitespace is to be trimmed.</span></li>
<li><span><kbd>integerMathOp()</kbd>: This method is used to perform a mathematical operation on an integer column with a scalar value. Similar methods are available for types such as <kbd>double</kbd> and</span><span> </span><kbd>long</kbd>.<span> </span><span>This method accepts three attributes:</span><span> </span>the integer column to apply the mathematical operation on, the mathematical operation itself, and the scalar value to be used for the mathematical operation.<span><br/></span></li>
</ul>
<div class="packt_tip"><kbd>TransformProcess</kbd><span> </span><span>is not just meant for data handling – it can also be used to overcome memory bottlenecks by a margin.</span></div>
<p><span>Refer to the DL4J API documentation to find more p</span>owerful DataVec features <span>for your data analysis tasks. There are other interesting operations supported in <kbd>TransformPorocess</kbd>, such as <kbd>reduce()</kbd> and <kbd>convertToString()</kbd>. </span>If you're a data analyst, then you should know that many of the data normalization strategies can be applied during this stage. <span>You can refer to the DL4J API documentation for more information on the normalization strategies that are available o</span>n <a href="https://deeplearning4j.org/docs/latest/datavec-normalization" target="_blank">https://deeplearning4j.org/docs/latest/datavec-normalization</a>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Serializing transforms</h1>
                </header>
            
            <article>
                
<p>DataVec gives us the ability to serialize the transforms so that they're portable for production environments. <span>In this recipe, we will serialize the transformation process. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Serialize the transforms into a human-readable format. We can transform to JSON using <kbd>TransformProcess</kbd> as follow<span>s:</span></li>
</ol>
<pre style="padding-left: 60px">String serializedTransformString = transformProcess.toJson()</pre>
<p style="padding-left: 60px">We can transform to YAML using <kbd>TransformProcess</kbd> as fo<span>llows:</span></p>
<pre style="padding-left: 60px">String serializedTransformString = transformProcess.toYaml()</pre>
<div class="packt_infobox" style="padding-left: 60px"><span>You can find an example of this at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/SerializationExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/SerializationExample.java</a>.<br/></span></div>
<ol start="2">
<li>Deserialize the transforms for JSON to <strong><kbd>TransformProcess</kbd></strong><span> as follows:</span></li>
</ol>
<pre style="padding-left: 60px">TransformProcess tp = TransformProcess.fromJson(serializedTransformString)<span><br/></span></pre>
<p style="padding-left: 60px">You can do the same for YAML to <kbd>TransformProcess</kbd> as follows:</p>
<pre style="padding-left: 60px">TransformProcess tp = TransformProcess.fromYaml(serializedTransformString)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, <kbd>toJson()</kbd> converts <kbd>TransformProcess</kbd> into a JSON string, while <kbd>toYaml()</kbd> converts<span> <kbd>TransformProcess</kbd> into a</span> YAML strin<span>g.</span></p>
<p>Both of these methods can be used for the serialization of <kbd>TransformProcess</kbd>. </p>
<p>In step 2, <kbd>fromJson()</kbd> deserializes a JSON string into a <kbd>TransformProcess</kbd>, while <kbd>fromYaml()</kbd> deserializes a YAML string into a <kbd>TransformProcess</kbd>. </p>
<p><kbd>serializedTransformString</kbd> is the JSON/YAML string that needs to be converted into a <kbd>TrasformProcess</kbd>. </p>
<p>This recipe is relevant while the application is being migrated to a different platform. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Executing a transform process</h1>
                </header>
            
            <article>
                
<p>After the transformation process has been defined, we can execute it in a controlled pipeline. It can be executed using batch processing, or we can distribute the effort to a Spark cluster. Previously, we look at <kbd>TransformProcessRecordReader</kbd>, which automatically does the transformation in the background. We cannot feed and execute the data if the dataset is huge. Effort can be distributed to a Spark cluster for a larger dataset. You can also perform regular local execution. In this recipe, we will discuss how to execute a transform process locally as well as remotely.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Load the dataset into <strong><kbd>RecordReader</kbd></strong>.<strong> </strong><span>Load the CSV data in the case of</span> <kbd>CSVRecordReader</kbd><span>:</span></li>
</ol>
<pre style="padding-left: 60px">RecordReader reader = new CSVRecordReader(0,',');<br/> reader.initialize(new FileSplit(file));<span> <br/></span></pre>
<ol start="2">
<li>Execute the transforms in local using <kbd>LocalTransformExecutor</kbd>:<strong> </strong></li>
</ol>
<pre style="padding-left: 60px">List&lt;List&lt;Writable&gt;&gt; transformed = LocalTransformExecutor.execute(recordReader, transformProcess)<strong><br/></strong></pre>
<ol start="3">
<li>Execute the transforms in Spark using <kbd>SparkTransformExecutor</kbd>:<strong> </strong></li>
</ol>
<pre style="padding-left: 60px">JavaRDD&lt;List&lt;Writable&gt;&gt; transformed = SparkTransformExecutor.execute(inputRdd, transformProcess)<strong><br/></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we load the dataset into a record reader object. For demonstration purposes, we used <kbd>CSVRecordReader</kbd>. </p>
<p>In step 2, t<span class="s1">he </span><kbd><span class="s2">execute()</span></kbd><span class="s1"> method can only be used if </span><kbd><span class="s2">TransformProcess</span></kbd><span class="s1"> returns</span> non-sequential da<span class="s1">ta. For local execution, it is assumed that you have loaded the dataset into a <kbd>RecordReader</kbd>. </span></p>
<p><span class="s1">For the </span><kbd>LocalTransformExecutor</kbd> example, please refer to the <kbd>LocalExecuteExample.java</kbd> file from this source:<br/>
<a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/executorexamples/LocalExecuteExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/executorexamples/LocalExecuteExample.java</a>.</p>
<div class="packt_infobox">For the <kbd>LocalTransformExecutor</kbd><span> example, you need to provide a file path for</span> <kbd>titanic.csv</kbd><span>. It is located in this chapter's GitHub directory.</span></div>
<p><span class="s1">In step 3, it is assumed that you have loaded the dataset into a</span> JavaRDD <span class="s1">object since we need to execute</span> the DataVec <span class="s1">transform process in a Spark cluster. Also, the <kbd><span class="s2">execute()</span></kbd> method can only be used if <kbd><span class="s2">TransformProcess</span></kbd> returns</span> non-sequential data.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">If </span><span class="s2"><kbd>TransformProcess</kbd></span><span class="s1"> returns</span> sequential data, then use the <kbd>executeSequence()</kbd> method instead:</p>
<pre class="p2">List&lt;List&lt;List&lt;Writable&gt;&gt;&gt; transformed = LocalTransformExecutor.executeSequence(sequenceRecordReader, transformProcess)<span class="s1"><br/></span></pre>
<p class="p1"><span class="s1">If you need to join two record readers based on</span> <kbd>joinCondition</kbd>, <span class="s1">then you need the </span><kbd><span class="s2">executeJoin()</span></kbd><span class="s1"> method:</span></p>
<pre class="p4">List&lt;List&lt;Writable&gt;&gt; transformed = LocalTransformExecutor.executeJoin(joinCondition, leftReader, rightReader)<span class="s3"> <br/></span></pre>
<p>The following is an overview of local/Spark executor methods:</p>
<ul class="ul1">
<li class="li1"><kbd><span class="s2">execute()</span></kbd><span class="s3">: This applies the transformation to the record reader. </span><kbd><span class="s2">LocalTransformExecutor</span></kbd><span class="s3"> takes the record reader as input, while </span><kbd><span class="s2">SparkTransformExecutor</span></kbd><span class="s3"> needs the input data to be loaded into a </span><span class="s2">JavaRDD</span><span class="s3"> object. This cannot be used for sequential data.<br/></span></li>
<li class="li1"><kbd><span class="s2">executeSequence()</span></kbd><span class="s3">: This applies the transformation to a sequence reader. However, the transform process should start with non-sequential data and then convert it into sequential data.</span></li>
<li class="li1"><kbd><span class="s2">executeJoin()</span></kbd><span class="s3">: This method is used for joining two different input readers based on</span> <kbd><span class="s2">joinCondition</span></kbd><span class="s3">.</span></li>
<li class="li1"><kbd><span class="s2">executeSequenceToSeparate()</span></kbd><span class="s3">: This applies the transformation to a sequence reader. However, the transform process should start with sequential data and return non-sequential data.</span></li>
<li class="li1"><kbd><span class="s2">executeSequenceToSequence()</span></kbd><span class="s3">: This applies the transformation to a sequence reader. However, the transform process should start with sequential data and return sequential data.</span></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalizing data for network efficiency</h1>
                </header>
            
            <article>
                
<p>Normalization makes a neural network's job much easier. It helps the neural network treat all the features the same, irrespective of their range of values. The main goal of normalization is to arrange the numeric values in a dataset on a common scale without actually disturbing the difference in the range of values. Not all datasets require a normalization strategy, but if they do have different numeric ranges, then it is a crucial step to perform normalization on the data. <span>Normalization </span><span>has a direct impact on the stability/accuracy of the model. ND4J has various preprocessors to handle normalization. </span>In this recipe, we will normalize the data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create a dataset iterator from the data. <span>Refer to the following demonstration for</span> <kbd>RecordReaderDataSetIterator</kbd><span>:</span></li>
</ol>
<pre style="padding-left: 60px">DataSetIterator iterator = new RecordReaderDataSetIterator(recordReader,batchSize);<span><br/></span></pre>
<ol start="2">
<li>Apply the normalization to the dataset by calling the <kbd>fit()</kbd> method of the normalizer implementation. <span>Refer to the following demonstration for</span> the <kbd>NormalizerStandardize</kbd><strong> </strong><span><span>preprocessor:</span></span></li>
</ol>
<pre style="padding-left: 60px">DataNormalization dataNormalization = new NormalizerStandardize();<br/>dataNormalization.fit(iterator);</pre>
<ol start="3">
<li>Call <kbd>setPreprocessor()</kbd> to set the preprocessor for the dataset:</li>
</ol>
<pre style="padding-left: 60px">iterator.setPreProcessor(dataNormalization);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>To start, you need to have an iterator to traverse and prepare the data. In step 1, we used the record reader data to create the dataset iterator. The purpose of the iterator is to have more control over the data and how it is presented to the neural network.</p>
<p>Once the appropriate normalization method has been identified (<span><kbd>NormalizerStandardize</kbd>, in step 2</span>), we use <kbd>fit()</kbd> to apply the normalization to the dataset. <span><kbd>NormalizerStandardize</kbd> normalizes the data in such a way that feature values will have a zero mean and standard deviation of 1.</span></p>
<p class="mce-root"/>
<p><span>The example for this recipe can be found at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/NormalizationExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/NormalizationExample.java</a>.</span></p>
<ul>
<li><strong>Sample input</strong>: A dataset iterator that holds feature variables (<kbd>INDArray</kbd> format). Iterators are created from the input data as mentioned in previous recipes.</li>
<li><strong>Sample output</strong>: Refer to the following snapshot for the normalized features (<kbd>INDArray</kbd> format) after applying normalization on the input data:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1205 image-border" src="assets/239553a8-700f-42fe-9403-1f0b2ecbb7b0.png" style="width:66.33em;height:19.75em;"/></p>
<p class="mce-root">Note that we can't skip step 3 while applying normalization. If we don't perform step 3, the dataset won't be auto-normalized. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Preprocessors normally have default range limits from<span> </span><kbd>0</kbd><span> </span>to<span> </span><kbd>1</kbd>. If you don't apply normalization to a dataset with a wide range of numeric values (when feature values that are too low and too high are present), then the neural network will tend to favor the feature values that have high numeric values. Hence, the accuracy of the neural network could be significantly reduced.</p>
<p>If values are spread across symmetric intervals such as (<span><kbd>0</kbd>,<kbd>1</kbd>), then all the feature values are considered equivalent during training. Hence, it also has an impact on the neural network's generalization.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>The following are the preprocessors that are provided b</span>y ND4J: </p>
<ul>
<li><kbd>NormalizerStandardize</kbd>: A preprocessor for datasets that normalizes feature values so that they have a <em>zero</em><span> </span>mean and a standard deviation of 1.</li>
<li><kbd>MultiNormalizerStandardize</kbd>: A preprocessor for multi-datasets that normalizes feature values so that they have a zero mean and a standard deviation of 1.</li>
<li><kbd>NormalizerMinMaxScaler</kbd><span>: A p</span>reprocessor for datasets that normalizes feature values so that they lie between a minimum and maximum value that's been specified. The default range is 0 to 1.</li>
<li><kbd>MultiNormalizerMinMaxScaler</kbd>: A preprocessor for multi-datasets that normalizes feature values that lie between a minimum and maximum value that's been specified. The default range is<span> </span>0 to 1.</li>
<li><kbd>ImagePreProcessingScaler</kbd>: A preprocessor for images with minimum and maximum scaling. The default ranges are (<kbd>miRange</kbd>,<span> </span><kbd>maxRange</kbd>) – (<kbd>0</kbd>,<kbd>1</kbd>).</li>
<li><span><kbd>VGG16ImagePreProcessor</kbd></span>: A preprocessor specifically for the VGG16 network architecture.<span> It c</span>omputes the mean RGB value and subtracts it from each pixel on the training set. </li>
</ul>


            </article>

            
        </section>
    </body></html>