["```py\npip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip\n```", "```py\ndef model(l_input, input_dim=28, num_units=256, num_classes=10, p=.5):\n\n    network = lasagne.layers.Conv2DLayer(\n            l_input, num_filters=32, filter_size=(5, 5),\n            nonlinearity=lasagne.nonlinearities.rectify,\n            W=lasagne.init.GlorotUniform())\n\n    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n\n    network = lasagne.layers.Conv2DLayer(\n            network, num_filters=32, filter_size=(5, 5),\n            nonlinearity=lasagne.nonlinearities.rectify)\n\n    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n\n    if num_units > 0:\n        network = lasagne.layers.DenseLayer(\n                lasagne.layers.dropout(network, p=p),\n                num_units=num_units,\n                nonlinearity=lasagne.nonlinearities.rectify)\n\n    if (num_units > 0) and (num_classes > 0):\n        network = lasagne.layers.DenseLayer(\n                lasagne.layers.dropout(network, p=p),\n                num_units=num_classes,\n                nonlinearity=lasagne.nonlinearities.softmax)\n\n    return network\n```", "```py\ninput_var = T.tensor4('inputs')\nl_input = lasagne.layers.InputLayer(shape=(None, 1, 28, 28), input_var=input_var)\nnetwork = mnist_cnn.model(l_input)\nprediction = lasagne.layers.get_output(network)\n```", "```py\nl_input = lasagne.layers.InputLayer(shape=(None, 1, 28, 28))\nnetwork = mnist_cnn.model(l_input)\n\ninput_var = T.tensor4('inputs')\nprediction = lasagne.layers.get_output(network, input_var)\n```", "```py\nprint(l_input.output_shape)\n```", "```py\nparams = lasagne.layers.get_all_params(network, trainable=True)\nfor p in params:\n    print p.name\n```", "```py\ntarget_var = T.ivector('targets')\nloss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\nloss = loss.mean()\n\nupdates = lasagne.updates.nesterov_momentum(\n        loss, params, learning_rate=0.01, momentum=0.9)\n\ntrain_fn = theano.function([input_var, target_var], loss, updates=updates)\n```", "```py\nwget http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz -P /sharedfiles\n```", "```py\npython 1-train-mnist.py\n```", "```py\nl_in = lasagne.layers.InputLayer((None, dim, dim))\nl_dim = lasagne.layers.DimshuffleLayer(l_in, (0, 'x', 1, 2))\nl_pool0_loc = lasagne.layers.MaxPool2DLayer(l_dim, pool_size=(2, 2))\nl_dense_loc = mnist_cnn.model(l_pool0_loc, input_dim=dim, num_classes=0)\n\nb = np.zeros((2, 3), dtype=theano.config.floatX)\nb[0, 0] = 1.0\nb[1, 1] = 1.0\n\nl_A_net = lasagne.layers.DenseLayer(\n    l_dense_loc,\n    num_units=6,\n    name='A_net',\n    b=b.flatten(),\n    W=lasagne.init.Constant(0.0),\n    nonlinearity=lasagne.nonlinearities.identity)\n```", "```py\nl_transform = lasagne.layers.TransformerLayer(\n    incoming=l_dim,\n    localization_network=l_A_net,\n    downsample_factor=args.downsample)\n```", "```py\nl_out = mnist_cnn.model(l_transform, input_dim=dim, p=sh_drp, num_units=400)\n```", "```py\npython create_mnist_sequence.py --nb_digits=1\n```", "```py\npython plot_data.py mnist_sequence1_sample_8distortions_9x9.npz\n```", "```py\npython 2-stn-cnn-mnist.py\n```", "```py\npython plot_crops.py res_test_2.npz\n```", "```py\nl_conv2_loc = mnist_cnn.model(l_pool0_loc, input_dim=dim, p=sh_drp, num_units=0)\n\nclass Repeat(lasagne.layers.Layer):\n    def __init__(self, incoming, n, **kwargs):\n        super(Repeat, self).__init__(incoming, **kwargs)\n        self.n = n\n\n    def get_output_shape_for(self, input_shape):\n        return tuple([input_shape[0], self.n] + list(input_shape[1:]))\n\n    def get_output_for(self, input, **kwargs):\n        tensors = [input]*self.n\n        stacked = theano.tensor.stack(*tensors)\n        dim = [1, 0] + range(2, input.ndim+1)\n        return stacked.dimshuffle(dim)\n\nl_repeat_loc = Repeat(l_conv2_loc, n=num_steps)\nl_gru = lasagne.layers.GRULayer(l_repeat_loc, num_units=num_rnn_units,\nunroll_scan=True)\n\nl_shp = lasagne.layers.ReshapeLayer(l_gru, (-1, num_rnn_units))  \n```", "```py\nb = np.zeros((2, 3), dtype=theano.config.floatX)\nb[0, 0] = 1.0\nb[1, 1] = 1.0\n\nl_A_net = lasagne.layers.DenseLayer(\n    l_shp,\n    num_units=6,\n    name='A_net',\n    b=b.flatten(),\n    W=lasagne.init.Constant(0.0),\n    nonlinearity=lasagne.nonlinearities.identity)\n\nl_conv_to_transform = lasagne.layers.ReshapeLayer(\n    Repeat(l_dim, n=num_steps), [-1] + list(l_dim.output_shape[-3:]))\n\nl_transform = lasagne.layers.TransformerLayer(\n    incoming=l_conv_to_transform,\n    localization_network=l_A_net,\n    downsample_factor=args.downsample)\n\nl_out = mnist_cnn.model(l_transform, input_dim=dim, p=sh_drp, num_units=400)\n```", "```py\npython create_mnist_sequence.py --nb_digits=3 --output_dim=100\n```", "```py\npython plot_data.py mnist_sequence3_sample_8distortions_9x9.npz\n```", "```py\npython 3-recurrent-stn-mnist.py\n*Epoch 0 Acc Valid 0.268833333333, Acc Train = 0.268777777778, Acc Test = 0.272466666667*\n*Epoch 1 Acc Valid 0.621733333333, Acc Train = 0.611116666667, Acc Test = 0.6086*\n*Epoch 2 Acc Valid 0.764066666667, Acc Train = 0.75775, Acc Test = 0.764866666667*\n*Epoch 3 Acc Valid 0.860233333333, Acc Train = 0.852294444444, Acc Test = 0.859566666667*\n*Epoch 4 Acc Valid 0.895333333333, Acc Train = 0.892066666667, Acc Test = 0.8977*\n*Epoch 53 Acc Valid 0.980433333333, Acc Train = 0.984261111111, Acc Test = 0.97926666666*\n\n```", "```py\npython plot_crops.py res_test_3.npz\n```"]