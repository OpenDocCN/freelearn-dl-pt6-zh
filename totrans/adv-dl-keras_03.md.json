["```py\nfrom keras.layers import Dense, Input\nfrom keras.layers import Conv2D, Flatten\nfrom keras.layers import Reshape, Conv2DTranspose\nfrom keras.models import Model\nfrom keras.datasets import mnist\nfrom keras.utils import plot_model\nfrom keras import backend as K\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# load MNIST dataset\n(x_train, _), (x_test, _) = mnist.load_data()\n\n# reshape to (28, 28, 1) and normalize input images\nimage_size = x_train.shape[1]\nx_train = np.reshape(x_train, [-1, image_size, image_size, 1])\nx_test = np.reshape(x_test, [-1, image_size, image_size, 1])\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# network parameters\ninput_shape = (image_size, image_size, 1)\nbatch_size = 32\nkernel_size = 3\nlatent_dim = 16\n# encoder/decoder number of filters per CNN layer\nlayer_filters = [32, 64]\n\n# build the autoencoder model\n# first build the encoder model\ninputs = Input(shape=input_shape, name='encoder_input')\nx = inputs\n# stack of Conv2D(32)-Conv2D(64)\nfor filters in layer_filters:\n    x = Conv2D(filters=filters,\n               kernel_size=kernel_size,\n               activation='relu',\n               strides=2,\n               padding='same')(x)\n\n# shape info needed to build decoder model \n# so we don't do hand computation\n# the input to the decoder's first Conv2DTranspose \n# will have this shape\n# shape is (7, 7, 64) which is processed by \n# the decoder back to (28, 28, 1)\nshape = K.int_shape(x)\n\n# generate latent vector\nx = Flatten()(x)\nlatent = Dense(latent_dim, name='latent_vector')(x)\n\n# instantiate encoder model\nencoder = Model(inputs, latent, name='encoder')\nencoder.summary()\nplot_model(encoder, to_file='encoder.png', show_shapes=True)\n\n# build the decoder model\nlatent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n# use the shape (7, 7, 64) that was earlier saved\nx = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n# from vector to suitable shape for transposed conv\nx = Reshape((shape[1], shape[2], shape[3]))(x)\n\n# stack of Conv2DTranspose(64)-Conv2DTranspose(32)\nfor filters in layer_filters[::-1]:\n    x = Conv2DTranspose(filters=filters,\n                        kernel_size=kernel_size,\n                        activation='relu',\n                        strides=2,\n                        padding='same')(x)\n\n# reconstruct the input\noutputs = Conv2DTranspose(filters=1,\n                          kernel_size=kernel_size,\n                          activation='sigmoid',\n                          padding='same',\n                          name='decoder_output')(x)\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()\nplot_model(decoder, to_file='decoder.png', show_shapes=True)\n\n# autoencoder = encoder + decoder\n# instantiate autoencoder model\n   autoencoder = Model(inputs,\n                       decoder(encoder(inputs)),\n                       name='autoencoder')\n   autoencoder.summary()\n   plot_model(autoencoder,\n              to_file='autoencoder.png',\n           show_shapes=True)\n\n# Mean Square Error (MSE) loss funtion, Adam optimizer\nautoencoder.compile(loss='mse', optimizer='adam')\n\n# train the autoencoder\nautoencoder.fit(x_train,\n                x_train,\n                validation_data=(x_test, x_test),\n                epochs=1,\n                batch_size=batch_size)\n\n# predict the autoencoder output from test data\nx_decoded = autoencoder.predict(x_test)\n\n# display the 1st 8 test input and decoded images\nimgs = np.concatenate([x_test[:8], x_decoded[:8]])\nimgs = imgs.reshape((4, 4, image_size, image_size))\nimgs = np.vstack([np.hstack(i) for i in imgs])\nplt.figure()\nplt.axis('off')\nplt.title('Input: 1st 2 rows, Decoded: last 2 rows')\nplt.imshow(imgs, interpolation='none', cmap='gray')\nplt.savefig('input_and_decoded.png')\nplt.show()\n```", "```py\ndef plot_results(models,\n                 data,\n                 batch_size=32,\n                 model_name=\"autoencoder_2dim\"):\n    \"\"\"Plots 2-dim latent values as color gradient\n        then, plot MNIST digits as function of 2-dim latent vector\n\n    Arguments:\n        models (list): encoder and decoder models\n        data (list): test data and label\n        batch_size (int): prediction batch size\n        model_name (string): which model is using this function\n    \"\"\"\n\n    encoder, decoder = models\n    x_test, y_test = data\n    os.makedirs(model_name, exist_ok=True)\n\n    filename = os.path.join(model_name, \"latent_2dim.png\")\n    # display a 2D plot of the digit classes in the latent space\n    z = encoder.predict(x_test,\n                        batch_size=batch_size)\n    plt.figure(figsize=(12, 10))\n    plt.scatter(z[:, 0], z[:, 1], c=y_test)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.savefig(filename)\n    plt.show()\n\n    filename = os.path.join(model_name, \"digits_over_latent.png\")\n    # display a 30x30 2D manifold of the digits\n    n = 30\n    digit_size = 28\n    figure = np.zeros((digit_size * n, digit_size * n))\n    # linearly spaced coordinates corresponding to the 2D plot\n    # of digit classes in the latent space\n    grid_x = np.linspace(-4, 4, n)\n    grid_y = np.linspace(-4, 4, n)[::-1]\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z = np.array([[xi, yi]])\n            x_decoded = decoder.predict(z)\n            digit = x_decoded[0].reshape(digit_size, digit_size)\n            figure[i * digit_size: (i + 1) * digit_size,\n                   j * digit_size: (j + 1) * digit_size] = digit\n\n    plt.figure(figsize=(10, 10))\n    start_range = digit_size // 2\n    end_range = n * digit_size + start_range + 1\n    pixel_range = np.arange(start_range, end_range, digit_size)\n    sample_range_x = np.round(grid_x, 1)\n    sample_range_y = np.round(grid_y, 1)\n    plt.xticks(pixel_range, sample_range_x)\n    plt.yticks(pixel_range, sample_range_y)\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.imshow(figure, cmap='Greys_r')\n    plt.savefig(filename)\n    plt.show()\n```", "```py\nfrom keras.layers import Dense, Input\nfrom keras.layers import Conv2D, Flatten\nfrom keras.layers import Reshape, Conv2DTranspose\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.datasets import mnist\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nnp.random.seed(1337)\n\n# load MNIST dataset\n(x_train, _), (x_test, _) = mnist.load_data()\n\n# reshape to (28, 28, 1) and normalize input images\nimage_size = x_train.shape[1]\nx_train = np.reshape(x_train, [-1, image_size, image_size, 1])\nx_test = np.reshape(x_test, [-1, image_size, image_size, 1])\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# generate corrupted MNIST images by adding noise with normal dist\n# centered at 0.5 and std=0.5\nnoise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\nx_train_noisy = x_train + noise\nnoise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\nx_test_noisy = x_test + noise\n\n# adding noise may exceed normalized pixel values>1.0 or <0.0\n# clip pixel values >1.0 to 1.0 and <0.0 to 0.0\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n\n# network parameters\ninput_shape = (image_size, image_size, 1)\nbatch_size = 32\nkernel_size = 3\nlatent_dim = 16\n# encoder/decoder number of CNN layers and filters per layer\nlayer_filters = [32, 64]\n\n# build the autoencoder model\n# first build the encoder model\ninputs = Input(shape=input_shape, name='encoder_input')\nx = inputs\n\n# stack of Conv2D(32)-Conv2D(64)\nfor filters in layer_filters:\n    x = Conv2D(filters=filters,\n               kernel_size=kernel_size,\n               strides=2,\n               activation='relu',\n               padding='same')(x)\n\n# shape info needed to build decoder model \n# so we don't do hand computation\n# the input to the decoder's first Conv2DTranspose \n# will have this shape\n# shape is (7, 7, 64) which can be processed by \n# the decoder back to (28, 28, 1)\nshape = K.int_shape(x)\n\n# generate the latent vector\nx = Flatten()(x)\nlatent = Dense(latent_dim, name='latent_vector')(x)\n\n# instantiate encoder model\nencoder = Model(inputs, latent, name='encoder')\nencoder.summary()\n\n# build the decoder model\nlatent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n# use the shape (7, 7, 64) that was earlier saved\nx = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n# from vector to suitable shape for transposed conv\nx = Reshape((shape[1], shape[2], shape[3]))(x)\n\n# stack of Conv2DTranspose(64)-Conv2DTranspose(32)\nfor filters in layer_filters[::-1]:\n    x = Conv2DTranspose(filters=filters,\n                        kernel_size=kernel_size,\n                        strides=2,\n                        activation='relu',\n                        padding='same')(x)\n\n# reconstruct the denoised input\noutputs = Conv2DTranspose(filters=1,\n                          kernel_size=kernel_size,\n                          padding='same',\n                          activation='sigmoid',\n                          name='decoder_output')(x)\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()\n\n# autoencoder = encoder + decoder\n# instantiate autoencoder model\nautoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\nautoencoder.summary()\n\n# Mean Square Error (MSE) loss function, Adam optimizer\nautoencoder.compile(loss='mse', optimizer='adam')\n\n# train the autoencoder\nautoencoder.fit(x_train_noisy,\n                x_train,\n                validation_data=(x_test_noisy, x_test),\n                epochs=10,\n                batch_size=batch_size)\n\n# predict the autoencoder output from corrupted test images\nx_decoded = autoencoder.predict(x_test_noisy)\n\n# 3 sets of images with 9 MNIST digits\n# 1st rows - original images\n# 2nd rows - images corrupted by noise\n# 3rd rows - denoised images\nrows, cols = 3, 9\nnum = rows * cols\nimgs = np.concatenate([x_test[:num], x_test_noisy[:num], x_decoded[:num]])\nimgs = imgs.reshape((rows * 3, cols, image_size, image_size))\nimgs = np.vstack(np.split(imgs, rows, axis=1))\nimgs = imgs.reshape((rows * 3, -1, image_size, image_size))\nimgs = np.vstack([np.hstack(i) for i in imgs])\nimgs = (imgs * 255).astype(np.uint8)\nplt.figure()\nplt.axis('off')\nplt.title('Original images: top rows, '\n          'Corrupted Input: middle rows, '\n          'Denoised Input:  third rows')\nplt.imshow(imgs, interpolation='none', cmap='gray')\nImage.fromarray(imgs).save('corrupted_and_denoised.png')\nplt.show()\n```", "```py\nfrom keras.layers import Dense, Input\nfrom keras.layers import Conv2D, Flatten\nfrom keras.layers import Reshape, Conv2DTranspose\nfrom keras.models import Model\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras.datasets import cifar10\nfrom keras.utils import plot_model\nfrom keras import backend as K\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# convert from color image (RGB) to grayscale\n# source: opencv.org\n# grayscale = 0.299*red + 0.587*green + 0.114*blue\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n\n# load the CIFAR10 data\n(x_train, _), (x_test, _) = cifar10.load_data()\n\n# input image dimensions\n# we assume data format \"channels_last\"\nimg_rows = x_train.shape[1]\nimg_cols = x_train.shape[2]\nchannels = x_train.shape[3]\n\n# create saved_images folder\nimgs_dir = 'saved_images'\nsave_dir = os.path.join(os.getcwd(), imgs_dir)\nif not os.path.isdir(save_dir):\n        os.makedirs(save_dir)\n\n# display the 1st 100 input images (color and gray)\nimgs = x_test[:100]\nimgs = imgs.reshape((10, 10, img_rows, img_cols, channels))\nimgs = np.vstack([np.hstack(i) for i in imgs])\nplt.figure()\nplt.axis('off')\nplt.title('Test color images (Ground Truth)')\nplt.imshow(imgs, interpolation='none')\nplt.savefig('%s/test_color.png' % imgs_dir)\nplt.show()\n\n# convert color train and test images to gray\nx_train_gray = rgb2gray(x_train)\nx_test_gray = rgb2gray(x_test)\n\n# display grayscale version of test images\nimgs = x_test_gray[:100]\nimgs = imgs.reshape((10, 10, img_rows, img_cols))\nimgs = np.vstack([np.hstack(i) for i in imgs])\nplt.figure()\nplt.axis('off')\nplt.title('Test gray images (Input)')\nplt.imshow(imgs, interpolation='none', cmap='gray')\nplt.savefig('%s/test_gray.png' % imgs_dir)\nplt.show()\n\n# normalize output train and test color images\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# normalize input train and test grayscale images\nx_train_gray = x_train_gray.astype('float32') / 255\nx_test_gray = x_test_gray.astype('float32') / 255\n\n# reshape images to row x col x channel for CNN output/validation\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)\n\n# reshape images to row x col x channel for CNN input\nx_train_gray = x_train_gray.reshape(x_train_gray.shape[0], img_rows, img_cols, 1)\nx_test_gray = x_test_gray.reshape(x_test_gray.shape[0], img_rows, img_cols, 1)\n\n# network parameters\ninput_shape = (img_rows, img_cols, 1)\nbatch_size = 32\nkernel_size = 3\nlatent_dim = 256\n# encoder/decoder number of CNN layers and filters per layer\nlayer_filters = [64, 128, 256]\n\n# build the autoencoder model\n# first build the encoder model\ninputs = Input(shape=input_shape, name='encoder_input')\nx = inputs\n# stack of Conv2D(64)-Conv2D(128)-Conv2D(256)\nfor filters in layer_filters:\n    x = Conv2D(filters=filters,\n               kernel_size=kernel_size,\n               strides=2,\n               activation='relu',\n               padding='same')(x)\n\n# shape info needed to build decoder model \n# so we don't do hand computation\n# the input to the decoder's first Conv2DTranspose \n# will have this shape\n# shape is (4, 4, 256) which is processed \n# by the decoder to (32, 32, 3)\nshape = K.int_shape(x)\n\n# generate a latent vector\nx = Flatten()(x)\nlatent = Dense(latent_dim, name='latent_vector')(x)\n\n# instantiate encoder model\nencoder = Model(inputs, latent, name='encoder')\nencoder.summary()\n\n# build the decoder model\nlatent_inputs = Input(shape=(latent_dim,), name='decoder_input')\nx = Dense(shape[1]*shape[2]*shape[3])(latent_inputs)\nx = Reshape((shape[1], shape[2], shape[3]))(x)\n\n# stack of Conv2DTranspose(256)-Conv2DTranspose(128)-\n# Conv2DTranspose(64)\nfor filters in layer_filters[::-1]:\n    x = Conv2DTranspose(filters=filters,\n                        kernel_size=kernel_size,\n                        strides=2,\n                        activation='relu',\n                        padding='same')(x)\n\noutputs = Conv2DTranspose(filters=channels,\n                          kernel_size=kernel_size,\n                          activation='sigmoid',\n                          padding='same',\n                          name='decoder_output')(x)\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()\n\n# autoencoder = encoder + decoder\n# instantiate autoencoder model\nautoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\nautoencoder.summary()\n\n# prepare model saving directory.\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\nmodel_name = 'colorized_ae_model.{epoch:03d}.h5'\nif not os.path.isdir(save_dir):\n        os.makedirs(save_dir)\nfilepath = os.path.join(save_dir, model_name)\n\n# reduce learning rate by sqrt(0.1) if the loss does not improve in 5 epochs\nlr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n                               cooldown=0,\n                               patience=5,\n                               verbose=1,\n                               min_lr=0.5e-6)\n\n# save weights for future use \n# (e.g. reload parameters w/o training)\ncheckpoint = ModelCheckpoint(filepath=filepath,\n                             monitor='val_loss',\n                             verbose=1,\n                             save_best_only=True)\n\n# Mean Square Error (MSE) loss function, Adam optimizer\nautoencoder.compile(loss='mse', optimizer='adam')\n\n# called every epoch\ncallbacks = clr_reducer, checkpoint]\n\n# train the autoencoder\nautoencoder.fit(x_train_gray,\n                x_train,\n                validation_data=(x_test_gray, x_test),\n                epochs=30,\n                batch_size=batch_size,\n                callbacks=callbacks)\n\n# predict the autoencoder output from test data\nx_decoded = autoencoder.predict(x_test_gray)\n\n# display the 1st 100 colorized images\nimgs = x_decoded[:100]\nimgs = imgs.reshape((10, 10, img_rows, img_cols, channels))\nimgs = np.vstack([np.hstack(i) for i in imgs])\nplt.figure()\nplt.axis('off')\nplt.title('Colorized test images (Predicted)')\nplt.imshow(imgs, interpolation='none')\nplt.savefig('%s/colorized.png' % imgs_dir)\nplt.show()\n```"]