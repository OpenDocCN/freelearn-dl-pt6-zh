["```py\nconda install nltk\n```", "```py\nimport nltk\nnltk.download(\"book\")\n```", "```py\nfrom load import parse_text\nX_train, y_train, index_to_word = parse_text(\"data/tiny-shakespear.txt\", type=\"word\")\n\nfor i in range(10):\n  print \"x\", \" \".join([index_to_word[x] for x in X_train[i]])\n  print \"y\",\" \".join([index_to_word[x] for x in y_train[i]])\n\n*Vocabulary size 9000*\n*Found 12349 unique words tokens.*\n*The least frequent word in our vocabulary is 'a-fire' and appeared 1 times.*\n*x START first citizen : before we proceed any further , hear me speak .*\n*y first citizen : before we proceed any further , hear me speak . END*\n*x START all : speak , speak .*\n*y all : speak , speak . END*\n*x START first citizen : you are all resolved rather to die than to famish ?*\n*y first citizen : you are all resolved rather to die than to famish ? END*\n*x START all : resolved .*\n*y all : resolved . END*\n*x START resolved .*\n*y resolved . END*\n*x START first citizen : first , you know caius marcius is chief enemy to the people .*\n*y first citizen : first , you know caius marcius is chief enemy to the people . END*\n*x START all : we know't , we know't .*\n*y all : we know't , we know't . END*\n*x START first citizen : let us kill him , and we 'll have corn at our own price .*\n*y first citizen : let us kill him , and we 'll have corn at our own price . END*\n*x START is't a verdict ?*\n*y is't a verdict ? END*\n*x START all : no more talking o n't ; let it be done : away , away !*\n*y all : no more talking o n't ; let it be done : away , away ! END*\n\n```", "```py\nfrom load import parse_text\nX_train, y_train, index_to_char = parse_text(\"data/tiny-shakespear.txt\", type=\"char\")\n\nfor i in range(10):\n  print \"x\",''.join([index_to_char[x] for x in X_train[i]])\n  print \"y\",''.join([index_to_char[x] for x in y_train[i]])\n\n*x ^first citizen: before we proceed any further, hear me speak*\n*y irst citizen: before we proceed any further, hear me speak.$*\n*x ^all: speak, speak*\n*y ll: speak, speak.$*\n*x ^first citizen: you are all resolved rather to die than to famish*\n*y irst citizen: you are all resolved rather to die than to famish?$*\n*x ^all: resolved*\n*y ll: resolved.$*\n*x ^resolved*\n*y esolved.$*\n*x ^first citizen: first, you know caius marcius is chief enemy to the people*\n*y irst citizen: first, you know caius marcius is chief enemy to the people.$*\n*x ^all: we know't, we know't*\n*y ll: we know't, we know't.$*\n*x ^first citizen: let us kill him, and we'll have corn at our own price*\n*y irst citizen: let us kill him, and we'll have corn at our own price.$*\n*x ^is't a verdict*\n*y s't a verdict?$*\n*x ^all: no more talking on't; let it be done: away, away*\n*y ll: no more talking on't; let it be done: away, away!$*\n\n```", "```py\nfrom theano import *\nimport theano.tensor as T\nfrom utils import shared_zeros, shared_glorot_uniform,save_params,load_params\n```", "```py\nx = T.ivector()\ny = T.ivector()\n```", "```py\nembedding_size = len(index_)\nn_hidden=500\n```", "```py\nU = shared_glorot_uniform(( embedding_size,n_hidden), name=\"U\")\nW = shared_glorot_uniform((n_hidden, n_hidden), name=\"W\")\nbh = shared_zeros((n_hidden,), name=\"bh\")\n```", "```py\nV = shared_glorot_uniform(( n_hidden, embedding_size), name=\"V\")\nby = shared_zeros((embedding_size,), name=\"by\")\n\nparams = [U,V,W,by,bh]\n\ndef step(x_t, h_tm1):\n    h_t = T.tanh(U[x_t] + T.dot( h_tm1, W) + bh)\n    y_t = T.dot(h_t, V) + by\n    return h_t, y_t\n```", "```py\nh0 = shared_zeros((n_hidden,), name='h0')\n[h, y_pred], _ = theano.scan(step, sequences=x, outputs_info=[h0, None], truncate_gradient=10)\n```", "```py\nmodel = T.nnet.softmax(y_pred)\ny_out = T.argmax(model, axis=-1)\ncost = -T.mean(T.log(model)[T.arange(y.shape[0]), y])\n```", "```py\nW_xi = shared_glorot_uniform(( embedding_size,n_hidden))\nW_hi = shared_glorot_uniform(( n_hidden,n_hidden))\nW_ci = shared_glorot_uniform(( n_hidden,n_hidden))\nb_i = shared_zeros((n_hidden,))\n```", "```py\nW_xf = shared_glorot_uniform(( embedding_size, n_hidden))\nW_hf = shared_glorot_uniform(( n_hidden,n_hidden))\nW_cf = shared_glorot_uniform(( n_hidden,n_hidden))\nb_f = shared_zeros((n_hidden,))\n```", "```py\nW_xo = shared_glorot_uniform(( embedding_size, n_hidden))\nW_ho = shared_glorot_uniform(( n_hidden,n_hidden))\nW_co = shared_glorot_uniform(( n_hidden,n_hidden))\nb_o = shared_zeros((n_hidden,))\n```", "```py\nW_xc = shared_glorot_uniform(( embedding_size, n_hidden))\nW_hc = shared_glorot_uniform(( n_hidden,n_hidden))\nb_c = shared_zeros((n_hidden,))\n```", "```py\nW_y = shared_glorot_uniform(( n_hidden, embedding_size), name=\"V\")\nb_y = shared_zeros((embedding_size,), name=\"by\")\n```", "```py\nparams = [W_xi,W_hi,W_ci,b_i,W_xf,W_hf,W_cf,b_f,W_xo,W_ho,W_co,b_o,W_xc,W_hc,b_c,W_y,b_y]\n```", "```py\ndef step(x_t, h_tm1, c_tm1):\n    i_t = T.nnet.sigmoid(W_xi[x_t] + T.dot(W_hi, h_tm1) + T.dot(W_ci, c_tm1) + b_i)\n    f_t = T.nnet.sigmoid(W_xf[x_t] + T.dot(W_hf, h_tm1) + T.dot(W_cf, c_tm1) + b_f)\n    c_t = f_t * c_tm1 + i_t * T.tanh(W_xc[x_t] + T.dot(W_hc, h_tm1) + b_c)\n    o_t = T.nnet.sigmoid(W_xo[x_t] + T.dot(W_ho, h_tm1) + T.dot(W_co, c_t) + b_o)\n    h_t = o_t * T.tanh(c_t)\n    y_t = T.dot(h_t, W_y) + b_y\n    return h_t, c_t, y_t\n```", "```py\nh0 = shared_zeros((n_hidden,), name='h0')\nc0 = shared_zeros((n_hidden,), name='c0')\n[h, c, y_pred], _ = theano.scan(step, sequences=x, outputs_info=[h0, c0, None], truncate_gradient=10)\n```", "```py\nW_xz = shared_glorot_uniform(( embedding_size,n_hidden))\nW_hz = shared_glorot_uniform(( n_hidden,n_hidden))\nb_z = shared_zeros((n_hidden,))\n```", "```py\nW_xr = shared_glorot_uniform(( embedding_size,n_hidden))\nW_hr = shared_glorot_uniform(( n_hidden,n_hidden))\nb_r = shared_zeros((n_hidden,))\n```", "```py\nW_xh = shared_glorot_uniform(( embedding_size,n_hidden))\nW_hh = shared_glorot_uniform(( n_hidden,n_hidden))\nb_h = shared_zeros((n_hidden,))\n```", "```py\nW_y = shared_glorot_uniform(( n_hidden, embedding_size), name=\"V\")\nb_y = shared_zeros((embedding_size,), name=\"by\")\n```", "```py\nparams = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_y, b_y]\n```", "```py\ndef step(x_t, h_tm1):\n    z_t = T.nnet.sigmoid(W_xz[x_t] + T.dot(W_hz, h_tm1) + b_z)\n    r_t = T.nnet.sigmoid(W_xr[x_t] + T.dot(W_hr, h_tm1) + b_r)\n    can_h_t = T.tanh(W_xh[x_t] + r_t * T.dot(W_hh, h_tm1) + b_h)\n    h_t = (1 - z_t) * h_tm1 + z_t * can_h_t\n    y_t = T.dot(h_t, W_y) + b_y\n    return h_t, y_t\n```", "```py\nh0 = shared_zeros((n_hidden,), name='h0')\n[h, y_pred], _ = theano.scan(step, sequences=x, outputs_info=[h0, None], truncate_gradient=10)\n```", "```py\nlr = T.scalar('learning_rate')\ntrain_model = theano.function(inputs=[x,y,lr], outputs=cost,updates=updates)\n```", "```py\nif (len(train_loss) > 1 and train_loss[-1] > train_loss[-2]):\n    learning_rate = learning_rate * 0.5\n```", "```py\nsentence = [0]\nwhile sentence[-1] != 1:\n    pred = predict_model(sentence)[-1]\n    sentence.append(pred)\nprint(\" \".join([ index_[w] for w in sentence[1:-1]]))\n```"]