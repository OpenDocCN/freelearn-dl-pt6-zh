["```py\n<properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <java.version>1.8</java.version>\n        <nd4j.version>1.0.0-alpha</nd4j.version>\n        <dl4j.version>1.0.0-alpha</dl4j.version>\n        <datavec.version>1.0.0-alpha</datavec.version>\n        <arbiter.version>1.0.0-alpha</arbiter.version>\n        <logback.version>1.2.3</logback.version>\n        <dl4j.spark.version>1.0.0-alpha_spark_2</dl4j.spark.version>\n</properties>\n```", "```py\n<dependencies>\n    <dependency>\n        <groupId>org.nd4j</groupId>\n        <artifactId>nd4j-native</artifactId>\n        <version>${nd4j.version}</version>\n    </dependency>\n    <dependency>\n        <groupId>org.deeplearning4j</groupId>\n        <artifactId>dl4j-spark_2.11</artifactId>\n        <version>1.0.0-alpha_spark_2</version>\n    </dependency>\n    <dependency>\n        <groupId>org.nd4j</groupId>\n        <artifactId>nd4j-native</artifactId>\n        <version>1.0.0-alpha</version>\n        <type>pom</type>\n    </dependency>\n    <dependency>\n        <groupId>org.deeplearning4j</groupId>\n        <artifactId>deeplearning4j-core</artifactId>\n        <version>${dl4j.version}</version>\n    </dependency>\n    <dependency>\n        <groupId>org.deeplearning4j</groupId>\n        <artifactId>deeplearning4j-nlp</artifactId>\n        <version>${dl4j.version}</version>\n    </dependency>\n    <dependency>\n        <groupId>org.deeplearning4j</groupId>\n        <artifactId>deeplearning4j-zoo</artifactId>\n        <version>${dl4j.version}</version>\n    </dependency>\n    <dependency>\n        <groupId>org.deeplearning4j</groupId>\n        <artifactId>arbiter-deeplearning4j</artifactId>\n        <version>${arbiter.version}</version>\n    </dependency>\n    <dependency>\n        <groupId>org.deeplearning4j</groupId>\n        <artifactId>arbiter-ui_2.11</artifactId>\n        <version>${arbiter.version}</version>\n    </dependency>\n    <dependency>\n        <artifactId>datavec-data-codec</artifactId>\n        <groupId>org.datavec</groupId>\n        <version>${datavec.version}</version>\n    </dependency>\n    <dependency>\n        <groupId>org.apache.httpcomponents</groupId>\n        <artifactId>httpclient</artifactId>\n        <version>4.3.5</version>\n    </dependency>\n    <dependency>\n        <groupId>ch.qos.logback</groupId>\n        <artifactId>logback-classic</artifactId>\n        <version>${logback.version}</version>\n        </dependency>\n</dependencies>\n```", "```py\n****************************************************************\n WARNING: COULD NOT LOAD NATIVE SYSTEM BLAS\n ND4J performance WILL be reduced\n ****************************************************************\n```", "```py\nSparkSession spark = SparkSession.builder()\n                  .master(\"local[*]\")\n                  .config(\"spark.sql.warehouse.dir\", \"temp/\")// change accordingly\n                  .appName(\"TitanicSurvivalPrediction\")\n                  .getOrCreate();\n```", "```py\nDataset<Row> df = spark.sqlContext()\n                .read()\n                .format(\"com.databricks.spark.csv\")\n                .option(\"header\", \"true\") // Use first line of all files as header\n                .option(\"inferSchema\", \"true\") // Automatically infer data types\n                .load(\"data/train.csv\");\n```", "```py\nMap<String, Object> m = new HashMap<String, Object>();\nm.put(\"Age\", 30);\nm.put(\"Fare\", 32.2);\nDataset<Row> trainingDF1 = df2.na().fill(m);  \n```", "```py\nDataset<Row> trainingDF2 = trainingDF1.drop(\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\");\n```", "```py\nStringIndexer sexIndexer = new StringIndexer()\n                                    .setInputCol(\"Sex\")\n                                    .setOutputCol(\"sexIndex\")\n                                    .setHandleInvalid(\"skip\");//// we skip column having nulls\n\nStringIndexer embarkedIndexer = new StringIndexer()\n                                    .setInputCol(\"Embarked\")\n                                    .setOutputCol(\"embarkedIndex\")\n                                    .setHandleInvalid(\"skip\");//// we skip column having nulls\n```", "```py\nPipeline pipeline = new Pipeline().setStages(new PipelineStage[] {sexIndexer, embarkedIndexer});\n```", "```py\nDataset<Row> trainingDF3 = pipeline.fit(trainingDF2).transform(trainingDF2).drop(\"Sex\", \"Embarked\");\n```", "```py\nDataset<Row> finalDF = trainingDF3.select(\"Pclass\", \"Age\", \"SibSp\",\"Parch\", \"Fare\",                                                                   \n                                           \"sexIndex\",\"embarkedIndex\", \"Survived\");\nfinalDF.show();\n```", "```py\nDataset<Row>[] splits = finalDF.randomSplit(new double[] {0.7, 0.3}); \nDataset<Row> trainingData = splits[0]; \nDataset<Row> testData = splits[1];\n```", "```py\ntrainingData\n      .coalesce(1)// coalesce(1) writes DF in a single CSV\n      .write() \n      .format(\"com.databricks.spark.csv\")\n      .option(\"header\", \"false\") // don't write the header\n      .option(\"delimiter\", \",\") // comma separated\n      .save(\"data/Titanic_Train.csv\"); // save location\n\ntestData\n      .coalesce(1)// coalesce(1) writes DF in a single CSV\n      .write() \n      .format(\"com.databricks.spark.csv\")\n      .option(\"header\", \"false\") // don't write the header\n      .option(\"delimiter\", \",\") // comma separated\n      .save(\"data/Titanic_Test.csv\"); // save location\n```", "```py\nDenseLayer input_layer = new DenseLayer.Builder()\n                .weightInit(WeightInit.XAVIER)\n                .activation(Activation.RELU)\n                .nIn(numInputs)\n                .nOut(16)\n                .build();\n```", "```py\nDenseLayer hidden_layer_1 = new DenseLayer.Builder()\n                .weightInit(WeightInit.XAVIER)\n                .activation(Activation.RELU)\n                .nIn(16).nOut(32)\n                .build();\n```", "```py\n DenseLayer hidden_layer_2 = new DenseLayer.Builder()\n                .weightInit(WeightInit.XAVIER)\n                .activation(Activation.RELU)\n                .nIn(32).nOut(16)\n                .build();\n```", "```py\nOutputLayer output_layer = new OutputLayer.Builder(LossFunction.XENT) // XENT for Binary Classification\n                .weightInit(WeightInit.XAVIER)\n                .activation(Activation.SOFTMAX)\n                .nIn(16).nOut(numOutputs)\n                .build();\n```", "```py\nMultiLayerConfiguration MLPconf = new NeuralNetConfiguration.Builder().seed(seed)\n                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n                .weightInit(WeightInit.XAVIER)\n                .updater(new Adam(0.0001))\n                .list()\n                    .layer(0, input_layer)\n                    .layer(1, hidden_layer_1)\n                    .layer(2, hidden_layer_2)\n                    .layer(3, output_layer)\n                .pretrain(false).backprop(true).build();// no pre-traning required    \n```", "```py\nMultiLayerNetwork model = new MultiLayerNetwork(MLPconf);\nmodel.init();\nlog.info(\"Train model....\");\nfor( int i=0; i<numEpochs; i++ ){\n    model.fit(trainingDataIt);\n        }\n```", "```py\nprivate static DataSetIterator readCSVDataset(String csvFileClasspath, int batchSize, \n               int labelIndex, int numClasses) throws IOException, InterruptedException {\n        RecordReader rr = new CSVRecordReader();\n        File input = new File(csvFileClasspath);\n        rr.initialize(new FileSplit(input));\n        DataSetIterator iterator = new RecordReaderDataSetIterator(rr, batchSize, labelIndex, numClasses);\n        return iterator;\n    }\n```", "```py\nString trainPath = \"data/Titanic_Train.csv\";\nString testPath = \"data/Titanic_Test.csv\";\n\nint labelIndex = 7; // First 7 features are followed by the labels in integer \nint numClasses = 2; // number of classes to be predicted -i.e survived or not-survived\nint numEpochs = 1000; // Number of training eopich\n\nint seed = 123; // Randome seed for reproducibilty\nint numInputs = labelIndex; // Number of inputs in input layer\nint numOutputs = numClasses; // Number of classes to be predicted by the network \n\nint batchSizeTraining = 128;         \n```", "```py\nDataSetIterator trainingDataIt = *readCSVDataset*(trainPath, batchSizeTraining, labelIndex, numClasses);\n```", "```py\nint batchSizeTest = 128;\nDataSetIterator testDataIt = *readCSVDataset*(testPath, batchSizeTest, labelIndex, numClasses);\n```", "```py\n*log*.info(\"Evaluate model....\"); \nEvaluation eval = new Evaluation(2) // for class 1 \n\nwhile(testDataIt.hasNext()){\nDataSet next = testDataIt.next(); \nINDArray output = model.output(next.getFeatureMatrix());\neval.eval(next.getLabels(), output);\n}\n*log*.info(eval.stats());\n*log*.info(\"****************Example finished********************\");\n>>>\n ==========================Scores========================================\n # of classes: 2\n Accuracy: 0.6496\n Precision: 0.6155\n Recall: 0.5803\n F1 Score: 0.3946\n Precision, recall & F1: reported for positive class (class 1 - \"1\") only\n =======================================================================\n```", "```py\n// Compute Matthews correlation coefficient \nEvaluationAveraging averaging = EvaluationAveraging.*Macro*; \ndouble MCC = eval.matthewsCorrelation(averaging); \nSystem.*out*.println(\"Matthews correlation coefficient: \"+ MCC);\n>>>\n Matthews's correlation coefficient: 0.22308172619187497\n```", "```py\nDataset<Row> data = spark.read()\n                .option(\"maxColumns\", 25000)\n                .format(\"com.databricks.spark.csv\")\n                .option(\"header\", \"true\") // Use first line of all files as header\n                .option(\"inferSchema\", \"true\") // Automatically infer data types\n                .load(\"TCGA-PANCAN-HiSeq-801x20531/data.csv\");// set your path accordingly\n```", "```py\nint numFeatures = data.columns().length;\nlong numSamples = data.count();\nSystem.*out*.println(\"Number of features: \" + numFeatures);\nSystem.*out*.println(\"Number of samples: \" + numSamples);\n>>>\n Number of features: 20532\n Number of samples: 801\n```", "```py\nDataset<Row> numericDF = data.drop(\"id\"); // now 20531 features left\n```", "```py\nDataset<Row> labels = spark.read()\n                .format(\"com.databricks.spark.csv\")\n                .option(\"header\", \"true\") // Use first line of all files as header\n                .option(\"inferSchema\", \"true\") // Automatically infer data types\n                .load(\"TCGA-PANCAN-HiSeq-801x20531/labels.csv\");\nlabels.show(10);\n```", "```py\nStringIndexer indexer = new StringIndexer()\n                        .setInputCol(\"Class\")\n                        .setOutputCol(\"label\")\n                        .setHandleInvalid(\"skip\");// skip null/invalid values\n```", "```py\nDataset<Row> indexedDF = indexer.fit(labels)\n                         .transform(labels)\n                         .select(col(\"label\")\n                         .cast(DataTypes.IntegerType));// casting data types to integer\n```", "```py\nindexedDF.show();\n```", "```py\nDataset<Row> combinedDF = numericDF.join(indexedDF);\n```", "```py\nDataset<Row>[] splits = combinedDF.randomSplit(newdouble[] {0.7, 0.3});//70% for training, 30% for testing\nDataset<Row> trainingData = splits[0];\nDataset<Row> testData = splits[1];\n```", "```py\nSystem.out.println(trainingData.count());// number of samples in training set\nSystem.out.println(testData.count());// number of samples in test set\n>>>\n 561\n 240\n```", "```py\ntrainingData.coalesce(1).write()\n                .format(\"com.databricks.spark.csv\")\n                .option(\"header\", \"false\")\n                .option(\"delimiter\", \",\")\n                .save(\"data/TCGA_train.csv\");\n\ntestData.coalesce(1).write()\n                .format(\"com.databricks.spark.csv\")\n                .option(\"header\", \"false\")\n                .option(\"delimiter\", \",\")\n                .save(\"data/TCGA_test.csv\");\n```", "```py\nString trainPath = \"data/TCGA_train.csv\"; // training set\nString testPath = \"data/TCGA_test.csv\"; // test set\n```", "```py\nint labelIndex = 20531;// number of features\nint numClasses = 5; // number of classes to be predicted\nint batchSize = 128; // batch size (feel free to adjust)\n```", "```py\nDataSetIterator trainingDataIt = readCSVDataset(trainPath, batchSize, labelIndex, numClasses);\n```", "```py\nDataSetIterator testDataIt = *readCSVDataset*(testPath, batchSize, labelIndex, numClasses);\n```", "```py\n// Network hyperparameters\nint numInputs = labelIndex; // number of input features\nint numOutputs = numClasses; // number of classes to be predicted\nint numHiddenNodes = 5000; // too many features, so 5000 sounds good\n```", "```py\n// Create network configuration and conduct network training\nMultiLayerConfiguration LSTMconf = new NeuralNetConfiguration.Builder()\n            .seed(seed)    //Random number generator seed for improved repeatability. Optional.\n            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n            .weightInit(WeightInit.XAVIER)\n            .updater(new Adam(0.001))\n            .list()\n            .layer(0, new LSTM.Builder()\n                        .nIn(numInputs)\n                        .nOut(numHiddenNodes)\n                        .activation(Activation.RELU)\n                        .build())\n            .layer(1, new LSTM.Builder()\n                        .nIn(numHiddenNodes)\n                        .nOut(numHiddenNodes)\n                        .activation(Activation.RELU)\n                        .build())\n            .layer(2, new LSTM.Builder()\n                        .nIn(numHiddenNodes)\n                        .nOut(numHiddenNodes)\n                        .activation(Activation.RELU)\n                        .build())\n            .layer(3, new RnnOutputLayer.Builder()\n                        .activation(Activation.SOFTMAX)\n                        .lossFunction(LossFunction.MCXENT)\n                        .nIn(numHiddenNodes)\n                        .nOut(numOutputs)\n                        .build())\n            .pretrain(false).backprop(true).build();\n```", "```py\nMultiLayerNetwork model = new MultiLayerNetwork(LSTMconf);\nmodel.init();\n\nlog.info(\"Train model....\");\nfor(int i=0; i<numEpochs; i++ ){\n    model.fit(trainingDataIt);\n }\n```", "```py\nLayer[] layers = model.getLayers();\nint totalNumParams = 0;\nfor( int i=0; i<layers.length; i++ ){\n         int nParams = layers[i].numParams();\n        System.*out*.println(\"Number of parameters in layer \" + i + \": \" + nParams);\n       totalNumParams += nParams;\n}\nSystem.*out*.println(\"Total number of network parameters: \" + totalNumParams);\n>>>\n Number of parameters in layer 0: 510655000\n Number of parameters in layer 1: 200035000\n Number of parameters in layer 2: 200035000\n Number of parameters in layer 3: 25005\n Total number of network parameters: 910750005\n```", "```py\n*log*.info(\"Evaluate model....\");\nEvaluation eval = new Evaluation(5) // for 5 classes\nwhile(testDataIt.hasNext()){\n        DataSet next = testDataIt.next();\n        INDArray output = model.output(next.getFeatureMatrix());\n        eval.eval(next.getLabels(), output);\n}\n*log*.info(eval.stats());\n*log*.info(\"****************Example finished********************\");\n>>>\n ==========================Scores========================================\n  # of classes:    5\n  Accuracy:        0.9950\n  Precision:       0.9944\n  Recall:          0.9889\n  F1 Score:        0.9915\n Precision, recall & F1: macro-averaged (equally weighted avg. of 5 classes)\n ========================================================================\n ****************Example finished********************\n```", "```py\nPredictions labeled as 0 classified by model as 0: 82 times\n Predictions labeled as 1 classified by model as 1: 17 times\n Predictions labeled as 1 classified by model as 2: 1 times\n Predictions labeled as 2 classified by model as 2: 35 times\n Predictions labeled as 3 classified by model as 3: 31 times\n Predictions labeled as 4 classified by model as 4: 35 times\n```", "```py\n// Create network configuration and conduct network training\nMultiLayerConfiguration MLPconf = new NeuralNetConfiguration.Builder().seed(seed)\n                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n                .updater(new Adam(0.001)).weightInit(WeightInit.XAVIER).list()\n                .layer(0,new DenseLayer.Builder().nIn(numInputs).nOut(32)\n                        .weightInit(WeightInit.XAVIER)\n                        .activation(Activation.RELU).build())\n                .layer(1,new DenseLayer.Builder().nIn(32).nOut(64).weightInit(WeightInit.XAVIER)\n                        .activation(Activation.RELU).build())\n                .layer(2,new DenseLayer.Builder().nIn(64).nOut(128).weightInit(WeightInit.XAVIER)\n                        .activation(Activation.RELU).build())\n                .layer(3, new OutputLayer.Builder(LossFunction.XENT).weightInit(WeightInit.XAVIER)\n                        .activation(Activation.SOFTMAX).weightInit(WeightInit.XAVIER).nIn(128)\n                        .nOut(numOutputs).build())\n                .pretrain(false).backprop(true).build();    \n```", "```py\nNormalizerMinMaxScaler preProcessor = new NormalizerMinMaxScaler();\npreProcessor.fit(trainingDataIt);\ntrainingDataIt.setPreProcessor(preProcessor);\n```", "```py\ntestDataIt.setPreProcessor(preProcessor);\n```", "```py\n==========================Scores========================================\n # of classes: 2\n Accuracy: 0.6654\n Precision: 0.7848\n Recall: 0.5548\n F1 Score: 0.2056\n Precision, recall & F1: reported for positive class (class 1 - \"1\") only\n ========================================================================\n```", "```py\n<dependency>\n    <groupId>org.deeplearning4j</groupId>\n    <artifactId>deeplearning4j-ui_2.11</artifactId>\n    <version>${dl4j.version}</version>\n</dependency>\n```", "```py\nUIServer uiServer = UIServer.*getInstance*();\n```", "```py\nStatsStorage statsStorage = new InMemoryStatsStorage();\n```", "```py\nuiServer.attach(statsStorage); \nint listenerFrequency = 1;\nmodel.setListeners(new StatsListener(statsStorage, listenerFrequency));\n```"]