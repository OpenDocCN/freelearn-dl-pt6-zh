- en: Constructing an LSTM Neural Network for Sequence Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we discussed classifying time series data for multi-variate
    features. In this chapter, we will create a **long short-term memory** (**LSTM**) neural
    network to classify univariate time series data. Our neural network will learn
    how to classify a univariate time series. We will have **UCI** (short for **University
    of California Irvine**) synthetic control data on top of which the neural network
    will be trained. There will be 600 sequences of data, with every sequence separated
    by a new line to make our job easier. Every sequence will have values recorded
    at 60 time steps. Since it is a univariate time series, we will only have columns
    in CSV files for every example recorded. Every sequence is an example recorded.
    We will split these sequences of data into train/test sets to perform training
    and evaluation respectively. The possible categories of class/labels are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Normal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cyclic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing trend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decreasing trend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upward shift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downward shift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing input layers for the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing output layers for the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the LSTM network for classified output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter's implementation code can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/07_Constructing_LSTM_Neural_network_for_sequence_classification/sourceCode/cookbookapp/src/main/java/UciSequenceClassificationExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/07_Constructing_LSTM_Neural_network_for_sequence_classification/sourceCode/cookbookapp/src/main/java/UciSequenceClassificationExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: After cloning our GitHub repository, navigate to the `Java-Deep-Learning-Cookbook/07_Constructing_LSTM_Neural_network_for_sequence_classification/sourceCode`
    directory. Then import the `cookbookapp` project as a Maven projectby importing `pom.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: Download the data from this UCI website: [https://archive.ics.uci.edu/ml/machine-learning-databases/synthetic_control-mld/synthetic_control.data](https://archive.ics.uci.edu/ml/machine-learning-databases/synthetic_control-mld/synthetic_control.data).
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to create directories to store the train and test data. Refer to the
    following directory structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ec9aeb2-e022-40dc-9b40-f463321fc911.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to create two separate folders for the train and test datasets and
    then create subdirectories for `features` and `labels` respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d26b709-f15b-482c-a21c-82d26854bc27.png)'
  prefs: []
  type: TYPE_IMG
- en: This folder structure is a prerequisite for the aforementioned data extraction. We
    separate features and labels while performing the extraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that, throughout this cookbook, we are using the DL4J version 1.0.0-beta
    3, except in this chapter. You might come across the following error while executing
    the code that we discuss in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: At the time of writing, a new version of DL4J has been released that resolves
    the issue. Hence, we will use version 1.0.0-beta 4to run the examples in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting time series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are using another time series use case, but this time we are targeting time
    series univariate sequence classification. ETL needs to be discussed before we
    configure the LSTM neural network. Data extraction is the first phase in the ETL
    process. This recipe covers data extraction for this use case.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Categorize the sequence data programmatically:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the features/labels in their corresponding directories by following the
    numbered format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `FileUtils` to write the data into files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we open the synthetic control data after the download, it will look like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32b32679-b82b-4f8c-9f86-99cf9f128d91.png)'
  prefs: []
  type: TYPE_IMG
- en: A single sequence is marked in the preceding screenshot. There are 600 sequences
    in total, and each sequence is separated by a new line. In our example, we can
    split the dataset in such a way that 450 sequences will be used for training and
    the remaining 150 sequences will be used for evaluation. We are trying to categorize
    a given sequence against six known classes.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is a univariate time series. The data that is recorded in a single
    sequence is spread across different time steps. We create separate files for every
    single sequence. A single data unit (observation) is separated by a space within
    the file. We will replace spaces with new line characters so that measurements
    for every time step in a single sequence will appear on a new line. The first
    100 sequences represent category 1, and the next 100 sequences represent category
    2, and so on. Since we have univariate time series data, there is only one column
    in the CSV files. So, one single feature is recorded over multiple time steps.
  prefs: []
  type: TYPE_NORMAL
- en: In step 1, the `contentAndLabels` list will have sequence-to-label mappings.
    Each sequence represents a label. The sequence and label together form a pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can have two different approaches to splitting data for training/testing
    purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly shuffle the data and take 450 sequences for training and the remaining
    150 sequences for evaluation/testing purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the train/test data in such a way that the categories are equally distributed
    across the dataset. For example, we can have 420 sequences of train data with
    70 samples for each of the six categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use randomization as a measure to increase the generalization power of the
    neural network. Every sequence-to-label pair was written to a separate CSV file
    following the numbered file naming convention.
  prefs: []
  type: TYPE_NORMAL
- en: In step 2, we mention that there are 450 samples for training, and the remaining
    150 are for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 3, we use `FileUtils` from the Apache Commons library to write the
    data to a file. The final code will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We fetch the sequence data and add it to the `features` directory, and each
    sequence will be represented by a separate CSV file. Similarly, we add the respective
    labels to a separate CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: '`1.csv` in the `label` directory will be the respective label for the `1.csv`
    feature in the `feature` directory.'
  prefs: []
  type: TYPE_NORMAL
- en: Loading training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data transformation is, as usual, the second phase after data extraction. The
    time series data we're discussing doesn't have any non-numeric fields or noise
    (it had already been cleaned). So we can focus on constructing the iterators from
    the data and loading them directly into the neural network. In this recipe, we
    will load univariate time series data for neural network training. We have extracted
    the synthetic control data and stored it in a suitable format so the neural network
    can process it effortlessly. Every sequence is captured over 60 time steps. In
    this recipe, we will load the time series data into an appropriate dataset iterator,
    which can be fed to the neural network for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a `SequenceRecordReader` instance to extract and load features from
    the time series data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SequenceRecordReader` instance to extract and load labels from the
    time series data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create sequence readers for testing and evaluation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `SequenceRecordReaderDataSetIterator` to feed the data into our neural
    network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Rewrite the train/test iterator (with `AlignmentMode`) to support time series
    of varying lengths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have used `NumberedFileInputSplit` in step 1\. It is necessary to use `NumberedFileInputSplit`
    to load data from multiple files that follow a numbered file naming convention.
    Refer to step 1 in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We stored files as a sequence of numbered files in the previous recipe. There
    are 450 files, and each one of them represents a sequence. Note that we have stored
    150 files for testing as demonstrated in step 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 5, `numOfClasses` specifies the number of categories against which
    the neural network is trying to make a prediction. In our example, it is `6`.
    We mentioned `AlignmentMode.ALIGN_END` while creating the iterator. The alignment
    mode deals with input/labels of varying lengths. For example, our time series
    data has 60 time steps, and there''s only one label at the end of the 60^(th)
    time step. That''s the reason why we use `AlignmentMode.ALIGN_END` in the iterator
    definition, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can also have time series data that produces labels at every time step. These
    cases refer to many-to-many input/label connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 4, we started with the regular way of creating iterators, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that this is not the only way to create sequence reader iterators. There
    are multiple implementations available in DataVec to support different configurations.
    We can also align the input/label at the last time step of the sample. For this
    purpose, we added `AlignmentMode.ALIGN_END` into the iterator definition. If there
    are varying time steps, shorter time series will be padded to the length of the
    longest time series. So, if there are samples that have fewer than 60 time steps
    recorded for a sequence, then zero values will be padded to the time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data transformation alone may not improve the neural network's efficiency. The
    existence of large and small ranges of values within the same dataset can lead
    to overfitting (the model captures noise rather than signals). To avoid these
    situations, we normalize the dataset, and there are multiple DL4J implementations
    to do this. The normalization process converts and fits the raw time series data
    into a definite value range, for example, *(0, 1)*. This will help the neural
    network process the data with less computational effort. We also discussed normalization
    in previous chapters, showing that it will reduce favoritism toward any specific
    label in the dataset while training a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a standard normalizer and fit the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the `setPreprocessor()` method to normalize the data on the fly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In step 1, we used `NormalizerStandardize` to normalize the dataset. `NormalizerStandardize`
    normalizes the data (features) so they have a mean of *0* and a standard deviation
    of *1*. In other words, all the values in the dataset will be normalized within
    the range of *(0, 1)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This is a standard normalizer in DL4J, although there are other normalizer implementations
    available in DL4J. Also, note that we don't need to call `fit()` on test data
    because we use the scaling parameters learned during training to scale the test
    data.
  prefs: []
  type: TYPE_NORMAL
- en: We need to call the `setPreprocessor()` method as we demonstrated in step 2
    for both train/test iterators. Once we have set the normalizer using `setPreprocessor()`,
    the data returned by the iterator will be auto-normalized using the specified
    normalizer. Hence it is important to call `setPreprocessor()` along with the `fit()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing input layers for the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Layer configuration is an important step in neural network configuration. We
    need to create input layers to receive the univariate time series data that was
    loaded from disk. In this recipe, we will construct an input layer for our use
    case. We will also add an LSTM layer as a hidden layer for the neural network.
    We can use either a computation graph or a regular multilayer network to build
    the network configuration. In most cases, a regular multilayer network is more
    than enough; however, we are using a computation graph for our use case. In this
    recipe, we will configure input layers for the network.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Configure the neural network with default configurations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the input layer labels by calling `addInputs()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Add an LSTM layer using the `addLayer()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, we specify the default `seed` values, the initial default weights
    (`weightInit`), the weight `updater`, and so on. We set the gradient normalization
    strategy to `ClipElementWiseAbsoluteValue`. We have also set the gradient threshold
    to `0.5` as an input to the `gradientNormalization` strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The neural network calculates the gradients across neurons at each layer. We
    normalized the input data earlier in the *Normalizing training data* recipe, using
    a normalizer. It makes sense to mention that we need to normalize the gradient
    values to achieve data preparation goals. As we can see in step 1, we have used
    `ClipElementWiseAbsoluteValue` gradient normalization. It works in such a way
    that the absolute value of the gradient cannot be greater than the threshold.
    For example, if the gradient threshold value is 3, then the value range would
    be [-3, 3]. Any gradient values that are less than -5 would be treated as -3 and
    any gradient values that are higher than 3 would be treated as 3\. Gradient values
    in the range [-3, 3] will be unmodified. We have mentioned the gradient normalization
    strategy as well as the threshold in the network configuration, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In step 3, the `trainFeatures` label is referred to the input layer label. The
    inputs are basically the graph vertex objects returned by the `graphBuilder()` method.
    The specified LSTM layer name (`L1` in our example) in step 2 will be used while
    configuring the output layer. If there's a mismatch, our program will throw an
    error during execution saying that the layers are configured in such a way that
    they are disconnected. We will discuss this in more depth in the next recipe,
    when we design output layers for the neural network. Note that we have yet to
    add output layers in the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing output layers for the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The very next step after the input/hidden layer design is the output layer design.
    As we mentioned in earlier chapters, the output layer should reflect the output
    you want to receive from the neural network. You may need a classifier or a regression
    model depending on the use case. Accordingly, the output layer has to be configured.
    The activation function and error function need to be justified for their use
    in the output layer configuration. This recipe assumes that the neural network
    configuration has been completed up to the input layer definition. This is going
    to be the last step in network configuration.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use `setOutputs()` to set the output labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Construct an output layer using the `addLayer()` method and `RnnOutputLayer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, we have added a `predictSequence` label for the output layer. Note
    that we mentioned the input layer reference when defining the output layer. In
    step 2, we specified it as `L1`, which is the LSTM input layer created in the
    previous recipe. We need to mention this to avoid any errors during execution
    due to disconnection between the LSTM layer and the output layer. Also, the output
    layer definition should have the same layer name we specified in the `setOutput()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: In step 2, we have used `RnnOutputLayer` to construct the output layer. This
    DL4J output layer implementation is used for use cases that involve recurrent
    neural networks. It is functionally the same as `OutputLayer` in multi-layer perceptrons,
    but output and label reshaping are automatically handled.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the LSTM network for classified output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have configured the neural network, the next step is to start the
    training instance, followed by evaluation. The evaluation phase is very important
    for the training instance. The neural network will try to optimize the gradients
    for optimal results. An optimal neural network will have good and stable evaluation
    metrics. So it is important to evaluate the neural network to direct the training
    process toward the desired results. We will use the test dataset to evaluate the
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we explored a use case for time series binary classification.
    Now we have six labels against which to predict. We have discussed various ways
    to enhance the network's efficiency. We follow the same approach in the next recipe
    to evaluate the neural network for optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Initialize the `ComputationGraph` model configuration using the `init()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Set a score listener to monitor the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the training instance by calling the `fit()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Call `evaluate()` to calculate the evaluation metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, we used a computation graph when configuring the neural network's
    structure. Computation graphs are the best choice for recurrent neural networks.
    We get an evaluation score of approximately 78% with a multi-layer network and
    a whopping 94% while using a computation graph. We get better results with `ComputationGraph` than
    the regular multi-layer perceptron. `ComputationGraph` is meant for complex network
    structures and can be customized to accommodate different types of layers in various
    orders. `InvocationType.EPOCH_END` is used (score iteration) in step 1 to call
    the score iterator at the end of a test iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we''re calling the score iterator for every test iteration, and not
    for the training set iteration. Proper listeners need to be set by calling `setListeners()`
    before your training event starts to log the scores for every test iteration,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In step 4, the model was evaluated by calling `evaluate()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We passed the test dataset to the `evaluate()` method in the form of an iterator
    that was created earlier in the *Loading the training data* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we use the `stats()` method to display the results. For a computation
    graph with 100 epochs, we get the following evaluation metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc6f1069-4d8d-4155-9feb-fd4c1c779f11.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, the following are the experiments you can perform to optimize the results
    even better.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used 100 epochs in our example. Reduce the epochs from 100 or increase this
    setting to a specific value. Note the direction that gives better results. Stop
    when the results are optimal. We can evaluate the results once in every epoch
    to understand the direction in which we can proceed. Check out the following training
    instance logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ac34bbc-fc03-4ba3-97ff-27e9597656b9.png)'
  prefs: []
  type: TYPE_IMG
- en: The accuracy declines after the previous epoch in the preceding example. Accordingly,
    you can decide on the optimal number of epochs. The neural network will simply
    memorize the results if we go for large epochs, and this leads to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of randomizing the data at first, you can ensure that the six categories
    are uniformly distributed across the training set. For example, we can have 420
    samples for training and 180 samples for testing. Then, each category will be
    represented by 70 samples. We can now perform randomization followed by iterator
    creation. Note that we had 450 samples for training in our example. In this case,
    the distribution of labels/categories isn't unique and we are totally relying
    on the randomization of data in this case.
  prefs: []
  type: TYPE_NORMAL
