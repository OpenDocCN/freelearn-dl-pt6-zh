- en: The Apache Spark Ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Spark ([http://spark.apache.org/](http://spark.apache.org/)) is an open
    source, fast cluster-computing platform. It was originally created by AMPLab at
    the University of California, Berkeley. Its source code was later donated to the
    Apache Software Foundation ([https://www.apache.org/](https://www.apache.org/)).
    Spark comes with a very fast computation speed because data is loaded into distributed
    memory (RAM) across a cluster of machines. Not only can data be quickly transformed,
    but also cached on demand for a variety of use cases. Compared to Hadoop MapReduce,
    it runs programs up to 100 times faster when the data fits in memory, or 10 times
    faster on disk. Spark provides support for four programming languages: Java, Scala,
    Python, and R. This book covers the Spark APIs (and deep learning frameworks)
    for Scala ([https://www.scala-lang.org/](https://www.scala-lang.org/)) and Python
    ([https://www.python.org/](https://www.python.org/)) only.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark fundamentals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resilient Distributed Dataset** (**RDD**) programming'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark SQL, Datasets, and DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster mode using a different manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section covers the Apache Spark fundamentals. It is important to become
    very familiar with the concepts that are presented here before moving on to the
    next chapters, where we'll be exploring the available APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the introduction to this chapter, the Spark engine processes
    data in distributed memory across the nodes of a cluster. The following diagram
    shows the logical structure of how a typical Spark job processes information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eeaba019-7780-4cff-b4ae-4826b460da54.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark executes a job in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/443b4632-a826-4125-8f2e-f94068acc6c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2
  prefs: []
  type: TYPE_NORMAL
- en: The **Master** controls how data is partitioned and takes advantage of data
    locality while keeping track of all the distributed data computation on the **Slave**
    machines. If a certain Slave machine becomes unavailable, the data on that machine
    is reconstructed on another available machine(s). In standalone mode, the Master
    is a single point of failure. This chapter's *Cluster mode using different managers* section
    covers the possible running modes and explains fault tolerance in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark comes with five major components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a81796d6-ff55-4a8f-9210-a8fd4335a2b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3
  prefs: []
  type: TYPE_NORMAL
- en: 'These components are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The core engine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark SQL**: A module for structured data processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark Streaming**: This extends the core Spark API. It allows live data stream
    processing. Its strengths include scalability, high throughput, and fault tolerance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLib**: The Spark machine learning library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GraphX**: Graphs and graph-parallel computation algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark can access data that's stored in different systems, such as HDFS, Cassandra,
    MongoDB, relational databases, and also cloud storage services such as Amazon
    S3 and Azure Data Lake Storage.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's get hands-on with Spark so that we can go deeper into the core APIs
    and libraries. In all of the chapters of this book, I will be referring to the
    2.2.1 release of Spark, however, several examples that are presented here should
    work with the 2.0 release or later. I will put a note when an example is specifically
    for 2.2+ releases only.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, you need to download Spark from its official website ([https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)).
    The download page should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f94983ad-59e1-4a1b-9846-824a1483a3b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4
  prefs: []
  type: TYPE_NORMAL
- en: You need to have JDK 1.8+ and Python 2.7+ or 3.4+ (only if you need to develop
    using this language). Spark 2.2.1 supports Scala 2.11\. The JDK needs to be present
    on your user path system variable, though, alternatively, you could have your
    user `JAVA_HOME` environment variable pointing to a JDK installation.
  prefs: []
  type: TYPE_NORMAL
- en: Extract the content of the downloaded archive to any local directory. Move to
    the `$SPARK_HOME/bin` directory. There, among the other executables, you will
    find the interactive Spark shells for Scala and Python. They are the best way
    to get familiar with this framework. In this chapter, I am going to present examples
    that you can run through these shells.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can run a Scala shell using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don''t specify an argument, Spark assumes that you''re running locally
    in standalone mode. Here''s the expected output to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The web UI is available at the following URL: `http://<host>:4040`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It will give you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c9ffd02-bce0-4053-9993-0e7524190af5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5
  prefs: []
  type: TYPE_NORMAL
- en: From there, you can check the status of your jobs and executors.
  prefs: []
  type: TYPE_NORMAL
- en: From the output of the console startup, you will notice that two built-in variables,
    `sc` and `spark`, are available. `sc` represents the `SparkContext` ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)),
    which in Spark < 2.0 was the entry point for each application. Through the Spark
    context (and its specializations), you can get input data from data sources, create
    and manipulate RDDs ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)),
    and attain the Spark primary abstraction before 2.0\. The *RDD programming* section
    will cover this topic and other operations in more detail. Starting from release
    2.0, a new entry point, `SparkSession` ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)),
    and a new main data abstraction, the Dataset ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)),
    were introduced. More details on them are presented in the following sections.
    The `SparkContext` is still part of the Spark API so that compatibility with existing
    frameworks not supporting Spark sessions is ensured, but the direction the project
    has taken is to move development to use the `SparkSession`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of how to read and manipulate a text file and put it into
    a Dataset using the Spark shell (the file used in this example is part of the
    resources for the examples that are bundled with the Spark distribution):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a Dataset instance that contains the file lines. You can then
    make several operations on this Dataset, such as counting the number of lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also get the first line of the Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we used a path on the local filesystem. In these cases, the
    file should be accessible from the same path by all of the workers, so you will
    need to copy the file across all workers or use a network-mounted shared filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'To close a shell, you can type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the list of all of the available shell commands, type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: All commands can be abbreviated, for example, `:he` instead of `:help`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the list of commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Commands** |  **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| `:edit <id>&#124;<line>`  | Edit history |'
  prefs: []
  type: TYPE_TB
- en: '| `:help [command]`  | Prints summary or command-specific help |'
  prefs: []
  type: TYPE_TB
- en: '| `:history [num]`   | Shows history (optional `num` is commands to show) |'
  prefs: []
  type: TYPE_TB
- en: '| `:h? <string>`     | Search history |'
  prefs: []
  type: TYPE_TB
- en: '| `:imports [name name ...]`  | Show import history, identifying the sources
    of names |'
  prefs: []
  type: TYPE_TB
- en: '| `:implicits [-v]`           | Show the `implicits` in scope |'
  prefs: []
  type: TYPE_TB
- en: '| `:javap <path&#124;class>`  | Disassemble a file or class name |'
  prefs: []
  type: TYPE_TB
- en: '| `:line <id>&#124;<line>`   | Place line(s) at the end of history |'
  prefs: []
  type: TYPE_TB
- en: '| `:load <path>`  | Interpret lines in a file |'
  prefs: []
  type: TYPE_TB
- en: '| `:paste [-raw] [path]`    | Enter paste mode or paste a file |'
  prefs: []
  type: TYPE_TB
- en: '| `:power`              | Enable power user mode |'
  prefs: []
  type: TYPE_TB
- en: '| `:quit`        | Exit the interpreter |'
  prefs: []
  type: TYPE_TB
- en: '| `:replay [options]`      | Reset the `repl` and `replay` on all previous
    commands |'
  prefs: []
  type: TYPE_TB
- en: '| `:require <path>`   | Add a `jar` to the classpath |'
  prefs: []
  type: TYPE_TB
- en: '| `:reset [options]`    | Reset the `repl` to its initial state, forgetting
    all session entries |'
  prefs: []
  type: TYPE_TB
- en: '| `:save <path>` | Save the replayable session to a file |'
  prefs: []
  type: TYPE_TB
- en: '| `:sh <command line>`      | Run a shell command (the result is `implicitly
    => List[String]`) |'
  prefs: []
  type: TYPE_TB
- en: '| `:settings <options>`  | Update compiler options, if possible; see `reset`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `:silent`           | Disable or enable the automatic printing of results
    |'
  prefs: []
  type: TYPE_TB
- en: '| `:type [-v] <expr>`     | Display the type of expression without evaluating it
    |'
  prefs: []
  type: TYPE_TB
- en: '| `:kind [-v] <expr>` | Display the kind of expression |'
  prefs: []
  type: TYPE_TB
- en: '| `:warnings`       | Show the suppressed warnings from the most recent line
    that had any |'
  prefs: []
  type: TYPE_TB
- en: 'Like Scala, an interactive shell is available for Python. You can run it using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'A built-in variable named `spark` representing the `SparkSession` is available.
    You can do the same things as for the Scala shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Unlike Java and Scala, Python is more dynamic and is not strongly typed. Therefore, a
    `DataSet` in Python is a `DataSet[Row]`, but you can call it a DataFrame so that
    it's consistent with the DataFrame concept of the Pandas framework ([https://pandas.pydata.org/](https://pandas.pydata.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To close a Python shell, you can type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Interactive shells aren''t the only choice for running code in Spark. It is
    also possible to implement self-contained applications. Here''s an example of
    reading and manipulating a file in Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Applications should define a `main()` method instead of extending `scala.App`.
    Note the code to create `SparkSession`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: It follows the builder factory design pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Always explicitly close the session before ending the program execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To build the application, you can use a build tool of your choice (`Maven`,
    `sbt`, or `Gradle`), adding the dependencies from Spark 2.2.1 and Scala 2.11\.
    Once a JAR file has been generated, you can use the `$SPARK_HOME/bin/spark-submit`
    command to execute it, specifying the JAR filename, the Spark master URL, and
    a list of optional parameters, including the job name, the main class, the maximum
    memory to be used by each executor, and many others.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same self-contained application could have been implemented in Python as
    well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This can be saved in a `.py` file and submitted through the same `$SPARK_HOME/bin/spark-submit`
    command for execution.
  prefs: []
  type: TYPE_NORMAL
- en: RDD programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, every Spark application is a driver program that runs the logic
    that has been implemented for it and executes parallel operations on a cluster.
    In accordance with the previous definition, the main abstraction provided by the
    core Spark framework is the RDD. It is an immutable distributed collection of
    data that is partitioned across machines in a cluster. Operations on RDDs can
    happen in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two types of operations are available on an RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **transformation** is an operation on an RDD that produces another RDD, while
    an **action** is an operation that triggers some computation and then returns
    a value to the master or can be persisted to a storage system. Transformations
    are lazy—they aren't executed until an action is invoked. Here's the strength
    point of Spark—Spark masters and their drivers both remember the transformations
    that have been applied to an RDD, so if a partition is lost (for example, a slave
    goes down), it can be easily rebuilt on some other node of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists some of the common transformations supported by Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Transformation** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| `map(func)` | Returns a new RDD by applying the `func` function on each data
    element of the source RDD. |'
  prefs: []
  type: TYPE_TB
- en: '| `filter(func)` | Returns a new RDD by selecting those data elements for which
    the applied `func` function returns `true`. |'
  prefs: []
  type: TYPE_TB
- en: '| `flatMap(func)` | This transformation is similar to `map`: the difference
    is that each input item can be mapped to zero or multiple output items (the applied `func` function
    should return a `Seq`). |'
  prefs: []
  type: TYPE_TB
- en: '| `union(otherRdd)` | Returns a new RDD that contains the union of the elements
    in the source RDD and the `otherRdd` argument. |'
  prefs: []
  type: TYPE_TB
- en: '| `distinct([numPartitions])` | Returns a new RDD that contains only the distinct
    elements of the source RDD. |'
  prefs: []
  type: TYPE_TB
- en: '| `groupByKey([numPartiotions])` | When called on an RDD of (*K*, *V*) pairs,
    it returns an RDD of (*K*, *Iterable<V>*) pairs. By default, the level of parallelism
    in the output RDD depends on the number of partitions of the source RDD. You can
    pass an optional `numPartitions` argument to set a different number of partitions.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `reduceByKey(func, [numPartitions])` | When called on an RDD of (*K*, *V*)
    pairs, it returns an RDD of (*K*, *V*) pairs, where the values for each key are
    aggregated using the given reduce `func` function, which must be of type *(V*,*V)
    => V*. The same as for the `groupByKey` transformation, the number of reduce partitions
    is configurable through an optional `numPartitions` second argument.  |'
  prefs: []
  type: TYPE_TB
- en: '| `sortByKey([ascending], [numPartitions])` | When called on an RDD of (*K*,
    *V*) pairs, it returns an RDD of (*K*, *V*) pairs sorted by keys in ascending
    or descending order, as specified in the Boolean `ascending` argument. The number
    of partitions for the output RDD is configurable through an optional `numPartitions`
    second argument. |'
  prefs: []
  type: TYPE_TB
- en: '| `join(otherRdd, [numPartitions])` | When called on RDDs of type (*K*, *V*)
    and (*K*, *W*), it returns an RDD of (*K*, (*V*, *W*)) pairs with all pairs of
    elements for each key. It supports left outer join, right outer join, and full
    outer join. The number of partitions for the output RDD is configurable through
    an optional `numPartitions` second argument. |'
  prefs: []
  type: TYPE_TB
- en: 'The following table lists some of the common actions supported by Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Action** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| `reduce(func)` | Aggregates the elements of an RDD using a given function, `func`
    (this takes two arguments and returns one). To ensure the correct parallelism
    at compute time, the reduce function, `func`, has to be commutative and associative.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `collect()` | Returns all the elements of an RDD as an array to the driver.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `count()` | Returns the total number of elements in an RDD. |'
  prefs: []
  type: TYPE_TB
- en: '| `first()` | Returns the first element of an RDD. |'
  prefs: []
  type: TYPE_TB
- en: '| `take(n)` | Returns an array containing the first *n* elements of an RDD.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `foreach(func)` | Executes the `func` function on each element of an RDD.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `saveAsTextFile(path)` | Writes the elements of an RDD as a text file in
    a given directory (with the absolute location specified through the `path` argument)
    in the local filesystem, HDFS, or any other Hadoop-supported filesystem. This
    is available for Scala and Java only. |'
  prefs: []
  type: TYPE_TB
- en: '| `countByKey()` | This action is only available on RDDs of type (*K*, *V*)
    – it returns a hashmap of (*K*, *Int*) pairs, where *K* is a key of the source
    RDD and its value is the count for that given key, *K*. |'
  prefs: []
  type: TYPE_TB
- en: 'Now, let''s understand the concepts of transformation and action through an
    example that could be executed in the Scala shell—this finds the *N* most commonly
    used words in an input text file. The following diagram depicts a potential implementation
    for this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5572d5c-5e5e-42db-8f01-0c624d1dc699.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6
  prefs: []
  type: TYPE_NORMAL
- en: Let's translate this into code.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let''s load the content of a text file into an RDD of strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will apply the necessary transformations and actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`flatMap(str=>str.split(" "))`: Splits each line into single words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter(!_.isEmpty)`: Removes empty strings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map(word=>(word,1))`: Maps each word into a key-value pair'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduceByKey(_+_)`: Aggregates the count'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map{case(word, count) => (count, word)}`: Reverses the `(word, count)` pairs
    to `(count, word)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sortByKey(false)`: Sorts by descending order'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, print the five most used words in the input content to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The same could be achieved in Python in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The result, of course, is the same as for the Scala example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark can persist RDDs (and Datasets as well) in memory while executing operations
    on them. Persisting and caching are synonyms in Spark. When persisting an RDD,
    each node of the cluster stores the RDD partitions that it needs to compute in
    memory and reuses them in further actions on the same RDD (or RDDs that have been
    derived from it through some transformations). This is the reason why future actions
    execute much faster. It is possible to mark an RDD to be persisted using its `persist()`
    method. The first time an action is executed on it, it will be kept in memory
    on the cluster''s nodes. The Spark cache is fault-tolerant—this means that, if
    for any reason all of the partitions of an RDD are lost, it will be automatically
    recalculated using the transformations that created it. A persisted RDD can be
    stored using different storage levels. Levels can be set by passing a `StorageLevel`
    object to the `persist()` method of the RDD. The following table lists all of
    the available storage levels and their meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Storage Level** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| `MEMORY_ONLY` | This is the default storage level. It stores RDDs as deserialized
    Java objects in memory. In those cases where an RDD shouldn''t fit in memory,
    some of its partitions won''t be cached and will be recalculated on the fly when
    needed. |'
  prefs: []
  type: TYPE_TB
- en: '| `MEMORY_AND_DISK` | It stores RDDs as deserialized Java objects in memory
    first, but, in those cases where an RDD shouldn''t fit in memory, it stores some
    partitions on disk (this is the main difference between `MEMORY_ONLY`), and reads
    them from there when needed. |'
  prefs: []
  type: TYPE_TB
- en: '| `MEMORY_ONLY_SER` | It stores RDDs as serialized Java objects. Compared to
    `MEMORY_ONLY`, this is more space-efficient, but more CPU-intensive in read operations.
    This is available for JVM languages only. |'
  prefs: []
  type: TYPE_TB
- en: '| `MEMORY_AND_DISK_SER` | Is similar to `MEMORY_ONLY_SER` (it stores RDDs as
    serialized Java objects), with the main difference being that it stores partitions
    that don''t fit in memory to disk. This is available only for JVM languages. |'
  prefs: []
  type: TYPE_TB
- en: '| `DISK_ONLY` | It stores the RDD partitions on disk only. |'
  prefs: []
  type: TYPE_TB
- en: '| `MEMORY_ONLY_2`, `MEMORY_AND_DISK_2`, and so on | The same as the two preceding
    levels (`MEMORY_ONLY` and `MEMORY_AND_DISK`), but each partition is replicated
    on two cluster nodes. |'
  prefs: []
  type: TYPE_TB
- en: '| `OFF_HEAP` | Similar to `MEMORY_ONLY_SER`, but it stores data in off-heap
    memory (assuming off-heap memory is enabled). Please be careful when using this
    storage level as it is still experimental. |'
  prefs: []
  type: TYPE_TB
- en: When a function is passed to a Spark operation, it is executed on a remote cluster
    node that will work on separate copies of all the variables that are used in the
    function. Once done, the variables will be copied to each machine. There will
    be no updates to the variables on the remote machine when propagated back to the
    driver program. It would be inefficient to support general, read-write shared
    variables across tasks.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are two limited types of shared variables that are available
    in Spark for two common usage patterns – broadcast variables and accumulators.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common operations in Spark programming is to perform joins
    on RDDs to consolidate data by a given key. In these cases, it is quite possible
    to have large Datasets sent around to slave nodes that host the partitions to
    be joined. You can easily understand that this situation presents a huge performance
    bottleneck, as network I/O is 100 times slower than RAM access. To mitigate this
    issue, Spark provides broadcast variables, which are broadcast to slave nodes.
    RDD operations on the nodes can quickly access the broadcast variable value. Spark
    also attempts to distribute broadcast variables using efficient broadcast algorithms
    to reduce communication costs. Broadcast variables are created from a variable, *v*,
    by calling the `SparkContext.broadcast(v)` method. The broadcast variable is a
    wrapper around *v*, and its value can be obtained by calling the `value` method.
    Here''s an example in Scala that you can run through the Spark shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: After its creation, the broadcast variable, `broadcastVar`, can be used in any
    function that's executed on the cluster, but not the initial value, *v,* as this
    prevents *v* being shipped to all the nodes more than once. To ensure that all
    the nodes get the same value of the broadcast variable, *v* must not be modified
    after `broadcastVar` has been broadcast.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the code for the same example in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To aggregate information across executors in a Spark cluster, `accumulator`
    variables should be used. The fact that they are added through an associative
    and commutative operation ensures their efficient support in parallel computation.
    Spark natively provides support for the accumulators of numeric types—they can
    be created by calling `SparkContext.longAccumulator()` (to accumulate values of
    type `Long`) or `SparkContext.doubleAccumulator()` (to accumulate values of type
    `Double`) methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it is possible to programmatically add support for other types. Any
    task running on a cluster can add to an accumulator using the `add` method, but
    they cannot read its value – this operation is only allowed for the driver program,
    which uses its `value` method. Here''s a code example in Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, an accumulator has been created, and has assigned a name to it.
    It is possible to create unnamed accumulators, but a named accumulator will display
    in the web UI for the stage that modifies that accumulator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54fa8bc2-1eb4-4f0a-bed6-8f5ce959ba84.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7
  prefs: []
  type: TYPE_NORMAL
- en: This can be helpful for understanding the progress of running stages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same example in Python is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Tracking accumulators in the web UI isn't supported for Python.
  prefs: []
  type: TYPE_NORMAL
- en: Please be aware that Spark guarantees to update accumulators *inside actions
    only*. When restarting a task, the accumulators will be updated only once. The
    same isn't true for transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL, Datasets, and DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark SQL is the Spark module for structured data processing. The main difference
    between this API and the RDD API is that the provided Spark SQL interfaces give
    more information about the structure of both the data and the performed computation.
    This extra information is used by Spark internally to add extra optimizations
    through the Catalyst optimization engine, which is the same execution engine that's
    used regardless of whatever API or programming language is involved.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL is commonly used to execute SQL queries (even if this isn't the only
    way to use it). Whatever programming language supported by Spark encapsulates
    the SQL code to be executed, the results of a query are returned as a **Dataset**.
    A Dataset is a distributed collection of data, and was added as an interface in
    Spark 1.6\. It combines the benefits of RDDs (such as strong typing and the ability
    to apply useful lambda functions) with the benefits of Spark SQL's optimized execution
    engine (Catalyst, [https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)).
    You can construct a Dataset by starting with Java/Scala objects and then manipulating
    it through the usual functional transformations. The Dataset API is available
    in Scala and Java, while Python doesn't have support for it. However, due to the
    dynamic nature of this programming language, many of the benefits of the Dataset
    API are already available for it.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from Spark 2.0, the DataFrame and Dataset APIs have been merged into
    the Dataset API, so a **DataFrame** is just a Dataset that's been organized into
    named columns and is conceptually equivalent to a table in an RDBMS, but with
    better optimizations under the hood (being part of the Dataset API, the Catalyst
    optimization engine works behind the scenes for DataFrames, too). You can construct
    a DataFrame from diverse sources, such as structured data files, Hive tables,
    database tables, and RDDs, to name a few. Unlike the Dataset API, the DataFrame
    API is available in any of the programming languages that are supported by Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start and get hands-on so that we can better understand the concepts behind
    Spark SQL. The first full example I am going to show is Scala-based. Start a Scala
    Spark shell to run the following code interactively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use `people.json` as a data source. One of the files that''s available
    as a resource for this example has been shipped along with the Spark distribution
    and can be used to create a DataFrame that''s a Dataset of Rows ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You can print the content of the DataFrame to the console to check that it
    is what you expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Before you perform DataFrame operations, you need to import the implicit conversions
    (such as converting RDDs to DataFrames) and use the `$` notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can print the DataFrame schema in a tree format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Select a single column (let''s say `name`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Filter the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then add a  `groupBy` clause:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Select all rows and increment a numeric field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to run SQL queries programmatically through the `sql` function
    of `SparkSession`. This function returns the results of the query in a DataFrame,
    which, for Scala, is a `Dataset[Row]`. Let''s consider the same DataFrame as for
    the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You can register it as an SQL temporary view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can execute an SQL query there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The same things can be done in Python as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Resulting in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Other features of Spark SQL and Datasets (data sources, aggregations, self-contained
    applications, and so on) will be covered in [Chapter 3](44fab060-12c9-4eec-9e15-103da589a510.xhtml), *Extract,
    Transform, Load*.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark Streaming is another Spark module that extends the core Spark API and
    provides a scalable, fault-tolerant, and efficient way of processing live streaming
    data. By converting streaming data into *micro* batches, Spark''s simple batch
    programming model can be applied in streaming use cases too. This unified programming
    model makes it easy to combine batch and interactive data processing with streaming.
    Diverse sources that ingest data are supported (Kafka, Kinesis, TCP sockets, S3,
    or HDFS, just to mention a few of the popular ones), as well as data coming from
    them, and can be processed using any of the high-level functions available in
    Spark. Finally, the processed data can be persisted to RDBMS, NoSQL databases,
    HDFS, object storage systems, and so on, or consumed through live dashboards.
    Nothing prevents other advanced Spark components, such as MLlib or GraphX, being
    applied to data streams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c308eea-7aa6-4d4c-a97a-8493b28bdaef.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how Spark Streaming works internally—it receives
    live input data streams and divides them into batches; these are processed by
    the Spark engine to generate the final batches of results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bef871b-ee77-48d0-bb68-d403cd2acc13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9
  prefs: []
  type: TYPE_NORMAL
- en: The higher-level abstraction of Spark Streaming is the **DStream** (short for
    **Discretized Stream**), which is a wrapper around a continuous flow of data.
    Internally, a DStream is represented as a sequence of RDDs. A DStream contains
    a list of other DStreams that it depends on, a function to convert its input RDDs
    into output ones, and a time interval at which to invoke the function. DStreams
    are created by either manipulating existing ones, for example, applying a map
    or filter function (which internally creates `MappedDStreams` and `FilteredDStreams`,
    respectively), or by reading from an external source (the base class in these
    cases is `InputDStream`).
  prefs: []
  type: TYPE_NORMAL
- en: Let's implement a simple Scala example—a streaming word count self-contained
    application. The code used for this class can be found among the examples that
    are bundled with the Spark distribution. To compile and package it, you need to
    add the dependency to Spark Streaming to your `Maven`, `Gradle`, or `sbt` project
    descriptor, along with the dependencies from Spark Core and Scala.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to create the `SparkConf` and a `StreamingContext` (which is
    the main entry point for any streaming functionality) from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The batch interval has been set to 1 second. A DStream representing streaming
    data from a TCP source can be created using the `ssc` streaming context; we need
    just to specify the source hostname and port, as well as the desired storage level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned `lines` DStream is the stream of data that is going to be received
    from the server. Each record will be a single line of text that we want to split
    into single words, thus specifying the space character as a separator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will count those words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The `words` DStream is mapped (a one-to-one transformation) to a DStream of
    (*word*, *1*) pairs, which is then reduced to get the frequency of words in each
    batch of data. The last command will print a few of the counts that are generated
    every second. Each RDD in a DStream contains data from a certain interval – any
    operation applied on a DStream translates to operations on the underlying RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/284d70b8-6143-4b6d-bce0-41c7bea81814.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the processing after all the transformations have been set up, use
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Before running this example, first you will need to run `netcat` (a small utility
    found in most Unix-like systems) as a data server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in a different Terminal, you can start the example by passing the following
    as arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Any line that's typed into the Terminal and run with the `netcat` server will
    be counted and printed on the application screen every second.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of whether `nc` shouldn''t be available in the system where you
    run this example, you can implement your own data server in Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The same self-contained application in Python could be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'DStreams support most parts of the transformations that are available for RDDs.
    This means that data from input DStreams can be modified in the same way as the
    data in RDDs. The following table lists some of the common transformations supported
    by Spark DStreams:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Transformation** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| `map(func)` | Returns a new DStream. The `func` map function is applied to
    each element of the source DStream. |'
  prefs: []
  type: TYPE_TB
- en: '| `flatMap(func)` | The same as for `map`. The only difference is that each
    input item in the new DStream can be mapped to 0 or more output items. |'
  prefs: []
  type: TYPE_TB
- en: '| `filter(func)` | Returns a new DStream containing only the elements of the
    source DStream for which the `func` filter function returned true. |'
  prefs: []
  type: TYPE_TB
- en: '| `repartition(numPartitions)` | This is used to set the level of parallelism
    by creating a different number of partitions. |'
  prefs: []
  type: TYPE_TB
- en: '| `union(otherStream)` | Returns a new DStream. It contains the union of the
    elements in the source DStream and the input `otherDStream` DStream. |'
  prefs: []
  type: TYPE_TB
- en: '| `count()` | Returns a new DStream. It contains single element RDDs that are
    obtained by counting the number of elements contained in each RDD arriving from
    the source. |'
  prefs: []
  type: TYPE_TB
- en: '| `reduce(func)` | Returns a new DStream. It contains single element RDDs that
    are obtained by aggregating those in each RDD of the source by applying the `func` function
    (which should be associative and commutative to allow for correct parallel computation).
    |'
  prefs: []
  type: TYPE_TB
- en: '| `countByValue()` | Returns a new DStream of (*K*, *Long*) pairs, where *K*
    is the type of the elements of the source. The value of each key represents its
    frequency in each RDD of the source. |'
  prefs: []
  type: TYPE_TB
- en: '| `reduceByKey(func, [numTasks])` | Returns a new DStream of (*K*, *V*) pairs
    (for a source DStream of (*K*, *V*) pairs). The values for each key are aggregated
    by applying the reduce `func` function. To do the grouping, this transformation
    uses Spark''s default number of parallel tasks (which is two in local mode, while
    it is determined by the `config` property `spark.default.parallelism` in cluster
    mode), but this can be changed by passing an optional `numTasks` argument. |'
  prefs: []
  type: TYPE_TB
- en: '| `join(otherStream, [numTasks])` | Returns a new DStream of (*K*, (*V*, *W*))
    pairs when called on two DStreams of (*K*, *V*) and (*K*, *W*) pairs, respectively.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `cogroup(otherStream, [numTasks])` | Returns a new DStream of (*K*, *Seq[V]*,
    *Seq[W]*) tuples when called on two DStreams of (*K*, *V*) and (*K*, *W*) pairs,
    respectively. |'
  prefs: []
  type: TYPE_TB
- en: '| `transform(func)` | Returns a new DStream. It applies an RDD-to-RDD `func` function
    to every RDD of the source. |'
  prefs: []
  type: TYPE_TB
- en: '| `updateStateByKey(func)` | Returns a new state DStream. The state for each
    key in the new DStream is updated by applying the `func` input function to the
    previous state and the new values for the key. |'
  prefs: []
  type: TYPE_TB
- en: 'Windowed computations are provided by Spark Streaming. As shown in the following
    diagram, they allow you to apply transformations over sliding windows of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ce148aa-91a4-4ed9-b146-ebcf9cc70cff.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11
  prefs: []
  type: TYPE_NORMAL
- en: 'When a window slides over a source DStream, all its RDDs that fall within that
    window are taken into account and transformed to produce the RDDs of the returned
    windowed DStream. Looking at the specific example that''s shown in the preceding
    diagram, the window-based operation is applied over three time units of data and
    it slides by two. Two parameters need to be specified by any window operation
    that''s used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Window length**: The duration of the window'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sliding interval**: The interval at which the window operation is performed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two parameters must be multiples of the batch interval of the source DStream.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this could be applied to the application that was presented
    at the beginning of this section. Suppose you want to generate a word count every
    10 seconds over the last 60 seconds of data. The `reduceByKey` operation needs
    to be applied on the (*word*, *1*) pairs of the DStream over the last 60 seconds
    of data. This can be achieved with the `reduceByKeyAndWindow` operation. When
    translated into Scala code, this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'For Python, it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table lists some of the common window operations supported by
    Spark for DStreams:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Transformation** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| `window(windowLength, slideInterval)` | Returns a new DStream. It is based
    on windowed batches of the source. |'
  prefs: []
  type: TYPE_TB
- en: '| `countByWindow(windowLength, slideInterval)` | Returns a sliding window count
    (based on the `windowLength` and `slideInterval` parameters) of elements in the
    source DStream. |'
  prefs: []
  type: TYPE_TB
- en: '| `reduceByWindow(func, windowLength, slideInterval)` | Returns a new single
    element DStream. It is created by aggregating elements in the source DStream over
    a sliding interval by applying the `func` reduce function (which, to allow for
    correct parallel computation, is associative and commutative). |'
  prefs: []
  type: TYPE_TB
- en: '| `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])` | Returns
    a new DStream of (*K*, *V*) pairs (the same *K* and *V* as for the source DStream).
    The values for each key are aggregated using the `func` input function over batches
    (defined by the `windowLength` and `slideInterval` arguments) in a sliding window.
    The number of parallel tasks to do the grouping is two (default) in local mode,
    while in cluster mode this is given by the Spark configuration property `spark.default.parallelism.
    numTask`, which is an optional argument to specify a custom number of tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| `reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])`
    | This is a more efficient version of the `reduceByKeyAndWindow` transformation.
    This time, the reduce value of the current window is calculated incrementally
    using the reduce values of the previous one. This happens by reducing the new
    data that enters a window while inverse reducing the old data that leaves the
    same one. Please note that this mechanism only works if the `func` reduce function
    has a corresponding inverse reduce function, `invFunc`. |'
  prefs: []
  type: TYPE_TB
- en: '| `countByValueAndWindow(windowLength, slideInterval, [numTasks])` | Returns
    a DStream of (*K*, *Long*) pairs (whatever (*K*, *V*) pairs the source DStream
    is made of). The value of each key in the returned DStream is its frequency within
    a given sliding window (defined by the `windowLength` and `slideInterval` arguments).
    `numTask` is an optional argument to specify a custom number of tasks. |'
  prefs: []
  type: TYPE_TB
- en: Cluster mode using different managers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following diagram shows how Spark applications run on a cluster. They are
    independent sets of processes that are coordinated by the `SparkContext` object
    in the **Driver Program**. `SparkContext` connects to a **Cluster Manager**, which
    is responsible for allocating resources across applications. Once the **SparkContext**
    is connected, Spark gets executors across cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Executors are processes that execute computations and store data for a given
    Spark application. **SparkContext** sends the application code (which could be
    a JAR file for Scala or .py files for Python) to the executors. Finally, it sends
    the tasks to run to the executors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb1e7d7d-9bfd-4eb2-88e2-c35d41606cb9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12
  prefs: []
  type: TYPE_NORMAL
- en: To isolate applications from each other, every Spark application receives its
    own executor processes. They stay alive for the duration of the whole application
    and run tasks in multithreading mode. The downside to this is that it isn't possible
    to share data across different Spark applications – to share it, data needs to
    be persisted to an external storage system.
  prefs: []
  type: TYPE_NORMAL
- en: Spark supports different cluster managers, but it is agnostic to the underlying
    type.
  prefs: []
  type: TYPE_NORMAL
- en: The driver program, at execution time, must be network addressable from the
    worker nodes because it has to listen for and accept incoming connections from
    its executors. Because it schedules tasks on the cluster, it should be executed
    close to the worker nodes, on the same local area network (if possible).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the cluster managers that are currently supported in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standalone**: A simple cluster manager that makes it easy to set up a cluster.
    It is included with Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Mesos**: An open source project that''s used to manage computer clusters,
    and was developed at the University of California, Berkeley.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop YARN**: The resource manager available in Hadoop starting from release
    2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes**: An open source platform for providing a container-centric infrastructure.
    Kubernetes support in Spark is still experimental, so it''s probably not ready
    for production yet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standalone mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For standalone mode, you only need to place a compiled version of Spark on
    each node of the cluster. All the cluster nodes need to be able to resolve the
    hostnames of the other cluster members and are routable to one another. The Spark
    master URL can be configured in the `$SPARK_HOME/conf/spark-defaults.conf` file
    on all of the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the hostname or IP address of the Spark master node needs to be specified
    in the `$SPARK_HOME/conf/spark-env.sh` file on all of the nodes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'It is now possible to start a standalone master server by executing the following
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the master has completed, a web UI will be available at the `http://<master_hostname_or_IP>:8080`
    URL. From there, it is possible to obtain the master URL that''s to be used when
    starting the workers. One or more workers can now be started by executing the
    following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Each worker, after the start, comes with its own web UI, whose URL is `http://<worker_hostname_or_IP>:8081`.
  prefs: []
  type: TYPE_NORMAL
- en: The list of workers, along with other information about their number of CPUs
    and memory, can be found in the master's web UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way to do this is to run a standalone cluster manually. It is also possible
    to use the provided launch scripts. A `$SPARK_HOME/conf/slaves` file needs to
    be created as a preliminary step. It must contain the hostnames – one per line –
    of all of the machines where the Spark workers should start. Passwordless **SSH**
    (short for **Secure Shell**) for the Spark master to the Spark slaves needs to
    be enabled to allow remote login for the slave daemon startup and shutdown actions.
    A cluster can then be launched or stopped using the following shell scripts, which
    are available in the `$SPARK_HOME/sbin` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`start-master.sh`: Starts a master instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start-slaves.sh`: Starts a slave instance on each machine specified in the
    `conf/slaves` file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start-slave.sh`: Starts a single slave instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start-all.sh`: Starts both a master and a number of slaves'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stop-master.sh`: Stops a master that has been started via the `sbin/start-master.sh`
    script'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stop-slaves.sh`: Stops all slave instances on the nodes specified in the `conf/slaves`
    file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stop-all.sh`: Stops both a master and its slaves'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These scripts must be executed on the machine the Spark master will run on.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to run an interactive Spark shell against a cluster in the following
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The `$SPARK_HOME/bin/spark-submit` script can be used to submit a compiled
    Spark application to the cluster. Spark currently supports two deploy modes for
    standalone clusters: client and cluster. In client mode, the driver and the client
    that submits the application are launched in the same process, while in cluster
    mode, the driver is launched from one of the worker processes and the client process
    exits as soon as it completes submitting the application (it doesn''t have to
    wait for the application to finish).'
  prefs: []
  type: TYPE_NORMAL
- en: When an application is launched through `spark-submit`, then its JAR file is
    automatically distributed to all the worker nodes. Any additional JAR that an
    application depends on should be specified through the `jars` flag using a comma
    as a delimiter (for example, `jars`, `jar1`, `jar2`).
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the *Apache Spark fundamentals* section, in standalone mode,
    the Spark master is a single point of failure. This means that if the Spark master
    node should go down, the Spark cluster would stop functioning and all currently
    submitted or running applications would fail, and it wouldn't be possible to submit
    new applications.
  prefs: []
  type: TYPE_NORMAL
- en: High availability can be configured using Apache ZooKeeper ([https://zookeeper.apache.org/](https://zookeeper.apache.org/)),
    an open source and highly reliable distributed coordination service, or can be
    deployed as a cluster through Mesos or YARN, which we will talk about in the following
    two sections.
  prefs: []
  type: TYPE_NORMAL
- en: Mesos cluster mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark can run on clusters that are managed by Apache Mesos ([http://mesos.apache.org/](http://mesos.apache.org/)).
    Mesos is a cross-platform, cloud provider-agnostic, centralized, and fault-tolerant
    cluster manager, designed for distributed computing environments. Among its main
    features, it provides resource management and isolation, and the scheduling of
    CPU and memory across the cluster. It can join multiple physical resources into
    a single virtual one, and in doing so is different from classic virtualization,
    where a single physical resource is split into multiple virtual resources. With
    Mesos, it is possible to build or schedule cluster frameworks such as Apache Spark
    (though it is not restricted to just this). The following diagram shows the Mesos
    architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/799fa1d4-15de-4229-abc1-907ef787edb9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13
  prefs: []
  type: TYPE_NORMAL
- en: Mesos consists of a master daemon and frameworks. The master daemon manages
    agent daemons running on each cluster node, while the Mesos frameworks run tasks
    on the agents. The master empowers fine-grained sharing of resources (including
    CPU and RAM) across frameworks by making them resource offers. It decides how
    much of the available resources to offer to each framework, depending on given
    organizational policies. To support diverse sets of policies, the master uses
    a modular architecture that makes it easy to add new allocation modules through
    a plugin mechanism. A Mesos framework consists of two components – a scheduler,
    which registers itself with the master to be offered resources, and an executor,
    a process that is launched on agent nodes to execute the framework's tasks. While
    it is the master that determines how many resources are offered to each framework,
    the frameworks' schedulers are responsible for selecting which of the offered
    resources to use. The moment a framework accepts offered resources, it passes
    a description of the tasks it wants to execute on them to Mesos. Mesos, in turn,
    launches the tasks on the corresponding agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of deploying a Spark cluster using Mesos to replace the Spark
    Master Manager include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic partitioning between Spark and other frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalable partitioning between multiple instances of Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark 2.2.1 is designed to be used with Mesos 1.0.0+. In this section, I won''t
    describe the steps to deploy a Mesos cluster – I am assuming that a Mesos cluster
    is already available and running. No particular procedure or patch is required
    in terms of Mesos installation to run Spark on it. To verify that the Mesos cluster
    is ready for Spark, navigate to the Mesos master web UI at port `5050`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b907d35e-657d-4d72-a920-01dbf429c2ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14
  prefs: []
  type: TYPE_NORMAL
- en: Check that all of the expected machines are present in the Agents tab.
  prefs: []
  type: TYPE_NORMAL
- en: To use Mesos from Spark, a Spark binary package needs to be available in a place
    that's accessible by Mesos itself, and a Spark driver program needs to be configured
    to connect to Mesos. Alternatively, it is possible to install Spark in the same
    location across all the Mesos slaves and then configure the `spark.mesos.executor.home`
    property (the default value is `$SPARK_HOME`) to point to that location.
  prefs: []
  type: TYPE_NORMAL
- en: The Mesos master URLs have the form `mesos://host:5050` for a single-master
    Mesos cluster, or `mesos://zk://host1:2181,host2:2181,host3:2181/mesos` for a
    multi-master Mesos cluster when using Zookeeper.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of how to start a Spark shell on a Mesos cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'A Spark application can be submitted to a Mesos managed Spark cluster as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: YARN cluster mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: YARN ([http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html](http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html)),
    which was introduced in Apache Hadoop 2.0, brought significant improvements in
    terms of scalability, high availability, and support for different paradigms.
    In the Hadoop version 1 **MapReduce** framework, job execution was controlled
    by types of processes—a single master process called `JobTracker` coordinates
    all the jobs running on the cluster and assigns `map` and `reduce` tasks to run
    on the `TaskTrackers`, which are a number of subordinate processes running assigned
    tasks and periodically reporting the progress to the `JobTracker`. Having a single
    `JobTracker` was a scalability bottleneck. The maximum cluster size was a little
    more than 4,000 nodes, with the number of concurrent tasks limited to 40,000\.
    Furthermore, the `JobTracker` was a single point of failure and the only available
    programming model was **MapReduce**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental idea of YARN is to split up the functionalities of resource
    management and job scheduling or monitoring into separate daemons. The idea is
    to have a global **ResourceManager** and per-application **ApplicationMaster**
    (**App Mstr**). An application is either a single job or a DAG of jobs. The following
    is a diagram of YARN''s architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43bba525-0732-49a6-a9fc-52110b582088.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15
  prefs: []
  type: TYPE_NORMAL
- en: The **ResourceManager** and the **NodeManager** form the YARN framework. The
    **ResourceManager** decides on resource usage across all the running applications,
    while the **NodeManager** is an agent running on any machine in the cluster and
    is responsible for the containers by monitoring their resource usage (including
    CPU and memory) and reporting to the **ResourceManager**. The **ResourceManager**
    consists of two components – the scheduler and the ApplicationsManager. The scheduler
    is the component that's responsible for allocating resources to the various applications
    running, and it doesn't perform any monitoring of applications' statuses, nor
    offer guarantees about restarting any failed tasks. It performs scheduling based
    on an application's resource requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The ApplicationsManager accepts job submissions and provides a service to restart
    the **App Mstr** container on any failure. The per-application **App Mstr** is
    responsible for negotiating the appropriate resource containers from the scheduler
    and monitoring their status and progress. YARN, by its nature, is a general scheduler,
    so support for non-MapReduce jobs (such as Spark jobs) is available for Hadoop
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Submitting Spark applications on YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To launch Spark applications on YARN, the `HADOOP_CONF_DIR or YARN_CONF_DIR
    env` variable needs to be set and pointing to the directory that contains the
    client-side configuration files for the Hadoop cluster. These configurations are
    needed to connect to the YARN ResourceManager and to write to HDFS. This configuration
    is distributed to the YARN cluster so that all the containers used by the Spark
    application have the same configuration. To launch Spark applications on YARN,
    two deployment modes are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '** Cluster mode**: In this case, the Spark driver runs inside an application
    master process that''s managed by YARN on the cluster. The client can finish its
    execution after initiating the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Client mode**: In this case, the driver runs and the client runs in the same
    process. The application master is used for the sole purpose of requesting resources
    from YARN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike the other modes, in which the master's address is specified in the `master`
    parameter, in YARN mode, the ResourceManager's address is retrieved from the Hadoop
    configuration. Therefore, the `master` parameter value is always `yarn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following command to launch a Spark application in cluster
    mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: In cluster mode, since the driver runs on a different machine than the client,
    the `SparkContext.addJar` method doesn't work with the files that are local to
    the client. The only choice is to include them using the `jars` option in the
    `launch` command.
  prefs: []
  type: TYPE_NORMAL
- en: Launching a Spark application in client mode happens the same way—the `deploy-mode`
    option value needs to change from cluster to client.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes cluster mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Kubernetes** ([https://kubernetes.io/](https://kubernetes.io/)) is an open
    source system that''s used automate the deployment, scaling, and management of
    containerized applications. It was originally implemented at Google and then open
    sourced in 2014\. The following are the main concepts of Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pod**: This is the smallest deployable unit of computing that can be created
    and managed. A pod can be seen as a group of one or more containers that share
    network and storage space, which also contains a specification for how to run
    those containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment**: This is a layer of abstraction whose primary purpose is to
    declare how many replicas of a pod should be running at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ingress**: This is an open channel for communication with a service running
    in a pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node**: This is a representation of a single machine in a cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Persistent volume**: This provides a filesystem that can be mounted to a
    cluster, not to be associated with any particular node. This is the way Kubernetes
    persists information (data, files, and so on).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram (source: [https://d33wubrfki0l68.cloudfront.net/518e18713c865fe67a5f23fc64260806d72b38f5/61d75/images/docs/post-ccm-arch.png](https://d33wubrfki0l68.cloudfront.net/518e18713c865fe67a5f23fc64260806d72b38f5/61d75/images/docs/post-ccm-arch.png))
    shows the Kubernetes architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f5c0874-a4ff-436e-ab31-90d0af1728bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16
  prefs: []
  type: TYPE_NORMAL
- en: 'The main components of the Kubernetes architecture are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud controller manager**: It runs the Kubernetes controllers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controllers**: There are four of them—node, route, service, and PersistenceVolumeLabels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubelets**: The primary agents that run on nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The submission of Spark jobs to a Kubernetes cluster can be done directly through
    `spark-submit`. Kubernetes requires that we supply Docker ([https://www.docker.com/](https://www.docker.com/))
    images that can be deployed into containers within pods. Starting from the 2.3
    release, Spark provides a Dockerfile (`$SPARK_HOME/kubernetes/dockerfiles/Dockerfile`,
    which can also be customized to match specific applications'' needs) and a script
    (`$SPARK_HOME/bin/docker-image-tool.sh`) that can be used to build and publish
    Docker images that are to be used within a Kubernetes backend. The following is
    the syntax that''s used to build a Docker image through the provided script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This following is the syntax to push an image to a Docker repository while
    using the same script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'A job can be submitted in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes requires application names to contain only lowercase alphanumeric
    characters, hyphens, and dots, and to start and end with an alphanumeric character.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the way the submission mechanism works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b892e4b-7efe-4945-865d-479e6a496840.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.17
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark creates a driver that's running within a Kubernetes pod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The driver creates the executors, which also run within Kubernetes pods, and
    then connects to them and executes application code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the execution, the executor pods terminate and are cleaned up,
    while the driver pod still persists logs and remains in a completed state (which
    means that it doesn't use cluster computation or memory resources) in the Kubernetes
    API (until it's eventually garbage collected or manually deleted)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we became familiar with Apache Spark and most of its main modules.
    We started to use the available Spark shells and wrote our first self-contained
    application using the Scala and Python programming languages. Finally, we explored
    different ways of deploying and running Spark in cluster mode. Everything we have
    learned about so far is necessary for understanding the topics that are presented
    from [Chapter 3](44fab060-12c9-4eec-9e15-103da589a510.xhtml), *Extract, Transform,
    Load*, onward. If you have any doubts about any of the presented topics, I suggest
    that you go back and read this chapter again before moving on.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to explore the basics of DL, with an emphasis
    on some particular implementations of multi-layer neural networks.
  prefs: []
  type: TYPE_NORMAL
