- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to Q-Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ladies and gentlemen, things are about to get even more interesting than before.
    The next model we are about to tackle is at the heart of many AIs built today;
    robots, autonomous vehicles, and even AI players of video games. They all use
    Q-learning at the core of their model. Some of them even combine Q-learning with
    deep learning, making a highly advanced version of Q-learning called deep Q-learning,
    which we will cover in *Chapter 9*, *Going Pro with Artificial Brains – Deep Q-Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the AI fundamentals still apply to Q-learning, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning is a Reinforcement Learning model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Q-learning works on the inputs (states) and outputs (actions) principle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Q-learning works on a predefined environment, including the states (the inputs),
    the actions (the outputs), and the rewards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Q-learning is modeled by a Markov decision process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Q-learning uses a training mode, during which the parameters that are learned
    are called the Q-values, and an inference mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now we can add two more fundamentals, this time specific to Q-learning:'
  prefs: []
  type: TYPE_NORMAL
- en: There are a finite number of states (there is not an infinity of possible inputs).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are a finite number of actions (only a certain number of actions can be performed).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's all! There are no more fundamentals to keep in mind; now we can really
    dig into Q-learning, which you'll see is not that hard and really quite intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain Q-learning, we''ll use an example so that you won''t get lost inside
    pure theory, and so that you can visualize what''s happening. On that note: welcome
    to the Maze.'
  prefs: []
  type: TYPE_NORMAL
- en: The Maze
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You are going to learn how Q-learning works inside a maze. Let''s draw our
    maze right away; here it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The Maze'
  prefs: []
  type: TYPE_NORMAL
- en: I know, it's the simplest maze you have ever seen. That's important for the
    sake of simplicity, so that you can mostly focus on how the AI works its magic.
    Imagine if you got lost in this chapter because of the maze and not because of
    the AI formulas! The important thing is that you have a clear maze, and you can
    visualize how the AI might manage to find its way from the beginning to the end.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of the beginning and the end, imagine a little robot inside this maze,
    starting at point **E** (Entrance). Its goal is to find the quickest way to point
    **G** (Goal). We humans can figure that out in no time, but that's only because
    our maze is so simple. What you are going to build is an AI that can go from a
    starting point to an ending point, regardless of how complex the maze is. Let's
    get started!
  prefs: []
  type: TYPE_NORMAL
- en: Beginnings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here is a question for you: what do you think is going to be the very first
    step?'
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ll give you three possible answers:'
  prefs: []
  type: TYPE_NORMAL
- en: We start writing some math equations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We build the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We try to make it work with Thompson Sampling (the AI model of the previous
    chapter).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The correct answer is…
  prefs: []
  type: TYPE_NORMAL
- en: 2\. We build the environment.
  prefs: []
  type: TYPE_NORMAL
- en: That was easy, but I wanted to highlight that in a question to make sure you
    keep in mind that this must always be the first step when building an AI. After
    clearly understanding the problem, the first step of building your AI solution
    is always to set up the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'That begs a further question:'
  prefs: []
  type: TYPE_NORMAL
- en: What steps, exactly, are you going to take when building that environment?
  prefs: []
  type: TYPE_NORMAL
- en: Try to remember the answer—you've already learned this—and then read on for
    a recap.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, you'll define the states (the inputs of your AI).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Secondly, you'll define the actions that can be performed (the outputs of your AI).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thirdly, you'll define the rewards. Remember, the reward is what the AI gets after
    performing an action in a certain state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we've secured the basics, so you can tackle that first step of defining
    the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Building the environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To build the environment, we need to define the states, the actions, and the
    rewards.
  prefs: []
  type: TYPE_NORMAL
- en: The states
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let's begin with the states. What do you think are going to be the states for
    this problem? Remember, the states are the inputs of your AI. And they should
    contain enough information for the AI to be able to take an action that will lead
    it to its final goal (reaching point E).
  prefs: []
  type: TYPE_NORMAL
- en: In this model, we don't have too much of a choice. The state, at a specific
    time or specific iteration, is simply going to be the position of the AI at that
    time. In other words, it is going to be the letter of the location, from **A**
    to **L**, where the AI is in at a specific time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you might guess, the next step after building the environment will be writing
    the mathematical equations at the heart of the AI, and to help you with that,
    it makes it much easier to encode the states into unique integers instead of keeping
    them as letters. That''s exactly what we are going to do, with the following mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Location to state mapping'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that we abide by the first specific fundamental of Q-learning, that
    is: **there are a finite number of states**.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the actions.
  prefs: []
  type: TYPE_NORMAL
- en: The actions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The actions are simply going to be the next moves the AI can make to go from
    one location to the next. For example, let's say the AI is in location **J**;
    the possible actions that the AI can perform are to go to **I**, to **F**, or
    to **K**. Again, since you'll be working with math equations, you can encode these
    actions with the same indexes as for the states.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the example where the AI is in location **J** at a specific time,
    the possible actions that the AI can perform are **5**, **8**, and **10**, according
    to our previous mapping above: the index **5** corresponds to **F**, the index
    **8** corresponds to **I**, and the index **10** corresponds to **K**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, the possible actions are simply the indexes of the different locations
    that can be reached:'
  prefs: []
  type: TYPE_NORMAL
- en: Possible actions = {0,1,2,3,4,5,6,7,8,9,10,11}
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that again, we abide by the second specific fundamental of Q-learning,
    that is: **there are a finite number of actions**.'
  prefs: []
  type: TYPE_NORMAL
- en: Now obviously, when in a specific location, there are some actions that the
    AI cannot perform. Taking the same previous example, if the AI is in location
    **J**, it can perform the actions **5**, **8**, and **10**, but it cannot perform
    the other actions. You can make sure to specify that by attributing a 0 reward
    to the actions it cannot perform, and a 1 reward to the actions it can perform.
    That brings us to the rewards.
  prefs: []
  type: TYPE_NORMAL
- en: The rewards
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You''re almost done with your environment—last, but not least, you have to
    define a system of rewards. More specifically, you have to define a reward function
    *R* that takes as input a state *s* and an action *a*, and returns a numerical
    reward *r* that the AI will get by performing the action *a* in the state *s*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'R: (s, a) ![](img/B14110_07_001.png) ![](img/B14110_07_002.png)'
  prefs: []
  type: TYPE_NORMAL
- en: So, how can you build such a function for our case study? Here, it is simple.
    Since there are a discrete and finite number of states (the indexes from 0 to
    11), as well as a discrete and finite number of actions (same indexes from 0 to
    11), the best way to build your reward function *R* is to simply make a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your reward function will be a matrix of exactly 12 rows and 12 columns, where
    the rows correspond to the states, and the columns correspond to the actions.
    That way, in your function R: (s, a) ![](img/B14110_07_003.png) ![](img/B14110_07_004.png),
    *s* will be the row index of the matrix, *a* will be the column index of the matrix,
    and *r* will be the cell of index (*s*, *a*) in the matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To build this reward matrix, what you first have to do is attribute, for each
    of the 12 locations, a 0 reward to the actions that the robot cannot perform,
    and a 1 reward to the actions the robot can perform. By doing that for each of
    the 12 locations, you will end up with a matrix of rewards. Let''s build it step
    by step, starting with the first location: location **A**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When in location **A**, the robot can only go to location **B**. Therefore,
    since location **A** has index 0 (first row of the matrix) and location **B**
    has index 1 (second column of the matrix), the first row of the matrix of rewards
    will get a 1 on the second column, and a 0 on all the other columns, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Rewards matrix – Step 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s move on to location **B**. When in location **B**, the robot can only
    go to three different locations: **A**, **C**, and **F**. Since **B** has index
    1 (second row), and **A**, **C**, and **F** have respective indexes 0, 2, and
    5 (1st, 3rd, and 6th column), then the second row of the matrix of rewards will
    get a 1 on the 1st, 3rd, and 6th columns, and 0 on all the other columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Rewards matrix – Step 2'
  prefs: []
  type: TYPE_NORMAL
- en: '**C** (of index 2) is only connected to **B** and **G** (of indexes 1 and 6)
    so the third row of the matrix of rewards is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Rewards matrix – Step 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'By doing the same for all the other locations, you eventually get your final
    matrix of rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Rewards matrix - Step 4'
  prefs: []
  type: TYPE_NORMAL
- en: And that's how you initialize the matrix of rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 'But wait—you''re not actually finished. There is one final thing you need to
    do. It''s a step that''s crucial to understand. In fact, let me ask you another
    question, the ultimate one, which will check if your intuition is already shaping
    up:'
  prefs: []
  type: TYPE_NORMAL
- en: '**How can you let the AI know that it has to go to that top priority location
    G?**'
  prefs: []
  type: TYPE_NORMAL
- en: It's easy—you do it simply by playing with the rewards. You must keep in mind
    that with Reinforcement Learning, everything works from the rewards. If you attribute
    a high reward to location **G**, for example 1000, then the AI will automatically
    try to go and catch that high reward, simply because it is larger than the rewards
    of the other locations.
  prefs: []
  type: TYPE_NORMAL
- en: In short, and it's a fundamental point to understand and remember in Reinforcement
    Learning in general, **the AI is always looking for the highest reward**. That's
    why the trick to reach location **G** is simply to attribute it a higher reward
    than the other locations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, manually put a high reward (1000) inside the cell corresponding to
    location **G**, because it is the goal location where we want our AI to go. Since
    location **G** has an index of 6, we put a 1000 reward on the cell of row 6 and
    column 6\. Accordingly, our matrix of rewards becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Rewards matrix - Step 5'
  prefs: []
  type: TYPE_NORMAL
- en: You have defined the rewards! You did it by simply building this matrix of rewards.
    It is important to understand that this is usually the way we define the system
    of rewards when doing Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 9*, *Going Pro with Artificial Brains – Deep Q-Learning,* which
    is about deep Q-learning, you will see that we will proceed very differently and
    build the environment much more easily. In fact, deep Q-learning is the advanced
    version of Q-learning that is widely used today in AI, far more than the simple
    Q-learning model itself. But you have to tackle Q-learning first, in depth, in
    order to be ready for deep Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: Since you've defined the states, the actions, and the rewards, you have finished
    building the environment. This means you are ready to tackle the next step, where
    you will build the AI itself that will do its magic inside this environment that
    you've just defined.
  prefs: []
  type: TYPE_NORMAL
- en: Building the AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you have built an environment in which you clearly defined the goal
    with a relevant system of rewards, it's time to build the AI. I hope you're ready
    for a little math.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ll break down this second step into several sub-steps, leading you to the
    final Q-learning model. To that end, we''ll cover three important concepts at
    the heart of Q-learning, in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: The Q-value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The temporal difference
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Bellman equation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's get started by learning about the Q-value.
  prefs: []
  type: TYPE_NORMAL
- en: The Q-value
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before you start getting into the details of Q-learning, I need to explain
    the concept of the Q-value. Here''s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To each couple of state and action (*s*, *a*), we are going to associate a
    numeric value *Q*(*s*, *a*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_005.png)'
  prefs: []
  type: TYPE_IMG
- en: We will say that *Q*(*s*, *a*) is "the Q-value of the action *a* performed in
    the state *s*."
  prefs: []
  type: TYPE_NORMAL
- en: 'Now I know the sort of questions you might be asking in your head: What does
    this Q-value mean? What does it represent? How do I even compute it? These were
    some of the questions I had in my mind when I first learned Q-learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to answer these questions, I need to introduce the temporal difference.
  prefs: []
  type: TYPE_NORMAL
- en: The temporal difference
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is where the math really comes in. Let's say we are in a specific state
    ![](img/B14110_04_001.png), at a specific time *t*. Let's just perform an action
    randomly, any of them. That brings us to the next state ![](img/B14110_07_045.png)
    and we get the reward ![](img/B14110_07_008.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The temporal difference at time *t*, denoted by ![](img/B14110_07_038.png),
    is the difference between:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_010.png), that is, the reward ![](img/B14110_07_011.png)
    obtained by performing the action ![](img/B14110_07_012.png) in the state ![](img/B14110_07_013.png),
    plus the Q-value of the best action performed in the future state ![](img/B14110_07_045.png),
    discounted by a factor ![](img/B14110_07_015.png), called the discount factor'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: and ![](img/B14110_07_016.png), that is, the Q-value of the action ![](img/B14110_07_017.png)
    performed in the state ![](img/B14110_07_018.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This leads to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_019.png)'
  prefs: []
  type: TYPE_IMG
- en: You might think that's great, that you understand all the terms, but you're
    probably also thinking "But what does that all mean?" Don't worry—that's exactly
    what I was thinking when I was learning this.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''m going to explain while at the same time improving your AI intuition. The
    first thing to understand is that the temporal difference represents how well
    the AI is learning. Here''s how it works exactly, with respect to the training
    process (during which the Q-values are learned):'
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of the training, the Q-values are set to zero. Since the AI
    is looking to get the good rewards (here 1 or 1000), it is looking for the high
    temporal differences (see the formula of TD). Accordingly, if in the first iterations,
    ![](img/B14110_07_020.png) is high, the AI gets a "pleasant surprise" because
    that means the AI was able to find a good reward. On the other hand, if ![](img/B14110_07_021.png)
    is small, the AI gets a "frustration."
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the AI gets a great reward, the specific Q-value of the (state, action)
    that led to that great reward increases, so the AI can remember how it got to
    that high reward (you'll see exactly how it increases in the next section). For
    example, let's say that it was the action ![](img/B14110_07_022.png) performed
    in the state ![](img/B14110_07_018.png) that led to that high reward ![](img/B14110_07_024.png).
    That would mean the Q-value ![](img/B14110_07_025.png) increases automatically
    (remember, you'll see how in the next section). Those increased Q-values are important
    information, because they indicate to the AI which transitions lead to the good
    rewards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step of the AI is not only to look for the great rewards, but also
    to look at the same time for the high Q-values. Why? Because the high Q-values
    are the ones that lead to the great reward. In fact, the high Q-values are the
    ones that lead to higher Q-values, themselves leading to even higher Q-values,
    themselves leading eventually to the highest reward (1000). That's the role of
    ![](img/B14110_07_026.png) in the temporal difference formula. Everything will
    become crystal clear when you put this into practice. The AI looks for the high
    Q-values, and as soon as it finds them, the Q-values of the (state, action) that
    led to these high Q-values will increase again, since they indicate the right
    path towards the goal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At some point, the AI will know all the transitions that lead to the good rewards
    and high Q-values. Since the Q-values of these transitions have already been increased
    over time, the temporal differences decrease in the end. In fact, the closer we
    get to the final goal, the smaller the temporal differences become.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In conclusion, the temporal difference is like a temporary intrinsic reward,
    of which the AI will try to find the large values at the beginning of the training.
    Eventually, the AI will minimize this reward as it gets to the end of the training—that
    is, as it gets closer to the final goal.
  prefs: []
  type: TYPE_NORMAL
- en: That's exactly the intuition of the temporal difference you must have in mind,
    because it will really help you understand the magic of Q-learning. Speaking of that magic,
    we are about to reveal the last piece of the puzzle.
  prefs: []
  type: TYPE_NORMAL
- en: Now you understand that the AI will iterate some updates of the Q-values towards
    the high temporal differences, which are ultimately decreased. But how does it
    do that? There is a specific answer to that question—the Bellman equation, the
    most famous equation in Reinforcement Learning.
  prefs: []
  type: TYPE_NORMAL
- en: The Bellman equation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In order to perform better and better actions that will lead the AI to reach
    its goal, you have to increase the Q-values of actions when you find high temporal
    differences. Only one question remains: How will the AI update these Q-values?
    Richard Bellman, a pioneer of Reinforcement Learning, created the answer. At each
    iteration, you update the Q-values from time t-1 (previous iteration) to t (current
    iteration) through the following equation, called the Bellman equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_027.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B14110_07_028.png) is the learning rate, which dictates how fast
    the learning of the Q-values goes. Its value is usually between 0 and 1, for example,
    0.75\. The lower the value of ![](img/B14110_07_029.png), the smaller the updates
    of the Q-values, and the longer the Q-learning will take. The higher its value,
    the bigger the updates of the Q-values and the faster the Q-learning will be.
    As you can clearly see in this equation, when the temporal difference ![](img/B14110_07_030.png)
    is high, the Q-value ![](img/B14110_07_031.png) increases.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now you have all the elements of Q-learning—congratulations, by the way—let's connect
    the dots between all these elements to reinforce your AI intuition.
  prefs: []
  type: TYPE_NORMAL
- en: The Q-values measure the accumulation of "good surprise" or "frustration" associated
    with the couple of action and state ![](img/B14110_07_032.png).
  prefs: []
  type: TYPE_NORMAL
- en: In the "good surprise" case of a high temporal difference, the AI is reinforced,
    and in the "frustration" case of a low temporal difference, the AI is weakened.
  prefs: []
  type: TYPE_NORMAL
- en: We want to learn the Q-values that will give the AI the maximum "good surprise,"
    and that's exactly what the Bellman equation does by updating the Q-values at
    each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: You've learned quite a lot of new information, and even though you've finished
    with an intuition section that connects the dots, that's not enough to get a really
    solid grasp of Q-learning. The next step is to take a step back, and the best
    way to do that is to go through the whole Q-learning process from start to finish
    so that it becomes crystal clear in your head.
  prefs: []
  type: TYPE_NORMAL
- en: The whole Q-learning process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's summarize the different steps of the whole Q-learning process. To be clear,
    the only purpose of this process is to update the Q-values over a certain number
    of iterations until they are no longer updated (we refer to that point as convergence).
  prefs: []
  type: TYPE_NORMAL
- en: The number of iterations depends on the complexity of the problem. For our problem,
    1,000 will be enough, but for more complex problems you might want to consider
    higher numbers such as 10,000\. In short, the Q-learning process is the part where
    we train our AI, and it's called Q-learning because it's the process during which
    the Q-values are learned. Then I'll explain what happens for the inference part (pure
    predictions), which comes, as always, after the training. The full Q-learning
    process starts with training mode.
  prefs: []
  type: TYPE_NORMAL
- en: Training mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Initialization (First iteration)**:'
  prefs: []
  type: TYPE_NORMAL
- en: For all couples of states *s* and actions *a*, the Q-values are initialized
    to 0.
  prefs: []
  type: TYPE_NORMAL
- en: '**Next iterations**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'At each iteration *t* ≥ 1, you repeat for a certain number of times (chosen
    by you the developer) the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: You select a random state ![](img/B14110_07_033.png) from the possible states.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From that state, you perform a random action ![](img/B14110_07_034.png) that
    can lead to a next possible state, that is, such that ![](img/B14110_07_035.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You reach the next state ![](img/B14110_07_0451.png) and you get the reward
    ![](img/B14110_07_037.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You compute the temporal difference ![](img/B14110_07_0381.png):![](img/B14110_07_039.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You update the Q-value by applying the Bellman equation:![](img/B14110_07_027.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of this process, you have obtained Q-values that no longer update.
    That means only one thing; you are ready to hack the maze by going into inference
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: Inference mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The training is complete, and now begins the inference. To remind you, the
    inference part is when you have a fully trained model with which you are going
    to make predictions. In our maze, the predictions that you are going to make are
    the actions to perform to take you from start (Location **E**) to finish (Location
    **G**). So, the question is:'
  prefs: []
  type: TYPE_NORMAL
- en: How are you going to use the learned Q-values to perform the actions?
  prefs: []
  type: TYPE_NORMAL
- en: 'Good news; for Q-learning this is very simple. When in a certain state ![](img/B14110_07_018.png),
    you simply perform the action ![](img/B14110_07_042.png) that has the highest
    Q-value for that state ![](img/B14110_04_001.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_044.png)'
  prefs: []
  type: TYPE_IMG
- en: That's all—by doing this at each location (each state), you get to your final
    destination through the shortest route. We'll implement this and see the result
    in the practical activities or the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter we studied the Q-learning model, which is only applied to environments
    that have a finite number of input states and a finite number of possible actions
    to perform.
  prefs: []
  type: TYPE_NORMAL
- en: When performing Q-learning, the AI learns Q-values through an iterative process,
    so that the higher the Q-value of a (state, action) pair, the closer the AI gets
    to the top reward.
  prefs: []
  type: TYPE_NORMAL
- en: At each iteration the Q-values are updated through the Bellman equation, which
    simply consists of adding the temporal difference, discounted by a learning rate
    factor. We will get to work on a full practical Q-learning activity in the next
    chapter, applied to a real-world business problem.
  prefs: []
  type: TYPE_NORMAL
