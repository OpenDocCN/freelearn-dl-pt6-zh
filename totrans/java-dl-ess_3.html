<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Deep Belief Nets and Stacked Denoising Autoencoders"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Deep Belief Nets and Stacked Denoising Autoencoders</h1></div></div></div><p>From this chapter through to the next chapter, you are going to learn the algorithms of deep learning. We'll follow the fundamental math theories step by step to fully understand each algorithm. Once you acquire the fundamental concepts and theories of deep learning, you can easily apply them to practical applications.</p><p>In this chapter, the topics you will learn about are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Reasons why deep learning could be a breakthrough</li><li class="listitem" style="list-style-type: disc">The differences between deep learning and past machine learning (neural networks)</li><li class="listitem" style="list-style-type: disc">Theories<a id="id166" class="indexterm"/> and implementations of the typical <a id="id167" class="indexterm"/>algorithms of deep learning, <span class="strong"><strong>deep belief nets</strong></span> (<span class="strong"><strong>DBN</strong></span>), and <span class="strong"><strong>Stacked Denoising Autoencoders</strong></span> (<span class="strong"><strong>SDA</strong></span>)</li></ul></div><div class="section" title="Neural networks fall"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec18"/>Neural networks fall</h1></div></div></div><p>In the <a id="id168" class="indexterm"/>previous chapter, you learned about the typical algorithm of neural networks and saw that nonlinear classification problems cannot be solved with perceptrons but can be solved by making multi-layer modeled neural networks. In other words, nonlinear problems can be learned and solved by inserting a hidden layer between the input and output layer. There is nothing else to it; but by increasing the number of neurons in a layer, the neural networks can express more patterns as a whole. If we ignore the time cost or an over-fitting problem, theoretically, neural networks can approximate any function.</p><p>So, can we think this way? If we increase the number of hidden layers—accumulate hidden layers over and over—can neural networks solve any complicated problem? It's quite natural to come up with this idea. And, as a matter of course, this idea has already been examined. However, as it turns out, this trial didn't work well. Just accumulating layers didn't make neural networks solve the world's problems. On the contrary, some cases have less accuracy when predicting than others with fewer layers.</p><p>Why do <a id="id169" class="indexterm"/>these cases happen? It's not wrong for neural networks with more layers to have more expression. So, where is the problem? Well, it is caused because of the feature that learning algorithms have in feed-forward networks. As we saw in the previous chapter, the backpropagation algorithm is used to propagate the learning error into the whole network efficiently with the multi-layer neural networks. In this algorithm, an error is reversed in each layer of the neural network and is conveyed to the input layer one by one in order. By backpropagating the error at the output layer to the input layer, the weight of the network is adjusted at each layer in order and the whole weight of a network is optimized.</p><p>This is where the problem occurs. If the number of layers of a network is small, an error backpropagating from an output layer can contribute to adjusting the weights of each layer well. However, once the number of layers increases, an error gradually disappears every time it backpropagates layers, and doesn't adjust the weight of the network. At a layer near the input layer, an error is not fed back at all.</p><p>The neural networks where the link among layers is dense have an inability to adjust weights. Hence, the weight of the whole of the networks cannot be optimized and, as a matter of course, the learning cannot go well. This serious problem is known as the <span class="strong"><strong>vanishing gradient problem</strong></span> and has troubled researchers as a huge problem that the neural network<a id="id170" class="indexterm"/> had for a long time until deep learning showed up. The neural network algorithm reached a limit at an early stage.</p></div></div>
<div class="section" title="Neural networks' revenge"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec19"/>Neural networks' revenge</h1></div></div></div><p>Because<a id="id171" class="indexterm"/> of the vanishing gradient problem, neural networks lost their popularity in the field of machine learning. We can say that the number of cases used for data mining in the real world by neural networks was remarkably small compared to other typical algorithms such as logistic regression and SVM.</p><p>But then deep learning showed up and broke all the existing conventions. As you know, deep learning is the neural network accumulating layers. In other words, it is deep neural networks, and it generates astounding predictability in certain fields. Now, speaking of AI research, it's no exaggeration to say that it's the research into deep neural networks. Surely it's the counterattack by neural networks. If so, why didn't the vanishing gradient problem matter in deep learning? What's the difference between this and the past algorithm?</p><p>In this section, we'll look at why deep learning can generate such predictability and its mechanisms.</p><div class="section" title="Deep learning's evolution – what was the breakthrough?"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec19"/>Deep learning's evolution – what was the breakthrough?</h2></div></div></div><p>We <a id="id172" class="indexterm"/>can say that there are two algorithms that triggered deep<a id="id173" class="indexterm"/> learning's popularity. The first one, as mentioned in <a class="link" href="ch01.html" title="Chapter 1. Deep Learning Overview">Chapter 1</a>, <span class="emphasis"><em>Deep Learning Overview</em></span>, is DBN pioneered by Professor Hinton (<a class="ulink" href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf</a>). The second one is SDA, proposed by Vincent et al. (<a class="ulink" href="http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf">http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf</a>). SDA was introduced a little after the introduction of DBN. It also recorded high predictability even with deep layers by taking a similar approach to DBN, although the details of the algorithm are different.</p><p>So, what is the common approach that solved the vanishing gradient problem? Perhaps you are nervously preparing to solve difficult equations in order to understand DBN or SDA, but don't worry. DBN is definitely an algorithm that is understandable. On the contrary, the mechanism itself is really simple. Deep learning was established by a very simple and<a id="id174" class="indexterm"/> elegant solution. The solution is: <span class="strong"><strong>layer-wise training</strong></span>. That's it. You might think it's obvious if you see it, but this is the approach that made deep learning popular.</p><p>As mentioned earlier, in theory if there are more units or layers of neural networks, it should have more expressions and increase the number of problems it is able to solve. It doesn't work well because an error cannot be fed back to each layer correctly and parameters, as a whole network, cannot be adjusted properly. This is where the innovation was brought in for learning at a respective layer. Because each layer adjusts the weights of the networks independently, the whole network (that is, the parameters of the model) can be optimized properly even though the numbers of layers are piled up.</p><p>Previous models didn't go well because they tried to backpropagate errors from an output layer to an input layer straight away and tried to optimize themselves by adjusting the weights of the network with backpropagated errors. So, the algorithm shifted to layer-wise training and then the model optimization went well. That's what the breakthrough was for deep learning.</p><p>However, although we simply say <span class="strong"><strong>layer-wise training</strong></span>, we need techniques for how to implement the learning. Also, as a matter of course, parameter adjustments for whole networks can't only be done with layer-wise training. We need the final adjustment. This phase of<a id="id175" class="indexterm"/> layer-wise training is called <span class="strong"><strong>pre-training</strong></span> and the last adjustment phase<a id="id176" class="indexterm"/> is called <span class="strong"><strong>fine-tuning</strong></span>. We can say that the bigger feature introduced in DBN and SDA is pre-training, but these two features are both part of the the necessary flow of deep learning. How do we do pre-training? What can be done in fine-tuning? Let's take a look at these questions one by one.</p></div><div class="section" title="Deep learning with pre-training"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec20"/>Deep learning with pre-training</h2></div></div></div><p>Deep <a id="id177" class="indexterm"/>learning is more like neural networks with accumulated hidden layers. The layer-wise training in pre-training undertakes learning at each layer. However, you might still have the following questions: if both layers are hidden (that is, neither of the layers are input nor output layers), then how is the training done? What can the input and output be?</p><p>Before thinking of these questions, remind yourself of the following point again (reiterated persistently): deep learning is neural networks with piled up layers. This mean, model parameters are still the weights of the network (and bias) in deep learning. Since these weights (and bias) need to be adjusted among each layer, in the standard three layered neural network (that is, the input layer, the hidden layer, and the output layer), we need to optimize only the weights between the input layer and the hidden layer and between the hidden layer and the output layer. In deep learning, however, the weight between two hidden layers also needs to be adjusted.</p><p>First of all, let's think about the input of a layer. You can imagine this easily with a quick thought. The value propagated from the previous layer will become the input as it is. The value propagated from the previous layer is none other than the value forward propagated from the previous layers to the current layer by using the weight of the network, the same as in general feed-forward networks. It looks simple in writing, but you can see that it has an important meaning if you step into it further and try to understand what it means. The value from the previous layer becomes the input, which means that the features the previous layer(s) learned become the input of the current layer, and from there the current layer newly learns the feature of the given data. In other words, in deep learning, features are learned from the input data in stages (and semi-automatically). This implies a mechanism where the deeper a layer becomes, the higher the feature it learns. This is what normal multi-layer neural networks couldn't do and the reason why it is said "a machine can learn a concept."</p><p>Now, let's think about the output. Please bear in mind that thinking about the output means thinking about how it learns. DBN and SDA have completely different approaches to learning, but both fill the following condition: to learn in order to equate output values and input values. You might think "What are you talking about?" but this is the technique that makes deep learning possible.</p><p>The value comes and goes back to the input layer through the hidden layer, and the technique is to adjust the weight of the networks (that is, to equate the output value and the input value) to eliminate the error at that time. The graphical model can be illustrated as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_01.jpg" alt="Deep learning with pre-training"/></div><p>It looks<a id="id178" class="indexterm"/> different from standard neural networks at a glance, but there's nothing special. If we intentionally draw the diagram of the input layer and the output layer separately, the mechanism is the same shape as the normal neural network:</p><div class="mediaobject"><img src="graphics/B04779_03_02.jpg" alt="Deep learning with pre-training"/></div><p>For a human, this action of <span class="emphasis"><em>matching input and output</em></span> is not intuitive, but for a machine it is a valid action. If so, how it can learn features from input data by matching the output layer and input layer?</p><p>Need a little explanation? Let's think about it this way: in the algorithm of machine learning, including neural networks, learning intends to minimize errors between the model's prediction <a id="id179" class="indexterm"/>output and the dataset output. The mechanism is to remove an error by finding a pattern from the input data and making data with a common pattern the same output value (for example, 0 or 1). What would then happen if we turned the output value into the input value?</p><p>When we look at problems that should be solved as a whole through deep learning, input data is, fundamentally, a dataset that can be divided into some patterns. This means that there are some common features in the input data. If so, in the process of learning where each output value becomes respective input data, the weight of networks should be adjusted to focus more on the part that reflects the common features. And, even within the data categorized in the same class, learning should be processed to reduce weight on the non-common feature part, that is, the noise part.</p><p>Now you should understand what the input and output is in a certain layer and how learning progresses. Once the pre-training is done at a certain layer, the network moves on to learning in the next layer. However, as you can see in the following images, please also keep in mind that a hidden layer becomes an input layer when the network moves to learning in the next layer:</p><div class="mediaobject"><img src="graphics/B04779_03_03.jpg" alt="Deep learning with pre-training"/></div><p>The <a id="id180" class="indexterm"/>point here is that the layer after the pre-training can be treated as normal feed-forward neural networks where the weight of the networks is adjusted. Hence, if we think about the input value, we can simply calculate the value forward propagated from the input layer to the current layer through the network.</p><p>Up to now, we've looked through the flow of pre-training (that is, layer-wise training). In the hidden layers of deep neural networks, features of input data are extracted in stages through learning where the input matches the output. Now, some of you might be wondering: I understand that features can be learned in stages from input data by pre-training, but that alone doesn't solve the classification problem. So, how can it solve the classification problem?</p><p>Well, during pre-training, the information pertaining to which data belongs to which class is not provided. This means the pre-training is unsupervised training and it just analyzes the hidden pattern using only input data. This is meaningless if it can't be used to solve the problem however it extracts features. Therefore, the model needs to complete one more step to solve classification problems properly. That is fine-tuning. The main roles of fine-tuning are the following:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">To add an output layer to deep neural networks that completed pre-training and to perform supervised training.</li><li class="listitem">To do final adjustments for the whole deep neural network.</li></ol></div><p>This<a id="id181" class="indexterm"/> can be illustrated as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_04.jpg" alt="Deep learning with pre-training"/></div><p>The supervised training in an output layer uses a machine learning algorithm, such as logistic regression or SVM. Generally, logistic regression is used more often considering the balance of the amount of calculation and the precision gained.</p><p>In fine-tuning, sometimes only the weights of an output layer will be adjusted, but normally the weights of whole neural networks, including the layer where the weights have been adjusted in pre-training, will also be adjusted. This means the standard learning algorithm, or in other words the backpropagation algorithm, is applied to the deep neural networks just as one multi-layer neural network. Thus, the model of neural networks with the problem of solving more complicated classification is completed.</p><p>Even so, you might have the following questions: why does learning go well with the standard backpropagation algorithm even in  multi-layer neural networks where layers are piled up? Doesn't the vanishing gradient problem occur? These questions can be solved by pre-training. Let's think about the following: in the first place, the problem is that the weights of each network are not correctly adjusted due to improperly fed back errors in multi-layer neural networks without pre-training; in other words, the multi-layer neural networks where the vanishing gradient problem occurs. On the other hand, once the pre-training<a id="id182" class="indexterm"/> is done, the learning starts from the point where the weight of the network is almost already adjusted. Therefore, a proper error can be propagated to a layer close to an input layer. Hence the name fine-tuning. Thus, through pre-training and fine-tuning, eventually deep neural networks become neural networks with increased expression by having deep layers.</p><p>From the next section onwards, we will finally look through the theory and implementation of DBN and SDA, the algorithms of deep learning. But before that, let's look back at the flow of deep learning once again. Below is the summarized diagram of the flow:</p><div class="mediaobject"><img src="graphics/B04779_03_05.jpg" alt="Deep learning with pre-training"/></div><p>The parameters of the model are optimized layer by layer during pre-training and then adjusted as single deep neural networks during fine-tuning. Deep learning, the breakthrough of AI, is a very simple algorithm.</p></div></div>
<div class="section" title="Deep learning algorithms"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>Deep learning algorithms</h1></div></div></div><p>Now, let's <a id="id183" class="indexterm"/>look through the theory and implementation of deep learning algorithms. In this chapter, we will see DBN and SDA (and the related methods). These algorithms were both researched explosively, mainly between 2012 and 2013 when deep learning started to spread out rapidly and set  the trend of deep learning on fire. Even though there are two methods, the basic flow is the same and consistent with pre-training and fine-tuning, as explained in the previous section. The difference between these two is which pre-training (that is, unsupervised training) algorithm is applied to them.</p><p>Therefore, if there<a id="id184" class="indexterm"/> could be difficult points in deep learning, it should be the theory and equation of the unsupervised training. However, you don't have to be afraid. All the theories and implementations will be explained one by one, so please read through the following sections carefully.</p><div class="section" title="Restricted Boltzmann machines"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec21"/>Restricted Boltzmann machines</h2></div></div></div><p>The<a id="id185" class="indexterm"/> method used in the layer-wise<a id="id186" class="indexterm"/> training of DBN, pre-training, is called <span class="strong"><strong>Restricted Boltzmann Machines</strong></span> (<span class="strong"><strong>RBM</strong></span>). To begin with, let's take a look at the RBM that forms the basis of DBN. As RBM stands for Restricted Boltzmann Machines, of course there's a method called <span class="strong"><strong>Boltzmann Machines</strong></span> (<span class="strong"><strong>BMs</strong></span>). Or rather, BMs are <a id="id187" class="indexterm"/>a more standard form and RBM is the special case of them. Both are one of the neural networks and both were proposed by Professor Hinton.</p><p>The implementation of RBM and DBNs can be done without understanding the detailed theory of BMs, but in order to understand these concepts, we'll briefly look at the idea BMs are based on. First of all, let's look at the following figure, which shows a graphical model of BMs:</p><div class="mediaobject"><img src="graphics/B04779_03_06.jpg" alt="Restricted Boltzmann machines"/></div><p>BMs look intricate because they are fully connected, but they are actually just simple neural networks with two layers. By rearranging all the units in the networks to get a better <a id="id188" class="indexterm"/>understanding, BMs can<a id="id189" class="indexterm"/> be shown as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_07.jpg" alt="Restricted Boltzmann machines"/></div><p>Please bear in mind that normally the input/output layer is called the <span class="strong"><strong>visible layer</strong></span> in BMs and<a id="id190" class="indexterm"/> RBMs (a hidden layer is commonly used as it is), for it is the networks that presume the hidden condition (unobservable condition) from the observable<a id="id191" class="indexterm"/> condition. Also, the neurons of the visible layer are called <span class="strong"><strong>visible units</strong></span> and the neurons of the hidden layer are called <span class="strong"><strong>hidden units</strong></span>. Signs in the previous<a id="id192" class="indexterm"/> figure are described to match the given names.</p><p>As you can see in the diagram, the structure of BMs is not that different from standard neural networks. However, its way of thinking has a big feature. The feature is to adopt the concept of <span class="emphasis"><em>energy</em></span> in neural networks. Each unit has a stochastic state respectively and the whole of the networks' energy is determined depending on what state each unit takes. (The first model that adopted the concept of energy in networks is called the <span class="strong"><strong>Hopfield network</strong></span>, and <a id="id193" class="indexterm"/>BMs are the developed version of it. Since details of the Hopfield network are not totally relevant to deep learning, it is not explained in this book.) The condition that memorizes the correct data is the steady state of networks and the least amount of energy these networks have. On the other hand, if data with noise is provided to the network, each unit has a different state, but not a steady state, hence its condition makes the transition to stabilize the whole network, in other words, to transform it into a steady state.</p><p>This <a id="id194" class="indexterm"/>means that the weights<a id="id195" class="indexterm"/> of the model are adjusted and the state of each unit is transferred to minimize the energy function the networks have. These operations can remove the noise and extract the feature from inputs as a whole. Although the energy of networks sounds enormous, it's not too difficult to imagine because minimizing the energy function has the same effect as minimizing the error function.</p><p>The concept of BMs was wonderful, but various problems occurred when BMs were actually applied to practical problems. The biggest problem was that BMs are fully connected networks and take an enormous amount of calculation time. Therefore, RBM was devised. RBM is the algorithm that can solve various problems in a realistic time frame by making BMs restricted. Just as in BM, RBM is a model based on the energy of a network. Let's look at RBM in the diagram below:</p><div class="mediaobject"><img src="graphics/B04779_03_08.jpg" alt="Restricted Boltzmann machines"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_03_14.jpg" alt="Restricted Boltzmann machines"/></span> is the number of units in the visible layer and <span class="inlinemediaobject"><img src="graphics/B04779_03_15.jpg" alt="Restricted Boltzmann machines"/></span> the number of units in the hidden layer. <span class="inlinemediaobject"><img src="graphics/B04779_03_16.jpg" alt="Restricted Boltzmann machines"/></span> denotes the value of a visible unit, <span class="inlinemediaobject"><img src="graphics/B04779_03_17.jpg" alt="Restricted Boltzmann machines"/></span> the value of a hidden unit, and <span class="inlinemediaobject"><img src="graphics/B04779_03_18.jpg" alt="Restricted Boltzmann machines"/></span> the weight between these two units. As you can see, the difference between BM and RBM is that RBM doesn't have connections between the same layer. Because of this restriction, the amount of calculation decreases and it can be applied to realistic problems.</p><p>Now, let's look through the theory.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip03"/>Tip</h3><p>Be careful that, as a prerequisite, the value that each visible unit and hidden unit in RBM can take is generally {0, 1}, that is, binary (this is the same as BMs).</p></div></div><p>If we<a id="id196" class="indexterm"/> expand the theory, it can also<a id="id197" class="indexterm"/> handle continuous values. However, this could make equations complex, where it's not the core of the theory and where it's implemented with binary in the original DBN proposed by Professor Hinton. Therefore, we'll also implement binary RBM in this book. RBM with binary inputs is sometimes<a id="id198" class="indexterm"/> called <span class="strong"><strong>Bernoulli RBM</strong></span>.</p><p>RBM is the energy-based model, and the status of a visible layer or hidden layer is treated as a stochastic variable. We'll look at the equations in order. First of all, each visible unit is propagated to the hidden units throughout a network. At this time, each hidden unit takes a binary value based on the probability distribution generated in accordance with its propagated inputs:</p><div class="mediaobject"><img src="graphics/B04779_03_21.jpg" alt="Restricted Boltzmann machines"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_03_19.jpg" alt="Restricted Boltzmann machines"/></span> is the bias in a hidden layer and <span class="inlinemediaobject"><img src="graphics/B04779_03_20.jpg" alt="Restricted Boltzmann machines"/></span> denotes the sigmoid function.</p><p>This time, it was conversely propagated from a hidden layer to a visible layer through the same network. As in the previous case, each visible unit takes a binary value based on probability distribution generated in accordance with propagated values.</p><div class="mediaobject"><img src="graphics/B04779_03_22.jpg" alt="Restricted Boltzmann machines"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_03_23.jpg" alt="Restricted Boltzmann machines"/></span> is the bias of the visible layer. This value of visible units is expected to match the <a id="id199" class="indexterm"/>original input values. This <a id="id200" class="indexterm"/>means if <span class="inlinemediaobject"><img src="graphics/B04779_03_24.jpg" alt="Restricted Boltzmann machines"/></span>, the weight of the network as a model parameter, and <span class="inlinemediaobject"><img src="graphics/B04779_03_25.jpg" alt="Restricted Boltzmann machines"/></span>, <span class="inlinemediaobject"><img src="graphics/B04779_03_26.jpg" alt="Restricted Boltzmann machines"/></span>, the bias of a visible layer and a hidden layer, are shown as a vector parameter, <span class="inlinemediaobject"><img src="graphics/B04779_03_27.jpg" alt="Restricted Boltzmann machines"/></span>, it leans <span class="inlinemediaobject"><img src="graphics/B04779_03_27.jpg" alt="Restricted Boltzmann machines"/></span> in order for the probability <span class="inlinemediaobject"><img src="graphics/B04779_03_28.jpg" alt="Restricted Boltzmann machines"/></span> that can be obtained above to get close to the distribution of <span class="inlinemediaobject"><img src="graphics/B04779_03_29.jpg" alt="Restricted Boltzmann machines"/></span>.</p><p>For this learning, the energy function, that is, the evaluation function, needs to be defined. The energy function is shown as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_30.jpg" alt="Restricted Boltzmann machines"/></div><p>Also, the joint probability density function showing the demeanor of a network can be shown as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_31.jpg" alt="Restricted Boltzmann machines"/></div><div class="mediaobject"><img src="graphics/B04779_03_32.jpg" alt="Restricted Boltzmann machines"/></div><p>From<a id="id201" class="indexterm"/> the preceding formulas, the<a id="id202" class="indexterm"/> equations for the training of parameters will be determined. We can get the following equation:</p><div class="mediaobject"><img src="graphics/B04779_03_33.jpg" alt="Restricted Boltzmann machines"/></div><p>Hence, the <span class="strong"><strong>log likelihood</strong></span> can be shown as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_34.jpg" alt="Restricted Boltzmann machines"/></div><p>Then, we'll calculate each gradient against the model parameter. The derivative can be calculated as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_35.jpg" alt="Restricted Boltzmann machines"/></div><p>Some <a id="id203" class="indexterm"/>equations in the middle are <a id="id204" class="indexterm"/>complicated, but it turns out to be simple with the term of the probability distribution of the model and the original data.</p><p>Therefore, the gradient of each parameter is shown as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_36.jpg" alt="Restricted Boltzmann machines"/></div><div class="mediaobject"><img src="graphics/B04779_03_37.jpg" alt="Restricted Boltzmann machines"/></div><div class="mediaobject"><img src="graphics/B04779_03_38.jpg" alt="Restricted Boltzmann machines"/></div><p>Now<a id="id205" class="indexterm"/> then, we could find the equation<a id="id206" class="indexterm"/> of the gradient, but a problem occurs when we try to apply this equation as it is. Think about the term of <span class="inlinemediaobject"><img src="graphics/B04779_03_39.jpg" alt="Restricted Boltzmann machines"/></span>. This term implies that we have to calculate the probability distribution for all the {0, 1} patterns, which can be assumed as input data that includes patterns that don't actually exist in the data.</p><p>We can easily imagine how this term can cause a combinatorial explosion, meaning we can't solve it within a realistic time frame. To solve this problem, the method for approximating<a id="id207" class="indexterm"/> data using Gibbs sampling, called <span class="strong"><strong>Contrastive Divergence</strong></span> (<span class="strong"><strong>CD</strong></span>), was introduced. Let's look at this method now.</p><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_03_40.jpg" alt="Restricted Boltzmann machines"/></span> is an input vector. Also, <span class="inlinemediaobject"><img src="graphics/B04779_03_41.jpg" alt="Restricted Boltzmann machines"/></span> is an input (output) vector that can be obtained by sampling for k-times using this input vector.</p><p>Then, we get:</p><div class="mediaobject"><img src="graphics/B04779_03_42.jpg" alt="Restricted Boltzmann machines"/></div><div class="mediaobject"><img src="graphics/B04779_03_43.jpg" alt="Restricted Boltzmann machines"/></div><p>Hence, when approximating <span class="inlinemediaobject"><img src="graphics/B04779_03_44.jpg" alt="Restricted Boltzmann machines"/></span> after reiterating Gibbs sampling, the derivative of the <a id="id208" class="indexterm"/>log likelihood function can be <a id="id209" class="indexterm"/>represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_45.jpg" alt="Restricted Boltzmann machines"/></div><p>Therefore, the model parameter can be shown as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_46.jpg" alt="Restricted Boltzmann machines"/></div><div class="mediaobject"><img src="graphics/B04779_03_47.jpg" alt="Restricted Boltzmann machines"/></div><div class="mediaobject"><img src="graphics/B04779_03_48.jpg" alt="Restricted Boltzmann machines"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_03_49.jpg" alt="Restricted Boltzmann machines"/></span> is the number of iterations and <span class="inlinemediaobject"><img src="graphics/B04779_03_50.jpg" alt="Restricted Boltzmann machines"/></span> is the learning rate. As shown in the preceding formulas, generally, CD that performs sampling k-times is shown as CD-k. It's known that CD-1 is sufficient when applying the algorithm to realistic problems.</p><p>Now, let's go<a id="id210" class="indexterm"/> through the implementation<a id="id211" class="indexterm"/> of RMB. The package structure is as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B04779_03_09.jpg" alt="Restricted Boltzmann machines"/></div><p>Let's look through the <code class="literal">RestrictedBoltzmannMachines.java</code> file. Because the first part of the main method just defines the variables needed for a model and generates demo data, we won't look at it here.</p><p>So, in the part where we generate an instance of a model, you may notice there are many <code class="literal">null</code> values in arguments:</p><div class="informalexample"><pre class="programlisting">// construct RBM
RestrictedBoltzmannMachines nn = new RestrictedBoltzmannMachines(nVisible, nHidden, null, null, null, rng);</pre></div><p>When you look at the constructor, you might know that these <code class="literal">null</code> values are the RBM's weight matrix, bias of hidden units, and bias of visible units. We define arguments as <code class="literal">null</code> here because they are for DBN's implementation. In the constructor, these are initialized as follows:</p><div class="informalexample"><pre class="programlisting">   if (W == null) {

       W = new double[nHidden][nVisible];
       double w_ = 1. / nVisible;

       for (int j = 0; j &lt; nHidden; j++) {
           for (int i = 0; i &lt; nVisible; i++) {
               W[j][i] = uniform(-w_, w_, rng);
           }
       }
   }

   if (hbias == null) {
       hbias = new double[nHidden];

       for (int j = 0; j &lt; nHidden; j++) {
           hbias[j] = 0.;
       }
   }

   if (vbias == null) {
       vbias = new double[nVisible];

       for (int i = 0; i &lt; nVisible; i++) {
           vbias[i] = 0.;
       }
   }</pre></div><p>The <a id="id212" class="indexterm"/>next step is training. CD-1 is applied <a id="id213" class="indexterm"/>for each mini-batch:</p><div class="informalexample"><pre class="programlisting">// train with contrastive divergence
for (int epoch = 0; epoch &lt; epochs; epoch++) {
   for (int batch = 0; batch &lt; minibatch_N; batch++) {
       nn.contrastiveDivergence(train_X_minibatch[batch], minibatchSize, learningRate, 1);
   }
   learningRate *= 0.995;
}</pre></div><p>Now, let's look into the essential point of RBM, the <code class="literal">contrastiveDivergence</code> method. CD-1 can obtain a sufficient solution when we actually run this program (and so we have k = 1 in the demo), but this method is defined to deal with CD-k as well:</p><div class="informalexample"><pre class="programlisting">// CD-k : CD-1 is enough for sampling (i.e. k = 1)
sampleHgivenV(X[n], phMean_, phSample_);

for (int step = 0; step &lt; k; step++) {

   // Gibbs sampling
   if (step == 0) {
       gibbsHVH(phSample_, nvMeans_, nvSamples_, nhMeans_, nhSamples_);
   } else {
       gibbsHVH(nhSamples_, nvMeans_, nvSamples_, nhMeans_, nhSamples_);
   }

}</pre></div><p>It<a id="id214" class="indexterm"/> appears that two different <a id="id215" class="indexterm"/>types of method, <code class="literal">sampleHgivenV</code> and <code class="literal">gibbsHVH</code>, are used in CD-k, but when you look into <code class="literal">gibbsHVH</code>, you can see:</p><div class="informalexample"><pre class="programlisting">public void gibbsHVH(int[] h0Sample, double[] nvMeans, int[] nvSamples, double[] nhMeans, int[] nhSamples) {
   sampleVgivenH(h0Sample, nvMeans, nvSamples);
   sampleHgivenV(nvSamples, nhMeans, nhSamples);
}</pre></div><p>So, CD-k consists of only two methods for sampling, <code class="literal">sampleVgivenH</code> and <code class="literal">sampleHgivenV</code>.</p><p>As the name of the method indicates, <code class="literal">sampleHgivenV</code> is the method that sets the probability distribution and sampling data generated in a hidden layer based on the given value of visible units and vice versa:</p><div class="informalexample"><pre class="programlisting">public void sampleHgivenV(int[] v0Sample, double[] mean, int[] sample) {

   for (int j = 0; j &lt; nHidden; j++) {
       mean[j] = propup(v0Sample, W[j], hbias[j]);
       sample[j] = binomial(1, mean[j], rng);
   }

}

public void sampleVgivenH(int[] h0Sample, double[] mean, int[] sample) {

   for(int i = 0; i &lt; nVisible; i++) {
       mean[i] = propdown(h0Sample, i, vbias[i]);
       sample[i] = binomial(1, mean[i], rng);
   }
}</pre></div><p>The <code class="literal">propup</code> and <code class="literal">propdown</code> tags that set values to respective means are the method that activates values of each unit by the <code class="literal">sigmoid</code> function:</p><div class="informalexample"><pre class="programlisting">public double propup(int[] v, double[] w, double bias) {

   double preActivation = 0.;

   for (int i = 0; i &lt; nVisible; i++) {
       preActivation += w[i] * v[i];
   }
   preActivation += bias;

   return sigmoid(preActivation);
}

public double propdown(int[] h, int i, double bias) {

   double preActivation = 0.;

   for (int j = 0; j &lt; nHidden; j++) {
       preActivation += W[j][i] * h[j];
   }
   preActivation += bias;

   return sigmoid(preActivation);
}</pre></div><p>The <code class="literal">binomial</code> method that sets a <a id="id216" class="indexterm"/>value<a id="id217" class="indexterm"/> to a sample is defined in <code class="literal">RandomGenerator.java</code>. The method returns <code class="literal">0</code> or <code class="literal">1</code> based on the binomial distribution. With this method, a value of each unit becomes binary:</p><div class="informalexample"><pre class="programlisting">public static int binomial(int n, double p, Random rng) {
   if(p &lt; 0 || p &gt; 1) return 0;

   int c = 0;
   double r;

   for(int i=0; i&lt;n; i++) {
       r = rng.nextDouble();
       if (r &lt; p) c++;
   }

   return c;
}</pre></div><p>Once approximated values are obtained by sampling, what we need to do is just calculate the gradient of a <code class="literal">model</code> parameter and renew a parameter using a mini-batch. There's nothing special here:</p><div class="informalexample"><pre class="programlisting">// calculate gradients
for (int j = 0; j &lt; nHidden; j++) {
   for (int i = 0; i &lt; nVisible; i++) {
       grad_W[j][i] += phMean_[j] * X[n][i] - nhMeans_[j] * nvSamples_[i];
   }

   grad_hbias[j] += phMean_[j] - nhMeans_[j];
}

for (int i = 0; i &lt; nVisible; i++) {
   grad_vbias[i] += X[n][i] - nvSamples_[i];
}


// update params
for (int j = 0; j &lt; nHidden; j++) {
   for (int i = 0; i &lt; nVisible; i++) {
       W[j][i] += learningRate * grad_W[j][i] / minibatchSize;
   }

   hbias[j] += learningRate * grad_hbias[j] / minibatchSize;
}

for (int i = 0; i &lt; nVisible; i++) {
   vbias[i] += learningRate * grad_vbias[i] / minibatchSize;
}</pre></div><p>Now<a id="id218" class="indexterm"/> we're done with the <code class="literal">model</code> training. Next <a id="id219" class="indexterm"/>comes the test and evaluation in general cases, but note that the model cannot be evaluated with barometers such as accuracy because RBM is a generative model. Instead, let's briefly look at how noisy data is changed by RBM here. Since RBM after training can be seen as a neural network, the weights of which are adjusted, the model can obtain reconstructed data by simply propagating input data (that is, noisy data) through a network:</p><div class="informalexample"><pre class="programlisting">public double[] reconstruct(int[] v) {

   double[] x = new double[nVisible];
   double[] h = new double[nHidden];

   for (int j = 0; j &lt; nHidden; j++) {
       h[j] = propup(v, W[j], hbias[j]);
   }

   for (int i = 0; i &lt; nVisible; i++) {
       double preActivation_ = 0.;

       for (int j = 0; j &lt; nHidden; j++) {
           preActivation_ += W[j][i] * h[j];
       }
       preActivation_ += vbias[i];

       x[i] = sigmoid(preActivation_);
   }

   return x;
}</pre></div></div><div class="section" title="Deep Belief Nets (DBNs)"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec22"/>Deep Belief Nets (DBNs)</h2></div></div></div><p>DBNs<a id="id220" class="indexterm"/> are deep neural networks where<a id="id221" class="indexterm"/> logistic regression is added to RBMS as the output layer. Since the theory necessary for implementation has already been explained, we can go directly to the implementation. The package structure is as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_10.jpg" alt="Deep Belief Nets (DBNs)"/></div><p>The flow of the program is very simple. The order is as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Setting up parameters for the model.</li><li class="listitem">Building the model.</li><li class="listitem">Pre-training the model.</li><li class="listitem">Fine-tuning the model.</li><li class="listitem">Testing and evaluating the model.</li></ol></div><a id="id222" class="indexterm"/><p>Just as in RBM, the first step in setting <a id="id223" class="indexterm"/>up the main method is the declaration of variables and the code for creating demo data (the explanation is omitted here).</p><p>Please check that in the demo data, the number of units for an input layer is 60, a hidden layer has 2 layers, their combined number of units is 20, and the number of units for an output layer is 3. Now, let's look through the code from the <span class="emphasis"><em>Building the Model</em></span> section:</p><div class="informalexample"><pre class="programlisting">// construct DBN
System.out.print("Building the model...");
DeepBeliefNets classifier = new DeepBeliefNets(nIn, hiddenLayerSizes, nOut, rng);
Sy
stem.out.println("done.");</pre></div><p>The variable of <code class="literal">hiddenLayerSizes</code> is an array and its length represents the number of hidden<a id="id224" class="indexterm"/> layers in deep neural networks. The deep learning<a id="id225" class="indexterm"/> algorithm takes a huge amount of calculation, hence the program gives us an output of the current status so that we can see which process is proceeding. The variable of <code class="literal">hiddenLayerSizes</code> is an array and its length represents the number of hidden layers in deep neural networks. Each layer is constructed in the constructor.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip04"/>Tip</h3><p>Please bear in mind that <code class="literal">sigmoidLayers</code> and <code class="literal">rbmLayers</code> are, of course, different objects but their weights and bias are shared.</p></div></div><p>This is because, as explained in the theory section, pre-training performs layer-wise training, whereas the whole model can be regarded as one neural network:</p><div class="informalexample"><pre class="programlisting">// construct multi-layer
for (int i = 0; i &lt; nLayers; i++) {
   int nIn_;
   if (i == 0) nIn_ = nIn;
   else nIn_ = hiddenLayerSizes[i-1];

   // construct hidden layers with sigmoid function
   //   weight matrices and bias vectors will be shared with RBM layers
   sigmoidLayers[i] = new HiddenLayer(nIn_, hiddenLayerSizes[i], null, null, rng, "sigmoid");

   // construct RBM layers
   rbmLayers[i] = new RestrictedBoltzmannMachines(nIn_, hiddenLayerSizes[i], sigmoidLayers[i].W, sigmoidLayers[i].b, null, rng);
}

// logistic regression layer for output
logisticLayer = new LogisticRegression(hiddenLayerSizes[nLayers-1], nOut);</pre></div><p>The first thing to do after building the model is pre-training:</p><div class="informalexample"><pre class="programlisting">// pre-training the model
System.out.print("Pre-training the model...");
classifier.pretrain(train_X_minibatch, minibatchSize, train_minibatch_N, pretrainEpochs, pretrainLearningRate, k);
System.out.println("done.");</pre></div><p>Pre-training<a id="id226" class="indexterm"/> needs to be processed with each <code class="literal">minibatch</code> but, at the same time, with each layer. Therefore, all training data is given<a id="id227" class="indexterm"/> to the <code class="literal">pretrain</code> method first, and then the data of each mini-batch is processed in the method:</p><div class="informalexample"><pre class="programlisting">public void pretrain(int[][][] X, int minibatchSize, int minibatch_N, int epochs, double learningRate, int k) {

   for (int layer = 0; layer &lt; nLayers; layer++) {  // pre-train layer-wise
       for (int epoch = 0; epoch &lt; epochs; epoch++) {
           for (int batch = 0; batch &lt; minibatch_N; batch++) {

               int[][] X_ = new int[minibatchSize][nIn];
               int[][] prevLayerX_;

               // Set input data for current layer
               if (layer == 0) {
                   X_ = X[batch];
               } else {

                   prevLayerX_ = X_;
                   X_ = new int[minibatchSize][hiddenLayerSizes[layer-1]];

                   for (int i = 0; i &lt; minibatchSize; i++) {
                       X_[i] = sigmoidLayers[layer-1].outputBinomial(prevLayerX_[i], rng);
                   }
               }

               rbmLayers[layer].contrastiveDivergence(X_, minibatchSize, learningRate, k);
           }
       }
   }

}</pre></div><p>Since the actual learning is done through CD-1 of RBM, the description of DBN within the code is very simple. In DBN (RBM), units of each layer have binary values, so the output method of <code class="literal">HiddenLayer</code> cannot be used because it returns double. Hence, the <code class="literal">outputBinomial</code> method is added to the class, which returns the <code class="literal">int</code> type (the code is omitted here). Once the pre-training is complete, the next step is fine-tuning.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip05"/>Tip</h3><p>Be careful not to use training data that was used in the pre-training.</p></div></div><p>We<a id="id228" class="indexterm"/> can easily fall into overfitting if we use the<a id="id229" class="indexterm"/> whole data set for both pre-training and fine-tuning. Therefore, the validation data set is prepared separately from the training dataset and is used for fine-tuning:</p><div class="informalexample"><pre class="programlisting">// fine-tuning the model
System.out.print("Fine-tuning the model...");
for (int epoch = 0; epoch &lt; finetuneEpochs; epoch++) {
   for (int batch = 0; batch &lt; validation_minibatch_N; batch++) {
       classifier.finetune(validation_X_minibatch[batch], validation_T_minibatch[batch], minibatchSize, finetuneLearningRate);
   }
   finetuneLearningRate *= 0.98;
}
System.out.println("done.");</pre></div><p>In the <code class="literal">finetune</code> method, the backpropagation algorithm in multi-layer neural networks is applied where the logistic regression is used for the output layer. To backpropagate unit values among multiple hidden layers, we define variables to maintain each layer's inputs:</p><div class="informalexample"><pre class="programlisting">public void finetune(double[][] X, int[][] T, int minibatchSize, double learningRate) {

   List&lt;double[][]&gt; layerInputs = new ArrayList&lt;&gt;(nLayers + 1);
   layerInputs.add(X);

   double[][] Z = new double[0][0];
   double[][] dY;

   // forward hidden layers
   for (int layer = 0; layer &lt; nLayers; layer++) {

       double[] x_;  // layer input
       double[][] Z_ = new double[minibatchSize][hiddenLayerSizes[layer]];

       for (int n = 0; n &lt; minibatchSize; n++) {

           if (layer == 0) {
               x_ = X[n];
           } else {
               x_ = Z[n];
           }

           Z_[n] = sigmoidLayers[layer].forward(x_);
       }

       Z = Z_.clone();
       layerInputs.add(Z.clone());
   }

   // forward &amp; backward output layer
   dY = logisticLayer.train(Z, T, minibatchSize, learningRate);

   // backward hidden layers
   double[][] Wprev;
   double[][] dZ = new double[0][0];

   for (int layer = nLayers - 1; layer &gt;= 0; layer--) {

       if (layer == nLayers - 1) {
           Wprev = logisticLayer.W;
       } else {
           Wprev = sigmoidLayers[layer+1].W;
           dY = dZ.clone();
       }

       dZ = sigmoidLayers[layer].backward(layerInputs.get(layer), layerInputs.get(layer+1), dY, Wprev, minibatchSize, learningRate);
   }
}</pre></div><p>The<a id="id230" class="indexterm"/> training part of DBN is just how it is seen in the preceding code. The hard part is probably the theory and implementation of RBM, so you might think it's not too hard when you just look at the code of DBN.</p><p>Since<a id="id231" class="indexterm"/> DBN after the training can be regarded as one (deep) neural network, you simply need to forward propagate data in each layer when you try to predict which class the unknown data belongs to:</p><div class="informalexample"><pre class="programlisting">public Integer[] predict(double[] x) {

   double[] z = new double[0];

   for (int layer = 0; layer &lt; nLayers; layer++) {

       double[] x_;

       if (layer == 0) {
           x_ = x;
       } else {
           x_ = z.clone();
       }

       z = sigmoidLayers[layer].forward(x_);
   }

   return logisticLayer.predict(z);
}</pre></div><p>As for evaluation, no explanation should be needed because it's not much different from the previous <a id="id232" class="indexterm"/>classifier model.</p><p>Congratulations! You have now acquired knowledge of one of the deep learning algorithms. You<a id="id233" class="indexterm"/> might be able to understand it more easily than expected. However, the difficult part of deep learning is actually setting up the parameters, such as setting how many hidden layers there are, how many units there are in each hidden layer, the learning rate, the iteration numbers, and so on. There are way more parameters to set than in the method of machine learning. Please remember that you might find this point difficult when you apply this to a realistic problem.</p></div><div class="section" title="Denoising Autoencoders"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec23"/>Denoising Autoencoders</h2></div></div></div><p>The <a id="id234" class="indexterm"/>method used in pre-training for SDA is<a id="id235" class="indexterm"/> called <span class="strong"><strong>Denoising Autoencoders</strong></span> (<span class="strong"><strong>DA</strong></span>). It can be said that DA is the method that emphasizes the role of equating inputs and outputs. What does this mean? The processing content of DA is as follows: DA adds some noise to input data intentionally and partially damages the data, and then DA performs learning as it restores corrupted data to the original input data. This intentional noise can be easily substantiated if the input data value is [0, 1]; by turning the value of the relevant part into 0 compulsorily. If a data value is out of this range, it can be realized, for example, by adding Gaussian noise, but in this book, we'll think about the former [0, 1] case to understand the core part of the algorithm.</p><p>In DA as well, an input/output layer is called a visible layer. DA's graphical model can be shown to be the same shape of RBM, but to get a better understanding, let's follow this diagram:</p><div class="mediaobject"><img src="graphics/B04779_03_11.jpg" alt="Denoising Autoencoders"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_03_51.jpg" alt="Denoising Autoencoders"/></span> is the corrupted data, the input data with noise. Then, forward propagation to the hidden<a id="id236" class="indexterm"/> layer and the output layer<a id="id237" class="indexterm"/> can be represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_52.jpg" alt="Denoising Autoencoders"/></div><div class="mediaobject"><img src="graphics/B04779_03_53.jpg" alt="Denoising Autoencoders"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_03_19.jpg" alt="Denoising Autoencoders"/></span> denotes the bias of the hidden layer and <span class="inlinemediaobject"><img src="graphics/B04779_03_23.jpg" alt="Denoising Autoencoders"/></span> the bias of the visible layer. Also, <span class="inlinemediaobject"><img src="graphics/B04779_03_20.jpg" alt="Denoising Autoencoders"/></span> denotes the sigmoid function. As seen in the preceding diagram, corrupting input data and <a id="id238" class="indexterm"/>mapping to a hidden layer is called <span class="strong"><strong>Encode</strong></span> and<a id="id239" class="indexterm"/> mapping to restore the encoded data to the original input data <a id="id240" class="indexterm"/>is called <span class="strong"><strong>Decode</strong></span>. Then, DA's evaluation function can be denoted with a negative log likelihood function of the original input data and decoded data:</p><div class="mediaobject"><img src="graphics/B04779_03_54.jpg" alt="Denoising Autoencoders"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_03_27.jpg" alt="Denoising Autoencoders"/></span> is the model parameter, the weight and the bias of the visible layer and the hidden layer. What we need to do is just find the gradients of these parameters against the evaluation <a id="id241" class="indexterm"/>function. To deform equations easily, we define the functions here:</p><div class="mediaobject"><img src="graphics/B04779_03_55.jpg" alt="Denoising Autoencoders"/></div><div class="mediaobject"><img src="graphics/B04779_03_56.jpg" alt="Denoising Autoencoders"/></div><p>Then, we get:</p><div class="mediaobject"><img src="graphics/B04779_03_57.jpg" alt="Denoising Autoencoders"/></div><div class="mediaobject"><img src="graphics/B04779_03_58.jpg" alt="Denoising Autoencoders"/></div><p>Using<a id="id242" class="indexterm"/> these functions, each gradient of a parameter<a id="id243" class="indexterm"/> can be shown as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_59.jpg" alt="Denoising Autoencoders"/></div><div class="mediaobject"><img src="graphics/B04779_03_60.jpg" alt="Denoising Autoencoders"/></div><div class="mediaobject"><img src="graphics/B04779_03_61.jpg" alt="Denoising Autoencoders"/></div><p>Therefore, only two terms are required. Let's derive them one by one:</p><div class="mediaobject"><img src="graphics/B04779_03_62.jpg" alt="Denoising Autoencoders"/></div><p>Here, we<a id="id244" class="indexterm"/> utilized the derivative of the<a id="id245" class="indexterm"/> <code class="literal">sigmoid</code> function:</p><div class="mediaobject"><img src="graphics/B04779_03_63.jpg" alt="Denoising Autoencoders"/></div><p>Also, we get:</p><div class="mediaobject"><img src="graphics/B04779_03_64.jpg" alt="Denoising Autoencoders"/></div><p>Therefore, the following equation can be obtained:</p><div class="mediaobject"><img src="graphics/B04779_03_65.jpg" alt="Denoising Autoencoders"/></div><p>On the other hand, we can also get the following:</p><div class="mediaobject"><img src="graphics/B04779_03_66.jpg" alt="Denoising Autoencoders"/></div><p>Hence, the<a id="id246" class="indexterm"/> renewed equation for each parameter will be as follows:</p><div class="mediaobject"><img src="graphics/B04779_03_67.jpg" alt="Denoising Autoencoders"/></div><div class="mediaobject"><img src="graphics/B04779_03_68.jpg" alt="Denoising Autoencoders"/></div><div class="mediaobject"><img src="graphics/B04779_03_69.jpg" alt="Denoising Autoencoders"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_03_70.jpg" alt="Denoising Autoencoders"/></span> is the number of iterations and <span class="inlinemediaobject"><img src="graphics/B04779_03_50.jpg" alt="Denoising Autoencoders"/></span> is the learning rate. Although DA requires a bit of technique for deformation, you can see that the theory itself is very simple compared to RBM.</p><p>Now, let's <a id="id247" class="indexterm"/>proceed with the implementation. The package structure is the same as the one for RBM.</p><div class="mediaobject"><img src="graphics/B04779_03_12.jpg" alt="Denoising Autoencoders"/></div><p>As for model parameters, in addition to the number of units in a hidden layer, the amount of noise being added to the input data is also a parameter in DA. Here, the corruption level is set at <code class="literal">0.3</code>. Generally, this value is often set at <code class="literal">0.1 ~ 0.3</code>:</p><div class="informalexample"><pre class="programlisting">double corruptionLevel = 0.3;</pre></div><p>The<a id="id248" class="indexterm"/> flow from the building model to training is<a id="id249" class="indexterm"/> the same as RBM. Although this method of training is called <code class="literal">contrastiveDivergence</code> in RBM, it's simply set as <code class="literal">train</code> in DA:</p><div class="informalexample"><pre class="programlisting">// construct DA
DenoisingAutoencoders nn = new DenoisingAutoencoders(nVisible, nHidden, null, null, null, rng);

// train
for (int epoch = 0; epoch &lt; epochs; epoch++) {
   for (int batch = 0; batch &lt; minibatch_N; batch++) {
       nn.train(train_X_minibatch[batch], minibatchSize, learningRate, corruptionLevel);
   }
}</pre></div><p>The content of <code class="literal">train</code> is as explained in the theory section. First of all, add noise to the input data, then encode and decode it:</p><div class="informalexample"><pre class="programlisting">// add noise to original inputs
double[] corruptedInput = getCorruptedInput(X[n], corruptionLevel);

// encode
double[] z = getHiddenValues(corruptedInput);

// decode
double[] y = getReconstructedInput(z);</pre></div><p>The <a id="id250" class="indexterm"/>process of adding noise is, as previously <a id="id251" class="indexterm"/>explained, the compulsory turning of the value of the corresponding part of the data into <code class="literal">0</code>:</p><div class="informalexample"><pre class="programlisting">public double[] getCorruptedInput(double[] x, double corruptionLevel) {

   double[] corruptedInput = new double[x.length];

   // add masking noise
   for (int i = 0; i &lt; x.length; i++) {
       double rand_ = rng.nextDouble();

       if (rand_ &lt; corruptionLevel) {
           corruptedInput[i] = 0.;
       } else {
           corruptedInput[i] = x[i];
       }
   }

   return corruptedInput;
}</pre></div><p>The other processes are just simple activation and propagation, so we won't go through them here. The calculation of the gradients follows math equations:</p><div class="informalexample"><pre class="programlisting">// calculate gradients

// vbias
double[] v_ = new double[nVisible];

for (int i = 0; i &lt; nVisible; i++) {
   v_[i] = X[n][i] - y[i];
   grad_vbias[i] += v_[i];
}

// hbias
double[] h_ = new double[nHidden];

for (int j = 0; j &lt; nHidden; j++) {

   for (int i = 0; i &lt; nVisible; i++) {
       h_[j] = W[j][i] * (X[n][i] - y[i]);
   }

   h_[j] *= z[j] * (1 - z[j]);
   grad_hbias[j] += h_[j];
}

// W
for (int j = 0; j &lt; nHidden; j++) {
   for (int i = 0; i &lt; nVisible; i++) {
       grad_W[j][i] += h_[j] * corruptedInput[i] + v_[i] * z[j];
   }
}</pre></div><p>Compared <a id="id252" class="indexterm"/>to RBM, the implementation of DA is also quite simple. When you test (<code class="literal">reconstruct</code>) the model, you don't <a id="id253" class="indexterm"/>need to corrupt the data. As in standard neural networks, you just need to forward propagate the given inputs based on the weights of the networks:</p><div class="informalexample"><pre class="programlisting">public double[] reconstruct(double[] x) {

   double[] z = getHiddenValues(x);
   double[] y = getReconstructedInput(z);

   return y;
}</pre></div></div><div class="section" title="Stacked Denoising Autoencoders (SDA)"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec24"/>Stacked Denoising Autoencoders (SDA)</h2></div></div></div><p>SDA is <a id="id254" class="indexterm"/>deep neural networks <a id="id255" class="indexterm"/>with piled up DA layers. In the same way that DBN consists of RBMs and logistic regression, SDA consists of DAs and logistic regression:</p><div class="mediaobject"><img src="graphics/B04779_03_13.jpg" alt="Stacked Denoising Autoencoders (SDA)"/></div><p>The<a id="id256" class="indexterm"/> flow of implementation<a id="id257" class="indexterm"/> is not that different between DBN and SDA. Even though there is a difference between RBM and DA in pre-training, the content of fine-tuning is exactly the same. Therefore, not much explanation might be needed.</p><p>The method for pre-training is not that different, but please note that the point where the <code class="literal">int</code> type was used for DBN is changed to double type, as DA can handle <code class="literal">[0, 1]</code>, not binary:</p><div class="informalexample"><pre class="programlisting">public void pretrain(double[][][] X, int minibatchSize, int minibatch_N, int epochs, double learningRate, double corruptionLevel) {

   for (int layer = 0; layer &lt; nLayers; layer++) {
       for (int epoch = 0; epoch &lt; epochs; epoch++) {
           for (int batch = 0; batch &lt; minibatch_N; batch++) {

               double[][] X_ = new double[minibatchSize][nIn];
               double[][] prevLayerX_;

               // Set input data for current layer
               if (layer == 0) {
                   X_ = X[batch];
               } else {

                 prevLayerX_ = X_;
                 X_ = new double[minibatchSize][hiddenLayerSizes[layer-1]];

                   for (int i = 0; i &lt; minibatchSize; i++) {
                       X_[i] = sigmoidLayers[layer-1].output(prevLayerX_[i]);
                   }
               }

               daLayers[layer].train(X_, minibatchSize, learningRate, corruptionLevel);
           }
       }
   }

}</pre></div><p>The<a id="id258" class="indexterm"/> <code class="literal">predict</code> method after<a id="id259" class="indexterm"/> learning is also exactly the same as in DBN. Considering that both DBN and SDA can be treated as one multi-layer neural network after learning (that is, the pre-training and fine-tuning), it's natural that most of the processes are common.</p><p>Overall, SDA can be implemented more easily than DBN, but the precision to be obtained is almost the same. This is the merit of SDA.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec21"/>Summary</h1></div></div></div><p>In this chapter, we looked at the problem of the previous neural networks algorithm and what the breakthrough was for deep learning. Also, you learned about the theory and implementation of DBN and SDA, the algorithm that fueled the boom of deep learning, and of RBM and DA used in each respective method.</p><p>In the next chapter, we'll look at more deep learning algorithms. They take different approaches to obtain high precision rates and are well developed.</p></div></body></html>