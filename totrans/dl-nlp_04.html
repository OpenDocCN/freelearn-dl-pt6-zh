<html><head></head><body>
		<div class="Content" id="_idContainer087">
			<h1 id="_idParaDest-102"><em class="italics"><a id="_idTextAnchor106"/>Chapter 4</em></h1>
		</div>
		<div class="Content" id="_idContainer088">
			<h1 id="_idParaDest-103"><a id="_idTextAnchor107"/>Foundations of Convolutional Neural Network</h1>
		</div>
		<div class="Content" id="_idContainer089">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Describe the inspiration for CNNs in neural science</li>
				<li class="bullets">Describe the convolution operations</li>
				<li class="bullets">Describe a basic CNN architecture for a classification task</li>
				<li class="bullets">Implement a simple CNN for image and text classification tasks</li>
				<li class="bullets">Implement a CNN for a sentiment analysis of text</li>
			</ul>
			<p><a id="_idTextAnchor108"/>In this chapter, we aim to cover the architecture of convolutional neural networks (CNNs) and gain an intuition of CNNs based on their applications on image data, before delving into their applications in natural language processing.</p>
		</div>
		<div class="Content" id="_idContainer117">
			<h2 id="_idParaDest-104"><a id="_idTextAnchor109"/>Introduction</h2>
			<p>Neural networks, as a broad field, borrow a lot from biological systems, particularly the brain. Advances in neural science have directly influenced research in to neural networks.</p>
			<p>CNNs are inspired by the work of two neural scientists, D.H. Hubel and T.N. Wiesel. Their research focused on the mammalian visual cortex, which is the part of the brain responsible for vision. Through their research back in the sixties, they found that the visual cortex is composed of layers of neurons. Furthermore, these layers are arranged in a hierarchical structure. This hierarchy ranges from simple-to hypercomplex neurons. They also advanced the notion of a 'receptive field,' which is the space within which certain stimuli activate or fire a neuron, with a degree of spatial invariance. Spatial or shift invariance allows animals to detect objects regardless of whether they are rotated, scaled, transformed, or partially obscured.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer090">
					<img alt="Figure 4.1: Examples of spatial variance&#13;&#10;" src="image/C13783_04_01.jpg"/>
				</div>
			</div>
			<h6>Figure 4.1: Examples of spatial variance</h6>
			<p>Inspired by neural concepts of how animals see, computer vision scientists have modelled neural networks that adhere to the same principles of locality, hierarchy, and spatial invariance. We will dive deeper into the architecture of CNNs in the next section.</p>
			<p>CNNs are a subset of neural networks that contain one or more 'convolution' layers. Typical neural networks are fully connected, which means every neuron is connected to every neuron in the next layer. When dealing with high-dimensional data such as images, sound, and so on, typical neural networks are slow and tend to overfit as there are too many weights being learned. Convolutional layers solve this problem by connecting a neuron to a region of the input in lower layers. We will discuss convolution layers in greater detail in the next section.</p>
			<p>To understand the general architecture of CNNs, we will first apply them to the task of image classification and then, subsequently, to natural language processing. To begin, we'll do a small exercise to understand how computers see images.</p>
			<h3 id="_idParaDest-105">E<a id="_idTextAnchor110"/>xercise 18: Finding Out How Computers See Images</h3>
			<p>Images and text share an important similarity. The location of a pixel in an image, or a word in text, matters. This spatial significance makes applying convolutional neural networks possible for both text and images.</p>
			<p>In this exercise, we want to determine how computers interpret images. We will do this by using the <strong class="bold">MNIST</strong> dataset, which contains a repository of handwritten digits perfect for demonstrating CNNs.</p>
			<h4>Note</h4>
			<p class="callout">MNIST is a built-in Keras dataset.</p>
			<p>You will need to have both Python and Keras installed. For easier visualization, you can run your code in a Jupyter notebook:</p>
			<ol>
				<li>Start by importing the necessary classes:<p class="snippet">%matplotlib inline</p><p class="snippet">import keras</p><p class="snippet">import matplotlib.pyplot as plt</p></li>
				<li>Since we'll be using this dataset throughout the chapter, we will import the training and test sets as shown here:<p class="snippet">(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()</p></li>
				<li>Visualize the first image in the dataset:<p class="snippet">sample_image = X_train[0]</p><p class="snippet">plt.imshow(sample_image)</p><p>Running the preceding code should result in an image being visualized, as shown here:</p><div class="IMG---Figure" id="_idContainer091"><img alt="Figure 4.2: Visualization of an image" src="image/C13783_04_02.jpg"/></div><h6>Figure 4.2: Visualization of an image</h6><p>The images are 28 by 28 pixels, with each pixel being a number between 0 and 255. Try playing around with different indices to display their values as follows. You can do this by putting arbitrary numbers between <strong class="inline">0</strong> and <strong class="inline">255</strong> as <strong class="inline">x</strong> and <strong class="inline">y</strong> in: </p><p class="snippet">print(sample_image[x][y]) </p></li>
				<li>When you run the print code as follows, expect to see numbers between 0 and 255: <p class="snippet">print(sample_image[22][11])</p><p class="snippet">print(sample_image[6][12])</p><p class="snippet">print(sample_image[5][23])</p><p class="snippet">print(sample_image[10][11])</p><p><strong class="bold">Expected Output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer092">
					<img alt="Figure 4.3: Numerical representation of an image" src="image/C13783_04_03.jpg"/>
				</div>
			</div>
			<h6>Figure 4.3: Numerical representation of an image</h6>
			<p>This exercise is meant to help you appreciate how image data is processed with each pixel as a number between <strong class="bold">0</strong> and <strong class="bold">255</strong>. This understanding is essential as we'll feed these images into a CNN as input in the next section.</p>
			<h2 id="_idParaDest-106">Und<a id="_idTextAnchor111"/>erstanding the Architecture of a CNN</h2>
			<p>Let's assume we have the task of classifying each of the <strong class="bold">MNIST</strong> images as a number between 0 and 9. The input in the previous example is an image matrix. For a colored image, each pixel is an array with three values corresponding to the <strong class="bold">RGB</strong> color scheme. For grayscale images, each pixel is just one number, as we saw earlier. </p>
			<p>To understand the architecture of a CNN, it is best to separate it into two sections as visualized in the image that follows.</p>
			<p>A forward pass of the CNN involves a set of operations in the two sections.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer093">
					<img alt="Figure 4.4: Application of convolution and ReLU operations" src="image/C13783_04_04.jpg"/>
				</div>
			</div>
			<h6>Figure 4.4: Application of convolution and ReLU operations</h6>
			<p>The figure is explained in the following sections:</p>
			<ul>
				<li>Feat<a id="_idTextAnchor112"/>ure extraction</li>
				<li>Neural network </li>
			</ul>
			<h3 id="_idParaDest-107"><a id="_idTextAnchor113"/>Feature Extraction</h3>
			<p>The first section of a CNN is all about feature extraction. Conceptually, it can be interpreted as the model's attempt to learn which features distinguish one class from another. In the task of classifying images, these features might include unique shapes and colors. </p>
			<p>CNNs learn the hierarchical structure of these features. The lower layers of a CNN abstract features such as edges, while the higher layers learn more defined features such as shapes.</p>
			<p>Feature learning occurs through a set of three operations repeated a number of times, as follows:</p>
			<ol>
				<li value="1">Convolution</li>
				<li>An activation function (the application of the ReLU activation function to achieve non-linearity)</li>
				<li>Pooling</li>
			</ol>
			<h3 id="_idParaDest-108">Conv<a id="_idTextAnchor114"/>olution</h3>
			<p>Convolution is the an operation that distinguishes CNNs from other neural networks. The convolution operation is not unique to machine learning; it is applied in many other fields, such as electrical engineering and signal processing. </p>
			<p>Convolution can be thought of as looking through a small window as we move the window to the right and down. Convolution, in this context, involves iteratively sliding a "filter" across an image, while applying a dot product as we move left and down.</p>
			<p>This window is called a "<em class="italics">filter</em>" or a "<em class="italics">kernel</em>". In the actual sense, a filter or kernel is a matrix of preferably smaller dimensions than the input. To better understand how filters are applied to images, consider the following example. After calculating the dot product on the area covered by the filter, we take a step to the right and calculate the dot product:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer094">
					<img alt="Figure 4.5: Filter application to images&#13;&#10;" src="image/C13783_04_05.jpg"/>
				</div>
			</div>
			<h6>Figure 4.5: Filter application on images</h6>
			<p>The result of this convolution is known as a feature map or an activation map.</p>
			<p>The size of the filter needs to be defined as a hyperparameter. This size can also be considered the area for which a neuron can "see" the input. This is called a neuron's <em class="italics">receptive field</em>. Additionally, we need to define the stride size, that is, the number of steps we need to take before applying the filter. Pixels at the center have the filters passing through several times compared with those at the edges. To avoid losing information at the corners, it is advisable to add an extra layer of zeros as padding.</p>
			<h3 id="_idParaDest-109">The ReLU <a id="_idTextAnchor115"/>Activation Function</h3>
			<p>Activation functions are used all across machine learning. They are useful for introducing non-linearity and allowing the a model to learn non-linear functions. In this particular context, we apply the <strong class="keyword">Rectified Linear Unit</strong> (<strong class="keyword">ReLU</strong>). It basically replaces all the negative values with zero.</p>
			<p>The following image demonstrates the change in an image after ReLU is applied. </p>
			<div>
				<div class="IMG---Figure" id="_idContainer095">
					<img alt="Figure 4.6: Image after applying ReLU function&#13;&#10;" src="image/C13783_04_06.jpg"/>
				</div>
			</div>
			<h6>Figure 4.6: Image after applying ReLU function</h6>
			<h3 id="_idParaDest-110"><a id="_idTextAnchor116"/>Exercise 19: Visualizing ReLU</h3>
			<p>In this exercise we will visualize the Rectified Linear Unit function. The ReLU function will be plotted on an X-Y axis, where X is numbers in the range of -15 to 15 and Y is the output after applying the ReLU function. The goal of this exercise is to visualize ReLU.</p>
			<ol>
				<li value="1">Import the required Python packages:<p class="snippet">from matplotlib import pyplot</p></li>
				<li>Define the ReLU function:<p class="snippet">def relu(x):</p><p class="snippet">    return max(0.0, x)</p></li>
				<li>Specify the input and output references:<p class="snippet">inputs = [x for x in range(-15, 15)]</p><p class="snippet">outputs = [relu(x) for x in inputs]</p></li>
				<li>Plot the input against the output:<p class="snippet">pyplot.plot(inputs, outputs) #Plot the input against the output</p><p class="snippet">pyplot.show()</p><p><strong class="bold">Expected Output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer096">
					<img alt="Figure 4.7: Graph plot for ReLU&#13;&#10;" src="image/C13783_04_07.jpg"/>
				</div>
			</div>
			<h6>Figure 4.7: Graph plot for ReLU</h6>
			<h3 id="_idParaDest-111">Pooling<a id="_idTextAnchor117"/></h3>
			<p>Pooling is a downsampling process that involves reducing dimensionality from a higher to a lower dimensional space. In machine learning, pooling is applied as a way to reduce the spatial complexity of the layers. This allows for fewer weights to be learned and consequently faster training times. </p>
			<p>Historically, different techniques have been used to perform pooling, such as average pooling and L2-norm pooling. The most preferred pooling technique is max pool. Max pooling involves taking the largest element within a defined window size. The following is an example of max pooling on a matrix:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer097">
					<img alt="Figure 4.8: Max pool&#13;&#10;" src="image/C13783_04_08.jpg"/>
				</div>
			</div>
			<h6>Figure 4.8: Max pool</h6>
			<p>If we apply max pooling to the preceding example, the section that has 2, 6, 3, and 7 is reduced to 7. Similarly, the section with 1, 0, 9, and 2 is reduced to 9. With max pooling, we pick the largest number in a section.</p>
			<h3 id="_idParaDest-112"><a id="_idTextAnchor118"/>Dropout</h3>
			<p>A common problem encountered in machine learning is overfitting. Overfitting occurs when a model "memorizes" the training data and is unable to generalize when presented with different examples in testing. There are several ways to avoid overfitting, particularly through regularization:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer098">
					<img alt="Figure 4.9: Regularization" src="image/C13783_04_09.jpg"/>
				</div>
			</div>
			<h6>Figure 4.9: Regularization</h6>
			<p>Regulation is the process of constraining coefficients toward zero. Regularization can be summarized as techniques used to penalize learned coefficients so that they tend towards zero. Dropout is a common regularization technique that is applied by randomly "dropping" some neurons during both the forward and backward passes. To implement dropout, we specify the probability of a neuron being dropped as a parameter. By randomly dropping neurons, we ensure that the model is able to generalize better and therefore be a little more flexible.</p>
			<h3 id="_idParaDest-113">Classific<a id="_idTextAnchor119"/>ation in Convolutional Neural Network</h3>
			<p>The secon<a id="_idTextAnchor120"/>d section of a CNN is more task-specific. For the task of classification, this section is basically a fully connected neural network. A neural network is regarded as fully connected when every neuron in one layer is connected to all the neurons in the next layer. The input to the fully connected layer is a flattened vector that is the output of section one. Flattening converts the matrix into a 1D vector.</p>
			<p>The number of hidden layers in the fully connected layer is a hyperparameter that can be optimized and fine-tuned. </p>
			<h3 id="_idParaDest-114">Exercise <a id="_idTextAnchor121"/>20: Creating a Simple CNN Architecture</h3>
			<p>In this exercise, you will construct a simple CNN model using Keras. This exercise will entail creating a model with the layers discussed so far. In the first section of the model, we will have two convolutional layers with the ReLU activation function, a pooling layer, and a dropout layer. In the second section, we will have a flattened layer and a fully connected layer. </p>
			<ol>
				<li value="1">First, we import the necessary classes: <p class="snippet">from keras.models import Sequential #For stacking layers</p><p class="snippet">from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout</p><p class="snippet">from keras.utils import plot_model</p></li>
				<li>Next, define the variables used:<p class="snippet">num_classes = 10</p></li>
				<li>Let's now define the model. Keras's Sequential model allows you to stack layers as you go:<p class="snippet">model = Sequential()</p></li>
				<li>We can now add section one layers. The convolution and ReLU layers are defined together. We have two convolutional layers. We define a kernel size of 3 for each. The first layer of the model receives the input. We need to define how it should expect that input to be structured. In our case, the input is in the form of 28 by 28 images. We also need to specify the number of neurons for each layer. In our case, we define 64 neurons for the first layer and 32 neurons for the second layer. Please note that these are hyperparameters that can be optimized:<p class="snippet">model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))</p><p class="snippet">model.add(Conv2D(32, kernel_size=3, activation='relu'))</p></li>
				<li>We then add a pooling layer, followed by a dropout layer with a 25% probability of neurons being 'dropped':<p class="snippet">model.add(MaxPooling2D(pool_size=(2, 2)))</p><p class="snippet">model.add(Dropout(0.25))</p><p>The section one layers are done. Please note that the number of layers is also a hyperparameter that can be optimized. </p></li>
				<li>For section two, we first flatten the input. We then add a fully connected or dense layer. Using the softmax activation function, we can calculate the probability for each of the 10 classes:<p class="snippet">model.add(Flatten())</p><p class="snippet">model.add(Dense(num_classes, activation='softmax'))</p></li>
				<li>To visualize the model architecture so far, we can print out the model as follows:<p class="snippet">model.summary()</p><p><strong class="bold">Expected Output</strong>:</p><div class="IMG---Figure" id="_idContainer099"><img alt="Figure 4.10: Model summary" src="image/C13783_04_10.jpg"/></div><h6>Figure 4.10: Model summary</h6></li>
				<li>You can also run the following code to export the image to a file:<p class="snippet">plot_model(model, to_file='model.png')</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer100">
					<img alt="Figure 4.11: Visualized architecture of a simple CNN&#13;&#10;" src="image/C13783_04_11.jpg"/>
				</div>
			</div>
			<h6>Figure 4.11: Visualized architecture of a simple CNN</h6>
			<p>In the preceding exercise, we created a simple CNN with two convolutional layers for the task of classification. In the preceding output image, you'll notice how the layers are stacked – starting from the input layer, then the two convolutional layers, the pooling, dropout, and flattening layers, and the fully connected layer at the end.</p>
			<h2 id="_idParaDest-115">Training a <a id="_idTextAnchor122"/>CNN</h2>
			<p>During the training of a CNN, the model tries to learn the weights of the filters in feature extraction and the weights at the fully connected layers in the neural network. To understand how a model is trained, we'll discuss how the probability of each output class is calculated, how we calculate the error or the loss, and finally, how we optimize or minimize that loss while updating the weights: </p>
			<ol>
				<li value="1">Probabilities<p>Recall that in the last layer of the neural network section, we used a softmax function to calculate the probability of each output class. This probability is calculated by dividing the exponent of that class score by the sum of the exponents of all scores:</p><div class="IMG---Figure" id="_idContainer101"><img alt="Figure 4.12: Expression to calculate probability" src="image/C13783_04_12.jpg"/></div><h6>Figure 4.12: Expression to calculate probability</h6></li>
				<li>Loss<p>We need to be able to quantify how well the calculated probabilities predict the actual class. This is done by calculating a loss, which in the case of classification probability is best done through the categorical cross-entropy loss function. The categorical cross-entropy loss function takes in two vectors, the predicted classes (let's call that y') and the actual classes (say y), and outputs the overall loss. Cross-entropy loss is calculated as the sum of the negative log likelihoods of the class probabilities. It can be represented as the H function here:</p><div class="IMG---Figure" id="_idContainer102"><img alt="" src="image/C13783_04_13.jpg"/></div><h6>Figure 4.13: Expression to calculate loss</h6></li>
				<li>Optimization<p>Consider the sketch of cross-entropy loss that follows. By minimizing the loss, we can predict the correct class with a higher probability: </p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer103">
					<img alt="Figure 4.14: Cross-entropy loss vs. predicted probability" src="image/C13783_04_14.jpg"/>
				</div>
			</div>
			<h6>Figure 4.14: Cross-entropy loss versus predicted probability</h6>
			<p>Gradient descent is an optimization algorithm for finding the minimum of a function, such as the loss function described earlier. Although the overall error is calculated, we need to go back and calculate how much of that loss was contributed by each node. Consequently, we can update the weights, so as to minimize the overall error. Backpropagation applies the chain rule of calculus to calculate the update for each weight. This is done by taking the partial derivative of the error or loss relative to the weights.</p>
			<p>To better visualize these steps, consider the following diagram, which summarizes the three steps. For the classification task, the first step involves the calculation of probabilities for each output class. We then apply a loss function to quantify how well the probabilities predict the actual class. In order to make a better prediction going forward, we then update our weights by performing backpropagation through gradient descent:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer104">
					<img alt="Figure 4.15: Steps for task of classification&#13; &#10;" src="image/C13783_04_15.jpg"/>
				</div>
			</div>
			<h6>Figure 4.15: Steps for the classification task</h6>
			<h3 id="_idParaDest-116">Exercise 21: Tr<a id="_idTextAnchor123"/>aining a CNN</h3>
			<p>In this exercise, we will train the model we created in exercise 20. The following steps will help you with the solution. Recall that this is for the overall task of classification.</p>
			<ol>
				<li value="1">We start by defining the number of epochs. An epoch is a common hyperparameter used in deep neural networks. One epoch is when the entire dataset is passed through a complete forward and backward pass. As training data is usually a lot, data can be divided into several batches:<p class="snippet">epochs=12</p></li>
				<li>Recall that we imported the MNIST dataset by running the following command:<p class="snippet">(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()</p></li>
				<li>We first reshape the data to fit the model:<p class="snippet">X_train = X_train.reshape(60000,28,28,1) #60,000 is the number of training examples</p><p class="snippet">X_test = X_test.reshape(10000,28,28,1)</p></li>
				<li>The to_categorical function changes a vector of integers to a matrix of one-hot encoded vectors. Given the following example, the function returns the array shown:<p class="snippet">#Demonstrating the to_categorical method</p><p class="snippet">Import numpy as np</p><p class="snippet">from keras.utils import to_categorical</p><p class="snippet">example = [1,0,3,2]</p><p class="snippet">to_categorical(example)</p><p>The array would be as follows:</p><div class="IMG---Figure" id="_idContainer105"><img alt="Figure 4.16: Array output" src="image/C13783_04_16.jpg"/></div><h6>Figure 4.16: Array output</h6></li>
				<li>We apply it to the target column as shown:<p class="snippet">from keras.utils import to_categorical</p><p class="snippet">y_train = to_categorical(y_train)</p><p class="snippet">y_test = to_categorical(y_test)</p></li>
				<li>We then define the loss function as a categorical cross-entropy loss function. Additionally, we define the optimizer and the metrics. The Adam(Adaptive Moment) optimizer is an optimization algorithm often used in place of stochastic gradient descent. It defines an adaptive learning rate for each parameter of the model:<p class="snippet">model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</p></li>
				<li>To train the model, run the .fit method:<p class="snippet">model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs)</p><p>The output should be as follows:</p><div class="IMG---Figure" id="_idContainer106"><img alt="Figure 4.17: Training the model&#13;&#10;" src="image/C13783_04_17.jpg"/></div><h6>Figure 4.17: Training the model</h6></li>
				<li>To evaluate the model's performance, you can run the following:<p class="snippet">score = model.evaluate(X_test, y_test, verbose=0)</p><p class="snippet">print('Test loss:', score[0])</p><p class="snippet">print('Test accuracy:', score[1])</p></li>
				<li>For this task, we expect a fairly high accuracy after a number of epochs:</li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer107">
					<img alt="Figure 4.18: Accuracy and loss output" src="image/C13783_04_18.jpg"/>
				</div>
			</div>
			<h6>Figure 4.18: Accuracy and loss output</h6>
			<h3 id="_idParaDest-117">Applying CNNs to T<a id="_idTextAnchor124"/>ext</h3>
			<p>Now that we have a general intuition of how CNNs work using images, let's look at how they can be applied in natural language processing. Just like images, text has spatial qualities that make it ideal for CNN usage. However, there is one main change to the architecture that we introduce when dealing with text. Instead of having two-dimensional convolutional layers, text is one-dimensional, as shown here.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer108">
					<img alt="Figure 4.19: One-dimensional convolution" src="image/C13783_04_19.jpg"/>
				</div>
			</div>
			<h6>Figure 4.19: One-<a id="_idTextAnchor125"/>dimensional convolution</h6>
			<p>It is important to note that the preceding input sequence can be either the character sequence or the word sequence. The application of CNNs on text, at the character level, can be visualized as shown in the following figure. CNNs have 6 convolutional layers and 3 fully connected layers as shown here. </p>
			<div>
				<div class="IMG---Figure" id="_idContainer109">
					<img alt="Figure 4.20: CNN with 6 convolutional and 3 fully connected layers&#13;&#10;" src="image/C13783_04_20.jpg"/>
				</div>
			</div>
			<h6>Figure 4.20: CNN with 6 convolutional and 3 fully connected layers</h6>
			<p>Character-level CNNs were shown to perform well when applied to large noisy data. They are also simpler than word-level applications because they require no preprocessing (such as stemming) and the characters are represented as one-hot encoding representations.</p>
			<p>In the following example, we will demonstrate the application of CNNs to text at a word level. We will therefore need to perform some vectorization and padding before feeding the data into the CNN architecture.</p>
			<h3 id="_idParaDest-118">Exercise 22: Applica<a id="_idTextAnchor126"/><a id="_idTextAnchor127"/>tion of a Simple CNN to a Reuters News Topic for Classification</h3>
			<p>In this exercise, we will be applying a CNN model to the built-in Keras Reuters dataset. </p>
			<h4>Note</h4>
			<p class="callout">If you are using Google Colab, you need to downgrade your version of <strong class="inline">numpy</strong> to 1.16.2 by running </p>
			<p class="callout"><strong class="inline">!pip install numpy==1.16.1 </strong></p>
			<p class="callout"><strong class="inline">import numpy as np</strong></p>
			<p class="callout">This downgrade is necessary since this version of <strong class="inline">numpy </strong>has the default value of <strong class="inline">allow_pickle</strong> as <strong class="inline">True</strong>.</p>
			<ol>
				<li value="1">Start by importing the necessary classes:<p class="snippet">import keras</p><p class="snippet">from keras.datasets import reuters</p><p class="snippet">from keras.preprocessing.text import Tokenizer</p><p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras import layers</p></li>
				<li>Define the variables:<p class="snippet">batch_size = 32</p><p class="snippet">epochs = 12</p><p class="snippet">maxlen = 10000</p><p class="snippet">batch_size = 32</p><p class="snippet">embedding_dim = 128</p><p class="snippet">num_filters = 64</p><p class="snippet">kernel_size = 5</p></li>
				<li>Load the Reuters dataset:<p class="snippet">(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)</p></li>
				<li>Prepare the data:<p class="snippet">word_index = reuters.get_word_index(path="reuters_word_index.json")</p><p class="snippet">num_classes = max(y_train) + 1 </p><p class="snippet">index_to_word = {}</p><p class="snippet">for key, value in word_index.items():</p><p class="snippet">    index_to_word[value] = key</p></li>
				<li>Tokenize the input data:<p class="snippet">tokenizer = Tokenizer(num_words=maxlen)</p><p class="snippet">x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')</p><p class="snippet">x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')</p><p class="snippet">y_train = keras.utils.to_categorical(y_train, num_classes)</p><p class="snippet">y_test = keras.utils.to_categorical(y_test, num_classes)</p></li>
				<li>Define the model:<p class="snippet">model = Sequential()</p><p class="snippet">model.add(layers.Embedding(512, embedding_dim, input_length=maxlen))</p><p class="snippet">model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))</p><p class="snippet">model.add(layers.GlobalMaxPooling1D())</p><p class="snippet">model.add(layers.Dense(10, activation='relu'))</p><p class="snippet">model.add(layers.Dense(num_classes, activation='softmax'))</p><p class="snippet">model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</p></li>
				<li>Train and evaluate the model. Print the accuracy score:<p class="snippet">history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)</p><p class="snippet">score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)</p><p class="snippet">print('Test loss:', score[0])</p><p class="snippet">print('Test accuracy:', score[1])</p><p><strong class="bold">Expected output</strong>:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer110">
					<img alt="Figure 4.21: Accuracy score" src="image/C13783_04_21.jpg"/>
				</div>
			</div>
			<h6>Figure 4.21: Accuracy score</h6>
			<p>We have thus created a model and trained it on a dataset.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor128"/>Application Areas of CNNs</h2>
			<p>Now that we understand the architecture of CNNs, let's look at some applications. In general, CNNs are great for data that has a spatial structure. Examples of types of data that has a spatial structure are sound, images, video, and text.</p>
			<p>In natural language processing, CNNs are used for various tasks such as sentence classification. One example is the task of sentiment classification, where a sentence is classified as belonging to a predetermined group of classes.</p>
			<p>As discussed earlier, CNNs are applied at the character level to classification tasks such as sentiment classification, especially on noisy datasets such as social media posts.</p>
			<p>CNNs are more commonly applied in computer vision. Here are some applications in this area: </p>
			<ul>
				<li><em class="italics">Facial recognition</em><p>Most social networking sites employ CNNs to detect faces and subsequently perform tasks such as tagging.</p></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer111">
					<img alt="Figure 4.22: Facial recognition" src="image/C13783_04_22.jpg"/>
				</div>
			</div>
			<h6>Figure 4.22: Facial recognition</h6>
			<ul>
				<li><em class="italics">Object detection</em><p>Similarly, CNNs are able to detect objects in images. There are several CNN-based architectures that are used to detect objects, one of the most popular being R-CNN. (R-CNN stands for Region CNN.) An R-CNN works by applying a selective search to come up with regions and subsequently use using CNNs to perform classification, one region at a time. </p></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer112">
					<img alt="Figure 4.23: Object detection&#13;&#10;" src="image/C13783_04_23.jpg"/>
				</div>
			</div>
			<h6>Figure 4.23: Object detection</h6>
			<ul>
				<li><em class="italics">Image captioning</em><p>This task involves creating a textual description for an image. One way to perform image captioning is to replace the fully connected layer in section two with a recurrent neural network (RNN).</p></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer113">
					<img alt="Figure 4.24: Image captioning&#13;&#10;" src="image/C13783_04_24.jpg"/>
				</div>
			</div>
			<h6>Figure 4.24: Image captioning</h6>
			<ul>
				<li><em class="italics">Semantic segmentation</em><p>Semantic segmentation is the task of segmenting an image into more meaningful parts. Each pixel in an image is classified as belonging to a class.</p></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer114">
					<img alt="Figure 4.25: Semantic segmentation" src="image/C13783_04_25.jpg"/>
				</div>
			</div>
			<h6>Figure 4.25: Semantic segmentation</h6>
			<p>An architecture that can be used to perform semantic segmentation is a <strong class="keyword">Fully</strong> <strong class="keyword">Convoluted</strong> <strong class="keyword">Network</strong> (<strong class="keyword">FCN</strong>). The architecture of FCNs is slightly different from the preceding one in two ways: it has no fully connected layer and it has upsampling. Upsampling is the process of making the output image larger preferably the same size as the input image.</p>
			<p>Here is a sample architecture:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer115">
					<img alt="Figure 4.26: Sample architecture of semantic segmentation" src="image/C13783_04_26.jpg"/>
				</div>
			</div>
			<h6>Figure 4.26: Sample architecture of semantic segmentation</h6>
			<h4>Note</h4>
			<p class="callout">For more on FCNs, refer to the paper by Jonathan Long, Evan Shelhamer, and Trevor Darrell titled Fully Convolutional Networks for Semantic Segmentation.</p>
			<h3 id="_idParaDest-120">Activity 5: Sentiment Anal<a id="_idTextAnchor129"/><a id="_idTextAnchor130"/>ysis on a Real-life Dataset</h3>
			<p>Imagine that you are taske<a id="_idTextAnchor131"/>d with creating a model to classify the reviews from a dataset. In this activity, we will build a CNN that performs the binary classification task of sentiment analysis. We will be using a real-life dataset from UCI's repository.</p>
			<h4>Note</h4>
			<p class="callout">This dataset is downloaded from <a href="">https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences</a></p>
			<p class="callout">From Group to Individual Labels using Deep Features, Kotziaa et al., KDD 2015 UCI machine learning Repository [http://archive.ics.uci.edu.ml]. Irvine, CA: University of California, School of Information and Computer Science</p>
			<p class="callout">You can also download it from our GitHub repository link:</p>
			<p class="callout">https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2004</p>
			<p>The following steps will help you with the solution.</p>
			<ol>
				<li value="1">Download the Sentiment Lab<a id="_idTextAnchor132"/>elled Sentences dataset.</li>
				<li>Create a directory labelled 'data' within your working directory and unzip the downloaded folder within the directory. </li>
				<li>Create and run your working script (for example, sentiment.ipynb) on Jupyter Notebook. </li>
				<li>Import your data using pandas read_csv method. Feel free to use one or all of the files in the dataset.</li>
				<li>Split your data into training and test sets by using scikit learn's train_test_split.  </li>
				<li>Tokenize using Keras's tokenizer. </li>
				<li>Convert the text into sequences using the texts_to_sequences method. </li>
				<li>Ensure that all sequences have the same length by padding them. You can use Keras's pad_sequences function.</li>
				<li>Define the model with a minimum of one convolutional layer and one fully connected layer. As this is a binary classification, we use a sigmoid activation function and calculate the loss through binary cross-entropy loss. </li>
				<li>Train and test the model.<h4>Note</h4><p class="callout">The solution for the activity can be found on page 305.</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer116">
					<img alt="Figure 4.27: Accuracy scores" src="image/C13783_04_27.jpg"/>
				</div>
			</div>
			<h6>Figure 4.27: Accuracy scores</h6>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor133"/>Summary</h2>
			<p>In this chapter, we studied the architecture and applications of convolutional neural networks (CNNs). CNNs are applied not just to text and images but also to datasets that have some form of spatial structure. In the upcoming chapters, you will explore how to apply other forms of neural networks to various natural language tasks.</p>
		</div>
	</body></html>