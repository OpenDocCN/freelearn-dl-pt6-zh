<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Recurrent and Convolutional Neural Networks</h1>
                
            
            <article>
                
<p class="calibre2">Until now, we have been studying feed-forward networks, where the data moves in one direction and there is no interconnection of nodes in each layer. In the presence of basic hypotheses that interact with some problems, the intrinsic unidirectional structure of feed-forward networks is strongly limiting. However, it is possible to start from it and create networks in which the results of computing one unit affect the computational process of the other. It is evident that algorithms that manage the dynamics of these networks must meet new convergence criteria.</p>
<p class="calibre2">In this chapter, we will introduce <strong class="calibre1">Recurrent Neural Networks</strong> (<strong class="calibre1">RNN</strong>), which are networks with cyclic data flows. We will also see <strong class="calibre1">Convolutional Neural Networks</strong> (<strong class="calibre1">CNN</strong>), which are standardized neural networks mainly used for image recognition. For both of these types of networks, we will do some sample implementations in R. <span>The following topics are covered:</span></p>
<ul class="calibre16">
<li class="calibre17">RNN</li>
<li class="calibre17"><span>The</span> <kbd class="calibre13">rnn</kbd> <span>package</span></li>
<li class="calibre17"><strong class="calibre1">Long Short-Term Memory</strong> (<strong class="calibre1">LSTM</strong>) model</li>
<li class="calibre17">CNN</li>
<li class="calibre17">Common CNN architecture--<strong class="calibre1">LeNet</strong></li>
</ul>
<p class="calibre2"><span>At the end of the chapter, we will understand training, testing, and evaluating an RNN. We will learn how to visualize the RNN model in R environment. We will also be able to train an LSTM model. We will cover the concepts as CNN</span> <span>and c</span>ommon CNN architecture--LeNet<span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Recurrent Neural Network</h1>
                
            
            <article>
                
<p class="calibre2">Within the set of <strong class="calibre1">Artificial Neural Networks</strong> (<strong class="calibre1">ANN</strong>), there are several variants based on the number of hidden layers and data flow. One of the variants is RNN, where the connections between neurons can form a cycle. Unlike feed-forward networks, RNNs can use internal memory for their processing. RNNs are a class of ANNs that feature connections between hidden layers that are propagated through time in order to learn sequences. RNN use cases include the following fields:</p>
<ul class="calibre16">
<li class="calibre17">Stock market predictions</li>
<li class="calibre17">Image captioning</li>
<li class="calibre17">Weather forecast</li>
<li class="calibre17">Time-series-based forecasts</li>
<li class="calibre17">Language translation</li>
<li class="calibre17">Speech recognition</li>
<li class="calibre17">Handwriting recognition</li>
<li class="calibre17">Audio or video processing</li>
<li class="calibre17">Robotics action sequencing</li>
</ul>
<p class="calibre2">The networks we have studied so far (feed-forward networks) are based on input data that is powered to the network and converted into output. If it is a supervised learning algorithm, the output is a label that can recognize the input. Basically, these algorithms connect raw data to specific categories by recognizing patterns.<br class="title-page-name"/>
Recurrent networks, on the other hand, take as their input not only current input data that is powered to the network but also what they have experienced over time.</p>
<p class="calibre2">The decision made by a recurrent network at a specific instant affects the decision it will reach immediately afterwards. So, recurrent networks have two input sources--the present and the recent past--that combine to determine how they respond to new data, just as people do in life everyday.</p>
<p class="calibre2">Recurrent networks are distinguished from feed-forward networks thanks to the feedback loop linked to their past decisions, thus accepting their output momentarily as inputs. This feature can be emphasized by saying that recurrent networks have memory. Adding memory to neural networks has a purpose: there is information in the sequence itself and recurrent networks use it to perform the tasks that feed-forward networks cannot.</p>
<p class="calibre2">RNN is a class of neural network where there are connections between neurons that form a directed cycle. A typical RNN is represented in the following figure:</p>
<div class="cdpaligncenter"><img class="alignnone11" src="../images/00120.gif"/></div>
<p class="calibre2">Here, the output of one instance is taken as input for the next instance for the same neuron. The way the data is kept in memory and flows at different time periods makes RNNs powerful and successful.</p>
<p class="calibre2">Under RNNs, there are more variants in the way the data flows backwards:</p>
<ul class="calibre16">
<li class="calibre17">Fully recurrent</li>
<li class="calibre17">Recursive</li>
<li class="calibre17">Hopfield</li>
<li class="calibre17">Elman and Jordan networks</li>
<li class="calibre17">Neural history compressor</li>
<li class="calibre17">LSTM</li>
<li class="calibre17"><strong class="calibre1">Gated Recurrent Unit</strong> (<strong class="calibre1">GRU</strong>)</li>
<li class="calibre17">Bidirectional</li>
<li class="calibre17">Recurrent MLP</li>
</ul>
<p class="calibre2">Recurrent networks are designed to recognize patterns as a sequence of data and are helpful in prediction and forecasting. They can work on text, images, speech, and time series data. RNNs are among the powerful ANNs and represent the biological brain, including memory with processing power. Recurrent networks take inputs from the current input (like a feed-forward network) and the output that was calculated previously:</p>
<div class="cdpaligncenter"><img class="alignnone12" src="../images/00121.gif"/></div>
<p class="calibre2">To understand this better, we consider the RNN as a network of neural networks, and the cyclic nature is <strong class="calibre1">unfolded</strong> in the following manner. The state of a neuron <em class="calibre14">h</em> is considered at different time periods (<em class="calibre14">-t-1</em>, <em class="calibre14">t</em>, <em class="calibre14">t+1</em> and so on) until convergence or the total number of epochs is reached.</p>
<p class="calibre2"><span>Vanilla is the first model of recurrent ANNs that was introduced.</span> A vanilla RNN is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="alignnone13" src="../images/00122.gif"/></div>
<p class="calibre2"><span>Other variants such as GRU or LSTM networks are more widespread given the simplicity of implementation, and they have demonstrated remarkable performance in a wide range of applications involving sequences such as language modeling, speech recognition, image captioning, and automatic translation.</span></p>
<p class="calibre2"><span>RNNs can be implemented in R through the following packages:</span></p>
<ul class="calibre16">
<li class="calibre17"><span><kbd class="calibre13">rnn</kbd></span></li>
<li class="calibre17"><kbd class="calibre13">MxNetR</kbd></li>
<li class="calibre17"><kbd class="calibre13">TensorFlow</kbd> for R</li>
</ul>
<div class="title-page-name">
<p class="calibre2">RNNs are mainly used for sequence modeling. The inputs and outputs are treated as vectors (a matrix of numbers). For another level of understanding of RNNs, I advise you to go through the character sequencing example by Andrej Karpathy.</p>
<p class="calibre2">The features of RNN make it like an ANN with memory. The ANN memory is more like the human brain. With memory, we can make machines think from scratch and learn from their "memory." RNNs are basically ANNs with loops that allow information to persist in the network. The looping allows information to be passed from state t to state <em class="calibre14">t+1</em>.</p>
<p class="calibre2">As seen in the preceding diagram, RNNs can be thought of as multiple copies of the same ANN, with the output of one passing on as input to the next one. When we persist the information, as the patterns change, RNN is able to predict the <em class="calibre14">t+1</em> value. This is particularly useful for analyzing time-series-based problems.</p>
<p class="calibre2">There is no specific labeling required; the value that is part of the input forms the time series variable, and RNN can learn the pattern and do the prediction.</p>
<p class="calibre2">The internal state of the RNN is updated for every time step of the learning process. The feed-forward mechanism in RNN is similar to ANN; however, the backpropagation is an error term correction following something called <strong class="calibre1">Backpropagation Through Time</strong> (<strong class="calibre1">BPTT</strong>).</p>
<p class="calibre2">Backpropagation through time follows this pseudocode:</p>
<ol class="calibre19">
<li value="1" class="calibre17">Unfold the RNN to contain <em class="calibre14">n</em> feed-forward networks.</li>
<li value="2" class="calibre17">Initialize the weights <em class="calibre14">w</em> to random values.</li>
<li value="3" class="calibre17">Perform the following until the stopping criteria is met or you are done with the required number of epochs.</li>
<li value="4" class="calibre17">Set inputs to each network with values as <em class="calibre14">x<sub class="calibre25">i.</sub></em></li>
<li value="5" class="calibre17">Forward-propagate the inputs over the whole unfolded network.</li>
<li value="6" class="calibre17">Back-propagate the error over the unfolded network.</li>
<li value="7" class="calibre17">Update all the weights in the network.</li>
<li value="8" class="calibre17">Average out the weights to find the final weight in the folded network.</li>
</ol>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The rnn package in R</h1>
                
            
            <article>
                
<p class="calibre2">To implement RNN <span>in an R environment, we can use t</span>he <kbd class="calibre13">rnn</kbd> package available through CRAN. This package is widely used to implement an RNN. <span>A brief description of the <kbd class="calibre13">rnn</kbd> package, extracted from the official documentation, is shown in the following table:</span></p>
<table class="calibre80">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">rnn</strong>: Recurrent Neural Network</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Description</strong>:</td>
</tr>
<tr class="calibre7">
<td class="calibre8">Implementation of an RNN in R</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Details</strong>:</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><span>Package: <kbd class="calibre13">rnn</kbd></span><br class="title-page-name"/>
<span>Type: Package</span><br class="title-page-name"/>
<span>Version: 0.8.0</span><br class="title-page-name"/>
<span>Date: 2016-09-11</span><br class="title-page-name"/>
<span>License: GPL-3</span></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Authors</strong>:</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2">Bastiaan Quast<br class="title-page-name"/>
Dimitri Fichou</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"><span>The main functions used from the</span> <kbd class="calibre13">rnn</kbd> <span>package are shown in this table:</span></p>
<table class="calibre80">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">predict_rnn</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Predicts the output of an RNN model:</p>
<p class="calibre2"><kbd class="calibre13">predict_rnn(model, X, hidden = FALSE, real_output = T, ...)</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">run.rnn_demo</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><span>A function to launch the <kbd class="calibre13">rnn_demo</kbd> app:</span></p>
<p class="calibre2"><kbd class="calibre13">run.rnn_demo(port = NULL)</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">trainr</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">This trains the RNN. The model is used by the <kbd class="calibre13">predictr</kbd> function.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">predictr</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">This predicts the output of an RNN model:</p>
<p class="calibre2"><kbd class="calibre13">predictr(model, X, hidden = FALSE, real_output = T, ...)</kbd></p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2"/>
<p class="calibre2">As always, to be able to use a library, we must first install and then load it into our script.</p>
<div class="packt_tip">Remember, to install a library that is not present in the initial distribution of R, you must use the <kbd class="calibre61">install.package</kbd> function. <span class="calibre62">This is the main function to install packages. It takes a vector of names and a destination library, downloads the packages from the repositories and installs them.</span> This function should be used only once and not every time you run the code.</div>
<p class="calibre2">So let's install and load the library:</p>
<pre class="calibre24"><strong class="calibre1">install.packages("rnn")</strong><br class="title-page-name"/><strong class="calibre1">library("rnn")</strong></pre>
<p class="calibre2">When we load the library (<kbd class="calibre13">library("rnn")</kbd>), we may receive the following error:</p>
<pre class="calibre24"><strong class="calibre1">&gt; library("rnn")</strong><br class="title-page-name"/><strong class="calibre1">Error: package or namespace load failed for ‘rnn’ in get(Info[i, 1], envir = env):</strong><br class="title-page-name"/><strong class="calibre1"> cannot open file 'C:/Users/Giuseppe/Documents/R/win-library/3.4/digest/R/digest.rdb': No such file or directory</strong></pre>
<p class="calibre2">Do not worry, as it's nothing serious! R is just saying that, in order to run the <kbd class="calibre13">rnn</kbd> library, you also need to install the <kbd class="calibre13">digest</kbd> library. Remember it; in future, if such a problem happens, you now know how to solve it. Just add the following command:</p>
<pre class="calibre24"><strong class="calibre1">install.packages("digest")</strong></pre>
<p class="calibre2">Now we can launch the demo:</p>
<pre class="calibre24"><strong class="calibre1">run.rnn_demo()</strong></pre>
<p class="calibre2"><span>When we run <kbd class="calibre13">run.rnn_demo()</kbd> after installing</span> <span>the</span> <kbd class="calibre13">rnn</kbd> <span>package, we can access a web page through <kbd class="calibre13">127.0.0.1:5876</kbd>, which allows us to run a demo of an RNN with preset values and also visually see how the parameters influence an RNN, as shown in the following figure:</span></p>
<div class="cdpaligncenter"><img src="../images/00123.jpeg" class="calibre81"/></div>
<p class="calibre2">At this point, we will be able to set the parameters of our network and choose the appropriate values to be inserted into the boxes via its labels. The following parameters must be set correctly:</p>
<ul class="calibre16">
<li class="calibre17"><kbd class="calibre13">time dimension</kbd></li>
<li class="calibre17"><kbd class="calibre13">training sample dimension</kbd></li>
<li class="calibre17"><kbd class="calibre13">testing sample dimension</kbd></li>
<li class="calibre17"><kbd class="calibre13">number of hidden layers</kbd></li>
<li class="calibre17"><kbd class="calibre13">Number of unit in the layer number 1</kbd></li>
<li class="calibre17"><kbd class="calibre13">Number of unit in the layer number 2</kbd></li>
<li class="calibre17"><kbd class="calibre13">learningrate</kbd></li>
<li class="calibre17"><kbd class="calibre13">batchsize</kbd></li>
<li class="calibre17"><kbd class="calibre13">numepochs</kbd></li>
<li class="calibre17"><kbd class="calibre13">momentum</kbd></li>
<li class="calibre17"><kbd class="calibre13">learningrate_decay</kbd></li>
</ul>
<p class="calibre2">After doing this, we <span>just</span> <span>have to click on the train button and the command will be built and trained.</span></p>
<p class="calibre2">The f<span>ollowing figure</span> <span>shows the results of the simulation:</span></p>
<p class="calibre2"/>
<div class="cdpaligncenter"><img class="alignnone14" src="../images/00124.jpeg"/></div>
<p class="calibre2">The <kbd class="calibre13">trainr</kbd> and <kbd class="calibre13">predictr</kbd> functions are the most important functions in the <kbd class="calibre13">rnn</kbd> package. The <kbd class="calibre13">trainr()</kbd> function trains a model with the set of <kbd class="calibre13">X</kbd> and <kbd class="calibre13">Y</kbd> parameters, which can be used for prediction using the <kbd class="calibre13">predictr()</kbd> function:</p>
<pre class="calibre24"><strong class="calibre1">trainr(Y, X, </strong><br class="title-page-name"/><strong class="calibre1">      learningrate, </strong><br class="title-page-name"/><strong class="calibre1">      learningrate_decay = 1, </strong><br class="title-page-name"/><strong class="calibre1">      momentum = 0, </strong><br class="title-page-name"/><strong class="calibre1">      hidden_dim = c(10), </strong><br class="title-page-name"/><strong class="calibre1">      network_type = "rnn", </strong><br class="title-page-name"/><strong class="calibre1">      numepochs = 1, </strong><br class="title-page-name"/><strong class="calibre1">      sigmoid = c("logistic", "Gompertz", "tanh"), </strong><br class="title-page-name"/><strong class="calibre1">      use_bias = F, </strong><br class="title-page-name"/><strong class="calibre1">      batch_size = 1, </strong><br class="title-page-name"/><strong class="calibre1">      seq_to_seq_unsync = F, </strong><br class="title-page-name"/><strong class="calibre1">      update_rule = "sgd", </strong><br class="title-page-name"/><strong class="calibre1">      epoch_function = c(epoch_print, epoch_annealing), </strong><br class="title-page-name"/><strong class="calibre1">      loss_function = loss_L1, ...) </strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">predictr(model, </strong><br class="title-page-name"/><strong class="calibre1">      X, </strong><br class="title-page-name"/><strong class="calibre1">      hidden = FALSE, </strong><br class="title-page-name"/><strong class="calibre1">      real_output = T, </strong><br class="title-page-name"/><strong class="calibre1">      arguments to pass to sigmoid function)</strong></pre>
<p class="calibre2"><span>The <kbd class="calibre13">trainr()</kbd> function</span> takes the following parameters. The output is a model that can be used for prediction:</p>
<table class="calibre40">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">Y</kbd></td>
<td class="calibre8">
<p class="calibre2">Array of output values:</p>
<ul class="calibre16">
<li class="calibre17"><kbd class="calibre13">dim 1</kbd>: Samples (must be equal to dim 1 of <kbd class="calibre13">X</kbd>)</li>
<li class="calibre17"><kbd class="calibre13">dim 2</kbd>: Time (must be equal to dim 2 of <kbd class="calibre13">X</kbd>)</li>
<li class="calibre17"><kbd class="calibre13">dim 3</kbd>: Variables (could be one or more, if a matrix, will be coerced to an array)</li>
</ul>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">X</kbd></td>
<td class="calibre8">
<p class="calibre2">Array of input values:</p>
<ul class="calibre16">
<li class="calibre17"><kbd class="calibre13">dim 1</kbd>: Samples</li>
<li class="calibre17"><kbd class="calibre13">dim 2</kbd>: Time</li>
<li class="calibre17"><kbd class="calibre13">dim 3</kbd>: Variables (could be one or more; if it is a matrix, will be coerced to an array)</li>
</ul>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">learningrate</kbd></td>
<td class="calibre8">Learning rate to be applied for weight iteration.</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">learningrate_decay</kbd></td>
<td class="calibre8">Coefficient to apply to the learning rate at each epoch via the <kbd class="calibre13">epoch_annealing</kbd> function.</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">momemtum</kbd></td>
<td class="calibre8">The coefficient of the last weight iteration to keep for faster learning.</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">hidden_dim</kbd></td>
<td class="calibre8">The dimensions of the hidden layers.</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">network_type</kbd></td>
<td class="calibre8">The type of network, which could be <kbd class="calibre13">rnn</kbd>, <kbd class="calibre13">gru</kbd> or <kbd class="calibre13">lstm</kbd>.</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">numepochs</kbd></td>
<td class="calibre8">The number of iterations, that is, the number of times the whole dataset is presented to the network</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13"><span>sigmoid</span></kbd></td>
<td class="calibre8">Method to be passed to the <kbd class="calibre13">sigmoid</kbd> function.</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13"><span>batch size</span></kbd></td>
<td class="calibre8">Number of samples used at each weight iteration. Only one is supported for the moment.</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13"><span>epoch_function</span></kbd></td>
<td class="calibre8">Vector of functions to be applied at each epoch loop. Use it to interact with the objects inside the list model or to print and plot at each epoch. It should return the model.</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13"><span>loss function</span></kbd></td>
<td class="calibre8">Applied in each sample loop, vocabulary to verify.</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">...</kbd></td>
<td class="calibre8">
<p class="calibre2">Arguments to be passed to methods, to be used in user defined functions.</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2">Now let's look at a simple example. This example included is in the official documentation of the CRAN <kbd class="calibre13">rnn</kbd> package to demonstrate the <kbd class="calibre13">trainr</kbd> and <kbd class="calibre13">predictr</kbd> functions and see the accuracy of the predictions.</p>
<p class="calibre2">We have <kbd class="calibre13">X1</kbd> and <kbd class="calibre13">X</kbd> with random numbers in the range <em class="calibre14">0-127</em>. <kbd class="calibre13">Y</kbd> is initialized as <kbd class="calibre13">X1+X2</kbd>. After converting <kbd class="calibre13">X1</kbd>, <kbd class="calibre13">X2</kbd>, and <kbd class="calibre13">Y</kbd> to binary values, we use <kbd class="calibre13">trainr</kbd> to train <kbd class="calibre13">Y</kbd> based on <kbd class="calibre13">X(array of X1 and X2)</kbd>.</p>
<p class="calibre2">Using the model, we predict <kbd class="calibre13">B</kbd> based on another sample of <kbd class="calibre13">A1+A2</kbd>. The difference of errors is plotted as a histogram:</p>
<pre class="calibre24"><strong class="calibre1">library("rnn")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">#Create a set of random numbers in X1 and X2</strong><br class="title-page-name"/><strong class="calibre1">X1=sample(0:127, 7000, replace=TRUE)</strong><br class="title-page-name"/><strong class="calibre1">X2=sample(0:127, 7000, replace=TRUE)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">#Create training response numbers</strong><br class="title-page-name"/><strong class="calibre1">Y=X1 + X2</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1"># Convert to binary</strong><br class="title-page-name"/><strong class="calibre1">X1=int2bin(X1)</strong><br class="title-page-name"/><strong class="calibre1">X2=int2bin(X2)</strong><br class="title-page-name"/><strong class="calibre1">Y=int2bin(Y)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1"># Create 3d array: dim 1: samples; dim 2: time; dim 3: variables.</strong><br class="title-page-name"/><strong class="calibre1">X=array( c(X1,X2), dim=c(dim(X1),2) )</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1"># Train the model</strong><br class="title-page-name"/><strong class="calibre1">model &lt;- trainr(Y=Y[,dim(Y)[2]:1],</strong><br class="title-page-name"/><strong class="calibre1">                X=X[,dim(X)[2]:1,],</strong><br class="title-page-name"/><strong class="calibre1">                learningrate = 0.1,</strong><br class="title-page-name"/><strong class="calibre1">                hidden_dim = 10,</strong><br class="title-page-name"/><strong class="calibre1">                batch_size = 100,</strong><br class="title-page-name"/><strong class="calibre1">                numepochs = 100)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">plot(colMeans(model$error),type='l',xlab='epoch',ylab='errors')</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1"># Create test inputs</strong><br class="title-page-name"/><strong class="calibre1">A1=int2bin(sample(0:127, 7000, replace=TRUE))</strong><br class="title-page-name"/><strong class="calibre1">A2=int2bin(sample(0:127, 7000, replace=TRUE))</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1"># Create 3d array: dim 1: samples; dim 2: time; dim 3: variables</strong><br class="title-page-name"/><strong class="calibre1">A=array( c(A1,A2), dim=c(dim(A1),2) )</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1"># Now, let us run prediction for new A</strong><br class="title-page-name"/><strong class="calibre1">B=predictr(model,</strong><br class="title-page-name"/><strong class="calibre1"> A[,dim(A)[2]:1,] )</strong><br class="title-page-name"/><strong class="calibre1">B=B[,dim(B)[2]:1]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1"># Convert back to integers</strong><br class="title-page-name"/><strong class="calibre1">A1=bin2int(A1)</strong><br class="title-page-name"/><strong class="calibre1">A2=bin2int(A2)</strong><br class="title-page-name"/><strong class="calibre1">B=bin2int(B)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1"># Plot the differences as histogram</strong><br class="title-page-name"/><strong class="calibre1">hist( B-(A1+A2)</strong> )</pre>
<p class="calibre2"><span>As usual, we will analyze the code line by line, explaining in detail all the features applied to capture the results:</span></p>
<pre class="calibre24"><strong class="calibre1">library("rnn")</strong></pre>
<p class="calibre2"><span>The first line of the initial code are used to load the library needed to run the analysis. Let's go to the following commands:</span></p>
<pre class="calibre24"><strong class="calibre1">X1=sample(0:127, 7000, replace=TRUE)</strong><br class="title-page-name"/><strong class="calibre1">X2=sample(0:127, 7000, replace=TRUE)</strong></pre>
<p class="calibre2">These lines create training response numbers; these two vectors will be the inputs of the network we are about to build. We have used the <kbd class="calibre13">sample()</kbd> function to take a sample of the specified size from the elements of <kbd class="calibre13">x</kbd> either with or without replacement. The two vectors contain 7,000 random integer values between <kbd class="calibre13">1</kbd> and <kbd class="calibre13">127</kbd>.</p>
<pre class="calibre24"><strong class="calibre1">Y = X1 + X2</strong></pre>
<p class="calibre2">This command creates training response numbers; this is our target, or what we want to predict with the help of the network.</p>
<pre class="calibre24"><strong class="calibre1">X1=int2bin(X1)</strong><br class="title-page-name"/><strong class="calibre1">X2=int2bin(X2)</strong><br class="title-page-name"/><strong class="calibre1">Y=int2bin(Y)</strong></pre>
<p class="calibre2">These three lines of code convert integers into binary sequences. We need to transform numbers into binaries before adding bit by bit. In the end, we get a sequence of eight values for each value, these values being <kbd class="calibre13">0</kbd> or <kbd class="calibre13">1</kbd>. To understand the transformation we analyze a preview of one of these variables:</p>
<pre class="calibre24"><strong class="calibre1">&gt; head(X1,n=10)</strong><br class="title-page-name"/><strong class="calibre1">      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]</strong><br class="title-page-name"/><strong class="calibre1"> [1,]    1    1    1    0    0    1    0    0</strong><br class="title-page-name"/><strong class="calibre1"> [2,]    0    0    0    1    0    0    0    0</strong><br class="title-page-name"/><strong class="calibre1"> [3,]    1    0    0    0    1    0    1    0</strong><br class="title-page-name"/><strong class="calibre1"> [4,]    0    0    0    0    0    0    1    0</strong><br class="title-page-name"/><strong class="calibre1"> [5,]    0    1    0    0    0    0    0    0</strong><br class="title-page-name"/><strong class="calibre1"> [6,]    0    0    0    1    1    1    0    0</strong><br class="title-page-name"/><strong class="calibre1"> [7,]    1    0    1    1    0    1    1    0</strong><br class="title-page-name"/><strong class="calibre1"> [8,]    1    1    0    0    0    1    0    0</strong><br class="title-page-name"/><strong class="calibre1"> [9,]    1    0    1    0    0    0    0    0</strong><br class="title-page-name"/><strong class="calibre1">[10,]    0    0    0    1    0    0    0    0</strong></pre>
<p class="calibre2">Let's go back to analyze the code:</p>
<pre class="calibre24"><strong class="calibre1">X=array( c(X1,X2), dim=c(dim(X1),2) )</strong></pre>
<p class="calibre2">This code creates a 3D array as required by the <kbd class="calibre13">trainr()</kbd> function. In this array, we have the following:</p>
<ul class="calibre16">
<li class="calibre17"><kbd class="calibre13">dim 1</kbd>: Samples (must be equal to <kbd class="calibre13">dim 1</kbd> of inputs)</li>
<li class="calibre17"><kbd class="calibre13">dim 2</kbd>: Time (must be equal to <kbd class="calibre13">dim 2</kbd> of <span>inputs</span>)</li>
<li class="calibre17"><kbd class="calibre13">dim 3</kbd>: Variables (could be one or more; if it is a matrix, this will be coerced to the array)</li>
</ul>
<pre class="calibre24"><strong class="calibre1">model &lt;- trainr(Y=Y[,dim(Y)[2]:1],</strong><br class="title-page-name"/><strong class="calibre1">                X=X[,dim(X)[2]:1,],</strong><br class="title-page-name"/><strong class="calibre1">                learningrate = 0.1,</strong><br class="title-page-name"/><strong class="calibre1">                hidden_dim = 10,</strong><br class="title-page-name"/><strong class="calibre1">                batch_size = 100,</strong><br class="title-page-name"/><strong class="calibre1">                numepochs = 100)</strong></pre>
<p class="calibre2">The <kbd class="calibre13">trainr()</kbd> function trains an RNN in native R. It takes a few minutes as the training happens based on <kbd class="calibre13">X</kbd> and <kbd class="calibre13">Y</kbd>. The following code shows the last 10 trained epoch results displayed on the R prompt:</p>
<pre class="calibre24"><strong class="calibre1">Trained epoch: 90 - Learning rate: 0.1</strong><br class="title-page-name"/><strong class="calibre1">Epoch error: 3.42915263914405</strong><br class="title-page-name"/><strong class="calibre1">Trained epoch: 91 - Learning rate: 0.1</strong><br class="title-page-name"/><strong class="calibre1">Epoch error: 3.44100549476955</strong><br class="title-page-name"/><strong class="calibre1">Trained epoch: 92 - Learning rate: 0.1</strong><br class="title-page-name"/><strong class="calibre1">Epoch error: 3.43627697030863</strong><br class="title-page-name"/><strong class="calibre1">Trained epoch: 93 - Learning rate: 0.1</strong><br class="title-page-name"/><strong class="calibre1">Epoch error: 3.43541472188254</strong><br class="title-page-name"/><strong class="calibre1">Trained epoch: 94 - Learning rate: 0.1</strong><br class="title-page-name"/><strong class="calibre1">Epoch error: 3.43753094787383</strong><br class="title-page-name"/><strong class="calibre1">Trained epoch: 95 - Learning rate: 0.1</strong><br class="title-page-name"/><strong class="calibre1">Epoch error: 3.43622412149714</strong><br class="title-page-name"/><strong class="calibre1">Trained epoch: 96 - Learning rate: 0.1</strong><br class="title-page-name"/><strong class="calibre1">Epoch error: 3.43604894997742</strong><br class="title-page-name"/><strong class="calibre1">Trained epoch: 97 - Learning rate: 0.1</strong><br class="title-page-name"/><strong class="calibre1">Epoch error: 3.4407798878595</strong><br class="title-page-name"/><strong class="calibre1">Trained epoch: 98 - Learning rate: 0.1</strong><br class="title-page-name"/><strong class="calibre1">Epoch error: 3.4472752590403</strong><br class="title-page-name"/><strong class="calibre1">Trained epoch: 99 - Learning rate: 0.1</strong><br class="title-page-name"/><strong class="calibre1">Epoch error: 3.43720125450988</strong><br class="title-page-name"/><strong class="calibre1">Trained epoch: 100 - Learning rate: 0.1</strong><br class="title-page-name"/><strong class="calibre1">Epoch error: 3.43542353819336</strong></pre>
<p class="calibre2">We can see the evolution of the algorithm by charting the error made by the algorithm to subsequent epochs:</p>
<pre class="calibre24"><strong class="calibre1">plot(colMeans(model$error),type='l',xlab='epoch',ylab='errors')</strong></pre>
<p class="calibre2">This graph shows the epoch versus error:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00125.gif"/></div>
<p class="calibre2">Now the model is ready and we can use it to test the network. But first, we need to create some test data:</p>
<pre class="calibre24"><strong class="calibre1">A1=int2bin(sample(0:127, 7000, replace=TRUE))</strong><br class="title-page-name"/><strong class="calibre1">A2=int2bin(sample(0:127, 7000, replace=TRUE))</strong><br class="title-page-name"/><strong class="calibre1">A=array( c(A1,A2), dim=c(dim(A1),2) )</strong></pre>
<p class="calibre2">Now, let us run the prediction for new data:</p>
<pre class="calibre24"><strong class="calibre1">B=predictr(model, A[,dim(A)[2]:1,] ) </strong><br class="title-page-name"/><strong class="calibre1">B=B[,dim(B)[2]:1]</strong></pre>
<p class="calibre2">Convert back to integers:</p>
<pre class="calibre24"><strong class="calibre1">A1=bin2int(A1)</strong><br class="title-page-name"/><strong class="calibre1">A2=bin2int(A2)</strong><br class="title-page-name"/><strong class="calibre1">B=bin2int(B)</strong></pre>
<p class="calibre2">Finally, plot the differences as a histogram:</p>
<pre class="calibre24"><strong class="calibre1">hist( B-(A1+A2) )</strong></pre>
<p class="calibre2">The histogram of errors is shown as follows:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00126.gif"/></div>
<p class="calibre2">As can be seen here, the bin with more frequent is near zero to indicate that in most cases, the predictions coincide with the current values. All the other bins are related to the errors. We can therefore say that the network simulates the system with good performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">LSTM model</h1>
                
            
            <article>
                
<p class="calibre2">We have seen that RNNs have a memory that uses persistent previous information to be used in the current neural network processing. The previous information is used in the present task. However, the memory is short-term and we do not have a list of all of the previous information available for the neural node.</p>
<p class="calibre2">When we introduce a long-term memory into the RNN, we are able to remember a lot of previous information and use it for the current processing. This concept is called LSTM model of RNN, which has numerous use cases in video, audio, text prediction, and various other applications.</p>
<p class="calibre2">LSTMs were introduced by Hochreiter &amp; Schmidhuber in 1997.</p>
<p class="calibre2">The LSTM network is trained using <strong class="calibre1">BPTT</strong> and diminishes the vanishing gradient problem. LSTMs have powerful applications in time series predictions and can create large, recurrent networks to address difficult sequence problems in machine learning.</p>
<p class="calibre2">LSTM have <strong class="calibre1">gates</strong> that make the long/short term memory possible. These are contained in memory blocks connected through layers:</p>
<div class="cdpaligncenter"><img class="alignnone15" src="../images/00127.gif"/></div>
<p class="calibre2">There are three types of gates within a unit:</p>
<ul class="calibre16">
<li class="calibre17"><strong class="calibre1">Input Gate:</strong> Scales input to cell (write)</li>
<li class="calibre17"><strong class="calibre1">Output Gate</strong>: Scales output to cell (read)</li>
<li class="calibre17"><strong class="calibre1">Forget Gate</strong>: Scales old cell value (reset)</li>
</ul>
<p class="calibre2">Each gate is like a switch that controls the read/write, thus incorporating the long-term memory function into the LSTM model.</p>
<p class="calibre2">LSTMs can be used to solve the following sequence prediction problems:</p>
<ul class="calibre16">
<li class="calibre17">Direct sequence prediction</li>
<li class="calibre17">Sequence classification</li>
<li class="calibre17">Sequence generation</li>
<li class="calibre17">Sequence to sequence prediction</li>
</ul>
<p class="calibre2">The key differences between GRU and LSTM are:</p>
<ul class="calibre16">
<li class="calibre17">A GRU has two gates, whereas an LSTM has three gates.</li>
<li class="calibre17">GRUs don't possess any internal memory that is different from the exposed hidden state. They don't have the output gate, which is present in LSTMs.</li>
<li class="calibre17">There is no second nonlinearity applied when computing the output in GRU.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Convolutional Neural Networks</h1>
                
            
            <article>
                
<p class="calibre2"><span>Another important set of neural networks in deep learning is CNN. They are designed specifically for image recognition and classification. CNNs have multiple layers of neural networks that extract information from images and determine the class they fall into.</span></p>
<p class="calibre2"><span>For example, a CNN can detect whether the image is a cat or not if it is trained with a set of images of cats. We will see the architecture and working of CNN in this section.</span></p>
<p class="calibre2">For a program, any image is a just a set of RGB numbers in a vector format. If we can make a neural network understand the pattern, it can form a CNN and detect images.</p>
<p class="calibre2"><span>Regular neural nets are universal mathematical approximators that take an input, transform it through a series of functions, and derive the output. However, these regular neural networks do not scale well for an image analysis. For a 32 x 32 pixel RGB image, the hidden layer would have <em class="calibre14">32*32*3=3072</em> weights. The regular neural nets work fine for this case. However, when the RGB image is scaled to size <em class="calibre14">200 x 200</em> pixel, the number of weights required in the hidden layer is <em class="calibre14">200*200*3=120,000</em> and the network does not perform well.</span></p>
<p class="calibre2"><span>Enter CNN to solve this scalability problem. In CNN, the layers of a CNN have neurons arranged in three dimensions (<strong class="calibre1">height</strong>, <strong class="calibre1">width</strong>, and <strong class="calibre1">depth</strong>).</span></p>
<p class="calibre2">The following diagram shows a neural net and a CNN:</p>
<div class="cdpaligncenter"><img class="alignnone16" src="../images/00128.gif"/></div>
<p class="calibre2"><span>CNN is a sequence of layers of neural nets, wherein each layer transforms one volume of activations to another through a differentiable function.</span> There are three types of layers that build the CNN:</p>
<ul class="calibre16">
<li class="calibre17">Convolutional layer</li>
<li class="calibre17">Pooling layer</li>
<li class="calibre17">Fully connected layer</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Step #1 – filtering</h1>
                
            
            <article>
                
<p class="calibre2">The convolutional layer does the heavy math operations. In computer vision, a typical approach to processing an image is to convolute it with a filter to extract only the salient features in it. This is the first operation in a CNN. The input image is applied a filter logic to create an <strong class="calibre1">activation map</strong> or <strong class="calibre1">feature map</strong>:</p>
<div class="cdpaligncenter"><img class="alignnone17" src="../images/00129.gif"/></div>
<p class="calibre2">The convoluted feature vector is created by applying the kernel vector on each 3 x 3 vector of the image.</p>
<p class="calibre2">The mathematical steps for filtering are as follows:</p>
<ol class="calibre19">
<li value="1" class="calibre17">Line up the feature and the image patch.</li>
<li value="2" class="calibre17">Multiply each image pixel by the corresponding feature pixel.</li>
<li value="3" class="calibre17">Add them up.</li>
<li value="4" class="calibre17">Divide each sum by the total number of pixels in the feature.</li>
</ol>
<p class="calibre2">Once the filtering is done, the next step is to compress the filtered pixels.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Step #2 – pooling</h1>
                
            
            <article>
                
<p class="calibre2">In this step, we shrink the image stack. For each feature obtained in the convolutional step, we build up a matrix and now find the maximum in each chosen matrix to shrink the entire input. The steps are below:</p>
<ol class="calibre19">
<li value="1" class="calibre17">Pick a window size (usually 2 or 3).</li>
<li value="2" class="calibre17">Pick a stride moving range of pixels (usually 2).</li>
<li value="3" class="calibre17">Slide the window across the filtered images.</li>
<li value="4" class="calibre17">For each window, we take the maximum value.</li>
</ol>
<p class="calibre2">If the slid window does not have the required number of cells as in the previous windows, we take whatever values are available.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Step #3 – ReLU for normalization</h1>
                
            
            <article>
                
<p class="calibre2">In this step, we take the pooling output and for each pixel and apply the ReLU normalization to tweak the values. If any of the values is negative, we make it zero.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Step #4 – voting and classification in the fully connected layer</h1>
                
            
            <article>
                
<p class="calibre2">The final layer is the fully connected layer and there is voting by the set of values to determine the class of the output. The fully connected layer is just a merged matrix of all the previous outputs.</p>
<p class="calibre2">This is the final layer and the output is determined based on the highest voted category.</p>
<p class="calibre2">By stacking up the layers in steps 1, 2, and 3, we form the convolution network, which can reduce the error term <span>with backpropagation</span> <span>to give us the best prediction.</span></p>
<p class="calibre2">The layers can be repeated multiple times and each layer output forms an input to the next layer.</p>
<p class="calibre2">A classical CNN architecture would look like this:</p>
<div class="cdpaligncenter"><img class="alignnone18" src="../images/00130.jpeg"/></div>
<p class="calibre2">An example classification prediction using CNN is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="alignnone19" src="../images/00131.jpeg"/></div>
<p class="calibre2">We will see an implementation of CNN using R in <a href="part0123.html#3L9L60-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 7</a>, <em class="calibre14">Use Cases of Neural Networks – Advanced Topics</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Common CNN architecture - LeNet</h1>
                
            
            <article>
                
<p class="calibre2"><span>LeNet-5 is a convolutional network designed by Le Cun in the 1990s for handwritten and machine-printed character recognition.</span><br class="title-page-name"/>
<span>It is the first successful application of convolutional networks. It has the following architecture:</span></p>
<div class="cdpaligncenter"><img class="alignnone20" src="../images/00132.jpeg"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Humidity forecast using RNN</h1>
                
            
            <article>
                
<p class="calibre2">As the first use case of RNNs, we see how we can train and predict an RNN using the <kbd class="calibre13">trainr()</kbd> function<em class="calibre14">.</em> Our purpose is to forecast the humidity of a certain location as a function of the day. The input file contains daily weather observations from multiple Australian weather stations. These observations are obtained from the Australian Commonwealth Bureau of Meteorology and are subsequently processed to create a relatively large sample dataset for illustrating analytics, data mining, and data science using R and the rattle.data package. The <kbd class="calibre13">weatherAUS</kbd> dataset is regularly updated and updates of this package usually correspond to updates to this dataset. The data is updated from the Bureau of Meteorology website. The <kbd class="calibre13">locationsAUS</kbd> dataset records the location of each weather station. The source dataset is copyrighted by the Australian Commonwealth Bureau of Meteorology and is used with permission.</p>
<div class="packt_infobox">A CSV version of this dataset is available at the following link:<br class="calibre82"/>
<a href="https://rattle.togaware.com/weatherAUS.csv" class="calibre68"><span class="calibre62">https://rattle.togaware.com/weatherAUS.csv</span></a></div>
<p class="calibre2">The <kbd class="calibre13">weatherAUS</kbd> dataset is a dataframe containing over 140,000 daily observations from over 45 Australian weather stations. This dataset contains the following variables:</p>
<ul class="calibre16">
<li class="calibre17"><kbd class="calibre13">Date</kbd>: The date of observation (a <kbd class="calibre13">Date</kbd> object).</li>
<li class="calibre17"><kbd class="calibre13">Location</kbd>: The common name of the location of the weather station.</li>
<li class="calibre17"><kbd class="calibre13">MinTemp</kbd>: The minimum temperature in degrees celsius.</li>
<li class="calibre17"><kbd class="calibre13">MaxTemp</kbd>: The maximum temperature in degrees celsius.</li>
<li class="calibre17"><kbd class="calibre13">Rainfall</kbd>: The amount of rainfall recorded for the day in mm.</li>
<li class="calibre17"><kbd class="calibre13">Evaporation</kbd>: The so-called class a pan evaporation (mm) in the 24 hours to 9 a.m.</li>
<li class="calibre17"><kbd class="calibre13">Sunshine</kbd>: The number of hours of bright sunshine in the day.</li>
<li class="calibre17"><kbd class="calibre13">WindGustDir</kbd>: The direction of the strongest wind gust in the 24 hours to midnight.</li>
<li class="calibre17"><span><kbd class="calibre13">WindGustSpeed</kbd>: The speed (km/h) of the strongest wind gust in the 24 hours to midnight.</span></li>
<li class="calibre17"><span><kbd class="calibre13">Temp9am</kbd>: Temperature (degrees C) at 9 a.m.</span></li>
<li class="calibre17"><span><kbd class="calibre13">RelHumid9am</kbd>: Relative humidity (percent) at 9 a.m.</span></li>
<li class="calibre17"><span><kbd class="calibre13">Cloud9am</kbd>: Fraction of the sky obscured by clouds at 9 a.m. This is measured in oktas, which are a unit of eighths. It records how many eighths of the sky are obscured by cloud. A zero measure indicates completely clear sky whilst an 8 indicates that it is completely overcast.</span></li>
<li class="calibre17"><span><kbd class="calibre13">WindSpeed9am</kbd>: Wind speed (km/hr) averaged over 10 minutes prior to 9 a.m. 6 <kbd class="calibre13">weatherAUS</kbd></span>.</li>
<li class="calibre17"><span><kbd class="calibre13">Pressure9am</kbd></span>: <span>Atmospheric pressure (hpa) reduced to mean sea level at 9 a.m.</span></li>
<li class="calibre17"><span><kbd class="calibre13">Temp3pm</kbd>: Temperature (degrees C) at 3 p.m.</span></li>
<li class="calibre17"><span><kbd class="calibre13">RelHumid3pm</kbd>: Relative humidity (percent) at 3 p.m.</span></li>
<li class="calibre17"><span><kbd class="calibre13">Cloud3pm</kbd>: Fraction of sky obscured by cloud (in oktas: eighths) at 3 p.m.</span></li>
<li class="calibre17"><span><kbd class="calibre13">WindSpeed3pm</kbd>: Wind speed (km/hr) averaged over 10 minutes prior to 3 p.m.</span></li>
<li class="calibre17"><span><kbd class="calibre13">Pressure3pm</kbd>: Atmospheric pressure (hpa) reduced to mean sea level at 3 p.m.</span></li>
<li class="calibre17"><span><kbd class="calibre13">ChangeTemp</kbd>: Change in temperature.</span></li>
<li class="calibre17"><span><kbd class="calibre13">ChangeTempDir</kbd>: Direction of change in temperature.</span></li>
<li class="calibre17"><span><kbd class="calibre13">ChangeTempMag</kbd>: Magnitude of change in temperature.</span></li>
<li class="calibre17"><span><kbd class="calibre13">ChangeWindDirect</kbd>: Direction of wind change.</span></li>
<li class="calibre17"><span><kbd class="calibre13">MaxWindPeriod</kbd>: Period of maximum wind.</span></li>
<li class="calibre17"><kbd class="calibre13">RainToday</kbd>: Integer 1 <span>if precipitation (mm) in the 24 hours to 9 a.m. exceeds 1 mm, and 0 otherwise.</span></li>
<li class="calibre17"><span><kbd class="calibre13">TempRange</kbd>: Difference between minimum and maximum temperatures (degrees C) in the 24 hours to 9 a.m.</span></li>
<li class="calibre17"><span><kbd class="calibre13">PressureChange</kbd>: Change in pressure.</span></li>
<li class="calibre17"><span><kbd class="calibre13">RISK_MM</kbd>: The amount of rain. A kind of measure of the risk.</span></li>
<li class="calibre17"><span><kbd class="calibre13">RainTomorrow</kbd>: The target variable. Will it rain tomorrow?</span></li>
</ul>
<p class="calibre2">In our case, we will use only two of the many variables contained in it:</p>
<ul class="calibre16">
<li class="calibre17"><kbd class="calibre13">Date</kbd>: The date of observation (a <kbd class="calibre13">Date</kbd> object)</li>
<li class="calibre17"><span><kbd class="calibre13">RelHumid9am</kbd>: Relative humidity (percent) at 9 a.m</span></li>
</ul>
<p class="calibre2"><span>As said previously,</span> <span>the objective of this example is to forecast the humidity of a certain location as a function of the day. Here is the code that we will use in this example:</span></p>
<pre class="calibre24"><strong class="calibre1">##########################################################</strong><br class="title-page-name"/><strong class="calibre1">### Chapter 6 - Introduction to RNNs - using R  ##########</strong><br class="title-page-name"/><strong class="calibre1">########## Humidity forecasting with RNNs#################</strong><br class="title-page-name"/><strong class="calibre1">##########################################################</strong><br class="title-page-name"/><strong class="calibre1"><br class="title-page-name"/>library("rattle.data")</strong><br class="title-page-name"/><strong class="calibre1">library("rnn")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">data(weatherAUS)</strong><br class="title-page-name"/><strong class="calibre1">View(weatherAUS)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">#extract only 1 and 14 clumn and first 3040 rows (Albury location)</strong><br class="title-page-name"/><strong class="calibre1">data=weatherAUS[1:3040,c(1,14)]</strong><br class="title-page-name"/><strong class="calibre1">summary(data)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">data_cleaned &lt;- na.omit(data) </strong><br class="title-page-name"/><strong class="calibre1">data_used=data_cleaned[1:3000]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">x=data_cleaned[,1]</strong><br class="title-page-name"/><strong class="calibre1">y=data_cleaned[,2]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">head(x)</strong><br class="title-page-name"/><strong class="calibre1">head(y)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">X=matrix(x, nrow = 30)</strong><br class="title-page-name"/><strong class="calibre1">Y=matrix(y, nrow = 30)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1"># Standardize in the interval 0 - 1</strong><br class="title-page-name"/><strong class="calibre1">Yscaled = (Y - min(Y)) / (max(Y) - min(Y))</strong><br class="title-page-name"/><strong class="calibre1">Y=t(Yscaled)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">train=1:70</strong><br class="title-page-name"/><strong class="calibre1">test=71:100</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">model &lt;- trainr(Y = Y[train,],</strong><br class="title-page-name"/><strong class="calibre1">                X = Y[train,],</strong><br class="title-page-name"/><strong class="calibre1">                learningrate = 0.05,</strong><br class="title-page-name"/><strong class="calibre1">                hidden_dim = 16,</strong><br class="title-page-name"/><strong class="calibre1">                numepochs = 1000)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">plot(colMeans(model$error),type='l',xlab='epoch',ylab='errors')</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">Yp &lt;- predictr(model, Y[test,])</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">plot(as.vector(t(Y[test,])), col = 'red', type='l', </strong><br class="title-page-name"/><strong class="calibre1">     main = "Actual vs Predicted Humidity: testing set", </strong><br class="title-page-name"/><strong class="calibre1">     ylab = "Y,Yp")</strong><br class="title-page-name"/><strong class="calibre1">lines(as.vector(t(Yp)), type = 'l', col = 'black')</strong><br class="title-page-name"/><strong class="calibre1">legend("bottomright", c("Predicted", "Actual"), </strong><br class="title-page-name"/><strong class="calibre1">       col = c("red","black"), </strong><br class="title-page-name"/><strong class="calibre1">       lty = c(1,1), lwd = c(1,1))</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">############################################################</strong></pre>
<p class="calibre2"><span>We begin analyzing the code line by line, explaining in detail all the features applied to capture the results:</span></p>
<pre class="calibre24"><strong class="calibre1">library("rattle.data")</strong><br class="title-page-name"/><strong class="calibre1">library("rnn")</strong></pre>
<p class="calibre2">The first two lines of the initial code are used to load the libraries needed to run the analysis.</p>
<div class="packt_tip">Remember that to install a library that is not present in the initial distribution of R, you must use the <kbd class="calibre61">install.package</kbd> function. <span class="calibre62">This is the main function to install packages. It takes a vector of names and a destination library, downloads the packages from the repositories and installs them.</span> This function should be used only once and not every time you run the code.</div>
<p class="calibre2">The <kbd class="calibre13">rattle.data</kbd> library <span>contains the datasets used as default examples by the <kbd class="calibre13">rattle</kbd> package. The datasets themselves can be used independently of the <kbd class="calibre13">rattle</kbd> package to illustrate analytics, data mining, and data science tasks.</span></p>
<p class="calibre2"><span>The <kbd class="calibre13">rnn</kbd> library contains several functions for implementing an RNN in R:</span></p>
<pre class="calibre24"><strong class="calibre1">data(weatherAUS)</strong><br class="title-page-name"/><strong class="calibre1">View(weatherAUS)</strong></pre>
<p class="calibre2"><span>With this command, we upload the dataset named <kbd class="calibre13">weatherAUS</kbd>, as mentioned, contained in the <kbd class="calibre13">rattle.data</kbd> library. In the second line, the <kbd class="calibre13">view</kbd> function is used to invoke a spreadsheet-style data viewer on the dataframe object, as shown in the following figure:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00133.jpeg"/></div>
<p class="calibre2"><span>Returning to the code,</span> as before, we use only two variables. In addition, the dataset contains data from different locations in Australia. We will limit our study to the first location (<kbd class="calibre13">Albury</kbd>):</p>
<pre class="calibre24"><strong class="calibre1">data=weatherAUS[1:3040,c(1,14)]</strong></pre>
<p class="calibre2">Let's get a preliminary data analysis using the <kbd class="calibre13">summary()</kbd> function:</p>
<pre class="calibre24"><strong class="calibre1">&gt; summary(data)</strong><br class="title-page-name"/><strong class="calibre1">      Date             Humidity9am  </strong><br class="title-page-name"/><strong class="calibre1"> Min.   :2008-12-01   Min.   : 18.00 </strong><br class="title-page-name"/><strong class="calibre1"> 1st Qu.:2010-12-30   1st Qu.: 61.00 </strong><br class="title-page-name"/><strong class="calibre1"> Median :2013-04-27   Median : 76.00 </strong><br class="title-page-name"/><strong class="calibre1"> Mean   :2013-03-22   Mean   : 74.07 </strong><br class="title-page-name"/><strong class="calibre1"> 3rd Qu.:2015-05-27   3rd Qu.: 88.00 </strong><br class="title-page-name"/><strong class="calibre1"> Max.   :2017-06-25   Max.   :100.00 </strong><br class="title-page-name"/><strong class="calibre1">                      NA's   :9      </strong> </pre>
<p class="calibre2"><span>The <kbd class="calibre13">summary()</kbd> function returns a set of statistics for each variable. In particular, it is useful to highlight the result provided for the</span> <kbd class="calibre13">Humidity9am</kbd> <span>variable; this represents our target. For this variable, nine cases of missing value were detected. To remove the missing values, we will use the</span> <kbd class="calibre13">na.omit()</kbd> function; it drops any rows with missing values and forgets them forever:</p>
<pre class="calibre24"><strong class="calibre1">data_cleaned &lt;- na.omit(data) </strong><br class="title-page-name"/><strong class="calibre1">data_used=data_cleaned[1:3000]</strong></pre>
<p class="calibre2">With the second line of code, we limit our analysis to the first <kbd class="calibre13">3000</kbd> observations. Now we must set the input and the output data to the format required by the <kbd class="calibre13">trainr()</kbd> function:</p>
<pre class="calibre24"><strong class="calibre1">x=data_cleaned[,1]</strong><br class="title-page-name"/><strong class="calibre1">y=data_cleaned[,2]</strong></pre>
<p class="calibre2">In this way, <kbd class="calibre13">x</kbd> <span>will represent our input and <kbd class="calibre13">y</kbd> our target:</span></p>
<pre class="calibre24"><strong class="calibre1">X=matrix(x, nrow = 30)</strong><br class="title-page-name"/><strong class="calibre1">Y=matrix(y, nrow = 30)</strong></pre>
<p class="calibre2">With this piece of code we construct a matrix of <kbd class="calibre13">30</kbd> lines and <kbd class="calibre13">100</kbd> columns with the data available. Recall is a size setting required for the function we will use for model building. <span>We can now standardize this:</span></p>
<pre class="calibre24"><strong class="calibre1">Yscaled = (Y - min(Y)) / (max(Y) - min(Y))</strong><br class="title-page-name"/><strong class="calibre1">Y=t(Yscaled)</strong></pre>
<p class="calibre2">For this example, we have used the min-max method (usually called feature scaling) to get all the scaled data in the range <em class="calibre14">[0,1]</em>. The formula for this is as follows:</p>
<div class="calibre28"><img src="../images/00134.gif" class="calibre83"/></div>
<p class="calibre2"> </p>
<p class="calibre2">During the <span>normalization</span>, we must calculate the minimum and maximum values of each database column. Then we transpose the matrix obtained:</p>
<pre class="calibre24"><strong class="calibre1">train=1:70</strong><br class="title-page-name"/><strong class="calibre1">test=71:100</strong></pre>
<p class="calibre2"><span>In these lines of code, the dataset is split into <kbd class="calibre13">70:30</kbd>, with the intention of using <kbd class="calibre13">70</kbd> percent of the data at our disposal to train the network and the remaining <kbd class="calibre13">30</kbd> percent to test the network. Now is the time to build and train the model:</span></p>
<pre class="calibre24"><strong class="calibre1">model &lt;- trainr(Y = Y[train,],</strong><br class="title-page-name"/><strong class="calibre1">                X = Y[train,],</strong><br class="title-page-name"/><strong class="calibre1">                learningrate = 0.05,</strong><br class="title-page-name"/><strong class="calibre1">                hidden_dim = 16,</strong><br class="title-page-name"/><strong class="calibre1">                numepochs = 1000)</strong></pre>
<p class="calibre2"><span>The <kbd class="calibre13">trainr()</kbd> function trains an RNN in R environment. We have used <kbd class="calibre13">16</kbd> neurons in the hidden layer and the number of epochs is <kbd class="calibre13">1,000</kbd>. The <kbd class="calibre13">trainr()</kbd> function takes a few minutes as the training happens based on <kbd class="calibre13">X</kbd> and <kbd class="calibre13">Y</kbd>. Here are the last 10</span> <kbd class="calibre13">Trained epoch</kbd> <span>results as displayed on the R prompt:</span></p>
<pre class="calibre24"><strong class="calibre1"><span>Trained epoch: 990 - Learning rate: 0.05<br class="title-page-name"/></span><span>Epoch error: 0.382192317958489<br class="title-page-name"/></span><span>Trained epoch: 991 - Learning rate: 0.05<br class="title-page-name"/></span><span>Epoch error: 0.376313106021699<br class="title-page-name"/></span><span>Trained epoch: 992 - Learning rate: 0.05<br class="title-page-name"/></span><span>Epoch error: 0.380178990096884<br class="title-page-name"/></span><span>Trained epoch: 993 - Learning rate: 0.05<br class="title-page-name"/></span><span>Epoch error: 0.379260612039631<br class="title-page-name"/></span><span>Trained epoch: 994 - Learning rate: 0.05<br class="title-page-name"/></span><span>Epoch error: 0.380475314573825<br class="title-page-name"/></span><span>Trained epoch: 995 - Learning rate: 0.05<br class="title-page-name"/></span><span>Epoch error: 0.38169633378182<br class="title-page-name"/></span><span>Trained epoch: 996 - Learning rate: 0.05<br class="title-page-name"/></span><span>Epoch error: 0.373951666567461<br class="title-page-name"/></span><span>Trained epoch: 997 - Learning rate: 0.05<br class="title-page-name"/></span><span>Epoch error: 0.374880624458934<br class="title-page-name"/></span><span>Trained epoch: 998 - Learning rate: 0.05<br class="title-page-name"/></span><span>Epoch error: 0.384185799764121<br class="title-page-name"/></span><span>Trained epoch: 999 - Learning rate: 0.05<br class="title-page-name"/></span><span>Epoch error: 0.381408598560978<br class="title-page-name"/></span><span>Trained epoch: 1000 - Learning rate: 0.05<br class="title-page-name"/></span><span>Epoch error: 0.375245688144538</span></strong></pre>
<p class="calibre2">We can see the evolution of the algorithm by charting the error made by the algorithm to subsequent epochs:</p>
<pre class="calibre24"><strong class="calibre1">plot(colMeans(model$error),type='l',xlab='epoch',ylab='errors')</strong></pre>
<p class="calibre2">This graph shows the <strong class="calibre1">epoch</strong> versus <strong class="calibre1">error</strong>:</p>
<div class="cdpaligncenter"><img class="alignnone21" src="../images/00135.gif"/></div>
<p class="calibre2"><span>We finally have the network trained and ready for use; now we can use it to make our predictions. Remember, we've set aside 30 percent of the available data to test the network. It's time to use it:</span></p>
<pre class="calibre24"><strong class="calibre1">Yp &lt;- predictr(model, Y[test,])</strong></pre>
<p class="calibre2">Finally, <span>to compare the results,</span> <span>let's plot a graph showing the moisture content in the test set and the predicted results in order:</span></p>
<pre class="calibre24"><strong class="calibre1">plot(as.vector(t(Y[test,])), col = 'red', type='l', </strong><br class="title-page-name"/><strong class="calibre1">     main = "Actual vs Predicted Humidity: testing set", </strong><br class="title-page-name"/><strong class="calibre1">     ylab = "Y,Yp")</strong><br class="title-page-name"/><strong class="calibre1">lines(as.vector(t(Yp)), type = 'l', col = 'black')</strong><br class="title-page-name"/><strong class="calibre1">legend("bottomright", c("Predicted", "Actual"), </strong><br class="title-page-name"/><strong class="calibre1">       col = c("red","black"), </strong><br class="title-page-name"/><strong class="calibre1">       lty = c(1,1), lwd = c(1,1))</strong></pre>
<p class="calibre2">The following figure shows the actual values and predicted values:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00136.gif"/></div>
<p class="calibre2">From the analysis of the figure, it is possible to note one thing: the data is adapted to a good approximation to indicate that the model is able to predict the humidity conditions with good performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, <span>we saw</span> RNNs and how to use <span>internal memory for their processing.</span> We <span>also</span> <span>covered</span> <span>CNNs, which are standardized neural networks mainly used for image recognition. For RNNs, we studied some sample implementations in R.</span></p>
<p class="calibre2">We learned how to train, test, and evaluate an RNN. We also learned how to visualize the RNN model in an R environment. We discovered the LSTM model. We introduced the concepts as <span>CNN and a c</span><span>ommon CNN architecture: LeNet</span><span>.</span></p>
<p class="calibre2">In the next chapter, we will see more use cases involving R implementations of neural networks and deep learning.</p>


            </article>

            
        </section>
    </body></html>