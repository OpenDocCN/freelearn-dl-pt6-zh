- en: PyTorch Experiments on NLP and RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to deep dive into the PyTorch library on **natural
    language processing** (**NLP**) and other experiments. Then, we will convert the
    developed model into a format that can be used in an Android or iOS application
    using TensorFlow and CoreML.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to PyTorch features and installation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using variables in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building our own model network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying **recurrent neural networks** (**RNN**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch is a Python-based library that''s used to perform scientific computing
    operations with GPUs. It helps by performing faster experimentation to run production-grade
    ecosystems and distribute the training of libraries. It also provides two high-level
    features: tensor computations and building neural networks on tape-based autograd
    systems.'
  prefs: []
  type: TYPE_NORMAL
- en: The features of PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch provides an end-to-end deep learning system. Its features are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python utilization**: PyTorch is not simply a Python binding to a C++ framework.
    It is deeply integrated in Python so that it can be used with other popular libraries
    and frameworks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tools and libraries**: It has an active community of researchers and developers
    in the areas of computer vision and reinforcement learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible frontend**: This includes ease of use and hybrid in eager mode,
    accelerate speeds and seamless transitions to graph mode, and functionality and
    optimization in C++ runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud support**: This is supported on all of the major cloud platforms, allowing
    for seamless development and scaling with prebuilt images, so that it is able
    to run as a production-grade application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed training**: This includes performance optimization with the advantage
    of native support for the asynchronous execution of operations and peer-to-peer
    (p2p) communications, so that we can access both C++ and Python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Native support for ONNX**: We can export models into the standard **Open
    Neural Network Exchange** (**ONNX**) format for access to other platforms, runtimes,
    and visualizers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a stable version of PyTorch available at the time of writing this
    book, that is, 1.0\. There is also a nightly preview build available if you want
    to have a hands-on look at the latest code repository. You need to have the dependencies
    installed based on your package manager. **Anaconda** is the recommended package
    manager, and it installs all the dependencies automatically. **LibTorch** is only
    available for C++. Here is a grid showing the installation options that are available
    for installing PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17ee8f45-5596-405d-9255-31d5f2fc0fd4.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot specifies the package grid that was used while this
    book was being written. You can pick any package grid as per your hardware configuration
    availability.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install PyTorch, and to start Jupyter Notebook, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The installation of PyTorch is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6d1e025-b6a4-4a84-b77c-fc04da37914e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When you initiate the Jupyter Notebook, a new browser session opens up with
    an empty notebook, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63b602b7-9b98-4f82-b22f-ed7fe087acb3.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's look at the basics of PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that PyTorch has been installed, we can start experimenting with it. We
    will start with `torch` and `numpy`.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the top menu, create a new notebook and include the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s do some mathematical operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s calculate the mean method and print the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows the output of the mathematical operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's look at how to use different variables in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Using variables in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Variables in `torch` are used to build a computational graph. Whenever a variable
    is calculated, it builds a computational graph. This computational graph is used
    to connect all the calculation steps (nodes), and finally, when the error is reversed,
    the modification range (gradient) in all the variables is calculated at once.
    In comparison, `tensor` does not have this ability. We will look into this difference
    with a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will be printing the results for all parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's try plotting data on a graph using `matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting values on a graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s work on one simple program to plot values on a graph. To do this, use
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Following code block lists down a few of the activation methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `matplotlib` to activate the functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot the values on the graph, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b0e9e83-cf7f-4146-b058-5e9f304ce5e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the first line in the preceding code is required to draw the graph
    inside Jupyter Notebook. If you are running the Python file directly from the
    Terminal, you can omit the first line of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Building our own model network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will work on building our own network using PyTorch with
    a step-by-step example.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by looking at linear regression as a starting point.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression is probably the first method that anyone will learn in terms
    of machine learning. The objective of linear regression is to find a relationship
    between one or more features (independent variables) and a continuous target variable
    (the dependent variable), which can be seen in the following code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary libraries and declare all the necessary variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define the linear regression class and run a simple `nn` to explain
    regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will see how to plot the graphs and display the process of learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot the output of this code on the graph, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61351240-2e18-4920-986a-c70d07c5c39a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final plot looks as follows, with the loss (meaning the deviation between
    the predicted output and the actual output) equaling 0.01:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f453783-0ea5-4706-b70b-d9108fe28426.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we will start working toward deeper use cases using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A classification problem runs a neural network model to classify the inputs.
    For example, it classifies images of clothing into trousers, tops, and shirts. When
    we provide more inputs to the classification model, it will predict the value
    of the outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: A simple example would be filtering an email as *spam* or *not spam*. Classification
    either predicts categorical class labels based on the training set or the values
    (class labels) when classifying attributes that are used in classifying new data.
    There are many classification models, such as Naive Bayes, random forests, decision
    tress, and logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will work on a simple classification problem. To do this, use the
    following this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s plot the graphs and display the learning processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We will pick only a few plots from the output, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/521a242d-fe44-4f20-ab09-3e8314c22e47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see that the accuracy levels have increased with the increased number
    of steps in the iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6356bef0-a6b8-4200-9586-b482aedd91a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can reach an accuracy level of 1.00 in the final step of our execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eab78e53-d218-4ff3-b0bf-898dc7358898.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple neural networks with torch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks are necessary when a heuristic approach is required to solve
    a problem. Let''s explore a basic neural network using the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the easiest and fastest way to build your network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Saving and reloading data on the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at one example of how to save data on the network and then restore
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Two ways to save the net:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the code will look similar to the graphs that are shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/221a7acc-e365-426a-ab76-8f37e4ffff1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Running with batches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Torch helps you organize your data through `DataLoader`. We can use it to package
    the data through batch training. We can have our own data format (NumPy array,
    for example, or any other) loaded into Tensor, along with a wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a dataset where random numbers are taken into
    the dataset in batches and trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Optimization algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is always doubt about which optimization algorithm should be used in our
    implementation of the neural network for a better output. This is done by modifying
    the key parameters, such as the **weights **and **bias **values.
  prefs: []
  type: TYPE_NORMAL
- en: These algorithms are used to minimize (or maximize) error (*E*(*x*)), which
    is dependent on the internal parameters. They are used for computing the target
    results (*Y*) from the set of predictors (*x*) that are used in the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the different types of algorithms by using the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting dateset into torch dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Training the model for various epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of executing the preceding code block is displayed in the following
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94766868-218f-469f-bf52-4d762dae6b47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output of the Epoch count will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We will plot all the optimizers and represent them in the graph, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/747f9f2a-2f1a-4180-bc46-f6f3d32b9554.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will look at RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With RNNs, unlike feedforward neural networks, we can use the internal memory
    to process inputs in a sequential manner. In RNN, the connection between nodes
    forms a directed graph along a temporal sequence. This helps in tasking the RNN
    with largely unsegmented and connected speech or character recognition.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **MNIST** database consists of 60,000 handwritten digits. It also consists
    of a test dataset that's made up of 10,000 digits. While it is a subset of the
    NIST dataset, all the digits in this dataset are size-normalized and have been
    centered on a 28 x 28 pixels-sized image. Here, every pixel contains a value of
    0-255 with its grayscale value.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST dataset can be found at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  prefs: []
  type: TYPE_NORMAL
- en: The NIST dataset can be found a [https://www.nist.gov/srd/nist-special-database-19](https://www.nist.gov/srd/nist-special-database-19).
  prefs: []
  type: TYPE_NORMAL
- en: RNN classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will look at an example of how to build an RNN to identify handwritten
    numbers from the MNIST database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting one example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Converting test data into Variable, pick 2000 samples to speed up testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Training and testing the epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following files need to be downloaded and extracted to train the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9e4d3d4-eb13-4231-9257-661e0a37dcf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take the processing further with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of epochs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: RNN cyclic neural network – regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will deal with a regression problem under RNN. The cyclic neural network
    provides memory to the neural network. For the serial data, the cyclic neural
    network can achieve better results. We will use RNN here in this example to predict
    time series data.
  prefs: []
  type: TYPE_NORMAL
- en: To find out more about circular neural networks, go to [https://iopscience.iop.org/article/10.1209/0295-5075/18/3/003/meta](https://iopscience.iop.org/article/10.1209/0295-5075/18/3/003/meta).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is for the logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `RNN` class is defined in the following code. We will use `r_out` in a
    linear way to calculated the predicted output. We can also use a `for` loop to
    calculate the predicted output with `torch.stack`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to optimize RNN parameters now, as shown in the following code, before
    running the `for` loop to give the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The following block of code will look like a motion picture effect when it''s
    run, which we can''t represent here in this book. We have added a few screenshots
    to help you visualize this. We are using `x` as an input `sin` value and `y` as
    an output fitting `cos` value. Because a relationship exists between the two curves,
    we will use `sin` to predict `cos`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30e1a235-cfa4-4e32-8513-e9ac707ebcf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is a plot graph that will be generated after iteration 10:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e9db5f3-5b02-4c09-8269-9eb57193c681.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is a plot graph that will be generated after iteration 25:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8aa151a8-b47a-41ba-9b09-faa0635f0a87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are not showing all 100 iteration output images here, but we will skip to
    the final output, iteration 100, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93c8dff1-5c2e-4c1d-8efb-e6f16c330095.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will look at NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now is the time to experiment with a few NLP techniques with the help of PyTorch.
    This will be more useful for those of you who haven't written code in any deep
    learning framework before, but who may have better understanding of NLP core problems
    and algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look into simple examples with small dimensions, so
    that we can see how the weight of the layers changes as the network is training.
    You can try out your own model once you understand the network and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Before working on any NLP-based problems, we need to understand the basic building
    blocks on deep learning, including affine maps, non-linearities, and objective
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Affine maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Affine maps** are one of the basic building components of deep learning,
    and are represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ac3ea96-07ab-425b-9c2b-2487fec88f51.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the matrix is represented by A and the vectors are represented by *x* and *b*.
    *A* and *b* are the parameters that need to be learned, while *b* is the bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple example to explain this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, run the program with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Non-linearities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to identify why we need non-linearities. Consider, we have two
    affine maps: `f(x)=Ax+b` and `g(x)=Cx+d`. `f(g(x))` is shown in the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c3d54a3-b248-4007-8b84-6f120f64839a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that when affine maps are composed together, the resultant
    is an affine map, where *Ad+b* is a vector and *AC* is a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: We can identify neural networks as long chains of affine compositions. Previously,
    it was possible that non-linearities were introduced in-between the affine layers.
    But thankfully, it isn't the case any longer, and hence that helps in building
    more powerful and efficient models.
  prefs: []
  type: TYPE_NORMAL
- en: 'While working with the most common functions such as tanh (x), σ(x) and ReLU
    (x), we see that there are a few core non-linearities, as shown in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Objective functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective function (also called the loss function or cost function) will
    help your network to minimize. It works by selecting a training instance, running
    it through your neural network, then computing the loss of the output.
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of the loss function is updated for finding the parameters of
    the model. Like, if your model predicts an answer confidently, and the answer
    turns out to be wrong, the the computed loss will be high. If the predicted answer
    is correct, then the loss is low.
  prefs: []
  type: TYPE_NORMAL
- en: How is the network minimized?
  prefs: []
  type: TYPE_NORMAL
- en: First, the function will select a training instance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it is passed through our neural network to get the output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the loss of the output is calculated
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our training examples we need to minimize the loss function to minimize the
    probability of wrong results with the actual dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Building network components in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before shifting our focus to NLP, in this section we will use non-linearities
    and affine maps to build a network in PyTorch. In this example, we will learn
    to compute a loss function using the built in negative log likelihood in PyTorch
    and using backpropagation for updating the parmeters.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that all the components of the network need to be inherited from
    `nn.Module` and also override the `forward()` method. Considering boilerplate,
    these are the details we should remember. The network components are provided
    functionality when we inherit those components from `nn.Module`
  prefs: []
  type: TYPE_NORMAL
- en: Now, as mentioned previously, we will look at an example, in which the network
    takes a scattered bag-of-words (BoW) representation and and the output is a probability
    distribution into two labels, that is, English and Spanish. Also, this model is
    an example of logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: BoW classifier using logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probabilities will be logged onto our two labels English and Spanish on which
    our generated model will map a sparse BoW representation. In the vocabulary, we
    will assign each word as an index. Let's say for example, we have two words in
    our vocabulary, that is hello and world, which have indices as zero and one, respectively.
    For example, for the sentence *hello hello hello hello hello,* the BoW vector
    is *[5,0]*. Similarly the BoW vector for *hello world world hello world* is *[2,3]*,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, it is *[Count(hello),Count(world)].*
  prefs: []
  type: TYPE_NORMAL
- en: Let us denote is BOW vector as *x.*
  prefs: []
  type: TYPE_NORMAL
- en: 'The network output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8905bb53-f8c9-4a9a-9d76-58c0c236c43e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we need to pass the input through an affine map and then use log softmax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define the parameters that are needed. Here, those parameters
    are `A` and `B`, and the following code block explains the further implementations
    are required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the model knows its own parameters. The first output is `A`, while the
    second is `B`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We got the tensor output values. But, as we can see from the preceding code,
    these values aren't in correspondence to the log probability whether which is
    `English` and which corresponds to word `Spanish`. We need to train the model,
    and for that it's important to define these values to the log probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Let's start training our model then. We start with passing instances through
    the model to the those log probabilities. Then, the loss function is computed,
    and once the loss function is computer we calculate the gradient of this loss
    function. Finally, the parameters are updated with a gradient step. The `nn` package
    in PyTorch provides the loss functions. We want nn.NLLLoss() as the negative log
    likelihood loss. Optimization functions are also defined is `torch.optim`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will just use **Stochastic Gradient Descent** (**SGD**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We don't want to pass the training data again and again for no reason. Real
    datasets have multiple instances and not just 2\. It is reasonable to train the
    model for epochs between 5 to 30.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the range for our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will compute the various factors such as loss, gradient, and updating
    the parameters by calling the function optimizer.step():'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we have the basic understanding of performing text-based processing using
    PyTorch. We also have a better understanding on how RNN works and how can we approach
    NLP-related problems using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapters, we will build applications using what we have learned
    about neural networks and NLP. Happy coding!
  prefs: []
  type: TYPE_NORMAL
