- en: '*Chapter 4*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural Networks with NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain what a Recurrent Neural Network is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design and build a Recurrent Neural Network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate non-numeric data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the different state-of-the-art language models with RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict a value with a temporal sequence of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter covers various aspects of RNNs. it deals with explaining, designing,
    and building the various RNN models.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the previous chapter, Natural Language Processing (NLP) is an
    area of Artificial Intelligence (AI) that covers how computers can understand
    and manipulate human language in order to perform useful tasks. Now, with the
    growth of deep learning techniques, deep NLP has become a new area of research.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what is deep NLP? It is a combination of NLP techniques and deep learning.
    The result of the combination of these techniques are advances in the following
    areas:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linguistics: Speech to text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tools: POS tagging, entity recognition, and sentence parsing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Applications: Sentiment analysis, question answering, dialogue agents, and
    machine translation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the most important approaches of deep NLP is the representation of words
    and sentences. Words can be represented as a vector located in a plane full of
    other words. Depending on the similarity of each word to another word, its distance
    in the plane would be accordingly set as greater or smaller.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Representation of words in multiple dimensions](img/C13550_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Representation of words in multiple dimensions'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The previous figure shows an example of word embedding. **Word embedding**
    is a collection of techniques and methods that map words and sentences from a
    corpus into vectors or real numbers. It generates a representation of each word
    in terms of the context in which a word appears. Then, word embedding can find
    the similarities between words. For example, the nearest words to dog are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Dogs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cow
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bird
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are different ways to generate embeddings, such as Word2Vec, which will
    be covered in *Chapter 7*, *Build a Conversational Agent to Manage the Robot*.
  prefs: []
  type: TYPE_NORMAL
- en: This is not the only big change deep learning brings to NLP on a morphological
    level. With deep learning, a word can be represented as a combination of vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Each morpheme is a vector, and a word is the result of combining several morpheme
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: This technique of combining vectors is also used on a semantic level, but for
    the creation of words and for the creation of a sentence. Each phrase is formed
    by a combination of many word vectors, so a sentence can be represented as one
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: Another improvement is in parsing sentences. This task is hard because it is
    ambiguous. Neural networks can accurately determine the grammatical structure
    of a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'In full application terms, the areas are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: Traditionally, this consists of a bag of words labeled
    with positive or negative sentiments. Then, combining these words returns the
    sentiment of the whole sentence. Now, using deep learning and word representation
    models, the results are better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question answering**: To find the answer to a question, vector representations
    can match a document, a paragraph, or a sentence with an input question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dialogue agents**: With neural language models, a model can understand a
    query and create a response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine translation**: Machine translation is one of the hardest tasks in
    NLP. A lot of approaches and models have been tried. Traditional models are very
    large and complex, but deep learning neural machine translation has solved that
    problem. Sentences are encoded with vectors, and the output is decoded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vector representation of words is fundamental to deep NLP. Creating a plane,
    many tasks can be completed. Before analyzing deep NLP techniques, we are going
    to review what a recurrent neural network (RNN) is, what its applications are
    within deep learning, and how to create our first RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Our future conversational agent will detect the intention of a conversation
    and respond with a predefined answer. But with a good dataset of conversations,
    we could create a Recurrent Neural Network to train a language model (LM) capable
    of generating a response to a given topic in a conversation. This task can be
    performed by other neural network architectures, such as seq2seq models.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we are going to review **Recurrent Neural Networks** (**RNNs**).
    This topic will first look at the theory of RNNs. It will review many architectures
    within this model and help you to work out which model to use to solve a certain
    problem, and it will also look at several types of RNN and their pros and cons.
    Also, we will look at how to create a simple RNN, train it, and make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Recurrent Neural Networks (RNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Human behavior shows a variety of serially ordered action sequences. A human
    is capable of learning dynamic paths based on a set of previous actions or sequences.
    This means that people do not start learning from scratch; we have some previous
    knowledge, which helps us. For example you could not understand a word if you
    did not understand the previous word in a sentence!
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, neural networks cannot solve these types of problem because they
    cannot learn previous information. But what happens with problems that cannot
    be solved with just current information?
  prefs: []
  type: TYPE_NORMAL
- en: In 1986, Michael I. Jordan proposed a model that deals with the classical problem
    of temporal organization. This model is capable of learning the trajectories of
    a dynamic object by studying its previous movements. Jordan created the first
    RNN.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: Example of non-previous information versus temporal sequences](img/C13550_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Example of non-previous information versus temporal sequences'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the previous figure, the image on the left shows us that, without any information,
    we cannot know what the next action of the black point will be, but if we suppose
    its previous movements are recorded as the red line on the right-hand side of
    the graph we can predict what its next action will be.
  prefs: []
  type: TYPE_NORMAL
- en: Inside Recurrent Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have seen that RNNs are different to neural networks (NNs). RNN neurons
    are like normal neurons, but with loops within them, allowing them to store a
    time state. Storing the state of a certain moment in time, they can make predictions
    based on previous state of time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3: Traditional neuron](img/C13550_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Traditional neuron'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The preceding figure shows a traditional neuron, used in an NN. *X**n* are
    the inputs of the neuron, and after the activation function, it generates a response.
    The schema of an RNN neuron is different:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: Recurrent neuron](img/C13550_04_04_(1).jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Recurrent neuron'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The loop in the previous figure allows the neuron to store the time state. *h**n*
    is the output of the input, *X**n*, and the previous state. The neuron changes
    and evolves over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the input of the neuron is a sequence, an unrolled RNN would be like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5: Unrolled recurrent neuron](img/C13550_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Unrolled recurrent neuron'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The chain-like schema in figure 4.5 shows that RNNs are closely related to sequences
    and lists. So, we have as many neurons as inputs, and each neuron passes its state
    to the next.
  prefs: []
  type: TYPE_NORMAL
- en: RNN architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Depending on the quantity of inputs and outputs in the RNN, there are many
    architectures with different numbers of neurons. Each architecture is specialized
    for a certain task. So far, there are many types of network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: Structures of RNNs](img/C13550_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Structures of RNNs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The previous figure shows the various classifications of RNNs. Earlier in this
    book, we reviewed the one-to-one architecture. In this chapter, we will learn
    about the many-to-one architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '**One-to-one**: Classification or regression tasks from one input (image classification).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-to-many**: Image captioning tasks. These are hard tasks in deep learning.
    For example, a model that passes an image as an input could describe the elements
    that are in the picture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many-to-one**: Temporal series, sentiment analysis… every task with just
    one output but based in a sequence of different inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many-to-many**: Machine automated translation systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synchronized many-to-many**: Video classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long-Dependency Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In some tasks, it is only necessary to use the most recent information to predict
    the next step of a model. With a temporal series, it is necessary to check older
    elements to learn or predict the next element or word in a sentence. For example,
    take a look at this sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: The clouds are in the sky.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now imagine this sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: The clouds are in the [?]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You would assume that the required word would be sky, and you know this because
    of the previous information:'
  prefs: []
  type: TYPE_NORMAL
- en: The clouds are in the
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But there are other tasks in which the model would need previous information
    to obtain a better prediction. For example, have a look at this sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: I was born in Italy, but when I was 3, I moved to France… that's the reason
    why I speak [?]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To predict the word, the model needs to take the information from the beginning
    of the sentence, and that could be a problem. This is a problem with RNNs: when
    the distance to the information is large, it is more difficult to learn. This
    problem is called the **vanishing gradient**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The vanishing gradient problem**'
  prefs: []
  type: TYPE_NORMAL
- en: Information travels through time in an RNN so that information from previous
    steps is used as input in the next step. At each step, the model calculates the
    cost function, so each time, the model may obtain an error measure. While propagating
    the error calculated through the network, and trying to minimize that error when
    updating the weights, the result of that operation is a number closer to zero
    (if you multiply two small numbers, the result is a smaller number). This means
    the gradient of the model becomes less and less with each multiplication. The
    problem here is that the network will not train properly. A solution to this problem
    with RNNs is to use Long Short-Term Memory (LSTM).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 14: Predict House Prices with an RNN'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are going to create our first RNN using Keras. This exercise is not a time-series
    problem. We are going to use a regression dataset to introduce RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use several methods included in the Keras library as a model or a type
    of layer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras models: These let us use the different available models in Keras. We
    are going to use the Sequential model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keras layers: We can add different types of layers to our neural network. In
    this exercise, we are going to use LSTM and a Dense layer. A dense layer is a
    regular layer of neurons in a neural network. Each neuron receives input from
    all the neurons in the previous layer, but they are densely connected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main objective of this exercise is to predict the value of a house in Boston,
    so our dataset will contain information on each house, such as the total area
    of the property or the number of rooms:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the dataset of Boston house prices from `sklearn` and take a look at
    the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.7: Boston house-prices data](img/C13550_04_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.7: Boston house prices data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can see the data has high values, so the best thing to do is to normalize
    the data. With the `MinMaxScaler` function of `sklearn`, we are going to transform
    our data into values between 0 and 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Divide the data into train and test sets. A good percentage for the test set
    is 20% of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.8: Shape of the train and test data](img/C13550_04_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.8: Shape of the train and test data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Import the Keras libraries and set a seed to initialize the weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a simple model. The dense layer is just a set of neurons. The last dense
    layer has only one neuron to return the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.9: Training the network](img/C13550_04_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.9: Training the network'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Compute the error of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.10: Computing the error in the model](img/C13550_04_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.10: Computing the error of the model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.11: Predictions of our model](img/C13550_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Predictions of our model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now you have an RNN for a regression problem! You can try to modify the parameters,
    add more layers, or change the number of neurons to see what happens. In the next
    exercise, we will solve time-series problems with LSTM layers.
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**LSTM** is a type of RNN that''s designed to solve the long-dependency problem.
    It can remember values for long or short time periods. The principal way it differs
    from traditional RNNs is that they include a cell or a loop to store the memory
    internally.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This type of neural network was created in 1997 by Hochreiter and Schmidhuber.
    This is the basic schema of an LSTM neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12: LSTM neuron structure](img/C13550_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: LSTM neuron structure'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see in the previous figure, the schema of an LSTM neuron is complex.
    It has three types of gate:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input gate: Allows us to control the input values to update the state of the
    memory cell.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Forget gate: Allows us to erase the content of the memory cell.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output gate: Allows us to control the returned values of the input and cell
    memory content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An LSTM model in Keras has a three-dimensional input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample: Is the amount of data you have (quantity of sequences).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Time step: Is the memory of your network. In other words, it stores previous
    information in order to make better predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Features: Is the number of features in every time step. For example, if you
    are processing pictures, the features are the number of pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: This complex design causes another type of network to be formed. This new type
    of neural network is a **Gated Recurrent Unit (GRU)**, and it solves the vanishing
    gradient problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Exercise 15: Predict the Next Solution of a Mathematical Function'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we are going to build an LSTM to predict the values of a
    sine function. In this exercise, you will learn how to train and predict a model
    with Keras, using the LSTM model. Also, this exercise will cover data generation
    and how to split data into training samples and test samples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With Keras, we can create an RNN using the Sequential class, and we can create
    an LSTM to add new recurrent neurons. Import the Keras libraries for LSTM models,
    NumPy for setting up the data, and matplotlib to print the graphs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the dataset to train and evaluate the model. We are going to generate
    an array of 1,000 values as a result of the sine function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To see if the data is good, let''s plot it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.13: Output with the plotted data](img/C13550_04_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.13: Output with the plotted data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As this chapter explains, RNN works with sequences of data, so we need to split
    our data into sequences. In our case, the maximum length of the sequences will
    be 5\. This is necessary because the RNNs need sequences as input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This model will be **many-to-one** because the input is a sequence and the
    output is just a value. To see why we are going to create an RNN using the many-to-one
    structure, we just need to know the dimensions of our input and output data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the data to introduce it to the LSTM model. Pay attention to the shape
    of the `x` and `y` variables. RNNs need a three-dimensional vector as input and
    a two-dimensional vector as output. That''s why we will reshape the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.14: Reshaping the variables](img/C13550_04_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.14: Reshaping the variables'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The input dimension of an LSTM is 3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.15: Splitting data as train and test](img/C13550_04_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.15: Splitting data into train and test sets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Build a simple model with one LSTM unit and one dense layer with one neuron
    and linear activation. The dense layer is just a regular layer of neurons receiving
    the input from the previous layer and generating many neurons as output. Because
    of that, our dense layer has only one neuron because we need a scalar value as
    the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model for 5 epochs (one epoch is when the entire dataset is processed
    by the neural network) and a batch size of 32 and evaluate it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.16: Training with 5 epochs with batch size 32](img/C13550_04_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.16: Training with 5 epochs with a batch size of 32'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the test predictions to see if it works well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.17: Plotting the predicted shape](img/C13550_04_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.17: Plotting the predicted shape'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s improve our model. Create a new one with four units in the LSTM layer
    and one dense layer with one neuron, but with the sigmoid activation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train and evaluate it for 25 epochs and with a batch size of 8:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.18: Training for 25 epochs with batch size 8](img/C13550_04_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.18: Training for 25 epochs with a batch size of 8'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the predictions of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.19: Prediction of our neural network](img/C13550_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.19: Predictions of our neural network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can now compare the plots of each model, and we can see that the second
    model is better. With this exercise, you have learned the basics of LSTM, how
    to train and evaluate the model you have created, and also how to determine whether
    it is good or not.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Chapter 3*, *Fundamentals of Natural Language Processing* introduced us to
    statistical language models (LMs), which are the probability distribution for
    a sequence of words. We know LMs can be used to predict the next word in a sentence,
    or to compute the probability distribution of the next word.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20: LM formula to compute the probability distribution of an upcoming
    word](img/C13550_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.20: LM formula to compute the probability distribution of an upcoming
    word'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The sequence of words is *x1* , *x2* … and the next word is *x**t+1*. *w**j*
    is a word in the vocabulary. *V* is the vocabulary and *j* is a position of a
    word in that vocabulary. *w**j* is the word located in position *j* within *V*.
  prefs: []
  type: TYPE_NORMAL
- en: You use LMs every day. The keyboards on cell phones use this technology to predict
    the next word of a sentence, and search engines such as Google use it to predict
    what you want to search in their search for engine.
  prefs: []
  type: TYPE_NORMAL
- en: We talked about the n-gram model and bigrams counting the words in a corpus,
    but that solution has some limitations, such as long dependencies. Deep NLP and
    neural LMs will help to get around these limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Neural Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neural LMs follow the same structure as statistical LMs. They aim to predict
    the next word in a sentence, but in a different way. A neural LM is motivated
    by an RNN because of the use of sequences as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '*Exercise 15*, *Predict the Next Solution of a Mathematical Function* predicts
    the next result of the sine function from a sequence of five previous steps. In
    this case, instead of sequences of sine function results, the data is words, and
    the model will predict the next word.'
  prefs: []
  type: TYPE_NORMAL
- en: These neural LMs emerged from the necessity to improve the statistical approach.
    Newer models can work around some of the limitations and problems of traditional
    LMs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Problems of statistical LMs**'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we reviewed LMs and the concepts of N-grams, bigrams,
    and the Markov model. These methods are executed by counting occurrences in the
    text. That's why these methods are called statistical LMs.
  prefs: []
  type: TYPE_NORMAL
- en: The main problem with LMs is data limitation. What can we do if the probability
    distribution of the sentence we want to compute does not exist in the data? A
    partial solution here is the smoothing method, but that is insufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Another solution is to use the Markov Assumption (each probability only depends
    on the previous step, simplifying the Chain Rule) to simplify the sentence, but
    that will not give a good prediction. What this means is, we could simplify our
    model using 3-grams.
  prefs: []
  type: TYPE_NORMAL
- en: A solution to this problem is to increase the size of the corpus, but the corpus
    will end up being to large. These limitations in n-gram models are called **sparsity
    problems**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Window-Based Neural Model**'
  prefs: []
  type: TYPE_NORMAL
- en: A first approximation of this new model was the use of a sliding window to compute
    the probabilities of the next word. The concept of this solution comes from window
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of words, it is hard to understand the meaning of a single word without
    any context. There are many problems if that word is not in a sentence or in a
    paragraph, for example, ambiguity between two similar words or auto-antonyms.
    Auto-antonyms are words with multiple meanings. The word handicap, depending on
    its context, can mean an advantage (for example, in sport) or a disadvantage (sometimes
    offensive, a physical problem).
  prefs: []
  type: TYPE_NORMAL
- en: 'Window classification classifies a word in the context (created by the window)
    of its neighboring words. The approach of a sliding window can be used to generate
    an LM. Here is a graphical example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21: Window-based neural LM](img/C13550_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.21: Window-based neural LM'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the previous figure, there is an example of how a window-based neural model
    works. The window size is 5 (word1 to word5). It creates a vector joining the
    embedding vector of each word, and computes this in a hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22: Hidden layer formula](img/C13550_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.22: Hidden layer formula'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'And finally, to predict a word, the model returns a value that can be used
    to classify the probability of the word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23: Softmax function](img/C13550_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.23: Softmax function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Then, the word with the highest value will be the predicted word.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We are not going to go deeper into these terms because we will use an LSTM to
    create the LM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits of this approach over the traditional one are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Less computational work. Window-based neural models need less computational
    resources because they don't need to iterate through the corpus computing probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It avoids the problem of changing the dimension of the N-gram to find a good
    probability distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generated text will have more sense in terms of meaning because this approach
    solves the sparsity problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But there are some problems:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Window limitations: The size of the window cannot be large, so the meaning
    of some words could be wrong.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each window has its own weight value, so it can cause ambiguity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the window grows in size, the model grows too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the problems with the window model, an RNN can improve its performance.
  prefs: []
  type: TYPE_NORMAL
- en: RNN Language Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An RNN is able to compute the probabilities of an upcoming word in a sequence
    of previous steps. The core idea of this approach is to apply the same weights
    repeatedly throughout the process of training.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some advantages of using an RNN LM over a window-based model:'
  prefs: []
  type: TYPE_NORMAL
- en: This architecture can process any length sentence; it does not have a fixed
    size, unlike the window-based approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is the same for every input size. It will not grow if the input is
    larger.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the NN architecture, it can use information from the previous steps
    and from the steps ahead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights are shared across the timesteps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we have talked about different ways to improve the statistical LM and
    the pros and cons of each one. Before developing an RNN LM, we need to know how
    to introduce a sentence as input in the NN.
  prefs: []
  type: TYPE_NORMAL
- en: '**One-hot encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks and machine learning are about numbers. As we have seen throughout
    this book, input elements are numbers and outputs are codified labels. But if
    a neural network has a sentence or a set of characters as input, how can it transform
    this into numerical values?
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding is a numerical representation of discrete variables. It assumes
    a feature vector with the same size for different values within a discrete set
    of variables. This means that if there is a corpus of size 10, each word will
    be codified as a vector of length 10\. So, each dimension corresponds to a unique
    element of the set.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24: RNN pre-processing data flow](img/C13550_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.24: RNN pre-processing data flow'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The previous figure shows how one-hot encoding works. It is important to understand
    the shapes of each vector because the neural network needs to understand what
    input data we have and what output we want to obtain. Next, *Exercise 16, Encoding
    a small Corpus* will help you examine the fundamentals of one-hot encoding in
    more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 16: Encoding a Small Corpus'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we are going to learn how to encode a set of words using one-hot
    encoding. It is the most basic encoding method, and it gives us a representation
    of discrete variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'This exercise will cover different ways of performing this task. One way is
    to manually perform encoding, while another way is to use libraries. After finishing
    the exercise, we will obtain a vector representation of each word, ready to use
    as the input for a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a corpus. This corpus is the same one that we used in *Chapter 3*, *Fundamentals
    of Natural Language Processing*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize it using `spaCy`. We are not going to use the stop-words (erasing
    useless words, such as articles) method because we have a small corpus. We want
    all the tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a list with every unique token in the corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.25: List with each unique token in the corpus](img/C13550_04_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.25: List with each unique token in the corpus'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Create a dictionary with each word in the corpus as the key and a unique number
    as the value. This dictionary will look like {word:value}, and this value will
    have the index of 1 in the one-hot encoded vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.26: Each word as a key and a unique number as value](img/C13550_04_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.26: Each word as a key and a unique number as a value'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Encode a sentence. This way of performing encoding is manual. There are some
    libraries, such as sklearn, that provide automatic encoding methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/C13550_04_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.27: Manual one-hot encoded vectors.'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `sklearn` methods. sklearn first encodes each unique token in the
    corpus with `LabelEncoder`, and then uses `OneHotEncoder` to create the vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/C13550_04_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.28: Vectors created with OneHotEncoder'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, take the same sentence that we encoded before and apply the `LabelEncoder`
    transform method we created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.29: LabelEncoder transform applied](img/C13550_04_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.29: LabelEncoder transform applied'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can decode `LabelEncoder` in the initial sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.30: Decoded LabelEncoder](img/C13550_04_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.30: Decoded LabelEncoder'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Declare `OneHotEncoder` with `sparse=False` (if you do not specify this, it
    will return a sparse matrix):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To encode our sentence with the label encoder that we have created, we need
    to reshape our labeled corpus to fit it into the `onehot_encoder` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can transform our sentence (encoded with LabelEncoder) into a one-hot
    vector. The results of this way of encoding and manual encoding will not be the
    same, but they will have the same shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.31: One-hot encoded vectors using Sklearn methods](img/C13550_04_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.31: One-hot encoded vectors using Sklearn methods'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This exercise is really important. If you do not understand the shapes of the
    matrices, it will be very hard to understand the inputs of RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Good job! You have finished *Exercise 16*. Now you can encode discrete variables
    into vectors. This is part of pre-processing data to train and evaluate a neural
    network. Next, we have the activity of the chapter, the objective of which is
    to create an LM using RNNs and one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For larger corpuses, one-hot encoding is not very useful because it would create
    huge vectors for the words. Instead, it is normal to use an embedding vector.
    This concept will be covered later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The Input Dimensions of RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before getting started with the RNN activity, you may not understand input dimensions.
    In this section, we will focus on understanding the shape of the n-dimensional
    arrays, and how we can add a new dimension or erase one.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequence data format**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve mentioned the many-to-one architecture, where each sample consists of
    a fixed sequence and a label. That label corresponds with the upcoming value in
    the sequence. It is something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.32: Format of sequence data](img/C13550_04_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.32: Format of sequence data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this example, we have two sequences in matrix X, and the two output labels
    in Y. So, the shapes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: X = (2, 4)
  prefs: []
  type: TYPE_NORMAL
- en: Y = (2)
  prefs: []
  type: TYPE_NORMAL
- en: But if you tried to insert this data into an RNN, it wouldn't work because it
    does not have the correct dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '**RNN data format**'
  prefs: []
  type: TYPE_NORMAL
- en: To implement an RNN with temporal sequences in Keras, the model will need an
    input vector with three dimensions and, as output, one vector with two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for the X matrix, we will have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Value length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.33: RNN data format](img/C13550_04_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.33: RNN data format'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The shapes here are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: X = (2, 4, 1)
  prefs: []
  type: TYPE_NORMAL
- en: Y = (2, 1)
  prefs: []
  type: TYPE_NORMAL
- en: '**One-hot format**'
  prefs: []
  type: TYPE_NORMAL
- en: 'With one-hot encoding, we have the same dimensions as input, but the value
    length changes. In the preceding figure, we can see the values ([1], [2], …) with
    one-dimensionality. But with one-hot encoding, these values will change to vectors,
    so the shape would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.34: One-hot format](img/C13550_04_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.34: One-hot format'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: X = (2, 4, 3)
  prefs: []
  type: TYPE_NORMAL
- en: Y = (2, 3)
  prefs: []
  type: TYPE_NORMAL
- en: To perform all these changes to the dimensions, the **reshape** method from
    the NumPy library will be used.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With this knowledge of dimensions, you can start the activity, and remember,
    the input dimension of an LSTM is three and the output dimension is two. So, if
    you create two LSTM layers continuously, how can you add the third dimension to
    the output of the first layer? Change the return state to True.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4: Predict the Next Character in a Sequence'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will predict the upcoming character in a long sequence.
    The activity has to be performed using one-hot encoding to create the input and
    output vectors. The architecture of the model will be an LSTM, as we saw in *Exercise
    14*, *Predict Houses Prices with an RNN*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario: You work in a global company as the security manager. One morning,
    you notice a hacker has discovered and changed all the passwords for the company''s
    databases. You and your team of engineers start trying to decode the hacker''s
    passwords to enter the system and fix everything. After analyzing all the new
    passwords, you see a common structure.'
  prefs: []
  type: TYPE_NORMAL
- en: You only need to decode one more character in the password, but you don't know
    what the character is and you only have one more opportunity to get the correct
    password.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you decide to create a program that analyzes long sequences of data and
    the five characters of the password you already know. With this information, it
    can predict the last character of the password.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first five characters of the password are: tyuio. What will the last character
    be?'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You have to use one-hot encoding and LSTM. You will train your model with one-hot
    encoded vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the sequence of data: qwertyuiopasdfghjklñzxcvbnm'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This sequence is repeated 100 times, so do this: sequence = ''qwertyuiopasdfghjklñzxcvbnm''
    * 100.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Divide the data into sequences of five characters and prepare the output data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode the input and the output sequences as one-hot encoded vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the train and test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Design the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The output has many zeroes, so it is hard to achieve an exact result. Use the
    LeakyRelu activation function with an alpha of 0.01, and when you do the prediction,
    round off the value of that vector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Train and evaluate it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a function that, when given five characters, predicts the next one in
    order to work out the last character of the password.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 308.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI and deep learning are making huge advances in terms of images and artificial
    vision thanks to convolutional networks. But RNNs also have a lot of power.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we reviewed how a neural network would can to predict the values
    of a sine function using temporal sequences. If you change the training data,
    this architecture can learn about stock movements for each distribution. Also,
    there are many architectures for RNNs, each of which is optimized for a certain
    task. But RNNs have a problem with vanishing gradients. A solution to this problem
    is a new model, called LSTM, which changes the structure of a neuron to memorize
    timesteps.
  prefs: []
  type: TYPE_NORMAL
- en: Focusing on linguistics, statistical LMs have many problems related with computational
    load and distribution probabilities. To solve the sparsity problem, the size of
    the n-gram model was lowered to 4 or 3 grams, but that was an insufficient number
    of steps back to predict an upcoming word. If we use this approach, the sparsity
    problem appears. A neural LM with a fixed window size can prevent the sparsity
    problem, but there are still problems with the limited size of the window and
    the weights. With RNNs, these problems do not arise, and depending on the architecture,
    it can obtain better results, looking many steps back and forward. But deep learning
    is about vectors and numbers. When you want to predict words, you need to encode
    the data to train the model. There are various different methods, such as the
    one-hot encoder or the label encoder. You can now generate text from a trained
    corpus and an RNN.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will talk about Convolutional Neural Networks (CNNs).
    We will review the fundamental techniques and architectures of CNNs, and also
    look at more complex implementations, such as transfer learning.
  prefs: []
  type: TYPE_NORMAL
