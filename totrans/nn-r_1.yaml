- en: Neural Network and Artificial Intelligence Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the scientific and philosophical studies conducted over the centuries,
    special mechanisms have been identified that are the basis of human intelligence.
    Taking inspiration from their operations, it was possible to create machines that
    imitate part of these mechanisms. The problem is that they have not yet succeeded
    in imitating and integrating all of them, so the **Artificial Intelligence** (**AI**)
    systems we have are largely incomplete.
  prefs: []
  type: TYPE_NORMAL
- en: A decisive step in the improvement of such machines came from the use of so-called
    **Artificial Neural Networks** (**ANNs**) that, starting from the mechanisms regulating
    natural neural networks, plan to simulate human thinking. Software can now imitate
    the mechanisms needed to win a chess match or to translate text into a different
    language in accordance with its grammatical rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter introduces the basic theoretical concepts of ANN and AI. Fundamental
    understanding of the following is expected:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic high school mathematics; differential calculus and functions such as *sigmoid*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R programming and usage of R libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will go through the basics of neural networks and try out one model using
    R. This chapter is a foundation for neural networks and all the subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: ANN concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neurons, perceptron, and multilayered neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias, weights, activation functions, and hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward and backpropagation methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brief overview of **Graphics Processing Unit** (**GPU**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the chapter, you will be able to recognize the different neural
    network algorithms and tools which R provides to handle them.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The brain is the most important organ of the human body. It is the central processing
    unit for all the functions performed by us. Weighing only 1.5 kilos, it has around
    86 billion neurons. A neuron is defined as a cell transmitting nerve impulses
    or electrochemical signals. The brain is a complex network of neurons which process
    information through a system of several interconnected neurons. It has always
    been challenging to understand the brain functions; however, due to advancements
    in computing technologies, we can now program neural networks artificially.
  prefs: []
  type: TYPE_NORMAL
- en: The discipline of ANN arose from the thought of mimicking the functioning of
    the same human brain that was trying to solve the problem. The drawbacks of conventional
    approaches and their successive applications have been overcome within well-defined
    technical environments.
  prefs: []
  type: TYPE_NORMAL
- en: AI or machine intelligence is a field of study that aims to give cognitive powers
    to computers to program them to learn and solve problems. Its objective is to
    simulate computers with human intelligence. AI cannot imitate human intelligence
    completely; computers can only be programmed to do some aspects of the human brain.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is a branch of AI which helps computers to program themselves
    based on the input data. Machine learning gives AI the ability to do data-based
    problem solving. ANNs are an example of machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep learning** (**DL**) is complex set of neural networks with more layers
    of processing, which develop high levels of abstraction. They are typically used
    for complex tasks, such as image recognition, image classification, and hand writing
    identification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the audience think that neural networks are difficult to learn and
    use it as a black box. This book intends to open the black box and help one learn
    the internals with implementation in R. With the working knowledge, we can see
    many use cases where neural networks can be made tremendously useful seen in the
    following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/00005.jpeg)**'
  prefs: []
  type: TYPE_NORMAL
- en: Inspiration for neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks are inspired by the way the human brain works. A human brain
    can process huge amounts of information using data sent by human senses (especially
    vision). The processing is done by neurons, which work on electrical signals passing
    through them and applying flip-flop logic, like opening and closing of the gates
    for signal to transmit through. The following images shows the structure of a
    neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The major components of each neuron are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dendrites**: Entry points in each neuron which take input from other neurons
    in the network in form of electrical impulses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cell Body**: It generates inferences from the dendrite inputs and decides
    what action to take'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Axon terminals**: They transmit outputs in form of electrical impulses to
    next neuron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each neuron processes signals only if it exceeds a certain threshold. Neurons
    either fire or do not fire; it is either *0* or *1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI has been a domain for sci-fi movies and fiction books. ANNs within AI have
    been around since the 1950s, but we have made them more dominant in the past 10
    years due to advances in computing architecture and performance. There have been
    major advancements in computer processing, leading to:'
  prefs: []
  type: TYPE_NORMAL
- en: Massive parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed representation and computation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning and generalization ability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault tolerance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low energy consumption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the domain of numerical computations and symbol manipulation, solving problems
    on-top of centralized architecture, modern day computers have surpassed humans
    to a greater extent. Where they actually lag behind with such an organizing structure
    is in the domains of pattern recognition, noise reduction, and optimizing. A toddler
    can recognize his/her mom in a huge crowd, but a computer with a centralized architecture
    wouldnâ€™t be able to do the same.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the biological neural network of the brain has been outperforming
    machines, and hence the inspiration to develop an alternative loosely held, decentralized
    architecture mimicking the brain.
  prefs: []
  type: TYPE_NORMAL
- en: ANNs are massively parallel computing systems consisting of an extremely large
    number of simple processors with many interconnections.
  prefs: []
  type: TYPE_NORMAL
- en: One of the leading global news agencies, Guardian, used big data in digitizing
    the archives by uploading the snapshots of all the archives they had had. However,
    for a user to copy the content and use it elsewhere is the limitation here. To
    overcome that, one can use an ANN for text pattern recognition to convert the
    images to text file and then to any format according to the needs of the end-users.
  prefs: []
  type: TYPE_NORMAL
- en: How do neural networks work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to the biological neuron structure, ANNs define the neuron as a central
    processing unit, which performs a mathematical operation to generate one output
    from a set of inputs. The output of a neuron is a function of the weighted sum
    of the inputs plus the bias. Each neuron performs a very simple operation that
    involves activating if the total amount of signal received exceeds an activation
    threshold, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00007.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The function of the entire neural network is simply the computation of the
    outputs of all the neurons, which is an entirely deterministic calculation. Essentially,
    ANN is a set of mathematical function approximations. We would now be introducing
    new terminology associated with ANNs:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layered approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Any neural network processing a framework has the following architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00008.gif)'
  prefs: []
  type: TYPE_IMG
- en: There is a set of inputs, a processor, and a set of outputs. This layered approach
    is also followed in neural networks. The inputs form the **input layer**, the
    **middle layer(s)** which performs the processing is called the **hidden layer(s)**,
    and the **output(s)** forms the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Our neural network architectures are also based on the same principle. The hidden
    layer has the magic to convert the input to the desired output. The understanding
    of the hidden layer requires knowledge of weights, bias, and activation functions,
    which is our next topic of discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Weights and biases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Weights in an ANN are the most important factor in converting an input to impact
    the output. This is similar to slope in linear regression, where a weight is multiplied
    to the input to add up to form the output. Weights are numerical parameters which
    determine how strongly each of the neurons affects the other.
  prefs: []
  type: TYPE_NORMAL
- en: For a typical neuron, if the inputs are *x[1]*, *x[2]*, and *x[3]*, then the
    synaptic weights to be applied to them are denoted as *w[1]*, *w[2]*, and *w[3]*.
  prefs: []
  type: TYPE_NORMAL
- en: Output is
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00009.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: where *i* is *1* to the number of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Simply, this is a matrix multiplication to arrive at the weighted sum.
  prefs: []
  type: TYPE_NORMAL
- en: Bias is like the intercept added in a linear equation. It is an additional parameter
    which is used to adjust the output along with the weighted sum of the inputs to
    the neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'The processing done by a neuron is thus denoted as :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00010.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A function is applied on this output and is called an **activation function**.
    The input of the next layer is the output of the neurons in the previous layer,
    as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00011.gif)'
  prefs: []
  type: TYPE_IMG
- en: Training neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training is the act of presenting the network with some sample data and modifying
    the weights to better approximate the desired function.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main types of training: supervised learning and unsupervised
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We supply the neural network with inputs and the desired outputs. Response of
    the network to the inputs is measured. The weights are modified to reduce the
    difference between the actual and desired outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We only supply inputs. The neural network adjusts its own weights, so that similar
    inputs cause similar outputs. The network identifies the patterns and differences
    in the inputs without any external assistance.
  prefs: []
  type: TYPE_NORMAL
- en: Epoch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One iteration or pass through the process of providing the network with an input
    and updating the network's weights is called an **epoch**. It is a full run of
    feed-forward and backpropagation for update of weights. It is also one full read
    through of the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, many epochs, in the order of tens of thousands at times, are required
    to train the neural network efficiently. We will see more about epochs in the
    forthcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The abstraction of the processing of neural networks is mainly achieved through
    the activation functions. An activation function is a mathematical function which
    converts the input to an output, and adds the magic of neural network processing.
    Without activation functions, the working of neural networks will be like linear
    functions. A linear function is one where the output is directly proportional
    to input, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00012.jpeg)![](img/00013.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A linear function is a polynomial of one degree. Simply, it is a straight line
    without any curves.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, most of the problems the neural networks try to solve are nonlinear
    and complex in nature. To achieve the nonlinearity, the activation functions are
    used. Nonlinear functions are high degree polynomial functions, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00014.jpeg)![](img/00015.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The graph of a nonlinear function is curved and adds the complexity factor.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions give the nonlinearity property to neural networks and make
    them true universal function approximators.
  prefs: []
  type: TYPE_NORMAL
- en: Different activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many activation functions available for a neural network to use. We
    shall see a few of them here.
  prefs: []
  type: TYPE_NORMAL
- en: Linear function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest activation function, one that is commonly used for the output
    layer activation function in neural network problems, is the linear activation
    function represented by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00016.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The output is same as the input and the function is defined in the range (*-infinity,
    +infinity*). In the following figure, a linear activation function is shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00017.gif)'
  prefs: []
  type: TYPE_IMG
- en: Unit step activation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A unit step activation function is a much-used feature in neural networks.
    The output assumes value *0* for negative argument and *1* for positive argument.
    The function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The range is between *(0,1)* and the output is binary in nature. These types
    of activation functions are useful for binary schemes. When we want to classify
    an input model in one of two groups, we can use a binary compiler with a unit
    step activation function. A unit step activation function is shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00019.gif)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *sigmoid* function is a mathematical function that produces a sigmoidal
    curve; a characteristic curve for its *S* shape. This is the earliest and often
    used activation function. This squashes the input to any value between *0* and
    *1*, and makes the model logistic in nature. This function refers to a special
    case of logistic function defined by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following figure is shown a sigmoid curve with an *S* shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00021.gif)'
  prefs: []
  type: TYPE_IMG
- en: Hyperbolic tangent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another very popular and widely used activation feature is the *tanh* function.
    If you look at the figure that follows, you can notice that it looks very similar
    to *sigmoid*; in fact, it is a scaled *sigmoid* function. This is a nonlinear
    function, defined in the range of values *(-1, 1)*, so you need not worry about
    activations blowing up. One thing to clarify is that the gradient is stronger
    for *tanh* than *sigmoid* (the derivatives are more steep). Deciding between *sigmoid*
    and *tanh* will depend on your gradient strength requirement. Like the *sigmoid*,
    *tanh* also has the missing slope problem. The function is defined by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following figure is shown a hyberbolic tangent activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00023.gif)'
  prefs: []
  type: TYPE_IMG
- en: This looks very similar to *sigmoid*; in fact, it is a scaled *sigmoid* function.
  prefs: []
  type: TYPE_NORMAL
- en: Rectified Linear Unit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Rectified Linear Unit** (**ReLU**) is the most used activation function since
    2015\. It is a simple condition and has advantages over the other functions. The
    function is defined by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following figure is shown a ReLU activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00025.gif)'
  prefs: []
  type: TYPE_IMG
- en: The range of output is between *0* and infinity. ReLU finds applications in
    computer vision and speech recognition using deep neural nets. There are various
    other activation functions as well, but we have covered the most important ones
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Which activation functions to use?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given that neural networks are to support nonlinearity and more complexity,
    the activation function to be used has to be robust enough to have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It should be differential; we will see why we need differentiation in backpropagation.
    It should not cause gradients to vanish.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be simple and fast in processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should not be zero centered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *sigmoid* is the most used activation function, but it suffers from the
    following setbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: Since it uses logistic model, the computations are time consuming and complex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It cause gradients to vanish and no signals pass through the neurons at some
    point of time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is slow in convergence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not zero centered
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These drawbacks are solved by ReLU. ReLU is simple and is faster to process.
    It does not have the vanishing gradient problem and has shown vast improvements
    compared to the *sigmoid* and *tanh* functions. ReLU is the most preferred activation
    function for neural networks and DL problems.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU is used for hidden layers, while the output layer can use a `softmax` function
    for logistic problems and a linear function of regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron and multilayer architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A perceptron is a single neuron that classifies a set of inputs into one of
    two categories (usually *1* or *-1*). If the inputs are in the form of a grid,
    a perceptron can be used to recognize visual images of shapes. The perceptron
    usually uses a step function, which returns *1* if the weighted sum of the inputs
    exceeds a threshold, and *0* otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: When layers of perceptron are combined together, they form a multilayer architecture,
    and this gives the required complexity of the neural network processing. **Multi-Layer
    Perceptrons** (**MLPs**) are the most widely used architecture for neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Forward and backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The processing from input layer to hidden layer(s) and then to the output layer
    is called **forward propagation**. The *sum(input*weights)+bias* is applied at
    each layer and then the activation function value is propagated to the next layer.
    The next layer can be another hidden layer or the output layer. The construction
    of neural networks uses large number of hidden layers to give rise to **Deep Neural
    Network** (**DNN**).
  prefs: []
  type: TYPE_NORMAL
- en: Once the output is arrived at, at the last layer (the output layer), we compute
    the error (the predicted output minus the original output). This error is required
    to correct the weights and biases used in forward propagation. Here is where the
    derivative function is used. The amount of weight that has to be changed is determined
    by **gradient descent**.
  prefs: []
  type: TYPE_NORMAL
- en: The backpropagation process uses the partial derivative of each neuron's activation
    function to identify the slope (or gradient) in the direction of each of the incoming
    weights. The gradient suggests how steeply the error will be reduced or increased
    for a change in the weight. The backpropagation keeps changing the weights until
    there is greatest reduction in errors by an amount known as the **learning rate**.
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate is a scalar parameter, analogous to step size in numerical integration,
    used to set the rate of adjustments to reduce the errors faster. Learning rate
    is used in backpropagation during adjustment of weights and bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'More the learning rate, the faster the algorithm will reduce the errors and
    faster will be the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Step-by-step illustration of a neuralnet and an activation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We shall take a step-by-step approach to understand the forward and reverse
    pass with a single hidden layer. The input layer has one neuron and the output
    will solve a binary classification problem (predict 0 or 1). In the following
    figure is shown a forward and reverse pass with a single hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let us analyze in detail, step by step, all the operations to be done
    for network training:'
  prefs: []
  type: TYPE_NORMAL
- en: Take the input as a matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the weights and biases with random values. This is one time and we
    will keep updating these with the error propagation process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the steps 4 to 9 for each training pattern (presented in random order),
    until the error is minimized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the inputs to the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the output for every neuron from the input layer, through the hidden
    layer(s), to the output layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the error at the outputs: actual minus predicted.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the output error to compute error signals for previous layers. The partial
    derivative of the activation function is used to compute the error signals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the error signals to compute weight adjustments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the weight adjustments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps 4 and 5 are forward propagation and steps 6 through 9 are backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate is the amount that weights are updated is controlled by a
    configuration parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The complete pass back and forth is called a **training cycle** or **epoch**.
    The updated weights and biases are used in the next cycle. We keep recursively
    training until the error is very minimal.
  prefs: []
  type: TYPE_NORMAL
- en: We shall cover more about the forward and backpropagation in detail throughout
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Feed-forward and feedback networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The flow of the signals in neural networks can be either in only one direction
    or in recurrence. In the first case, we call the neural network architecture feed-forward,
    since the input signals are fed into the input layer, then, after being processed,
    they are forwarded to the next layer, just as shown in the following figure. MLPs
    and radial basis functions are also good examples of feed-forward networks. In
    the following figure is shown an MLPs architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00028.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'When the neural network has some kind of internal recurrence, meaning that
    the signals are fed back to a neuron or layer that has already received and processed
    that signal, the network is of the type feedback, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The special reason to add recurrence in a network is the production of a dynamic
    behavior, particularly when the network addresses problems involving time series
    or pattern recognition, that require an internal memory to reinforce the learning
    process. However, such networks are particularly difficult to train, eventually
    failing to learn. Most of the feedback networks are single layer, such as the
    **Elman** and **Hopfield** networks, but it is possible to build a recurrent multilayer
    network, such as echo and recurrent MLP networks.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gradient descent is an iterative approach for error correction in any learning
    model. For neural networks during backpropagation, the process of iterating the
    update of weights and biases with the error times derivative of the activation
    function is the gradient descent approach. The steepest descent step size is replaced
    by a similar size from the previous step. Gradient is basically defined as the
    slope of the curve and is the derivative of the activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The objective of deriving gradient descent at each step is to find the global
    cost minimum, where the error is the lowest. And this is where the model has a
    good fit for the data and predictions are more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent can be performed either for the full batch or stochastic. In
    full batch gradient descent, the gradient is computed for the full training dataset,
    whereas **Stochastic Gradient Descent** (**SGD**) takes a single sample and performs
    gradient calculation. It can also take mini-batches and perform the calculations.
    One advantage of SGD is faster computation of gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Taxonomy of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The basic foundation for ANNs is the same, but various neural network models
    have been designed during its evolution. The following are a few of the ANN models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adaptive Linear Element** (**ADALINE**), is a simple perceptron which can
    solve only linear problems. Each neuron takes the weighted linear sum of the inputs
    and passes it to a bi-polar function, which either produces a *+1* or *-1* depending
    on the sum. The function checks the sum of the inputs passed and if the net is
    *>= 0*, it is *+1*, else it is *-1*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple ADALINEs** (**MADALINE**), is a multilayer network of ADALINE units.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perceptrons are single layer neural networks (single neuron or unit), where
    the input is multidimensional (vector) and the output is a function on the weight
    sum of the inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radial basis function network is an ANN where a radial basis function is used
    as an activation function. The network output is a linear combination of radial
    basis functions of the inputs and some neuron parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feed-forward is the simplest form of neural networks. The data is processed
    across layers without any loops are cycles. We will study the following feed-
    forward networks in this book:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoder
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Probabilistic
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Time delay
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Covolutional
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**), unlike feed-forward networks, propagate
    data forward and also backwards from later processing stages to earlier stages.
    The following are the types of RNNs; we shall study them in our later chapters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopfield networks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Boltzmann machine
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self Organizing Maps** (**SOMs**)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bidirectional Associative Memory** (**BAM**)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long Short Term Memory** (**LSTM**)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following images depict **(a) Recurrent neural network** and **(b) Forward
    neural network**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00031.gif)'
  prefs: []
  type: TYPE_IMG
- en: Simple example using R neural net library - neuralnet()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider a simple dataset of a square of numbers, which will be used to train
    a `neuralnet` function in R and then test the accuracy of the built neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **INPUT** | **OUTPUT** |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `0` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `1` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `4` |'
  prefs: []
  type: TYPE_TB
- en: '| `3` | `9` |'
  prefs: []
  type: TYPE_TB
- en: '| `4` | `16` |'
  prefs: []
  type: TYPE_TB
- en: '| `5` | `25` |'
  prefs: []
  type: TYPE_TB
- en: '| `6` | `36` |'
  prefs: []
  type: TYPE_TB
- en: '| `7` | `49` |'
  prefs: []
  type: TYPE_TB
- en: '| `8` | `64` |'
  prefs: []
  type: TYPE_TB
- en: '| `9` | `81` |'
  prefs: []
  type: TYPE_TB
- en: '| `10` | `100` |'
  prefs: []
  type: TYPE_TB
- en: 'Our objective is to set up the weights and bias so that the model can do what
    is being done here. The output needs to be modeled on a function of input and
    the function can be used in future to determine the output based on an input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let us go through the code line-by-line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand all the steps in the code just proposed, we will look at them
    in detail. Do not worry if a few steps seem unclear at this time, you will be
    able to look into it in the following examples. First, the code snippet will be
    shown, and the explanation will follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The line in R includes the library `neuralnet()` in our program. `neuralnet()`
    is part of **Comprehensive R Archive Network** (**CRAN**), which contains numerous
    R libraries for various applications.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This reads the CSV file with separator `,`(comma), and header is the first line
    in the file. `names()` would display the header of the file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The training of the output with respect to the input happens here. The `neuralnet()`
    library is passed the output and input column names (`ouput~input`), the dataset
    to be used, the number of neurons in the hidden layer, and the stopping criteria
    (`threshold`).
  prefs: []
  type: TYPE_NORMAL
- en: 'A brief description of the `neuralnet` package, extracted from the official
    documentation, is shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **neuralnet-package**: |'
  prefs: []
  type: TYPE_TB
- en: '| **Description**: |'
  prefs: []
  type: TYPE_TB
- en: '| Training of neural networks using the backpropagation, resilient backpropagation
    with (Riedmiller, 1994) or without weight backtracking (Riedmiller, 1993), or
    the modified globally convergent version by Anastasiadis et al. (2005). The package
    allows flexible settings through custom-choice of error and activation function.
    Furthermore, the calculation of generalized weights (Intrator O & Intrator N,
    1993) is implemented. |'
  prefs: []
  type: TYPE_TB
- en: '| **Details**: |'
  prefs: []
  type: TYPE_TB
- en: '| Package: `neuralnet`Type: PackageVersion: 1.33Date: 2016-08-05License: GPL
    (>=2) |'
  prefs: []
  type: TYPE_TB
- en: '| **Authors**: |'
  prefs: []
  type: TYPE_TB
- en: '| Stefan Fritsch, Frauke Guenther (email: `guenther@leibniz-bips.de`)Maintainer:
    Frauke Guenther (email: `guenther@leibniz-bips.de`) |'
  prefs: []
  type: TYPE_TB
- en: '| **Usage**: |'
  prefs: []
  type: TYPE_TB
- en: '| `neuralnet(formula, data, hidden = 1, threshold = 0.01, stepmax = 1e+05,
    rep = 1, startweights = NULL, learningrate.limit = NULL, learningrate.factor =
    list(minus = 0.5, plus = 1.2), learningrate=NULL, lifesign = "none", lifesign.step
    = 1000, algorithm = "rprop+", err.fct = "sse", act.fct = "logistic", linear.output
    = TRUE, exclude = NULL,` `constant.weights = NULL, likelihood = FALSE)` |'
  prefs: []
  type: TYPE_TB
- en: '| **Meaning of the arguments**: |'
  prefs: []
  type: TYPE_TB
- en: '| `formula`: A symbolic description of the model to be fitted.`data`: A dataframe
    containing the variables specified in formula.`hidden`: A vector of integers specifying
    the number of hidden neurons (vertices) in each layer.`threshold`: A numeric value
    specifying the threshold for the partial derivatives of the error function as
    stopping criteria.`stepmax`: The maximum steps for the training of the neural
    network. Reaching this maximum leads to a stop of the neural network''s training
    process.`rep`: The number of repetitions for the neural network''s training.`startweights`:
    A vector containing starting values for the weights. The weights will not be randomly
    initialized.`learningrate.limit`: A vector or a list containing the lowest and
    highest limit for the learning rate. Used only for `RPROP` and `GRPROP`.`learningrate.factor`:
    A vector or a list containing the multiplication factors for the upper and lower
    learning rate, used only for `RPROP` and `GRPROP`.`learningrate`: A numeric value
    specifying the learning rate used by traditional backpropagation. Used only for
    traditional backpropagation.`lifesign`: A string specifying how much the function
    will print during the calculation of the neural network-`''none''`, `''minimal''`,
    or `''full''`.`lifesign.step`: An integer specifying the step size to print the
    minimal threshold in full lifesign mode.`algorithm`: A string containing the algorithm
    type to calculate the neural network.`err.fct`: A differentiable function that
    is used for the calculation of the error.`act.fct`: A differentiable function
    that is used for smoothing the result of the cross product of the covariate or
    neurons and the weights.`linear.output`: Logical. If `act.fct` should not be applied
    to the output neurons set linear output to `TRUE`, otherwise to `FALSE`.`exclude`:
    A vector or a matrix specifying the weights that are excluded from the calculation.`constant.weights`:
    A vector specifying the values of the weights that are excluded from the training
    process and treated as fix.`likelihood`: Logical. If the error function is equal
    to the negative log-likelihood function, the information criteria AIC and BIC
    will be calculated. Furthermore the usage of confidence. interval is meaningful.
    |'
  prefs: []
  type: TYPE_TB
- en: 'After giving a brief glimpse into the package documentation, let''s review
    the remaining lines of the proposed code sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This command prints the model that has just been generated, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go back to the code analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This preceding command plots the neural network for us, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00032.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This preceding code prints the final output, comparing the output predicted
    and actual as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Implementation using nnet() library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To improve our practice with the `nnet` library, we look at another example.
    This time we will use the data collected at a restaurant through customer interviews.
    The customers were asked to give a score to the following aspects: service, ambience,
    and food. They were also asked whether they would leave the tip on the basis of
    these scores. In this case, the number of inputs is `2` and the output is a categorical
    value (`Tip=1` and `No-tip=0`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The input file to be used is shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **No** | **CustomerWillTip** | **Service** | **Ambience** | **Food** | **TipOrNo**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `1` | `4` | `4` | `5` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `1` | `6` | `4` | `4` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `3` | `1` | `5` | `2` | `4` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `4` | `1` | `6` | `5` | `5` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `5` | `1` | `6` | `3` | `4` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `6` | `1` | `3` | `4` | `5` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `7` | `1` | `5` | `5` | `5` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `8` | `1` | `5` | `4` | `4` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `9` | `1` | `7` | `6` | `4` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `10` | `1` | `7` | `6` | `4` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `11` | `1` | `6` | `7` | `2` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `12` | `1` | `5` | `6` | `4` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `13` | `1` | `7` | `3` | `3` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `14` | `1` | `5` | `1` | `4` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `15` | `1` | `7` | `5` | `5` | `Tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `16` | `0` | `3` | `1` | `3` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `17` | `0` | `4` | `6` | `2` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `18` | `0` | `2` | `5` | `2` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `19` | `0` | `5` | `2` | `4` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `20` | `0` | `4` | `1` | `3` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `21` | `0` | `3` | `3` | `4` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `22` | `0` | `3` | `4` | `5` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `23` | `0` | `3` | `6` | `3` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `24` | `0` | `4` | `4` | `2` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `25` | `0` | `6` | `3` | `6` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `26` | `0` | `3` | `6` | `3` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `27` | `0` | `4` | `3` | `2` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `28` | `0` | `3` | `5` | `2` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `29` | `0` | `5` | `5` | `3` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: '| `30` | `0` | `1` | `3` | `2` | `No-tip` |'
  prefs: []
  type: TYPE_TB
- en: 'This is a classification problem with three inputs and one categorical output.
    We will address the problem with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Let us go through the code line-by-line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand all the steps in the code just proposed, we will look at them
    in detail. First, the code snippet will be shown, and the explanation will follow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This includes the libraries `NeuralNetTools` and `nnet()` for our program.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This sets the working directory and reads the input CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This calls the `nnet()` function with the arguments passed. The output is as
    follows. `nnet()` processes the forward and backpropagation until convergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'A brief description of the `nnet` package, extracted from the official documentation,
    is shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **nnet-package**: Feed-forward neural networks and multinomial log-linear
    models |'
  prefs: []
  type: TYPE_TB
- en: '| **Description**: |'
  prefs: []
  type: TYPE_TB
- en: '| Software for feed-forward neural networks with a single hidden layer, and
    for multinomial log-linear models. |'
  prefs: []
  type: TYPE_TB
- en: '| **Details**: |'
  prefs: []
  type: TYPE_TB
- en: '| Package: `nnet` Type: Package'
  prefs: []
  type: TYPE_NORMAL
- en: 'Version: 7.3-12'
  prefs: []
  type: TYPE_NORMAL
- en: 'Date: 2016-02-02'
  prefs: []
  type: TYPE_NORMAL
- en: 'License: GPL-2 &#124; GPL-3 |'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Author(s)**: |'
  prefs: []
  type: TYPE_TB
- en: '| *Brian Ripley* *William Venables* |'
  prefs: []
  type: TYPE_TB
- en: '| **Usage**: |'
  prefs: []
  type: TYPE_TB
- en: '| `nnet(formula, data, weights,subset, na.action, contrasts = NULL)` |'
  prefs: []
  type: TYPE_TB
- en: '| **Meaning of the arguments**: |'
  prefs: []
  type: TYPE_TB
- en: '| `Formula`: A formula of the form class *~ x1 + x2 + ...* `data`: Dataframe
    from which variables specified in formula are preferentially to be taken'
  prefs: []
  type: TYPE_NORMAL
- en: '`weights`: (Case) weights for each example; if missing, defaults to *1*'
  prefs: []
  type: TYPE_NORMAL
- en: '`subset`: An index vector specifying the cases to be used in the training sample'
  prefs: []
  type: TYPE_NORMAL
- en: '`na.action`: A function to specify the action to be taken if NAs are found'
  prefs: []
  type: TYPE_NORMAL
- en: '`contrasts:` A list of contrasts to be used for some or all of the factors
    appearing as variables in the model formula |'
  prefs: []
  type: TYPE_NORMAL
- en: 'After giving a brief glimpse into the package documentation, let''s review
    the remaining lines of the proposed in the following code sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This command prints the details of the `net()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To plot the `model`, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot of the `model` is as follows; there are five nodes in the single hidden
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using `NeuralNetTools`, it''s possible to obtain the relative importance of
    input variables in neural networks using `garson` algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This command prints the various input parameters and their importance to the
    output prediction, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/00034.jpeg)**'
  prefs: []
  type: TYPE_NORMAL
- en: From the chart obtained from the application of the Garson algorithm, it is
    possible to note that, in the decision to give the tip, the service received by
    the customers has the greater influence.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen two neural network libraries in R and used them in simple examples.
    We would deep dive with several practical use cases throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DL forms an advanced neural network with numerous hidden layers. DL is a vast
    subject and is an important concept for building AI. It is used in various applications,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Image recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handwriting detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiclass classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression problems, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We would see more about DL with R in the future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Pros and cons of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks form the basis of DL, and applications are enormous for DL,
    ranging from voice recognition to cancer detection. The pros and cons of neural
    networks are described in this section. The pros outweigh the cons and give neural
    networks as the preferred modeling technique for data science, machine learning,
    and predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Pros
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some of the advantages of neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are flexible and can be used for both regression and classification
    problems. Any data which can be made numeric can be used in the model, as neural
    network is a mathematical model with approximation functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks are good to model with nonlinear data with large number of inputs;
    for example, images. It is reliable in an approach of tasks involving many features.
    It works by splitting the problem of classification into a layered network of
    simpler elements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once trained, the predictions are pretty fast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks can be trained with any number of inputs and layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks work best with more data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us take a look at some of the cons of neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are black boxes, meaning we cannot know how much each independent
    variable is influencing the dependent variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is computationally very expensive and time consuming to train with traditional
    CPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks depend a lot on training data. This leads to the problem of
    over-fitting and generalization. The mode relies more on the training data and
    may be tuned to the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices in neural network implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some best practices that will help in the implementation
    of neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are best implemented when there is good training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More the hidden layers in an MLP, the better the accuracy of the model for predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is best to have five nodes in the hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU and **Sum of Square of Errors** (**SSE**) are respectively best techniques
    for activation function and error deduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quick note on GPU processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The increase in processing capabilities has been a tremendous booster for usage
    of neural networks in day-to-day problems. GPU is a specialized processor designed
    to perform graphical operations (for example, gaming, 3D animation, and so on).
    They perform mathematically intensive tasks and are additional to the CPU. The
    CPU performs the operational tasks of the computer, while the GPU is used to perform
    heavy workload processing.
  prefs: []
  type: TYPE_NORMAL
- en: The neural network architecture needs heavy mathematical computational capabilities
    and GPU is the preferred candidate here. The vectorized dot matrix product between
    the weights and inputs at every neuron can be run in parallel through GPUs. The
    advancements in GPUs is popularizing neural networks. The applications of DL in
    image processing, computer vision, bioinformatics, and weather modeling are benefiting
    through GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw an overview of ANNs. Neural networks implementation
    is simple, but the internals are pretty complex. We can summarize neural network
    as a universal mathematical function approximation. Any set of inputs which produce
    outputs can be made a black box mathematical function through a neural network,
    and the applications are enormous in the recent years.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw the following in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Neural network is a machine learning technique and is data-driven
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI, machine learning, and neural networks are different paradigms of making
    machines work like humans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks can be used for both supervised and unsupervised machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights, biases, and activation functions are important concepts in neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks are nonlinear and non-parametric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks are very fast in prediction and are most accurate in comparison
    with other machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are input, hidden, and output layers in any neural network architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Neural networks are based on building MLP, and we understood the basis for
    neural networks: weights, bias, activation functions, feed-forward, and backpropagation
    processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward and backpropagation are techniques to derive a neural network model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks can be implemented through many programming languages, namely
    Python, R, MATLAB, C, and Java, among others. The focus of this book will be building
    applications using R. DNN and AI systems are evolving on the basis of neural networks.
    In the forthcoming chapter, we will drill through different types of neural networks
    and their various applications.
  prefs: []
  type: TYPE_NORMAL
