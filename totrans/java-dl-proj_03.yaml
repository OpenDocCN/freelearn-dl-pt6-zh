- en: Multi-Label Image Classification Using Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we developed a project that accurately classifies cancer
    patients based on cancer types using an LSTM network. This is a challenging problem
    in biomedical informatics. Unfortunately, when it comes to classifying multimedia
    objects such as images, audio, or videos, linear ML models and other regular **deep
    neural network** (**DNN**) models, such as **Multilayer Perceptron** (**MLP**)
    or **Deep Belief Networks** (**DBN**), often fail to learn or model non-linear
    features from images.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, **convolutional neural networks** (**CNNs**) can be utilized
    to overcome these limitations. In CNNs, the connectivity pattern between neurons
    is inspired by the human visual cortex, which more accurately resembles human
    vision, so it is perfect for image processing-related tasks. Consequently, CNNs
    have shown outstanding successes in numerous domains: computer vision, NLP, multimedia
    analytics, image searches, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering this motivation, in this chapter, we will see how to develop an
    end-to-end project for handling multi-label (that is, each entity can belong to
    multiple classes) image classification problems using CNNs based on the Scala
    and **Deeplearning4j** (**DL4J**) frameworks on real Yelp image datasets. We will
    also discuss some theoretical aspects of CNNs before getting started. Nevertheless,
    we will discuss how to tune hyperparameters for better classification results.
    Concisely, we will learn the following topics throughout our end-to-end project:'
  prefs: []
  type: TYPE_NORMAL
- en: Drawbacks of regular DNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CNN architectures: convolution operations and pooling layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large-scale image classification using CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequently asked questions (FAQs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image classification and drawbacks of DNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this project, we will show a step-by-step example of developing real-life
    ML projects for image classification using Scala and CNN. One such image data
    source is Yelp, where there are many photos and many users uploading photos. These
    photos provide rich local business information across categories. Thus, using
    these photos, developing an ML application by understanding the context of these
    photos is not an easy task. We will see how to use the DL4j platform to do so
    using Java. However, some theoretical background is a prior mandate before we
    start formally.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start developing the end-to-end project for image classification using
    CNN, let's take a look at the drawbacks of regular DNNs. Although regular DNNs
    work fine for small images (for example, MNIST and CIFAR-10), it breaks down for
    large-scale and high-quality images because of the huge number of hyperparameters
    it requires. For example, a 200 × 200 image has 40,000 pixels, and if the first
    layer has just 2,000 neurons, this means there will have 80 million different
    connections just in the first layer. Thus, if your network is very deep, there
    might be even billions of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs solve this problem using partially connected layers. Because consecutive
    layers are only partially connected and because it heavily reuses its weights,
    a CNN has far fewer parameters than a fully connected DNN, which makes it much
    faster to train, reduces the risk of overfitting, and requires much less training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, when a CNN has learned a kernel that can detect a particular feature,
    it can detect that feature anywhere on the image. In contrast, when a DNN learns
    a feature in one location, it can detect it only in that particular location.
    Since images typically have very repetitive features, CNNs are able to generalize
    much better than DNNs for image processing tasks such as classification, using
    fewer training examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Importantly, DNN has no prior knowledge of how pixels are organized: it does
    not know that nearby pixels are close. A CNN''s architecture embeds this prior
    knowledge. Lower layers typically identify features in small areas of the images,
    while higher layers combine the lower-level features into larger features. This
    works well with most natural images, giving CNNs a decisive head start compared
    to DNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6a9a8ed-d7aa-4f6e-9d9e-854ca4fd78b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Regular DNN versus CNN where each layer has neurons arranged in 3D
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the preceding diagram, on the left, you can see a regular three-layer
    neural network. On the right, a ConvNet arranges its neurons into three dimensions
    (width, height, and depth), as visualized in one of the layers. Every layer of
    a `CNN` transforms the 3D structure into a 3D output structure of neuron activations.
    The red input layer holds the image, so its width and height would be the dimensions
    of the image, and the depth would be three (red, green, and blue channels).
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, all the multilayer neural networks we looked at had layers composed
    of a long line of neurons, and we had to flatten input images to 1D before feeding
    them to the network. However, feeding 2D images directly to CNNs is possible since
    each layer in CNN is represented in 2D, which makes it easier to match neurons
    with their corresponding inputs. We will see examples of this in the upcoming
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Another important fact is that all the neurons in a feature map share the same
    parameters, so it dramatically reduces the number of parameters in the model.
    Also, more importantly, once a CNN has learned to recognize a pattern in one location,
    it can do the same for other locations as well.
  prefs: []
  type: TYPE_NORMAL
- en: CNN architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In CNN networks, the way connectivity is defined among layers is significantly
    different compared to MLP or DBN. The **convolutional** (**conv**) layer is the
    main type of layer in a CNN, where each neuron is connected to a certain region
    of the input image, which is called a **receptive field**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be more specific, in a CNN architecture, a few conv layers are connected
    in a cascade style: each layer is followed by a **rectified linear unit** (**ReLU**)
    layer, then a pooling layer, then a few more conv layers (+ReLU), then another
    pooling layer, and so on. The output from each conv layer is a set of objects
    called feature maps, which are generated by a single kernel filter. Then, the
    feature maps are fed to the next layer as a new input. In the fully connected
    layer, each neuron produces an output followed by an activation layer (that is,
    the Softmax layer):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50345908-1f46-4a96-801b-e62f4648cf8b.png)'
  prefs: []
  type: TYPE_IMG
- en: A conceptual architecture of CNN
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding diagram, the pooling layers are usually placed
    after the convolutional layers (that is, between two such layers). A pooling layer
    into sub-regions then divides the convolutional region. Then, a single representative
    value is selected using either a max-pooling or an average pooling technique to
    reduce the computational time of subsequent layers. This way, a CNN can be thought
    of as a feature extractor. To understand this more clearly, refer to the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f436e2e8-09eb-4a3b-8910-1f3389993208.png)'
  prefs: []
  type: TYPE_IMG
- en: A CNN is an end-to-end network that acts as both a feature extractor and a classifier.
    This way, it can accurately identify (under the given condition that it gets sufficient
    training data) the label of a given input image. For example, it can classify
    that the input image is really a tiger.
  prefs: []
  type: TYPE_NORMAL
- en: The robustness of the feature with respect to its spatial position is increased
    too. To be more specific, when feature maps are used as image properties and pass
    through the grayscale image, it gets smaller and smaller as it progresses through
    the network, but it also typically gets deeper and deeper since more feature maps
    will be added. The convolution operation brings a solution to this problem as
    it reduces the number of free parameters, allowing the network to be deeper with
    fewer parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A convolution is a mathematical operation that slides one function over another
    and measures the integrity of their pointwise multiplication. Convolutional layers
    are probably the most important building blocks in a CNN. For the first conv layer,
    neurons are not connected to every single pixel in the input image, but only to
    pixels in their receptive fields (refer to the preceding diagram), whereas each
    neuron in the second conv layer is only connected to neurons located within a
    small rectangle in the first layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48f7701f-6398-490e-ba5d-65ac689f0d22.png)'
  prefs: []
  type: TYPE_IMG
- en: Each convolutional neuron only processes data for its receptive field
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](e27fb252-7892-4659-81e2-2289de8ce570.xhtml), *Cancer Types Prediction
    Using Recurrent Type Networks,* we have seen that all multilayer neural networks
    (for example, MLP) have layers composed of so many neurons, and we had to flatten
    input images to 1D before feeding them to the network. Instead, in a CNN, each
    layer is represented in 2D, which makes it easier to match neurons with their
    associated inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The receptive field is used to exploit spatial locality by enforcing a local
    connectivity pattern between neurons of adjacent layers.
  prefs: []
  type: TYPE_NORMAL
- en: This architecture allows the network to concentrate on low-level features in
    the first hidden layer, and then assemble them into higher-level features in the
    next hidden layer, and so on. This hierarchical structure is common in real-world
    images, which is one of the reasons why CNNs work so well for image recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling and padding operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you understand how convolutional layers work, pooling layers are quite
    easy to grasp. A pooling layer typically works on every input channel independently,
    so the output depth is the same as the input depth. Alternatively, you may pool
    over the depth dimension, as we will see next, in which case the image''s spatial
    dimensions (for example, height and width) remain unchanged, but the number of
    channels is reduced. Let''s see a formal definition of pooling layers from TensorFlow
    API documentation (see more at [https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/python/ops/nn.py](https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/python/ops/nn.py)):'
  prefs: []
  type: TYPE_NORMAL
- en: '"The pooling ops sweep a rectangular window over the input tensor, computing
    a reduction operation for each window (average, max, or max with argmax). Each
    pooling op uses rectangular windows of size called ksize separated by offset strides.
    For example, if strides are all ones, every window is used, if strides are all
    twos, every other window is used in each dimension, and so on."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to a conv layer, each neuron in a pooling layer is connected to the
    outputs of a limited number of neurons in the previous layer that are located
    within a small rectangular receptive field. However, the size, the stride, and
    the padding type have to be defined. So, in summary, the output from a pooling
    layer can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: where indices are also taken into consideration along with the padding values.
    In other words, the goal of using pooling is to subsample the input image in order
    to reduce the computational load, the memory usage, and the number of parameters.
    This helps to avoid overfitting in the training stage.
  prefs: []
  type: TYPE_NORMAL
- en: A pooling neuron has no weights. Therefore, it only aggregate the inputs using
    an aggregation function such as the max or mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'The spatial semantics of the convolution ops depend on the padding scheme chosen.
    Padding is an operation to increase the size of the input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**For 1D input**: Just an array is appended with a constant, say, `c`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For a 2D input**: A matrix that is surrounded with `c`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For a milt-dimensional (that is, nD) input**: The nD hypercube is surrounded
    with `c`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, the question is, what''s this constant `c`? Well, in most of the cases
    (but not always), `c` is zero called **zero padding**. This concept can be further
    broken down into two types of padding called `VALID` and `SAME`, which are outlined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VALID padding**: Only drops the right-most columns (or bottom-most rows).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SAME padding**: In this scheme, padding is applied evenly or both left and
    right. However, if the number of columns to be added is odd, then an extra column
    is added to the right.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've explained the previous definition graphically in the following diagram.
    If we want a layer to have the same height and width as the previous layer, it
    is common to add zeros around the inputs. This is called `SAME` or zero padding.
  prefs: []
  type: TYPE_NORMAL
- en: The term `SAME` means that the output feature map has the same spatial dimensions
    as the input feature map.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, zero padding is introduced to make the shapes match as needed,
    equally on every side of the input map. On the other hand, `VALID` means no padding
    and only drops the right-most columns (or bottom-most rows):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b37960ea-1ba3-4df2-9d86-9017f1424776.png)'
  prefs: []
  type: TYPE_IMG
- en: SAME versus VALID padding with CNN
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we use a 2 × 2 pooling kernel, a stride of 2 with
    no padding. Only the max input value in each kernel makes it to the next layer
    since the other inputs are dropped (we will see this later on):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed02b9c1-62ad-479e-8ec0-a2fd75b91cb7.png)'
  prefs: []
  type: TYPE_IMG
- en: An example using max pooling, that is, subsampling
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layer (dense layer)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the top of the stack, a regular fully connected layer (feedforward neural
    network or dense layer) is added, which acts similar to an MLP that might be composed
    of a few fully connected layers (+ReLUs), and the final layer outputs the prediction:
    typically, a Softmax layer is used that outputs estimated class probabilities
    for a multiclass classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Well, up to this point, we have minimum theoretical knowledge about CNNs and
    their architectures for image classification. Now, it is time to do a hands-on
    project, which is about classifying large-scale Yelp images. At Yelp, there are
    many photos and many users uploading photos. These photos provide rich local business
    information across categories. Teaching a computer to understand the context of
    these photos is not an easy task.
  prefs: []
  type: TYPE_NORMAL
- en: Yelp engineers work on deep learning-based image classification projects in-house
    (see more at [https://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html](https://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Multi-label image classification using CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show you a systematic example of developing real-life
    ML projects for image classification. However, we need to know the problem description
    first so as to know what sort of image classification needs to be done. Moreover,
    knowledge about the dataset is a mandate before getting started.
  prefs: []
  type: TYPE_NORMAL
- en: Problem description
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, food selfies and photo-centric social storytelling are becoming social
    trends. Consequently, an enormous amount of selfies that include foods and a picture
    of the restaurant are being uploaded on social media and websites. In many instances,
    food lovers also provide the written reviews that can significantly boost the
    popularity of a business (for example, a restaurant).
  prefs: []
  type: TYPE_NORMAL
- en: For example, millions of unique visitors have visited the Yelp website and have
    written more than 135 million reviews. Besides, many photos and users are uploading
    photos. Nevertheless, business owners can post photos and message their customers.
    This way, Yelp makes money by **selling ads** to those local businesses.
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting fact is that these photos provide rich local business information
    across categories. Thus, developing deep learning applications to understand the
    context of these photos would be a useful task. Take a look at the following screenshot
    to get an insight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/954a1298-4086-4644-9a69-ae3a20bf1ebb.png)'
  prefs: []
  type: TYPE_IMG
- en: Mining some insights about a business from a Yelp dataset
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if we're given photos that belong to a business, we need to build a model
    so that it can tag restaurants with multiple labels of the user-submitted photos
    automatically in order to predict business attributes. Eventually, the goal of
    this project is to turn Yelp pictures into words.
  prefs: []
  type: TYPE_NORMAL
- en: Description of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Yelp dataset for this fun project was downloaded from [https://www.kaggle.com/c/yelp-restaurant-photo-classification](https://www.kaggle.com/c/yelp-restaurant-photo-classification).
    We got permission from the Yelp guys under the condition that the images won't
    be redistributed. However, you need to get usage permission from [https://www.yelp.com/dataset](https://www.yelp.com/dataset).
  prefs: []
  type: TYPE_NORMAL
- en: 'Submitting a review is tricky. When Yelp users want to submit a review, they
    have to select the labels of the restaurants manually from nine different labels
    that are annotated by the Yelp community, which are associated with the dataset.
    These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0`: `good_for_lunch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1`: `good_for_dinner`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2`: `takes_reservations`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`3`: `outdoor_seating`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`4`: `restaurant_is_expensive`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`5`: `has_alcohol`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`6`: `has_table_service`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`7`: `ambience_is_classy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`8`: `good_for_kids`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, this is a multiple label multiclass classification problem, where each
    business can have one or more of the nine characteristics listed previously. Therefore,
    we have to predict these labels as accurately as possible. There are six files
    in the dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_photos.tgz`: Photos to be used as the training set (234,842 images)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_photos.tgz`: Photos to be used as the test set (237,152 images)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_photo_to_biz_ids.csv`: Provides the mapping between the photo ID and
    the business ID (234,842 rows)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_photo_to_biz_ids.csv`: Provides the mapping between the photo ID and
    business the ID (1,190,225 rows)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train.csv`: This is the main training dataset, which includes business IDs
    and their corresponding labels (2,000 rows)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_submission.csv`: A sample submission—reference the correct format for
    your predictions including `business_id` and the corresponding predicted labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing invalid images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I do not know why, but each image folder (train and test) also contains some
    temporary images with the `_*.jpg` name pattern, but not actual images. Therefore,
    I removed them using a UNIX command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, I unzipped and copied each `.csv` file into a folder called `label`.
    Additionally, I moved the training and test images into the `train` and `test`
    folders (that is, inside the `images` folder), respectively. In short, after extraction
    and copying, the following folder structure is used in our projects. Therefore,
    the resulting structure will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84b1fa36-fa32-4d88-9743-09c6f5da2f10.png)'
  prefs: []
  type: TYPE_IMG
- en: Folder structure in the Large Movie Review Dataset
  prefs: []
  type: TYPE_NORMAL
- en: Workflow of the overall project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we already know that this is a multi-label multiclass image classification
    problem, we have to deal with the multiple-instance issue**.** Since DL4J does
    not provide an example of how to solve amulti-label multiclass image classification
    problem, I found Andrew Brooks's blog article (see [http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/](http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/))
    motivation for this project**.**
  prefs: []
  type: TYPE_NORMAL
- en: 'I simply applied the labels of the restaurant to all of the images associated
    with it and treated each image as a separate record. To be more technical, I handled
    each class as a separate binary classification problem. Nevertheless, at the beginning
    of this project, we will see how to read images from `.jpg` format into a matrix
    representation in Java. Then, we will further process and prepare those images
    so that they are feedable by the CNNs. Also, since images do not come with uniform
    shapes and sizes, we need to apply several rounds of image preprocessing ops,
    such as squaring and resizing every image to the uniform dimensions, before we
    apply a grayscale filter to the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04a01dab-a68c-4ec0-83ba-c393c91dc1ea.png)'
  prefs: []
  type: TYPE_IMG
- en: A conceptualized view of a CNN for image classification
  prefs: []
  type: TYPE_NORMAL
- en: Then, we train nine CNNs on training data for each class. Once the training
    is complete, we save the trained model, CNN configurations, and parameters so
    that they can be restored later on. Then, we apply a simple aggregate function
    to assign classes to each restaurant, where each one has multiple images associated
    with it, each with its own vector of probabilities for each of the nine classes.
    Then, we score test data and finally, we evaluate the model using test images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see the structure of each CNN. Well, each network will have two
    convolutional layers, two subsampling layers, one dense layer, and the output
    layer as the dense layer. The first layer is a conv layer, followed by a subsampling
    layer, which is again followed by another conv layer, then a subsampling layer,
    then a dense layer, which is followed by an output layer. We will see each layer''s
    structure later on. In short, the Java class (`YelpImageClassifier.java`) has
    the following workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: We read all the business labels from the `train.csv` file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then read and create a map from the image ID to the business ID as imageID
    | busID
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we generate a list of images from the `photoDir` directory to load and
    process, which helps us to retrieve image IDs of a certain number of images
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then read and process images into a photoID | vector map
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We chain the output of step 3 and step 4 to align the business feature, image
    IDs, and label IDs to extract image features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we construct nine CNNs for nine possible labels in a multi-label setting
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then train all the CNNs and specify the model savings locations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: S*teps 2* to *6* are repeated several times to extract the features from the
    test set as well
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we evaluate the model and save the prediction in a CSV file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let''s see what the preceding steps would look like in a high-level diagram,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ee535a5-af3c-4880-a53f-6d5477b45aff.png)'
  prefs: []
  type: TYPE_IMG
- en: DL4j image processing pipeline for image classification
  prefs: []
  type: TYPE_NORMAL
- en: Too much of a mouthful? Don't worry; we will now see each step in detail. If
    you look at the previous steps carefully, you will see that steps 1 to 5 are image
    processing and feature constructions. Then, step 6 is training nine CNNs and then,
    in step 7, we save the trained CNNs so that we can restore them during result
    submission.
  prefs: []
  type: TYPE_NORMAL
- en: Image preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When I tried to develop this application, I found that the photos are different
    shapes and sizes: some images are tall, some of them are wide, some of them are
    outside, some images are inside, and most of them are food. Also, images come
    in different shapes (most were roughly square, though), of pixel and many of them
    are exactly 500 x 375 in dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9b618f0-ad81-48e5-8a5e-b75997f17044.png)'
  prefs: []
  type: TYPE_IMG
- en: Resized figure (left, the original and tall one, right, the squared one)
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen that CNN cannot work with images with heterogeneous shapes
    and sizes. There are many robust and efficient image processing techniques to
    extract only the **region of interes**t (**ROI**), but honestly, I am not an image-processing
    expert, so I decided to keep this resizing step simple. In simple terms, I made
    all the images square but still, I tried to preserve their quality. The thing
    is that ROIs are cantered in most cases. So, capturing only the middle-most square
    of each image is not a trivial task. Nevertheless, we also need to convert each
    image into a grayscale image. Let's make irregularly shaped images square. Look
    at the preceding image, where the one on the left is the original one and the
    one on the right is the squared one.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have generated a square image, but how did we achieve this? Well, I first
    checked whether the height and the width were the same, and then I resized the
    image. In the other two cases, I cropped the central region. The following method
    does the trick (but feel free to execute the `SquaringImage.java` script to see
    the output):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Well done! Now that all of our training images are squared, the next step is
    to use the import-preprocessing task to resize them all. I decided to make all
    the images 128 x 128 in size. Let''s see what the previous image (the original
    one) looks like after resizing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13daa415-e364-4988-b0c9-35c650fd0b93.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Image resizing (256 x 256, 128 x 128, 64 x 64 and 32 x 32, respectively)
  prefs: []
  type: TYPE_NORMAL
- en: 'The following method does this trick (but feel free to execute the `imageUtils.java`
    script to see a demo):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'By the way, for the image resizing and squaring, I used some built-in package
    for image reading and some third-party packages for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the previous packages, add the following dependencies in a Maven-friendly
    `pom.xml` file (for the complete list of dependencies, refer to the `pom.xml`
    file provided for this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Processing color images is more exciting and effective, and DL4J-based CNNs
    can handle color images, too. However, it's better to simplify the computation
    with the grayscale images. Nevertheless, this way, we can make the overall representation
    simpler and space efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s give an example for our previous step; we resized each 256 x 256 pixel
    image—which is represented by 16,384 features rather than 16,384 x 3 for a color
    image having three RGB channels (execute `GrayscaleConverter.java` to see a demo).
    Let''s see what the converted image would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e7e7c72-9b24-4331-8382-6dc595c1d4d6.png)'
  prefs: []
  type: TYPE_IMG
- en: On the left—the original image, on the right—the grayscale one with RGB averaging
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous conversion is done using two methods called `pixels2Gray()` and
    `makeGray()`. The former converts RGB pixels into corresponding grayscale ones.
    Let''s see the signature for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'So, what happens under the hood? We chain all the previous three steps: make
    all the images square, then convert all of them to 256 x 256, and finally convert
    the resized image into a grayscale one (I assume that `x` is the image to be converted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, in summary, now all of the images are in grey, but only after squaring
    and resizing. The following image gives some sense of the conversion step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fee1cc5f-9747-4161-837c-20e182d2f710.png)'
  prefs: []
  type: TYPE_IMG
- en: Resized figure (left, the original and tall one, right, the squared one)
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous chaining also comes with some additional effort. Now, putting
    all these three coding steps together, we can finally prepare all of our images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Extracting image metadata
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to this point, we have loaded and pre-processed raw images. However, we have
    no idea about the image metadata that is added, which is needed so that our CNNs
    can learn. Thus, it's time to load those CSV files containing metadata about each
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'I wrote a method called `readMetadata()` to read such metadata in CSV format
    so that it can be used by two other methods called `readBusinessLabels` and `readBusinessToImageLabels`.
    These three methods are defined in the `CSVImageMetadataReader.java` script. Here''s
    the signature of the `readMetadata()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `readBusinessLabels()` method maps from the business ID to labels of the
    form businessID | Set(labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `readBusinessToImageLabels()` method maps from the image ID to the business
    ID of the form imageID | businessID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Image feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how to preprocess images and extract image metadata by
    linking them with the original images. Now, we need to extract features from those
    preprocessed images so that they can be fed into CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need the map operations for feature extractions for business, data, and
    labels. These three operations will ensure that we don''t lose any image provenance
    (see the `imageFeatureExtractor.java` script):'
  prefs: []
  type: TYPE_NORMAL
- en: Business mapping with the form imageID | businessID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data map of the form imageID | image data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label map of the form businessID | labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, we must define a regular expression pattern to extract a jpg name from
    the `CSVImageMetadataReaderclass`, which is used to match against training labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we extract all of the image IDs associated with their respective business
    IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to load and process all the images that are already preprocessed
    to extract the image IDs by mapping them with the IDs extracted from the business
    IDs in the earlier examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we get a list of images from the photoDir directory
    (which is where the raw images reside). The `ids` parameter is an optional parameter
    to subset the images loaded from the photoDir. So far, we have been able to extract
    all the image IDs that are somehow associated with at least one business. The
    next move will be to read and process the images into an imageID → vector map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we read and processed the images into a photoID
    → vector map. The `processImages()` method takes the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`images`: A list of images in the `getImageIds()` method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resizeImgDim`: Dimension to rescale square images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nPixels`: Number of pixels used to sample the image to drastically reduce
    runtime while testing features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Well done! We are just one step away from extracting the data that is required
    to train our CNNs. The final step in feature extraction is to extract the pixel
    data, which consists of four objects to keep track of each image -- that is, imageID,
    businessID, labels, and pixel data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f48580d-b6fc-44aa-ba0a-f40ea4946ae7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image data representation
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, as shown in the preceding diagram, the primary data structure is constructed
    with four data types (that is, four tuples) -- `imgID`, `businessID`, `pixel data
    vector`, and `labels`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we should have a class containing all of the parts of these objects.
    Don''t worry; all we need is defined in the `FeatureAndDataAligner.java` script.
    Once we have the instantiated instance of `FeatureAndDataAligner` using the following
    line of code in the `YelpImageClassifier.java` script (under the main method),
    `businessMap`, `dataMap`, and `labMap` are provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the option type for `labMap` is used since we don''t have this information
    when we score on test data -- that is, it is optional for invocation. Now, let''s
    see how I did this. We start from the constructor of the class that is being used,
    initializing the preceding data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we initialize the values through the constructor of the `FeatureAndDataAligner.java`
    class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when aligning the data, if `labMap` is empty -- which is not provided
    with the training data -- the following can be used too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to align the image IDs and image data with the business IDs. For
    this, I have written the `BusinessImgageIds()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual implementation lies in the following overloaded method, which returns
    optional if an image does not have a business ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, as shown in the preceding diagram, we now need to align the labels,
    which is a four-tuple list compromising of `dataMap`, `bizMap`, `labMap`, and
    `rowindices`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code block, `Quarta` is a case class that help us to maintain
    our desired data structure, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we pre-compute and save the data so that the method does not need
    to re-compute each time it is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, as used in the preceding code block, we now create some getter methods
    so that in each invocation, we can retrieve the `image id`, `business id`, business
    label, and image for each business easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Excellent! Up to this point, we have managed to extract the features to train
    our CNNs. However, the thing is that the feature in its current form is still
    not suitable to feed into the CNNs. This is because we only have the feature vectors
    without labels. Thus, it needs another intermediate conversion.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the ND4J dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As I said previously, we need an intermediate conversion to prepare the training
    set containing feature vectors and labels: features from images, but labels from
    the business labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we have the `makeND4jDataSets` class (see `makeND4jDataSets.java`
    for details). The class creates an ND4J dataset object from the data structure
    from the `alignLables` function in the `List[(imgID, bizID, labels, pixelVector)]`
    form. First, we prepare the dataset using the `makeDataSet()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we further need to convert the preceding data structure into an `INDArray`,
    which can then be consumed by the CNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, the `toNDArray()` method is used to convert the
    double or float matrix into `INDArray` format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Fantastic! We were able to extract all the metadata and features from the images
    and prepared the training data in an ND4J format that can now be consumed by the
    DL4J-based model. However, since we will be using CNN as our model, we still need
    to convert this 2D object into 4D by using the `convolutionalFlat` operation during
    network construction. Anyway, we will see this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training, evaluating, and saving the trained CNN models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how to prepare the training set. Now that we have, a more
    challenging part lies ahead as we have to train our CNNs with 234,545 images,
    although the testing phase could be less exhaustive with a limited number of images,
    for example, 500 images. Therefore, it is better to train each CNN involving batchmode
    using DL4j's `MultipleEpochsIterator`, which is a dataset iterator for doing multiple
    passes over a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '`MultipleEpochsIterator` is a dataset iterator for doing multiple passes over
    a dataset. See more at [https://deeplearning4j.org/doc/org/deeplearning4j/datasets/iterator/MultipleEpochsIterator.html](https://deeplearning4j.org/doc/org/deeplearning4j/datasets/iterator/MultipleEpochsIterator.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Network construction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a list of important hyperparameters and their details. Here,
    I will try to construct a five-layered CNN, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer 0 has a `ConvolutionLayer` having a 6 x 6 kernel, one channel (since
    they are grayscale images), a stride of 2 x 2, and 20 feature maps where ReLU
    is the activation function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Layer 1 has `SubsamplingLayer` max pooling, and a stride of 2x2\. Thus, by
    using stride, we down sample by a factor of 2\. Note that only MAX, AVG, SUM,
    and PNORM are supported. Here, the kernel size will be the same as the filter
    size from the last `ConvolutionLayer`. Therefore, we do not need to define the
    kernel size explicitly:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Layer 2 has a `ConvolutionLayer` having a 6 x 6 kernel, one channel (since
    they are grayscale images), a stride of 2 x 2, and 20 output neurons where RELU
    is the activation function. We will use Xavier for network weight initialization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Layer 3 has `SubsamplingLayer` max pooling and a stride of 2 x 2\. Thus, by
    using stride, we down sample by a factor of 2\. Note that only MAX, AVG, SUM,
    and PNORM are supported. Here, the kernel size will be the same as the filter
    size from the last `ConvolutionLayer`. Therefore, we do not need to define the
    kernel size explicitly:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Layer 4 has a `DenseLayer`, that is, a fully connected feed forward layer trainable
    by backpropagation with 50 neurons and ReLU as an activation function. It should
    be noted that we do not need to specify the number of input neurons as it assumes
    the input from the previous `ConvolutionLayer`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Layer 5 is an `OutputLayer` having two output neurons driven by the softmax
    activation (that is, probability distribution over the classes). We compute the
    loss using XENT (that is, cross entropy for binary classification) as the loss
    function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Apart from these layers, we also need to perform image flattening—that is,
    converting a 2D object into a 4D consumable using CNN layers by invoking the following
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, in summary, using DL4J, our CNN will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The other important aspects related to the training are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The number of samples**: If you were training all the images other than GPU,
    that is, CPU, it would take days. When I tried with 50,000 images, it took one
    whole day with a machine having a core i7 processor and 32 GB of RAM. Now, you
    can imagine how long it would take for the whole dataset. In addition, it will
    require at least 256 GB of RAM even if you do the training in batch mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of epochs**: This is the number of iterations through all the training
    records. I iterated for 10 epochs due to time constraints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of batches**: This is the number of records in each batch, for example,
    32, 64, and 128\. I used 128.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, with the preceding hyperparameters, we can start training our CNNs. The
    following code does the trick. The thing is that at first, we prepare the training
    set, then we define the required hyperparameters, and then we normalize the dataset
    so the ND4j data frame is encoded so that any labels that are considered true
    are ones and the rest zeros. Then, we shuffle both the rows and labels of the
    encoded dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we need to create epochs for the dataset iterator using `ListDataSetIterator`
    and `MultipleEpochsIterator`, respectively. Once the dataset is converted into
    the batchmodel, we are then ready to train the constructed CNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we finish the training, we can evaluate the model on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'When the evaluation is finished, we can now inspect the result of each CNN
    (run the `YelpImageClassifier.java` script):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Oops! Unfortunately, we have not seen good accuracy. However, do not worry,
    since in the FAQ section, we will see how to improve upon this. Finally, we can
    save the layer-wise network configuration and network weights to be used later
    on (that is, scoring before submission):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, we also saved a JSON file containing all the network
    configurations and a binary file for holding all the weights and parameters of
    all the CNNs. This is done using two methods, namely `saveNN()` and `loadNN()`,
    which are defined in the `NetwokSaver.java` script. First, let''s look at the
    signature of the `saveNN()` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The idea is visionary as well as important since, as I said earlier, you would
    not train your whole network for the second time to evaluate a new test set. For
    example, suppose you want to test just a single image. The thing is, we also have
    another method named `loadNN()` that reads back the `.json` and `.bin` files we
    created earlier to a `MultiLayerNetwork`, which can be used to score new test
    data. This method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Scoring the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The scoring approach that we are going to use is simple. It assigns business-level
    labels by averaging the image-level predictions. I did this in a simplistic manner,
    but you can try using a better approach. What I did is assign a business as label
    `0` if the average of the probabilities across all of its images belonging to
    class `0` are greater than a certain threshold, say, 0.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, I collected the model predictions from the `scoreModel()` method and
    merged them with `alignedData`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can restore the trained and saved models, restore them back, and
    generate the submission file for Kaggle. The thing is that we need to aggregate
    image predictions to business scores for each model.
  prefs: []
  type: TYPE_NORMAL
- en: Submission file generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this, I wrote a class called `ResultFileGenerator.java`. According to the
    Kaggle web page, we will have to write the result in the `business_ids, labels`
    format. Here, `business_id` is the ID for the corresponding business, and the
    label is the multi-label prediction. Let's see how easily we can do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we aggregate image predictions to business scores for each model. Then,
    we transform the preceding data structure into a list for each `bizID` containing
    a Tuple (`bizid`, `List[Double]`) where the `Vector[Double]` is the vector of
    probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, once we have the result aggregated from each model, we then need
    to generate the submission file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have managed to do everything up to this point, we can now wrap
    this up and generate a sample prediction and submission file for Kaggle. For simplicity,
    I randomly sliced this to only 20,000 images to save time. Interested readers
    can try building CNNs for all the images, too. However, it might take days. Nevertheless,
    we will look at some performance tuning tips in the FAQ section.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping everything up by executing the main() method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s wrap the overall discussion by watching the performance of our model
    programmatically (see the main `YelpImageClassifier.java` class):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: It's true that we haven't achieved outstanding classification accuracy. Nevertheless,
    we can still try this with tuned hyperparameters. The following sections provide
    some insight.
  prefs: []
  type: TYPE_NORMAL
- en: Frequently asked questions (FAQs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although we have been able to solve this multi-label classification problem,
    the accuracy we experienced was below par. Therefore, in this section, we will
    see some **frequently asked questions** (**FAQs**) that might already be on your
    mind. Knowing the answers to these questions might help you to improve the accuracy
    of the CNNs we trained. Answers to these questions can be found in the Appendix:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the hyperparameters that I can try tuning while implementing this project?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: My machine is getting OOP while running this project. What should I do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While training the networks with full images, my GPU is getting OOP. What should
    I do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I understand that the predictive accuracy using CNN in this project is still
    very low. Did our network under or overfit? Is there any way to observe how the
    training went?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I am very interested in implementing the same project in Scala. How can I do
    that?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which optimizer should I use for this type of project where we need to process
    large-scale images?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many hyperparameters do we have? I also want to see them for each layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen how to develop a real-life application using CNNs
    on the DL4J framework. We have seen how to solve a multi-label classification
    problem through nine CNNs and a series of complex feature engineering and image
    manipulation operations. Albeit, we couldn't achieve higher accuracy, but readers
    are encouraged to tune hyperparameters in the code and try the same approach with
    the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Also, training the CNNs with all the images is recommended so that networks
    can get enough data to learn the features from Yelp images. One more suggestion
    is improving the feature extraction process so that the CNNs can have more quality
    features.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to implement and deploy a hands-on deep
    learning project that classifies review texts as either positive or negative based
    on the words they contain. A large-scale movie review dataset that contains 50,000
    reviews (training plus testing) will be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'A combined approach using Word2Vec (that is, a widely used word embedding technique
    in NLP) and the LSTM network for modeling will be applied: the pre-trained Google
    news vector model will be used as the neural word embeddings. Then, the training
    vectors, along with the labels, will be fed into the LSTM network to classify
    them as negative or positive sentiments. This will evaluate the trained model
    on the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: Answers to questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Answer** **to question 1**: The following hyperparameters are very important
    and must be tuned to achieve optimized results:'
  prefs: []
  type: TYPE_NORMAL
- en: Dropout is used to randomly off certain neurons (that is, feature detectors)
    to prevent overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate optimization—Adagrad can be used for feature-specific learning
    rate optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization—L1 and/or L2 regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient normalization and clipping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, apply batch normalization to reduce internal covariate shift in training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, for dropout, we can add dropout in each convolutional and dense layer and
    in case of overfitting, the model is specifically adjusted to the training dataset,
    so it will not be used for generalization. Therefore, although it performs well
    on the training set, its performance on the test dataset and subsequent tests
    is poor because it lacks the generalization property.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anyway, we can apply dropout on a CNN and DenseLayer. Now, for better learning
    rate optimization, Adagrad can be used for feature-specific learning rate optimization.
    Then, for better regularization, we can use either L1 and/or L2\. Thus, considering
    this, our network configuration should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '**Answer to question 2**: Due to the layering architecture''s perspective and
    convolutional layers, training a CNN requires a huge amount of RAM. This is because
    the reverse pass of backpropagation requires all the intermediate values computed
    during the forward pass. Fortunately, during the inferencing stage, memory occupied
    by one layer is released as soon as the computation is completed when the next
    layer has been computed.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, as stated earlier, DL4J is built upon ND4J and ND4J utilizes off-heap
    memory management. This enables us to control the maximum amount of off-heap memory.
    We can set the `org.bytedeco.javacpp.maxbytes` system property. For example, for
    a single JVM run, you can pass `-Dorg.bytedeco.javacpp.maxbytes=1073741824` to
    limit the off-heap memory to 1 GB.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer to question 3**: As I mentioned previously, training a CNN with Yelp''s
    50,000 images took one whole day with a machine with a core i7 processor and 32
    GB of RAM. Naturally, performing this on all of the images can take a week. Therefore,
    in such cases, training on GPU makes much more sense.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, we have already seen that DL4J works on distributed GPUs, as well
    as on native. For this, it has what we call **backbends**, or different types
    of hardware that it works on. Finally, a funny question would be: What should
    we do if our GPU runs out of memory? Well, if your GPU runs out of memory while
    training a CNN, here are five things you could do in order to try to solve the
    problem (other than purchasing a GPU with more RAM):'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the mini-batch size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce dimensionality using a larger stride in one or more layers, but don't
    go with PCA or SVD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove one or more layers unless it's strictly essential to have a very deep
    network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use 16-bit floats instead of 32-bit (but precision has to be compromised)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribute the CNN across multiple devices (that is, GPUs/CPUs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more on distributed training on GPUs using DL4J, refer to [Chapter 8](a59fb1f2-b585-44f5-a467-903b8c25867b.xhtml),
    *Distributed Deep Learning – Video Classification Using Convolutional LSTM Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 4**: It is true that we did not experience good accuracy.
    However, there are several reasons as to why we have not performed hyperparameter
    tuning. Secondly, we have not trained our network with all the images, so our
    network does not have enough data to learn the Yelp images. Finally, we can still
    see the model versus iteration score and other parameters from the following graph,
    so we can see that our model was not overfitted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36122f86-763f-4da4-8628-de1c9a84a1c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Model versus iteration score and other parameters of the LSTM sentiment analyzer
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 5**: Yes, it is possible since Scala is also a JVM
    language, so it would not be that difficult to convert this Java project into
    Scala. Nevertheless, one of my previous books solves this same problem in Scala.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the reference: Md. Rezaul Karim, *Scala Machine Learning Projects*,
    Packt Publishing Ltd., January 2018\. Note that in that book, I used an old version
    of ND4J and DL4J, but I believe you can upgrade it by following this project.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 6**: Since, in CNN, one of the objective functions
    is to minimize the evaluated cost, we must define an optimizer. DL4j supports
    the following optimizers:'
  prefs: []
  type: TYPE_NORMAL
- en: SGD (learning rate only)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nesterovs momentum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adagrad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RMSProp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaDelta
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information, interested readers can refer to the DL4J page on available
    updaters at [https://deeplearning4j.org/updater](https://deeplearning4j.org/updater).
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer to question 7**: Just use the following code immediately after network
    initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: This also tell us that the subsampling layers do not have any hyperparameters.
    Nevertheless, if you want to create an MLP or DBN, we will require millions of
    hyperparameters. However, here, we can see that we only need 263,000 hyperparameters.
  prefs: []
  type: TYPE_NORMAL
