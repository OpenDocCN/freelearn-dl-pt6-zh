<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image Analysis Applications in Self-Driving Cars</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapters, we learned about object classification and also object localization. In this chapter, we will go through multiple case studies that are relevant to self-driving cars.</p>
<p>You will be learning about the following:</p>
<ul>
<li>Traffic sign identification</li>
<li>Predicting the angle within which a car needs to be turned</li>
<li>Identifying cars on the road using the U-net architecture</li>
<li>Semantic segmentation of objects on the road</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Traffic sign identification</h1>
                </header>
            
            <article>
                
<p>In this case study, we will understand the way in which we can classify a signal into one of the 43 possible classes.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>For this exercise, we will adopt the following strategy:</p>
<ol>
<li>Download the dataset that contains all possible traffic signs</li>
<li>Perform histogram normalization on top of input images:
<ul>
<li>Certain images are taken in broad day light, while others might be taken in twilight</li>
<li>Different lighting conditions result in a variation in pixel values, depending on the lighting condition at which the picture is taken</li>
<li>Histogram normalization performs normalization on pixel values so that they all have a similar distribution</li>
</ul>
</li>
<li>Scale the input images</li>
<li>Build, compile, and fit a model to reduce the categorical cross entropy loss value</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Download the dataset, as follows (the code file is available as <kbd>Traffic_signal_detection.ipynb</kbd> in GitHub). The dataset is available through the paper: J. Stallkamp, M. Schlipsing, J. Salmen, C. Igel, Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ wget http://benchmark.ini.rub.de/Dataset/GTSRB_Final_Training_Images.zip</strong><br/><strong>$ unzip GTSRB_Final_Training_Images.zip</strong></pre>
<ol start="2">
<li>Read the image paths into a list<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">from skimage import io<br/>import os<br/>import glob<br/><br/>root_dir = '/content/GTSRB/Final_Training/Images/'<br/>all_img_paths = glob.glob(os.path.join(root_dir, '*/*.ppm'))</pre>
<p style="padding-left: 60px">A sample of the images looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1319 image-border" src="Images/12bf31d5-f7a6-4255-afeb-78f1abe0d861.png" style="width:24.75em;height:18.67em;" width="332" height="250"/></p>
<p style="padding-left: 60px">Note that certain images have a smaller shape when compared to others and also that certain images have more lighting when compared to others. Thus, we'll have to preprocess the images so that all images are normalized per exposure to lighting as well as shape.</p>
<ol start="3">
<li>Perform histogram normalization on top of the input dataset<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>from skimage import color, exposure, transform<br/><br/>NUM_CLASSES = 43<br/>IMG_SIZE = 48<br/><br/>def preprocess_img(img):<br/>     hsv = color.rgb2hsv(img)<br/>     hsv[:, :, 2] = exposure.equalize_hist(hsv[:, :, 2])<br/>     img = color.hsv2rgb(hsv)<br/>     img = transform.resize(img, (IMG_SIZE, IMG_SIZE))<br/>     return img</pre>
<p style="padding-left: 60px">In the preceding code, we are first converting an image that is in RGB format into a <strong>Hue Saturation Value (HSV)</strong> format. By transforming the image from RGB to HSV format, we are essentially converting the combined RGB values into an array that can then be transformed into an array of single dimension.</p>
<p style="padding-left: 60px">Post that, we are normalizing the values obtained in HSV format so that they belong to the same scale by using the <kbd>equalize_hist</kbd> method.</p>
<p style="padding-left: 60px">Once the images are normalized in the last channel of the HSV format, we convert them back in to RGB format.</p>
<p style="padding-left: 60px">Finally, we resize the images to a standard size.</p>
<ol start="4">
<li>Check the image prior to passing it through histogram normalization and contrast that with post histogram normalization (post passing the image through the <kbd>preprocess_img</kbd> function), as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1320 image-border" src="Images/c4e7a40e-ffcd-451d-9a24-7547c009ff94.png" style="width:23.50em;height:22.92em;" width="282" height="275"/><img class="aligncenter size-full wp-image-1321 image-border" src="Images/548e4913-54ac-4117-814e-c8b5207d0392.png" style="width:21.58em;height:22.83em;" width="265" height="280"/></p>
<p style="padding-left: 60px">From the preceding pictures, we can see that there is a considerable change in the visibility of the image (the image on the left) post histogram normalization (the image on the right).</p>
<ol start="5">
<li>Prepare the input and output arrays<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">count = 0<br/>imgs = []<br/>labels = []<br/>for img_path in all_img_paths:<br/>     img = preprocess_img(io.imread(img_path))<br/>     label = img_path.split('/')[-2]<br/>     imgs.append(img)<br/>     labels.append(label)<br/><br/>X = np.array(imgs)<br/>Y = to_categorical(labels, num_classes = NUM_CLASSES)</pre>
<ol start="6">
<li>Build the training and test datasets<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state= 42)</pre>
<ol start="7">
<li>Build and compile the model<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Conv2D(32, (3, 3), padding='same',input_shape=(IMG_SIZE, IMG_SIZE, 3), activation='relu'))<br/>model.add(Conv2D(32, (3, 3), activation='relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Dropout(0.2))<br/>model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))<br/>model.add(Conv2D(64, (3, 3), activation='relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Dropout(0.2))<br/>model.add(Conv2D(128, (3, 3), padding='same',activation='relu'))<br/>model.add(Conv2D(128, (3, 3), activation='relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Dropout(0.2))<br/>model.add(Flatten())<br/>model.add(Dense(512, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(NUM_CLASSES, activation='softmax'))<br/>model.summary()<br/><br/>model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']</pre>
<p style="padding-left: 60px">A summary of <span>the</span> model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1328 image-border" src="Images/23d3c55c-1505-4cf1-a1b8-10214e8d9207.png" style="width:34.92em;height:44.83em;" width="461" height="593"/></p>
<ol start="8">
<li>Fit the model<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">model.fit(X_train, y_train,batch_size=32,epochs=5,validation_data = (X_test, y_test))</pre>
<p style="padding-left: 60px">The preceding code, results in a model that has an accuracy of ~99%:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1330 image-border" src="Images/fe909540-e0d5-466a-bd5c-bfc9597710c9.png" style="width:33.33em;height:25.92em;" width="400" height="311"/></p>
<p>Additionally, if you perform the exact same analysis like we did, but without histogram normalization (correcting for exposure), the accuracy of the model is ~97%.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Predicting the angle within which a car needs to be turned</h1>
                </header>
            
            <article>
                
<p>In this case study, we will understand the angle within which a car needs to be turned based on the image provided.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The strategy we adopt to build a steering angle prediction is as follows:</p>
<ol>
<li>Gather a dataset that has the images of the road and the corresponding angle within which the steering needs to be turned</li>
<li>Preprocess the image</li>
</ol>
<ol start="3">
<li>Pass the image through the VGG16 model to extract features</li>
<li>Build a neural network that performs regression to predict the steering angle, which is a continuous value to be predicted</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Download the following dataset. This dataset is available from the following link: <a href="https://github.com/SullyChen/driving-datasets" target="_blank">https://github.com/SullyChen/driving-datasets</a>: (the code file is available as <kbd>Car_steering_angle_detection.ipynb</kbd> in GitHub):</li>
</ol>
<pre style="padding-left: 60px"><strong>$ pip install PyDrive</strong> <br/><br/>from pydrive.auth import GoogleAuth<br/>from pydrive.drive import GoogleDrive<br/>from google.colab import auth<br/>from oauth2client.client import GoogleCredentials<br/><br/>auth.authenticate_user()<br/>gauth = GoogleAuth()<br/>gauth.credentials = GoogleCredentials.get_application_default()<br/>drive = GoogleDrive(gauth)<br/><br/>file_id = '0B-KJCaaF7elleG1RbzVPZWV4Tlk' # URL id. <br/>downloaded = drive.CreateFile({'id': file_id})<br/>downloaded.GetContentFile('steering_angle.zip')<br/><br/><strong>$ unzip steering_angle.zip</strong></pre>
<ol start="2">
<li>Import the relevant packages<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">import os<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from scipy import pi<br/>import cv2<br/>import scipy.misc<br/>import tensorflow as tf</pre>
<ol start="3">
<li>Read the images and their corresponding angles in radians into separate lists<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">DATA_FOLDER = "/content/driving_dataset/"<br/>DATA_FILE = os.path.join(DATA_FOLDER, "data.txt")<br/>x = []<br/>y = []<br/><br/>train_batch_pointer = 0<br/>test_batch_pointer = 0<br/><br/>with open(DATA_FILE) as f:<br/>     for line in f:<br/>         image_name, angle = line.split() <br/>         image_path = os.path.join(DATA_FOLDER, image_name)<br/>         x.append(image_path) <br/>         angle_radians = float(angle) * (pi / 180) #converting angle into radians<br/>         y.append(angle_radians)<br/>y = np.array(y)</pre>
<ol start="4">
<li>Create the train and test datasets<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">split_ratio = int(len(x) * 0.8)<br/>train_x = x[:split_ratio]<br/>train_y = y[:split_ratio]<br/>test_x = x[split_ratio:]<br/>test_y = y[split_ratio:]</pre>
<ol start="5">
<li>Check the output label values in the train and test datasets<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">fig = plt.figure(figsize = (10, 7))<br/>plt.hist(train_y, bins = 50, histtype = "step",color='r')<br/>plt.hist(test_y, bins = 50, histtype = "step",color='b')<br/>plt.title("Steering Wheel angle in train and test")<br/>plt.xlabel("Angle")<br/>plt.ylabel("Bin count")<br/>plt.grid('off')<br/>plt.show()</pre>
<p class="CDPAlignCenter CDPAlign"><img src="Images/40a861cd-fd88-473f-a389-2d38d2e18b25.png" width="626" height="433"/></p>
<ol start="6">
<li>Remove the pixels in the first 100 rows, as they do not correspond to the image of a road, and then pass the resulting image through the VGG16 model. Additionally, for this exercise, we will work on only the first 10,000 images in the dataset so that we are able to build a model faster. <span>Remove the pixels in the first 100 rows, as follows:</span></li>
</ol>
<pre style="padding-left: 60px">x = []<br/>y = []<br/>for i in range(10000):<br/>     im = cv2.imread(train_x[i])<br/>     im = im[100:,:,:]/255<br/>     vgg_im = vgg16_model.predict(im.reshape(1,im.shape[0],im.shape[1],3))<br/>     x.append(vgg_im)<br/>     y.append(train_y[i])<br/>x1 = np.array(x)<br/>x1 = x1.reshape(x1.shape[0],4,14,512)<br/>y1 = np.array(y)</pre>
<ol start="7">
<li>Build and compile the model, as follows:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Flatten(input_shape=(4,14,512)))<br/>model.add(Dense(512, activation='relu'))<br/>model.add(Dropout(.5))<br/>model.add(Dense(100, activation='linear'))<br/>model.add(Dropout(.2))<br/>model.add(Dense(50, activation='linear'))<br/>model.add(Dropout(.1))<br/>model.add(Dense(10, activation='linear'))<br/>model.add(Dense(1, activation='linear'))<br/>model.summary()</pre>
<p style="padding-left: 60px">Note that the output layer has linear activation as the output is a continuous value that ranges from -9 to +9. A summary of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1333 image-border" src="Images/edb25ad9-1735-49e8-8eac-039a1b89c67e.png" style="width:35.50em;height:28.00em;" width="512" height="404"/></p>
<p style="padding-left: 60px">Now, we'll compile the model we've defined as follows:</p>
<pre style="padding-left: 60px">model.compile(loss='mean_squared_error',optimizer='adam')</pre>
<ol start="8">
<li>Fit the model<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">model.fit(x1/11, y1,batch_size=32,epochs=10, validation_split = 0.1, verbose = 1)</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1334 image-border" src="Images/971589a3-0d22-4c0f-8fa9-30f7742401d4.png" style="width:28.92em;height:20.00em;" width="391" height="271"/></p>
<p style="padding-left: 60px">Test loss is the line that has the lower loss in the preceding diagram.</p>
<p style="padding-left: 60px">Note that we have divided the input dataset by 11 so that we can scale it to have a values between 0 to 1. Now, we should be in a position to simulate the movement of the car based on the angle that it is predicted.</p>
<p style="padding-left: 60px">The steering angle predictions <span>obtained by the model </span>for a sample of images are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1336 image-border" src="Images/6d08b8a4-ce50-464a-9b5d-46afa3198668.png" style="width:27.50em;height:18.08em;" width="387" height="254"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1337 image-border" src="Images/342b5540-0861-4d72-b58c-908420c84c6f.png" style="width:31.00em;height:19.42em;" width="372" height="233"/></p>
<div class="packt_infobox">Note that you should be very careful while taking a model like the preceding one and implementing it. It should be first tested on multiple daylight conditions before finally going to production.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Instance segmentation using the U-net architecture</h1>
                </header>
            
            <article>
                
<p>So far, in the previous two chapters, we have learned about detecting objects and also about identifying the bounding boxes within which the objects within an image are located. In this section, we will learn about performing instance segmentation, where all the pixels belonging to a certain object are highlighted while every other pixel isn't (this is similar to masking all the other pixels that do not belong to an object with zeros and masking the pixels that belong to the object with pixel values of one).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To perform instance segmentation, we will perform the following:</p>
<ol>
<li>Work on a dataset that has the input image and the corresponding masked image of the pixels where the object is located in the image:
<ul>
<li>The image and its masked image</li>
</ul>
</li>
<li>We'll pass the image through the pre-trained VGG16 model to extract features out of each convolution layer</li>
<li>We'll gradually up sample the convolution layers so that we get an output image that is of 224 x 224 x 3 in shape</li>
<li>We'll freeze the layers where VGG16 weights are used</li>
<li>Concatenate the up sampled convolution layers with the down sampled convolution layers:
<ul>
<li>This forms the U-shaped connection</li>
<li>The U-shaped connection helps in model having the context in a way similar to ResNet (previously down sampled layer provides context in addition to the up sampled layer)</li>
<li>Reconstructing an image is much easier if we take the first layer's output, as much of the image is intact in the first layer (earlier layers learn the contours). If we try to reconstruct an image from the last few layers by up sampling them, there is a good chance that the majority of the information about the image is lost</li>
</ul>
</li>
<li>Fit a model that maps the input image to masked image:
<ul>
<li>Note that the masked image is binary in nature—where the black values correspond to a pixel value of 0 and the white pixels have a value of 1</li>
</ul>
</li>
<li>Minimize the binary cross entropy loss function across all the 224 x 224 x 1 pixels</li>
</ol>
<p>The reason this model is called a <strong>U-net architecture</strong> is because the visualization of the model looks as follows—a rotated U-like structure:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1339 image-border" src="Images/b4e97c5a-c390-4983-92f7-c9da19ef8318.png" style="width:105.83em;height:27.50em;" width="1270" height="330"/></p>
<p>The U-like structure of the model is due to the early layers connecting to up sampled versions of the down sampled layers.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In the following code, we will perform instance segmentation to detect a car within an image:</p>
<ol start="1">
<li>Download and import files from <a href="https://github.com/divamgupta/image-segmentation-keras">https://github.com/divamgupta/image-segmentation-keras</a><span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ wget https://www.dropbox.com/s/0pigmmmynbf9xwq/dataset1.zip</strong><br/><strong>$ unzip dataset1.zip</strong><br/>dir_data = "/content/dataset1"<br/>dir_seg = dir_data + "/annotations_prepped_train/"<br/>dir_img = dir_data + "/images_prepped_train/"<br/>import glob, os<br/>all_img_paths = glob.glob(os.path.join(dir_img, '*.png'))<br/>all_mask_paths = glob.glob(os.path.join(dir_seg, '*.png'))</pre>
<ol start="2">
<li>Read the images and their corresponding masks into arrays<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">import cv2<br/>from scipy import ndimage<br/>x = []<br/>y = []<br/>for i in range(len(all_img_paths)):<br/>  img = cv2.imread(all_img_paths[i])<br/>  img = cv2.resize(img,(224,224))<br/>  mask_path = dir_seg+all_img_paths[i].split('/')[4]<br/>  img_mask = ndimage.imread(mask_path)<br/>  img_mask = cv2.resize(img_mask,(224,224))<br/>  x.append(img)<br/>  y.append(img_mask)<br/><br/>x = np.array(x)/255<br/>y = np.array(y)/255<br/>y2 = np.where(y==8,1,0)</pre>
<p style="padding-left: 60px">In the preceding step, we have created the input and output arrays and also normalized the input array. Finally, we separated the mask of a car from everything else, as this dataset has 12 unique classes of which cars are masked with a pixel value of 8.</p>
<p style="padding-left: 60px">A sample of input and masked images are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1341 image-border" src="Images/1c1e655a-3c45-45ce-bfda-8863dbc5dbf8.png" style="width:24.67em;height:20.25em;" width="296" height="243"/></p>
<p style="padding-left: 60px">Furthermore, we create input and output arrays where we scale the input array and reshape the output array (so that it can be passed to network), as follows:</p>
<pre style="padding-left: 60px">x = np.array(x)<br/>x = x/255<br/>y2 = np.array(y2)<br/>y2 = y2.reshape(y2.shape[0],y2.shape[1],y2.shape[2],1)</pre>
<ol start="4">
<li>Build the model where the image is first passed through the VGG16 model layers and the convolution features are extracted<span>, as follows</span>:</li>
</ol>
<p style="padding-left: 60px"><span>In the following code, we are importing the pre-trained VGG16 model</span>:</p>
<pre style="padding-left: 60px">from keras.applications.vgg16 import VGG16 as PTModel<br/>from keras.layers import Input, Conv2D, concatenate, UpSampling2D, BatchNormalization, Activation, Cropping2D, ZeroPadding2D<br/>from keras.layers import Input, merge, Conv2D, MaxPooling2D,UpSampling2D, Dropout, Cropping2D, merge, concatenate<br/>from keras.optimizers import Adam<br/>from keras.callbacks import ModelCheckpoint, LearningRateScheduler<br/>from keras import backend as K<br/>from keras.models import Model</pre>
<pre style="padding-left: 60px">base_pretrained_model = PTModel(input_shape = (224,224,3), include_top = False, weights = 'imagenet')<br/>base_pretrained_model.trainable = False</pre>
<p style="padding-left: 60px"><span>In the following code, the features of various convolution layers when passed through the VGG16 model are extracted</span>:</p>
<pre style="padding-left: 60px">conv1 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block1_conv2').output).output<br/>conv2 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block2_conv2').output).output<br/>conv3 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block3_conv3').output).output<br/>conv4 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block4_conv3').output).output<br/>drop4 = Dropout(0.5)(conv4)<br/>conv5 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block5_conv3').output).output<br/>drop5 = Dropout(0.5)(conv5)</pre>
<p style="padding-left: 60px"><span>In the following code, we are up scaling the features using the</span> <kbd>UpSampling</kbd><span> method and then concatenating with the down scaled VGG16 convolution features at each layer</span>:</p>
<pre style="padding-left: 60px">up6 = Conv2D(512, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(drop5))<br/>merge6 = concatenate([drop4,up6], axis = 3) <br/><br/>conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge6)<br/>conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv6)<br/>conv6 = BatchNormalization()(conv6)<br/>up7 = Conv2D(256, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(conv6))<br/>merge7 = concatenate([conv3,up7], axis = 3)<br/>conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge7)<br/>conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv7)<br/>conv7 = BatchNormalization()(conv7)<br/>up8 = Conv2D(128, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(conv7))<br/>merge8 = concatenate([conv2,up8],axis = 3)<br/>conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge8)<br/>conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv8)<br/>conv8 = BatchNormalization()(conv8)<br/>up9 = Conv2D(64, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(conv8))<br/>merge9 = concatenate([conv1,up9], axis = 3)<br/>conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge9)<br/>conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv9)<br/>conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv9)<br/>conv9 = BatchNormalization()(conv9)<br/>conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)</pre>
<p style="padding-left: 60px">In the following code, <span>we are defining the input and output to the model, where the input is passed to the </span><kbd>base_pretrained_model</kbd><span> first and the output is <kbd>conv10</kbd> (which has the shape of 224 x 224 x 1—the intended shape of our output):</span></p>
<pre style="padding-left: 60px">model = Model(input = base_pretrained_model.input, output = conv10)</pre>
<ol start="5">
<li>Freeze the convolution layers obtained from the multiplication of the VGG16 model from training<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">for layer in model.layers[:18]:<br/>     layer.trainable = False</pre>
<ol start="6">
<li>Compile and fit the model for the first 1,000 images in our dataset<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">from keras import optimizers<br/>adam = optimizers.Adam(1e-3, decay = 1e-6)<br/>model.compile(loss='binary_crossentropy',optimizer=adam,metrics=['accuracy'])<br/>history = model.fit(x,y,validation_split = 0.1,batch_size=1,epochs=5,verbose=1)</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1345 image-border" src="Images/3027213c-790c-4e63-8d37-c02db987f53a.png" style="width:32.83em;height:25.67em;" width="394" height="308"/></p>
<ol start="7">
<li>Test the preceding model on a test image (the last 2 images of our dataset—they are test images that have <kbd>validtion_split = 0.1</kbd>)<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">y_pred = model.predict(x[-2:].reshape(2,224,224,3))</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1348 image-border" src="Images/35f3e4be-1c52-4657-a157-aaf44df7d756.png" style="width:30.17em;height:19.92em;" width="362" height="239"/></p>
<p>We can see that the generated mask is realistic for the given input of road and also in a way that's better than what we were doing prior as the noisy dots are not present in the predicted mask image.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Semantic segmentation of objects in an image</h1>
                </header>
            
            <article>
                
<p>In the previous section, we learned about performing segmentation on top of an image where the image contained only one object. In this segmentation, we will learn about performing segmentation so that we are able to distinguish between multiple objects that are present in an image of a road.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The strategy that we'll adopt to perform semantic segmentation on top of images of a road is as follows:</p>
<ol>
<li>Gather a dataset that has the annotation of where the multiple objects within an image are located:
<ul>
<li>A sample of the semantic image looks as follows:</li>
</ul>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1350 image-border" src="Images/9bee9bc6-479d-41cb-a8f7-25a68aa90b73.png" style="width:30.67em;height:20.50em;" width="368" height="246"/></p>
<ol start="2">
<li>Convert the output mask into a multi dimensional array where there are as many columns as the number of all possible unique objects</li>
<li>If there are 12 possible unique values (12 unique objects), convert the output image into  an image that is 224 x 224 x 12 in shape:
<ul>
<li>A value of a channel represents that the object corresponding to that channel is present in that location of image</li>
</ul>
</li>
<li>Leverage the model architecture that we have seen in previous sections to train a model that has 12 possible output values</li>
<li>Reshape the prediction into three channels by assigning all three channels to have the same output:
<ul>
<li>The output is the argmax of prediction of the probabilities of the 12 possible classes</li>
</ul>
</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Semantic segmentation in code is performed as follows (The code file is available as <kbd>Semantic_segmentation.ipynb</kbd> in GitHub):</p>
<ol>
<li>Download the dataset, as follows:</li>
</ol>
<pre style="padding-left: 60px">!wget https://www.dropbox.com/s/0pigmmmynbf9xwq/dataset1.zip<br/>!unzip dataset1.zip<br/>dir_data = "/content/dataset1"<br/>dir_seg = dir_data + "/annotations_prepped_train/"<br/>dir_img = dir_data + "/images_prepped_train/"<br/>import glob, os<br/>all_img_paths = glob.glob(os.path.join(dir_img, '*.png'))<br/>all_mask_paths = glob.glob(os.path.join(dir_seg, '*.png'))</pre>
<ol start="2">
<li>Read the images and their corresponding labels into separate lists<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">import cv2<br/>from scipy import ndimage<br/>for i in range(len(all_img_paths)):<br/>     img = cv2.imread(all_img_paths[i])<br/>     img = cv2.resize(img,(224,224))<br/>     mask_path = dir_seg+all_img_paths[i].split('/')[4]<br/>     img_mask = ndimage.imread(mask_path)<br/>     img_mask = cv2.resize(img_mask,(224,224))<br/>     x.append(img)<br/>     y.append(img_mask)</pre>
<ol start="3">
<li>Define a function that converts the three channel output images into 12 channels where there are 12 unique values of output:
<ol>
<li>Extract the number of unique values (objects) that are present in the output<span>, as follows</span>:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 120px">n_classes = len(set(np.array(y).flatten()))</pre>
<ol start="2">
<li style="list-style-type: none">
<ol start="2">
<li>Convert the masked image into a one-hot encoded version with as many channels as the number of objects in the total dataset<span>, as follows</span>:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 120px">def getSegmentationArr(img):<br/>      seg_labels = np.zeros(( 224, 224, n_classes ))<br/>      for c in range(n_classes):<br/>            seg_labels[: , : , c ] = (img == c ).astype(int)<br/>      return seg_labels<br/><br/>y2 = []<br/>for i in range(len(y)):<br/>     y2.append(getSegmentationArr(y[i]))<br/><br/>y2 = np.array(y2)<br/>x = x/255</pre>
<ol start="4">
<li>Build the model:
<ol>
<li>Pass the images through the pre-trained VGG16 model<span>, as follows</span>:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 60px">from keras.applications.vgg16 import VGG16 as PTModel<br/>base_pretrained_model = PTModel(input_shape = (224,224,3), include_top = False, weights = 'imagenet')<br/>base_pretrained_model.trainable = False</pre>
<ol>
<li style="list-style-type: none">
<ol start="2">
<li>Extract the VGG16 features of the image<span>, as follows</span>:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 60px">conv1 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block1_conv2').output).output<br/>conv2 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block2_conv2').output).output<br/>conv3 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block3_conv3').output).output<br/>conv4 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block4_conv3').output).output<br/>drop4 = Dropout(0.5)(conv4)<br/>conv5 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block5_conv3').output).output<br/>drop5 = Dropout(0.5)(conv5)</pre>
<ol>
<li style="list-style-type: none">
<ol start="3">
<li>Pass the convolution features through up sampling layers and concatenate them to form a U-net architecture in a sim<span>, as follows:</span></li>
</ol>
</li>
</ol>
<pre style="padding-left: 90px" class="mce-root">conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge6)<br/>conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv6)<br/>conv6 = BatchNormalization()(conv6)<br/>up7 = Conv2D(256, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(conv6))<br/>merge7 = concatenate([conv3,up7], axis = 3)<br/>conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge7)<br/>conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv7)<br/>conv7 = BatchNormalization()(conv7)<br/>up8 = Conv2D(128, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(conv7))<br/>merge8 = concatenate([conv2,up8],axis = 3)<br/>conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge8)<br/>conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv8)<br/>conv8 = BatchNormalization()(conv8)<br/>up9 = Conv2D(64, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(conv8))<br/>merge9 = concatenate([conv1,up9], axis = 3)<br/>conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge9)<br/>conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv9)<br/>conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv9)<br/>conv9 = BatchNormalization()(conv9)<br/>conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)<br/><br/>model = Model(input = base_pretrained_model.input, output = conv10)</pre>
<ol start="5">
<li>Freeze the VGG16 layers<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">for layer in model.layers[:18]:<br/>     layer.trainable = False</pre>
<ol start="6">
<li>Compile and fit the model<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">model.compile(optimizer=Adam(1e-3, decay = 1e-6), <br/> loss='categorical_crossentropy', metrics = ['accuracy'])<br/><br/>history = model.fit(x,y2,epochs=15,batch_size=1,validation_split=0.1)</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1353 image-border" src="Images/cae3a4fb-af64-4933-b45b-f0ace18dc9c3.png" style="width:29.33em;height:23.25em;" width="393" height="312"/></p>
<ol start="7">
<li>Predict on a test image<span>, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">y_pred = model.predict(x[-2:].reshape(2,224,224,3))<br/>y_predi = np.argmax(y_pred, axis=3)<br/>y_testi = np.argmax(y2[-2:].reshape(2,224,224,12), axis=3)<br/><br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>plt.subplot(231)<br/>plt.imshow(x[-1])<br/>plt.axis('off')<br/>plt.title('Original image')<br/>plt.grid('off')<br/>plt.subplot(232)<br/>plt.imshow(y[-1])<br/>plt.axis('off')<br/>plt.title('Masked image')<br/>plt.grid('off')<br/>plt.subplot(233)<br/>plt.imshow(y_predi[-1])<br/>plt.axis('off')<br/>plt.title('Predicted masked image')<br/>plt.grid('off')<br/>plt.subplot(234)<br/>plt.imshow(x[-2])<br/>plt.axis('off')<br/>plt.grid('off')<br/>plt.subplot(235)<br/>plt.imshow(y[-2])<br/>plt.axis('off')<br/>plt.grid('off')<br/>plt.subplot(236)<br/>plt.imshow(y_predi[-2])<br/>plt.axis('off')<br/>plt.grid('off')<br/>plt.show()</pre>
<p style="padding-left: 60px">The preceding code results in an image where the predicted and actual semantic images are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/9a872688-dd90-47ec-8d9c-675650960d84.png" style="width:31.25em;height:20.83em;" width="368" height="246"/></p>
<p>From the preceding images, we can see that we are able to accurately identify the semantic structures within an image with a high degree of accuracy (~90% for the model we trained).</p>


            </article>

            
        </section>
    </div>



  </body></html>