- en: Chapter 3. Transfer Image Style Across Various Domains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative Adversarial Network is the most rapidly emerging branch of deep learning
    that is suitable for a wide range of creative applications (such as image editing
    or painting, style transfer, object transfiguration, photo enhancement, and many
    more).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will first learn the technique of generating or editing
    images based on certain conditions or characteristics. Then, you will stabilize
    GAN training to overcome the mode-collapse problem and apply a convergence measure
    metric with the **Boundary Equilibrium** approach. Finally, you will perform image
    to image translation across various domains (such as changing apple to orange
    or horse to zebra) using **Cycle Consistent Generative Network**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is CGAN? Its concept and architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating fashion wardrobe from `Fashion-MNIST` data using CGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stabilizing GAN training using Boundary Equilibrium GAN with Wasserstein distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image style transfer across different domains using CycleGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating oranges from apples using Tensorflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing horse images into zebras automatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridging the gap between supervised and unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Humans learn by observing and experiencing the physical world and our brains
    are very good at prediction without doing explicit computations to arrive at the
    correct answer. Supervised learning is all about predicting a label associated
    with the data and the goal is to generalize to new unseen data. In unsupervised
    learning, the data comes in with no labels, and the goal is often not to generalize
    any kind of prediction to new data.
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, labeled data is often scarce and expensive. The Generative
    Adversarial Network takes up a supervised learning approach to do unsupervised
    learning, by generating fake/synthetic looking data, and tries to determine if
    the generated sample is fake or real. This part (a discriminator doing classification)
    is a supervised component. But the actual goal of GAN is to understand what the
    data looks like (that is, its distribution or density estimation) and be able
    to generate new examples of what it has learned.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Conditional GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Generative Adversarial Network** (**GAN**) simultaneously trains two networks—a
    generator that learns to generate fake samples from an unknown distribution or
    noise and a discriminator that learns to distinguish fake from real samples.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Conditional GAN** (**CGAN**), the generator learns to generate a fake
    sample with a specific condition or characteristics (such as a label associated
    with an image or more detailed tag) rather than a generic sample from unknown
    noise distribution. Now, to add such a condition to both generator and discriminator,
    we will simply feed some vector *y*, into both networks. Hence, both the discriminator
    *D(X,y)* and generator *G(z,y)* are jointly conditioned to two variables, *z*
    or *X* and *y*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the objective function of CGAN is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Conditional GAN](img/B08086_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The difference between GAN loss and CGAN loss lies in the additional parameter
    *y* in both a discriminator and generator function. The architecture of CGAN shown
    in the following figure now has an additional input layer (in the form of condition
    vector **C**) that gets fed into both the discriminator network and generator
    network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Conditional GAN](img/B08086_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Generating a fashion wardrobe with CGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we will implement conditional GAN to generate a fashion wardrobe
    using a `Fashion-MNIST` dataset ([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)).
    The `Fashion-MNIST` dataset is similar to the original `MNIST` dataset with a
    new set of gray-scale images and labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating a fashion wardrobe with CGAN](img/B08086_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's jump into the code to understand the working of CGAN with simple neural
    network architecture for both generator and discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define a new input variable to hold our condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we incorporate the new variable `y` into the discriminator `D(X)` and
    generator `G(z)`. Now, the discriminator`(x,y)` and generator`(z,y)` are different
    than the original GAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use our new networks and define a `loss` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'During training, we feed the value of `y` into both a generator network and
    discriminator network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we generate new data samples based on certain conditions. For this
    example, we use the image label as our condition and set the label to be `7`,
    that is, generating the image of `Sneaker`. The conditional variable `y_sample`
    is a collection of one-hot encoded vectors with value `1` in the seventh index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us execute the following steps to generate wardrobe images based on
    class label condition. First download the `Fashion-MNIST` dataset and save it
    under the `data`/`fashion` directory by running the `download.py` script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Generating a fashion wardrobe with CGAN](img/B08086_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next train the CGAN model using the following command, which will generate
    sample images after every 1000 iterations under the `output` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Generating a fashion wardrobe with CGAN](img/B08086_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the output of running CGAN using a condition label set to
    **4 (Coat)** after **80k** iteration and **7 (Sneaker)** after **60k** iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating a fashion wardrobe with CGAN](img/B08086_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Stabilizing training with Boundary Equilibrium GAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The popularity of GAN is rising rapidly among machine learning researchers.
    GAN researches can be categorized into two types: one that applies GAN into challenging
    problems and one that attempts to stabilize the training. Stabilizing GAN training
    is very crucial as the original GAN architecture suffers and has several shortcomings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mode collapse**: Where generators collapse into very narrow distribution
    and the samples generated are not diverse. This problem of course violates the
    spirit of GAN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation of convergence metric**: There is no well-defined metric that
    tells us about the convergence between discriminator loss and generator loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The improved **Wasserstein GAN** (*arXiv: 1704.00028,2017*) is a newly proposed
    GAN algorithm that promises to solve the preceding problems by minimizing the
    Wasserstein distance (or Earth-Mover distance) by providing simple gradients to
    the networks (+1 if the output is considered real and -1 if the output is considered
    fake).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main idea behind the **BEGAN** (*arXiv: 1703.10717,2017*) is to have a
    new `loss` function by using **auto-encoder** as a discriminator, where the real
    loss is derived from the Wasserstein distance (to cater the problem of mode collapse)
    between the reconstruction losses of real and generated images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stabilizing training with Boundary Equilibrium GAN](img/B08086_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A hyper-parameter gamma is added through the use of a weighting parameter *k*
    to give users the power to control the desired diversity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stabilizing training with Boundary Equilibrium GAN](img/B08086_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Unlike most GANs where discriminator and the generator are trained alternatively,
    BEGAN allows simultaneous training of both the networks in an adversarial way
    at each time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stabilizing training with Boundary Equilibrium GAN](img/B08086_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, it allows an approximate measure of convergence *M* to understand
    the performance of the whole network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stabilizing training with Boundary Equilibrium GAN](img/B08086_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The training procedure of BEGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Steps involved in training BEGAN are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator (the autoencoder) updates its weights to minimize the reconstruction
    loss of real images and in that way, starts to reconstruct real images better.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simultaneously, the discriminator starts to maximize the reconstruction loss
    of generated images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The generator works in an adversarial way to minimize the reconstruction loss
    of generated images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Architecture of BEGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As shown in the following figure, the discriminator is a convolutional network
    with both deep encoder and decoder. The decoder has multiple layers of 3x3 convolution
    followed by an **Exponential Linear Unit** (**ELU**). Downsampling is done with
    stride 2 convolutions. The embedding state of the autoencoder is mapped to fully
    connected layers. Both the generator and the decoder are deep deconvolution with
    identical architectures, but with different weights, and the upsampling is done
    using nearest-neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture of BEGAN](img/B08086_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-1: The architecture of the BEGAN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: *arXiv: 1703.10717,2017,2017*'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figures, both the generator and the decoder of the discriminator
    is shown on the left-hand side. The encoder network of the discriminator is shown
    on the right-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of BEGAN using Tensorflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us now dive deep into the code and implement the preceding concept along
    with the architecture to generate realistic attractive images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator network has multiple layers of 3x3 convolution with an `elu activation`
    function, followed by nearest neighbor upscaling, except at the final layer. The
    number of convolution layers is calculated from the height of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The encoder of the discriminator network has multiple layers of convolution
    with the `elu activation` function, followed by down-sampling using maxpooling
    except at the final convolution layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The decoder of the discriminator network is similar to the generator network,
    having multiple layers of convolution with an `elu activation` function followed
    by upsampling using nearest neighbor except at the final convolution layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the generator loss and discriminator loss for both real, fake images discussed
    previously are optimized using **Adam Optimizer** by executing the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to execute the code for generating impressive celebrity images:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First clone the following repository and then change the directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, run the following scripts to download the `CelebA` dataset under the
    `data` directory, and split it into training, validation, and test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make sure p7zip is installed on your machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now start the training process as follows, which will save the generated samples
    under the `logs` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you face the error **Conv2DCustomBackpropInputOp only supports NHWC**, then
    refer to the following issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/carpedm20/BEGAN-tensorflow/ issues/29](https://github.com/carpedm20/BEGAN-tensorflow/
    issues/29)'
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the preceding command, while the training is going on you will
    notice information such as `Model` directory, logging directory, and various losses
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementation of BEGAN using Tensorflow](img/B08086_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The output faces generated by BEGAN are visually realistic and attractive as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementation of BEGAN using Tensorflow](img/B08086_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-2: Generator output images (64x64) with gamma=0.5 after 350k steps'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following sample output images (128 x 128) are generated after 250k steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementation of BEGAN using Tensorflow](img/B08086_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-3: Generator output images (128x128) with gamma=0.5 after 250k steps'
  prefs: []
  type: TYPE_NORMAL
- en: Image to image style transfer with CycleGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Cycle Consistent Generative Network** (**CycleGAN**), originally proposed
    in the paper *Unpaired image-to-image translation using CycleGAN*—*arXiv: 1703.10593,
    2017*, aims at finding mapping between the source domain and a target domain for
    a given image without any pairing information (such as greyscale to color, image
    to semantic labels, edge-map to photograph, horse to zebra, and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: The key idea behind CycleGAN is to have two translator's F and G, where F will
    translate an image from domain *A* to domain *B*, and *G* will translate an image
    from domain *B* to domain *A*. So, for an image *x* in domain *A*, we should expect
    the function *G(F(x))* to be equivalent to *x* and similarly for an image *y*
    in domain *B*, we should expect the function *F(G(y))* to be equivalent to *y*.
  prefs: []
  type: TYPE_NORMAL
- en: Model formulation of CycleGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main goal of the CycleGAN model is to learn mapping between the two domains
    *X* and *Y* using the training samples *{xi}Ni=1* *∈* *X* and *{yj}Mj=1* *∈* *Y*.
    It also has two adversarial discriminators *D* [X] and *D* [Y]: where *D* [X]
    tries to distinguish between original images *{x}* and translated images *{F(y)}*,
    and similarly, *D* [Y] tries to distinguish between *{y}* and *{G(x)}*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CycleGAN model has two `loss` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adversarial loss**: It matches the generated image''s distribution to the
    target domain distribution:![Model formulation of CycleGAN](img/B08086_03_15.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cycle consistency loss**: It prevents the learned mappings *G* and *F* from
    contradicting each other:![Model formulation of CycleGAN](img/B08086_03_16.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The full CycleGAN objective function is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model formulation of CycleGAN](img/B08086_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Transforming apples into oranges using Tensorflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we will transfer the style from an image in domain *A* to
    an image in another domain *B*: more specifically, we will apply CycleGAN to transform
    apples into oranges or vice-versa by executing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First clone the following `git` repository and change the directory to CycleGAN-tensorflow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now download the `apple2orange` dataset ZIP file using the `download_dataset.sh`
    script, extract and save it under the `datasets` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next train the CycleGAN model using the downloaded `apple2orange` dataset.
    During the training phase, the model will be saved in the `checkpoint` directory
    and logging is enabled in the `logs` directory for visualization with TensorBoard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Transforming apples into oranges using Tensorflow](img/B08086_03_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Run the following command to visualize various losses (discriminator loss and
    generator loss) during the training phase in your browser, by navigating to `http://localhost:6006/`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Transforming apples into oranges using Tensorflow](img/B08086_03_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Finally, we will load the trained model from the `checkpoint` directory to
    transfer a style across images, hence generating oranges from apple or vice-versa
    (based on the value passed (`AtoB` or `BtoA`) to the `which_direction` parameter
    that indicates a style transfer from domain 1 to domain 2):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The following are the sample output images generated in the `test` phase:![Transforming
    apples into oranges using Tensorflow](img/B08086_03_20.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure- 4: The left-hand side shows transforming apples to oranges by passing
    AtoB in the direction parameter, whereas the right-hand side shows the output
    generated by passing BtoA in the direction parameter.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transfiguration of a horse into a zebra with CycleGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like the previous example, in this section we will use CycleGAN to transform
    a horse into a zebra or vice-versa by executing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First clone the following `git` repository and change the directory to `CycleGAN-tensorflow`
    (you can skip this step if you have already executed the previous example):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now download the `horse2zebra` ZIP file from Berkley, extract it, and save
    it under the `datasets` directory using the `download_dataset.sh` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will train our CycleGAN model using the `horse2zebra` dataset and
    use TensorBoard for visualizing the losses while training is going on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Transfiguration of a horse into a zebra with CycleGAN](img/B08086_03_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Run the following command and navigate to `http://localhost:6006/` for the
    visualizing of various generator or discriminator losses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Transfiguration of a horse into a zebra with CycleGAN](img/B08086_03_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Finally, we will use the trained model from the `checkpoint` directory to transform
    a horse into a zebra or vice-versa, depending on whether the value `AtoB` or `BtoA`
    is passed to the `which_direction` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following sample output images are generated in the `test` phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Transfiguration of a horse into a zebra with CycleGAN](img/B08086_03_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-5: The left-hand side shows transforming horse to zebra, whereas the
    right-hand side shows translating zebra into horse.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far you have learned the approach of creating images based on certain characteristics
    or conditions, by passing that condition vector into both generator and discriminator.
    Also, you have understood how to overcome model collapse problems by stabilizing
    your network training using BEGAN. Finally, you have implemented image to image
    style transfer by generating an orange from an apple and a zebra from a horse,
    or vice-versa, using CycleGAN. In the next chapter, we will solve complex real-life
    problems such as text to image synthesis and cross domain discovery by stacking
    or coupling two or more GAN models together.
  prefs: []
  type: TYPE_NORMAL
