<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Rewards and Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">Rewards are a fundamental aspect of reinforcement learning, and the concept is easy to grasp. After all, we partly teach and train others—dogs and children, for instance—with reinforcement through rewards. The concept of implementing rewards or a <kbd>reward</kbd> function in a simulation can be somewhat difficult, and prone to a lot of trial and error. This is the reason for waiting until a later and more advanced chapter to talk about rewards, building <kbd>reward</kbd> functions, and reward assistance methods such as Curriculum Learning, Backplay, Curiosity Learning, and Imitation Learning / Behavioral Cloning. </p>
<p>Here is a quick summary of the concepts we will cover in this chapter:</p>
<ul>
<li>Rewards and <kbd>reward</kbd> functions</li>
<li>Sparsity of rewards</li>
<li>Curriculum Learning</li>
<li>Understanding Backplay</li>
<li>Curiosity Learning</li>
</ul>
<p>While this is an advanced chapter, it is also an essential one and not something you want to skip over. Likewise, many of the top-performing RL demos, such as AlphaStar from DeepMind, use the advanced algorithms in this chapter to teach agents to do tasks that were previously not thought possible.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rewards and reward functions</h1>
                </header>
            
            <article>
                
<p>We often face this preconceived notion of rewards-based learning or training as comprising of an action being completed, followed by a reward, be it good or bad. While this notion of RL works completely fine for a single action-based task, such as the old multi-arm bandit problem we looked at earlier, or teaching a dog a trick, recall that reinforcement learning is really about an agent learning the value of actions by anticipating future rewards through a series of actions. At each action step, when the agent is not exploring, the agent will determine its next course of action based on what it perceives as having the best reward. What is not always so clear is what those rewards should represent numerically, and to what extent that matters. Therefore, it is often helpful to map out a simple set of <kbd>reward</kbd> functions that describe the learning behavior we want our agent to train on.</p>
<p>Let's open up the Unity editor to the <span class="packt_screen">GridWorld</span> example and learn how to create a set of <kbd>reward</kbd> functions and mappings that describe that training, as follows:</p>
<ol>
<li>Open up the <kbd>GridWorld</kbd> example from the <span class="packt_screen">Assets</span> | <span class="packt_screen">ML-Agents</span> | <span class="packt_screen">Examples</span> | <span class="packt_screen">GridWorld</span> | <span class="packt_screen">Scenes</span> folder.</li>
<li>Select the <span class="packt_screen">trueAgent</span> object in the <span class="packt_screen">Hierarchy</span> and then switch the agent's brain, at <span class="packt_screen">Grid Agent</span> | <span class="packt_screen">Brain</span>, to <span class="packt_screen">GridWorldLearning</span>.</li>
<li>Select the <span class="packt_screen">GridAcademy</span> and set the <span class="packt_screen">Grid Academy</span> | <span class="packt_screen">Brains</span> | <span class="packt_screen">Control</span> option to enabled.</li>
<li>Select and disable the <span class="packt_screen">Main Camera</span> in the scene. This will make the agent's camera the primary camera, and the one we can view the scene with.</li>
<li>Open up and prepare a Python or Anaconda window for training. Check previous chapters or the Unity documentation if you need to remember how to do this.</li>
<li>Save the scene and project.</li>
<li>Launch the sample into training using the following command at the Python/Anaconda window:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=gridworld --train</strong></pre>
<ol start="8">
<li>One of the first things you will appreciate about this sample is how quickly it trains. Remember that the primary reason the agent trains so quickly is because the state space is so small; 5x5 in this example. An example of the simulation running is shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/9c4ababe-df7d-4469-8711-394029344d77.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>GridWorld example running on 5x5 grid</span></div>
<ol start="9">
<li>Run the sample until completion. It does not take long to run, even on older systems. </li>
</ol>
<p>Notice how the agent quickly goes from a negative reward to a positive reward as it learns to place the cube over the green +. However, did you notice that the agent starts training from a negative mean reward? The agent starts with a zero reward value, so let's examine where the negative reward is coming from. In the next section, we look at how to build the <kbd>reward</kbd> functions by looking at the code.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building reward functions</h1>
                </header>
            
            <article>
                
<p>Building <kbd>reward</kbd> functions can be quite simple, as this one will be, or extremely complex, as you may well imagine. While this step is optional for training these examples, it is almost mandatory when you go to build your own environments. It can also identify problems in your training, and ways of enhancing or easing training as well. </p>
<p>Open up the Unity editor and follow this exercise to build these sample <kbd>reward</kbd> functions:</p>
<ol>
<li>Select the <span class="packt_screen">trueAgent</span> object in the <span class="packt_screen">Hierarchy</span> window and then click the target icon beside the <span class="packt_screen">Grid Agent</span> component.</li>
<li>Select <span class="packt_screen">Edit Script</span> from the <span class="packt_screen">Contact</span> menu.</li>
<li>After the script opens in your editor, scroll down to the <kbd>AgentAction</kbd> method as follows:</li>
</ol>
<pre style="padding-left: 60px">public override void AgentAction(float[] vectorAction, string textAction)<br/>{<br/>  <strong>AddReward(-0.01f);</strong><br/>  int action = Mathf.FloorToInt(vectorAction[0]);<br/><br/>  ... // omitted for brevity<br/><br/>  Collider[] blockTest = Physics.OverlapBox(targetPos, new Vector3(0.3f, 0.3f, 0.3f));<br/>  if (blockTest.Where(col =&gt; col.gameObject.CompareTag("wall")).ToArray().Length == 0)<br/>  {<br/>    transform.position = targetPos;<br/>    if (blockTest.Where(col =&gt; col.gameObject.CompareTag("goal")).ToArray().Length == 1)<br/>    {<br/>      Done();<br/>      <strong>SetReward(1f);</strong><br/>    }<br/>    if (blockTest.Where(col =&gt; col.gameObject.CompareTag("pit")).ToArray().Length == 1)<br/>    {<br/>      Done();<br/>      <strong>SetReward(-1f);</strong><br/>    }<br/>  }<br/>}</pre>
<ol start="4">
<li>We want to focus on the highlighted lines, <kbd>AddReward</kbd> and <kbd>SetReward</kbd>:
<ul>
<li><kbd>AddReward(-.1f)</kbd>: This first line denotes a step reward. Every step the agent takes will cost the agent a negative reward. This is the reason we see the agent show negative rewards until it finds the positive reward.</li>
<li><kbd>SetReward(1f)</kbd>: This the final positive reward the agent receives, and it is set to the maximum value of <kbd>1</kbd>. In these types of training scenarios, we prefer to use a range of rewards from -1 to +1.</li>
<li><kbd>SetReward(-1f)</kbd>: This is the pit of death reward, and a final negative reward.</li>
</ul>
</li>
<li>Using each of the previous statements, we can map these to <kbd>reward</kbd> functions as follows:
<ul>
<li><kbd>AddReward(-.1f)</kbd> = <img class="fm-editor-equation" src="assets/7a5780bb-6d40-4c81-9281-60fa27a826b0.png" style="width:9.33em;height:1.17em;"/></li>
<li><kbd>SetReward(1f)</kbd> = <img class="fm-editor-equation" src="assets/46cf177b-acec-48c4-895b-52e4affbe006.png" style="width:4.33em;height:1.33em;"/></li>
<li><kbd>SetReward(-1f)</kbd><span> </span>= <img class="fm-editor-equation" src="assets/93703d4c-bc45-428f-8af7-cabf822b5d37.png" style="width:5.25em;height:1.33em;"/></li>
</ul>
</li>
<li>One thing to notice here is that <kbd>AddReward</kbd> is an incremental reward, while <kbd>SetReward</kbd> sets the final value. So, the agent only ever sees a positive reward by reaching the final goal. </li>
</ol>
<p>By mapping these <kbd>reward</kbd> functions, we can see that the only way an agent can learn a positive reward is by finding its way to a goal. This is the reason the agent begins with a negative reward, it essentially only first learns to avoid wasting time or moves until it randomly encounters the goal. From there, the agent can quickly assign value to states based on previous positive rewards received. The issue is that the agent first needs to encounter a positive reward before we begin with the actual training. We discuss this particular problem in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sparsity of rewards</h1>
                </header>
            
            <article>
                
<p>We call the situation where an agent does not get enough, or any, positive rewards, a sparsity of rewards. The simplest way to show how a sparsity of rewards can happen is by example, and fortunately, the <span class="packt_screen">GridWorld</span> example can easily demonstrate this for us. Open the editor to the <span class="packt_screen">GridWorld</span> example and follow this exercise:</p>
<ol>
<li>Open the <span class="packt_screen">GridWorld</span> sample scene from where we left it in the last exercise. For the purposes of this exercise, it is also helpful to have trained the original sample to completion. <span class="packt_screen">GridWorld</span> is one of those nice compact examples that train quickly and is an excellent place to test basic concepts, or even hyperparameters.</li>
</ol>
<ol start="2">
<li>Select the <span class="packt_screen">GridAcademy</span> and change the <span class="packt_screen">Grid Academy </span>| <span class="packt_screen">Reset Parameters</span> | <span class="packt_screen">gridSize</span> to <kbd>25</kbd>, as shown in the following screen excerpt:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2fe193fd-2c8a-4336-ac0b-1d21aa78cf42.png" style="width:35.75em;height:34.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Setting the GridAcademy gridSize parameter</span></div>
<ol start="3">
<li>Save the scene and the project.</li>
<li>Launch the sample into training with the following command from your Python/Anaconda window:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=grid25x25 --train</strong></pre>
<ol start="5">
<li>This will launch the sample and, assuming you still have the <span class="packt_screen">agentCam</span> as the main camera, you should see the following in the <span class="packt_screen">Game</span> window:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a0dfe420-200c-4c5c-bd83-ccb2c2ce722f.png" style="width:42.58em;height:30.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The GridWorld with a grid size of 25x25</span></div>
<ol start="6">
<li>We have extended the game play space from a 5x5 grid to a 25x25 grid, making the goal (<span class="packt_screen">+</span>) symbol much more difficult for the agent to randomly find.</li>
<li>What you will quickly notice after a few reported iterations is how poorly the agent is performing in some cases even, reporting less than a -1 mean reward. What's more, the agent could continue training like this for a long time. In fact, it is possible the agent could never discover a reward within 100, 200, 1,000, or more iterations. Now, this may appear to be a problem of state, and, in some ways, you may think of it that way. However, remember that the input state into our agent is the same camera view, a state of 84x84 pixels image, and we have not changed that. So, for the purposes of this example, think of state in the policy RL algorithm as remaining fixed. Therefore, our best course of action in order to fix the problem is to increase the rewards.</li>
</ol>
<ol start="8">
<li>Stop the training example from the Python/Anaconda window by typing <em>Ctrl</em> + <em>C</em>. In order to be fair, we will increase the number of rewards for goals and deaths equally.</li>
<li>Back in the editor, select the <span class="packt_screen">GridAcademy</span> and increase the <span class="packt_screen">numObstacles</span> and <span class="packt_screen">numGoals</span> on the <span class="packt_screen">Grid Academy</span> | <span class="packt_screen">Reset Parameters</span> component properties, as shown in the following excerpt:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a0980bb1-9142-4054-8d0d-873996c266bb.png" style="width:31.58em;height:30.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Updating the number of Obstacles and Goals</span></span></div>
<ol start="10">
<li>Save the scene and the project.</li>
<li>Launch the training session with the following code:</li>
</ol>
<pre><strong>mlagents-learn config/trainer_config.yaml --run-id=grid25x25x5 --train</strong></pre>
<ol start="12">
<li>This is to denote that we are running the sample with five times the number of obstacles and goals. </li>
<li>Let the agent train for 25,000 iterations and notice the performance increase. Let the agent train to completion and compare the results to our first run.</li>
</ol>
<div class="packt_infobox">The problem of sparsity of rewards is generally encountered more frequently in discrete action tasks, such as <span class="packt_screen">GridWorld</span>/<span class="packt_screen">Hallway</span> and so on. because the <kbd>reward</kbd> function is often absolute. In continuous learning tasks, the <kbd>reward</kbd> function is often more gradual and is typically measured by some progress to a goal, and not just the goal itself. </div>
<p>By increasing the number of obstacles and goals—the negative and positive rewards—we are able to train the agent much more quickly, although it is likely you will see very erratic cycles of training, and the agent never truly gets as good as the original. In fact, the training actually may diverge at some point later on. The reason for this is partly because of its limited vision, and we have only partially corrected the sparse rewards problem. We can, of course, fix the issue of sparse rewards in this example by simply increasing the number of goals and obstacles. You can go back and try a value of 25 for the number of obstacles and rewards and see much more stable, long-term results.</p>
<p>Of course, in many RL problems, an increasing number of rewards is not an option, and we need to look at cleverer methods, as we will see in the next section. Fortunately, a<span> number of methods have arisen, in very brief time, looking to address the problem of sparse or difficult rewards. Unity, being at the top, quickly jumped on and implemented a number of methods, the first of which we will look at is called Curriculum Learning, which we will discuss in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Curriculum Learning</h1>
                </header>
            
            <article>
                
<p>Curriculum Learning allows for an agent to progressively learn a difficult task by stepping up the <kbd>reward</kbd> function. While the reward remains absolute, the agent finds or achieves the goal in a simpler manner, and so learns the purpose of the reward. Then, as the training progresses and as the agent learns, the difficulty of receiving a reward increases, which, in turn, forces the agent to learn. </p>
<p>Unity, of course, has a few samples of this, and we will look at the <kbd>WallJump</kbd> example of how a Curriculum Learning sample is set up in the following exercise:</p>
<ol>
<li>Open the <span class="packt_screen">WallJump</span> scene from the <span class="packt_screen">Assets</span> | <span class="packt_screen">ML-Agents</span> | <span class="packt_screen">Examples</span> | <span class="packt_screen">WallJump</span> | <span class="packt_screen">Scenes</span> folder.</li>
<li>Select the <span class="packt_screen">Academy</span> object in the <span class="packt_screen">Hierarchy</span> window.</li>
<li>Click both <span class="packt_screen">Control</span> options on <span class="packt_screen">Wall Jump Academy</span> | <span class="packt_screen">Brains</span> | <span class="packt_screen">Control</span> parameter as shown in the following excerpt:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/561b1d1e-5aca-4ddf-8e80-565983988cad.png" style="width:32.08em;height:32.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Setting the multiple brains to learning</span></span></div>
<ol start="4">
<li>This sample uses multiple brains in order to better separate the learning by task. In fact, all the brains will be trained in tandem.</li>
<li>Curriculum Learning uses a second configuration file to describe the curriculum or steps of learning the agent will undergo.</li>
<li>Open the <kbd>ML-Agents/ml-agents/config/curricul/wall-jump</kbd> folder.</li>
<li>Open the <kbd>SmallWallJumpLearning.json</kbd> file in a text editor. The file is shown for reference as follows:</li>
</ol>
<pre>      {<br/>          "measure" : "progress",<br/>          "thresholds" : [0.1, 0.3, 0.5],<br/>          "min_lesson_length": 100,<br/>          "signal_smoothing" : true, <br/>          "parameters" : <br/>          {<br/>              "small_wall_height" : [1.5, 2.0, 2.5, 4.0]<br/>          }<br/>      }</pre>
<ol start="8">
<li>This JSON file defines the configuration the <span class="packt_screen">SmallWallJumpLearning</span> brain will take as part of its curriculum or steps to learning. The definition for all these parameters are well documented in the Unity documentation, but we will take a look at parameters from the documentation as follows:
<ul>
<li><kbd>measure</kbd> <em>–</em> What to measure learning progress, and advancement in lessons by:
<ul>
<li>reward – Uses a measure received reward.</li>
<li>progress – Uses ratio of steps/max_steps.</li>
</ul>
</li>
<li><kbd>thresholds</kbd> (float array) –<em> </em>Points in value of measure where the lesson should be increased.</li>
<li><kbd>min_lesson_length</kbd> (int) <em>–</em> The minimum number of episodes that should be completed before the lesson can change. If a measure is set to reward, the average cumulative reward of the last <kbd>min_lesson_length</kbd> episodes will be used to determine if the lesson should change. Must be non-negative.</li>
</ul>
</li>
<li>What we can see by reading this file is that there are three lessons set by a <kbd>measure</kbd> of <kbd>progress</kbd> defined by the number of episodes. The episode boundaries are defined at <kbd>.1</kbd> or 10%, <kbd>.3</kbd> or 30%, and <kbd>.5</kbd> or 50% of the total episodes. With each lesson, we set parameters defined by boundaries, and in this example the parameter is <kbd>small_wall_height</kbd> with a first lesson boundary of <kbd>1.5</kbd> to <kbd>2.0</kbd>, a second lesson boundary of <kbd>2.0</kbd> to <kbd>2.5</kbd>, and a third lesson at <kbd>2.5</kbd> to <kbd>4.0</kbd>. </li>
<li>Open up a Python/Anaconda window and prepare it for training.</li>
<li>Launch the training session with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --curriculum=config/curricula/wall-jump/ --run-id=wall-jump-curriculum --train</strong></pre>
<ol start="12">
<li>The extra bit that is highlighted adds the folder to the secondary curriculum configuration. </li>
<li>You will need to wait for at least half of the full training steps to run in order to see all three levels of training. </li>
</ol>
<p>This example introduced one technique we can use to solve the problem of sparse or difficult to achieve rewards. In the next section, we look at a specialized form of Curriculum Training called Backplay.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding Backplay</h1>
                </header>
            
            <article>
                
<p>In late 2018, Cinjon Resnick released an innovative paper, titled <em>Backplay:</em> <em>Man muss immer umkehren</em>, (<a href="https://arxiv.org/abs/1807.06919">https://arxiv.org/abs/1807.06919</a>) that introduced a refined form of Curriculum Learning called Backplay. The basic premise is that you start the agent <span>more or less</span> at the goal, and then progressively move the agent back during training. This method may not work for all situations, but we will use this method with Curriculum Training to see how we can improve the <span class="packt_screen">VisualHallway</span> example in the following exercise:</p>
<ol>
<li>Open the <span class="packt_screen">VisualHallway</span> scene from the <span class="packt_screen">Assets</span> |<strong> </strong><span class="packt_screen">ML-Agents</span> | <span class="packt_screen">Examples</span> | <span class="packt_screen">Hallway</span> | <span class="packt_screen">Scenes</span> folder.</li>
<li>Make sure the scene is reset to the default starting point. If you need to, pull down the source from ML-Agents again.</li>
<li>Set the scene for learning using the <span class="packt_screen">VisualHallwayLearning</span> brain, and make sure that the agent is just using the default visual observations of 84x84.</li>
</ol>
<ol start="4">
<li>Select the <span class="packt_screen">Academy</span> object and in the <span class="packt_screen">Inspector</span> window add a new <span class="packt_screen">Hallway Academy </span>| <span class="packt_screen">Reset Parameter</span> called <kbd>distance</kbd>, as shown in the following excerpt:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cf28b7dd-01eb-4697-bab2-15e476686232.png" style="width:33.67em;height:29.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Setting a new Reset Parameter on the Academy</span></div>
<ol start="5">
<li>You can use <span class="packt_screen">Reset Parameters</span> for more than just Curriculum Learning, as they can help you easily configure training parameters within the editor. The parameter we are defining here is going to set the distance, the agent is away from the back goal region. This sample is intended to show the concept of Backplay, and in order to properly implement it we would need to move the agent right in front of the proper goal—we will defer from doing this for now.</li>
<li>Select the <span class="packt_screen">VisualHallwayArea</span> | <span class="packt_screen">Agent</span> and open the <span class="packt_screen">Hallway Academy</span> script in your code editor of choice.</li>
</ol>
<ol start="7">
<li>Scroll down to the <kbd>AgentReset</kbd> method and adjust the top line to that shown as follows:</li>
</ol>
<pre style="padding-left: 90px">public override void AgentReset()<br/>{<br/>  <strong>float agentOffset = academy.resetParameters["distance"];</strong><br/>  float blockOffset = 0f;<br/>  // ... rest removed for brevity</pre>
<ol start="8">
<li>This single line of code will adjust the starting offset of the agent to the now preset <span class="packt_screen">Reset Parameters</span> of the <span class="packt_screen">Academy</span>. Likewise, as the <span class="packt_screen">Academy</span> updates those parameters during training, the agent will also see updated values.</li>
<li>Save the file and return to the editor. The editor will recompile your code changes and let you know if everything is okay. A red error in the console will typically mean you have a compiler error, likely caused by incorrect syntax.</li>
<li>Open a prepared Python/Anaconda window and run the training session with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=vh_backplay --train</strong></pre>
<ol start="11">
<li>This will run the session in regular mode, without Curriculum Learning, but it will adjust the starting position of the agent to be closer to the goals. Let this sample run and see how well the agent performs now that it starts so close to the goals.</li>
</ol>
<p>Let the training run for a while and observe the difference in training from the original. One thing you will notice is that the agent can't help but run into the reward now, which is what we are after. The next piece we need to implement is the Curriculum Learning part, where we will move the agent back as it learns to find the reward in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing Backplay through Curriculum Learning</h1>
                </header>
            
            <article>
                
<p>In the last section, we implemented the first part of Backplay, which is having the agent start next to, or very close to the goal. The next part we need to accomplish is progressively moving the agent back to its intended starting point using Curriculum Learning. Open up the Unity editor to the <span class="packt_screen">VisualHallway</span> scene again and follow these steps:</p>
<ol>
<li>Open the <kbd>ML-Agents/ml-agents/config</kbd> folder with a file explorer or command shell.</li>
<li>Create a new folder called <kbd>hallway</kbd> and navigate to the new folder.</li>
<li>Open a text editor or create a new JSON text file called <kbd>VisualHallwayLearning.json</kbd> in the new directory. <strong>JavaScript Object Notation</strong> (<strong>JSON</strong>)<strong> </strong>is intended to describe objects in JavaScript, it has become a standard for configuration settings as well.</li>
<li>Enter the following JSON text in the new file:</li>
</ol>
<pre style="padding-left: 60px">{<br/>    "measure" : "rewards",<br/>    "thresholds" : [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7],<br/>    "min_lesson_length": 100,<br/>    "signal_smoothing" : true, <br/>    "parameters" : <br/>    {<br/>        "distance" : [12, 8, 4, 2, -2, -4, -8, -12]<br/>    }</pre>
<ol start="5">
<li>This configuration file defines a curriculum that we will use to train an agent on Backplay. The file defines a <kbd>measure</kbd> of <kbd>rewards</kbd> and <kbd>thresholds</kbd> that define when the agent will advance to the next level of training. When a reward threshold is hit for a minimum episode length of <kbd>100</kbd> steps, than the training will advance to the next <kbd>distance</kbd> parameter. Notice how we define the distance parameter with <kbd>12</kbd>, representing a distance close to the goals, and then decreasing. You could, of course, create a function that maps different range values, but we will leave that up to you.</li>
<li>Save the file after you are done editing.</li>
<li> Launch a training session from a Python/Anaconda window with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --curriculum=config/curricula/hallway/ --run-id=hallway-curriculum --train</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="8">
<li>After the training starts, notice how the curriculum is getting set in the Python/Anaconda window, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6fe16960-1238-46f1-ba9e-737ee9c2831b.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Watching the curriculum parameters getting set in training</span></div>
<ol start="9">
<li>Wait for the agent to train, and see how many levels of training it can accomplish before the end of the session.</li>
</ol>
<p>Now, one thing we need to come clean about is that this sample is more an innovative example than a true example of Backplay. Actual Backplay is described as putting the agent at the goal and working backward. In this example, we are putting the agent almost at the goal and working backward. The difference is subtle, but, by now, hopefully you can appreciate that, in terms of training, it could be significant.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Curiosity Learning</h1>
                </header>
            
            <article>
                
<p>Up until now, we have considered just the extrinsic or external rewards an agent may receive in an environment. The <span class="packt_screen">Hallway</span> example, for instance, gives a +1 external reward when the agent reaches the goal, and a -1 external reward if it gets the wrong goal. However, real animals like us can actually learn based on internal motivations, or by using an internal <kbd>reward</kbd> function. A great example of this is a baby (a cat, a human, or whatever) that has an obvious natural motivation to be curious through play. The curiosity of playing provides the baby with an internal or intrinsic reward, but the actual act itself gives it a negative external or extrinsic reward. After all, the baby is expending energy, a negative external reward, yet it plays on and on in order to learn more general information about its environment. This, in turn, allows it to explore more of the environment and ultimately attain some very difficult goal, such as hunting, or going to work.</p>
<p>This form of internal or intrinsic reward modeling falls into a subclass of RL, called Motivated Reinforcement Learning. As you may well imagine, this whole arc of learning could have huge applications in gaming, from creating NPCs to more believable opponents that actually get motivated by some personality trait or emotion. Imagine having a computer opponent that can get angry, or even, compassionate? Of course, we are a long way from getting there, but in the interim, Unity has added an intrinsic reward system in order to model agent curiosity, and this is called Curiosity Learning.</p>
<p><strong>Curiosity Learning</strong> (<strong>CL</strong>) was first developed by researchers at the University of California, Berkley, in a paper called <em><span>Curiosity-Driven Exploration by </span></em><span><em>Self-Supervised Prediction,</em> which</span> you can find at <a href="https://pathak22.github.io/noreward-rl/">https://pathak22.github.io/noreward-rl/.</a> The paper goes on to describe a system of solving sparse rewards problems using forward and inverse neural networks. They called the system an <strong>Intrinsic Curiosity Module</strong> (<strong>ICM</strong>), with the intent for it to be used as a layer or module on top of other RL systems. This is exactly what Unity did, and they have added this as a module to ML-Agents.</p>
<div class="packt_tip">The Lead Researcher at Unity, Dr. Arthur Juliani, has an excellent blog post on their implementation that can be found at <a href="https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/">https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/</a>.</div>
<p>ICM works by using an inverse neural network that is trained using the current and next observation of the agent. It uses an encoder to encode a prediction on what the action was between the two states, current and next. Then, the forward network is trained on the current observation and action in which it encodes to the next observation. The difference is then taken between the real and predicted encodings from the inverse and forward models. In this case, the bigger the difference, the bigger the surprise, and the more intrinsic the rewards. A diagram extracted from Dr. Juliani's blog is shown as follows, describing how this works:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/7c0c6c7f-8ad5-4fa4-a258-c0327d8e0ddf.png" style="width:40.00em;height:26.67em;"/><br/>
<br/>
Inner workings of the Curiosity Learning Module</div>
<p>The diagram shows the depiction of the two models and layers in blue, forward and inverse, with the blue lines depicting network flow, the green box representing the intrinsic model calculation, and the reward output in the form of the green dotted lines. </p>
<p>Well, that's enough theory, its time to see how this CL works in practice. Fortunately, Unity has a very well developed environment that features this new module that is called Pyramids. Let's open Unity and follow the next exercise to see this environment in action:</p>
<ol>
<li>Open the <span class="packt_screen">Pyramid</span> scene from the <span class="packt_screen">Assets</span> | <span class="packt_screen">M</span><span class="packt_screen">L-Agents</span> | <span class="packt_screen">Examples</span> | <span class="packt_screen">Pyramids</span> | <span class="packt_screen">Scenes</span> folder.</li>
</ol>
<ol start="2">
<li>Select the <span class="packt_screen">AreaPB(1)</span> to <span class="packt_screen">AreaPB(15) </span>in the <span class="packt_screen">Hierarchy</span> window and then deactivate these objects in the <span class="packt_screen">Inspector</span> window.</li>
<li>Leave the scene in player mode. For the first time, we want you to play the scene on your own and figure out the goal. Even if you read the blog or played the scene, try again, but this time, think what reward functions would need to be in place.</li>
</ol>
<ol start="4">
<li>Press <span class="packt_screen">Play</span> in the editor and start playing the game in <span class="packt_screen">Player</span> mode. If you have not played the game before or understand the premise, don't be surprised if it takes you a while to solve the puzzle.</li>
</ol>
<p>Now, for those of you that didn't read or play ahead, here is the premise. The scene starts where the agent is randomly placed into an area of rooms with pyramids of stone in which one has a switch. The goal of the agent is to activate the switch that then spawns a pyramid of sand boxes with a large gold box on top. The switch turns from red to green after it is activated. After the pyramid appears, the agent then needs to knock the pyramid over and retrieve the gold box. It certainly is not the most complex of puzzles, but one that does require a bit of exploration and curiosity.</p>
<p>Imagine if we tried to model this form of curiosity, or need to explore, with a set of <kbd>reward</kbd> functions. We would need a <kbd>reward</kbd> function for activating the button, moving to rooms, knocking over blocks, and, of course, getting the gold box. Then we would have to determine the value of each of those objectives, perhaps using some form of <strong>Inverse Reinforcement Learning</strong> (<strong>IRL</strong>). However, with Curiosity Learning, we can create the reward function for just the end goal of getting the box (+1), and perhaps a small negative step goal (.0001), then use intrinsic curiosity rewards to let the agent learn the remaining steps. Quite a clever trick, and we will see how this works in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Curiosity Intrinsic module in action</h1>
                </header>
            
            <article>
                
<p>With our appreciation of the difficulty of the <span class="packt_screen">Pyramids</span> task, we can move on to training the agent with curiosity in the following exercise:</p>
<ol>
<li>Open the <span class="packt_screen">Pyramids</span> scene in the editor.</li>
<li>Select the <span class="packt_screen">AreaRB</span> | <span class="packt_screen">Agent</span> object in the <span class="packt_screen">Hierarchy</span> window.</li>
<li>Switch the <span class="packt_screen">Pyramid Agent</span> | <span class="packt_screen">Brain</span> for the <span class="packt_screen">PyramidsLearning</span> brain.</li>
<li>Select the <span class="packt_screen">Academy</span> object in the <span class="packt_screen">Hierarchy</span> window.</li>
</ol>
<ol start="5">
<li>Enable the <span class="packt_screen">Control</span> option on the <span class="packt_screen">Academy | Pyramid Academy | Brains | Control</span> property, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/31a355e2-ae0d-4f6f-8635-5f620ace0450.png" style="width:34.75em;height:22.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Setting the Academy to Control</span></div>
<ol start="6">
<li>Open a Python or Anaconda console and prepare it for training.</li>
<li>Open the <kbd>trainer_config.yaml</kbd> file located in the <kbd>ML-Agents/ml-agents/config</kbd> folder.</li>
<li>Scroll down to the <kbd>PyramidsLearning</kbd> configuration section, as follows:</li>
</ol>
<pre>      PyramidsLearning:<br/>          <strong>use_curiosity: true</strong><br/>          summary_freq: 2000<br/>          <strong>curiosity_strength: 0.01</strong><br/><strong>          curiosity_enc_size: 256</strong><br/>          time_horizon: 128<br/>          batch_size: 128<br/>          buffer_size: 2048<br/>          hidden_units: 512<br/>          num_layers: 2<br/>          beta: 1.0e-2<br/>          max_steps: 5.0e5<br/>          num_epoch: 3</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<ol start="9">
<li>There are three new configuration parameters highlighted in bold:
<ul>
<li><kbd>use_curiosity</kbd>: Set this to <kbd>true</kbd> to use the module, but it is generally <kbd>false</kbd> by default.</li>
<li><kbd>curiosity_strength</kbd>: This is how strongly the agent values the intrinsic reward of curiosity over the extrinsic ones.</li>
<li><kbd>curiosity_enc_size</kbd>: This is the size of the encoded layer we compress the network to. If you think back to autoencoders, you can see the size of 256 is quite large, but also consider the size of the state space or observation space you may be encoding.</li>
</ul>
</li>
</ol>
<div class="packt_infobox" style="padding-left: 60px">Leave the parameters at the values they are set.</div>
<ol start="10">
<li>Launch the training session with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>      mlagents-learn config/trainer_config.yaml --run-id=pyramids --train</strong></pre>
<p>While this training session may take a while, it can be entertaining to watch how the agent explores. Even with the current settings, using only one training area, you may be able to see the agent solve the puzzle on a few iterations. </p>
<p>Since ICM is a module, it can quickly be activated for any other example we want to see the effects on, which is what we will do in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Trying ICM on Hallway/VisualHallway</h1>
                </header>
            
            <article>
                
<p>Not unlike the agents we train, we learn quite well from trial and error. This is the reason we practice, practice, and practice more of those very difficult tasks such as dancing, singing, or playing an instrument. RL is no different and requires the practitioner to learn the ins and outs training through the rigors of trial, error, and further exploration. Therefore, in this next exercise, we are going to combine Backplay (Curriculum Learning) and Curiosity Learning together into our old friend, the Hallway, and see what effect it has, as follows:</p>
<ol>
<li>Open the <span class="packt_screen">Hallway</span> or <span class="packt_screen">VisualHallway</span> scene (your preference) as we last left it, with Curriculum Learning enabled and set to simulate Backplay. </li>
<li>Open the <kbd>trainer_config.yaml</kbd> configuration file location in the <kbd>ML-Agents/ml-agents/config</kbd> folder.</li>
</ol>
<ol start="3">
<li>Scroll down to the <kbd>HallwayLearning </kbd>or <kbd>VisualHallwayLearning</kbd> brain configuration parameters and add the following additional configuration lines:</li>
</ol>
<pre style="padding-left: 60px">HallwayLearning:<br/>    <strong>use_curiosity: true</strong><br/><strong>    curiosity_strength: 0.01</strong><br/><strong>    curiosity_enc_size: 256</strong><br/>    use_recurrent: true<br/>    sequence_length: 64<br/>    num_layers: 2<br/>    hidden_units: 128<br/>    memory_size: 256<br/>    beta: 1.0e-2<br/>    gamma: 0.99<br/>    num_epoch: 10<br/>    buffer_size: 1024<br/>    batch_size: 1000<br/>    max_steps: 5.0e5<br/>    summary_freq: 1000<br/>    time_horizon: 64</pre>
<ol start="4">
<li>This will enable the curiosity module for this example. We use the same settings for curiosity as we used for the last <span class="packt_screen">Pyrmarids</span> example.</li>
<li>Make sure this sample is prepared for curriculum Backplay as we configured it in that section. If you need to, go back and review that section and add the capability to this example before continuing. </li>
</ol>
<div class="packt_tip"><span>This may require you to create a new curricula file that uses the same parameters as we did previously. Remember that the curricula file needs to have the same name as the brain it is being used against.</span></div>
<ol start="6">
<li>Open a Python/Anaconda window prepared for training and start training with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --curriculum=config/curricula/hallway/ --run-id=hallway_bp_cl --train</strong></pre>
<ol start="7">
<li>Let the training run until completion, as the results can be interesting and show some of the powerful possibilities of layering learning enhancements for extrinsic and intrinsic rewards.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>This exercise showed how to run an agent with both Curriculum Learning simulating Backplay, and Curiosity Learning adding an aspect of agent motivation to the learning. As you may well imagine, intrinsic reward learning and the whole field of Motivated Reinforcement Learning may lead to some interesting advances and enhancements to our DRL.</p>
<p>In the next section, we will review a number of helpful exercises that should help you learn more about these concepts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>While your motivation may vary as to why you are reading this book, hopefully by now you can appreciate the value of just doing things on your own. As always, we present these exercises for your enjoyment and learning, and hope you have fun completing them:</p>
<ol>
<li>Select another sample scene that uses discrete actions and write the reward functions that go with it. Yes, that means you will need to open up and look at the code.</li>
<li>Select a continuous action scene and try writing the reward functions for it. While this one may be difficult, it is essential if you want to build your own control training agent.</li>
<li>Add Curriculum Learning to one of the other discrete action samples we have explored. Decide on how you can break the training into levels of difficulty and create parameters for controlling the evolution of the training.</li>
<li>Add Curriculum Learning to a continuous action sample. This is more difficult, and you likely want to perform exercise number two first.</li>
<li>Implement actual Backplay on the Hallway environment by placing the agent starting at the goal and then, as the agent trains, move it back to the desired start with Curriculum Learning. </li>
<li>Implement Backplay on another discrete action example you have run and see the effect it has on training.</li>
<li>Implement Curiosity Learning on the <span class="packt_screen">VisualPyramids</span> example and notice the difference in training.</li>
<li>Implement Curiosity Learning on a continuous action example and notice the effect it has on training. Is it what you expected?</li>
<li> Disable Curiosity Learning on the <span class="packt_screen">Pyramids</span> example and see what effect this has on agent training.</li>
<li>Think of a way in which you could add Backplay to the <span class="packt_screen">VisualPyramids</span> example. You'll get bonus points if you actually build it.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As you can see, the exercises are getting more demanding as we progress through the book. Remember, even completing one or two of these exercises will make a difference in your take-away knowledge.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at a fundamental component of RL, and that is rewards. We learned that, when building training environments, it was best that we defined a set of <kbd>reward</kbd> functions our agent will live by. By understanding these equations, we get a better sense of how frequent or sparse rewards can negatively affect training. We then looked at a few methods, the first of which is called Curriculum Learning, that could be used to ease or step the agent's extrinsic rewards. After that, we explored another technique, called Backplay, that used a reverse play technique and Curriculum Training to enhance an agent's training. Finally, we looked at internal or intrinsic rewards, and the concept of Motivated Reinforcement Learning. We then learned that the first intrinsic reward system developed into ML-Agents was to give an agent a motivation for curiosity. We looked at how to use Curiosity Learning on a few examples, and even incorporated it with Backplay via Curriculum Learning.</p>
<p>In the next chapter, we look to more reward helper solutions in the form of Imitation and Transfer Learning, where we will learn how a human's gameplay experience can be mapped to a form of learning called Imitation Learning or Behavioral Cloning.</p>


            </article>

            
        </section>
    </body></html>