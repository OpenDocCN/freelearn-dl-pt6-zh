["```py\nval conf: MultiLayerConfiguration = new NeuralNetConfiguration.Builder\n   .updater(Updater.ADAM)\n   .l2(1e-5)\n   .weightInit(WeightInit.XAVIER)\n   .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)\n   .gradientNormalizationThreshold(1.0)\n   .list\n   .layer(0, new GravesLSTM.Builder().nIn(vectorSize).nOut(256)\n     .activation(Activation.TANH)\n     .build)\n   .layer(1, new RnnOutputLayer.Builder().activation(Activation.SOFTMAX)\n     .lossFunction(LossFunctions.LossFunction.MCXENT).nIn(256).nOut(2).build)\n   .pretrain(false).backprop(true).build\n```", "```py\nval net = new MultiLayerNetwork(conf)\n net.init()\n net.setListeners(new ScoreIterationListener(1))\n```", "```py\nval WORD_VECTORS_PATH: String = getClass().getClassLoader.getResource(\"GoogleNews-vectors-negative300.bin\").getPath\n val wordVectors = WordVectorSerializer.loadStaticModel(new File(WORD_VECTORS_PATH))\n```", "```py\nval DATA_PATH: String = getClass.getClassLoader.getResource(\"aclImdb\").getPath\n val train = new SentimentExampleIterator(DATA_PATH, wordVectors, batchSize, truncateReviewsToLength, true)\n val test = new SentimentExampleIterator(DATA_PATH, wordVectors, batchSize, truncateReviewsToLength, false)\n```", "```py\ngroupId: com.google.guava\n artifactId: guava\n version: 19.0\ngroupId: org.apache.commons\n artifactId: commons-math3\n version: 3.4\n```", "```py\nval filePath: String = new ClassPathResource(\"rawSentences.txt\").getFile.getAbsolutePath\n```", "```py\nval iter: SentenceIterator = new BasicLineIterator(filePath)\n```", "```py\nval tokenizerFactory: TokenizerFactory = new DefaultTokenizerFactory\n tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor)\n```", "```py\nval vec = new Word2Vec.Builder()\n   .minWordFrequency(5)\n   .iterations(1)\n   .layerSize(100)\n   .seed(42)\n   .windowSize(5)\n   .iterate(iter)\n   .tokenizerFactory(tokenizerFactory)\n   .build\n```", "```py\nvec.fit()\n```", "```py\nWordVectorSerializer.writeWordVectors(vec, \"wordVectors.txt\")\n```", "```py\nval lst = vec.wordsNearest(\"house\", 10)\n println(\"10 Words closest to 'house': \" + lst)\n```", "```py\nval glove = new Glove.Builder()\n   .iterate(iter)\n   .tokenizerFactory(tokenizerFactory)\n   .alpha(0.75)\n   .learningRate(0.1)\n   .epochs(25)\n   .xMax(100)\n   .batchSize(1000)\n   .shuffle(true)\n   .symmetric(true)\n   .build\n```", "```py\nglove.fit()\n```", "```py\nval simD = glove.similarity(\"old\", \"new\")\n println(\"old/new similarity: \" + simD)\n```", "```py\nval words: util.Collection[String] = glove.wordsNearest(\"time\", 10)\n println(\"Nearest words to 'time': \" + words)\n```", "```py\npip install tensorflow-hub\n```", "```py\npip install pandas\n```", "```py\nimport tensorflow as tf\n import tensorflow_hub as hub\n import os\n import pandas as pd\n import re\n```", "```py\ndef load_directory_data(directory):\n   data = {}\n   data[\"sentence\"] = []\n   data[\"sentiment\"] = []\n   for file_path in os.listdir(directory):\n     with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n       data[\"sentence\"].append(f.read())\n       data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n   return pd.DataFrame.from_dict(data)\n```", "```py\ndef load_dataset(directory):\n   pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n   neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n   pos_df[\"polarity\"] = 1\n   neg_df[\"polarity\"] = 0\n   return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n```", "```py\ndef download_and_load_datasets(force_download=False):\n   dataset = tf.keras.utils.get_file(\n       fname=\"aclImdb.tar.gz\",\n       origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n       extract=True)\n\n   train_df = load_dataset(os.path.join(os.path.dirname(dataset),\n                                        \"aclImdb\", \"train\"))\n   test_df = load_dataset(os.path.join(os.path.dirname(dataset),\n                                       \"aclImdb\", \"test\"))\n\n   return train_df, test_df\n```", "```py\ntrain_df, test_df = download_and_load_datasets()\n```", "```py\nprint(train_df.head())\n```", "```py\ntrain_input_fn = tf.estimator.inputs.pandas_input_fn(\n     train_df, train_df[\"polarity\"], num_epochs=None, shuffle=True)\npredict_train_input_fn \n```", "```py\npredict_train_input_fn = tf.estimator.inputs.pandas_input_fn(\n     train_df, train_df[\"polarity\"], shuffle=False)\n```", "```py\npredict_test_input_fn = tf.estimator.inputs.pandas_input_fn(\n     test_df, test_df[\"polarity\"], shuffle=False)\n```", "```py\nembedded_text_feature_column = hub.text_embedding_column(\n     key=\"sentence\",\n     module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n```", "```py\nestimator = tf.estimator.DNNClassifier(\n     hidden_units=[500, 100],\n     feature_columns=[embedded_text_feature_column],\n     n_classes=2,\n     optimizer=tf.train.AdagradOptimizer(learning_rate=0.003))\n```", "```py\nestimator.train(input_fn=train_input_fn, steps=1000);\n```", "```py\ntrain_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\n print(\"Training set accuracy: {accuracy}\".format(**train_eval_result))\n```", "```py\ntest_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\n print(\"Test set accuracy: {accuracy}\".format(**test_eval_result))\n```", "```py\ndef get_predictions(estimator, input_fn):\n   return [x[\"class_ids\"][0] for x in estimator.predict(input_fn=input_fn)]\n```", "```py\nwith tf.Graph().as_default():\n   cm = tf.confusion_matrix(train_df[\"polarity\"],\n                            get_predictions(estimator, predict_train_input_fn))\n   with tf.Session() as session:\n     cm_out = session.run(cm)\n```", "```py\ncm_out = cm_out.astype(float) / cm_out.sum(axis=1)[:, np.newaxis]\n```", "```py\nfrom keras.datasets import imdb\n```", "```py\nvocabulary_size = 5000\n\n (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n```", "```py\nprint('---review---')\n print(X_train[6])\n print('---label---')\n print(y_train[6])\n```", "```py\nword2id = imdb.get_word_index()\n id2word = {i: word for word, i in word2id.items()}\n print('---review with words---')\n print([id2word.get(i, ' ') for i in X_train[6]])\n print('---label---')\n print(y_train[6])\n```", "```py\nprint('Maximum review length: {}'.format(\n len(max((X_train + X_test), key=len))))\n print('Minimum review length: {}'.format(\n len(min((X_test + X_test), key=len))))\n```", "```py\nfrom keras.preprocessing import sequence\n\n max_words = 500\n X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n```", "```py\nfrom keras import Sequential\n from keras.layers import Embedding, LSTM, Dense, Dropout\n\n embedding_size=32\n model=Sequential()\n model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n model.add(LSTM(100))\n model.add(Dense(1, activation='sigmoid'))\n```", "```py\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n```", "```py\nbatch_size = 64\n num_epochs = 3\n```", "```py\nX_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n\n model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)\n```", "```py\nscores = model.evaluate(X_test, y_test, verbose=0)\n print('Test accuracy:', scores[1])\n```", "```py\nmodel.save('sa_rnn.h5')\n```", "```py\nval saRnn = new ClassPathResource(\"sa_rnn.h5\").getFile.getPath\n val model = KerasModelImport.importKerasSequentialModelAndWeights(saRnn)\n```"]