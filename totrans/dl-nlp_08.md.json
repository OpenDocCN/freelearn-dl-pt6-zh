["```py\n    from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n    from keras.layers import RepeatVector, Dense, Activation, Lambda\n    from keras.optimizers import Adam\n    from keras.utils import to_categorical\n    from keras.models import load_model, Model\n    import keras.backend as K\n    import numpy as np\n    from babel.dates import format_date\n    from faker import Faker\n    import random\n    from tqdm import tqdm\n    ```", "```py\n    fake = Faker()\n    fake.seed(12345)\n    random.seed(12345)\n    ```", "```py\n    FORMATS = ['short',\n               'medium',\n               'long',\n               'full',\n               'full',\n               'full',\n               'full',\n               'full',\n               'full',\n               'full',\n               'full',\n               'full',\n               'full',\n               'd MMM YYY',\n               'd MMMM YYY',\n               'dd MMM YYY',\n               'd MMM, YYY',\n               'd MMMM, YYY',\n               'dd, MMM YYY',\n               'd MM YY',\n               'd MMMM YYY',\n               'MMMM d YYY',\n               'MMMM d, YYY',\n               'dd.MM.YY']\n    # change this if you want it to work with another language\n    LOCALES = ['en_US']\n    def load_date():\n        \"\"\"\n            Loads some fake dates\n            :returns: tuple containing human readable string, machine readable string, and date object\n        \"\"\"\n        dt = fake.date_object()\n        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))\n            human_readable = human_readable.lower()\n            human_readable = human_readable.replace(',','')\n            machine_readable = dt.isoformat()\n        return human_readable, machine_readable, dt\n    ```", "```py\n    def load_dataset(m):\n        \"\"\"\n            Loads a dataset with m examples and vocabularies\n            :m: the number of examples to generate\n        \"\"\"\n        human_vocab = set()\n        machine_vocab = set()\n        dataset = []\n        Tx = 30\n        for i in tqdm(range(m)):\n            h, m, _ = load_date()\n            if h is not None:\n                dataset.append((h, m))\n                human_vocab.update(tuple(h))\n                machine_vocab.update(tuple(m))\n        human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'],\n                         list(range(len(human_vocab) + 2))))\n        inv_machine = dict(enumerate(sorted(machine_vocab)))\n        machine = {v:k for k,v in inv_machine.items()}\n        return dataset, human, machine, inv_machine\n    ```", "```py\n    m = 10000\n    dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)\n    ```", "```py\n    def string_to_int(string, length, vocab):\n        \"\"\"\n        Converts all strings in the vocabulary into a list of integers representing the positions of the\n        input string's characters in the \"vocab\"\n        Arguments:\n        string -- input string, e.g. 'Wed 10 Jul 2007'\n        length -- the number of timesteps you'd like, determines if the output will be padded or cut\n        vocab -- vocabulary, dictionary used to index every character of your \"string\"\n        Returns:\n        rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n        \"\"\"\n    ```", "```py\n        string = string.lower()\n        string = string.replace(',','')\n        if len(string) > length:\n            string = string[:length]\n        rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n        if len(string) < length:\n            rep += [vocab['<pad>']] * (length - len(string))\n        return rep\n    ```", "```py\n    def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n        X, Y = zip(*dataset)\n        print(\"X shape before preprocess: {}\".format(X))\n        X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n        Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n        print(\"X shape from preprocess: {}\".format(X.shape))\n        print(\"Y shape from preprocess: {}\".format(Y))\n        Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n        Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n        return X, np.array(Y), Xoh, Yoh\n    Tx = 30\n    Ty = 10\n    X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n    ```", "```py\n    print(\"X.shape:\", X.shape)\n    print(\"Y.shape:\", Y.shape)\n    print(\"Xoh.shape:\", Xoh.shape)\n    print(\"Yoh.shape:\", Yoh.shape)\n    ```", "```py\n    index = 0\n    print(\"Source date:\", dataset[index][0])\n    print(\"Target date:\", dataset[index][1])\n    print()\n    print(\"Source after preprocessing (indices):\", X[index].shape)\n    print(\"Target after preprocessing (indices):\", Y[index].shape)\n    print()\n    print(\"Source after preprocessing (one-hot):\", Xoh[index].shape)\n    print(\"Target after preprocessing (one-hot):\", Yoh[index].shape)\n    ```", "```py\n    def softmax(x, axis=1):\n        \"\"\"Softmax activation function.\n        # Arguments\n            x : Tensor.\n            axis: Integer, axis along which the softmax normalization is applied.\n        # Returns\n            Tensor, output of softmax transformation.\n        # Raises\n            ValueError: In case 'dim(x) == 1'.\n        \"\"\"\n        ndim = K.ndim(x)\n        if ndim == 2:\n            return K.softmax(x)\n        elif ndim > 2:\n            e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n            s = K.sum(e, axis=axis, keepdims=True)\n            return e / s\n        else:\n            raise ValueError('Cannot apply softmax to a tensor that is 1D')\n    ```", "```py\n    # Defined shared layers as global variables\n    repeator = RepeatVector(Tx)\n    concatenator = Concatenate(axis=-1)\n    densor1 = Dense(10, activation = \"tanh\")\n    densor2 = Dense(1, activation = \"relu\")\n    activator = Activation(softmax, name='attention_weights')\n    dotor = Dot(axes = 1)\n    ```", "```py\n    def one_step_attention(h, s_prev):\n        \"\"\"\n        Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n        \"alphas\" and the hidden states \"h\" of the Bi-LSTM.\n\n        Arguments:\n        h -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_h)\n        s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n\n        Returns:\n        context -- context vector, input of the next (post-attetion) LSTM cell\n        \"\"\"\n    ```", "```py\n        s_prev = repeator(s_prev)\n    ```", "```py\n        concat = concatenator([h, s_prev])\n    ```", "```py\n        e = densor1(concat)\n    ```", "```py\n        energies = densor2(e)\n    ```", "```py\n        alphas = activator(energies)\n    ```", "```py\n        context = dotor([alphas, h])\n\n        return context\n    ```", "```py\n    n_h = 32\n    n_s = 64\n    post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n    output_layer = Dense(len(machine_vocab), activation=softmax)\n    ```", "```py\n    def model(Tx, Ty, n_h, n_s, human_vocab_size, machine_vocab_size):\n        \"\"\"\n        Arguments:\n        Tx -- length of the input sequence\n        Ty -- length of the output sequence\n        n_h -- hidden state size of the Bi-LSTM\n        n_s -- hidden state size of the post-attention LSTM\n        human_vocab_size -- size of the python dictionary \"human_vocab\"\n        machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n        Returns:\n        model -- Keras model instance\n        \"\"\"\n    ```", "```py\n        X = Input(shape=(Tx, human_vocab_size), name=\"input_first\")\n        s0 = Input(shape=(n_s,), name='s0')\n        c0 = Input(shape=(n_s,), name='c0')\n        s = s0\n        c = c0\n    ```", "```py\n        outputs = []\n    ```", "```py\n        h = Bidirectional(LSTM(n_h, return_sequences=True))(X)\n    ```", "```py\n        for t in range(Ty):\n    ```", "```py\n            context = one_step_attention(h, s)\n    ```", "```py\n            s, _, c = post_activation_LSTM_cell(context, initial_state = [s,c])\n    ```", "```py\n            out = output_layer(s)\n\n            # Append \"out\" to the \"outputs\" list\n            outputs.append(out)\n\n    ```", "```py\n        model = Model(inputs=[X, s0, c0], outputs=outputs)\n\n        return model\n    model = model(Tx, Ty, n_h, n_s, len(human_vocab), len(machine_vocab))\n    model.summary()\n    ```", "```py\n    opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    ```", "```py\n    s0 = np.zeros((m, n_s))\n    c0 = np.zeros((m, n_s))\n    outputs = list(Yoh.swapaxes(0,1))\n    model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)\n    ```", "```py\n    EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n    for example in EXAMPLES:\n\n        source = string_to_int(example, Tx, human_vocab)\n        source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))#.swapaxes(0,1)\n        source = source[np.newaxis, :]\n        prediction = model.predict([source, s0, c0])\n        prediction = np.argmax(prediction, axis = -1)\n        output = [inv_machine_vocab[int(i)] for i in prediction]\n\n        print(\"source:\", example)\n        print(\"output:\", ''.join(output))\n    ```"]