["```py\ndef LSTM( hidden_size):\n  W = shared_norm((hidden_size, 4*hidden_size))\n  U = shared_norm((hidden_size, 4*hidden_size))\n  b = shared_zeros(4*hidden_size)\n\n  params = [W, U, b]\n\n  def forward(m, X, h_, C_ ):\n    XW = T.dot(X, W)\n    h_U = T.dot(h_, U)\n    bfr_actv = XW + h_U + b\n\n    f = T.nnet.sigmoid( bfr_actv[:, 0:hidden_size] )\n    i = T.nnet.sigmoid( bfr_actv[:, 1*hidden_size:2*hidden_size] )\n    o = T.nnet.sigmoid( bfr_actv[:, 2*hidden_size:3*hidden_size] )\n    Cp = T.tanh( bfr_actv[:, 3*hidden_size:4*hidden_size] )\n\n    C = i*Cp + f*C_\n    h = o*T.tanh( C )\n    C = m[:, None]*C + (1.0 - m)[:, None]*C_\n    h = m[:, None]*h + (1.0 - m)[:, None]*h_\n\n    h, C = T.cast(h, theano.config.floatX), T.cast(h, theano.config.floatX)\n    return h, C\n\n  return forward, params\n```", "```py\ndef stack( voca_size, hidden_size, num_layers, embedding=None, target_voca_size=0):\n    params = []\n\n    if embedding == None:\n        embedding = shared_norm( (voca_size, hidden_size) )\n        params.append(embedding)\n\n    layers = []\n    for i in range(num_layers):\n        f, p = LSTM(hidden_size)\n        layers.append(f)\n        params += p\n\n    def forward( mask, inputs, h_, C_, until_symbol = None):\n        if until_symbol == None :\n            output = embedding[inputs]\n        else:\n            output = embedding[T.cast( inputs.argmax(axis=-1), \"int32\" )]\n\n        hos = []\n        Cos = []\n      for i in range(num_layers):\n            hs, Cs = layers[i](mask, output, h_[i], C_[i])\n            hos.append(hs)\n            Cos.append(Cs)\n            output = hs\n\n        if target_voca_size != 0:\n            output_embedding = shared_norm((hidden_size, target_voca_size))\n            params.append(output_embedding)\n            output = T.dot(output, output_embedding)\n\n        outputs = (T.cast(output, theano.config.floatX),T.cast(hos, theano.config.floatX),T.cast(Cos, theano.config.floatX))\n\n        if until_symbol != None:\n            return outputs, theano.scan_module.until( T.eq(output.argmax(axis=-1)[0], until_symbol) )\n\n        return outputs\n\n    return forward, params\n```", "```py\nencoderInputs, encoderMask = T.imatrices(2)\nh0,C0 = T.tensor3s(2)\n\nencoder, encoder_params = stack(valid_data.source_size, opt.hidden_size, opt.num_layers)\n\n([encoder_outputs, hS, CS], encoder_updates) = theano.scan(\n  fn = encoder,\n  sequences = [encoderMask, encoderInputs],\n  outputs_info = [None, h0, C0])\n```", "```py\ndecoderInputs, decoderMask, decoderTarget = T.imatrices(3)\n\ndecoder, decoder_params = stack(valid_data.target_size, opt.hidden_size, opt.num_layers, target_voca_size=valid_data.target_size)\n\n([decoder_outputs, h_vals, C_vals], decoder_updates) = theano.scan(\n  fn = decoder,\n  sequences = [decoderMask, decoderInputs],\n  outputs_info = [None, hS[-1], CS[-1]])\n\nparams = encoder_params + decoder_params\n\n```", "```py\nprediction_mask = theano.shared(np.ones(( opt.max_sent_size, 1), dtype=\"int32\"))\n\nprediction_start = np.zeros(( 1, valid_data.target_size), dtype=theano.config.floatX)\nprediction_start[0, valid_data.idx_start] = 1\nprediction_start = theano.shared(prediction_start)\n\n([decoder_outputs, h_vals, C_vals], decoder_updates) = theano.scan(\n  fn = decoder,\n  sequences = [prediction_mask],\n  outputs_info = [prediction_start, hS[-1], CS[-1]],\n  non_sequences = valid_data.idx_stop\n  )\n```", "```py\n    python 0-preprocess_translations.py --srcfile data/src-train.txt --targetfile data/targ-train.txt --srcvalfile data/src-val.txt --targetvalfile data/targ-val.txt --outputfile data/demo\n    First pass through data to get vocab...\n    Number of sentences in training: 10000\n    Number of sentences in valid: 2819\n    Source vocab size: Original = 24995, Pruned = 24999\n    Target vocab size: Original = 35816, Pruned = 35820\n    (2819, 2819)\n    Saved 2819 sentences (dropped 181 due to length/unk filter)\n    (10000, 10000)\n    Saved 10000 sentences (dropped 0 due to length/unk filter)\n    Max sent length (before dropping): 127\n    ```", "```py\n    python 1-train.py  --dataset translation\n    ```", "```py\n    python 1-train.py  --dataset translation --model model_translation_e100_n2_h500\n    ```", "```py\nwget http://www.mpi-sws.org/~cristian/data/cornell_movie_dialogs_corpus.zip -P /sharedfiles/\nunzip /sharedfiles/cornell_movie_dialogs_corpus.zip  -d /sharedfiles/cornell_movie_dialogs_corpus\n\npython 0-preprocess_movies.py\n```", "```py\nif opt.dataset == \"chatbot\":\n    embeddings = encoder_params[0]\n```", "```py\npython 1-train.py  --dataset chatbot # training\npython 1-train.py  --dataset chatbot --model model_chatbot_e100_n2_h500 # answer my question\n```"]