["```py\nfrom keras.datasets import mnist\nimport numpy\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.utils import np_utils\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n```", "```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.subplot(221)\nplt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\nplt.grid('off')\nplt.subplot(222)\nplt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\nplt.grid('off')\nplt.subplot(223)\nplt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\nplt.grid('off')\nplt.subplot(224)\nplt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\nplt.grid('off')\nplt.show()\n```", "```py\n# flatten 28*28 images to a 784 vector for each image\nnum_pixels = X_train.shape[1] * X_train.shape[2]\nX_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n```", "```py\n# one hot encode outputs\ny_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)\nnum_classes = y_test.shape[1]\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(1000, input_dim=784, activation='relu'))\nmodel.add(Dense(10,  activation='softmax'))\n```", "```py\nmodel.summary()\n\n```", "```py\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n```", "```py\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=32, verbose=1)\n```", "```py\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nacc_values = history_dict['acc']\nval_acc_values = history_dict['val_acc']\nepochs = range(1, len(val_loss_values) + 1)\n```", "```py\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nplt.subplot(211)\nplt.plot(epochs, history.history['loss'], 'rx', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Test loss')\nplt.title('Training and test loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.subplot(212)\nplt.plot(epochs, history.history['acc'], 'rx', label='Training accuracy')\nplt.plot(epochs, val_acc_values, 'b', label='Test accuracy')\nplt.title('Training and test accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()]) \nplt.legend()\nplt.show()\n```", "```py\npreds = model.predict(X_test)\n```", "```py\nimport numpy as np\ncorrect = 0\nfor i in range(len(X_test)):\n    pred = np.argmax(preds[i],axis=0)\n    act = np.argmax(y_test[i],axis=0)\n    if(pred==act):\n        correct+=1\n    else:\n        continue\n\ncorrect/len(X_test)\n```", "```py\noutput = 1/(1+np.exp(-(w*x + b))\n```", "```py\nfrom keras.datasets import mnist\nimport numpy as np\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.utils import np_utils\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n```", "```py\n# flatten 28*28 images to a 784 vector for each image\nnum_pixels = X_train.shape[1] * X_train.shape[2]\nX_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n\nX_train = X_train/255\nX_test = X_test/255\n```", "```py\n# one hot encode outputs\ny_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)\nnum_classes = y_test.shape[1]\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(1000, input_dim=784, activation='relu'))\nmodel.add(Dense(10,  activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n```", "```py\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=32, verbose=1)\n```", "```py\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nnum_pixels = X_train.shape[1] * X_train.shape[2]\nX_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\nX_train = X_train/255\nX_test = X_test/255\ny_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)\nnum_classes = y_test.shape[1]\n```", "```py\nX_train.flatten()\n```", "```py\nplt.hist(X_train.flatten())\nplt.grid('off')\nplt.title('Histogram of input values')\nplt.xlabel('Input values')\nplt.ylabel('Frequency of input values')\n```", "```py\nX_train = 1-X_train\nX_test = 1-X_test\n```", "```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.subplot(221)\nplt.imshow(X_train[0].reshape(28,28), cmap=plt.get_cmap('gray'))\nplt.grid('off')\nplt.subplot(222)\nplt.imshow(X_train[1].reshape(28,28), cmap=plt.get_cmap('gray'))\nplt.grid('off')\nplt.subplot(223)\nplt.imshow(X_train[2].reshape(28,28), cmap=plt.get_cmap('gray'))\nplt.grid('off')\nplt.subplot(224)\nplt.imshow(X_train[3].reshape(28,28), cmap=plt.get_cmap('gray'))\nplt.grid('off')\nplt.show()\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(1000,input_dim=784,activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32, verbose=1)\n```", "```py\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nnum_pixels = X_train.shape[1] * X_train.shape[2]\nX_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\nX_train = X_train/255\nX_test = X_test/255\ny_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)\nnum_classes = y_test.shape[1]\nmodel = Sequential()\nmodel.add(Dense(1000,input_dim=784,activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=30000, verbose=1)\n```", "```py\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nnum_pixels = X_train.shape[1] * X_train.shape[2]\nX_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\nX_train = X_train/255\nX_test = X_test/255\ny_train = np_utils.to_categorical(y_train)\ny_test = np_utils.to_categorical(y_test)\nnum_classes = y_test.shape[1]\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(1000, input_dim=784, activation='relu'))\nmodel.add(Dense(1000,activation='relu'))\nmodel.add(Dense(1000,activation='relu'))\nmodel.add(Dense(10,  activation='softmax'))\n```", "```py\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=250, batch_size=1024, verbose=1)\n```", "```py\ndef feed_forward(inputs, outputs, weights):\n     hidden = np.dot(inputs,weights[0])\n     out = hidden+weights[1]\n     squared_error = (np.square(out - outputs))\n     return squared_error\n\ndef update_weights(inputs, outputs, weights, epochs, lr): \n    for epoch in range(epochs):\n        org_loss = feed_forward(inputs, outputs, weights)\n        wts_tmp = deepcopy(weights)\n        wts_tmp2 = deepcopy(weights)\n        for ix, wt in enumerate(weights):\n            print(ix, wt)\n            wts_tmp[-(ix+1)] += 0.0001\n            loss = feed_forward(inputs, outputs, wts_tmp)\n            del_loss = np.sum(org_loss - loss)/(0.0001*len(inputs))\n            wts_tmp2[-(ix+1)] += del_loss*lr\n            wts_tmp = deepcopy(weights)\n        weights = deepcopy(wts_tmp2)\n    return wts_tmp2\n```", "```py\nw_val = []\nb_val = []\nfor k in range(1000):\n     w_new, b_new = update_weights(x,y,w,(k+1),0.01)\n     w_val.append(w_new)\n     b_val.append(b_new)\n```", "```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(w_val)\nplt.title('Weight value over different epochs when learning rate is 0.01')\nplt.xlabel('epochs')\nplt.ylabel('weight value')\nplt.grid('off')\n```", "```py\nfrom keras import optimizers\nadam=optimizers.Adam(lr=0.01)\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(1000, input_dim=784, activation='relu'))\nmodel.add(Dense(10,  activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy']) \n\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=1024, verbose=1)\n```", "```py\nfrom keras import optimizers\nadam=optimizers.Adam(lr=0.1)\n\nmodel = Sequential()\nmodel.add(Dense(1000, input_dim=784, activation='relu'))\nmodel.add(Dense(10,  activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=1024, verbose=1)\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(1000, input_dim=784, activation='relu'))\nmodel.add(Dense(10,  activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1)\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(1000,input_dim=784,activation='relu',kernel_regularizer=l2(0.1)))model.add(Dense(10,  activation='softmax',kernel_regularizer=l2(0.1)))\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=1024, verbose=1)\n```", "```py\nmodel.get_weights()[0].flatten()\n```", "```py\nplt.hist(model.get_weights()[0].flatten())\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(1000, input_dim=784, activation='relu'))\nmodel.add(Dropout(0.75))\nmodel.add(Dense(10,  activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=1024, verbose=1)\n```", "```py\nplt.hist(model.get_weights()[-2].flatten())\n```", "```py\nfrom keras.layers.normalization import BatchNormalization\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(1000, input_dim=784,activation='relu', kernel_regularizer = l2(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation='softmax', kernel_regularizer = l2(0.01)))\n```", "```py\nfrom keras.optimizers import Adam\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=1024, verbose=1)\n```"]