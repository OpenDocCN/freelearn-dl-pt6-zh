["```py\ncd ch4\njupyter notebook\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y)\n```", "```py\nfrom cntk.losses import cross_entropy_with_softmax\nfrom cntk.learners import sgd \nfrom cntk.logging import ProgressPrinter\n\nprogress_writer = ProgressPrinter(0)\nloss = cross_entropy_with_softmax(z, labels)\nlearner = sgd(z.parameters, 0.1)\n\ntrain_summary = loss.train((X_train,y_train), \n                           parameter_learners=[learner], \n                           callbacks=[progress_writer], \n                           minibatch_size=16, max_epochs=15)\n```", "```py\nfrom sklearn.metrics import confusion_matrix\n\ny_true = np.argmax(y_test, axis=1)\ny_pred = np.argmax(z(X_test), axis=1)\n\nmatrix = confusion_matrix(y_true=y_true, y_pred=y_pred)\n\nprint(matrix)\n```", "```py\n[[ 8 0 0]\n [ 0 4 6]\n [ 0 0 10]]\n```", "```py\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ng = sns.heatmap(matrix, \n                annot=True, \n                xticklabels=label_encoder.classes_.tolist(), \n                yticklabels=label_encoder.classes_.tolist(), \n                cmap='Blues')\n\ng.set_yticklabels(g.get_yticklabels(), rotation=0)\n\ng.set_xlabel('Predicted species')\ng.set_ylabel('Actual species')\ng.set_title('Confusion matrix for iris prediction model')\n\nplt.show()\n```", "```py\nimport cntk\n\n@cntk.Function\ndef criterion_factory(output, target):\n    loss = cntk.losses.cross_entropy_with_softmax(output, target)\n    metric = cntk.metrics.classification_error(output, target)\n\n    return loss, metric\n```", "```py\nfrom cntk.losses import cross_entropy_with_softmax\nfrom cntk.learners import sgd \nfrom cntk.logging import ProgressPrinter\n\nprogress_writer = ProgressPrinter(0)\nloss = criterion_factory(z, labels)\nlearner = sgd(z.parameters, 0.1)\n\ntrain_summary = loss.train((X_train,y_train), \n                           parameter_learners=[learner], \n                           callbacks=[progress_writer], \n                           minibatch_size=16, max_epochs=15)\n```", "```py\n average      since    average      since      examples\n loss       last     metric       last \n ------------------------------------------------------\nLearning rate per minibatch: 0.1\n 1.48       1.48       0.75       0.75            16\n 1.18       1.03       0.75       0.75            48\n 0.995      0.855      0.518      0.344           112\n 1.03       1.03      0.375      0.375            16\n 0.973      0.943      0.396      0.406            48\n 0.848      0.753      0.357      0.328           112\n 0.955      0.955      0.312      0.312            16\n 0.904      0.878      0.375      0.406            48\n```", "```py\nloss.test((X_test, y_test))\n```", "```py\n{'metric': 0.36666666666666664, 'samples': 30}\n```", "```py\nimport cntk\nimport numpy \n\ncntk.cntk_py.set_fixed_random_seed(1337)\nnumpy.random.seed = 1337\n```", "```py\nimport cntk\n\n@cntk.Function\ndef criterion_factory(output, target):\n    loss = cntk.losses.cross_entropy_with_softmax(output, target)\n    metric = cntk.losses.fmeasure(output, target)\n\n    return loss, metric\n```", "```py\n{'metric': 0.831014887491862, 'samples': 30}\n```", "```py\nfrom cntk import default_options, input_variable\nfrom cntk.layers import Dense, Sequential\nfrom cntk.ops import relu\n\nwith default_options(activation=relu):\n    model = Sequential([\n        Dense(64),\n        Dense(64),\n        Dense(1,activation=None)\n    ])\n\nfeatures = input_variable(X.shape[1])\ntarget = input_variable(1)\n\nz = model(features)\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX = df_cars.drop(columns=['mpg']).values.astype(np.float32)\ny = df_cars.iloc[:,0].values.reshape(-1,1).astype(np.float32)\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n```", "```py\nimport cntk \n\ndef absolute_error(output, target):\n    return cntk.ops.reduce_mean(cntk.ops.abs(output - target))\n\n@cntk.Function\ndef criterion_factory(output, target):\n    loss = squared_error(output, target)\n    metric = absolute_error(output, target)\n\n    return loss, metric\n```", "```py\nfrom cntk.logging import ProgressPrinter\nfrom cntk.losses import squared_error\nfrom cntk.learners import sgd\n\nloss = criterion_factory(z, target)\nlearner = sgd(z.parameters, 0.001)\n\nprogress_printer = ProgressPrinter(0)\n\ntrain_summary = loss.train((X_train,y_train), \n                           parameter_learners=[learner], \n                           callbacks=[progress_printer],\n                           minibatch_size=16,\n                           max_epochs=10)\n```", "```py\n average      since    average      since      examples\n    loss       last     metric       last              \n ------------------------------------------------------\nLearning rate per minibatch: 0.001\n      690        690       24.9       24.9            16\n      654        636       24.1       23.7            48\n      602        563       23.1       22.3           112\n      480        373       20.4         18           240\n       62         62       6.19       6.19            16\n     47.6       40.4       5.55       5.24            48\n     44.1       41.5       5.16       4.87           112\n     32.9       23.1        4.5       3.92           240\n     15.5       15.5       3.12       3.12            16\n     15.7       15.7       3.13       3.14            48\n     15.8       15.9       3.16       3.18           112\n[...]\n```", "```py\nloss.test((X_test,y_test))\n```", "```py\n{'metric': 1.8967978561980814, 'samples': 79}\n```", "```py\nfrom cntk.io import StreamDef, StreamDefs, MinibatchSource, CTFDeserializer, INFINITELY_REPEAT\n\ndef create_datasource(filename, limit=INFINITELY_REPEAT):\n    labels_stream = StreamDef(field='labels', shape=3, is_sparse=False)\n    features_stream = StreamDef(field='features', shape=4, is_sparse=False)\n\n    deserializer = CTFDeserializer(filename, StreamDefs(labels=labels_stream, features=features_stream))\n    minibatch_source = MinibatchSource(deserializer, randomize=True, max_sweeps=limit)\n\n    return minibatch_source\n\ntraining_source = create_datasource('iris_train.ctf')\ntest_source = create_datasource('iris_test.ctf', limit=1)\n```", "```py\nfrom cntk.logging import ProgressPrinter\nfrom cntk.train import Trainer, training_session\n\nminibatch_size = 16\nsamples_per_epoch = 150\nnum_epochs = 30\nmax_samples = samples_per_epoch * num_epochs\n\ninput_map = {\n    features: training_source.streams.features,\n    labels: training_source.streams.labels\n}\n\nprogress_writer = ProgressPrinter(0)\ntrainer = Trainer(z, (loss, metric), learner, progress_writer)\n\nsession = training_session(trainer, \n                           mb_source=training_source,\n                           mb_size=minibatch_size, \n                           model_inputs_to_streams=input_map, \n                           max_samples=max_samples,\n                           test_config=test_config)\n\nsession.train()\n```", "```py\nfrom cntk.train import TestConfig\n\ntest_config = TestConfig(test_source)\n```", "```py\n average      since    average      since      examples\n    loss       last     metric       last              \n ------------------------------------------------------\nLearning rate per minibatch: 0.1\n     1.57       1.57      0.214      0.214            16\n     1.38       1.28      0.264      0.289            48\n     1.41       1.44      0.147     0.0589           112\n     1.27       1.15     0.0988     0.0568           240\n     1.17       1.08     0.0807     0.0638           496\n      1.1       1.03     0.0949      0.109          1008\n    0.973      0.845      0.206      0.315          2032\n    0.781       0.59      0.409       0.61          4080\nFinished Evaluation [1]: Minibatch[1-1]: metric = 70.72% * 30;\n```", "```py\nimport cntk\nfrom cntk.losses import cross_entropy_with_softmax, fmeasure\n\n@cntk.Function\ndef criterion_factory(outputs, targets):\n    loss = cross_entropy_with_softmax(outputs, targets)\n    metric = fmeasure(outputs, targets, beta=1)\n\n    return loss, metric\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom cntk.logging import ProgressPrinter\nfrom cntk.train import Trainer\n\nprogress_writer = ProgressPrinter(0)\ntrainer = Trainer(z, loss, learner, progress_writer)\n\nfor _ in range(0,30):\n    input_data = pd.read_csv('iris.csv', \n        names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], \n        index_col=False, chunksize=16)\n\n    for df_batch in input_data:\n        feature_values = df_batch.iloc[:,:4].values\n        feature_values = feature_values.astype(np.float32)\n\n        label_values = df_batch.iloc[:,-1]\n\n        label_values = label_values.map(lambda x: label_mapping[x])\n        label_values = label_values.values\n\n        encoded_labels = np.zeros((label_values.shape[0], 3))\n        encoded_labels[np.arange(label_values.shape[0]), label_values] = 1.\n\n        trainer.train_minibatch({features: feature_values, labels: encoded_labels})\n```", "```py\naverage since average since  examples \nloss    last  metric  last \n------------------------------------------------------ \nLearning rate per minibatch: 0.1\n1.45    1.45  -0.189  -0.189  16 \n1.24    1.13  -0.0382  0.0371 48 \n1.13    1.04  0.141    0.276  112 \n1.21    1.3   0.0382  -0.0599 230 \n1.2     1.18  0.037    0.0358 466\n```", "```py\nfrom cntk import Evaluator\n\nevaluator = Evaluator(loss.outputs[1], [progress_writer])\n\ninput_data = pd.read_csv('iris.csv', \n        names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], \n        index_col=False, chunksize=16)\n\nfor df_batch in input_data:\n    feature_values = df_batch.iloc[:,:4].values\n    feature_values = feature_values.astype(np.float32)\n\n    label_values = df_batch.iloc[:,-1]\n\n    label_values = label_values.map(lambda x: label_mapping[x])\n    label_values = label_values.values\n\n    encoded_labels = np.zeros((label_values.shape[0], 3))\n    encoded_labels[np.arange(label_values.shape[0]), label_values] = 1.\n\n    evaluator.test_minibatch({ features: feature_values, labels: encoded_labels})\n\nevaluator.summarize_test_progress()\n```", "```py\nFinished Evaluation [1]: Minibatch[1-11]: metric = 65.71% * 166;\n```", "```py\ntrain_summary = loss.train((X_train,y_train), \n                           parameter_learners=[learner], \n                           callbacks=[progress_writer], \n                           minibatch_size=16, max_epochs=15)\n```", "```py\nfrom cntk.logging import ProgressPrinter\n\ncallbacks = [\n    ProgressPrinter(0)\n]\n\ntrainer = Trainer(z, (loss, metric), learner, [callbacks])\n```", "```py\nProgressPrinter(0, log_to_file='test_log.txt'),\n```", "```py\ntest_log.txt\nCNTKCommandTrainInfo: train : 300\nCNTKCommandTrainInfo: CNTKNoMoreCommands_Total : 300\nCNTKCommandTrainBegin: train\n average since average since examples\n loss  last  metric  last \n ------------------------------------------------------\nLearning rate per minibatch: 0.1\n 8.91 8.91   0.296 0.296 16\n 3.64 1      0.229 0.195 48\n 2.14 1.02   0.215 0.204 112\n 0.875 0.875  0.341 0.341 16\n 0.88 0.883  0.331 0.326 48\n```", "```py\npip install tensorboard\n```", "```py\nimport time\nfrom cntk.logging import TensorBoardProgressWriter\n\ntensorboard_writer = TensorBoardProgressWriter(log_dir='logs/{}'.format(time.time()), freq=1, model=z)\n```", "```py\ntensorboard --logdir logs\n```"]