<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapters, we learned about mapping input to a target—where, the input and output values are provided. In this chapter, we will be learning about reinforcement learning, where the objective that we want to achieve and the environment that we operate in are provided, but not any input or output mapping. The way in which reinforcement learning works is that we generate input values (the state in which the agent is) and the corresponding output values (the reward the agent achieves for taking certain actions in a state) by taking random actions at the start and gradually learning from the generated input data (actions in a state) and output values (rewards achieved by taking certain actions).</p>
<p>In this chapter, we will cover the following:</p>
<ul>
<li>The optimal action to take in a simulated game with a non-negative reward</li>
<li>The optimal action to take in a state in a simulated game</li>
<li>Q-learning to maximize rewards when playing Frozen Lake</li>
<li>Deep Q-learning to balance a cart pole</li>
<li>Deep Q-learning to play <span>the </span>Space Invaders game</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The optimal action to take in a simulated game with a non-negative reward</h1>
                </header>
            
            <article>
                
<p>In this section, we will understand the way in which we can take the right action for a simulated game. Note that this exercise will primarily help you to grasp how reinforcement learning works.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Let's define the environment we are operating in this simulated setting.</p>
<p>You have three boxes, on which two players are playing a game. Player 1 marks a box with 1 and player 2 marks one with 2. The player who is able to mark two consecutive boxes wins.</p>
<p>The empty board for this game looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/a6def8c7-0658-4359-b45a-8cae86613db7.png" style="width:8.92em;height:3.83em;" width="114" height="49"/></p>
<p>For the problem we just defined, only player 1 has an opportunity to win the game. The possible scenarios in which player 1 wins are either of the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/129d43fc-ab45-4bc1-acc2-63b404e4cd47.png" style="width:8.92em;height:7.67em;" width="116" height="100"/></p>
<p>From the problem setting, the intuitive way in which player 1 wins is when player 1 chooses the middle box. This way, irrespective of which box is chosen by player 2, player 1 will win on their subsequent move.</p>
<p>While the first step for player 1 is intuitive for us, in the next section we will learn about how an agent can automatically figure out the optimal first move.</p>
<p>The strategy that we will adopt to solve this problem is as follows:</p>
<ul>
<li>We initialize an empty board</li>
<li>Player 1 chooses a box randomly</li>
<li>Player 2 chooses a box randomly from the remaining 2 boxes</li>
<li>Depending on the box player 1 is left with, we update the reward for player 1:
<ul>
<li>If player 1 is able to place 1s in two consecutive boxes, he is a winner and will a reward of 1</li>
<li>Otherwise, player 1 will get a reward of 0</li>
</ul>
</li>
<li>Repeat the preceding exercise 100 times, where the game is played and we store a reward for the given sequence of moves</li>
<li>Now, we will go ahead and calculate the average reward for the various first moves that were taken</li>
<li>The box that was chosen in the first move, that has the highest average reward over 100 iterations is the optimal first move for player 1</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The strategy defined above is coded as follows (the code file is available as <kbd>Finding_optimal_policy.ipynb</kbd> in GitHub):</p>
<ol>
<li>Define the game environment and the function to play the game:</li>
</ol>
<pre style="padding-left: 60px">def play_game():<br/>     empty_board = [0,0,0]<br/>     move = []<br/>     for step in range(3):<br/>         index_to_choose = [i for i,j in enumerate(empty_board) if j==0]<br/>         samp = random.sample(range(len(index_to_choose)), 1)[0] <br/>         if(step%2==0):<br/>             empty_board[index_to_choose[samp]]=1<br/>             move.append(index_to_choose[samp])<br/>         else:<br/>             empty_board[index_to_choose[samp]]=2 <br/>     return(reward(empty_board), move[0])</pre>
<p style="padding-left: 60px">In the preceding code, we are initializing an empty board with zero values and playing a random move named <kbd>samp</kbd>. Player 1 takes the first move and then player 2 takes their turn, followed by player 1. We fill up the empty board in this manner.</p>
<ol start="2">
<li>Define a function to calculate <span>the </span>reward at the end of a game:</li>
</ol>
<pre style="padding-left: 60px">def reward(empty_board):<br/>     reward = 0<br/>     if((empty_board[0]==1 &amp; empty_board[1]==1) | (empty_board[1]==1 &amp; empty_board[2]==1)):<br/>         reward = 1<br/>     else:<br/>         reward = 0<br/>     return reward</pre>
<ol start="3">
<li>Play the game <kbd>100</kbd> times:</li>
</ol>
<pre style="padding-left: 60px">rew = []<br/>step = []<br/>for i in range(100):<br/>     r, move = play_game()<br/>     rew.append(r)<br/>     step.append(move) </pre>
<ol start="4">
<li>Calculate the reward for choosing a certain first move:</li>
</ol>
<pre style="padding-left: 60px">sub_list = [i for i,j in enumerate(step) if j==1]<br/>final_reward = 0<br/>count = 0<br/>for i in sub_list:<br/>     final_reward += rew[i]<br/>     count+=1<br/>final_reward/count</pre>
<p>When you repeat the preceding code for multiple options for the first move, you will notice that the average reward is highest when occupying the second square.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The optimal action to take in a state in a simulated game </h1>
                </header>
            
            <article>
                
<p>In the previous scenario, we considered a simplistic case where there is a reward when the objective is achieved. In this scenario, we will complicate game by having negative rewards too. However, the objective remains the same: maximizing <span>the </span>reward in the given problem setting where the environment has both positive and negative rewards.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The environment we are working on is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/77011aa8-e3e7-41cc-9bf4-452707112100.png" style="width:8.25em;height:5.75em;" width="113" height="79"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We start at the cell with <strong>S</strong> in it and our objective is to reach the cell where the reward is <strong>+1</strong>. In order to maximize the chances of achieving <span>the </span>reward, we will be using Bellman's equation, which calculates the value of each cell in the preceding grid as follows:</p>
<p><em>Value of current cell = reward of moving from the current cell to next cell + discount factor * value of next cell</em></p>
<p>Additionally, in the current problem, the reward for moving to any cell other than the cell with a reward of <strong>+1</strong> is <em>0</em>. </p>
<p>The discount factor can be thought of as the energy expended moving from one cell to another. Thus, a cell that is far away from the rewarding cell will have a lower value compared to other cells in the current problem setting.</p>
<p>Once we calculate the value of each cell, we move to <span>the </span>cell that has the highest value of all the cells that an agent could move to.</p>
<p>The strategy that we'll adopt to calculate the value of each cell is as follows:</p>
<ul>
<li>Initialize an empty board.</li>
<li>Define the possible actions that an agent could take in a cell.</li>
<li>Define the state that an agent will be in, for the action the agent takes in <span>the </span>current cell.</li>
<li>Calculate the value of <span>the </span>current state, which depends on the reward for moving to the next state, as well as the value of <span>the </span>next state.</li>
<li>Update the cell value of <span>the </span>current state based on <span>the earlier </span>calculation.</li>
<li>Additionally, store the action taken in the current state to move to the next state.</li>
<li>Note that, in the initial iterations, <span>the </span>values of cells that are far away from the end goal remain zero, while the values of cells that are adjacent to the end state rise.</li>
<li>As we iterate the previous steps multiple times, we will be in a position to update the cell values and, thus, in a position to decide the optimal route for the agent to follow.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this section, we'll code the strategy that we laid out in <span>the </span>previous section <span>(t</span>he code file is available as <kbd>Finding_optimal_policy.ipynb</kbd> in GitHub):</p>
<ol>
<li>Initialize an empty board:</li>
</ol>
<pre style="padding-left: 60px">empty_board = [[0,0,0]<br/>             ,[0,0,1]]</pre>
<ol start="2">
<li>Define the actions that can be taken in different states—where <kbd>D</kbd> represents moving down, <kbd>R</kbd> is right, <kbd>L</kbd> is left, and <kbd>U</kbd> is moving up:</li>
</ol>
<pre style="padding-left: 60px">state_actions = {(0,0):('D','R')<br/>                 ,(0,1):('D','R','L')<br/>                 ,(0,2):('D','L')<br/>                 ,(1,0):('U','R')<br/>                 ,(1,1):('L','U','R') <br/>                 }</pre>
<ol start="3">
<li>Define the function that extracts the next state given the current state and <span>the </span>action taken in <span>the </span>current state:</li>
</ol>
<pre style="padding-left: 60px">def get_next_state(curr_state, action):<br/>     i,j = curr_state<br/>     if action=='D':<br/>         i = i+1<br/>     elif action=='U':<br/>         i = i-1<br/>     elif action=='R':<br/>         j = j+1<br/>     elif action=='L':<br/>         j = j-1<br/>     else:<br/>         print('unk')<br/>     return((i,j))</pre>
<ol start="4">
<li>Initialize the lists where <span>the </span>state, action, and rewards are appended:</li>
</ol>
<pre style="padding-left: 60px">curr_state = (0,0)<br/>state_action_reward = []<br/>state = []<br/>state_action = []</pre>
<p class="mce-root"/>
<ol start="5">
<li>Execute 100 actions at most in an episode (an episode is an instance of a game) where random action is taken in a cell (state) and calculate the value of <span>the </span>current state based on the reward for moving to the next state, as well as the value of next state.</li>
</ol>
<p style="padding-left: 60px">Repeat the above exercise for <kbd>100</kbd> iterations (episodes/games) and calculate the value of each cell:</p>
<pre style="padding-left: 60px">for m in range(100):<br/>     curr_state = (0,0)<br/>     for k in range(100):<br/>         reward = 0<br/>         action = state_actions[curr_state][random.sample(range(len(state_actions[curr_state])),1)[0]]<br/>         next_state = get_next_state(curr_state, action)</pre>
<p style="padding-left: 60px">In the preceding code, we are taking random actions in a state and then calculating the next state for the action taken in <span>the </span>current state:</p>
<pre style="padding-left: 60px">        state.append(curr_state)<br/>        empty_board[curr_state[0]][curr_state[1]] = reward + empty_board[next_state[0]][next_state[1]]*0.9 <br/>        empty_board[curr_state[0]][curr_state[1]])</pre>
<p style="padding-left: 60px">In the preceding code, we are updating the value of a state:</p>
<pre class="mce-root">        curr_state = next_state<br/>        state_action.append(action)<br/><br/>        if(next_state==(1,2)):<br/>             reward+=1<br/>             break</pre>
<p style="padding-left: 60px" class="mce-root">The preceding results in the following final cell state values across all the cells:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/695f027e-2888-4b08-b8ca-dba64348fc4e.png" width="264" height="40"/></p>
<p>Based on the preceding output, the agent can take take either the right action or <span>the </span>down action at the start of <span>the </span>game (where the agent starts from <span>the </span>top-left corner). However, if the agent takes the down action in <span>the </span>first step, it is better off taking <span>the <em>r</em></span><em>ight</em> action in the next step, as the cell state value is higher for the state to the right compared to the state that is above the current cell state.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Imagine the environment (the cells and their corresponding rewards) looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/a7b18107-739a-4819-b47b-2a8998b62a92.png" style="width:8.42em;height:6.08em;" width="109" height="79"/></p>
<p>The actions that could be taken at different states areas follows:</p>
<pre>state_actions = {(0,0):('D','R')<br/>                 ,(0,1):('D','R')<br/>                 ,(1,0):('R')<br/>                 ,(1,1):('R') <br/>                 }</pre>
<p>The cell state values in various cells after iterating through the game multiple times is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/96dfeacf-52d9-47f3-928c-275ff5b32948.png" width="257" height="40"/></p>
<p>From the preceding results, we can see that the agent is better off taking an action down from the top-left corner than moving to its right, as the cell state value is higher for the cell state that is below the starting cell.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Q-learning to maximize rewards when playing Frozen Lake</h1>
                </header>
            
            <article>
                
<p>So far, in the previous sections, we have been taking random actions in a given state. Additionally, we have also been defining the environment and calculating the next state, actions, and the reward for a move via code. In this section, we will leverage OpenAI's Gym package to navigate through <span>the F</span>rozen Lake environment.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The Frozen Lake environment looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/2f4064cc-944d-4d94-ab94-0a5c8b963098.png" width="45" height="70"/></p>
<p>The agent starts from <span>the </span><strong>S</strong> state and the goal is to reach the <strong>G</strong> state by avoiding the <strong>H</strong> state as far as possible.</p>
<p>In the preceding environment, there are 16 possible states that an agent can be in. Additionally, the agent can take four possible actions (move up, down, right, or left).</p>
<p>We'll define a q-table where there are 16 rows corresponding to the 16 states and four columns corresponding to the four actions that can be taken in each state.</p>
<p>In the previous section, we learned that:</p>
<p><em>Value of action taken in a state = reward + discount factor * <span>value of the best possible action taken in the next state</span></em></p>
<p>We'll modify the preceding formula as follows:</p>
<p><em>Value of action taken in a state = <span>value of action taken in a state + </span>1*(reward + discount factor* <span>value of the best possible action taken in the next state - </span><span>value of action taken in a state)</span></em></p>
<p>Finally, we'll replace the 1 with <span>the </span>learning rate, so that a value update of an action in a state does not change drastically. This is similar to the effect of having learning rates in neural networks.</p>
<p><em>Value of action taken in a state =<span> </span><span>value of action taken in a state + </span>learning rate*(reward + discount factor* <span>value of the best possible action taken in the next state</span> - <span>value of action taken in a state)</span></em></p>
<p>From the preceding, we can now update the q-table so that we can identify the optimal action that can be taken in different states.</p>
<p>The strategy that we'll adopt to solve this case study is as follows:</p>
<ul>
<li>Register the environment in OpenAI's Gym</li>
<li>Initialize a zero array q-table shaped as 16 x 4</li>
<li>Employ an exploration-versus-exploitation approach in choosing an action in a given state:
<ul>
<li>So far, we have merely explored possible overall actions as we randomly chose an action in a given state.</li>
<li>In this section, we will explore the initial iterations as we are not sure of the optimal action to take during the initial few episodes of <span>the </span>game.</li>
<li>However, as we learn more about the game, we exploit what we have learned in terms of possible actions to take while still taking random actions (with decreasing frequency as the number of episodes increases).</li>
</ul>
</li>
<li>In a given episode:
<ul>
<li>Choose an action depending on whether we try and explore or exploit</li>
<li>Identify the new state and reward, and check whether the game is over by taking the action chosen in the previous step</li>
<li>Initialize a learning rate parameter and discount factor</li>
<li>Update the value of taking the preceding action in a state in <span>the </span>q-table by using the formula discussed earlier</li>
<li>Repeat the preceding steps until <span>the </span>game is over</li>
</ul>
</li>
<li>Additionally, repeat the preceding steps for 1,000 different games</li>
<li>Check the q-table to identify the optimal action to take in a given state</li>
<li>Plot the path of an agent as it takes the actions in a state as per the q-table</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this section, we will code the strategy we discussed earlier (the code file is available as <kbd>Frozen_Lake_with_Q_Learning.ipynb</kbd> in GitHub):</p>
<ol>
<li>Import <span>the </span>relevant packages:</li>
</ol>
<pre style="padding-left: 60px">import gym<br/>from gym import envs<br/>from gym.envs.registration import register</pre>
<p class="mce-root"/>
<p style="padding-left: 60px"><span>Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from</span><span> </span><span>walking</span><span> </span><span>to playing games such as </span><span>Pong</span><span> and </span><span>Pinball</span><span>.</span></p>
<p style="padding-left: 60px">More about Gym can be found at: <a href="https://gym.openai.com/" target="_blank">https://gym.openai.com/</a>.</p>
<ol start="2">
<li>Register the environment:</li>
</ol>
<pre style="padding-left: 60px">register(<br/> id = 'FrozenLakeNotSlippery-v1',<br/> entry_point = 'gym.envs.toy_text:FrozenLakeEnv',<br/> kwargs = {'map_name': '4x4', 'is_slippery':False},<br/> max_episode_steps = 100,<br/> reward_threshold = 0.8196)</pre>
<ol start="3">
<li>Create the environment:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make('FrozenLakeNotSlippery-v1')</pre>
<ol start="4">
<li>Inspect the created environment:</li>
</ol>
<pre style="padding-left: 60px">env.render()</pre>
<p class="CDPAlignCenter CDPAlign"><img src="Images/eb411224-67a2-44b0-921e-610d1061e23a.png" width="45" height="70"/></p>
<p style="padding-left: 60px">The preceding step renders (prints) the environment:</p>
<pre style="padding-left: 60px">env.observation_space</pre>
<p style="padding-left: 60px">The preceding code provides the number of state action pairs in the environment. In our case, given that it is a 4 x 4 grid, we have a total of 16 states. Thus, we have a total of 16 observations.</p>
<pre style="padding-left: 60px">env.action_space.n</pre>
<p style="padding-left: 60px">The <span>preceding code</span> defines the number of actions that can be taken in a state in the environment:</p>
<pre style="padding-left: 60px">env.action_space.sample()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The <span>preceding code</span> samples an action from the possible set of actions:</p>
<pre style="padding-left: 60px">env.step(action)</pre>
<p style="padding-left: 60px">The <span>preceding code</span> takes the action and generates the new state and <span>the </span>reward of <span>the </span>action, flags whether the game is done, and provides additional information for the step:</p>
<pre style="padding-left: 60px">env.reset()</pre>
<p style="padding-left: 60px">The <span>preceding code</span> resets the environment so that the agent is back to the starting state.</p>
<ol start="5">
<li>Initialize the q-table:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>qtable = np.zeros((16,4))</pre>
<p style="padding-left: 60px">We have initialized it to a shape of (16, 4) as there are 16 states and 4 possible actions in each state.</p>
<ol start="6">
<li>Run multiple iterations of playing a game:</li>
</ol>
<p style="padding-left: 60px">Initialize hyper-parameters:</p>
<pre style="padding-left: 60px">total_episodes=15000<br/>learning_rate=0.8<br/>max_steps=99<br/>gamma=0.95<br/>epsilon=1<br/>max_epsilon=1<br/>min_epsilon=0.01<br/>decay_rate=0.005</pre>
<p style="padding-left: 60px">Play multiple episodes of the game:</p>
<pre style="padding-left: 60px">rewards=[]<br/>for episode in range(total_episodes):<br/>    state=env.reset()<br/>    step=0<br/>    done=False<br/>    total_rewards=0</pre>
<p style="padding-left: 60px"><span>In the code below, we are defining the action to be taken. If </span><kbd>eps</kbd><span> (which is a random number generated between 0 to 1) is less than 0.5, we explore; otherwise, we exploit (to consider the best action in a q-table)</span></p>
<pre style="padding-left: 60px">    for step in range(max_steps):<br/>        exp_exp_tradeoff=random.uniform(0,1)        <br/>        ## Exploitation:<br/>        if exp_exp_tradeoff&gt;epsilon:<br/>            action=np.argmax(qtable[state,:])<br/>        else:<br/>            ## Exploration<br/>            action=env.action_space.sample()</pre>
<p style="padding-left: 60px"><span>In the </span><span>code below,</span><span> we are fetching the new state and the reward, and flag whether the game is done by taking the action in </span><span>the </span><span>given step:</span></p>
<pre style="padding-left: 60px">        new_state, reward, done, _ = env.step(action)</pre>
<p style="padding-left: 60px"><span>In the </span><span>code below,</span><span> we are updating the q-table based on the action taken in a state. Additionally, we are also updating the state with the new state obtained after taking action in the current state:</span></p>
<pre style="padding-left: 60px">        qtable[state,action]=qtable[state,action]+learning_rate*(reward+gamma*np.max(qtable[new_state,:])-qtable[state,action])<br/>        total_rewards+=reward<br/>        state=new_state</pre>
<p style="padding-left: 60px"><span>In the following code, as the game is over, we proceed to a new episode of </span><span>the </span><span>game. However, we ensure that the randomness factor (</span><kbd>eps</kbd><span>), which is used in deciding whether we are going for exploration or exploitation, is updated.</span></p>
<pre style="padding-left: 60px">        if(done):<br/>             break<br/>        epsilon=min_epsilon+(max_epsilon-min_epsilon)*np.exp(decay_rate*episode)<br/>        rewards.append(total_rewards)</pre>
<ol start="7">
<li>Once we have built the q-table, we now deploy the agent to maneuver in line with the optimal actions suggested by <span>the </span>q-table:</li>
</ol>
<pre style="padding-left: 60px">env.reset()<br/><br/>for episode in range(1):<br/>    state=env.reset()<br/>    step=0<br/>    done=False<br/>    print("-----------------------")<br/>    print("Episode",episode)<br/>    for step in range(max_steps):<br/>        env.render()<br/>        action=np.argmax(qtable[state,:])<br/>        print(action)<br/>        new_state,reward,done,info=env.step(action)<br/>        <br/>        if done:<br/>            #env.render()<br/>            print("Number of Steps",step+1)<br/>            break<br/>        state=new_state</pre>
<p>The preceding gives the optimal path that the agent has to traverse to reach the end goal.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep Q-learning to balance a cart pole</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we learned about taking an action based on q-table values. However, arriving at an optimal value is time-consuming, as the agent would have to play multiple times to arrive at the optimal q-table.</p>
<p>In this section, we will learn about using a neural network so that we can arrive at the optimal values faster than what we achieved when we used Q-learning.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>For this exercise, we will register the cart-pole environment where the possible actions are to move either right or left so that we balance the pole. Additionally the cart position, cart velocity, pole angle, and pole velocity at <span>the </span>tip is the information we have about the states.</p>
<p>The rules of this game can be found here: <a href="https://gym.openai.com/envs/CartPole-v1/" target="_blank">https://gym.openai.com/envs/CartPole-v1/</a>.</p>
<p><span>A pole is attached to a cart by an un-actuated joint, and the cart moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every time-step that the pole remains upright. The episode ends when the pole is more than 15 degrees from the vertical, or the cart moves more than 2.4 units from the center.</span></p>
<p>In order to balance <span>the </span>cart-pole, we'll adopt the same strategy as we adopted in the previous section. However, the difference in deep q-learning is that we'll use a neural network to help us predict the optimal action that the agent needs to take.</p>
<p>The way in which we train the neural network is as follows:</p>
<ul>
<li>We'll store the information on state values, the action taken, and the reward achieved:
<ul>
<li>The reward will be 1 if the game does not end (is not over) and 0 otherwise.</li>
</ul>
</li>
<li>Initially, the model predicts based on randomly initialized weights, where the output layer of the model has two nodes that correspond to <span>the </span>new state's values for the two possible actions.</li>
<li>The new state value will be based on the action that maximizes the value of new state</li>
<li>If the game is not over, we update the current state's value with the sum of <span>the </span>reward and the product of <span>the </span>maximum state value of <span>the </span>new state and the discount factor.</li>
<li>We'll now override the value of <span>the </span>action from the updated current state's value that we obtained previously:
<ul>
<li>If the action taken in the current step is wrong (that is, the game is over) the value of the action in the current state will be 0.</li>
<li>Otherwise, the value of <span>the </span>target in the current step is a positive number.</li>
<li>This way, we are letting the model figure out the right action to take.</li>
<li>Additionally, we can consider this a way to specify that the action is wrong when <span>the </span>reward is zero. However, given that we are not sure whether it is the right action when <span>the </span>reward is 1, we'll just update it for the action we took and leave the new state's value (if we take the other action) untouched.</li>
</ul>
</li>
<li>We append the state values to the input array, and also the values of taking one or an other action in the current state as <span>the </span>output array.</li>
<li>We fit the model that minimizes the mean-squared error for the preceding data points.</li>
<li>Finally, we keep reducing the exploration over an increasing number of episodes.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>We'll code the strategy we discussed earlier as follows (the code file is available as <kbd>Deep_Q_learning_to_balance_a_cart_pole.ipynb</kbd> in GitHub):</p>
<ol>
<li>Create the environment and store the action size and state size in variables:</li>
</ol>
<pre style="padding-left: 60px">import gym <br/>env = gym.make('CartPole-v0') <br/>state_size = env.observation_space.shape[0] <br/>action_size = env.action_space.n</pre>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">A cart-pole environment looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1470 image-border" src="Images/2a3a72ea-30b2-4b46-b545-908bc873ef5a.png" style="width:31.83em;height:21.08em;" width="382" height="253"/></p>
<ol start="2">
<li>Import the relevant packages:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import random<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.optimizers import Adam<br/>from collections import deque</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>Define a model:</li>
</ol>
<pre style="padding-left: 60px">model=Sequential()<br/>model.add(Dense(24,input_dim=state_size,activation='relu'))<br/>model.add(Dense(24,activation='relu'))<br/>model.add(Dense(2,activation='linear'))<br/>model.compile(loss='mse',optimizer=Adam(lr=0.01))</pre>
<ol start="4">
<li>Define the lists that need to be appended:</li>
</ol>
<pre style="padding-left: 60px">memory = deque(maxlen=2000)<br/>gamma = 0.95 # discount rate<br/>epsilon = 1.0 # exploration rate<br/>epsilon_min = 0.01<br/>epsilon_decay = 0.995<br/>done = False<br/>batch_size=32</pre>
<ol start="5">
<li>Define a function that replays the game:</li>
</ol>
<pre style="padding-left: 60px">def replay(model, batch_size,epsilon):<br/>    epsilon_min = 0.01<br/>    epsilon_decay = 0.995<br/>    minibatch = random.sample(memory, batch_size)<br/>    for state, action, reward, next_state, done in minibatch:<br/>        target = reward<br/>        if not done:<br/>            target = (reward + gamma *np.amax(model.predict(next_state)[0]))<br/>        new_action_value = model.predict(state)<br/>        new_action_value[0][action] = target<br/>        model.fit(state,new_action_value, epochs=1, verbose=0)<br/>    if epsilon &gt; epsilon_min:<br/>        epsilon *= epsilon_decay<br/>    return model,epsilon</pre>
<p style="padding-left: 60px">In the preceding code, we are defining a function that takes the neural network model, batch size, and epsilon (the parameter that signifies whether we'll explore or exploit). We are fetching a random sample of the size of <kbd>batch_size</kbd>. Note that you will learn about memory structure (which comprises state, action, reward, and <kbd>next_state</kbd>) in the next step. If the game is not done, we are updating the reward for taking the action that is taken; otherwise, the target will be 0 (as the reward would be 0 when the game is over).</p>
<p class="mce-root"/>
<p style="padding-left: 60px">Additionally, the model predicts the value of taking a certain action (as the model has 2 nodes in the output—where each provides the output of taking one action over the other). The function returns the updated model and the coefficient of exploration/exploitation (epsilon).</p>
<ol start="6">
<li>Play the game over multiple episodes and append the scores obtained by <span>the </span>agent. Additionally, ensure that the actions taken by <span>the </span>agent are dictated by the model based on the epsilon value:</li>
</ol>
<pre style="padding-left: 60px">episodes=200<br/>maxsteps=200<br/>score_list = []<br/>for e in range(episodes):<br/>    state = env.reset()<br/>    state = np.reshape(state, [1, state_size])</pre>
<p style="padding-left: 60px">In the preceding code, we are playing a total of 200 episodes where we are resetting the environment at the start of <span>the </span>episode. Additionally, we are reshaping the state so that it can be passed to the neural network model:</p>
<pre style="padding-left: 60px">    for step in range(maxsteps):<br/>        if np.random.rand()&lt;=epsilon:<br/>            action=env.action_space.sample()<br/>        else:<br/>            action = np.argmax(model.predict(state)[0])</pre>
<p style="padding-left: 60px">In the preceding step, we are taking an action based on the exploration parameter (epsilon), where we take a random action (<kbd>env.actionspace.sample()</kbd>) in certain cases, and leverage the model's predictions in other cases:</p>
<pre style="padding-left: 60px">        next_state, reward, done, _ = env.step(action)<br/>        reward = reward if not done else -10<br/>        next_state = np.reshape(next_state, [1, state_size])<br/>        memory.append((state, action, reward, next_state, done))</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">In the preceding step, we are performing an action and extracting the next state, the reward, and the information about whether the game is over. Additionally, we are overwriting the reward value with -10 if the game is over (which means the agent made an incorrect move). Further, we are extracting the next state and appending it into the memory. This way, we are creating a dataset for the model to be trained on, where the model takes the current state and reward to calculate the reward for one of the two possible actions:</p>
<pre style="padding-left: 60px">        state = next_state<br/>        if done:<br/>          print("episode: {}/{}, score: {}, exp prob: {:.2}".format(e, episodes, step, epsilon))<br/>          score_list.append(step)<br/>          break<br/>        if len(memory) &gt; batch_size:<br/>          model,epsilon=replay(model, batch_size,epsilon)</pre>
<p style="padding-left: 60px">In the preceding code, if the game is done, we  append the score (the number of steps taken during <span>the </span>game); otherwise, we update the model. Additionally, we are updating the model only when memory has as many data points as the pre-defined batch size.</p>
<p>Plotting the scores over increasing epochs looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/c8b05e8e-9ac2-4b2c-94cc-ee1ca482baad.png" width="372" height="259"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep Q-learning to play Space Invaders game</h1>
                </header>
            
            <article>
                
<p>In the previous section, we used Deep Q-learning to play the Cart-Pole game. In this section, we will leverage Deep Q-learning to play Space Invaders, which is a more complex environment than Cart-Pole.</p>
<p>A sample screenshot of <span>the </span>Space Invaders game looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/61dfcc9b-3271-482d-aef3-0eaebddf9a93.png" style="width:24.75em;height:32.33em;" width="468" height="611"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">source: https://gym.openai.com/envs/SpaceInvaders-v0/</div>
<p>The objective of this exercise is to maximize the score obtained in a single game.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The strategy that we'll adopt to build an agent that is able to maximize <span>the </span>score is as follows:</p>
<ul>
<li>Initialize the environment of <span>the </span><em>Space Invaders-Atari2600</em> game.</li>
<li>Preprocess the image frame:
<ul>
<li>Remove pixels that do not necessarily impact the action prediction
<ul>
<li>For example, pixels below the location of <span>the </span>player</li>
</ul>
</li>
<li>Normalize <span>the </span>input image.</li>
<li>Resize the image before passing it to <span>the </span>neural network model</li>
</ul>
</li>
<li>Stack frames as required by <span>the </span>Gym environment</li>
<li>Let the agent play the game over multiple episodes:
<ul>
<li>During the initial episodes, we'll have high exploration which decays over increasing episodes.</li>
<li>The action that needs to be taken in a state depends on the value of <span>the </span>exploration coefficient.</li>
<li>Store the game state and the corresponding reward for the action taken in a state in memory.</li>
<li>Update the model depending on the reward that was received in the previous episodes.</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The strategy that we discussed earlier is coded as follows:</p>
<ol>
<li>Download the ROM that contains <span>the </span>Space Invaders game and also install <span>the</span> <kbd>retro</kbd> package:</li>
</ol>
<pre><strong>$ wget http://www.atarimania.com/roms/Roms.rar &amp;&amp; unrar x Roms.rar &amp;&amp; unzip Roms/ROMS.zip</strong><br/><strong>$ pip3 install gym-retro</strong><br/><strong>$ python3 -m retro.import ROMS/</strong></pre>
<ol start="2">
<li>Create the environment and extract the observation space:</li>
</ol>
<pre>env=retro.make(game='SpaceInvaders-Atari2600')<br/>env.observation_space<br/># Box(210,160,3)</pre>
<p class="mce-root"/>
<ol start="3">
<li>Build a function that preprocesses the frame (image/screenshot of the Space Invaders game):</li>
</ol>
<pre>def preprocess_frame(frame):<br/>     # Greyscale frame <br/>     gray = rgb2gray(frame)<br/>     # Crop the screen (remove the part below the player)<br/>     # [Up: Down, Left: right]<br/>     cropped_frame = gray[8:-12,4:-12]<br/>     # Normalize Pixel Values<br/>     normalized_frame = cropped_frame/255.0<br/>     # Resize<br/>     preprocessed_frame = transform.resize(normalized_frame, [110,84])<br/>     return preprocessed_frame </pre>
<ol start="4">
<li>Build a function that stacks frames given a state:</li>
</ol>
<pre>stack_size = 4 # We stack 4 frames<br/># Initialize deque with zero-images one array for each image<br/>stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)<br/>def stack_frames(stacked_frames, state, is_new_episode):<br/>     # Preprocess frame<br/>     frame = preprocess_frame(state) <br/>     if is_new_episode:<br/>         # Clear our stacked_frames<br/>         stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4) <br/>         # Because we're in a new episode, copy the same frame 4x<br/>         stacked_frames.append(frame)<br/>         stacked_frames.append(frame)<br/>         stacked_frames.append(frame)<br/>         stacked_frames.append(frame) <br/>         # Stack the frames<br/>         stacked_state = np.stack(stacked_frames, axis=2) <br/>     else:<br/>         # Append frame to deque, automatically removes the oldest frame<br/>         stacked_frames.append(frame)<br/>         # Build the stacked state (first dimension specifies different frames)<br/>         stacked_state = np.stack(stacked_frames, axis=2) <br/>     return stacked_state, stacked_frames</pre>
<p class="mce-root"/>
<ol start="5">
<li>Initialize the model hyperparameters:</li>
</ol>
<pre style="padding-left: 60px">### MODEL HYPERPARAMETERS<br/>state_size = [110, 84, 4] # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels) <br/>action_size = env.action_space.n # 8 possible actions<br/>learning_rate = 0.00025 # Alpha (aka learning rate)</pre>
<pre style="padding-left: 60px" class="mce-root">### TRAINING HYPERPARAMETERS<br/>total_episodes = 50 # Total episodes for training<br/>max_steps = 50000 # Max possible steps in an episode<br/>batch_size = 32 # Batch size</pre>
<pre style="padding-left: 60px" class="mce-root"># Exploration parameters for epsilon greedy strategy<br/>explore_start = 1.0 # exploration probability at start<br/>explore_stop = 0.01 # minimum exploration probability <br/>decay_rate = 0.00001 # exponential decay rate for exploration prob</pre>
<pre style="padding-left: 60px" class="mce-root"># Q learning hyperparameters<br/>gamma = 0.9 # Discounting rate</pre>
<pre style="padding-left: 60px" class="mce-root">### MEMORY HYPERPARAMETERS<br/>pretrain_length = batch_size # Number of experiences stored in the Memory when initialized for the first time<br/>memory_size = 1000000 # Number of experiences the Memory can keep</pre>
<pre style="padding-left: 60px" class="mce-root">### PREPROCESSING HYPERPARAMETERS<br/>stack_size = 4 # Number of frames stacked</pre>
<pre style="padding-left: 60px" class="mce-root">### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT<br/>training = False</pre>
<pre style="padding-left: 60px" class="mce-root">## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT<br/>episode_render = False</pre>
<ol start="6">
<li>Build a function that samples data from the total memory:</li>
</ol>
<pre style="padding-left: 60px">memory = deque(maxlen=100000)</pre>
<pre style="padding-left: 60px">def sample(memory, batch_size):<br/>     buffer_size = len(memory)<br/>     index = np.random.choice(np.arange(buffer_size),<br/>     size = batch_size,<br/>     replace = False) <br/>     return [memory[i] for i in index]</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="7">
<li>Build a function that returns the action that the agent needs to take:</li>
</ol>
<pre style="padding-left: 60px">def predict_action(model,explore_start, explore_stop, decay_rate, decay_step, state, actions):<br/>     exp_exp_tradeoff = np.random.rand()<br/>     explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)<br/>     if (explore_probability &gt; exp_exp_tradeoff):<br/>         choice = random.randint(1,len(possible_actions))-1<br/>         action = possible_actions[choice]<br/>     else:<br/>         Qs = model.predict(state.reshape((1, *state.shape)))<br/>         choice = np.argmax(Qs)<br/>         action = possible_actions[choice]<br/>     return action, explore_probability</pre>
<ol start="8">
<li>Build a function that fine-tunes the model:</li>
</ol>
<pre style="padding-left: 60px">def replay(agent,batch_size,memory):<br/>     minibatch = sample(memory,batch_size)<br/>     for state, action, reward, next_state, done in minibatch:<br/>     target = reward<br/>     if not done:<br/>         target = reward + gamma*np.max(agent.predict(next_state.reshape((1,*next_state.shape)))[0])<br/>     target_f = agent.predict(state.reshape((1,*state.shape)))<br/>     target_f[0][action] = target<br/>     agent.fit(state.reshape((1,*state.shape)), target_f, epochs=1, verbose=0)<br/> return agent</pre>
<ol start="9">
<li>Define the neural network model:</li>
</ol>
<pre style="padding-left: 60px">def DQNetwork():<br/>     model=Sequential()<br/>     model.add(Convolution2D(32,input_shape=(110,84,4),kernel_size=8, strides=4, padding='valid',activation='elu'))<br/>     model.add(Convolution2D(64, kernel_size=4, strides=2, padding='valid',activation='elu'))<br/>     model.add(Convolution2D(128, kernel_size=3, strides=2, padding='valid',activation='elu'))<br/>     model.add(Flatten())<br/>     model.add(Dense(units=512))<br/>     model.add(Dense(units=3,activation='softmax'))<br/>     model.compile(optimizer=Adam(0.01),loss='mse')<br/>     return model</pre>
<p class="mce-root">A summary of model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/4a623396-afa7-491b-96da-6853a83906e6.jpg" style="width:37.25em;height:21.58em;" width="511" height="296"/></p>
<ol start="10">
<li>Loop through multiple episodes and keep playing the game while updating the model:</li>
</ol>
<pre style="padding-left: 60px">agent = DQNetwork()<br/>agent.summary()<br/>rewards_list=[]<br/>Episodes=200<br/># Iterate the game<br/>for episode in range(Episodes):<br/>     # reset state in the beginning of each game<br/>     step = 0<br/>     decay_step = 0<br/>     episode_rewards = []<br/>     state = env.reset()<br/>     state, stacked_frames = stack_frames(stacked_frames, state, True)<br/>     while step &lt; max_steps:<br/>         step += 1<br/>         decay_step +=1<br/>         # Predict the action to take and take it<br/>         action, explore_probability = predict_action(agent,explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)<br/> #Perform the action and get the next_state, reward, and done information<br/>         next_state, reward, done, _ = env.step(action)<br/>         # Add the reward to total reward<br/>         episode_rewards.append(reward)<br/>     if done:<br/> # The episode ends so no next state<br/>         next_state = np.zeros((110,84), dtype=np.int)<br/>         next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)<br/> # Set step = max_steps to end the episode<br/>         step = max_steps<br/> # Get the total reward of the episode<br/>         total_reward = np.sum(episode_rewards)<br/>         print('Episode:{}/{} Score:{} Explore Prob:{}'.format(episode,Episodes,total_reward,explore_probability))<br/>         rewards_list.append((episode, total_reward))<br/> # Store transition &lt;st,at,rt+1,st+1&gt; in memory D<br/>         memory.append((state, action, reward, next_state, done))<br/>     else:<br/> # Stack the frame of the next_state<br/>         next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)<br/> # Add experience to memory<br/>         memory.append((state, action, reward, next_state, done))<br/> # st+1 is now our current state<br/>         state = next_state<br/>     env.render() <br/> # train the agent with the experience of the episode<br/> agent=replay(agent,batch_size,memory)</pre>
<ol start="11">
<li>Plot <span>the </span>rewards obtained over increasing episodes:</li>
</ol>
<pre style="padding-left: 60px">score=[]<br/>episode=[]<br/>for e,r in rewards_list:<br/>     episode.append(e)<br/>     score.append(r)<br/>import matplotlib.pyplot as plt<br/>plt.plot(episode,score)</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1469 image-border" src="Images/3f3345e3-c0b6-49bc-9009-b2f000a06908.png" style="width:32.67em;height:22.50em;" width="384" height="264"/></p>
<p>From this, we can see that the model has learned to score over 800 in some episodes.</p>


            </article>

            
        </section>
    </div>



  </body></html>