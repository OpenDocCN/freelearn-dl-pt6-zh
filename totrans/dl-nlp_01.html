<html><head></head><body>
		<div class="Content" id="_idContainer009">
			<h1 id="_idParaDest-17"><em class="italics"><a id="_idTextAnchor017"/>Chapter 1</em></h1>
		</div>
		<div class="Content" id="_idContainer010">
			<h1 id="_idParaDest-18"><a id="_idTextAnchor018"/>Introduction to Natural Language Processing</h1>
		</div>
		<div class="Content" id="_idContainer011">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Describe natural language processing and its applications</li>
				<li class="bullets">Explain different text preprocessing techniques</li>
				<li class="bullets">Perform text preprocessing on text corpora</li>
				<li class="bullets">Explain the functioning of Word2Vec and GloVe word embeddings </li>
				<li class="bullets">Generate word embeddings using Word2Vec and GloVe</li>
				<li class="bullets">Use the NLTK, Gensim, and Glove-Python libraries for text preprocessing and generating word embeddings</li>
			</ul>
			<p>This chapter aims to equip you with knowledge of the basics of natural language processing and experience with the various text preprocessing techniques used in Deep Learning.</p>
		</div>
		<div class="Content" id="_idContainer040">
			<h2 id="_idParaDest-19"><a id="_idTextAnchor019"/>Introduction</h2>
			<p>Welcome to deep learning for Natural Language Processing. This book guides you in understanding and optimizing deep learning techniques for the purpose of natural language processing, which furthers the reality of generalized artificial intelligence. You will journey through the concepts of natural language processing – its applications and implementations – and learn the ways of deep neural networks, along with utilizing them to enable machines to understand natural language.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor020"/>The Basics of Natural Language Processing</h2>
			<p>To understand what natural language processing is, let's break the term into two:</p>
			<ul>
				<li>Natural language is a form of written and spoken communication that has developed organically and naturally.</li>
				<li>Processing means analyzing and making sense of input data with computers.</li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer012">
					<img alt="Figure 1.1: Natural language processing&#13;&#10;" src="image/C13783_01_01.jpg"/>
				</div>
			</div>
			<h6>Figure 1.1: Natural language processing</h6>
			<p>Therefore, natural language processing is the machine-based processing of human communication. It aims to teach machines how to process and understand the language of humans, thereby allowing an easy channel of communication between human and machines. </p>
			<p>For example, the personal voice assistants found in our phones and smart speakers, such as Alexa and Siri, are a result of natural language processing. They have been created in such a manner that they are able to not only understand what we say to them but also to act upon what we say and respond with feedback. Natural language processing algorithms aid these technologies in communicating with humans.</p>
			<p>The key thing to consider in the mentioned definition of natural language processing is that the communication needs to occur in the natural language of humans. We've been communicating with machines for decades now by creating programs to perform certain tasks and executing them. However, these programs are written in languages that are not natural languages, because they are not forms of spoken communication and they haven't developed naturally or organically. These languages, such as Java, Python, C, and C++, were created with machines in mind and the consideration always being, "what will the machine be able to understand and process easily?" </p>
			<p>While Python is a more user-friendly language and so is easier for humans to learn and be able to write code in, the basic point remains the same – to communicate with a machine, humans must learn a language that the machine is able to understand.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer013">
					<img alt="Figure 1.2: Venn diagram for natural language processing&#13;&#10;" src="image/C13783_01_02.jpg"/>
				</div>
			</div>
			<h6>Figure 1.2: Venn diagram for natural language processing</h6>
			<p>The purpose of natural language processing is the opposite of this. Rather than having humans conform to the ways of a machine and learn how to effectively communicate with them, natural language processing enables machines to conform to humans and learn their way of communication. This makes more sense since the aim of technology is to make our lives easier.</p>
			<p>To clarify this with an example, your first ever program was probably a piece of code that asked the machine to print 'hello world'. This was you conforming to the machine and asking it to execute a task in a language that it understood. Asking your voice assistant to say 'hello world' by voicing this command to it, and having it say 'hello world' back to you, is an example of the application of natural language processing, because you are communicating with a machine in your natural language (in this case, English). The machine is conforming to your form of communication, understanding what you're saying, processing what you're asking it to do, and then executing the task.</p>
			<h3 id="_idParaDest-21"><a id="_idTextAnchor021"/>Importance of natural language processing</h3>
			<p>The following figure illustrates the various sections of the field of artificial intelligence:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer014">
					<img alt="Fig 1.3: Artificial intelligence and some of its subfields&#13;&#10;" src="image/C13783_01_03.jpg"/>
				</div>
			</div>
			<h6>Fig 1.3: Artificial intelligence and some of its subfields</h6>
			<p>Along with machine learning and deep learning, natural language processing is a subfield of artificial intelligence, and because it deals with natural language, it's actually at the intersection of artificial intelligence and linguistics.</p>
			<p>As mentioned, natural language processing is what enables machines to understand the language of humans, thus allowing an efficient channel of communication between the two. However, there is another reason Natural language processing is necessary, and that is because, like machines, machine learning and deep learning models work best with numerical data. Numerical data is hard for humans to naturally produce; imagine us talking in numbers rather than words. So, natural language processing works with textual data and converts it into numerical data, enabling machine learning and deep learning models to be fitted on it. Thus, it exists to bridge the communication gap between humans and machines by taking the spoken and written forms of language from humans and converting them into data that can be understood by machines. Thanks to natural language processing, the machine is able to make sense of, answer questions based on, solve problems using, and communicate in a natural language, among other things.</p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor022"/>Capabilities of Natural language processing</h2>
			<p>Natural language processing has many real-world applications that benefit the lives of humans. These applications fall under three broad capabilities of natural language processing:</p>
			<ul>
				<li>Speech Recognition<p>The machine is able to recognize a natural language in its spoken form and translate it into a textual form. An example of this is dictation on your smartphones – you can enable dictation and speak to your phone, and it will convert whatever you are saying into text.</p></li>
				<li>Natural Language Understanding<p>The machine is able to understand a natural language in both its spoken and written form. If given a command, the machine is able to understand and execute it. An example of this would be saying 'Hey Siri, call home' to Siri on your iPhone for Siri to automatically call 'home' for you.</p></li>
				<li>Natural Language Generation<p>The machine is able to generate natural language itself. An example of this is asking 'Siri, what time is it?' to Siri on your iPhone and Siri replying with the time – 'It's 2:08pm'.</p></li>
			</ul>
			<p>These three capabilities are used to accomplish and automate a lot of tasks. Let's take a look at some of the things natural language processing contributes to, and how.</p>
			<h4>Note</h4>
			<p class="callout">Textual data is known as corpora (plural) and a corpus (singular).</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor023"/>Applications of Natural Language Processing</h2>
			<p>The following figure depicts the general application areas of natural language processing:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer015">
					<img alt="Figure 1.4: Application areas of natural language processing&#13;&#10;" src="image/C13783_01_04.jpg"/>
				</div>
			</div>
			<h6>Figure 1.4: Application areas of natural language processing</h6>
			<ul>
				<li>Automatic text summarization<p>This involves processing corpora to provide a summary.</p></li>
				<li>Translation<p>This entails translation tools that translate text to and from different languages, for example, Google Translate.</p></li>
				<li>Sentiment analysis<p>This is also known as emotional artificial intelligence or opinion mining, and it is the process of identifying, extracting, and quantifying emotions and affective states from corpora, both written and spoken. Sentiment analysis tools are used to process things such as customer reviews and social media posts to understand emotional responses to and opinions regarding particular things, such as the quality of food at a new restaurant.</p></li>
				<li>Information extraction<p>This is the process of identifying and extracting important terms from corpora, known as entities. Named entity recognition falls under this category and is a process that will be explained in the next chapter.</p></li>
				<li>Relationship extraction<p>Relationship extraction involves extracting semantic relationships from corpora. Semantic relationships occur between two or more entities (such as people, organizations, and things) and fall into one of the many semantic categories. For example, if a relationship extraction tool was given a paragraph about Sundar Pichai and how he is the CEO of Google, the tool would be able to produce "Sundar Pichai works for Google" as output, with Sundar Pichai and Google being the two entities, and 'works for' being the semantic category that defines their relationship.</p></li>
				<li>Chatbot<p>Chatbots are forms of artificial intelligence that are designed to converse with humans via speech and text. The majority of them mimic humans and make it feel as though you are speaking to another human being. Chatbots are being used in the health industry to help people who suffer from depression and anxiety.</p></li>
				<li>Social media analysis<p>Social media applications such as Twitter and Facebook have hashtags and trends that are tracked and monitored using natural language processing to understand what is being talked about around the world. Additionally, natural language processing aids the process of moderation by filtering out negative, offensive, and inappropriate comments and posts.</p></li>
				<li>Personal voice assistants<p>Siri, Alexa, Google Assistant, and Cortana are all personal voice assistants that leverage natural language processing techniques to understand and respond to what we say.</p></li>
				<li>Grammar checking<p>Grammar-checking software automatically checks and corrects your grammar, punctuation, and typing errors.</p></li>
			</ul>
			<h3 id="_idParaDest-24"><a id="_idTextAnchor024"/>Text Preprocessing</h3>
			<p>When answering questions on a comprehension passage, the questions are specific to different parts of the passage, and so while some words and sentences are important to you, others are irrelevant. The trick is to identify key words from the questions and match them to the passage to find the correct answer.</p>
			<p>Text preprocessing works in a similar fashion – the machine doesn't need the irrelevant parts of the corpora; it just needs the important words and phrases required to execute the task at hand. Thus, text preprocessing techniques involve prepping the corpora for proper analysis and for the machine learning and deep learning models. Text preprocessing is basically telling the machine what it needs to take into consideration and what it can disregard.</p>
			<p>Each corpus requires different text preprocessing techniques depending on the task that needs to be executed, and once you've learned the different preprocessing techniques, you'll understand where to use what and why. The order in which the techniques have been explained is usually the order in which they are performed.</p>
			<p>We will be using the <strong class="keyword">NLTK</strong> Python library in the following exercises, but feel free to use different libraries while doing the activities. <strong class="keyword">NLTK</strong> stands for <strong class="keyword">Natural Language Toolkit</strong> and is the simplest and one of the most popular Python libraries for natural language processing, which is why we will be using it to understand the basic concepts of natural language processing.</p>
			<h4>Note</h4>
			<p class="callout">For further information on NLTK, go to <a href="">https://www.nltk.org/</a>.</p>
			<h3 id="_idParaDest-25"><a id="_idTextAnchor025"/>Text Preprocessing Techniques</h3>
			<p>The following are the most popular text preprocessing techniques in natural language processing:</p>
			<ul>
				<li>Lowercasing/uppercasing</li>
				<li>Noise removal</li>
				<li>Text normalization</li>
				<li>Stemming</li>
				<li>Lemmatization</li>
				<li>Tokenization</li>
				<li>Removing stop words</li>
			</ul>
			<p>Let's look at each technique one by one.</p>
			<h3 id="_idParaDest-26"><a id="_idTextAnchor026"/>Lowercasing/Uppercasing</h3>
			<p>This is one of the most simple and effective preprocessing techniques that people often forget to use. It either converts all the existing uppercase characters into lowercase ones so that the entire corpus is in lowercase, or it converts all the lowercase characters present in the corpus into uppercase ones so that the entire corpus is in uppercase.</p>
			<p>This method is especially useful when the size of the corpus isn't too large and the task involves identifying terms or outputs that could be recognized differently due to the case of the characters, since a machine inherently processes uppercase and lowercase letters as separate entities – 'A' is different from 'a.' This kind of variation in the input capitalization could result in incorrect output or no output at all.</p>
			<p>An example of this would be a corpus that contains both 'India' and 'india.' Without applying lowercasing, the machine would recognize these as two separate terms, when in reality they're both different forms of the same word and correspond to the same country. After lowercasing, there would exist only one instance of the term "India," which would be 'india,' simplifying the task of finding all the places where India has been mentioned in the corpus.</p>
			<h4>Note</h4>
			<p class="callout">All exercises and activities will be primarily developed on Jupyter Notebook. You will need to have Python 3.6 and NLTK installed on your system.</p>
			<p class="callout">Exercises 1 – 6 can be done within the same Jupyter notebook.</p>
			<h3 id="_idParaDest-27"><a id="_idTextAnchor027"/>Exercise 1: Performing Lowercasing on a Sentence</h3>
			<p>In this exercise, we will take an input sentence with both uppercase and lowercase characters and convert them all into lowercase characters. The following steps will help you with the solution:</p>
			<ol>
				<li>Open <strong class="bold">cmd</strong> or another terminal depending on your operating system.</li>
				<li>Navigate to the desired path and use the following command to initiate a <strong class="inline">Jupyter</strong> notebook:<p class="snippet">jupyter notebook</p></li>
				<li>Store an input sentence in an '<strong class="bold">s</strong>' variable, as shown:<p class="snippet"><strong class="inline">s = "The cities I like most in India are Mumbai, Bangalore, Dharamsala and Allahabad."</strong></p></li>
				<li>Apply the <strong class="inline">lower()</strong> function to convert the capital letters into lowercase characters and then print the new string, as shown:<p class="snippet">s = s.lower()</p><p class="snippet">print(s)</p><p><strong class="bold">Expected output:</strong></p><div class="IMG---Figure" id="_idContainer016"><img alt="" src="image/C13783_01_05.jpg"/></div><h6>Figure 1.5: Output for lowercasing with mixed casing in a sentence</h6></li>
				<li>Create an array of words with capitalized characters, as shown:<p class="snippet">words = ['indiA', 'India', 'india', 'iNDia']</p></li>
				<li>Using list comprehension, apply the <strong class="inline">lower()</strong> function on each element of the <strong class="inline">words</strong> array and then print the new array, as follows:<p class="snippet">words = [word.lower() for word in words]</p><p class="snippet">print(words)</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer017">
					<img alt="Figure 1.6: Output for lowercasing with mixed casing of words&#13;&#10;" src="image/C13783_01_06.jpg"/>
				</div>
			</div>
			<h6>Figure 1.6: Output for lowercasing with mixed casing of words</h6>
			<h3 id="_idParaDest-28"><a id="_idTextAnchor028"/>Noise Removal</h3>
			<p>Noise is a very general term and can mean different things with respect to different corpora and different tasks. What is considered noise for one task may be what is considered important for another, and thus this is a very domain-specific preprocessing technique. For example, when analyzing tweets, hashtags might be important to recognize trends and understand what's being spoken about around the globe, but hashtags may not be important when analyzing a news article, and so hashtags would be considered noise in the latter's case.</p>
			<p>Noise doesn't include only words, but can also include symbols, punctuation marks, HTML markup (<strong class="bold">&lt;</strong>,<strong class="bold">&gt;</strong>, <strong class="bold">*</strong>, <strong class="bold">?</strong>,<strong class="bold">.</strong>), numbers, whitespaces, stop words, particular terms, particular regular expressions, non-ASCII characters (<strong class="bold">\W</strong>|<strong class="bold">\d+</strong>), and parse terms.</p>
			<p>Removing noise is crucial so that only the important parts of the corpora are fed into the models, ensuring accurate results. It also helps by bringing words into their root or standard form. Consider the following example:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer018">
					<img alt="Figure 1.7: Output for noise removal&#13;&#10;" src="image/C13783_01_07.jpg"/>
				</div>
			</div>
			<h6>Figure 1.7: Output for noise removal</h6>
			<p>After removing all the symbols and punctuation marks, all the instances of sleepy correspond to the one form of the word, enabling more efficient prediction and analysis of the corpus.</p>
			<h3 id="_idParaDest-29"><a id="_idTextAnchor029"/>Exercise 2: Removing Noise from Words</h3>
			<p>In this exercise, we will take an input array containing words with noise attached (such as punctuation marks and HTML markup) and convert these words into their clean, noise-free forms. To do this, we will need to make use of Python's regular expression library. This library has several functions that allow us to filter through input data and remove the unnecessary parts, which is exactly what the process of noise removal aims to do.</p>
			<h4>Note</h4>
			<p class="callout">To learn more about '<strong class="bold">re</strong>,' click on <a href="">https://docs.python.org/3/library/re.html</a>.</p>
			<ol>
				<li value="1">In the same <strong class="inline">Jupyter</strong> notebook, import the regular expression library, as shown:<p class="snippet">import<a id="_idTextAnchor030"/> re</p></li>
				<li>Create a function called '<strong class="inline">clean_words</strong>', which will contain methods to remove different types of noise from the words, as follows:<p class="snippet">def clean_words(text):</p><p class="snippet">  </p><p class="snippet">  #remove html markup</p><p class="snippet">  text = re.sub("(&lt;.*?&gt;)","",text)</p><p class="snippet">  #remove non-ascii and digits</p><p class="snippet">  text=re.sub("(\W|\d+)"," ",text)</p><p class="snippet">  #remove whitespace</p><p class="snippet">  text=text.strip()</p><p class="snippet">  return text </p></li>
				<li>Create an array of raw words with noise, as demonstrated:<p class="snippet">raw = ['..sleepy', 'sleepy!!', '#sleepy', '&gt;&gt;&gt;&gt;&gt;sleepy&gt;&gt;&gt;&gt;', '&lt;a&gt;sleepy&lt;/a&gt;']</p></li>
				<li>Apply the <strong class="inline">clean_words()</strong> function on the words in the raw array and then print the array of clean words, as shown:<p class="snippet">clean = [clean_words(r) for r in raw]</p><p class="snippet">print(clean)</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer019">
					<img alt="" src="image/C13783_01_08.jpg"/>
				</div>
			</div>
			<h6>Figure 1.8: Output for noise removal</h6>
			<h3 id="_idParaDest-30"><a id="_idTextAnchor031"/>Text Normalization</h3>
			<p>This is the process of converting a raw corpus into a canonical and standard form, which is basically to ensure that the textual input is guaranteed to be consistent before it is analyzed, processed, and operated upon.</p>
			<p>Examples of text normalization would be mapping an abbreviation to its full form, converting several spellings of the same word to one spelling of the word, and so on.</p>
			<p>The following are examples for canonical forms of incorrect spellings and abbreviations:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer020">
					<img alt="Figure 1.9: Canonical form for incorrect spellings&#13;&#10;" src="image/C13783_01_09.jpg"/>
				</div>
			</div>
			<h6>Figure 1.9: Canonical form for incorrect spellings</h6>
			<div>
				<div class="IMG---Figure" id="_idContainer021">
					<img alt="Figure 1.10: Canonical form for abbreviations&#13;&#10;" src="image/C13783_01_10.jpg"/>
				</div>
			</div>
			<h6>Figure 1.10: Canonical form for abbreviations</h6>
			<p>There is no standard way to go about normalization since it is very dependent on the corpus and the task at hand. The most common way to go about it is with dictionary mapping, which involves manually creating a dictionary that maps all the various forms of one word to that one word, and then replaces each of those words with one standard form of the word.</p>
			<h3 id="_idParaDest-31"><a id="_idTextAnchor032"/>Stemming</h3>
			<p>Stemming is performed on a corpus to reduce words to their stem or root form. The reason for saying "stem or root form" is that the process of stemming doesn't always reduce the word to its root but sometimes just to its canonical form.</p>
			<p>The words that undergo stemming are known as inflected words. These words are in a form that is different from the root form of the word, to imply an attribute such as the number or gender. For example, "journalists" is the plural form of "journalist." Thus, stemming would cut off the '<strong class="bold">s</strong>', bringing "journalists" to its root form:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer022">
					<img alt="Figure 1.11: Output for stemming&#13;&#10;" src="image/C13783_01_11.jpg"/>
				</div>
			</div>
			<h6>Figure 1.11: Output for stemming</h6>
			<p>Stemming is beneficial when building search applications due to the fact that when searching for something in particular, you might also want to find instances of that thing even if they're spelled differently. For example, if you're searching for exercises in this book, you might also want 'Exercise' to show up in your search.</p>
			<p>However, stemming doesn't always provide the desired stem, since it works by chopping off the ends of the words. It's possible for the stemmer to reduce 'troubling' to 'troubl' instead of 'trouble' and this won't really help in problem solving, and so stemming isn't a method that's used too often. When it is used, Porter's stemming algorithm is the most common algorithm for stemming.</p>
			<h3 id="_idParaDest-32"><a id="_idTextAnchor033"/>Exercise 3: Performing Stemming on Words</h3>
			<p>In this exercise, we will take an input array containing various forms of one word and convert these words into their stem forms.</p>
			<ol>
				<li value="1">In the same <strong class="inline">Jupyter</strong> notebook, import the <strong class="inline">nltk</strong> and <strong class="inline">pandas</strong> libraries as well as <strong class="inline">Porter Stemmer</strong>, as shown:<p class="snippet">import nltk</p><p class="snippet">import pandas as pd</p><p class="snippet">from nltk.stem import PorterStemmer as ps</p></li>
				<li>Create an instance of <strong class="inline">stemmer</strong>, as follows:<p class="snippet">stemmer = ps()</p></li>
				<li>Create an array of different forms of the same word, as shown:<p class="snippet">words=['annoying', 'annoys', 'annoyed', 'annoy']</p></li>
				<li>Apply the stemmer to each of the words in the <strong class="inline">words</strong> array and store them in a new array, as given:<p class="snippet">stems =[stemmer.stem(word = word) for word in words]</p></li>
				<li>Print the raw words and their stems in the form of a DataFrame, as shown:<p class="snippet">sdf = pd.DataFrame({'raw word': words,'stem': stems})</p><p class="snippet">sdf</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer023">
					<img alt="Figure 1.12: Output of stemming&#13;&#10;" src="image/C13783_01_12.jpg"/>
				</div>
			</div>
			<h6>Figure 1.12: Output of stemming</h6>
			<h3 id="_idParaDest-33"><a id="_idTextAnchor034"/>Lemmatization</h3>
			<p>Lemmatization is a process that is like stemming – its purpose is to reduce a word to its root form. What makes it different is that it doesn't just chop the ends of words off to obtain this root form, but instead follows a process, abides by rules, and often uses WordNet for mappings to return words to their root forms. (WordNet is an English language database that consists of words and their definitions along with synonyms and antonyms. It is considered to be an amalgamation of a dictionary and a thesaurus.) For example, lemmatization is capable of transforming the word 'better' into its root form 'good', since 'better' is just the comparative form of 'good."</p>
			<p>While this quality of lemmatization makes it highly appealing and more efficient when compared with stemming, the drawback is that since lemmatization follows such an organized procedure, it takes a lot more time than stemming does. Hence, lemmatization is not recommended when you're working with a large corpus.</p>
			<h3 id="_idParaDest-34"><a id="_idTextAnchor035"/>Exercise 4: Performing Lemmatization on Words</h3>
			<p>In this exercise, we will take an input array containing various forms of one word and convert these words into their root form.</p>
			<ol>
				<li value="1">In the same Jupyter notebook as the previous exercise, import <strong class="inline">WordNetLemmatizer</strong> and download <strong class="inline">WordNet</strong>, as shown:<p class="snippet">from nltk.stem import WordNetLemmatizer as wnl</p><p class="snippet">nltk.download('wordnet')</p></li>
				<li>Create an instance of <strong class="inline">lemmatizer</strong>, as follows:<p class="snippet">lemmatizer = wnl()</p></li>
				<li>Create an array of different forms of the same word, as demonstrated:<p class="snippet">words = ['troubling', 'troubled', 'troubles', 'trouble']</p></li>
				<li>Apply <strong class="inline">lemmatizer</strong> to each of the words in the <strong class="inline">words</strong> array and store them in a new array, as follows. The <strong class="inline">word</strong> parameter provides the lemmatize function with the word it is supposed to lemmatize. The <strong class="inline">pos</strong> parameter is the part of speech you want the lemma to be. '<strong class="inline">v</strong>' stands for verb and thus the lemmatizer will reduce the word to its closest verb form:<p class="snippet"># v denotes verb in "pos"</p><p class="snippet">lemmatized = [lemmatizer.lemmatize(word = word, pos = 'v') for word in words]</p></li>
				<li>Print the raw words and their root forms in the form of a DataFrame, as shown:<p class="snippet">ldf = pd.DataFrame({'raw word': words,'lemmatized': lemmatized})</p><p class="snippet">ldf = ldf[['raw word','lemmatized']]</p><p class="snippet">ldf</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer024">
					<img alt="Figure 1.13: Output of lemmatization&#13;&#10;" src="image/C13783_01_13.jpg"/>
				</div>
			</div>
			<h6>Figure 1.13: Output of lemmatization</h6>
			<h3 id="_idParaDest-35"><a id="_idTextAnchor036"/>Tokenization</h3>
			<p>Tokenization is the process of breaking down a corpus into individual tokens. Tokens are the most commonly used words – thus, this process breaks down a corpus into individual words – but can also include punctuation marks and spaces, among other things.</p>
			<p>This technique is one of the most important ones since it is a prerequisite for a lot of applications of natural language processing that we will be learning about in the next chapter, such as <strong class="keyword">Parts-of-Speech</strong> (<strong class="keyword">PoS</strong>) tagging. These algorithms take tokens as input and can't function with strings or paragraphs of text as input.</p>
			<p>Tokenization can be performed to obtain individual words as well as individual sentences as tokens. Let's try both of these out in the following exercises.</p>
			<h3 id="_idParaDest-36"><a id="_idTextAnchor037"/>Exercise 5: Tokenizing Words</h3>
			<p>In this exercise, we will take an input sentence and produce individual words as tokens from it. </p>
			<ol>
				<li value="1">In the same <strong class="inline">Jupyter</strong> notebook, import <strong class="inline">nltk</strong>:<p class="snippet">import nltk</p></li>
				<li>From <strong class="inline">nltk</strong>, import <strong class="inline">word_tokenize</strong> and <strong class="inline">punkt</strong>, as shown:<p class="snippet">nltk.download('punkt')</p><p class="snippet">from nltk import word_tokenize</p></li>
				<li>Store words in a variable and apply <strong class="inline">word_tokenize()</strong> on it, then print the results, as follows:<p class="snippet">s = "hi! my name is john."</p><p class="snippet">tokens = word_tokenize(s)</p><p class="snippet">tokens</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<h6> </h6>
			<div>
				<div class="IMG---Figure" id="_idContainer025">
					<img alt="Figure 1.14: Output for the tokenization of words&#13;&#10;" src="image/C13783_01_14.jpg"/>
				</div>
			</div>
			<h6>Figure 1.14: Output for the tokenization of words</h6>
			<p>As you can see, even the punctuation marks are tokenized and considered as individual tokens. </p>
			<p>Now let's see how we can tokenize sentences.</p>
			<h3 id="_idParaDest-37"><a id="_idTextAnchor038"/>Exercise 6: Tokenizing Sentences</h3>
			<p>In this exercise, we will take an input sentence and produce individual words as tokens from it.</p>
			<ol>
				<li value="1">In the same <strong class="inline">Jupyter</strong> notebook, import <strong class="inline">sent_tokenize</strong>, as shown:<p class="snippet">from nltk import sent_tokenize</p></li>
				<li>Store two sentences in a variable (our sentence from the previous exercise was actually two sentences, so we can use the same one to see the difference between word and sentence tokenization) and apply <strong class="inline">sent_tokenize()</strong> on it, then print the results, as follows:<p class="snippet">s = "hi! my name is shubhangi."</p><p class="snippet">tokens = sent_tokenize(s)</p><p class="snippet">tokens</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer026">
					<img alt="Figure 1.15: Output for tokenizing sentences&#13;&#10;" src="image/C13783_01_15.jpg"/>
				</div>
			</div>
			<h6>Figure 1.15: Output for tokenizing sentences</h6>
			<p>As you can see, the two sentences have formed two individual tokens.</p>
			<h3 id="_idParaDest-38"><a id="_idTextAnchor039"/>Additional Techniques</h3>
			<p>There are several ways to perform text preprocessing, including the usage of a variety of Python libraries such as <strong class="bold">BeautifulSoup</strong> to strip away HTML markup. The previous exercises serve the purpose of introducing some techniques to you. Depending on the task at hand, you may need to use just one or two or all of them, including the modifications made to them. For example, at the noise removal stage, you may find it necessary to remove words such as 'the,' 'and,' 'this,' and 'it.' So, you will need to create an array containing these words and pass the corpus through a <strong class="bold">for</strong> loop to store only the words that are not a part of that array, removing the noisy words from the corpus. Another way of doing this is given later in this chapter and is done after tokenization has been performed.</p>
			<h3 id="_idParaDest-39"><a id="_idTextAnchor040"/>Exercise 7: Removing Stop Words</h3>
			<p>In this exercise, we will take an input sentence and remove the stop words from it.</p>
			<ol>
				<li value="1">Open a <strong class="inline">Jupyter</strong> notebook and download '<strong class="inline">stopwords</strong>' using the following line of code:<p class="snippet">nltk.download('stopwords')</p></li>
				<li>Store a sentence in a variable, as shown:<p class="snippet">s = "the weather is really hot and i want to go for a swim"</p></li>
				<li>Import <strong class="inline">stopwords</strong> and create a set of the English stop words, as follows:<p class="snippet">from nltk.corpus import stopwords</p><p class="snippet">stop_words = set(stopwords.words('english'))</p></li>
				<li>Tokenize the sentence using <strong class="inline">word_tokenize</strong>, and then store those tokens that do not occur in <strong class="inline">stop_words</strong> in an array. Then, print that array:<p class="snippet">tokens = word_tokenize(s)</p><p class="snippet">tokens = [word for word in tokens if not word in stop_words]</p><p class="snippet">print(tokens)</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer027">
					<img alt="Figure 1.16: Output after removing stopwords&#13;&#10;" src="image/C13783_01_16.jpg"/>
				</div>
			</div>
			<h6>Figure 1.16: Output after removing stopwords</h6>
			<p>Additionally, you may need to convert numbers into their word forms. This is also a method you can add to the noise removal function. Furthermore, you might need to make use of the contractions library, which serves the purpose of expanding the existing contractions in the text. For example, the contractions library will convert 'you're' into 'you are,' and if this is necessary for your task, then it is recommended to install this library and use it.</p>
			<p>Text preprocessing techniques go beyond the ones that have been discussed in this chapter and can include anything and everything that is required for a task or a corpus. In some instances, some words may be important, while in others they won't be.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor041"/>Word Embeddings</h2>
			<p>As mentioned in the earlier sections of this chapter, natural language processing prepares textual data for machine learning and deep learning models. The models perform most efficiently when provided with numerical data as input, and thus a key role of natural language processing is to transform preprocessed textual data into numerical data, which is a numerical representation of the textual data.</p>
			<p>This is what word embeddings are: they are numerical representations in the form of real-value vectors for text. Words that have similar meanings map to similar vectors and thus have similar representations. This aids the machine in learning the meaning and context of different words. Since word embeddings are vectors mapping to individual words, word embeddings can only be generated once tokenization has been performed on the corpus.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer028">
					<img alt="Figure 1.17: Example for word embeddings&#13;&#10;" src="image/C13783_01_17.jpg"/>
				</div>
			</div>
			<h6>Figure 1.17: Example for word embeddings</h6>
			<p>Word embeddings encompass a variety of techniques used to create a learned numerical representation and are the most popular way to represent a document's vocabulary. The beneficial aspect of word embeddings is that they are able to capture contextual, semantic, and syntactic similarities, and the relations of a word with other words, to effectively train the machine to comprehend natural language. This is the main aim of word embeddings – to form clusters of similar vectors that correspond to words with similar meanings.</p>
			<p>The reason for using word embeddings is to make machines understand synonyms the same way we do. Consider an example of online restaurant reviews – they consist of adjectives describing food, ambience, and the overall experience. They are either positive or negative, and comprehending which reviews fall into which of these two categories is important. The automatic categorization of these reviews can provide a restaurant with quick insights as to what areas they need to improve on, what people liked about their restaurant, and so on.</p>
			<p>There exist a variety of adjectives that can be classified as positive, and the same goes with negative adjectives. Thus, not only does the machine need to be able to differentiate between negative and positive, it also needs to learn and understand that multiple words can relate to the same category because they ultimately mean the same thing. This is where word embeddings are helpful.</p>
			<p>Consider the example of restaurant reviews received on a food service application. The following two sentences are from two separate restaurant reviews:</p>
			<ul>
				<li>Sentence A – The food here was great.</li>
				<li>Sentence B – The food here was good.</li>
			</ul>
			<p>The machine needs to be able to comprehend that both these reviews are positive and mean a similar thing, despite the adjective in both sentences being different. This is done by creating word embeddings, because the two words 'good' and 'great' map to two separate but similar real-value vectors and, thus, can be clustered together.</p>
			<h3 id="_idParaDest-41"><a id="_idTextAnchor042"/>The Generation of Word Embeddings</h3>
			<p>We've understood what word embeddings are and their importance; now we need to understand how they're generated. The process of transforming words into their real-value vectors is known as vectorization and is done by word embedding techniques. There are many word embedding techniques available, but in this chapter, we will be discussing the two main ones – Word2Vec and GloVe. Once word embeddings (vectors) have been created, they combine to form a vector space, which is an algebraic model consisting of vectors that follow the rules of vector addition and scalar multiplication. If you don't remember your linear algebra, this might be a good time to quickly review it.</p>
			<h3 id="_idParaDest-42"><a id="_idTextAnchor043"/>Word2Vec</h3>
			<p>As mentioned earlier, Word2Vec is one of the word embedding techniques used to generate vectors from words – something you can probably understand from the name itself.</p>
			<p>Word2Vec is a shallow neural network – it has only two layers – and thus does not qualify as a deep learning model. The input is a text corpus, which it uses to generate vectors as the output. These vectors are known as feature vectors for the words present in the input corpus. It transforms a corpus into numerical data that can be understood by a deep neural network.</p>
			<p>The aim of Word2Vec is to understand the probability of two or more words occurring together and thus to group words with similar meanings together to form a cluster in a vector space. Like any other machine learning or deep learning model, Word2Vec becomes more and more efficient by learning from past data and past occurrences of words. Thus, if provided with enough data and context, it can accurately guess a word's meaning based on past occurrences and context, similar to how we understand language.</p>
			<p>For example, we are able to create a connection between the words 'boy' and 'man', and 'girl' and 'woman,' once we have heard and read about them and understood what they mean. Likewise, Word2Vec can also form this connection and generate vectors for these words that lie close together in the same cluster so as to ensure that the machine is aware that these words mean similar things.</p>
			<p>Once Word2Vec has been given a corpus, it produces a vocabulary wherein each word has a vector of its own attached to it, which is known as its neural word embedding, and simply put, this neural word embedding is a word written in numbers.</p>
			<h3 id="_idParaDest-43"><a id="_idTextAnchor044"/>Functioning of Word2Vec</h3>
			<p>Word2Vec trains a word against words that neighbor the word in the input corpus, and there are two methods of doing so:</p>
			<ul>
				<li><em class="italics">Continuous Bag of Words (CBOW)</em>:<p>This method predicts the current word based on the context. Thus, it takes the word's surrounding words as input to produce the word as output, and it chooses this word based on the probability that this is indeed the word that is a part of the sentence. </p><p>For example, if the algorithm is provided with the words "the food was" and needs to predict the adjective after it, it is most likely to output the word "good" rather than output the word "delightful," since there would be more instances where the word "good" was used, and thus it has learned that "good" has a higher probability than "delightful." CBOW it said to be faster than skip-gram and has a higher accuracy with more frequent words.</p></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer029">
					<img alt="Fig 1.18: The CBOW algorithm&#13;&#10;" src="image/C13783_01_18.jpg"/>
				</div>
			</div>
			<h6>Fig 1.18: The CBOW algorithm</h6>
			<ul>
				<li><em class="italics">Skip-gram</em><p>This method predicts the words surrounding a word by taking the word as input, understanding the meaning of the word, and assigning it to a context. For example, if the algorithm was given the word "delightful," it would have to understand its meaning and learn from past context to predict that the probability that the surrounding words are "the food was" is highest. Skip-gram is said to work best with a small corpus.</p></li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer030">
					<img alt="Fig 1.19: The skip-gram algorithm&#13;&#10;" src="image/C13783_01_19.jpg"/>
				</div>
			</div>
			<h6>Fig 1.19: The skip-gram algorithm</h6>
			<p>While both methods seem to be working in opposite manners, they are essentially predicting words based on the context of local (nearby) words; they are using a window of context to predict what word will come next. This window is a configurable parameter.</p>
			<p>The decision of choosing which algorithm to use depends on the corpus at hand. CBOW works on the basis of probability and thus chooses the word that has the highest probability of occurring given a specific context. This means it will usually predict only common and frequent words since those have the highest probabilities, and rare and infrequent words will never be produced by CBOW. Skip-gram, on the other hand, predicts context, and thus when given a word, it will take it as a new observation rather than comparing it to an existing word with a similar meaning. Due to this, rare words will not be avoided or looked over. However, this also means that a lot of training data will be required for skip-gram to work efficiently. Thus, depending on the training data and corpus at hand, the decision to use either algorithm should be made.</p>
			<p>Essentially, both algorithms, and thus the model as a whole, require an intense learning phase where they are trained over thousands and millions of words to better understand context and meaning. Based on this, they are able to assign vectors to words and thus aid the machine in learning and predicting natural language. To understand Word2Vec better, let's do an exercise using Gensim's Word2Vec model.</p>
			<p>Gensim is an open source library for unsupervised topic modeling and natural language processing using statistical machine learning. Gensim's Word2Vec algorithm takes an input of sequences of sentences in the form of individual words (tokens).</p>
			<p>Also, we can use the <strong class="bold">min_count</strong> parameter. It exists to ask you how many instances of a word should be there in a corpus for it to be important to you, and then takes that into consideration when generating word embeddings. In a real-life scenario, when dealing with millions of words, a word that occurs only once or twice may not be important at all and thus can be ignored. However, right now, we are training our model only on three sentences each with only 5-6 words in every sentence. Thus, <strong class="bold">min_count</strong> is set to 1 since a word is important to us even if it occurs only once.</p>
			<h3 id="_idParaDest-44"><a id="_idTextAnchor045"/>Exercise 8: Generating Word Embeddings Using Word2Vec</h3>
			<p>In this exercise, we will be using Gensim's Word2Vec algorithm to generate word embeddings post tokenization.</p>
			<h4>Note</h4>
			<p class="callout">You will need to have <strong class="inline">gensim</strong> installed on your system for the following exercise. You can use the following command to install it, if it is not already installed: </p>
			<p class="callout"><strong class="inline">pip install –-upgrade gensim</strong></p>
			<p class="callout">For further information, click on <a href="">https://radimrehurek.com/gensim/models/word2vec.html</a>.</p>
			<p>The following steps will help you with the solution:</p>
			<ol>
				<li value="1">Open a new <strong class="inline">Jupyter</strong> notebook.</li>
				<li>Import the Word2Vec model from <strong class="inline">gensim</strong>, and import <strong class="inline">word_tokenize</strong> from <strong class="inline">nltk</strong>, as shown:<p class="snippet">from gensim.models import Word2Vec as wtv</p><p class="snippet">from nltk import word_tokenize</p></li>
				<li>Store three strings with some common words into three separate variables, and then tokenize each sentence and store all the tokens in an array, as shown:<p class="snippet">s1 = "Ariana Grande is a singer"</p><p class="snippet">s2 = "She has been a singer for many years"</p><p class="snippet">s3 = "Ariana is a great singer"</p><p class="snippet">sentences = [word_tokenize(s1), word_tokenize(s2), word_tokenize(s3)]</p><p>You can print the array of sentences to view the tokens.</p></li>
				<li>Train the model, as follows:<p class="snippet">model = wtv(sentences, min_count = 1)</p><p>Word2Vec's default value for <strong class="inline">min_count</strong> is 5. </p></li>
				<li>Summarize the model, as demonstrated:<p class="snippet">print('this is the summary of the model: ')</p><p class="snippet">print(model)</p><p>Your output will look something like this:</p><div class="IMG---Figure" id="_idContainer031"><img alt="Figure 1.20: Output for model summary&#13;&#10;" src="image/C13783_01_20.jpg"/></div><h6>Figure 1.20: Output for model summary</h6><p>Vocab = 12 signifies that there are 12 different words present in the sentences that were input to the model.</p></li>
				<li>Let's find out what words are present in the vocabulary by summarizing it, as shown:<p class="snippet">words = list(model.wv.vocab)</p><p class="snippet">print('this is the vocabulary for our corpus: ')</p><p class="snippet">print(words)</p><p>Your output will look something like this:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer032">
					<img alt="Figure 1.21: Output for the vocabulary of the corpus&#13;&#10;" src="image/C13783_01_21.jpg"/>
				</div>
			</div>
			<h6>Figure 1.21: Output for the vocabulary of the corpus</h6>
			<p>Let's see what the vector (word embedding) for the word 'singer' is:</p>
			<p class="snippet">print("the vector for the word singer: ")</p>
			<p class="snippet">print(model['singer'])</p>
			<p><strong class="bold">Expected output:</strong></p>
			<div>
				<div class="IMG---Figure" id="_idContainer033">
					<img alt="Figure 1.22: Vector for the word ‘singer’&#13; &#10;" src="image/C13783_01_22.jpg"/>
				</div>
			</div>
			<h6>Figure 1.22: Vector for the word 'singer'</h6>
			<p>Our Word2Vec model has been trained on these three sentences, and thus its vocabulary only includes the words present in this sentence. If we were to find words that are similar to a particular input word from our Word2Vec model, we wouldn't get words that actually make sense since the vocabulary is so small. Consider the following examples:</p>
			<p class="snippet">#lookup top 6 similar words to great</p>
			<p class="snippet">w1 = ["great"]</p>
			<p class="snippet">model.wv.most_similar (positive=w1, topn=6)</p>
			<p>The 'positive' refers to the depiction of only positive vector values in the output.</p>
			<p>The top six similar words to 'great' would be:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer034">
					<img alt="Figure 1.23: Word vectors similar to the word ‘great’&#13;&#10;" src="image/C13783_01_23.jpg"/>
				</div>
			</div>
			<h6>Figure 1.23: Word vectors similar to the word 'great'</h6>
			<p>Similarly, for the word 'singer', it could be as follows:</p>
			<p class="snippet">#lookup top 6 similar words to singer</p>
			<p class="snippet">w1 = ["singer"]</p>
			<p class="snippet">model.wv.most_similar (positive=w1, topn=6)</p>
			<div>
				<div class="IMG---Figure" id="_idContainer035">
					<img alt="Figure 1.24: Word vector similar to word ‘singer’&#13;&#10;" src="image/C13783_01_24.jpg"/>
				</div>
			</div>
			<h6>Figure 1.24: Word vector similar to word 'singer'</h6>
			<p>We know that these words are not actually similar in meaning to our input words at all, and that also shows up in the correlation value beside them. However, they show up because these are the only words that exist in our vocabulary.</p>
			<p>Another important parameter of the <strong class="inline">Gensim</strong> Word2Vec model is the size parameter. Its default value is 100 and implies the size of the neural network layers that are being used to train the model. This corresponds to the amount of freedom the training algorithm has. A larger size requires more data but also leads to higher accuracy.</p>
			<h4>Note</h4>
			<p class="callout">For more information on Gensim's Word2Vec model, click on</p>
			<p class="callout"> <a href="">https://rare-technologies.com/word2vec-tutorial/</a>.</p>
			<h3 id="_idParaDest-45"><a id="_idTextAnchor046"/>GloVe</h3>
			<p>GloVe, an abbreviation of "global vectors," is a word embedding technique that has been developed by Stanford. It is an unsupervised learning algorithm that builds on Word2Vec. While Word2Vec is quite successful in generating word embeddings, the issue with it is that is it has a small window through which it focuses on local words and local context to predict words. This means that it is unable to learn from the frequency of words present globally, that is, in the entire corpus. GloVe, as mentioned in its name, looks at all the words present in a corpus.</p>
			<p>While Word2Vec is a predictive model as it learns vectors to improve its predictive abilities, GloVe is a count-based model. What this means is that GloVe learns its vectors by performing dimensionality reduction on a co-occurrence counts matrix. The connections that GloVe is able to make are along the lines of this:</p>
			<p><em class="italics">king – man + woman = queen</em></p>
			<p>This means it's able to understand that "king" and "queen" share a relationship that is similar to that between "man" and "woman".</p>
			<p>These are complicated terms, so let's understand them one by one. All of these concepts come from statistics and linear algebra, so if you already know what's going on, you can skip to the activity!</p>
			<p>When dealing with a corpus, there exist algorithms to construct matrices based on term frequencies. Basically, these matrices contain words that occur in a document as rows, and the columns are either paragraphs or separate documents. The elements of the matrices represent the frequency with which the words occur in the documents. Naturally, with a large corpus, this matrix will be huge. Processing such a large matrix will take a lot of time and memory, thus we perform dimensionality reduction. This is the process of reducing the size of the matrix so it is possible to perform further operations on it.</p>
			<p>In the case of GloVe, the matrix is known as a co-occurrence counts matrix, which contains information on how many times a word has occurred in a particular context in a corpus. The rows are the words and the columns are the contexts. This matrix is then factorized in order to reduce the dimensions, and the new matrix has a vector representation for each word.</p>
			<p>GloVe also has pretrained words with vectors attached to them that can be used if the semantics match the corpus and task at hand. The following activity guides you through the process of implementing GloVe in Python, except that the code isn't directly given to you, so you'll have to do some thinking and maybe some googling. Try it out!</p>
			<h3 id="_idParaDest-46"><a id="_idTextAnchor047"/>Exercise 9: Generating Word Embeddings Using GloVe</h3>
			<p>In this exercise, we will be generating word embeddings using <strong class="bold">Glove-Python</strong>.</p>
			<h4>Note</h4>
			<p class="callout">To install Glove-Python on your platform, go to <a href="">https://pypi.org/project/glove/#files</a>.</p>
			<p class="callout">Download the Text8Corpus from <a href="">http://mattmahoney.net/dc/text8.zip</a>.</p>
			<p class="callout">Extract the file and store it with your Jupyter notebook.</p>
			<ol>
				<li value="1">Import <strong class="inline">itertools</strong>:<p class="snippet">import itertools</p></li>
				<li>We need a corpus to generate word embeddings for, and the <strong class="inline">gensim.models.word2vec</strong> library, luckily, has one called <strong class="inline">Text8Corpus</strong>. Import this along with two modules from the <strong class="bold">Glove-Python</strong> library:<p class="snippet">from gensim.models.word2vec import Text8Corpus</p><p class="snippet">from glove import Corpus, Glove</p></li>
				<li>Convert the corpus into sentences in the form of a list using <strong class="inline">itertools</strong>:<p class="snippet">sentences = list(itertools.islice(Text8Corpus('text8'),None))</p></li>
				<li>Initiate the <strong class="inline">Corpus()</strong> model and fit it on to the sentences:<p class="snippet">corpus = Corpus()</p><p class="snippet">corpus.fit(sentences, window=10)</p><p>The <strong class="inline">window</strong> parameter controls how many neighboring words are considered.</p></li>
				<li>Now that we have prepared our corpus, we need to train the embeddings. Initiate the <strong class="inline">Glove()</strong> model:<p class="snippet">glove = Glove(no_components=100, learning_rate=0.05)</p></li>
				<li>Generate a co-occurrence matrix based on the corpus and fit the <strong class="inline">glove</strong> model on to this matrix:<p class="snippet">glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)</p><p>The model has been trained!</p></li>
				<li>Add the dictionary of the corpus:<p class="snippet">glove.add_dictionary(corpus.dictionary)</p></li>
				<li>Use the following command to see which words are similar to your choice of word based on the word embeddings generated:<p class="snippet">glove.most_similar('man')</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer036">
					<img alt="Figure 1.25: Output of word embeddings for ‘man’&#13; &#10;" src="image/C13783_01_25.jpg"/>
				</div>
			</div>
			<h6>Figure 1.25: Output of word embeddings for 'man'</h6>
			<p>You can try this out for several different words to see which words neighbor them and are the most similar to them:</p>
			<p class="snippet">glove.most_similar('queen', number = 10)</p>
			<p><strong class="bold">Expected output:</strong></p>
			<div>
				<div class="IMG---Figure" id="_idContainer037">
					<img alt="Figure 1.26: Output of word embeddings for ‘queen’&#13; &#10;" src="image/C13783_01_26.jpg"/>
				</div>
			</div>
			<h6>Figure 1.26: Output of word embeddings for 'queen'</h6>
			<h4>Note</h4>
			<p class="callout">To learn more about GloVe, go to <a href="">https://nlp.stanford.edu/projects/glove/</a>.</p>
			<h3 id="_idParaDest-47">Activity 1: Generating Wor<a id="_idTextAnchor048"/>d Embeddings from a Corpus Using Word2Vec.</h3>
			<p>You have been given the task of training a Word2Vec model on a particular corpus – the Text8Corpus, in this case – to determine which words are similar to each other. The following steps will help you with the solution.</p>
			<h4>Note</h4>
			<p class="callout">You can find the text corpus file at http://mattmahoney.net/dc/text8.zip.</p>
			<ol>
				<li value="1">Upload the text corpus from the link given previously.</li>
				<li>Import <strong class="inline">word2vec</strong> from <strong class="inline">gensim </strong>models.</li>
				<li>Store the corpus in a variable.</li>
				<li>Fit the word2vec model on the corpus.</li>
				<li>Find the most similar word to 'man'.</li>
				<li><em class="italics">'Father' is to 'girl', 'x' is to "boy</em>." Find the top 3 words for x.<h4>Note</h4><p class="callout">The solution for the activity can be found on page 296.</p><p><strong class="bold">Expected Outputs:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer038">
					<img alt="Figure 1.27: Output for similar word embeddings&#13;&#10;" src="image/C13783_01_27.jpg"/>
				</div>
			</div>
			<h6>Figure<a id="_idTextAnchor049"/> 1.27: Output for similar word embeddings</h6>
			<p>Top three words for '<strong class="bold">x</strong>' could be:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer039">
					<img alt="Figure 1.28: Output for top three words for ‘x’" src="image/C13783_01_28.jpg"/>
				</div>
			</div>
			<h6>Figure 1.28: Output for top three words for 'x'</h6>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor050"/>Summary</h2>
			<p>In this chapter, we learned about how natural language processing enables humans and machines to communicate in natural human language. There are three broad applications of natural language processing, and these are speech recognition, natural language understanding, and natural language generation.</p>
			<p>Language is a complicated thing, and so text is required to go through several phases before it can make sense to a machine. This process of filtering is known as text preprocessing and comprises various techniques that serve different purposes. They are all task- and corpora-dependent and prepare text for operations that will enable it to be input into machine learning and deep learning models.</p>
			<p>Since machine learning and deep learning models work best with numerical data, it is necessary to transform preprocessed corpora into numerical form. This is where word embeddings come into the picture; they are real-value vector representations of words that aid models in predicting and understanding words. The two main algorithms used to generate word embeddings are Word2Vec and GloVe.</p>
			<p>In the next chapter, we will be building on the algorithms used for natural language processing. The processes of POS tagging and named entity recognition will be introduced and explained.</p>
		</div>
	</body></html>