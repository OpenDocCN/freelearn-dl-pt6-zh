- en: Chapter 4. Building Realistic Images from Your Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many real-life complex problems, a single Generative Adversarial Network
    may not be sufficient to solve it. Instead it's better to decompose the complex
    problem into multiple simpler sub-problems and use multiple GANs to work on each
    sub-problem separately. Finally, you can stack or couple the GANs together to
    find a solution.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will first learn the technique of stacking multiple generative
    networks to generate realistic images from textual information. Next, you will
    couple two generative networks, to automatically discover relationships among
    various domains (relationships between shoes and handbags or actors and actresses).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is StackGAN? Its concept and architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthesizing realistic images from text description using TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering cross-domain relationships with DiscoGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating handbag images from edges using PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming gender (actor-to-actress or vice-versa) with facescrub data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to StackGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The idea of StackGAN was originally proposed by *Han Zhang*, *Tao Xu*, *Hongsheng
    Li*, *Shaoting Zhang*, *Xiaolei Huang*, *Xiaogang Wang*, and *Dimitris Metaxas*
    [*arXiv: 1612.03242,2017*] in the paper *Text to Photo-realistic Image Synthesis
    with Stacked Generative Adversarial Networks*, where GAN has been used to synthesize
    forged images starting from the text description.'
  prefs: []
  type: TYPE_NORMAL
- en: Synthesizing photo realistic images from text is a challenging problem in Computer
    Vision and has tremendous practical application. The problem of generating images
    from text can be decomposed into two manageable sub-problems using StackGAN. In
    this approach, we stack two stages of the generative network based on certain
    conditions (such as textual description and the output of the previous stage)
    to achieve this challenging task of realistic image generation from text input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us define some concepts and notation before diving into the model architecture
    and implementation details:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Io*: This is the original image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*t*: Text description'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*t*: Text embedding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*µ(t)*: Mean of text embedding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*∑(t)*: Diagonal covariance matrix of text embedding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*pdata*: Real data distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*pz*: Gaussian distribution of noise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*z*: Randomly sampled noise from Gaussian distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conditional augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we already know from [Chapter 2](ch02.html "Chapter 2. Unsupervised Learning
    with GAN"), *Unsupervised Learning with GAN*, in Conditional GAN both the generator
    and discriminator network receive additional conditioning variables *c* to yield
    *G(z;c)* and *D(x;c)*. This formulation helps the generator to generate images
    conditioned on variable *c*. The conditioning augmentation yields more training
    pairs given a small number of image-text pairs and is useful for modeling text
    to image translation as the same sentence usually maps to objects with various
    appearances. The textual description is first converted to text embedding *t*
    by encoding through an encoder and then transformed nonlinearly using a char-CNN-RNN
    model to create conditioning latent variables as the input of a stage-I generator
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Since the latent space for text embedding is usually high dimensional, to mitigate
    the problem of discontinuity in latent data manifold with a limited amount of
    data, a conditioning augmentation technique is applied to produce additional conditioning
    variable *c^* sampled from a Gaussian distribution *N(µ(t), ∑(t))*.
  prefs: []
  type: TYPE_NORMAL
- en: Stage-I
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this stage, the GAN network learns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating rough shapes and basic colors for creating objects conditioned on
    textual description
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating background regions from random noise sampled from prior distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The low resolution coarse images generated in this stage might not look real
    because they have some defects such as object shape distortion, missing object
    parts, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The stage-I GAN trains the discriminator *D0* (maximize the loss) and the generator
    *G0* (minimize the loss), alternatively as shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stage-I](img/B08086_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Stage-II
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this stage, the GAN network only focuses on drawing details and rectifying
    defects in low resolution images generated from stage-I (such as a lack of vivid
    object parts, shape distortion, and some omitted details from the text) to generate
    high resolution realistic images conditioned on textual description.
  prefs: []
  type: TYPE_NORMAL
- en: 'The stage-II GAN alternatively trains the discriminator *D* (maximize the loss)
    and generator *G* (minimize the loss), conditioned on the result of low resolution
    *G*[*0*]*(z; c^0)* and the Gaussian latent variable *c^*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stage-II](img/B08086_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Random noise *z* is replaced with Gaussian conditioning variables *c^* in stage-II.
    Also, the conditioning augmentation in stage-II has different fully connected
    layers to generate different means and standard deviation of the text embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture details of StackGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As illustrated in the following figure, for the generator network *G*[0] of
    stage-I, the text embedding *t* is first fed into a fully connected layer to generate
    *µ0* and *σ0* (*σ0* is the diagonal values of *∑0*) for the Gaussian distribution
    *N(µ0(t); ∑0(t))* and then the text conditioning variable *c^0* is then sampled
    from the Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: For the discriminator network *D0* of stage-I, the text embedding *t* is first
    compressed to *Nd* dimensions with a fully connected layer and then spatially
    replicated to *Md* x *Md* x *Nd* tensor. The image is passed through a series
    of down-sampling blocks to squeeze into *Md* x *Md* spatial dimension and then
    concatenated using a filter map along the channel dimension with the text tensor.
    The resulting tensor goes through a 1x1 convolutional layer to jointly learn features
    across the image and the text and finally output the decision score using one
    node fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: The generator of stage-II is designed as an encoder-decoder network with residual
    blocks and the text embedding *t* to generate the *Ng* dimensional text conditioning
    vector *c^*, which is spatially replicated to *Md* x *Md* x *Nd* tensor. The stage-I
    result *s0* generated is then fed into several down-sampling blocks (that is,
    encoder) until it is squeezed to spatial size *Mg* x *Mg*. The image features
    concatenated with text features along the channel dimension are passed through
    several residual blocks, to learn multi-modal representations across image and
    text features. Finally, the resulting tensors goes through a series of up-sampling
    layers (that is, decoder) to generate a *W* x *H* high resolution image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The discriminator of stage-II is similar to stage-1 with only extra down-sampling
    blocks to cater for the large image size in this stage. During training of the
    discriminator, the positive sample pairs is built from the real images and their
    corresponding text descriptions, whereas the negative sample consists of two groups:
    one having real images with mismatched text embedding and the other having synthetic
    images with their corresponding text embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture details of StackGAN](img/B08086_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. The architecture of the StackGAN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: *arXiv: 1612.03242,2017*'
  prefs: []
  type: TYPE_NORMAL
- en: The stage-I generator first draws a low resolution image by sketching a rough
    shape and basic colors of the object from the given text and painting the background
    from a random noise vector. The stage-II generator corrects defects and adds compelling
    details into stage-I results, yielding a more realistic high resolution image
    conditioned on stage-I results.
  prefs: []
  type: TYPE_NORMAL
- en: The up-sampling blocks consist of the nearest-neighbor up-sampling followed
    by the 33 convolutions, each of stride of 1\. Batch normalization and `ReLU` activation
    functions are applied after every convolution except the last one. The residual
    blocks again consist of 33 convolutions, each of stride 1, followed by batch normalization
    and `ReLU` activation function. The down-sampling blocks consist of 44 convolutions
    each of stride 2, followed by batch normalization and Leaky-ReLU, except batch
    normalization is not present in the first convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: Synthesizing images from text with TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us implement the code to synthesize realistic images from text and produce
    mind blowing result:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First clone the `git` repository: [https://github.com/Kuntal-G/StackGAN.git](https://github.com/Kuntal-G/StackGAN.git)
    and change the directory to `StackGAN`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Currently the code is compatible with an older version of TensorFlow (0.11),
    so you need to have TensorFlow version below 1.0 to successfully run this code.
    You can modify your TensorFlow version using: `sudo pip install tensorflow==0.12.0`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Also make sure torch is installed in your system. More information can be found
    here: [http://torch.ch/docs/getting-started.html](http://torch.ch/docs/getting-started.html).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then install the following packages using the `pip` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next download the pre-processed char-CNN-RNN text embedding birds model from:
    [https://drive.google.com/file/d/0B3y_msrWZaXLT1BZdVdycDY5TEE/view](https://drive.google.com/file/d/0B3y_msrWZaXLT1BZdVdycDY5TEE/view)
    using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now extract the downloaded file using the `unzip` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next download and extract the birds image data from Caltech-UCSD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will do preprocessing on the images to split into training and test
    sets and save the images in pickle format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Synthesizing images from text with TensorFlow](img/B08086_04_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Now we will download the pre-trained char-CNN-RNN text embedding model from:
    [https://drive.google.com/file/d/0B3y_msrWZaXLNUNKa3BaRjAyTzQ/view](https://drive.google.com/file/d/0B3y_msrWZaXLNUNKa3BaRjAyTzQ/view)
    and save it to the `models/` directory using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Also download the char-CNN-RNN text encoder for birds from [https://drive.google.com/file/d/0B0ywwgffWnLLU0F3UHA3NzFTNEE/view](https://drive.google.com/file/d/0B0ywwgffWnLLU0F3UHA3NzFTNEE/view)
    and save it under the `models/text_encoder` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will add some sentences to the `example_captions.txt` file to generate
    some exciting images of birds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`A white bird with a black crown and red beak`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`this bird has red breast and yellow belly`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we will execute the `birds_demo.sh` file under the `demo` directory
    to generate realistic bird images from the text description given in the `example_captions.txt`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Synthesizing images from text with TensorFlow](img/B08086_04_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Now the generated images will be saved under the `Data/birds/example_captions/`
    directory as shown in the following screenshot:![Synthesizing images from text
    with TensorFlow](img/B08086_04_06.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Voila, you have now generated impressive bird images from the textual description.
    Play with your own sentences to describe birds and visually verify the results
    with the description.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering cross-domain relationships with DiscoGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-domain relationships are often natural to humans and they can easily identify
    the relationship between data from various domains without supervision (for example,
    recognizing relationships between an English sentence and its translated sentence
    in Spanish or choosing a shoe to fit the style of a dress), but learning this
    relation automatically is very challenging and requires a lot of ground truth
    pairing information that illustrates the relations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Discovery Generative Adversarial Networks** (**DiscoGAN**) *arXiv: 1703.05192
    ,2017* discovers the relationship between two visual domains and successfully
    transfers styles from one domain to another by generating new images of one domain
    given an image from the other domain without any pairing information. DiscoGAN
    seeks to have two GANs coupled together that can map each domain to its counterpart
    domain. The key idea behind DiscoGAN is to make sure that all images in domain
    1 are representable by images in domain 2, and use the reconstruction loss to
    measure how well the original image is reconstructed after the two translations—that
    is, from domain 1 to domain 2 and back to domain 1.'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture and model formulation of DiscoGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into the model formulation and various `loss` functions associated
    with DiscoGAN, let us first define some related terminology and concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*G*[*AB*]: The `generator` function that translates input image *x*[*A*] from
    domain A into image *x*[*AB*] in domain B'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*G*[*BA*]: The `generator` function that translates input image *x[B]* from
    domain B into image *x*[*BA*] in domain A'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*G*[*AB*](*x*[*A*]): This is the complete set of all possible resulting values
    for all *x*[*A*]s in domain A that should be contained in domain B'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*G*[*BA*](*x*[*B*]): This is the complete set of all possible resulting values
    for all *x*[*B*]s in domain B, that should be contained in domain A'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*D*[*A*]: The `discriminator` function in domain A'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*D*[*B*]: The `discriminator` function in domain B![The architecture and model
    formulation of DiscoGAN](img/B08086_04_07.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure-2: DiscoGAN architecture with two coupled GAN models'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Source: *arXiv- 1703.05192, 2017*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The generator modules of DiscoGAN consist of an encoder-decoder pair to perform
    back to back image translation. A generator *G*[*AB*] first translates input image
    *x*[*A*] from domain A into the image *x*[*AB*] in domain B. Then the generated
    image is translated back to domain A image *x*[*ABA*] to match the original input
    image using reconstruction loss (equation-3) with some form of distance metrics,
    such as MSE, Cosine distance, and hinge-loss. Finally, the translated output image
    *x*[*AB*] of the generator is fed into the discriminator and gets scored by comparing
    it to the real image of domain B:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The architecture and model formulation of DiscoGAN](img/B08086_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The generator *GAB* receives two types of losses as shown (equation-5):'
  prefs: []
  type: TYPE_NORMAL
- en: '*L*[*CONSTA*]: A reconstruction loss that measures how well the original image
    is reconstructed after the two translations domain A-> domain B-> domain A'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*L*[*GANB*]: Standard GAN loss that measures how realistic the generated image
    is in domain B'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whereas the discriminator *D*[*B*] receives the standard GAN discriminator
    loss as shown (equation-6):'
  prefs: []
  type: TYPE_NORMAL
- en: '![The architecture and model formulation of DiscoGAN](img/B08086_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The two coupled GANs are trained simultaneously and both the GANs learn mapping
    from one domain to another along with reverse mapping for reconstruction of the
    input images from both domains using two reconstruction losses: *L*[*CONSTA*]
    and *L*[*CONSTB*].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters are shared between generators *G*[*AB*] and *G*[*BA*] of two
    GANs and the generated images *x*[*BA*] and *x*[*AB*] are then fed into the separate
    discriminators *L*[*DA*] and *L*[*DB*] respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The architecture and model formulation of DiscoGAN](img/B08086_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The total generator loss, *L*[*G*], is the sum of two GAN losses of the coupled
    model and reconstruction loss of each partial model as shown (equation-7). And
    the total discriminator loss, *L*[*D*], is the sum of the two discriminators losses
    *L*[*DA*] and *L*[*DB*], which discriminate real and fake images in domain A and
    domain B, respectively (equation- 8). In order to achieve bijective mapping having
    one-to-one correspondence, the DiscoGAN model is constrained by two *L*[*GAN*]
    losses and two *L*[*CONST*] reconstruction losses.
  prefs: []
  type: TYPE_NORMAL
- en: Injective mapping means that every member of **A** has its own unique matching
    member in **B** and surjective mapping means that every **B** has at least one
    matching **A**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bijective mapping means both injective and surjective are together and there
    is a perfect one-to-one correspondence between the members of the sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The architecture and model formulation of DiscoGAN](img/B08086_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Implementation of DiscoGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's now dig into the code to understand the concept (loss and measuring criteria)
    along with the architecture of DiscoGAN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator takes an input image of size 64x64x3 and feeds it through an
    encoder-decoder pair. The encoder part of the generator consists of five convolution
    layers with 4x4 filters, each followed by batch normalization and Leaky ReLU.
    The decoder part consists of five deconvolution layers with 4x4 filters, followed
    by a batch normalization and `ReLU` activation function, and outputs a target
    domain image of size 64x64x3\. The following is the generator code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The discriminator is similar to the encoder part of the generator and consists
    of five convolution layers with 4x4 filters, each followed by a batch normalization
    and `LeakyReLU` activation function. Finally, we apply the `sigmoid` function
    on the final convolution layer (`conv-5`) to generate a scalar probability score
    between [0,1] to judge real/fake data. The following is the discriminator code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we define the loss criteria for the generator and reconstruction using
    mean square error and binary cross entropy measures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we start generating images from one domain to other and calculate the reconstruction
    loss to understand how well the original image is reconstructed after two translations
    (`ABA` or `BAB`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we calculate the generator loss and discriminator loss across each domain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we calculate the total loss of the `discogan` model by summing up
    the losses from two cross domains (`A` and `B`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Generating handbags from edges with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will generate realistic handbag images from corresponding
    edges using the `pix2pix` dataset from Berkley. Make sure you have PyTorch ([http://pytorch.org/](http://pytorch.org/))
    and OpenCV ([http://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html](http://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html))
    installed on your machine before going through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First clone the `git` repository and change the directory to `DiscoGAN`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next download the `edges2handbags` dataset using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And then apply image translation between two domains: edges and handbags with
    the downloaded dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Generating handbags from edges with PyTorch](img/B08086_04_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Now, the images will be saved after every 1,000 iterations (as per the `image_save_interval`
    argument) per epoch, under the `results` directory with the respective task name
    used previously during the image translation step:![Generating handbags from edges
    with PyTorch](img/B08086_04_13.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is a sample output of the cross-domain images generated from
    domain A to domain B:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating handbags from edges with PyTorch](img/B08086_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-3: On the left-hand side are cross-domain (A -> B -> A) generated images
    (edges -> handbags -> edges), while on the right-hand side are cross-domain (B
    -> A -> B) generated images of (handbags -> edges -> handbags)'
  prefs: []
  type: TYPE_NORMAL
- en: Gender transformation using PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will transform the gender of actor-to-actress or vice versa
    using facial images of celebrities from the `facescrub` dataset. Just like the
    previous example, please make sure you have PyTorch and OpenCV installed on your
    machine before executing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First clone the `git` repository and change directory to `DiscoGAN` (you can
    skip this step if you executed the previous example of generating handbags from
    edges):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next download the `facescrub` dataset using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And then apply image translation between two domains, male and female, with
    the downloaded dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Gender transformation using PyTorch](img/B08086_04_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Now, the images will be saved after every 1,000 iterations (as per the `image_save_interval`
    argument) per epoch, under the `results` directory with the respective task name
    (`facescrub`) and epoch interval:![Gender transformation using PyTorch](img/B08086_04_16.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the sample output of the cross-domain images generated from
    domain A (male) to domain B (female):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gender transformation using PyTorch](img/B08086_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-4: On the left-hand side are cross-domain (A -> B -> A) generated images
    (Male -> Female -> Male), while on the right-hand side are cross-domain (B ->
    A -> B) generated images of (Female -> Male -> Female)'
  prefs: []
  type: TYPE_NORMAL
- en: DiscoGAN versus CycleGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main objective of both DiscoGAN (discussed previously) and CycleGAN (discussed
    in [Chapter 2](ch02.html "Chapter 2. Unsupervised Learning with GAN"), *Unsupervised
    Learning with GAN*) introduces a new approach to the problem of image-to-image
    translation by finding a mapping between a source domain X and a target domain
    Y for a given image, without pairing information.
  prefs: []
  type: TYPE_NORMAL
- en: From an architecture standpoint both the models consist of two GANs that map
    one domain to its counterpart domain and compose their losses as functions of
    the traditional generator loss (normally seen in GANs) and the reconstruction
    loss/cycle consistency loss.
  prefs: []
  type: TYPE_NORMAL
- en: There isn't a huge dissimilarity between the two models, except from the fact
    that DiscoGAN uses two reconstruction losses (a measure of how well the original
    image is reconstructed after the two translations X->Y->X), whereas CycleGAN uses
    a single cycle consistency loss with two translators F and G (F translates the
    image from domain X to domain Y and G performs the reverse) to make sure the two
    equilibriums (*F(G(b)) = b and G(F(a)) = a*, given *a*, *b* are images in domain
    X , Y respectively) are maintained.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far you have learned the approach of solving complex real-life problems (such
    as synthesizing images from text and discovering cross-domain relationships) by
    combining multiple GAN models together using StackGAN and DiscoGAN. In the next
    chapter, you will learn an important technique for dealing with small datasets
    in deep learning using pre-trained models and feature transfer and how to run
    your deep models at a large scale on a distributed system.
  prefs: []
  type: TYPE_NORMAL
