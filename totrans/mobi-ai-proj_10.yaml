- en: What is Next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computers are developing more and more day by day, and the form factors of devices
    are changing tremendously. In the past, we would only see computers in offices;
    however, now we see them on our home desks, on our laps, in our pockets, and on
    our wrists. The market is becoming increasingly varied as machines are equipped
    with more and more intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Almost every adult currently carries a device with them, and it is estimated
    that we look at our smartphones at least 50 times a day, whether there is a need
    for it or not. These machines affect our daily decision-making processes. Devices
    are now equipped with applications such as Siri, Google Assistant, Alexa, or Cortana—these
    are features that are designed to mimic human intelligence. The ability to answer
    any query thrown at them presents these types of technology as superior to humans.
    On the backend of this, these systems continuously improve by using the collective
    intelligence that is acquired from all users. The more you interact with virtual
    assistants, the better the results they give out.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these advancements, how much closer are we to actually creating a human
    brain through a machine? We are in 2019 now; if science discovers a way to control
    the neurons of our brain, this may be possible in the near future. Machines that
    mimic the capabilities of a human are helping to solve complex textual, visual,
    and audio problems. They resemble the tasks carried out by a human brain on a
    daily basis; to put this into perspective, on average, the human brain makes approximately
    35,000 decisions in a day.
  prefs: []
  type: TYPE_NORMAL
- en: While we will be able to mimic the human brain in the future, it will come at
    a cost. We don't have a cheaper solution for it at the moment. The magnitude of
    power consumption of a human brain simulation limits the development efforts in
    comparison to an actual human brain. The human brain consumes about 20 W of power,
    while a simulation program consumes about 1 MW of power or more. Neurons in the
    human brain operate at a speed of 200 Hz, while a typical microprocessor operates
    at a speed of 2 GHz, which is 10 million times more than the speed of neurons
    in the human brain.
  prefs: []
  type: TYPE_NORMAL
- en: While we are still far from cloning a human brain, we can implement an algorithm
    that makes conscious decisions based on previous data, as well as data from similar
    devices. This is where the subset of **Artificial Intelligence** (**AI**) comes
    in handy. With predefined algorithms that identify patterns from the complex data
    we have, these types of intelligence can then give us useful information.
  prefs: []
  type: TYPE_NORMAL
- en: When the computer starts making decisions without being instructed explicitly
    every time, we achieve **Machine Learning** (**ML**) capability. ML is used everywhere
    right now, including through features such as identifying email spam, recommending
    the best product to buy on an e-commerce website, tagging your face automatically
    on a social media photograph, and more. All of these are done using patterns identified
    in historical data, and also through algorithms that reduce unnecessary noise
    from the data and produce quality output. When the data accumulates more and more,
    the computers can make better decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Mobile phones have become the default consumption medium for most of the digital
    products that are being produced today. As data consumption increases, we have
    to get results to the user as soon as possible. For example, when you scroll through
    your Facebook feed, a lot of content will be based on your interests and what
    your friends have liked. Since the time that users spend on these apps is limited,
    there are a lot of algorithms running on the server and client side, to load and
    organize content based on what you prefer to see on your Facebook feed. If there
    was the possibility of running all the algorithms on the local device itself,
    we wouldn't need to depend on the internet to load the content faster. This is
    only possible by performing the processes on the client's device itself, instead
    of processing in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: As the processing capability of mobile devices increases, we will be encouraged
    to run all ML models on the mobile device itself. There are a lot of services
    that are already being processed on the client's device, such as identifying a
    face from a photo (such as Apple's Face ID feature), which uses ML on the local
    device.
  prefs: []
  type: TYPE_NORMAL
- en: While multiple topics are trending such as AI, **augmented reality** (**AR**),
    **virtual reality** (**VR**), ML, and blockchain—ML is growing faster than others,
    with proper use cases evident across all sectors. ML algorithms are being applied
    to images, text, audio, and video in order to get the output that we desire.
  prefs: []
  type: TYPE_NORMAL
- en: If you are a beginner, then there are multiple ways to start your work, by utilizing
    all of the free and open source frameworks that are being built. If you are worried
    about building a model yourself, you can start with ML Kit for Firebase from Google.
    Alternatively, you can build your own model using TensorFlow, and convert that
    model into a TensorFlow Lite model (for Android), and a Core ML model (for iOS).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Popular ML-based cloud services
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Where to start when you build your first ML-based mobile app
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: References to further reading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular ML–based cloud services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To begin your ML journey, you can use one of the existing cloud-based services
    on ML. **ML as a Service** (**MLaaS**) is widely used across all contemporary
    business sectors. Data is becoming cheaper, and data volume is growing exponentially.
    As a result, the processing power of devices is increasing at a much quicker rate.
    This trend has made way for multiple cloud-based services, such as **Software
    as a Service** (**SaaS**), **Platform as a Service** (**PaaS**), and **Infrastructure
    as a Service** (**IaaS**), now joined by MLaaS.
  prefs: []
  type: TYPE_NORMAL
- en: Even though we can run an ML model on our mobile device, it is still greatly
    impacted by limited memory, CPU and **Graphics Processing Unit** (**GPU**) resources.
    In this case, a cloud service comes in handy.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with ML in the cloud, there are multiple services available, such as
    facial recognition, optical character recognition, image recognition, landmark
    recognition, data visualization, and **natural language processing** (**NLP**).
    All of these options are supported by deep neural networks, **convolutional neural
    networks** (**CNNs**), probabilistic models, and more. Most cloud providers run
    a business model that offers some free limits for a developer to explore, before
    deciding which is the best fit to develop their application.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections will explain the four major services that are available
    now, and are also popular among developers and enterprises. As a beginner, you
    can explore the functionality under each provider and pick the one that suits
    your application.
  prefs: []
  type: TYPE_NORMAL
- en: IBM Watson services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IBM Watson provides deep learning as a service through a variety of products.
    There is a textbot service, called AI assistant, which supports mobile platforms
    and chat services; and a service called Watson Studio, which is helpful for building
    and analyzing the model. IBM Watson also has another separate API service, which
    processes text, vision, and speech.
  prefs: []
  type: TYPE_NORMAL
- en: There is a sample application available for developing a vision application
    using Core ML. This can be found at [https://github.com/watson-developer-cloud/visual-recognition-coreml](https://github.com/watson-developer-cloud/visual-recognition-coreml).
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure Cognitive Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Microsoft provides out-of-the-box Cognitive Services in five categories, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Vision APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function of these APIs is mentioned in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Vision APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vision APIs are algorithms used for image processing to recognize, caption,
    and moderate your pictures smartly.
  prefs: []
  type: TYPE_NORMAL
- en: Speech APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Through speech APIs, the spoken audio is converted to text. The APIs use voice
    print for verification, or for adding speaker recognition to an application.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To solve tasks such as intelligent recommendations and semantic searches, we
    use knowledge APIs that help us to map the complex information and data.
  prefs: []
  type: TYPE_NORMAL
- en: Search APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Search APIs provide Bing web search APIs to your apps. They also harnesses the
    ability to combine billions of web pages, images, videos, and news with a single
    API call.
  prefs: []
  type: TYPE_NORMAL
- en: Language APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Language APIs allow your apps to process natural language with pre-built scripts,
    evaluate sentiments, and learn how to recognize what the user needs.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple sample applications for the preceding APIs. These can be
    found at [https://azure.microsoft.com/en-in/resources/samples/?%20sort=0&sort=0](https://azure.microsoft.com/en-in/resources/samples/?%20sort=0&sort=0).
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Web Services** (**AWS**) has multiple offerings for ML-based services.
    All of these services are tightly coupled, in order to work efficiently in the
    AWS cloud. A few of the services are highlighted in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Vision services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS has Amazon Rekognition, which is a deep learning-based service that is designed
    to process images and videos. We can integrate the service on mobile devices as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Chat services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Lex helps to build chatbots. This industry is still growing, with more
    and more data coming in; the service will become more intelligent, allowing it
    to answer queries even better.
  prefs: []
  type: TYPE_NORMAL
- en: Language services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some examples of language services include Amazon Comprehend, which helps to
    uncover insights and relationships in text; Amazon Translate, which helps with
    the fluent translation of text; Amazon Transcribe, which helps with automatic
    speech recognition; and Amazon Polly, which helps to turn natural-sounding text
    into speech.
  prefs: []
  type: TYPE_NORMAL
- en: You can see a few sample applications at [https://github.com/aws-samples/machine-learning-samples](https://github.com/aws-samples/machine-learning-samples).
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to run your model in the cloud, the Google Cloud ML Engine offers
    the power and flexibility of TensorFlow, scikit-learn, and XGBoost in the cloud.
    If this is not suitable, you can pick the API services that best fit your scenario.
    In Google Cloud ML, multiple APIs are available. These are classified into four
    major categories, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vision**: The Cloud Vision API helps with image recognition and classification;
    the Cloud Video Intelligence API helps with scene-level video annotation; and
    AutoML Vision helps with custom image classification models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conversation**: Dialogflow Enterprise Edition builds conversational interfaces;
    the Cloud Text-to-Speech API converts text to speech; and the Cloud Speech-to-Text
    API converts speech to text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language**: The Cloud Translation API is used in language detection and translation;
    the Cloud Natural Language API is used in text parsing and analysis; AutoML Translation
    is used in custom domain-specific translation; and AutoML Natural Language helps
    in building custom text classification models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge**: The Cloud Inference API derives insights from time series datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find a number of Google Vision APIs at [https://github.com/GoogleCloudPlatform/cloud-vision](https://github.com/GoogleCloudPlatform/cloud-vision).
  prefs: []
  type: TYPE_NORMAL
- en: There are also other services that are popular with developers, including **Dialogflow**
    and **Wit.ai**.
  prefs: []
  type: TYPE_NORMAL
- en: Building your first ML model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the knowledge that you have gained from this book, you can start to develop
    your own model that runs on a mobile phone. You will need to identify the problem
    statement first. There are many use cases where you will not need an ML model;
    we can''t unnecessarily force ML into everything. Consequently, you need to follow
    a step-by-step approach before you build your own model:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plan the effectiveness of your model; decide whether the data could be useful
    in predicting the output for future, similar cases. For example, collecting the
    purchase history for people of a similar age, gender, and location will be helpful
    in predicting a new customer's purchasing preferences. However, the data won't
    be helpful in predicting the height of a new customer, if that is the data that
    you are looking for.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop a simple model (this can be based on SQL). This will be useful for reducing
    the effort when building actual models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Validate the data and throw the unnecessary data out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the actual model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As data is growing exponentially across various parameters (data from multiple
    sensors), on the local devices as well as with cloud providers, we can build better
    use cases with more and more personalized content. There are many applications
    that are already using ML, such as mail services (Gmail and Outlook) and cab services
    (Uber, Ola, and Lyft).
  prefs: []
  type: TYPE_NORMAL
- en: The limitations of building your own model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While ML is getting popular, it is not yet feasible to run ML models on mobile
    platforms to reach the masses. When you are building your own model for mobile
    apps, there are some limitations as well. While it is possible to make predictions
    on a local device without a cloud service, it is not advisable to build an evolving
    model that makes predictions based on your current actions and accumulates data
    on the local device itself. As of right now, we can run pre-built models and get
    inferences out of them on mobile devices, due to the constraints on memory and
    the processing power of the mobile devices. Once we have better processors on
    mobile devices, we can train and improve the model on the local device.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of use cases related to this. Apple's Face ID is one such example,
    running a model on a local device that requires computations from a CPU or GPU.
    When the device's capability increases in the future, it will be possible to build
    a completely new model on the device itself.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is another reason why people refrain from developing models on their
    mobile devices. Since we are currently unable to run heavy operations on our mobile
    devices, the accuracy, as compared to a cloud-based service, seems bleak, the
    reason for this being the limitations on both memory and computational capability.
    You could run the models that are available for mobile devices in the TensorFlow
    and Core ML libraries instead.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow Lite models can be found at [https://www.tensorflow.org/lite/models](https://www.tensorflow.org/lite/models);
    and the Core ML models can be found at [https://github.com/likedan/Awesome-CoreML-Models](https://github.com/likedan/Awesome-CoreML-Models).
  prefs: []
  type: TYPE_NORMAL
- en: Personalized user experience
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A personalized **user experience** (**UX**) will be the basic use case for
    any mobile based consumer business, to provide a more curated and personalized
    experience for the users of their mobile applications. This can be done by analyzing
    data points, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Who is your customer?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where do they come from?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are their interests?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do they say about you?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where did they find you?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do they have any pain points?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can they afford your products?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do they have a history of purchases or searches?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, consider the customer of a retail company or a restaurant. If we
    have answers to the preceding questions, we have rich data about the customers,
    from which we can build data models that will provide more personalized experiences
    (with the help of ML). We can analyze and identify similar customer, to provide
    a better UX for all of the users, as well as targeting the right future customers.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at these ideas in further detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Providing better search results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Providing better search results is one of the major use cases, especially on
    a mobile application, to provide more contextual results, rather than text-based
    results. This will help to improve the business's customer base. ML algorithms
    should learn from user searches and optimize the results. Even spelling corrections
    can be done intuitively. Moreover, collecting the user's behavioral data (concerning
    how they use your app) will be helpful in providing the best search results, so
    that you can rank the results in a way that is personalized to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Targeting the right user
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most apps capture the user's age and gender data when they install the application
    for the first time. This will help you to understand the common user group of
    your application. You will also have the user's data, which will give you the
    usage and frequency of how much the user utilizes the app, as well as location
    data, if that is permitted from the user's end. This will be helpful in predicting
    customer targets in the near future. For example, you will be able to see whether
    your user audience is coming from an age group of between 18 and 25, and is predominantly
    female. In that case, you could devise a strategy to pull more male users, or
    just stick to targeting female users only. The algorithm should be able to predict
    and analyze all of this data, which will be helpful in marketing to and increasing
    your user base.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of niche use cases where ML-based mobile apps can be of great
    help; some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic product tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time estimations, as used in Pedometer, Uber, and Lyft
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Health-based recommendations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shipping cost estimations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supply chain predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Money management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistics optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing productivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all the basic ideas that you have gained from this book, you can start
    building your own application with ML capabilities. Furthermore, with all of the
    new ways to interact with devices such as Amazon Alexa, Google Home, or the Facebook
    portal, you will find more use cases to build ML applications. Ultimately, we
    are moving toward a world with more connected devices, bringing the connections
    and communications closer to us, and leading us to create better experiences with
    ML.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a lot of ML courses that are available online. A few of these courses
    are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If you are beginner, you can start with the Coursera tutorial on ML by Andrew
    Ng, which can be found at [https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ML crash course from Google can be found at [https://developers.google.com/machine-learning/crash-course/](https://developers.google.com/machine-learning/crash-course/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the best (and most enlightening) ML based blog series, by Adam Geitgey,
    can be found at [https://medium.comi@ageitgey/machine-learning-is-fun-80ea3ec3c471](https://medium.comi@ageitgey/machine-learning-is-fun-80ea3ec3c471)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can kick-start your skills in TensorFlow at [https://codelabs.developers.google.com/codelabs/tensorflow-for-poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A more thorough look at TensorFlow can be found at [https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
