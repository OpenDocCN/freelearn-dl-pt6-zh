<html><head></head><body>
		<div id="_idContainer146" class="Content">
			<h1 id="_idParaDest-94"><em class="italics"><a id="_idTextAnchor114"/>Chapter 5</em></h1>
		</div>
		<div id="_idContainer147" class="Content">
			<h1 id="_idParaDest-95"><a id="_idTextAnchor115"/>Convolutional Neural Networks for Computer Vision</h1>
		</div>
		<div id="_idContainer148" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Explain how convolutional neural networks work</li>
				<li class="bullets">Construct a convolutional neural network</li>
				<li class="bullets">Improve the constructed model by using data augmentation</li>
				<li class="bullets">Use state-of-the-art models by implementing transfer learning</li>
			</ul>
			<p>In this chapter, we will learn how to use probability distributions as a form of unsupervised learning.</p>
		</div>
		<div id="_idContainer171" class="Content">
			<h2 id="_idParaDest-96"><a id="_idTextAnchor116"/>Introduction</h2>
			<p><a id="_idTextAnchor117"/><a id="_idTextAnchor118"/><a id="_idTextAnchor119"/><a id="_idTextAnchor120"/><a id="_idTextAnchor121"/><a id="_idTextAnchor122"/><a id="_idTextAnchor123"/><a id="_idTextAnchor124"/><a id="_idTextAnchor125"/><a id="_idTextAnchor126"/><a id="_idTextAnchor127"/><a id="_idTextAnchor128"/><a id="_idTextAnchor129"/><a id="_idTextAnchor130"/><a id="_idTextAnchor131"/>In the previous chapter, we learned about how a neural network can be trained to predict values and how a <strong class="bold">recurrent neural network (RNN)</strong>, based on its architecture, can prove to be useful in many scenarios. In this chapter, we will discuss and observe how <strong class="keyword">convolutional neural networks (CNNs)</strong> work in a similar way to dense neural networks (also called fully-connected neural networks, as mentioned in <em class="italics">Chapter 2</em>, <em class="italics">Introduction to Computer Vision</em>). </p>
			<p>CNNs have neurons with weights and biases that are updated during training time. CNNs are mainly used for image processing. Images are interpreted as pixels and the network outputs the class it thinks the image belongs to, along with loss functions that state the errors with every classification and every output.</p>
			<p>These types of networks make an assumption that the input is an image or works like an image, allowing them to work more efficiently (CNNs are faster and better than deep neural networks). In the following sections, you will learn more about CNNs.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor132"/>Fundamentals of CNNs</h2>
			<p>In this topic, we will see how CNNs work and explain the process of convolving an image. </p>
			<p>We know that images are made up of pixels, and if the image is in RGB, for example, it will have three channels where each letter/color (Red-Green-Blue) has its own channel with a set of pixels of the same size. Fully-connected neural networks do not represent this depth in an image in every layer. Instead, they have a single dimension to represent this depth, which is not enough. Furthermore, they connect every single neuron of one layer to every single neuron of the next layer, and so on. This in turn results in lower performance, meaning you would have to train a network for longer and would still not get good results.</p>
			<p><strong class="bold">CNNs</strong> are a category of neural networks that has ended up being very effective for tasks such as classification and image recognition. Although, they also work very well for sound and text data. CNNs consist of an input, hidden layers, and an output layer, just like normal neural networks. The input and hidden layers are commonly formed by <strong class="bold">convolutional layers</strong>, <strong class="bold">pooling layers</strong> (layers that reduce the spatial size of the input), and <strong class="bold">fully-connected layers</strong> (fully-connected layers are explained in <em class="italics">Chapter 2</em>, <em class="italics">Introduction to Computer Vision</em>). Convolutional layers and pooling layers will be explained later on in this chapter.</p>
			<p>CNNs give depth to every layer, starting from the original depth of the image to deeper hidden layers as well. The following figure shows how a CNN works and what one looks like:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/C13550_05_01.jpg" alt="Figure 5.1: Representation of a CNN"/>
				</div>
			</div>
			<h6>Figure 5.1: Representation of a CNN</h6>
			<p>In the preceding figure, the CNN takes an input image of 224 x 224 x 3, which by convolutional processes is transformed into the next layer, which compresses the size but has more depth to it (we will explain how these processes work later on). These operations continue over and over until the graphical representation is flattened and these dense layers are used to end up with the corresponding classes of the dataset as output.</p>
			<p><strong class="keyword">Convolutional Layers:</strong> Convolutional layers consist of a set of <strong class="bold">filters</strong> of fixed size (typically a small size), which are matrices with certain values/weights, that are applied all over the input (an image, for example), by computing the scalar product between the filters and the input, which is called convolution. Each of these filters produces a two-dimensional activation map, which is stacked along the depth of the input. These activation maps look for features in the input and will determine how well the network learns. The more filters you have, the deeper the layer is, thus, the more your network learns, but the slower it gets at training time. For instance, in a particular image say, you would like to have 3 filters in the first layer, 96 filters in the next layer, 256 in the next, and so on. Note that, at the beginning of the network, there are usually fewer filters than at the end or in the middle of the network. This is because the middle and the end of the network have more potential features to extract, thus we need more filters, of a smaller size, toward the end of the network. This is because the deeper we advance into the network, the more we look at little details within an image, therefore we want to extract more features from those details to get a good understanding of the image.</p>
			<p>The sizes of the filters of convolutional layers often go from 2x2 to 7x7, for example, depending on whether you are at the beginning of the network (higher sizes) or toward the end (smaller sizes).</p>
			<p>In Figure 5.1, we can see convolution being applied using filters (in light blue) and the output would be a single value that goes to the next step/layer.</p>
			<p>After performing convolution, and before another convolution is applied, a max pooling (<strong class="keyword">pooling layer</strong>) layer is normally applied in order to reduce the size of the input so that the network can get a deeper understanding of the image. Nevertheless, lately, there is a tendency to avoid max pooling and instead encourage strides, which are naturally applied when performing convolution, so we are going to explain image reduction by naturally applying convolution.</p>
			<p><strong class="keyword">Strides:</strong> This is the length, defined in pixels, for the steps of the filter being applied over the entire image. If a stride of one is selected, the filter will be applied, but one pixel at a time. Similarly, if a stride of two is selected, then the filter will be applied two pixels at a time, the output size is smaller than the input, and so on.</p>
			<p>Let's look at an example. Firstly, Figure 5.2 will be used as the filter to convolve the image, which is a 2x2 matrix:</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/C13550_05_02.jpg" alt="Figure 5.2: Convolution filter"/>
				</div>
			</div>
			<h6>Figure 5.2: Convolution filter</h6>
			<p>And the following could be the image (matrix) we are convolving:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/C13550_05_03.jpg" alt="Figure 5.3: Image to convolve"/>
				</div>
			</div>
			<h6>Figure 5.3: Image to convolve</h6>
			<p>Of course, this is not a real image, but for the sake of simplicity, we are taking a matrix of 4x4 with random values to demonstrate how convolution works.</p>
			<p>Now, if we want to apply convolution with stride equal to 1, this would be the process, graphically:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/C13550_05_04.jpg" alt="Figure 5.4: Convolution process Stride=1"/>
				</div>
			</div>
			<h6>Figure 5.4: Convolution process Stride=1</h6>
			<p>The preceding Figure shows a 2x2 filter being applied to the input image, pixel by pixel. The process goes from left to right and from top to bottom.</p>
			<p>The filter multiplies every value of every position in its matrix to every value of every position of the zone (matrix) where it's being applied. For instance, in the first part of the process, the filter is being applied to the first 2x2 part of the image [1 2; 5 6] and the filter we have is [2 1; -1 2], so it would be 1*2 + 2*1 + 5*(-1) + 6*2 = 11.</p>
			<p>The resulting image, after applying the filter matrix, would be as shown here:</p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/C13550_05_05.jpg" alt="Figure 5.5: Convolution result Stride=1"/>
				</div>
			</div>
			<h6>Figure 5.5: Convolution result Stride=1</h6>
			<p>As you can see, the resulting image is now one dimension smaller. This is because there is another parameter, called <strong class="bold">padding</strong>, which is set to "valid" by default, which means that the convolution will be applied normally; that is, applying the convolution makes the image one pixel thinner by nature. If it is set to "same," the image will be surrounded by one line of pixels with a value equal to zero, thus the output matrix will have the same size as the input matrix.</p>
			<p>Now, we are going to apply a stride of 2, to reduce the size by 2 (just like a max pooling layer of 2x2 would do). Remember that we are using a padding equal to "valid."</p>
			<p>The process would have fewer steps, just like in the following figure:</p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/C13550_05_06.jpg" alt="Figure 5.6: Convolution process Stride=2"/>
				</div>
			</div>
			<h6>Figure 5.6: Convolution process Stride=2</h6>
			<p>And the output image/matrix would look like this:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/C13550_05_07.jpg" alt="Figure 5.7:  Convolution result Stride=2"/>
				</div>
			</div>
			<h6>Figure 5.7: Convolution result Stride=2</h6>
			<p>The resulting image would be an image of 2x2 pixels. This is due to the natural process of convolution with stride equal to 2.</p>
			<p>These filters, which are applied on every convolutional layer, have weights that the neural network adjusts so that the outputs of those filters help the neural network learn valuable features. These weights, as explained, are updated by the process of backpropagation. As a reminder, backpropagation is the process where the network's loss (or the amount of errors) of the predictions made versus the expected results in a training step of the network is calculated, updating all the weights of the neurons of the network that have contributed to that error so that they do not make the same mistake again.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor133"/>Building Your First CNN</h2>
			<h4>Note</h4>
			<p class="callout">For this chapter, we are going to still use Keras on top of TensorFlow as the backend, as mentioned in <em class="italics">Chapter 2, Introduction to Computer Vision</em> of this book. Also, we will still use Google Colab to train our network.</p>
			<p>Keras is a very good library for implementing convolutional layers, as it abstracts the user so that layers do not have to be implemented by hand.</p>
			<p>In <em class="italics">Chapter 2</em>, <em class="italics">Introduction to Computer Vision,</em> we imported the Dense, Dropout, and BatchNormalization layers by using the <strong class="inline">keras.layers</strong> package, and to declare convolutional layers of two dimensions, we are going to use the same package:</p>
			<p class="snippet">from keras.layers import Conv2D</p>
			<p>The <strong class="inline">Conv2D</strong> module is just like the other modules: you have to declare a sequential model, which was explained in <em class="italics">Chapter 2, Introduction to Computer Vision</em> of this book, and we also add <strong class="inline">Conv2D</strong>:</p>
			<p class="snippet">model = Sequential()</p>
			<p class="snippet">model.add(Conv2D(32, kernel_size=(3, 3), padding='same', strides=(2,2), input_shape=input_shape))</p>
			<p>For the first layer, the input shape has to be specified, but after that, it is no longer needed.</p>
			<p>The first parameter that must be specified is the <strong class="bold">number of filters</strong> that the network is going to learn in that layer. As mentioned before, in the earlier layers, we will filter few layers which will be learned, rather than the layers deeper in the network.</p>
			<p>The second parameter that must be specified is the <strong class="bold">kernel size</strong>, which is the size of the filter applied to the input data. Usually, a kernel of size 3x3 is set, or even 2x2, but sometimes when the image is large, a bigger kernel size is set.</p>
			<p>The third parameter is <strong class="bold">padding</strong>, which is set to "valid" by default, but it needs to be set to "same," as we want to preserve the size of the input in order to understand the behavior of down-sampling the input.</p>
			<p>The fourth parameter is <strong class="bold">strides</strong>, which, by default, is set to (1, 1). We will be setting it to (2, 2), since there are two numbers here and it has to be set for both the x and y axes.</p>
			<p>After the first layer, we will apply the same methodology as was mentioned in <em class="italics">Chapter 2</em>, <em class="italics">Introduction to Computer Vision</em>:</p>
			<p class="snippet">model.add(BatchNormalization())</p>
			<p class="snippet">model.add(Activation('relu'))</p>
			<p class="snippet">model.add(Dropout(0.2))</p>
			<p>As a reminder, the <strong class="bold">BatchNormalization</strong> layer is used to normalize the inputs of each layer, which helps the network converge faster and may give better results overall. </p>
			<p>The <strong class="bold">Activation</strong> layer is where the activation function is applied, and an <strong class="inline">activation</strong> function is a function that takes the input and calculates a weighted sum of it, adding a bias and deciding whether it should be activated or not (outputting 1 and 0, respectively).</p>
			<p><span class="normaltextrun">The </span><strong class="bold">Dropout</strong> layer helps the network avoid overfitting, which is when the accuracy of the training set is much higher than the accuracy of the validation set, by switching off a percentage of neurons.</p>
			<p>We could apply more sets of layers like this, varying the parameters, depending on the size of the problem to solve.</p>
			<p>The last layers remain the same as those of dense neural networks, depending on the problem.</p>
			<h3 id="_idParaDest-99"><a id="_idTextAnchor134"/>Exercise 17: Building a CNN</h3>
			<h4>Note</h4>
			<p class="callout">This exercise uses the same packages and libraries as <em class="italics">Chapter 2, Introduction to Computer Vision</em>. These libraries are Keras, Numpy, OpenCV, and Matplotlib.</p>
			<p>In this exercise, we are going to take the same problem as <em class="italics">Chapter 2</em>, <em class="italics">Activity 2</em>, <em class="italics">Classify 10 Types of Clothes of the Fashion-MNIST Database</em>.</p>
			<p>Remember that, in that activity, the neural network that was built was not capable of generalizing well enough to classify the unseen data that we passed to it.</p>
			<p>As a reminder, this problem is a classification problem, where the model has to classify 10 types of clothes correctly:</p>
			<ol>
				<li>Open up your Google Colab interface.</li>
				<li>Create a folder for the book and download the <strong class="inline">Datasets</strong> folder from GitHub and upload it in the folder in your drive.</li>
				<li>Import drive and mount it as follows:<p class="snippet">from google.colab import drive</p><p class="snippet">drive.mount('/content/drive')</p><h4>Note</h4><p class="callout">Every time you use a new collaborator, mount the drive to the desired folder.</p></li>
				<li>Once you have mounted your drive for the first time, you will have to enter the authorization code mentioned by clicking on the URL given by Google and press the <strong class="bold">Enter</strong> key on your keyboard:<div id="_idContainer156" class="IMG---Figure"><img src="image/C13550_05_08.jpg" alt="Figure 5.8: Mounting on Google Collab"/></div><h6>Figure 5.8: Mounting on Google Collab</h6></li>
				<li>Now that you have mounted the drive, you need to set the path of the directory:<p class="snippet">cd /content/drive/My Drive/C13550/Lesson05/</p><h4>Note</h4><p class="callout">The path mentioned in step 5 may change as per your folder setup on Google Drive. The path will always begin with <strong class="inline">cd /content/drive/My Drive/</strong>.</p></li>
				<li>First, let's import the data from Keras and initialize the random seed to 42 for reproducibility:<p class="snippet">from keras.datasets import fashion_mnist </p><p class="snippet">(x_train, y_train), (x_test, y_test) =fashion_mnist.load_data()</p><p class="snippet">import random</p><p class="snippet">random.seed(42) </p></li>
				<li>We import NumPy in order to pre-process the data and Keras utils to one-hot encode the labels:<p class="snippet">import numpy as np</p><p class="snippet">from keras import utils as np_utils</p><p class="snippet">x_train = (x_train.astype(np.float32))/255.0 </p><p class="snippet">x_test = (x_test.astype(np.float32))/255.0 </p><p class="snippet">x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) </p><p class="snippet">x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) </p><p class="snippet">y_train = np_utils.to_categorical(y_train, 10) </p><p class="snippet">y_test = np_utils.to_categorical(y_test, 10) </p><p class="snippet">input_shape = x_train.shape[1:]</p></li>
				<li>We declare the <strong class="inline">Sequential</strong> function to make a sequential model in Keras, the callbacks, and, of course, the layers:<p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.callbacks import EarlyStopping, ModelCheckpoint</p><p class="snippet">from keras.layers import Input, Dense, Dropout, Flatten</p><p class="snippet">from keras.layers import Conv2D, Activation, BatchNormalization</p><h4>Note </h4><p class="callout">We have imported a callback called <strong class="bold">EarlyStopping</strong>. What this callback does is stop the training after a number of epochs, where the metric that you choose (for example, validation accuracy) has dropped. You can set that number with the number of epochs that you want.</p></li>
				<li>Now, we are going to build our first CNN. First, let's declare the model as <strong class="inline">Sequential</strong> and add the first <strong class="inline">Conv2D</strong>:<p class="snippet">def CNN(input_shape):</p><p class="snippet">    model = Sequential()</p><p class="snippet">    model.add(Conv2D(32, kernel_size=(3, 3), padding='same', strides=(2,2), input_shape=input_shape))</p><p>We add 32 filters as is the first layer, and a filter size of 3x3. Padding is set to "<strong class="inline">same</strong>" and the strides are set to 2 to naturally reduce the dimensionality of the <strong class="inline">Conv2D</strong> module.  </p></li>
				<li>We follow this layer by adding <strong class="inline">Activation</strong> and <strong class="inline">BatchNormalization</strong> layers:<p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p></li>
				<li>We are going to add another three layers with the same characteristics as before, applying dropout and jumping to another block:<p class="snippet">    model.add(Conv2D(32, kernel_size=(3, 3), padding='same', strides=(2,2)))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p></li>
				<li>Now, we apply dropout of 20%, which turns off 20% of the neurons in the network:<p class="snippet">    model.add(Dropout(0.2))</p></li>
				<li>We are going to do the same procedure one more time but with 64 filters:<p class="snippet">    model.add(Conv2D(64, kernel_size=(3, 3), padding='same', strides=(2,2)))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Conv2D(64, kernel_size=(3, 3), padding='same', strides=(2,2)))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Dropout(0.2))</p></li>
				<li>For the end of the network, we apply the <strong class="inline">Flatten</strong> layer to make the output of the last layer one-dimensional. We apply a <strong class="inline">Dense</strong> layer with 512 neurons. Where the logistics of the network occur, we apply the <strong class="inline">Activation</strong> layer and the <strong class="inline">BatchNormalization</strong> layer, before applying a <strong class="inline">Dropout</strong> of 50%:<p class="snippet">    model.add(Flatten())</p><p class="snippet">    model.add(Dense(512))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Dropout(0.5))</p></li>
				<li>And, finally, we declare the last layer as a <strong class="inline">dense</strong> layer with 10 neurons, which is the number of classes of the dataset, and a <strong class="inline">Softmax</strong> activation function, which establishes which class the image is more likely to be, and we return the model:<p class="snippet">    model.add(Dense(10, activation="softmax"))</p><p class="snippet">    return model</p></li>
				<li>Let's declare the model along with the callbacks and compile it:<p class="snippet">model = CNN(input_shape)</p><p class="snippet"> </p><p class="snippet">model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])</p><p class="snippet"> </p><p class="snippet">ckpt = ModelCheckpoint('Models/model.h5', save_best_only=True,monitor='val_loss', mode='min', save_weights_only=False) </p><p class="snippet">earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0,mode='min')</p><p>For compiling, we are using the same optimizer. For declaring the checkpoint, we are using the same parameters. For declaring <strong class="inline">EarlyStopping</strong>, we are using the validation loss as the main metric and we set a patience of five epochs.</p></li>
				<li>Let the training begin!<p class="snippet">model.fit(x_train, y_train, batch_size=128, epochs=100, verbose=1, validation_data=(x_test, y_test), callbacks=[ckpt,earlyStopping]) </p><p>We set the batch size to 128 because there are enough images and because this way, it will take less time to train. The number of epochs is set to 100, as <strong class="inline">EarlyStopping</strong> will take care of stopping the training.</p><p>The accuracy obtained is better than in the exercise in <em class="italics">Chapter 2</em>, <em class="italics">Introduction to Computer Vision</em> – we have obtained an accuracy of <strong class="bold">92.72%</strong>. </p><p>Take a look at the following output:</p><div id="_idContainer157" class="IMG---Figure"><img src="image/C13550_05_09.jpg" alt=""/></div><h6>Figure 5.9: val_acc shown as 0.9240, which is 92.72%</h6><h4>Note</h4><p class="callout">The entire code for this exercise is available on GitHub: <a href="https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise17/Exercise17.ipynb">https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise17/Exercise17.ipynb</a>.</p></li>
				<li>Let's try with the same examples that we tried in <em class="italics">Activity 2</em>, <em class="italics">Classify 10 Types of Clothes of the Fashion-MNIST Database</em> of <em class="italics">Chapter 2</em>, which is located in <strong class="inline">Dataset/testing/</strong>:<p class="snippet">import cv2 </p><p class="snippet"> </p><p class="snippet">images = ['ankle-boot.jpg', 'bag.jpg', 'trousers.jpg', 't-shirt.jpg'] </p><p class="snippet"> </p><p class="snippet">for number in range(len(images)):</p><p class="snippet">    imgLoaded = cv2.imread('Dataset/testing/%s'%(images[number]),0) </p><p class="snippet">    img = cv2.resize(imgLoaded, (28, 28)) </p><p class="snippet">    img = np.invert(img) </p><p class="snippet">    img = (img.astype(np.float32))/255.0 </p><p class="snippet">    img = img.reshape(1, 28, 28, 1) </p><p class="snippet">  </p><p class="snippet">    plt.subplot(1,5,number+1),plt.imshow(imgLoaded,'gray') </p><p class="snippet">    plt.title(np.argmax(model.predict(img)[0])) </p><p class="snippet">    plt.xticks([]),plt.yticks([]) </p><p class="snippet">plt.show()</p></li>
			</ol>
			<p>Here's the output: </p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/C13550_05_10.jpg" alt="Figure 5.10: Prediction of clothes using CNNs"/>
				</div>
			</div>
			<h6>Figure 5.10: Prediction of clothes using CNNs</h6>
			<p>As a reminder, here is the table with the number of corresponding clothes:</p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/C13550_05_11.jpg" alt="Figure 5.11: The table with the number of corresponding clothes      "/>
				</div>
			</div>
			<h6>Figure 5.11: The table with the number of corresponding clothes</h6>
			<p>We can see that the model has predicted all the pictures well, so we can state that the model is far better than one with only dense layers.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor135"/>Improving Your Model - Data Augmentation</h2>
			<p>There are situations, at times, where you would not be able to improve the accuracy of your model by building a better model. Sometimes, the problem is not the model but the data. One of the most important things to consider when working with machine learning is that the data you work with has to be good enough for a potential model to generalize that data.</p>
			<p>Data can represent real-life things, but it can also include incorrect data that may perform badly. This can happen when you have incomplete data or data that does not represent the classes well. For those cases, data augmentation has become one of the most popular approaches.</p>
			<p>Data augmentation actually increases the number of samples of the original dataset. For computer vision, this could mean increasing the number of images in a dataset. There are several data augmentation techniques, and you may want to use a specific technique, depending on the dataset. Some of these techniques are mentioned here:</p>
			<ul>
				<li><strong class="bold">Rotation</strong>: The user sets the degree of rotation for images in the dataset.</li>
				<li><strong class="bold">Flip</strong>: To flip the images horizontally or vertically.</li>
				<li><strong class="bold">Crop</strong>: Crop a section from the images randomly.</li>
				<li><strong class="bold">Change color</strong>: Change or vary the color of the images.</li>
				<li><strong class="bold">Add Noise</strong>: To add noise to images.</li>
			</ul>
			<p>Applying these or other techniques, you end up generating new images that vary from the original ones.</p>
			<p>In order to implement this in code, Keras has a module called <strong class="inline">ImageDataGenerator</strong>, where you declare transformations that you want to apply to your dataset. You can import that module using this line of code:</p>
			<p class="snippet">from keras.preprocessing.image import ImageDataGenerator</p>
			<p>In order to declare the variable that is going to apply all those changes to your dataset, you have to declare it as in the following code snippet:</p>
			<p class="snippet">datagen = ImageDataGenerator(</p>
			<p class="snippet">        rotation_range=20,</p>
			<p class="snippet">        zoom_range = 0.2,</p>
			<p class="snippet">        width_shift_range=0.1,</p>
			<p class="snippet">        height_shift_range=0.1,</p>
			<p class="snippet">        horizontal_flip=True</p>
			<p class="snippet">        )</p>
			<h4>Note</h4>
			<p class="callout">You can see what attributes you can pass to <strong class="inline">ImageDataGenerator</strong> by looking at this documentation from Keras: <a href="https://keras.io/preprocessing/image/">https://keras.io/preprocessing/image/</a>.</p>
			<p>After declaring <strong class="inline">datagen</strong>, you have to compute some calculations for feature-wise normalization by using the following:</p>
			<p class="snippet">datagen.fit(x_train)</p>
			<p>Here, <strong class="inline">x_train</strong> is your training set.</p>
			<p>In order to train the model using data augmentation, the following code should be used:</p>
			<p class="snippet">model.fit_generator(datagen.flow(x_train, y_train,</p>
			<p class="snippet">                                 batch_size=batch_size),</p>
			<p class="snippet">                    epochs=epochs,</p>
			<p class="snippet">                    validation_data=(x_test, y_test),</p>
			<p class="snippet">                    callbacks=callbacks,</p>
			<p class="snippet">                    steps_per_epoch=len(x_train) // batch_size)</p>
			<p><strong class="inline">Datagen.flow()</strong> is used so that data augmentation can be applied. As Keras does not know when to stop applying data augmentation in the given data, <strong class="inline">Steps_per_epoch</strong> is the parameter that sets that limit, which should be the length of the training set divided by the batch size.</p>
			<p>Now we are going to jump right into the second exercise of this chapter to observe the output. Data augmentation promises better results and better accuracy. Let's find out whether that is true or not.</p>
			<h3 id="_idParaDest-101"><a id="_idTextAnchor136"/>Exercise 18: Improving Models Using Data Augmentation</h3>
			<p>In this exercise, we are going to use the The Oxford - III Pet dataset, which is RGB images, of varying sizes and several classes, of different cat/dog breeds. In this case, we will separate the dataset into two classes: cats and dogs, for simplicity. There are 1,000 images for each class, which is not much, but it will increment the effect of data augmentation. This dataset is stored in the <strong class="inline">Dataset/dogs-cats/</strong> folder, added on GitHub. </p>
			<p>We will build a CNN and train it with and without data augmentation, and we will compare the results:</p>
			<h4>Note</h4>
			<p class="callout">For this exercise, we are going to open another Google Colab notebook.</p>
			<p class="callout">The entire code for this exercise can be found on GitHub: <a href="https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise18/Exercise18.ipynb">https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise18/Exercise18.ipynb</a>.</p>
			<ol>
				<li value="1">Open up your Google Colab interface.</li>
				<li>Create a folder for the book and download the <strong class="inline">Datasets</strong> folder from GitHub and upload it in the folder in your drive.</li>
				<li>Import drive and mount it as follows:<p class="snippet">from google.colab import drive</p><p class="snippet">drive.mount('/content/drive')</p><h4>Note</h4><p class="callout">Every time you use a new collaborator, mount the drive to the desired folder.</p></li>
				<li>Once you have mounted your drive for the first time, you have to enter the authorization code mentioned by clicking on the URL given by Google. </li>
				<li>Now that you have mounted the drive, you need to set the path of the directory:<p class="snippet">cd /content/drive/My Drive/C13550/Lesson5/Dataset</p><h4>Note</h4><p class="callout">The path mentioned in step 5 may change as per your folder setup on Google Drive. The path will always begin with <strong class="inline">cd /content/drive/My Drive/</strong>.</p></li>
				<li>First, let's use these two methods, which we have already used before, to load the data from disk:<p class="snippet">import re, os, cv2</p><p class="snippet">import numpy as np</p><p class="snippet">rows,cols = 128,128</p><p class="snippet">//{…}##the detailed code can be found on Github##</p><p class="snippet">def list_files(directory, ext=None):</p><p class="snippet">//{…}##the detailed code can be found on Github##</p><p class="snippet">def load_images(path,label):</p><p class="snippet">//{…}</p><p class="snippet">    for fname in list_files( path, ext='jpg' ): </p><p class="snippet">        img = cv2.imread(fname)</p><p class="snippet">        img = cv2.resize(img, (rows, cols))</p><p class="snippet">//{…}##the detailed code can be found on Github##</p><h4>Note </h4><p class="callout">The size of the image is specified as 128x128. This size is larger than the sizes used before, because we need more detail in these images, as the classes are more difficult to differentiate and the subjects are presented in varying positions, which makes the work even more difficult.</p></li>
				<li>We load the corresponding images of dogs and cats as <strong class="inline">X</strong> for the images and <strong class="inline">y</strong> for the labels, and we print the shape of those:<p class="snippet">X, y = load_images('Dataset/dogs-cats/dogs',0)</p><p class="snippet">X_aux, y_aux = load_images('Dataset/dogs-cats/cats',1)</p><p class="snippet">X = np.concatenate((X, X_aux), axis=0)</p><p class="snippet">y = np.concatenate((y, y_aux), axis=0)</p><p class="snippet">print(X.shape)</p><p class="snippet">print(y.shape)</p><div id="_idContainer160" class="IMG---Figure"><img src="image/C13550_05_12.jpg" alt="Figure 5.12: Dogs-cats data shape"/></div><h6>Figure 5.12: Dogs-cats data shape</h6></li>
				<li>Now we will import <strong class="inline">random</strong>, set the seed, and show some samples of the data:<p class="snippet">import random </p><p class="snippet">random.seed(42) </p><p class="snippet">from matplotlib import pyplot as plt</p><p class="snippet"> </p><p class="snippet">for idx in range(5): </p><p class="snippet">    rnd_index = random.randint(0, X.shape[0]-1)</p><p class="snippet">    plt.subplot(1,5,idx+1)</p><p class="snippet">    plt.imshow(cv2.cvtColor(X[rnd_index],cv2.COLOR_BGR2RGB)) </p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show() </p><div id="_idContainer161" class="IMG---Figure"><img src="image/C13550_05_13.jpg" alt="Figure 5.13: Image samples of the Oxford Pet dataset"/></div><h6>Figure 5.13: Image samples of the Oxford Pet dataset</h6></li>
				<li>To pre-process the data, we are going to use the same procedure as in <em class="italics">Exercise 17: Building a CNN</em>:<p class="snippet">from keras import utils as np_utils</p><p class="snippet">X = (X.astype(np.float32))/255.0</p><p class="snippet">X = X.reshape(X.shape[0], rows, cols, 3) </p><p class="snippet">y = np_utils.to_categorical(y, 2)</p><p class="snippet">input_shape = X.shape[1:]</p></li>
				<li>Now, we separate <strong class="inline">X</strong> and <strong class="inline">y</strong> into <strong class="inline">x_train</strong> and <strong class="inline">y_train</strong> for the training set, and <strong class="inline">x_test</strong> and <strong class="inline">y_test</strong> for the testing set, and we print the shapes:<p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet">x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</p><p class="snippet">print(x_train.shape)</p><p class="snippet">print(y_train.shape)</p><p class="snippet">print(x_test.shape)</p><p class="snippet">print(y_test.shape)</p><div id="_idContainer162" class="IMG---Figure"><img src="image/C13550_05_14.jpg" alt="Figure 5.14: Training and testing set shapes"/></div><h6>Figure 5.14: Training and testing set shapes</h6></li>
				<li>We import the corresponding data to build, compile, and train the model:<p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.callbacks import EarlyStopping, ModelCheckpoint</p><p class="snippet">from keras.layers import Input, Dense, Dropout, Flatten</p><p class="snippet">from keras.layers import Conv2D, Activation, BatchNormalization</p></li>
				<li>Let's build the model:<p class="snippet">def CNN(input_shape):</p><p class="snippet">    model = Sequential()</p><p class="snippet">    </p><p class="snippet">    model.add(Conv2D(16, kernel_size=(5, 5), padding='same', strides=(2,2), input_shape=input_shape))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Conv2D(16, kernel_size=(3, 3), padding='same', strides=(2,2)))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">//{…}##the detailed code can be found on Github##</p><p class="snippet">    </p><p class="snippet">    model.add(Conv2D(128, kernel_size=(2, 2), padding='same', strides=(2,2)))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    </p><p class="snippet">    model.add(Flatten())</p><p class="snippet">    model.add(Dense(512))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Dropout(0.5))</p><p class="snippet"> </p><p class="snippet">    model.add(Dense(2, activation="softmax"))</p><p class="snippet"> </p><p class="snippet">    return model</p><p>The model goes from 16 filters in the very first layer to 128 filters at the end, doubling the size in every 2 layers.</p><p>Because this problem is harder (we have bigger images with 3 channels and 128x128 images), we have made the model deeper, adding another couple of layers with 16 filters at the beginning (the first layer having a kernel size of 5x5, which is better in the very first stages) and another couple of layers with 128 filters at the end of the model.</p></li>
				<li>Now, let's compile the model:<p class="snippet">model = CNN(input_shape)</p><p class="snippet">model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])</p><p class="snippet">ckpt = ModelCheckpoint('Models/model_dogs-cats.h5', save_best_only=True,monitor='val_loss', mode='min', save_weights_only=False) </p><p class="snippet">earlyStopping = EarlyStopping(monitor='val_loss', patience=15, verbose=0,mode='min')</p><p>We have set the patience to 15 epochs for the EarlyStopping callback because it takes more epochs for the model to converge to the sweet spot, and the validation loss can vary a lot until then.</p></li>
				<li>Then, we train the model:<p class="snippet">model.fit(x_train, y_train,</p><p class="snippet">          batch_size=8,</p><p class="snippet">          epochs=100,</p><p class="snippet">          verbose=1, </p><p class="snippet">          validation_data=(x_test, y_test),</p><p class="snippet">          callbacks=[ckpt,earlyStopping]) </p><p>The batch size is also low as we do not have much data, but it could be increased to 16 easily.</p></li>
				<li>Then, evaluate the model:<p class="snippet">from sklearn import metrics</p><p class="snippet">model.load_weights('Models/model_dogs-cats.h5')</p><p class="snippet">y_pred = model.predict(x_test, batch_size=8, verbose=0)</p><p class="snippet">y_pred = np.argmax(y_pred, axis=1)</p><p class="snippet">y_test_aux = y_test.copy()</p><p class="snippet">y_test_pred = list()</p><p class="snippet">for i in y_test_aux:</p><p class="snippet">    y_test_pred.append(np.argmax(i))</p><p class="snippet"> </p><p class="snippet">print (y_pred)</p><p class="snippet"> </p><p class="snippet"># Evaluate the prediction</p><p class="snippet">accuracy = metrics.accuracy_score(y_test_pred, y_pred)</p><p class="snippet">precision, recall, f1, support = metrics.precision_recall_fscore_support(y_test_pred, y_pred, average=None)</p><p class="snippet">print('\nFinal results...')</p><p class="snippet">print(metrics.classification_report(y_test_pred, y_pred))</p><p class="snippet">print('Acc      : %.4f' % accuracy)</p><p class="snippet">print('Precision: %.4f' % np.average(precision))</p><p class="snippet">print('Recall   : %.4f' % np.average(recall))</p><p class="snippet">print('F1       : %.4f' % np.average(f1))</p><p class="snippet">print('Support  :', np.sum(support))</p><p>You should see the following output:</p><div id="_idContainer163" class="IMG---Figure"><img src="image/C13550_05_15.jpg" alt=""/></div><h6>Figure 5.15: Output showing the accuracy of the model</h6><p>As you can see from the preceding figure, the accuracy achieved in this dataset with this model is <strong class="bold">67.25%</strong>.</p></li>
				<li>We are going to apply data augmentation to this process. We have to import ImageDataGenerator from Keras and declare it with transformations that we are going to make:<p class="snippet">from keras.preprocessing.image import ImageDataGenerator</p><p class="snippet">datagen = ImageDataGenerator(</p><p class="snippet">        rotation_range=15,</p><p class="snippet">        width_shift_range=0.2,</p><p class="snippet">        height_shift_range=0.2,</p><p class="snippet">        horizontal_flip=True,</p><p class="snippet">        zoom_range=0.3</p><p class="snippet">        )</p><p>The following transformations have been applied:</p><p>We have set a rotation range of 15 degrees because dogs and cats within images can be positioned in slightly different ways (feel free to tweak this parameter).</p><p>We have set the width shift range and height shift range to 0.2 to shift the image horizontally and vertically, as an animal could be anywhere within the image (also tweakable).</p><p>We have set the horizontal flip property to <strong class="inline">True</strong> because these animals can be flipped in the dataset (horizontally; with vertical flipping, it is much more difficult to find an animal).</p><p>Finally, we set zoom range to 0.3 to make random zooms on the images as the dogs and cats may be farther in the image or closer.</p></li>
				<li>We fit the <strong class="inline">datagen</strong> instance declared with the training data in order to compute quantities for feature-wise normalization and declare and compile the model again to make sure we are not using the previous one:<p class="snippet">datagen.fit(x_train)</p><p class="snippet"> </p><p class="snippet">model = CNN(input_shape)</p><p class="snippet"> </p><p class="snippet">model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])</p><p class="snippet">ckpt = ModelCheckpoint('Models/model_dogs-cats.h5', save_best_only=True,monitor='val_loss', mode='min', save_weights_only=False)</p></li>
				<li>Finally, we train the model with the <strong class="inline">fit_generator</strong> method of the model and the <strong class="inline">flow()</strong> method of the <strong class="inline">datagen</strong> instance generated:<p class="snippet">model.fit_generator(</p><p class="snippet">          datagen.flow(x_train, y_train, batch_size=8),</p><p class="snippet">          epochs=100,</p><p class="snippet">          verbose=1, </p><p class="snippet">          validation_data=(x_test, y_test),</p><p class="snippet">          callbacks=[ckpt,earlyStopping],</p><p class="snippet">          steps_per_epoch=len(x_train) // 8,</p><p class="snippet">          workers=4) </p><p>We set the <strong class="inline">steps_per_epoch</strong> parameter equal to the length of the training set divided by the batch size (8).</p><p>We also set the number of workers to 4 to take advantage of the 4 cores of the processor:</p><p class="snippet">from sklearn import metrics</p><p class="snippet"># Make a prediction</p><p class="snippet">print ("Making predictions...")</p><p class="snippet">model.load_weights('Models/model_dogs-cats.h5')</p><p class="snippet">#y_pred = model.predict(x_test)</p><p class="snippet">y_pred = model.predict(x_test, batch_size=8, verbose=0)</p><p class="snippet">y_pred = np.argmax(y_pred, axis=1)</p><p class="snippet">y_test_aux = y_test.copy()</p><p class="snippet">y_test_pred = list()</p><p class="snippet">for i in y_test_aux:</p><p class="snippet">    y_test_pred.append(np.argmax(i))</p><p class="snippet">print (y_pred)</p><p class="snippet"># Evaluate the prediction</p><p class="snippet">accuracy = metrics.accuracy_score(y_test_pred, y_pred)</p><p class="snippet">precision, recall, f1, support = metrics.precision_recall_fscore_support(y_test_pred, y_pred, average=None)</p><p class="snippet">print('\nFinal results...')</p><p class="snippet">print(metrics.classification_report(y_test_pred, y_pred))</p><p class="snippet">print('Acc      : %.4f' % accuracy)</p><p class="snippet">print('Precision: %.4f' % np.average(precision))</p><p class="snippet">print('Recall   : %.4f' % np.average(recall))</p><p class="snippet">print('F1       : %.4f' % np.average(f1))</p><p class="snippet">print('Support  :', np.sum(support))</p><p>You should see the following output:</p><div id="_idContainer164" class="IMG---Figure"><img src="image/C13550_05_16.jpg" alt="Figure 5.16: Output showing the accuracy of the model"/></div><h6>Figure 5.16: Output showing the accuracy of the model</h6><p>As you can see from the preceding figure, with data augmentation, we achieve an accuracy of <strong class="bold">81%</strong>, which is far better.</p></li>
				<li>If we want to load the model that we just trained (dogs versus cats), the following code achieves that:<p class="snippet">from keras.models import load_model</p><p class="snippet">model = load_model('Models/model_dogs-cats.h5')</p></li>
				<li>Let's try the model with unseen data. The data can be found in the <strong class="inline">Dataset/testing</strong> folder and the code from <em class="italics">Exercise 17</em>, <em class="italics">Building a CNN</em> will be used (but with different names for the samples):<p class="snippet">images = ['dog1.jpg', 'dog2.jpg', 'cat1.jpg', 'cat2.jpg'] </p><p class="snippet"> </p><p class="snippet">for number in range(len(images)):</p><p class="snippet">    imgLoaded = cv2.imread('testing/%s'%(images[number])) </p><p class="snippet">    img = cv2.resize(imgLoaded, (rows, cols)) </p><p class="snippet">    img = (img.astype(np.float32))/255.0 </p><p class="snippet">    img = img.reshape(1, rows, cols, 3) </p><p class="snippet">    </p><p>In these lines of code, we are loading an image, resizing it to the expected size (128 x 128), normalizing the image – as we did with the training set – and reshaping it to (1, 128, 128, 3) to fit as input in the neural network.</p><p>We continue the for loop:</p><p class="snippet">  plt.subplot(1,5,number+1),plt.imshow(cv2.cvtColor(imgLoad ed,cv2.COLOR_BGR2RGB))</p><p class="snippet">    plt.title(np.argmax(model.predict(img)[0])) </p><p class="snippet">    plt.xticks([]),plt.yticks([]) </p><p class="snippet">fig = plt.gcf()</p><p class="snippet">plt.show()</p></li>
			</ol>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/C13550_05_17.jpg" alt=""/>
				</div>
			</div>
			<h6>Figure 5.17: Prediction of the Oxford Pet dataset with unseen data using CNNs and data augmentation</h6>
			<p>We can see that the model has made all the predictions well. Note that not all the breeds are stored in the dataset, so not all the cats and dogs will be predicted properly. Adding more types of breeds would be necessary in order to achieve that.</p>
			<h3 id="_idParaDest-102"><a id="_idTextAnchor137"/>Activity 5: Making Use of Data Augmentation to Classify correctly Images of Flowers</h3>
			<p>In this activity, you are going to put into practice what you have learned. We are going to use a different dataset, where the images are bigger (150x150). There are 5 classes in this dataset: daisy, dandelion, rose, sunflower, and tulip. There are, in total, 4,323 images, which is fewer when compared to the previous exercises we performed. The classes do not have the same number of images either, but do not worry about that. The images are RGB, so there will be three channels. We have stored them in NumPy arrays of each class, so we will provide a way to load them properly.</p>
			<p>The following steps will guide you through this:</p>
			<ol>
				<li value="1">Load the dataset by using this code, as the data is stored in NumPy format:<p class="snippet">import numpy as np</p><p class="snippet">classes = ['daisy','dandelion','rose','sunflower','tulip']</p><p class="snippet">X = np.load("Dataset/flowers/%s_x.npy"%(classes[0]))</p><p class="snippet">y = np.load("Dataset/flowers/%s_y.npy"%(classes[0]))</p><p class="snippet">print(X.shape)</p><p class="snippet">for flower in classes[1:]:</p><p class="snippet">    X_aux = np.load("Dataset/flowers/%s_x.npy"%(flower))</p><p class="snippet">    y_aux = np.load("Dataset/flowers/%s_y.npy"%(flower))</p><p class="snippet">    print(X_aux.shape)</p><p class="snippet">    X = np.concatenate((X, X_aux), axis=0)</p><p class="snippet">    y = np.concatenate((y, y_aux), axis=0)</p><p class="snippet">print(X.shape)</p><p class="snippet">print(y.shape)</p></li>
				<li>Show some samples from the dataset by importing <strong class="inline">random</strong> and <strong class="inline">matplotlib</strong>, using a random index to access the <strong class="inline">X</strong> set.<h4>Note</h4><p class="callout">The NumPy arrays were stored in BGR format (OpenCV format), so in order to show the images properly, you will need to use the following code to change the format to RGB (only to show the image): <strong class="inline">image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)</strong>. </p><p class="callout">You will need to import <strong class="inline">cv2</strong>.</p></li>
				<li>Normalize the <strong class="inline">X</strong> set and set the labels to categorical (the <strong class="inline">y</strong> set).</li>
				<li>Split the sets into a training and testing set.</li>
				<li>Build a CNN.<h4>Note</h4><p class="callout">As we have bigger images, you should consider adding more layers, thus reducing the image size, and the first layer should contain a bigger kernel (the kernel should be an odd number when it is bigger than 3).</p></li>
				<li>Declare ImageDataGenerator from Keras with the changes that you think will suit the variance of the dataset.</li>
				<li>Train the model. You can either choose an EarlyStopping policy or set a high number of epochs and wait or stop it whenever you want. If you declare the Checkpoint callback, it will always save only the best validation loss model (if that is the metric you are using).</li>
				<li>Evaluate the model using this code:<p class="snippet">from sklearn import metrics</p><p class="snippet">y_pred = model.predict(x_test, batch_size=batch_size, verbose=0)</p><p class="snippet">y_pred = np.argmax(y_pred, axis=1)</p><p class="snippet">y_test_aux = y_test.copy()</p><p class="snippet">y_test_pred = list()</p><p class="snippet">for i in y_test_aux:</p><p class="snippet">    y_test_pred.append(np.argmax(i))</p><p class="snippet">accuracy = metrics.accuracy_score(y_test_pred, y_pred)</p><p class="snippet">print(accuracy)</p><h4>Note</h4><p class="callout">This will print the accuracy of the model. Note that batch_size is the batch size you have set for your training sets and for <strong class="inline">x_test</strong> and <strong class="inline">y_test</strong>, which are your testing sets.</p><p class="callout">You can use this code in order to evaluate any model, but first you need to load the model. If you want to load the entire model from a <strong class="inline">.h5</strong> file, you will have to use this code:</p><p class="callout"><strong class="inline">from keras.models import load_model</strong> <strong class="inline">model = load_model('model.h5')</strong></p></li>
				<li>Try the model with unseen data. In the <strong class="inline">Dataset/testing/</strong> folder, you will find five images of flowers that you can load to try it out. Remember that the classes are in this order: <p>classes=['daisy','dandelion','rose','sunflower','tulip']</p><p>So, the result should look like this:</p></li>
			</ol>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/C13550_05_18.jpg" alt="Figure 5.18: Prediction of roses using CNNs"/>
				</div>
			</div>
			<h6>Figure 5.18: Prediction of roses using CNNs</h6>
			<h4>Note</h4>
			<p class="callout">The solution for this activity can be found on page 313.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor138"/>State-of-the-Art Models - Transfer Learning</h2>
			<p>Humans do not learn each and every task that they want to achieve from scratch; they usually take previous knowledge as a base in order to learn tasks much faster.</p>
			<p>When training neural networks, there are some tasks that are extremely expensive to train for every individual, such as having hundreds of thousands of images for training and having to distinguish between two or more similar objects, ending up having a cost of days to achieve good performance, for example. These neural networks are trained to achieve this expensive task, and because neural networks are capable of saving that knowledge, then other models can take advantage of those weights to retrain specific models for similar tasks.</p>
			<p><strong class="keyword">Transfer learning</strong> does just that – it transfers the knowledge of a pretrained model to your model, so you can take advantage of that knowledge.</p>
			<p>So, for example, if you want to make a classifier that is capable of identifying five objects but that task seems too expensive to train (it takes knowledge and time), you can take advantage of a pretrained model (usually trained on the famous <strong class="bold">ImageNet</strong> dataset) and retrain the model adapted to your problem. The ImageNet dataset is a large visual database designed for use in visual object recognition research and has more than 14 million images with more than 20,000 categories, which is very expensive for an individual to train.</p>
			<p>Technically, you load the model with the weights of the dataset where it was trained, and if you want to achieve a different problem, you only have to change the last layer of the model. If the model is trained on ImageNet, it could have, 1000 classes but you only have 5 classes, so you would change the last layer to a dense layer with only 5 neurons. You could add more layers before the last one, though.</p>
			<p>The layers of the model that you have imported (the base model) can be frozen so their weights do not reflect on the training time. Depending on this, there are two types of transfer learning:</p>
			<ul>
				<li><strong class="bold">Traditional</strong>: Freeze all the layers of the base model</li>
				<li><strong class="bold">Fine-tuning</strong>: Freeze only a part of the base model, typically the first layers</li>
			</ul>
			<p>In Keras, we can import famous pretrained models such as Resnet50 and VGG16. You can import a pretrained model with or without weights (in Keras, there are only weights for ImageNet), which includes the top of the model or not. The input shape can only be only specified if the top is not included and with a minimum size of 32.</p>
			<p>With the following lines of code, you would import the Resnet50 model without the top, with the <strong class="inline">imagenet</strong> weights and with a shape of 150x150x3:</p>
			<p class="snippet">from keras.applications import resnet50</p>
			<p class="snippet">model = resnet50.ResNet50(include_top=False, weights='imagenet', input_shape=(150,150,3))</p>
			<p>If you have included the top of the model because you want to use the last dense layers of the model (let's say your problem is similar to ImageNet but with different classes), then you should write this code:</p>
			<p class="snippet">from keras.models import Model</p>
			<p class="snippet">from keras.layers import Dense</p>
			<p class="snippet"> </p>
			<p class="snippet">model.layers.pop()</p>
			<p class="snippet">model.outputs = [model.layers[-1].output]</p>
			<p class="snippet">model.layers[-1].outbound_nodes = []</p>
			<p class="snippet"> </p>
			<p class="snippet">x=Dense(5, activation='softmax')(model.output)</p>
			<p class="snippet">model=Model(model.input,x)</p>
			<p>This code gets rid of the classification layer (the last dense layer) and prepares the model so that you can add your own last layer. Of course, you could add more layers at the end before adding your classification layer.</p>
			<p>If you have not added the top of the model, then you should add your own top with this code:</p>
			<p class="snippet">from keras.models import Model</p>
			<p class="snippet">from keras.layers import Dense, GlobalAveragePooling2D</p>
			<p class="snippet">x=base_model.output</p>
			<p class="snippet">x=GlobalAveragePooling2D()(x)</p>
			<p class="snippet">x=Dense(512,activation='relu')(x) #dense layer 2</p>
			<p class="snippet">x=Dropout(0.3)(x)</p>
			<p class="snippet">x=Dense(512,activation='relu')(x) #dense layer 3</p>
			<p class="snippet">x=Dropout(0.3)(x)</p>
			<p class="snippet">preds=Dense(5,activation='softmax')(x) #final layer with softmax activation</p>
			<p class="snippet">model=Model(inputs=base_model.input,outputs=preds)</p>
			<p>Here, <strong class="inline">GlobalAveragePooling2D</strong> is like a type of max pooling.</p>
			<p>With these kinds of models, you should preprocess the data just as you did with the data that trained those models (if you are using the weights). Keras has a <strong class="inline">preprocess_input</strong> method that does that for every model. For example, for ResNet50, it would be like this:</p>
			<p class="snippet">from keras.applications.resnet50 import preprocess_input</p>
			<p>You pass your array of images to that function and then you will have your data ready for training. </p>
			<p>The <strong class="bold">learning rate</strong> in a model is how fast it should convert the model to a local minimum. Usually, you do not have to worry about this but if you are retraining a neural network, this is a parameter that you have to tweak. When you are retraining a neural network, you should decrease the value of this parameter so that the neural network does not unlearn what it has already learned. This parameter is tweaked when declaring the optimizer. You can avoid tweaking this parameter, although the model may end up not ever converging or overfitting.</p>
			<p>With this kind of approach, you could train your network with very little data and get good results overall, because you take advantage of the weights of the model.</p>
			<p>You can combine transfer learning with data augmentation as well.</p>
			<h3 id="_idParaDest-104"><a id="_idTextAnchor139"/>Exercise 19: Classifying €5 and €20 Bills Using Transfer Learning with Very Little Data</h3>
			<p>This problem is about differentiating €5 bills from €20 bills with very little data. We have 30 images for every class, which is much less than we have had in previous exercises. We are going to load the data, declare the pretrained model, then declare the changes on the data with data augmentation and train the model. After that, we will check how well the model performs with unseen data:</p>
			<ol>
				<li value="1">Open up your Google Colab interface.<h4>Note</h4><p class="callout">You would need to mount the <strong class="inline">Dataset</strong> folder on your drive and set the path accordingly.</p></li>
				<li>Declare functions to load the data:<p class="snippet">import re, os, cv2</p><p class="snippet">import numpy as np</p><p class="snippet">def list_files(directory, ext=None):</p><p class="snippet">//{…}</p><p class="snippet">##the detailed code can be found on Github##</p><p class="snippet"> </p><p class="snippet">def load_images(path,label):</p><p class="snippet">//{…}</p><p class="snippet">##the detailed code can be found on Github##</p><p class="snippet">    for fname in list_files( path, ext='jpg' ): </p><p class="snippet">        img = cv2.imread(fname)</p><p class="snippet">        img = cv2.resize(img, (224, 224))</p><p class="snippet">//{…}</p><p class="snippet">##the detailed<a id="_idTextAnchor140"/> code can be found on Github##</p><p>Note that the data is resized to 224x224.</p></li>
				<li>The data is stored in <strong class="inline">Dataset/money/</strong>, where you have both classes in subfolders. In order to load the data, you have to write the following code:<p class="snippet">X, y = load_images('Dataset/money/20',0)</p><p class="snippet">X_aux, y_aux = load_images('Dataset/money/5',1)</p><p class="snippet">X = np.concatenate((X, X_aux), axis=0)</p><p class="snippet">y = np.concatenate((y, y_aux), axis=0)</p><p class="snippet">print(X.shape)</p><p class="snippet">print(y.shape)</p><p>The label for the €20 bill is 0 and it's 1 for the €5 bill.</p></li>
				<li>Let's show the data:<p class="snippet">import random </p><p class="snippet">random.seed(42) </p><p class="snippet">from matplotlib import pyplot as plt</p><p class="snippet"> </p><p class="snippet">for idx in range(5): </p><p class="snippet">    rnd_index = random.randint(0, 59)</p><p class="snippet">    plt.subplot(1,5,idx+1),plt.imshow(cv2.cvtColor(X[rnd_index],cv2.COLOR_BGR2RGB)) </p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.savefig("money_samples.jpg", bbox_inches='tight')</p><p class="snippet">plt.show() </p><div id="_idContainer167" class="IMG---Figure"><img src="image/C13550_05_19.jpg" alt="Figure 5.21: Samples of bills"/></div><h6>Figure 5.19: Samples of bills</h6></li>
				<li>Now we are going to declare the pretrained model:<p class="snippet">from keras.applications.mobilenet import MobileNet, preprocess_input</p><p class="snippet">from keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout</p><p class="snippet">from keras.models import Model</p><p class="snippet"> </p><p class="snippet">input_tensor = Input(shape=(224, 224, 3))</p><p class="snippet"> </p><p class="snippet">base_model = MobileNet(input_tensor=input_tensor,weights='imagenet',include_top=False)</p><p class="snippet"> </p><p class="snippet">x = base_model.output</p><p class="snippet">x = GlobalAveragePooling2D()(x)</p><p class="snippet">x = Dense(512,activation='relu')(x)</p><p class="snippet">x = Dropout(0.5)(x)</p><p class="snippet">x = Dense(2, activation='softmax')(x)</p><p class="snippet"> </p><p class="snippet">model = Model(base_model.input, x)</p><p>In this case, we are loading the MobileNet model with the weights of imagenet. We are not including the top so we should build our own top. The input shape is 224x224x3.</p><p>We have built the top of the model by taking the output of the last layer of MobileNet (which is not the classification layer) and start building on top of that. We have added <strong class="inline">GlobalAveragePooling2D</strong> for image reduction, a dense layer that we can train for our specific problem, a <strong class="inline">Dropout</strong> layer to avoid overfitting, and the classifier layer at the end.</p><p>The dense layer at the end has two neurons, as we have only two classes, and it has the <strong class="inline">Softmax</strong> activation function. For binary classification, the Sigmoid function can also be used, but it changes the entire process as you should not make the labels categorical and the predictions look different.</p><p>Afterward, we create the model that we are going to train with the input of MobileNet as input and the classification dense layer as output.</p></li>
				<li>We are going to do fine-tuning. In order to do that, we have to freeze some of the input layers and keep the rest of the trainable data, unchanged:<p class="snippet">for layer in model.layers[:20]:</p><p class="snippet">    layer.trainable=False</p><p class="snippet">for layer in model.layers[20:]:</p><p class="snippet">    layer.trainable=True</p></li>
				<li>Let's compile the model with the <strong class="inline">Adadelta</strong> optimizer:<p class="snippet">import keras</p><p class="snippet">model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])</p></li>
				<li>Now we will use the <strong class="inline">preprocess_input</strong> method that we imported previously to preprocess the <strong class="inline">X</strong> set for MobileNet, and then we convert label <strong class="inline">y</strong> to one-hot encoding:<p class="snippet">from keras import utils as np_utils</p><p class="snippet">X = preprocess_input(X)</p><p class="snippet">#X = (X.astype(np.float32))/255.0 </p><p class="snippet">y = np_utils.to_categorical(y)</p></li>
				<li>We use the <strong class="inline">train_test_split</strong> method to split the data into a training set and testing set:<p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet">x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</p><p class="snippet">print(x_train.shape)</p><p class="snippet">print(y_train.shape)</p><p class="snippet">print(x_test.shape)</p><p class="snippet">print(y_test.shape)</p></li>
				<li>We are going to apply data augmentation to our dataset:<p class="snippet">from keras.preprocessing.image import ImageDataGenerator</p><p class="snippet">train_datagen = ImageDataGenerator(  </p><p class="snippet">      rotation_range=90,     </p><p class="snippet">      width_shift_range = 0.2,</p><p class="snippet">      height_shift_range = 0.2,</p><p class="snippet">      horizontal_flip=True,    </p><p class="snippet">      vertical_flip=True,</p><p class="snippet">      zoom_range=0.4)</p><p class="snippet">train_datagen.fit(x_train)</p><p>As a bill can be at different angles, we choose to make a rotation range of 90º. The other parameters seem reasonable for this task.</p></li>
				<li>Let's declare a checkpoint to save the model when the validation loss decreases and train the model:<p class="snippet">from keras.callbacks import ModelCheckpoint</p><p class="snippet">ckpt = ModelCheckpoint('Models/model_money.h5', save_best_only=True, monitor='val_loss', mode='min', save_weights_only=False)</p><p class="snippet">model.fit_generator(train_datagen.flow(x_train, y_train,</p><p class="snippet">                                batch_size=4),</p><p class="snippet">                    epochs=50,</p><p class="snippet">                    validation_data=(x_test, y_test),</p><p class="snippet">                    callbacks=[ckpt],</p><p class="snippet">                    steps_per_epoch=len(x_train) // 4,</p><p class="snippet">                    workers=4)</p><p>We have set the batch size to 4 because we have only a few samples of data and we do not want to pass all the samples to the neural network at once, but in batches. We are not using the EarlyStopping callback because the loss goes up and down due to the lack of data and the use of Adadelta with a high learning rate.</p></li>
				<li>Check the results:<div id="_idContainer168" class="IMG---Figure"><img src="image/C13550_05_20.jpg" alt="Figure 5.22: Showing the desired output"/></div><h6>Figure 5.20: Showing the desired output</h6><p>In the preceding figure, we can see that, in the 7th epoch, we already achieve 100% accuracy with low loss. This is due to the lack of data on the validation set, because with only 12 samples you cannot tell whether the model is performing well against unseen data.</p></li>
				<li>Let's run the code to calculate the accuracy of this model:<p class="snippet">y_pred = model.predict(x_test, batch_size=4, verbose=0)</p><p class="snippet">y_pred = np.argmax(y_pred, axis=1)</p><p class="snippet">y_test_aux = y_test.copy()</p><p class="snippet">y_test_pred = list()</p><p class="snippet">for i in y_test_aux:</p><p class="snippet">    y_test_pred.append(np.argmax(i))</p><p class="snippet"> </p><p class="snippet">accuracy = metrics.accuracy_score(y_test_pred, y_pred)</p><p class="snippet">print('Acc: %.4f' % accuracy)</p><p>The output is as follows:</p><div id="_idContainer169" class="IMG---Figure"><img src="image/C13550_05_21.jpg" alt="Figure 5.23: Accuracy achieved of 100%"/></div><h6>Figure 5.21: Accuracy achieved of 100%</h6></li>
				<li>Let's try this model with new data. There are test images in the <strong class="inline">Dataset/testing</strong> folder. We have added four examples of bills to check whether the model predicts them well:<h4>Note</h4><p class="callout">Remember that we set the label of €20 to 0 and 1 for €5.</p><p class="snippet">images = ['20.jpg','20_1.jpg','5.jpg','5_1.jpg']</p><p class="snippet">model.load_weights('Models/model_money.h5')</p><p class="snippet">for number in range(len(images)):</p><p class="snippet">    imgLoaded = cv2.imread('Dataset/testing/%s'%(images[number])) </p><p class="snippet">    img = cv2.resize(imgLoaded, (224, 224)) </p><p class="snippet">    #cv2.imwrite('test.jpg',img) </p><p class="snippet">    img = (img.astype(np.float32))/255.0 </p><p class="snippet">    img = img.reshape(1, 224, 224, 3) </p><p class="snippet">    plt.subplot(1,5,number+1),plt.imshow(cv2.cvtColor(imgLoaded,cv2.COLOR_BGR2RGB)) </p><p class="snippet">    plt.title('20' if np.argmax(model.predict(img)[0]) == 0 else '5') </p><p class="snippet">    plt.xticks([]),plt.yticks([]) </p><p class="snippet">plt.show()</p><p>In this code, we have loaded the unseen examples as well, and we have clubbed the output image, which looks like this:</p></li>
			</ol>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="image/C13550_05_22.jpg" alt="Figure 5.24: Prediction of bills"/>
				</div>
			</div>
			<h6>Figure 5.22: Prediction of bills</h6>
			<p>The model has predicted all the images precisely!</p>
			<p>Congratulations! Now you are able to train a model with your own dataset when you have little data, thanks to transfer learning.</p>
			<h4>Note</h4>
			<p class="callout">The complete code for this exercise is uploaded on GitHub: https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise19/Exercise19.ipynb.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor141"/>Summary</h2>
			<p>CNNs have shown much better performance than fully-connected neural networks when dealing with images. In addition, CNNs are also capable of accomplishing good results with text and sound data.</p>
			<p>CNNs have been explained in depth, as have how convolutions work and all the parameters that come along with them. Afterward, all this theory was put into practice with an exercise.</p>
			<p>Data augmentation is a technique for overcoming a lack of data or a lack of variation in a dataset by applying simple transformations to the original data in order to generate new images. This technique has been explained and also put into practice with an exercise and an activity, where you were able to experiment with the knowledge you acquired.</p>
			<p>Transfer learning is a technique used when there is a lack of data or the problem is so complex that it would take too long to train on a normal neural network. Also, this technique does not need much of an understanding of neural networks at all, as the model is already implemented. It can also be used with data augmentation.</p>
			<p>Transfer learning was also covered and put into practice with an exercise where the amount of data was very small.</p>
			<p>Learning how to build CNNs is very useful for recognizing objects or environments in computer vision. When a robot is using its vision sensors to recognize an environment, normally, CNNs are employed and data augmentation is used to improve the CNNs performance. In <em class="italics">Chapter 8</em>, <em class="italics">Object Recognition to Guide the Robot Using CNNs,</em> the CNN concepts you have learned about will be applied to a real-world application, and you will be able to recognize an environment using deep learning.</p>
			<p>Before applying these techniques to recognize the environment, first you need to learn how to manage a robot that will be able to recognize an environment. In  <em class="italics">Chapter 6, Robot Operating System (ROS)</em>, you will learn how to manage a robot using a simulator by taking advantage of software called ROS.</p>
		</div>
	</body></html>