<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Deep Learning Using Multilayer Neural Networks</h1>
                
            
            <article>
                
<p class="calibre2">Deep learning is the recent hot trend in machine learning/AI. It is all about building advanced neural networks. By making multiple hidden layers work in a neural network model, we can work with complex nonlinear representations of data. We create deep learning using base neural networks. Deep learning has numerous use cases in real life, such as, driverless cars, medical diagnostics, computer vision, speech recognition, <strong class="calibre1">Natural Language Processing</strong> (<strong class="calibre1">NLP</strong>), handwriting recognition, language translation, and many other fields.</p>
<p class="calibre2">In this chapter, we will deal with the deep learning process: how to train, test, and deploy a <strong class="calibre1">Deep Neural Network</strong> (<strong class="calibre1">DNN</strong>). We will look at the different packages available in R to handle DNNs. We will understand how to build and train a DNN with the <kbd class="calibre13">neuralnet</kbd> package. Finally, we will analyze an example of training and modeling a DNN using <span><span>h2o</span></span>, the scalable open-memory learning platform, to create models with large datasets and implement prediction with high-precision methods.</p>
<p class="calibre2">The following are the topics covered in this chapter:</p>
<ul class="calibre16">
<li class="calibre17">Types of DNNs</li>
<li class="calibre17">R packages for deep learning</li>
<li class="calibre17">Training and modeling a DNN with <kbd class="calibre13">neuralnet</kbd></li>
<li class="calibre17">The <kbd class="calibre13">h2o</kbd> library</li>
</ul>
<p class="calibre2">By the end of the chapter, we will understand the basic concepts of deep learning and how to implement it in the R environment. We will discover different types of DNNs. We will learn how to train, test, and deploy a model. We will know how to train and model a DNN using <kbd class="calibre13"><span><span>h2o</span></span></kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Introduction of DNNs</h1>
                
            
            <article>
                
<p class="calibre2">With the advent of big data processing infrastructure, GPU, and GP-GPU, we are now able to overcome the challenges with shallow neural networks, namely overfitting and vanishing gradient, using various activation functions and L1/L2 regularization techniques. Deep learning can work on large amounts of labeled and unlabeled data easily and efficiently.</p>
<p class="calibre2">As mentioned, deep learning is a class of machine learning wherein learning happens on multiple levels of neuron networks. The standard diagram depicting a DNN is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border16" src="../images/00068.jpeg"/></div>
<p class="calibre2">From the analysis of the previous figure, we can notice a remarkable analogy with the neural networks we have studied so far. We can then be quiet, unlike what it might look like, deep learning is simply an extension of the neural network. In this regard, most of what we have seen in the previous chapters is valid. In short, a DNN is a multilayer neural network that contains two or more hidden layers. Nothing very complicated here. By adding more layers and more neurons per layer, we increase the specialization of the model to train data but decrease the performance on the test data.</p>
<p class="calibre2">As we anticipated, DNN are derivatives of ANN. By making the number of hidden layers more than one, we build DNNs. There are many variations of DNNs, as illustrated by the different terms shown next:</p>
<ul class="calibre16">
<li class="calibre17"><strong class="calibre1">Deep Belief Network</strong> (<strong class="calibre1">DBN</strong>): It is typically a feed-forward network in which data flows from one layer to another without looping back. There is at least one hidden layer and there can be multiple hidden layers, increasing the complexity.</li>
<li class="calibre17"><strong class="calibre1">Restricted Boltzmann Machine</strong> (<strong class="calibre1">RBM</strong>): It has a single hidden layer and there is no connection between nodes in a group. It is a simple MLP model of neural networks.</li>
<li class="calibre17"><strong class="calibre1">Recurrent Neural Networks</strong> (<strong class="calibre1">RNN</strong>) and <strong class="calibre1">Long Short Term Memory</strong> (<strong class="calibre1">LSTM</strong>): These networks have data flowing in any direction within groups and across groups.</li>
</ul>
<p class="calibre2">As with any machine learning algorithm, even DNNs require building, training, and evaluating processes. A basic workflow for deep learning in shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border17" src="../images/00069.jpeg"/></div>
<p class="calibre2">The workflow we have seen in the previous figure remembers very closely that typical of a supervised learning algorithm. But what makes it different from other machine learning algorithms?</p>
<p class="calibre2">Almost all machine learning algorithms demonstrate their limits in identifying the characteristics of raw input data, especially when they are complex and lacking an apparent order, such as in images. Usually, this limit is exceeded through the help of humans, who are concerned with identifying what the machine can not do. Deep learning removes this step, relying on the training process to find the most useful models through input examples. Also in that case human intervention is necessary in order to make choices before starting training, but automatic discovery of features makes life much easier. What makes the neural networks particularly advantageous, compared to the other solutions offered by machine learning, is the great generalization ability of the model.</p>
<p class="calibre2">These features have made deep learning very effective for almost all tasks that require automatic learning; although it is particularly effective in a case of complex hierarchical data. Its underlying ANN forms highly nonlinear representations; these are usually composed of multiple layers together with nonlinear transformations and custom architectures.</p>
<p class="calibre2">Essentially, deep learning works really well with messy data from the real world, making it a key instrument in several technological fields of the next few years. Until recently, it was a dark and daunting area to know, but its success has brought many great resources and projects that make it easier than ever to start.</p>
<p class="calibre2">Now that we know what the DNNs are, let's see what tools the R development environment offers us to deal with this particular topic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">R for DNNs</h1>
                
            
            <article>
                
<p class="calibre2">In the previous section, we clarified some key concepts that are at the deep learning base. We also understood the features that make the use of deep learning particularly convenient. Moreover, its rapid diffusion is also due to the great availability of a wide range of frameworks and libraries for various programming languages.</p>
<p class="calibre2">The R programming language is widely used by scientists and programmers, thanks to its extreme ease of use. Additionally, there is an extensive collection of libraries that allow professional data visualization and analysis with the most popular algorithms. The rapid diffusion of deep learning algorithms has led to the creation of an ever-increasing number of packages available for deep learning, even in R.</p>
<p class="calibre2">The following table shows the various packages/interfaces available for deep learning using R:</p>
<table class="calibre40">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">CRAN package</strong></p>
</td>
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Supported taxonomy of neural network</strong></p>
</td>
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Underlying language/</strong> <strong class="calibre1">vendor</strong></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">MXNet</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Feed-forward, CNN</p>
</td>
<td class="calibre8">
<p class="calibre2">C/C++/CUDA</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">darch</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">RBM, DBN</p>
</td>
<td class="calibre8">
<p class="calibre2">C/C++</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">deepnet</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Feed-forward, RBM, DBN, autoencoders</p>
</td>
<td class="calibre8">
<p class="calibre2">R</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">h2o</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Feed-forward network, autoencoders</p>
</td>
<td class="calibre8">
<p class="calibre2">Java</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">nnet</kbd> and <kbd class="calibre13">neuralnet</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Feed-forward</p>
</td>
<td class="calibre8">
<p class="calibre2">R</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Keras</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><span>Variety of DNNs</span></p>
</td>
<td class="calibre8">
<p class="calibre2">Python/keras.io</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">TensorFlow</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Variety of DNNs</p>
</td>
<td class="calibre8">
<p class="calibre2">C++, Python/Google</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2"><kbd class="calibre13">MXNet</kbd> is a modern, portable, deep learning library that can support multiple machines. The world's largest companies and universities have adopted <kbd class="calibre13">MXNet</kbd> as a machine learning framework. These include Amazon, Intel, Data, Baidu, Microsoft, Wolfram Research, Carnegie Mellon, MIT, University of Washington, and Hong Kong University of Science and Technology.<br class="title-page-name"/>
<kbd class="calibre13">MXNet</kbd> is an open source framework that allows for fast modeling, and supports a flexible programming model in multiple programming languages (C ++, Python, Julia, MATLAB, JavaScript, Go, R, Scala, Perl, and Wolfram Language).<br class="title-page-name"/>
The <kbd class="calibre13">MXNet</kbd> framework supports R programming language. The <kbd class="calibre13">MXNet</kbd> R package provides flexible and efficient GPU computing and a state-of-the-art deepening at R. It allows us to write a seamless tensorial/ matrix calculation with multiple GPUs in R. It also allows us to build and customize the state-of-the-art deep learning models in R and apply them to activities such as image classification and data science challenges.</p>
<p class="calibre2">The <kbd class="calibre13">darch</kbd> framework is based on the code written by G. E. Hinton and R. R. Salakhutdinov, and is available in the MATLAB environment for DBN. This package can generate neural networks with many levels (deep architectures) and form them with an innovative method developed by the authors. This method provides a pre-formation with the contrasting divergence method published by G. Hinton (2002) and fine-tuning with common training algorithms known as backpropagation or conjugated gradients. In addition, fine-tuning supervision can be improved with maxout and dropout, two recently developed techniques to improve fine-tuning for deep learning.</p>
<p class="calibre2">The <kbd class="calibre13">deepnet</kbd> library is a relatively small, yet quite powerful package with variety of architectures to pick from. This library implements some deep learning architectures and neural network algorithms, including backpropagation, RBM, DBN, deep autoencoder, and so on. Unlike the other libraries we have analyzed, it was specifically written for R. It has several functions, including:</p>
<ul class="calibre16">
<li class="calibre17"><kbd class="calibre13">nn.train</kbd>: For training single or multiple hidden layers neural network by BP</li>
<li class="calibre17"><kbd class="calibre13">nn.predict</kbd>: For predicting new samples by trained neural network</li>
<li class="calibre17"><kbd class="calibre13">dbn.dnn.train</kbd>: For training a DNN with weights initialized by DBN</li>
<li class="calibre17"><kbd class="calibre13">rbm.train</kbd>: For training an RBM</li>
</ul>
<p class="calibre2">The <kbd class="calibre13"><span><span>h2o</span></span></kbd> R package has functions for building general linear regression, K-means, Naive Bayes, <strong class="calibre1">Principal Component Analysis</strong> (<strong class="calibre1">PCA</strong>), forests, and deep learning (multilayer <kbd class="calibre13">neuralnet</kbd> models). <kbd class="calibre13"><span><span>h2o</span></span></kbd> is an external package to CRAN and is built using Java, and is available for a variety of platforms. It is an open source math engine for big data that computes parallel distributed machine learning algorithms.</p>
<p class="calibre2">The packages <kbd class="calibre13">nnet</kbd> and <kbd class="calibre13">neuralnet</kbd> have been widely discussed in the previous chapters. These are two packages for the management of neural networks in R. They are also able to build and train multicore neural networks, so they rely on deep learning.</p>
<p class="calibre2"><kbd class="calibre13">Keras</kbd> is an open source neural network library written in Python. Designed to enable fast experimentation with DNNs, it focuses on being minimal, modular, and extensible. The library contains numerous implementations of commonly used neural network building blocks, such as layers, objectives, activation functions, optimizers, and a host of tools to make working with image and text data easier. The code is hosted on GitHub, and community support forums include the GitHub issues page, a Gitter channel, and a Slack channel.</p>
<p class="calibre2"><kbd class="calibre13">TensorFlow</kbd> is an open source software library for machine learning. It contains a system for building and training neural networks to detect and decipher patterns and correlations, with methods similar to those adopted by human learning. It is used both for search and for Google production.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Multilayer neural networks with neuralnet</h1>
                
            
            <article>
                
<p class="calibre2">After understanding the basics of deep learning, it's time to apply the skills acquired to a practical case. We've seen in the previous section that two libraries we know are listed in packages available in <em class="calibre14">R for DNNs</em> section. I refer to the <kbd class="calibre13">nnet</kbd> and <kbd class="calibre13">neuralnet</kbd> packages that we learned to use in the previous chapters through practical examples. Since we have some practice with the <kbd class="calibre13">neuralnet</kbd> library, I think we should start our practical exploration of the amazing world of deep learning <span>from here.</span></p>
<p class="calibre2">To start, we introduce the dataset we will use to build and train the network. It is named the <kbd class="calibre13">College</kbd> dataset, and it contains statistics for a large number of US colleges, collected from the 1995 issue of <em class="calibre14">US News and World Report</em>. This dataset was taken from the <kbd class="calibre13">StatLib</kbd> library, which is maintained at Carnegie Mellon University, and was used in the <em class="calibre14">ASA Section on Statistical Graphics</em>.</p>
<p class="calibre2">Things for us are further simplified because we do not have to retrieve the data and then import it into R, as these data are contained in a R package. I refer to the <kbd class="calibre13">ISLR</kbd> package. We just have to install the package and load the relative library. But we will see this later, when we explain the codices in detail. Now let's just look at the content of the dataset <kbd class="calibre13">College</kbd>. It is a dataframe with <kbd class="calibre13">777</kbd> observations on the following <kbd class="calibre13">18</kbd> variables:</p>
<ul class="calibre16">
<li class="calibre17"><kbd class="calibre13">Private</kbd>: A factor with levels <kbd class="calibre13">No</kbd> and <kbd class="calibre13">Yes</kbd> indicating private or public university</li>
<li class="calibre17"><kbd class="calibre13">Apps</kbd>: Number of applications received</li>
<li class="calibre17"><kbd class="calibre13">Accept</kbd>: Number of applications accepted</li>
<li class="calibre17"><kbd class="calibre13">Enroll</kbd>: Number of new students enrolled</li>
<li class="calibre17"><kbd class="calibre13">Top10perc</kbd>: Percentage of new students from top 10 percent of H.S. class</li>
<li class="calibre17"><kbd class="calibre13">Top25perc</kbd>: <span>Percentage of</span> new students from top 25 percent of H.S. class</li>
<li class="calibre17"><kbd class="calibre13">F.Undergrad</kbd>: Number of full time undergraduates</li>
<li class="calibre17"><kbd class="calibre13">P.Undergrad</kbd>: Number of part time undergraduates</li>
<li class="calibre17"><kbd class="calibre13">Outstate</kbd>: Out-of-state tuition</li>
<li class="calibre17"><kbd class="calibre13">Room.Board</kbd>: Room and board costs</li>
<li class="calibre17"><kbd class="calibre13">Books</kbd>: Estimated book costs</li>
<li class="calibre17"><kbd class="calibre13">Personal</kbd>: Estimated personal spending</li>
<li class="calibre17"><kbd class="calibre13">PhD</kbd>: <span>Percentage of</span> faculty with Ph.D.s</li>
<li class="calibre17"><kbd class="calibre13">Terminal</kbd>: <span>Percentage of</span> faculty with terminal degree</li>
<li class="calibre17"><kbd class="calibre13">S.F.Ratio</kbd>: Student-faculty ratio</li>
<li class="calibre17"><kbd class="calibre13">perc.alumni</kbd>: <span>Percentage of</span> alumni who donate</li>
<li class="calibre17"><kbd class="calibre13">Expend</kbd>: Instructional expenditure per student</li>
<li class="calibre17"><kbd class="calibre13">Grad.Rate</kbd>: Graduation rate</li>
</ul>
<p class="calibre2">Our aim will be to build a multilayer neural network capable of predicting whether the school is public or private, based on the values assumed by the other <kbd class="calibre13">17</kbd> variables:</p>
<pre class="calibre24"><strong class="calibre1">###########################################################################</strong><br class="title-page-name"/><strong class="calibre1">#############Chapter 3 - Deep Learning with neuralnet###################### ###########################################################################</strong><br class="title-page-name"/><strong class="calibre1">library("neuralnet")</strong><br class="title-page-name"/><strong class="calibre1">library(ISLR)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">data = College</strong><br class="title-page-name"/><strong class="calibre1">View(data)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">max_data &lt;- apply(data[,2:18], 2, max) </strong><br class="title-page-name"/><strong class="calibre1">min_data &lt;- apply(data[,2:18], 2, min)</strong><br class="title-page-name"/><strong class="calibre1">data_scaled &lt;- scale(data[,2:18],center = min_data, scale = max_data - min_data) </strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">Private = as.numeric(College$Private)-1</strong><br class="title-page-name"/><strong class="calibre1">data_scaled = cbind(Private,data_scaled)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">index = sample(1:nrow(data),round(0.70*nrow(data)))</strong><br class="title-page-name"/><strong class="calibre1">train_data &lt;- as.data.frame(data_scaled[index,])</strong><br class="title-page-name"/><strong class="calibre1">test_data &lt;- as.data.frame(data_scaled[-index,])</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">n = names(train_data)</strong><br class="title-page-name"/><strong class="calibre1">f &lt;- as.formula(paste("Private ~", paste(n[!n %in% "Private"], collapse = " + ")))</strong><br class="title-page-name"/><strong class="calibre1">deep_net = neuralnet(f,data=train_data,hidden=c(5,3),linear.output=F)</strong><br class="title-page-name"/><strong class="calibre1">plot(deep_net)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">predicted_data &lt;- compute(deep_net,test_data[,2:18])</strong><br class="title-page-name"/><strong class="calibre1">print(head(predicted_data$net.result))</strong><br class="title-page-name"/><strong class="calibre1">predicted_data$net.result &lt;- sapply(predicted_data$net.result,round,digits=0)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">table(test_data$Private,predicted_data$net.result)</strong></pre>
<p class="calibre2">As usual, we will analyze the code line-by-line, by explaining in detail all the features applied to capture the results.</p>
<pre class="calibre24"><strong class="calibre1">library("neuralnet")</strong><br class="title-page-name"/><strong class="calibre1">library(ISLR)</strong></pre>
<p class="calibre2">As usual, the first two lines of the initial code are used to load the libraries needed to run the analysis.</p>
<div class="packt_tip">Remember, to install a library that is not present in the initial distribution of R, you must use the <kbd class="calibre61">install.package</kbd> function. <span class="calibre62">This is the main function to install packages. It takes a vector of names and a destination library, downloads the packages from the repositories and installs them.</span> This function should be used only once and not every time you run the code.</div>
<pre class="calibre24"><strong class="calibre1">data = College</strong><br class="title-page-name"/><strong class="calibre1">View(data)</strong></pre>
<p class="calibre2"><span>This command loads the <kbd class="calibre13">College</kbd> dataset, which as we anticipated is contained in the <kbd class="calibre13">ISLR</kbd> library, and saves it in a given dataframe. Use the <kbd class="calibre13">View</kbd> function to view a compact display of the structure of an arbitrary R object. The following figure shows some of the data contained in the <kbd class="calibre13">College</kbd> dataset:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00070.jpeg"/></div>
<p class="calibre2">How it is possible to note for each college are listed a series of statistics; the rows represent the observations on the columns instead are present the detected features:</p>
<pre class="calibre24"><strong class="calibre1">max_data &lt;- apply(data[,2:18], 2, max) </strong><br class="title-page-name"/><strong class="calibre1">min_data &lt;- apply(data[,2:18], 2, min)</strong><br class="title-page-name"/><strong class="calibre1">data_scaled &lt;- scale(data[,2:18],center = min_data, scale = max_data - min_data)</strong> </pre>
<p class="calibre2">In this snippet of code we need to normalize the data.</p>
<div class="packt_tip">Remember, it is good practice to normalize the data before training a neural network. With normalization, data units are eliminated, allowing you to easily compare data from different locations.</div>
<p class="calibre2"><span>For this example, we will use the <strong class="calibre1">min-max method</strong> (usually called feature <strong class="calibre1">scaling</strong>) to get all the scaled data in the range <em class="calibre14">[0,1]</em>. Before applying the method chosen for normalization, you must calculate the minimum and maximum values of each database column. This procedure has already been adopted in the example we analyzed in <a href="part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 2</a>, <em class="calibre14">Learning Process in Neural Networks</em>.</span></p>
<p class="calibre2">The last line scales the data by adopting the expected normalization rule. Note that we performed normalization only on the last <em class="calibre14">17</em> rows (from <em class="calibre14">2</em> to <em class="calibre14">18</em>), excluding the first column, <kbd class="calibre13">Private</kbd>, that contains a factor with levels <kbd class="calibre13">No</kbd> and <kbd class="calibre13">Yes</kbd>, indicating private or public university. This variable will be our target in the network we are about to build. To get a confirmation of what we say, check the typologies of the variables contained in the dataset. To do this, we will use the function <kbd class="calibre13">str</kbd> to view a compactly display the structure of an arbitrary R object:</p>
<pre class="calibre24"><strong class="calibre1">&gt; str(data)</strong><br class="title-page-name"/><strong class="calibre1">'data.frame': 777 obs. of 18 variables:</strong><br class="title-page-name"/><strong class="calibre1"> $ Private : Factor w/ 2 levels "No","Yes": 2 2 2 2 2 2 2 2 2 2 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ Apps : num 1660 2186 1428 417 193 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ Accept : num 1232 1924 1097 349 146 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ Enroll : num 721 512 336 137 55 158 103 489 227 172 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ Top10perc : num 23 16 22 60 16 38 17 37 30 21 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ Top25perc : num 52 29 50 89 44 62 45 68 63 44 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ F.Undergrad: num 2885 2683 1036 510 249 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ P.Undergrad: num 537 1227 99 63 869 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ Outstate : num 7440 12280 11250 12960 7560 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ Room.Board : num 3300 6450 3750 5450 4120 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ Books : num 450 750 400 450 800 500 500 450 300 660 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ Personal : num 2200 1500 1165 875 1500 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ PhD : num 70 29 53 92 76 67 90 89 79 40 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ Terminal : num 78 30 66 97 72 73 93 100 84 41 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ S.F.Ratio : num 18.1 12.2 12.9 7.7 11.9 9.4 11.5 13.7 11.3 11.5 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ perc.alumni: num 12 16 30 37 2 11 26 37 23 15 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ Expend : num 7041 10527 8735 19016 10922 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ Grad.Rate : num 60 56 54 59 15 55 63 73 80 52 ...</strong></pre>
<p class="calibre2">As anticipated, the first variable is of the <kbd class="calibre13">Factor</kbd> type, with two <kbd class="calibre13">levels</kbd>: <kbd class="calibre13">No</kbd> and <kbd class="calibre13">Yes</kbd>. For the remaining <kbd class="calibre13">17</kbd> variables, these are of the numeric type. As anticipated in <a href="part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 1</a>, <em class="calibre14">Neural Network and Artificial Intelligence Concepts</em>, only numeric data can be used in the model, as neural network is a mathematical model with approximation functions. So we have a problem with the first variable, <kbd class="calibre13">Private</kbd>. Do not worry, the problem can be easily resolved; just turn it into a numeric variable:</p>
<pre class="calibre24"><strong class="calibre1">Private = as.numeric(College$Private)-1</strong><br class="title-page-name"/><strong class="calibre1">data_scaled = cbind(Private,data_scaled)</strong></pre>
<p class="calibre2">In this regard, the first line transforms the <kbd class="calibre13">Private</kbd> variable into numeric, while the second line of code is used to reconstruct the dataset with that variable and the remaining <em class="calibre14">17</em> appropriately normalized variables. To do this, we use the <kbd class="calibre13">cbind</kbd> function, that takes a sequence of vector, matrix, or dataframe arguments and combines by columns or rows, respectively:</p>
<pre class="calibre24"><strong class="calibre1">index = sample(1:nrow(data),round(0.70*nrow(data)))</strong><br class="title-page-name"/><strong class="calibre1">train_data &lt;- as.data.frame(data_scaled[index,])</strong><br class="title-page-name"/><strong class="calibre1">test_data &lt;- as.data.frame(data_scaled[-index,])</strong></pre>
<p class="calibre2"><span>The time has come to split the data for training and testing of the network. In the first line of the code just suggested, the dataset is split into <em class="calibre14">70:30</em>, with the intention of using <em class="calibre14">70</em> percent of the data at our disposal to train the network and the remaining <em class="calibre14">30</em> percent to test the network. In the third line, the data of the dataframe named data is subdivided into two new dataframes, called <kbd class="calibre13">train_data</kbd> and <kbd class="calibre13">test_data</kbd>:</span></p>
<pre class="calibre24"><strong class="calibre1">n = names(train_data)</strong><br class="title-page-name"/><strong class="calibre1">f &lt;- as.formula(paste("Private ~", paste(n[!n %in% "Private"], collapse = " + ")))</strong></pre>
<p class="calibre2"><span>In this piece of code, we first recover all the variable names using the <kbd class="calibre13">names</kbd> function. This function gets or sets the name of an object.</span> <span>Next, we build the formula that we will use to build the network, so we use the <kbd class="calibre13">neuralnet</kbd> function to build and train the network. Everything so far has only been used to prepare the data. Now it is time to build the network:</span></p>
<pre class="calibre24"><strong class="calibre1">deep_net = neuralnet(f,data=train_data,hidden=c(5,3),linear.output=F)</strong></pre>
<p class="calibre2">This is the key line of the code. Here the network is built and trained; let's analyze it in detail. We had anticipated that we would use the <kbd class="calibre13">neuralnet</kbd> library to build our DNN. But what has changed with respect to the cases which we have built a single hidden layer network? Everything is played in the <kbd class="calibre13">hidden</kbd> argument setting.</p>
<div class="packt_tip">Remember that the <kbd class="calibre61">hidden</kbd> argument must contain a vector of integers specifying the number of hidden neurons (vertices) in each layer.</div>
<p class="calibre2">In our case, we set the hidden layer to contain the vector <em class="calibre14">(5,3)</em>, which corresponds to two hidden levels with respective five neurons in the first hidden layer and three neurons in the second.</p>
<pre class="calibre24"><strong class="calibre1">plot(deep_net)</strong></pre>
<p class="calibre2">The previous line simply plots the network diagram, as shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00071.jpeg"/></div>
<p class="calibre2">As we can see, the network is built and trained, and we only have to verify its ability to predict:</p>
<pre class="calibre24"><strong class="calibre1">predicted_data &lt;- compute(deep_net,test_data[,2:18])</strong><br class="title-page-name"/><strong class="calibre1">print(head(predicted_data$net.result))</strong></pre>
<p class="calibre2">To predict the data reserved for testing, we can use the <kbd class="calibre13">compute</kbd> method. This is a method for objects of class <kbd class="calibre13">nn</kbd>, typically produced by the <kbd class="calibre13">neuralnet</kbd> function. Computes the outputs of all the neurons for specific arbitrary covariate vectors given a trained neural network. It's crucial to make sure that the order of the covariates is the same in the new matrix or dataframe as in the original neural network. Subsequently, to visualize, the first lines of the prediction result is used the <kbd class="calibre13">print</kbd> function, shown as follows:</p>
<pre class="calibre24"><strong class="calibre1">&gt; print(head(predicted_data$net.result))</strong><br class="title-page-name"/><strong class="calibre1"> [,1]</strong><br class="title-page-name"/><strong class="calibre1">Abilene Christian University 0.1917109322</strong><br class="title-page-name"/><strong class="calibre1">Adelphi University           1.0000000000</strong><br class="title-page-name"/><strong class="calibre1">Adrian College               1.0000000000</strong><br class="title-page-name"/><strong class="calibre1">Agnes Scott College          1.0000000000</strong><br class="title-page-name"/><strong class="calibre1">Albertus Magnus College      1.0000000000</strong><br class="title-page-name"/><strong class="calibre1">Albion College               1.0000000000</strong></pre>
<p class="calibre2">As can be seen, the forecast results are provided in the form of decimal numbers, which approach the values of the two expected classes (one and zero), but do not exactly assume these values. We need to assume these values precisely, so we can make a comparison with the current values. To do this, we will use the <kbd class="calibre13">sapply()</kbd> function to round these off to either zero or one class, so we can evaluate them against the test labels:</p>
<pre class="calibre24"><strong class="calibre1">predicted_data$net.result &lt;- sapply(predicted_data$net.result,round,digits=0)</strong></pre>
<p class="calibre2">As anticipated, the <kbd class="calibre13">sapply()</kbd> function has rounded the prediction results in the two available classes. Now we have everything we need to make a comparison in order to assess the DNN as a prediction tool:</p>
<pre class="calibre24"><strong class="calibre1">table(test_data$Private,predicted_data$net.result)</strong></pre>
<p class="calibre2">To make a comparison, we rely on the confusion matrix. To build it, just use the <kbd class="calibre13">table</kbd> function. Indeed, the <kbd class="calibre13">table</kbd> function uses the cross-classifying factors to construct a contingency table of counts at each combination of factor levels.</p>
<div class="packt_tip">The confusion matrix is a specific table layout that allows visualization of the performance of an algorithm. Each row represents the instances in an actual class, while each column represents the instances in a predicted class. The term confusion matrix results from the fact that it makes it easy to see if the system is confusing two classes.</div>
<p class="calibre2">Let's see then the results obtained:</p>
<pre class="calibre24"><strong class="calibre1">&gt; table(test_data$Private,predicted_data$net.result)</strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1">    0   1</strong><br class="title-page-name"/><strong class="calibre1"> 0 49   8</strong><br class="title-page-name"/><strong class="calibre1"> 1  9 167</strong></pre>
<p class="calibre2">Let us understand the results. Let us first remember that in a confusion matrix, the terms on the main diagonal represent the number of correct predictions, that is, the number of instances of the predicted class that coincide with the instances of the actual class. It seems that in our simulation, things have gone well. In fact, we got <kbd class="calibre13">49</kbd> occurrences of class <kbd class="calibre13">0</kbd> (<kbd class="calibre13">No</kbd>) and <kbd class="calibre13">167</kbd> of class <kbd class="calibre13">1</kbd> (<kbd class="calibre13">Yes</kbd>). But let's now analyze the other two terms, these represent the mistakes made by the model.</p>
<p class="calibre2">As defined in <a href="part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 2</a>, <em class="calibre14">Learning Process in Neural Networks</em>, <kbd class="calibre13">8</kbd> are FN and <kbd class="calibre13">9</kbd> are FP. In this regard, we recall that FN means the number of negative predictions that are positive in actual data, while FPs are the number of positive predictions that are negative in actual data. We can check this by again using the <kbd class="calibre13">table</kbd> function:</p>
<pre class="calibre24"><strong class="calibre1">&gt; table(test_data$Private)</strong><br class="title-page-name"/><strong class="calibre1">  0   1 </strong><br class="title-page-name"/><strong class="calibre1"> 57 176</strong></pre>
<p class="calibre2">These represent the actual results, in particular, <kbd class="calibre13">57</kbd> results belonging to class <kbd class="calibre13">0</kbd> and <kbd class="calibre13">176</kbd> to class <kbd class="calibre13">1</kbd>. By summing the data contained in the rows of the confusion matrix, we get exactly those values in fact results:</p>
<pre class="calibre24"><strong class="calibre1">&gt; 49 + 8</strong><br class="title-page-name"/><strong class="calibre1">[1] 57</strong><br class="title-page-name"/><strong class="calibre1">&gt; 9 + 167</strong><br class="title-page-name"/><strong class="calibre1">[1] 176</strong></pre>
<p class="calibre2">Now we again use the <kbd class="calibre13">table</kbd> function to obtain the occurrences in the predicted data:</p>
<pre class="calibre24"><strong class="calibre1">&gt; table(predicted_data$net.result)</strong><br class="title-page-name"/><strong class="calibre1">  0   1 </strong><br class="title-page-name"/><strong class="calibre1"> 58 175</strong></pre>
<p class="calibre2">These represent the results of the prediction, in particular, <kbd class="calibre13">58</kbd> results belonging to class <kbd class="calibre13">0</kbd> and <kbd class="calibre13">175</kbd> to class <kbd class="calibre13">1</kbd>. By summing the data contained in the columns of the confusion matrix, we get exactly those values in fact results:</p>
<pre class="calibre24"><strong class="calibre1">&gt; 49 + 9</strong><br class="title-page-name"/><strong class="calibre1">[1] 58</strong><br class="title-page-name"/><strong class="calibre1">&gt; 8 + 167</strong><br class="title-page-name"/><strong class="calibre1">[1] 175</strong></pre>
<p class="calibre2">At this point, we calculate the accuracy of the simulation by using the data contained in the confusion matrix. Let's remember that accuracy is defined by the following equation:</p>
<div class="calibre66"><img src="../images/00072.gif" class="calibre67"/></div>
<p class="calibre2"> </p>
<p class="calibre2">Here:</p>
<p class="cdpaligncenter1"><em class="calibre14">TP = TRUE POSITIVE</em></p>
<p class="cdpaligncenter1"><em class="calibre14">TN = TRUE NEGATIVE</em></p>
<p class="cdpaligncenter1"><em class="calibre14">FP = FALSE POSITIVE</em></p>
<p class="cdpaligncenter1"><em class="calibre14">FN = TRUE NEGATIVE</em></p>
<p class="calibre2">Let us take a look at this in the following code sample:</p>
<pre class="calibre24"><strong class="calibre1">&gt; Acc = (49 + 167) / (49 + 167 + 9 + 8) </strong><br class="title-page-name"/><strong class="calibre1">&gt; Acc</strong><br class="title-page-name"/><strong class="calibre1">[1] 0.9270386266</strong></pre>
<p class="calibre2">We've got an accuracy of around 93 percent, confirming that our model is able to predict data with a good result.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Training and modeling a DNN using H2O</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will cover an example of training and modeling a DNN using <kbd class="calibre13">h2o</kbd>. <kbd class="calibre13">h2o</kbd> is an open source, in-memory, scalable machine learning and AI platform used to build models with large datasets and implement predictions with high-accuracy methods. The <kbd class="calibre13"><span><span>h2o</span></span></kbd> library is adapted at a large scale in numerous organizations to operationalize data science and provide a platform to build data products. <kbd class="calibre13">h2o</kbd> can run on individual laptops or large clusters of high-performance scalable servers. It works very fast, exploiting the machine architecture advancements and GPU processing. It has high-accuracy implementations of deep learning, neural networks, and other machine learning algorithms.</p>
<p class="calibre2"><span>As said earlier, the <kbd class="calibre13">h2o</kbd> R package has functions for building general linear regression, K-means, Naive Bayes,</span> PCA, <span>forests, and deep learning (multilayer <kbd class="calibre13">neuralnet</kbd> models).</span> The <kbd class="calibre13">h2o</kbd> package is an external package to CRAN and is built using Java. It is available for a variety of platforms.</p>
<p class="calibre2">We will install <kbd class="calibre13">h2o</kbd> in R using the following code:</p>
<pre class="calibre24"><strong class="calibre1">install.packages("h2o")</strong></pre>
<p class="calibre2">We obtain the following results:</p>
<pre class="calibre24"><strong class="calibre1">&gt; install.packages("h2o")</strong><br class="title-page-name"/><strong class="calibre1">Installing package into ‘C:/Users/Giuseppe/Documents/R/win-library/3.4’</strong><br class="title-page-name"/><strong class="calibre1">(as ‘lib’ is unspecified)</strong><br class="title-page-name"/><strong class="calibre1">trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.4/h2o_3.10.5.3.zip'</strong><br class="title-page-name"/><strong class="calibre1">Content type 'application/zip' length 73400625 bytes (70.0 MB)</strong><br class="title-page-name"/><strong class="calibre1">downloaded 70.0 MB</strong><br class="title-page-name"/><strong class="calibre1">package ‘h2o’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">The downloaded binary packages are in</strong><br class="title-page-name"/><strong class="calibre1"> C:\Users\Giuseppe\AppData\Local\Temp\RtmpGEc5iI\downloaded_packages</strong></pre>
<p class="calibre2">To test the package, let's go through the following example that uses the popular dataset named <kbd class="calibre13">Irisdataset</kbd>. I'm referring to the Iris flower dataset, a multivariate dataset introduced by the British statistician and biologist Ronald Fisher in his 1936 paper <em class="calibre14">The use of multiple measurements in taxonomic problems</em> as an example of linear discriminant analysis.</p>
<p class="calibre2">The dataset contains <em class="calibre14">50</em> samples from each of the three species of Iris (Iris <kbd class="calibre13">setosa</kbd>, Iris <kbd class="calibre13">virginica</kbd>, and Iris <kbd class="calibre13">versicolor</kbd>). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.</p>
<p class="calibre2">The following variables are contained:</p>
<ul class="calibre16">
<li class="calibre17"><kbd class="calibre13">Sepal.Length</kbd> in centimeter</li>
<li class="calibre17"><kbd class="calibre13">Sepal.Width</kbd> in <span>centimeter</span></li>
<li class="calibre17"><kbd class="calibre13">Petal.Length</kbd> in <span>centimeter</span></li>
<li class="calibre17"><kbd class="calibre13">Petal.Width</kbd> in <span>centimeter</span></li>
<li class="calibre17">Class: <kbd class="calibre13">setosa</kbd>, <kbd class="calibre13">versicolour</kbd>, <kbd class="calibre13">virginica</kbd></li>
</ul>
<p class="calibre2">The following figure shows a compactly display the structure of the <kbd class="calibre13">iris</kbd> dataset:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00073.jpeg"/></div>
<p class="calibre2">We want to build a classifier that, depending on the size of the sepal and petal, is able to classify the flower species:</p>
<pre class="calibre24"><strong class="calibre1">##########################################################################</strong><br class="title-page-name"/><strong class="calibre1">#################Chapter 3 - Deep Learning with H2O and R################# ##########################################################################</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">library(h2o)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">c1=h2o.init(max_mem_size = "2G", </strong><br class="title-page-name"/><strong class="calibre1">      nthreads = 2, </strong><br class="title-page-name"/><strong class="calibre1">      ip = "localhost", </strong><br class="title-page-name"/><strong class="calibre1">      port = 54321)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">data(iris)</strong><br class="title-page-name"/><strong class="calibre1">summary(iris)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">iris_d1 &lt;- h2o.deeplearning(1:4,5,</strong><br class="title-page-name"/><strong class="calibre1">              as.h2o(iris),hidden=c(5,5),</strong><br class="title-page-name"/><strong class="calibre1">              export_weights_and_biases=T)</strong><br class="title-page-name"/><strong class="calibre1">iris_d1</strong><br class="title-page-name"/><strong class="calibre1">plot(iris_d1)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">h2o.weights(iris_d1, matrix_id=1)</strong><br class="title-page-name"/><strong class="calibre1">h2o.weights(iris_d1, matrix_id=2)</strong><br class="title-page-name"/><strong class="calibre1">h2o.weights(iris_d1, matrix_id=3)</strong><br class="title-page-name"/><strong class="calibre1">h2o.biases(iris_d1, vector_id=1)</strong><br class="title-page-name"/><strong class="calibre1">h2o.biases(iris_d1, vector_id=2)</strong><br class="title-page-name"/><strong class="calibre1">h2o.biases(iris_d1, vector_id=3)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">#plot weights connecting `Sepal.Length` to first hidden neurons</strong><br class="title-page-name"/><strong class="calibre1">plot(as.data.frame(h2o.weights(iris_d1, matrix_id=1))[,1])</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">##########################################################################</strong></pre>
<p class="calibre2">Now, let's go through the code to understand how to apply the <kbd class="calibre13">h2o</kbd> package to solve a pattern recognition problem.</p>
<div class="packt_tip">Before proceeding, it is necessary to specify that running <kbd class="calibre61">h2o</kbd> on R requires Java 8 runtime. Verify the Java version installed on your machine beforehand and eventually download Java version 8 from <a href="https://www.java.com/en/download/win10.jsp" class="calibre68">https://www.java.com/en/download/win10.jsp</a>.</div>
<p class="calibre2">The following figure shows the Java download page from Oracle's site:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00074.jpeg"/></div>
<p class="calibre2">Furthermore, the <kbd class="calibre13">h2o</kbd> package is built with some required packages; so in order to properly install the <kbd class="calibre13">h2o</kbd> package, remember to install the following dependencies, all of which are available in CRAN:</p>
<ul class="calibre16">
<li class="calibre17"><kbd class="calibre13">RCurl</kbd></li>
<li class="calibre17"><kbd class="calibre13">bitops</kbd></li>
<li class="calibre17"><kbd class="calibre13">rjson</kbd></li>
<li class="calibre17"><kbd class="calibre13">jsonlite</kbd></li>
<li class="calibre17"><kbd class="calibre13">statmod</kbd></li>
<li class="calibre17"><kbd class="calibre13">tools</kbd></li>
</ul>
<p class="calibre2">After we have successfully installed the <kbd class="calibre13">h2o</kbd> package, we can proceed with loading the library:</p>
<pre class="calibre24"><strong class="calibre1">library(h2o)</strong></pre>
<p class="calibre2">This command loads the library in R environment. The following messages are returned:</p>
<pre class="calibre24"><strong class="calibre1">Your next step is to start H2O:</strong><br class="title-page-name"/><strong class="calibre1"> &gt; h2o.init()</strong><br class="title-page-name"/><strong class="calibre1">For H2O package documentation, ask for help:</strong><br class="title-page-name"/><strong class="calibre1"> &gt; ??h2o</strong><br class="title-page-name"/><strong class="calibre1">After starting H2O, you can use the Web UI at http://localhost:54321</strong><br class="title-page-name"/><strong class="calibre1">For more information visit http://docs.h2o.ai</strong><br class="title-page-name"/><strong class="calibre1">c1=h2o.init(max_mem_size = "2G", </strong><br class="title-page-name"/><strong class="calibre1">      nthreads = 2, </strong><br class="title-page-name"/><strong class="calibre1">      ip = "localhost", </strong><br class="title-page-name"/><strong class="calibre1">      port = 54321)</strong></pre>
<p class="calibre2">We follow the directions on the R prompt:</p>
<pre class="calibre24"><strong class="calibre1">c1=h2o.init(max_mem_size = "2G", </strong><br class="title-page-name"/><strong class="calibre1">      nthreads = 2, </strong><br class="title-page-name"/><strong class="calibre1">      ip = "localhost", </strong><br class="title-page-name"/><strong class="calibre1">      port = 54321)</strong></pre>
<p class="calibre2">The <kbd class="calibre13">h20.init</kbd> function initiates the <kbd class="calibre13">h2o</kbd> engine with a maximum memory size of 2 GB and two parallel cores. The console for <kbd class="calibre13">h2o</kbd> is initialized and we get the following messages once we run this script:</p>
<pre class="calibre24"><strong class="calibre1">&gt; c1=h2o.init(max_mem_size = "2G", nthreads = 2)</strong><br class="title-page-name"/><strong class="calibre1">H2O is not running yet, starting it now...</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">Note: In case of errors look at the following log files:</strong><br class="title-page-name"/><strong class="calibre1">    C:\Users\Giuseppe\AppData\Local\Temp\RtmpU3xPvT/h2o_Giuseppe_started_from_r.out</strong><br class="title-page-name"/><strong class="calibre1">    C:\Users\Giuseppe\AppData\Local\Temp\RtmpU3xPvT/h2o_Giuseppe_started_from_r.err</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">java version "1.8.0_144"</strong><br class="title-page-name"/><strong class="calibre1">Java(TM) SE Runtime Environment (build 1.8.0_144-b01)</strong><br class="title-page-name"/><strong class="calibre1">Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">Starting H2O JVM and connecting: . Connection successful!</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">R is connected to the H2O cluster: </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster uptime: 6 seconds 912 milliseconds </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster version: 3.10.5.3 </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster version age: 2 months and 9 days </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster name: H2O_started_from_R_Giuseppe_woc815 </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster total nodes: 1 </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster total memory: 1.78 GB </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster total cores: 4 </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster allowed cores: 2 </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster healthy: TRUE </strong><br class="title-page-name"/><strong class="calibre1">    H2O Connection ip: localhost </strong><br class="title-page-name"/><strong class="calibre1">    H2O Connection port: 54321 </strong><br class="title-page-name"/><strong class="calibre1">    H2O Connection proxy: NA </strong><br class="title-page-name"/><strong class="calibre1">    H2O Internal Security: FALSE </strong><br class="title-page-name"/><strong class="calibre1">    R Version: R version 3.4.1 (2017-06-30)</strong></pre>
<p class="calibre2">Once <kbd class="calibre13">h2o</kbd> is initiated, the console can be viewed from any browser by pointing to <kbd class="calibre13">localhost:54321</kbd>. The <kbd class="calibre13">h2o</kbd> library runs on a JVM and the console allows:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00075.jpeg"/></div>
<p class="calibre2">The console is intuitive and provides an interface to interact with the h<sub class="calibre25">2</sub>o engine. We can train and test models and run predictions on top of them. The first textbox, labeled <span>CS</span>, allows us to enter routines for execution. The <kbd class="calibre13">assist</kbd> command gives the list of the routines available. Let us continue to analyze the following sample code.</p>
<pre class="calibre24"><strong class="calibre1">data(iris)</strong><br class="title-page-name"/><strong class="calibre1">summary(iris)</strong></pre>
<p class="calibre2"><span>The first command loads the <kbd class="calibre13">iris</kbd> dataset, which is contained in the datasets library, and saves it in a given dataframe. Then we use the <kbd class="calibre13">summary</kbd> function to produce result summaries of the results of the dataset. The function invokes particular methods which depend on the class of the first argument. The results are shown as follows:</span></p>
<pre class="calibre24"><strong class="calibre1">&gt; summary(iris)</strong><br class="title-page-name"/><strong class="calibre1">  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width </strong><br class="title-page-name"/><strong class="calibre1"> Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100 </strong><br class="title-page-name"/><strong class="calibre1"> 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300</strong><br class="title-page-name"/><strong class="calibre1"> Median :5.800   Median :3.000   Median :4.350   Median :1.300  </strong><br class="title-page-name"/><strong class="calibre1"> Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199               </strong><br class="title-page-name"/><strong class="calibre1"> 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800              </strong><br class="title-page-name"/><strong class="calibre1"> Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500<br class="title-page-name"/></strong>     <strong class="calibre1"> Species</strong><br class="title-page-name"/><strong class="calibre1">setosa    :50 </strong><br class="title-page-name"/><strong class="calibre1">versicolor:50 </strong><br class="title-page-name"/><strong class="calibre1">virginica :50<br class="title-page-name"/></strong></pre>
<p class="calibre2">Let's analyze the next lines of code:</p>
<pre class="calibre24"><strong class="calibre1">iris_d1 &lt;- h2o.deeplearning(1:4,5,</strong><br class="title-page-name"/><strong class="calibre1">              as.h2o(iris),hidden=c(5,5),</strong><br class="title-page-name"/><strong class="calibre1">              export_weights_and_biases=T)</strong></pre>
<p class="calibre2">The <kbd class="calibre13">h2o.deeplearning</kbd> function is an important function within <kbd class="calibre13">h2o</kbd> and can be used for variety of operations. This function builds a DNN model using CPUs builds a feed-forward multilayer ANN on an <kbd class="calibre13">H2OFrame</kbd>. The <kbd class="calibre13">hidden</kbd> argument is used to set the number of hidden layers and the number of neurons for each hidden layer. In our case, we have set up a DNN with two hidden layers and <kbd class="calibre13">5</kbd> neurons for each hidden layer. Finally, the parameter <kbd class="calibre13">export_weights_and_biases</kbd> tells us that the weights and biases can be stored in <kbd class="calibre13">H2OFrame</kbd> and can be accessed like other dataframes for further processing.</p>
<p class="calibre2">Before proceeding with the code analysis, a clarification should be made. The attentive reader can ask that on the basis of which evaluation we have chosen the number of hidden layers and the number of neurons for each hidden layer. Unfortunately, there is no precise rule or even a mathematical formula that allows us to determine which numbers are appropriate for that specific problem. This is because every problem is different from each other and each network approximates a system differently. So what makes the difference between one model and another? The answer is obvious and once again very clear: the researcher's experience.</p>
<p class="calibre2">The advice I can give, which stems from the vast experience in data analysis, is to try, try, and try again. The secret to experimental activity is just that. In the case of neural networks, this results in trying to set up different networks and then verifying their performance. For example, in our case, we could have started from a network with two hidden layers and <em class="calibre14">100</em> neurons per hidden layer, then progressively reduced those values, and then arrive at those that I proposed in the example. This procedure can be automated with the use of the iterative structures that R owns.</p>
<p class="calibre2">However, some things can be said, for example, for the optimum choice of the number of neurons we need to know that:</p>
<ul class="calibre16">
<li class="calibre17">Small number of neurons will lead to high error for your system, as the predictive factors might be too complex for a small number of neurons to capture</li>
<li class="calibre17">Large number of neurons will overfit to your training data and not generalize well</li>
<li class="calibre17">The number of neurons in each hidden layer should be somewhere between the size of the input and the output layer, potentially the mean</li>
<li class="calibre17">The number of neurons in each hidden layer shouldn't exceed twice the number of input neurons, as you are probably grossly overfit at this point</li>
</ul>
<p class="calibre2">That said, we return to the code:</p>
<pre class="calibre24"><strong class="calibre1">iris_d1</strong></pre>
<p class="calibre2"><span>At the R prompt, t</span>his command prints a brief description of the features of the model we just created, as shown in the following figure:</p>
<div class="cdpaligncenter"><strong class="calibre1"><img class="image-border18" src="../images/00076.jpeg"/></strong></div>
<p class="calibre2">By carefully analyzing the previous figure, we can clearly distinguish the details of the model along with the confusion matrix. Let's now look at how the training process went on:</p>
<pre class="calibre24"><strong class="calibre1">plot(iris_d1)</strong></pre>
<p class="calibre2">The <kbd class="calibre13">plot</kbd> method dispatches on the type of h2o model to select the correct scoring history. The arguments are restricted to what is available in the scoring history for a particular type of model, <span>as shown in the following figure:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00077.gif"/></div>
<p class="calibre2">In this plot are shown the training classification errors versus epochs, as we can see that <span>the gradient descents and the errors decrease as we progress in the epochs</span>. <span>How many times the dataset should be iterated (streamed) can be fractional. It defaults to</span> <kbd class="calibre13">10</kbd>:</p>
<pre class="calibre24"><strong class="calibre1">h2o.weights(<span>iris_d1</span>, <span>matrix_id</span><span>=</span><span>1</span>)
h2o.weights(<span>iris_d1</span>, <span>matrix_id</span><span>=</span><span>2</span>)
h2o.weights(<span>iris_d1</span>, <span>matrix_id</span><span>=</span><span>3</span>)
h2o.biases(<span>iris_d1</span>, <span>vector_id</span><span>=</span><span>1</span>)
h2o.biases(<span>iris_d1</span>, <span>vector_id</span><span>=</span><span>2</span>)
h2o.biases(<span>iris_d1</span>, <span>vector_id</span><span>=</span><span>3</span>)</strong></pre>
<p class="calibre2">The last six lines of the code simply print a short summary of the weights and biases for the three species of iris flower, as is shown next:</p>
<pre class="calibre24"><strong class="calibre1">&gt; h2o.weights(iris_d1, matrix_id=1)  </strong><br class="title-page-name"/><strong class="calibre1">  Sepal.Length Sepal.Width Petal.Length  Petal.Width</strong><br class="title-page-name"/><strong class="calibre1">1 -0.013207575 -0.06818321  -0.02756812  0.092810206</strong><br class="title-page-name"/><strong class="calibre1">2  0.036195096  0.02568028   0.05634377  0.035429616</strong><br class="title-page-name"/><strong class="calibre1">3 -0.002411760 -0.11541270   0.08219513  0.001957144</strong><br class="title-page-name"/><strong class="calibre1">4  0.091338813 -0.03271343  -0.25603485 -0.205898494</strong><br class="title-page-name"/><strong class="calibre1">6 -0.151234403  0.01785624  -0.11815275 -0.110585481 </strong><br class="title-page-name"/><strong class="calibre1">[200 rows x 4 columns] </strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">&gt; h2o.biases(iris_d1, vector_id=1)         </strong><br class="title-page-name"/><strong class="calibre1">C11 0.48224932 0.47699773 0.48994124 0.49552965 0.48991496 0.4739439 </strong><br class="title-page-name"/><strong class="calibre1">[200 rows x 1 column]</strong> </pre>
<p class="calibre2">We have restricted ourselves to seeing weights and biases only for the <kbd class="calibre13">setosa</kbd> species, for space reasons. In the following code we use the plot function again:</p>
<pre class="calibre24"><strong class="calibre1">plot(as.data.frame(h2o.weights(<span>iris_d1</span>, <span>matrix_id</span><span>=</span><span>1</span>))[,<span>1</span>])</strong></pre>
<p class="calibre2"><span>This command</span> plots the weights of the first hidden layer neurons versus sepal lengths, as is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00078.gif"/></div>
<p class="calibre2">Now, let us dedicate some time to the analysis of the results; in particular, we recover the confusion matrix that we just glimpsed in the model summary screen, shown earlier. To invoke the confusion matrix, we can use the <kbd class="calibre13">h2o.confusionMatrix</kbd> function as shown in the following code sample, which retrieves either single or multiple confusion matrices from the <kbd class="calibre13">h2o</kbd> objects.</p>
<pre class="calibre24"><strong class="calibre1">&gt; h2o.confusionMatrix(iris_d1)</strong><br class="title-page-name"/><strong class="calibre1">Confusion Matrix: Row labels: Actual class; Column labels: Predicted class           </strong><br class="title-page-name"/><strong class="calibre1">           setosa versicolor virginica  Error      Rate</strong><br class="title-page-name"/><strong class="calibre1">setosa         50          0         0 0.0000 =  0 / 50</strong><br class="title-page-name"/><strong class="calibre1">versicolor      0         48         2 0.0400 =  2 / 50</strong><br class="title-page-name"/><strong class="calibre1">virginica       0          2        48 0.0400 =  2 / 50</strong><br class="title-page-name"/><strong class="calibre1">Totals         50         50        50 0.0267 = 4 / 150</strong></pre>
<p class="calibre2">From the analysis of the confusion matrix, it can be seen that the model manages to correctly classify the three floral species by committing only four errors. These errors are fairly divided among only two species: <kbd class="calibre13">versicolor</kbd>, and <kbd class="calibre13">virginica</kbd>. However, the <kbd class="calibre13">setosa</kbd> species is correctly classified in all <kbd class="calibre13">50</kbd> occurrences. But why is this happening? To understand, let's take a look at the starting data. In the case of multidimensional data, the best way to do this is to plot a scatterplot matrix of selected variables in a dataset:</p>
<pre class="calibre24"><strong class="calibre1">&gt; pairs(iris[1:4], main = "Scatterplot matrices of Iris Data", pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)])</strong></pre>
<p class="calibre2">The results are shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00079.gif"/></div>
<p class="calibre2">Let's analyze in detail the plot just proposed. The variables are written in a diagonal line from top left to bottom right. Then each variable is plotted against each other. For example, the second box in the first column is an individual scatterplot of <kbd class="calibre13">Sepal.Length</kbd> versus <kbd class="calibre13">Sepal.Width</kbd>, with <kbd class="calibre13">Sepal.Length</kbd> as the <em class="calibre14">X</em> axis and <kbd class="calibre13">Sepal.Width</kbd> as the <em class="calibre14">Y</em> axis. This same plot is replicated in the first plot of the second column. In essence, the boxes on the upper right hand side of the whole scatterplot are mirror images of the plots on the lower left hand.</p>
<p class="calibre2">From the analysis of the figure just seen, it can be seen that the <kbd class="calibre13">versicolor</kbd> and <kbd class="calibre13">virginica</kbd> species show overlapping boundaries. This makes us understand that the model's attempt to classify it when it falls into that area can cause errors. We can see what happens for the <kbd class="calibre13">setosa</kbd> species, which instead has far distant borders from other floral species without any classification error.</p>
<p class="calibre2">That said, we evaluate the accuracy of the model in classifying floral species on the basis of the size of petals and sepals:</p>
<pre class="calibre24"><strong class="calibre1">&gt; h2o.hit_ratio_table(iris_d1)</strong><br class="title-page-name"/><strong class="calibre1">Top-3 Hit Ratios: </strong><br class="title-page-name"/><strong class="calibre1"> k hit_ratio</strong><br class="title-page-name"/><strong class="calibre1">1 1 0.973333</strong><br class="title-page-name"/><strong class="calibre1">2 2 1.000000</strong><br class="title-page-name"/><strong class="calibre1">3 3 1.000000</strong></pre>
<p class="calibre2">The results show that the simulation, based on the first hypothesis, ranked the species with <em class="calibre14">97</em> percent accuracy. I would say that is a good result; the model fit the data very well. But how can we measure this feature? One method to find a better fit is to calculate the coefficient of determination (R-squared). To calculate R-squared in <kbd class="calibre13">h2o</kbd>, we can use the <kbd class="calibre13">h2o.r2</kbd> method:</p>
<pre class="calibre24"><strong class="calibre1">&gt; h2o.r2(iris_d1)</strong><br class="title-page-name"/><strong class="calibre1">[1] 0.9596034</strong></pre>
<p class="calibre2">Now let's understand what we've calculated and how to read the result. R-squared measures how well a model can predict the data, and falls between zero and one. The higher the value of coefficient of determination, the better the model is at predicting the data.<br class="title-page-name"/>
We got a value of <em class="calibre14">0.96</em>, so according to what we have said, this is a great result. To get a confirmation of this, we have to compare it with the result of another simulation model. So, we build a linear regression model based on the same data, that is the <kbd class="calibre13">iris</kbd> dataset.</p>
<p class="calibre2">To build a linear regression model, we can use the <kbd class="calibre13">glm</kbd> function. This function is used to fit generalized linear models, specified by giving a symbolic description of the linear predictor and a description of the error distribution:</p>
<pre class="calibre24"><strong class="calibre1">m=iris.lm &lt;- h2o.glm(x=2:5,y=1,training_frame=as.h2o(iris))</strong></pre>
<p class="calibre2">Now we calculate the model's coefficient of determination:</p>
<pre class="calibre24"><strong class="calibre1">&gt; h2o.r2(m)</strong><br class="title-page-name"/><strong class="calibre1">[1] 0.8667852</strong></pre>
<p class="calibre2">Now we can make a comparison between the model based on DNNs and the linear regression model. DNN had provided a R-squared value of <em class="calibre14">0.96</em>, while the resulting regression model provided a R-squared value of <em class="calibre14">0.87</em>. It is clear that DNN provides much better performance.</p>
<p class="calibre2">Finally, it may be useful to analyze the parameters that are important for a neural network specialist, as shown in the following table:</p>
<table class="calibre40">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Argument</strong></p>
</td>
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Description</strong></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">x</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">A vector containing the names or indices of the predictor variables to use in building a model. If <kbd class="calibre13">x</kbd> is missing, then all columns except <kbd class="calibre13">y</kbd> are used.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">y</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">The name of the response variable in a model. If the data does not contain a header, this is the first column index, increasing from left to right (the response must be either an integer or a categorical variable).</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">model_id</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">This is the destination <kbd class="calibre13">id</kbd> for a model; it is autogenerated if not specified.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">standardize</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It is a logical function. If enabled, it automatically standardizes the data. If disabled, the user must provide properly scaled input data. It defaults to <kbd class="calibre13">TRUE</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">activation</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It is an activation function. It must be one of <kbd class="calibre13">Tanh</kbd>, <kbd class="calibre13">TanhWithDropout</kbd>, <kbd class="calibre13">Rectifier</kbd>, <kbd class="calibre13">RectifierWithDropout</kbd>, <kbd class="calibre13">Maxout</kbd>, or <kbd class="calibre13">MaxoutWithDropout</kbd>. It defaults to <kbd class="calibre13">Rectifier</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">hidden</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">This argument specifies hidden layer sizes (for example, <kbd class="calibre13">[100, 100]</kbd>). It defaults to <kbd class="calibre13">[200, 200]</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">epochs</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">How many times the dataset should be iterated (streamed) can be fractional. It defaults to <kbd class="calibre13">10</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">adaptive_rate</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It is a logical argument specifying the Adaptive learning rate. It defaults to <kbd class="calibre13">TRUE</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">rho</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">This describes the adaptive learning rate time decay factor (similar to prior updates). It defaults to <kbd class="calibre13">0.99</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">rate_annealing</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Learning rate annealing is given by <kbd class="calibre13">rate/(1 + rate_annealing * samples</kbd>). It defaults to <kbd class="calibre13">1e- 06</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">rate_decay</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">This is the learning rate decay factor between layers (<em class="calibre14">N<sup class="calibre69">th</sup></em> layer: <kbd class="calibre13">rate * rate_decay ^ (n - 1</kbd>). It defaults to <kbd class="calibre13">1</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">input_dropout_ratio</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Input layer dropout ratio (can improve generalization, try <kbd class="calibre13">0.1</kbd> or <kbd class="calibre13">0.2</kbd>). It defaults to <kbd class="calibre13">0</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">hidden_dropout_ratios</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Hidden layer dropout ratios can improve generalization. Specify one value per hidden layer. It defaults to <kbd class="calibre13">0.5</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">l1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">L1 regularization can add stability and improve generalization, and it causes many weights to become <kbd class="calibre13">0</kbd>. It defaults to <kbd class="calibre13">0</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">l2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">L2 regularization can add stability and improve generalization, and it causes many weights to be small. It defaults to <kbd class="calibre13">0</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">initial_weights</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">This is a list of the <kbd class="calibre13">H2OFrame</kbd> IDs to initialize the weight matrices of this model with.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">initial_biases</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It is a list of the <kbd class="calibre13">H2OFrame</kbd> IDs to initialize the bias vectors of this model with.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">loss</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">The loss function must be one of <kbd class="calibre13">Automatic</kbd>, <kbd class="calibre13">CrossEntropy</kbd>, <kbd class="calibre13">Quadratic</kbd>, <kbd class="calibre13">Huber</kbd>, <kbd class="calibre13">Absolute</kbd>, or <kbd class="calibre13">Quantile</kbd>. It defaults to <kbd class="calibre13">Automatic</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">distribution</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">The distribution function must be one of <kbd class="calibre13">AUTO</kbd>, <kbd class="calibre13">bernoulli</kbd>, <kbd class="calibre13">multinomial</kbd>, <kbd class="calibre13">gaussian</kbd>, <kbd class="calibre13">poisson</kbd>, <kbd class="calibre13">gamma</kbd>, <kbd class="calibre13">tweedie</kbd>, <kbd class="calibre13">laplace</kbd>, <kbd class="calibre13">quantile</kbd>, or <kbd class="calibre13">huber</kbd>. It defaults to <kbd class="calibre13">AUTO</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">score_training_samples</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It is the number of training set samples for scoring (0 for all). It defaults to <kbd class="calibre13">10000</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">score_validation_samples</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It is the number of validation set samples for scoring (0 for all). It defaults to <kbd class="calibre13">0</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">classification_stop</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">The stopping criterion for the classification error fraction on training data (-1 to disable). It defaults to <kbd class="calibre13">0</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">regression_stop</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It is the stopping criterion for the regression error (MSE) on training data (<kbd class="calibre13">-1</kbd> to disable). It defaults to <kbd class="calibre13">1e-06</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">stopping_rounds</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Early stopping based on convergence of <kbd class="calibre13">stopping_metric</kbd>. Stop if simple moving average of length <kbd class="calibre13">k</kbd> of the <kbd class="calibre13">stopping_metric</kbd> does not improve for <kbd class="calibre13">k:=stopping_rounds</kbd> scoring events (0 to disable) It defaults to <kbd class="calibre13">5</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">max_runtime_secs</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It is maximum allowed runtime in seconds for model training. Use <kbd class="calibre13">0</kbd> to disable it. It defaults to <kbd class="calibre13">0</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">diagnostics</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It enables diagnostics for hidden layers. It defaults to <kbd class="calibre13">TRUE</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">fast_mode</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It enables fast mode (minor approximation in backpropagation). It defaults to <kbd class="calibre13">TRUE</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">replicate_training_data</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It replicates the entire training dataset on every node for faster training on small datasets. It defaults to <kbd class="calibre13">TRUE</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">single_node_mode</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It runs on a single node for fine-tuning of model parameters. It defaults to <kbd class="calibre13">FALSE</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">shuffle_training_data</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It enables shuffling of training data (recommended if training data is replicated and <kbd class="calibre13">train_samples_per_iteration</kbd> is close to <kbd class="calibre13">#nodes</kbd> <kbd class="calibre13">x</kbd> <kbd class="calibre13">#rows</kbd>, or if using <kbd class="calibre13">balance_classes</kbd>). It defaults to <kbd class="calibre13">FALSE</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">missing_values_handling</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Handling of missing values must be one of <kbd class="calibre13">MeanImputation</kbd> or <kbd class="calibre13">Skip</kbd>. It defaults to <kbd class="calibre13">MeanImputation</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">quiet_mode</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It enables quiet mode for less output to standard output. It defaults to <kbd class="calibre13">FALSE</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">verbose</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">It prints scoring history to the console (metrics per tree for GBM, DRF, and XGBoost; metrics per epoch for deep learning. It defaults to <kbd class="calibre13">False</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">autoencoder</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Logical autoencoder defaults to <kbd class="calibre13">FALSE</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">export_weights_and_biases</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Whether to export neural network weights and biases to <kbd class="calibre13">H2OFrame</kbd>. It defaults to <kbd class="calibre13">FALSE</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">mini_batch_size</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Mini-batch size (smaller leads to better fit, whereas larger can speed up and generalize better). It defaults to <kbd class="calibre13">1</kbd>.</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">There are other functions related to R with <kbd class="calibre13">h2o</kbd> for deep learning. Some useful ones are listed as follows:</p>
<table class="calibre40">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Function</strong></p>
</td>
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Description</strong></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">predict.H2Omodel</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Returns an <kbd class="calibre13">H2OFrame</kbd> object with probabilities and default predictions.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">h2o.deepwater</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Builds a deep learning model using multiple native GPU backends. Builds DNN on <kbd class="calibre13">H2OFrame</kbd> containing various data sources.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">as.data.frame.H2OFrame</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Converts <kbd class="calibre13">H2OFrame</kbd> to a dataframe.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">h2o.confusionMatrix</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Displays the confusion matrix for a classification model.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">print.H2OFrame</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Prints <kbd class="calibre13">H2OFrame</kbd>.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">h2o.saveModel</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Saves an <kbd class="calibre13">h2o</kbd> model object to disk.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">h2o.importFile</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2">Imports a file into <span><span>h2o</span></span>.</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Deep autoencoders using H2O</h1>
                
            
            <article>
                
<p class="calibre2">Autoencoders are unsupervised learning methods on neural networks. We'll see more of this in <a target="_blank" href="part0123.html#3L9L60-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 7</a>, <em class="calibre14">Use Cases of Neural Networks – Advanced Topics</em>. <kbd class="calibre13">h2o</kbd> can be used to detect an anomaly by using deep autoencoders. To train such a model, the same function, <kbd class="calibre13">h2o.deeplearning()</kbd><span>,</span> is used, with some changes in the parameters:</p>
<pre class="calibre24"><strong class="calibre1">anomaly_model &lt;- h2o.deeplearning(1:4,</strong><br class="title-page-name"/><strong class="calibre1">                                  training_frame = as.h2o(iris),</strong><br class="title-page-name"/><strong class="calibre1">                                  activation = "Tanh",</strong><br class="title-page-name"/><strong class="calibre1">                                  autoencoder = TRUE,</strong><br class="title-page-name"/><strong class="calibre1">                                  hidden = c(50,20,50),</strong><br class="title-page-name"/><strong class="calibre1">                                  sparse = TRUE,</strong><br class="title-page-name"/><strong class="calibre1">                                  l1 = 1e-4,</strong><br class="title-page-name"/><strong class="calibre1">                                  epochs = 100)</strong></pre>
<p class="calibre2">The <kbd class="calibre13">autoencoder=TRUE</kbd> sets the <kbd class="calibre13">deeplearning</kbd> method to use the autoencoder technique unsupervised learning method. We are using only the training data, without the test set and the labels. The fact that we need a deep <kbd class="calibre13">autoencoder</kbd> instead of a feed-forward network is specified by the <kbd class="calibre13">autoencoder</kbd> parameter.</p>
<p class="calibre2">We can choose the number of hidden units to be present in different layers. If we choose an integer value, what we get is called a <strong class="calibre1">naive autoencoder</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">Deep learning is a subject of importance right from image detection to speech recognition and AI-related activity. There are numerous products and packages in the market for deep learning. Some of these are <kbd class="calibre13">Keras</kbd>, <kbd class="calibre13">TensorFlow</kbd>, <kbd class="calibre13"><span><span>h2o</span></span></kbd>, and many others.</p>
<p class="calibre2">In this chapter, we learned the basics of deep learning, many variations of DNNs, <span>the most important deep learning algorithms, and the</span> basic workflow for deep learning. <span>We explored the different packages available in R to handle DNNs.</span></p>
<p class="calibre2"><span>To understand how to build and train a DNN, we analyzed a practical example of DNN implementation with the <kbd class="calibre13">neuralnet</kbd> package. We learned how to normalize data across the various available techniques, to remove data units, allowing you to easily compare data from different locations. We saw how to split the data for the training and testing of the network. We learned to use the <kbd class="calibre13">neuralnet</kbd> function to build and train a multilayered neural network. So we understood how to use the trained network to make predictions and we learned to use the confusion matrix to evaluate model performance.</span></p>
<p class="calibre2">We saw some basics of the <span><span>h2o</span></span> package. Overall, The <kbd class="calibre13">h2o</kbd> package is a highly user-friendly package that can be used to train feed-forward networks or deep autoencoders. It supports distributed computations and provides a web interface. By including the <kbd class="calibre13">h2o</kbd> package like any other package in R, we can do all kinds of modeling and processing of DNNs. The power of <span><span>h2o</span></span> can be utilized by the various features available in the package.</p>
<p class="calibre2"><span>In the next chapter, we will</span> <span>understand what a perceptron is and what are the applications that can be built using the basic perceptron. We will learn a simple perceptron implementation function in R environment. We will also learn how to train and model a MLP . We will discover the linear separable classifier.</span></p>
<p class="calibre2"/>


            </article>

            
        </section>
    </body></html>