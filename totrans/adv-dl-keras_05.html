<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Improved GANs"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Improved GANs</h1></div></div></div><p>Since the introduction<a id="id194" class="indexterm"/> of the <span class="strong"><strong>Generative Adversarial Networks</strong></span> (<span class="strong"><strong>GAN</strong></span>s) in 2014[1], its popularity has rapidly increased. GANs have proved to be a useful generative model that can synthesize new data that look real. Many of the research papers in deep learning that followed, proposed measures to address the difficulties and limitations of the original GAN.</p><p>As we discussed in previous chapters, GANs can be notoriously difficult to train and prone to mode collapse. Mode collapse is a situation where the generator is producing outputs that look the same even though the loss functions are already optimized. In the context of MNIST digits, with mode collapse, the generator may only be producing digits 4 and 9<a id="id195" class="indexterm"/> since they look similar. <span class="strong"><strong>Wasserstein GAN</strong></span> (<span class="strong"><strong>WGAN</strong></span>)[2] addressed these problems by arguing that stable training and mode collapse can be avoided by simply replacing the GAN loss function based on Wasserstein 1 or <span class="strong"><strong>Earth-Mover distance</strong></span> (<span class="strong"><strong>EMD</strong></span>).</p><p>However, the issue of stability is not the only problem of GANs. There<a id="id196" class="indexterm"/> is also the increasing need to improve the perceptive quality of the generated images. <span class="strong"><strong>Least Squares GAN</strong></span> (<span class="strong"><strong>LSGAN</strong></span>)[3] proposed to address both these problems simultaneously. The basic premise is that sigmoid cross entropy loss leads to a vanishing gradient during training. This results in poor image quality. Least squares loss does not induce vanishing gradients. The resulting generated images are of higher perceptive quality when compared to vanilla GAN generated images.</p><p>In the previous chapter, CGAN introduced a method for conditioning the output of the generator. For example, if we wanted to get digit 8, we would <a id="id197" class="indexterm"/>include the conditioning label in the input to the generator. Inspired by CGAN, the <span class="strong"><strong>Auxiliary Classifier GAN</strong></span> (<span class="strong"><strong>ACGAN</strong></span>)[4] proposed a modified conditional algorithm that results in better perceptive quality and diversity of the outputs.</p><p>In summary, the goal of this chapter is to introduce these improved GANs and to present:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The theoretical formulation of the WGAN</li><li class="listitem" style="list-style-type: disc">An understanding of the principles of LSGAN</li><li class="listitem" style="list-style-type: disc">An understanding of the principles of ACGAN</li><li class="listitem" style="list-style-type: disc">Knowledge of how to implement improved GANs - WGAN, LSGAN, and ACGAN using Keras</li></ul></div><div class="section" title="Wasserstein GAN"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec29"/>Wasserstein GAN</h1></div></div></div><p>As we've mentioned before, GANs are <a id="id198" class="indexterm"/>notoriously hard to train. The opposing objectives of the two networks, the discriminator and the generator, can easily cause training instability. The discriminator attempts to correctly classify the fake data from the real data. Meanwhile, the generator tries its best to trick the discriminator. If the discriminator learns faster than the generator, the generator parameters will fail to optimize. On the other hand, if the discriminator learns more slowly, then the gradients may vanish before reaching the generator. In the worst case, if the discriminator is unable to converge, the generator is not going to be able to get any useful feedback.</p><div class="section" title="Distance functions"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec19"/>Distance functions</h2></div></div></div><p>The stability in <a id="id199" class="indexterm"/>training a GAN can be understood by examining its loss functions. To better understand the GAN loss functions, we're going to review the common distance or divergence functions between two probability distributions. Our concern is the distance between <span class="emphasis"><em>p</em></span><sub>data</sub> for true data distribution and <span class="emphasis"><em>p</em></span><sub>g</sub> for generator data distribution. The goal of GANs is to make <span class="emphasis"><em>p</em></span>
<sub>g</sub> → <span class="emphasis"><em>p</em></span><sub>data</sub>. <span class="emphasis"><em>Table 5.1.1</em></span> shows the divergence functions.</p><p>In most maximum likelihood tasks, we'll use <span class="strong"><strong>Kullback-Leibler</strong></span> (<span class="strong"><strong>KL</strong></span>)<a id="id200" class="indexterm"/> divergence or <span class="emphasis"><em>D</em></span><sub>KL</sub> in the loss function as a measure of how far our neural network model prediction is from the true distribution function. As shown in <span class="emphasis"><em>Equation</em></span> <span class="emphasis"><em>5.1.1</em></span>, <span class="emphasis"><em>D</em></span><sub>KL</sub> is not symmetric since <span class="inlinemediaobject"><img src="graphics/B08956_05_001.jpg" alt="Distance functions"/></span>.</p><p>
<span class="strong"><strong>Jensen-Shannon</strong></span> (<span class="strong"><strong>JS</strong></span>) or <span class="emphasis"><em>D</em></span><sub>JS</sub> is a <a id="id201" class="indexterm"/>divergence that is based on <span class="emphasis"><em>D</em></span><sub>KL</sub>. However, unlike <span class="emphasis"><em>D</em></span><sub>KL</sub>, <span class="emphasis"><em>D</em></span><sub>JS</sub> is symmetrical and will be finite. In this section, we'll show that optimizing the GAN loss functions is equivalent to optimizing <span class="emphasis"><em>D</em></span><sub>JS</sub>.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Divergence</p>
</th><th style="text-align: left" valign="bottom">
<p>Expression</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Kullback-Leibler (KL)</p>
<p>5.1.1</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_05_002.jpg" alt="Distance functions"/></div>
<div class="mediaobject"><img src="graphics/B08956_05_03.jpg" alt="Distance functions"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Jensen-Shannon (JS)</p>
<p>5.1.2</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_05_004.jpg" alt="Distance functions"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Earth-Mover Distance (EMD) or Wasserstein 1</p>
<p>5.1.3</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_05_005.jpg" alt="Distance functions"/></div>
<p>where<span class="inlinemediaobject"><img src="graphics/B08956_05_006.jpg" alt="Distance functions"/></span> is the set of all <a id="id202" class="indexterm"/>joint distributions <span class="emphasis"><em>y(x,y)</em></span> whose marginal are <span class="emphasis"><em>p</em></span><sub>data</sub> and <span class="emphasis"><em>p</em></span><sub>g</sub>.</p>

</td></tr></tbody></table></div><p>Table 5.1.1: The divergence functions between two probability distribution functions <span class="emphasis"><em>p</em></span><sub>data</sub> and <span class="emphasis"><em>p</em></span><sub>g</sub></p><div class="mediaobject"><img src="graphics/B08956_05_01.jpg" alt="Distance functions"/><div class="caption"><p>Figure 5.1.1: The EMD is the weighted amount of mass from <span class="strong"><strong>x</strong></span> to be transported in order to match the target distribution, <span class="strong"><strong>y</strong></span>
</p></div></div><p>The intuition behind EMD is<a id="id203" class="indexterm"/> that it is a measure of how much mass <span class="inlinemediaobject"><img src="graphics/B08956_05_008.jpg" alt="Distance functions"/></span> should be transported by <span class="emphasis"><em>d</em></span> = ||<span class="emphasis"><em>x</em></span> - <span class="emphasis"><em>y</em></span>|| for the probability distribution <span class="emphasis"><em>p</em></span><sub>data</sub> in order to match the probability distribution <span class="emphasis"><em>p</em></span><sub>g</sub>. <span class="inlinemediaobject"><img src="graphics/B08956_05_009.jpg" alt="Distance functions"/></span>is a joint distribution in the space of all possible joint distributions <span class="inlinemediaobject"><img src="graphics/B08956_05_010.jpg" alt="Distance functions"/></span>. <span class="inlinemediaobject"><img src="graphics/B08956_05_011.jpg" alt="Distance functions"/></span> is also known as a transport plan to reflect the strategy for transporting masses to match the two probability distributions. There are many possible transport plans given the two probability distributions. Roughly speaking, <span class="emphasis"><em>inf</em></span> indicates a transport plan with the minimum cost.</p><p>For example, <span class="emphasis"><em>Figure 5.1.1</em></span> shows us two simple discrete distributions <span class="inlinemediaobject"><img src="graphics/B08956_05_012.jpg" alt="Distance functions"/></span> and <span class="inlinemediaobject"><img src="graphics/B08956_05_013.jpg" alt="Distance functions"/></span>. <span class="inlinemediaobject"><img src="graphics/B08956_05_014.jpg" alt="Distance functions"/></span> has masses <span class="emphasis"><em>m</em></span>
<span class="emphasis"><em>i</em></span> <span class="emphasis"><em>for</em></span> i = 1, 2, 3 and 4 at locations <span class="emphasis"><em>x</em></span>
<span class="emphasis"><em>i</em></span>
<span class="emphasis"><em> for</em></span> i = 1, 2, 3 and 4. Meanwhile <span class="inlinemediaobject"><img src="graphics/B08956_05_015.jpg" alt="Distance functions"/></span> has masses <span class="emphasis"><em>m</em></span>
<span class="emphasis"><em>i</em></span>
<span class="emphasis"><em> for</em></span> i =1 and 2 at locations <span class="emphasis"><em>y</em></span>
<span class="emphasis"><em>i</em></span>
<span class="emphasis"><em> for</em></span> i = 1 and 2. To match the distribution <span class="inlinemediaobject"><img src="graphics/B08956_05_016.jpg" alt="Distance functions"/></span>, the arrows show the minimum transport plan to move each mass <span class="emphasis"><em>x</em></span>
<span class="emphasis"><em>i</em></span> by <span class="emphasis"><em>d</em></span>
<span class="emphasis"><em>i</em></span>. The EMD is computed as:</p><div class="mediaobject"><img src="graphics/B08956_05_017.jpg" alt="Distance functions"/></div><p>      (Equation 5.1.4)</p><p>In <span class="emphasis"><em>Figure 5.1.1</em></span>, the EMD can be <a id="id204" class="indexterm"/>interpreted as the least amount of work needed to move the pile of dirt <span class="inlinemediaobject"><img src="graphics/B08956_05_018.jpg" alt="Distance functions"/></span> to fill the hole <span class="inlinemediaobject"><img src="graphics/B08956_05_019.jpg" alt="Distance functions"/></span>. While in this example, the <span class="emphasis"><em>inf</em></span> can also be deduced from the figure, in most cases especially in continuous distributions, it is intractable to exhaust all possible transport plans. We will come back to this problem later on in this chapter. In the meantime, we'll show how the GAN loss functions are, in fact, minimizing<a id="id205" class="indexterm"/> the <span class="strong"><strong>Jensen-Shannon</strong></span> (<span class="strong"><strong>JS</strong></span>) divergence.</p></div><div class="section" title="Distance function in GANs"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec20"/>Distance function in GANs</h2></div></div></div><p>We're now going to compute the optimal <a id="id206" class="indexterm"/>discriminator given any generator from the loss function in the previous chapter. We'll recall the following equation:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_020.jpg" alt="Distance function in GANs"/></span>     (Equation 4.1.1)</p><p>Instead of sampling from the noise distribution, the preceding equation can also be expressed as sampling from the generator distribution:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_021.jpg" alt="Distance function in GANs"/></span>      (Equation 5.1.5)</p><p>To find the minimum <span class="inlinemediaobject"><img src="graphics/B08956_05_022.jpg" alt="Distance function in GANs"/></span>:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_023.jpg" alt="Distance function in GANs"/></span>
          (Equation 5.1.6)</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_024.jpg" alt="Distance function in GANs"/></span>          (Equation 5.1.7)</p><p>The term inside the<a id="id207" class="indexterm"/> integral is in the form of <span class="emphasis"><em>y</em></span> → <span class="emphasis"><em>a</em></span> log <span class="emphasis"><em>y</em></span> + <span class="emphasis"><em>b</em></span> log(1 - <span class="emphasis"><em>y</em></span>) which has a known maximum value at <span class="inlinemediaobject"><img src="graphics/B08956_05_025.jpg" alt="Distance function in GANs"/></span> for <span class="inlinemediaobject"><img src="graphics/B08956_05_026.jpg" alt="Distance function in GANs"/></span>, for any <span class="inlinemediaobject"><img src="graphics/B08956_05_027.jpg" alt="Distance function in GANs"/></span> not including {0,0}. Since the integral does not change the location of the maximum value (or the minimum value of <span class="inlinemediaobject"><img src="graphics/B08956_05_028.jpg" alt="Distance function in GANs"/></span>) for this expression, the optimal discriminator is:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_029.jpg" alt="Distance function in GANs"/></span>          (Equation 5.1.8)</p><p>Consequently, the loss function is given the optimal discriminator:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_030.jpg" alt="Distance function in GANs"/></span>          (Equation 5.1.9)</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_031.jpg" alt="Distance function in GANs"/></span>          (Equation 5.1.10)</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_032.jpg" alt="Distance function in GANs"/></span>          (Equation 5.1.11)</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_033.jpg" alt="Distance function in GANs"/></span>          (Equation 5.1.12)</p><p>We can observe from <a id="id208" class="indexterm"/>
<span class="emphasis"><em>Equation 5.1.12</em></span> that the loss function of the optimal discriminator is a constant minus twice the Jensen-Shannon divergence between the true distribution, <span class="emphasis"><em>p</em></span><sub>data</sub>, and any generator distribution, <span class="emphasis"><em>p</em></span><sub>g</sub>. Minimizing <span class="inlinemediaobject"><img src="graphics/B08956_05_034.jpg" alt="Distance function in GANs"/></span> implies maximizing <span class="inlinemediaobject"><img src="graphics/B08956_05_035.jpg" alt="Distance function in GANs"/></span> or the discriminator must correctly classify fake from real data.</p><p>Meanwhile, we can safely argue that the optimal generator is when the generator distribution is equal to the true data distribution: </p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_036.jpg" alt="Distance function in GANs"/></span>     (Equation 5.1.13)</p><p>This makes sense since the objective of the generator is to fool the discriminator by learning the true data distribution. Effectively, we can arrive at the optimal generator by minimizing <span class="emphasis"><em>D</em></span><sub>JS</sub><span class="emphasis"><em>,</em></span> or by making <span class="emphasis"><em>p</em></span><sub>g</sub> → <span class="emphasis"><em>p</em></span><sub>data</sub>. Given an optimal generator, the optimal discriminator is <span class="inlinemediaobject"><img src="graphics/B08956_05_037.jpg" alt="Distance function in GANs"/></span> with <span class="inlinemediaobject"><img src="graphics/B08956_05_038.jpg" alt="Distance function in GANs"/></span>.</p><div class="mediaobject"><img src="graphics/B08956_05_02.jpg" alt="Distance function in GANs"/><div class="caption"><p>Figure 5.1.2: An example of two distributions with no overlap. <span class="inlinemediaobject"><img src="graphics/B08956_05_039.jpg" alt="Distance function in GANs"/></span> for <span class="emphasis"><em>p</em></span><sub>g</sub></p></div></div><p>The problem is that when the two<a id="id209" class="indexterm"/> distributions have no overlap, there's no smooth function that will help to close the gap between them. Training the GANs will not converge by gradient descent. For example, let's suppose:</p><p>
<span class="emphasis"><em>p</em></span><sub>data</sub> =(<span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>y</em></span>) where <span class="inlinemediaobject"><img src="graphics/B08956_05_040.jpg" alt="Distance function in GANs"/></span>     (Equation 5.1.14)</p><p>
<span class="emphasis"><em>p</em></span><sub>g</sub> = (<span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>y</em></span>) where <span class="inlinemediaobject"><img src="graphics/B08956_05_041.jpg" alt="Distance function in GANs"/></span>     (Equation 5.1.15)</p><p>As shown in <span class="emphasis"><em>Figure 5.1.2</em></span>. <span class="emphasis"><em>U</em></span>(0,1) is the uniform distribution. The divergence for each distance function is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><div class="mediaobject"><img src="graphics/B08956_05_42.jpg" alt="Distance function in GANs"/></div></li><li class="listitem" style="list-style-type: disc"><div class="mediaobject"><img src="graphics/B08956_05_043.jpg" alt="Distance function in GANs"/></div></li><li class="listitem" style="list-style-type: disc"><div class="mediaobject"><img src="graphics/B08956_05_44.jpg" alt="Distance function in GANs"/></div></li><li class="listitem" style="list-style-type: disc"><div class="mediaobject"><img src="graphics/B08956_05_045.jpg" alt="Distance function in GANs"/></div></li></ul></div><p>Since <span class="emphasis"><em>D</em></span><sub>JS</sub> is a constant, the GAN <a id="id210" class="indexterm"/>will not have a sufficient gradient to drive <span class="emphasis"><em>p</em></span><sub>g</sub> → <span class="emphasis"><em>p</em></span><sub>data</sub>. We'll also find that <span class="emphasis"><em>D</em></span><sub>KL</sub> or reverse <span class="emphasis"><em>D</em></span><sub>KL</sub> is not helpful either. However, with <span class="emphasis"><em>W</em></span>(<span class="emphasis"><em>p</em></span><sub>data</sub>,<span class="emphasis"><em>p</em></span><sub>g</sub>) we can have a smooth function in order to attain <span class="emphasis"><em>p</em></span><sub>g</sub> → <span class="emphasis"><em>p</em></span><sub>data</sub> by gradient descent. EMD or Wasserstein 1 seems to be a more logical loss function in order to optimize GANs since <span class="emphasis"><em>D</em></span><sub>JS</sub> fails in situations when two distributions have minimal to no overlap.</p><p>For further understanding, an excellent discussion on distance functions can be found at <a class="ulink" href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-W">https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-W</a>
<a class="ulink" href="http://GAN.html">GAN.html</a>.</p></div><div class="section" title="Use of Wasserstein loss"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec21"/>Use of Wasserstein loss</h2></div></div></div><p>Before using EMD or <a id="id211" class="indexterm"/>Wasserstein 1, there is one more problem to overcome. It is intractable to exhaust the space of <span class="inlinemediaobject"><img src="graphics/B08956_05_046.jpg" alt="Use of Wasserstein loss"/></span> to find <span class="inlinemediaobject"><img src="graphics/B08956_05_047.jpg" alt="Use of Wasserstein loss"/></span>. The proposed solution is to use its Kantorovich-Rubinstein dual:</p><div class="mediaobject"><img src="graphics/B08956_05_048.jpg" alt="Use of Wasserstein loss"/></div><p>     (Equation 5.1.16)</p><p>Equivalently, EMD, <span class="inlinemediaobject"><img src="graphics/B08956_05_049.jpg" alt="Use of Wasserstein loss"/></span>, is the supremum (roughly, maximum value) over all the <span class="emphasis"><em>K</em></span>-Lipschitz functions: <span class="inlinemediaobject"><img src="graphics/B08956_05_050.jpg" alt="Use of Wasserstein loss"/></span>. <span class="emphasis"><em>K</em></span>-Lipschitz functions satisfy the constraint:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_51.jpg" alt="Use of Wasserstein loss"/></span>     (Equation 5.1.17)</p><p>For all <span class="inlinemediaobject"><img src="graphics/B08956_05_052.jpg" alt="Use of Wasserstein loss"/></span>, the <span class="emphasis"><em>K</em></span>-Lipschitz functions have bounded derivatives and almost always continuously differentiable (for example, <span class="emphasis"><em>f</em></span>(<span class="emphasis"><em>x</em></span>), = |<span class="emphasis"><em>x</em></span>| has bounded derivatives and continuous but not differentiable at <span class="emphasis"><em>x</em></span> = 0).</p><p>
<span class="emphasis"><em>Equation 5.1.16</em></span> can be solved by <a id="id212" class="indexterm"/>finding a family of <span class="emphasis"><em>K</em></span>-Lipschitz functions <span class="inlinemediaobject"><img src="graphics/B08956_05_053.jpg" alt="Use of Wasserstein loss"/></span>:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_54.jpg" alt="Use of Wasserstein loss"/></span>     (Equation 5.1.18)</p><p>In the context of GANs, <span class="emphasis"><em>Equation </em></span>
<span class="emphasis"><em>5.1.18</em></span> can be rewritten by sampling from <span class="emphasis"><em>z</em></span>-noise distribution and replacing <span class="emphasis"><em>f</em></span><sub>w</sub> by the discriminator function, <span class="emphasis"><em>D</em></span><sub>w</sub>:</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_55.jpg" alt="Use of Wasserstein loss"/></span>     (Equation 5.1.19)</p><p>We use the bold letter to highlight the generality to multi-dimensional samples. The final problem we face is how to find the family of functions <span class="inlinemediaobject"><img src="graphics/B08956_05_056.jpg" alt="Use of Wasserstein loss"/></span>. The proposed solution we're going to go over is that at every gradient update, the weights of the discriminator, <span class="emphasis"><em>w</em></span>, are clipped between lower and upper bounds, (for example, -0.0,1 and 0.01):</p><p><span class="inlinemediaobject"><img src="graphics/B08956_05_57.jpg" alt="Use of Wasserstein loss"/></span>     (Equation 5.1.20)</p><p>The small values of <span class="emphasis"><em>w</em></span> constrains the discriminator to a compact parameter space thus ensuring Lipschitz continuity.</p><p>We can use <span class="emphasis"><em>Equation 5.1.19</em></span> as the basis of our new GAN loss functions. EMD or Wasserstein 1 is the loss function that the generator aims to minimize, and the cost function that the discriminator tries to maximize (or minimize -<span class="emphasis"><em>W</em></span>(<span class="emphasis"><em>p</em></span><sub>data</sub>,<span class="emphasis"><em>p</em></span><sub>g</sub>)):</p><div class="mediaobject"><img src="graphics/B08956_05_058.jpg" alt="Use of Wasserstein loss"/></div><p>     (Equation 5.1.21)</p><div class="mediaobject"><img src="graphics/B08956_05_59.jpg" alt="Use of Wasserstein loss"/></div><p>     (Equation 5.1.22)</p><p>In the generator loss function, the first term disappears since it is not directly optimizing with respect to the real data.</p><p>Following table shows the<a id="id213" class="indexterm"/> difference between the loss functions of GAN and WGAN. For conciseness, we've simplified the notation for <span class="inlinemediaobject"><img src="graphics/B08956_05_060.jpg" alt="Use of Wasserstein loss"/></span>, and <span class="inlinemediaobject"><img src="graphics/B08956_05_061.jpg" alt="Use of Wasserstein loss"/></span>. These loss functions are used in training the WGAN as shown in <span class="emphasis"><em>Algorithm</em></span> <span class="emphasis"><em>5.1.1</em></span>. <span class="emphasis"><em>Figure 5.1.3</em></span> illustrates that the WGAN model is practically the same as the DCGAN model except for the fake/true data labels and loss functions:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Network</p>
</th><th style="text-align: left" valign="bottom">
<p>Loss Functions</p>
</th><th style="text-align: left" valign="bottom">
<p>Equation</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>GAN</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_05_062.jpg" alt="Use of Wasserstein loss"/></div>
<div class="mediaobject"><img src="graphics/B08956_05_063.jpg" alt="Use of Wasserstein loss"/></div>
</td><td style="text-align: left" valign="top">
<p>4.1.1</p>
<p>4.1.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>WGAN</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_05_064.jpg" alt="Use of Wasserstein loss"/></div>
<div class="mediaobject"><img src="graphics/B08956_05_065.jpg" alt="Use of Wasserstein loss"/></div>
<div class="mediaobject"><img src="graphics/B08956_05_066.jpg" alt="Use of Wasserstein loss"/></div>
</td><td style="text-align: left" valign="top">
<p>5.1.21</p>
<p>5.1.22</p>
<p>5.1.20</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p>Table 5.1.1: A comparison between the loss functions of GAN and WGAN</p></blockquote></div><p>
<span class="strong"><strong>Algorithm 5.1.1 WGAN</strong></span>
</p><p>The values of the parameters are <span class="inlinemediaobject"><img src="graphics/B08956_05_067.jpg" alt="Use of Wasserstein loss"/></span>, <span class="emphasis"><em>c</em></span> = 0.01 <span class="emphasis"><em>m</em></span> = 64, and <span class="emphasis"><em>n</em></span><sub>critic</sub> = 5.</p><p>
<span class="emphasis"><em>Require</em></span>: <span class="inlinemediaobject"><img src="graphics/B08956_05_068.jpg" alt="Use of Wasserstein loss"/></span>, the learning<a id="id214" class="indexterm"/> rate. <span class="emphasis"><em>c</em></span>, the clipping parameter. <span class="emphasis"><em>m</em></span>, the batch size. <span class="emphasis"><em>n</em></span><sub>critic</sub>, the number of the critic (discriminator) iterations per generator iteration.</p><p>
<span class="emphasis"><em>Require</em></span>: <span class="emphasis"><em>w</em></span><sub>0</sub>, initial critic (discriminator) parameters. <span class="inlinemediaobject"><img src="graphics/B08956_05_069.jpg" alt="Use of Wasserstein loss"/></span>, initial generator parameters</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><code class="literal">while</code> <div class="mediaobject"><img src="graphics/B08956_05_070.jpg" alt="Use of Wasserstein loss"/></div><p> has not converged <code class="literal">do</code>
</p></li><li class="listitem"><code class="literal">    for</code> <span class="emphasis"><em>t</em></span> = 1, …, <span class="emphasis"><em>n</em></span><sub>critic</sub><code class="literal">do</code></li><li class="listitem"><code class="literal">        </code>Sample a batch <div class="mediaobject"><img src="graphics/B08956_05_071.jpg" alt="Use of Wasserstein loss"/></div><p> from the real data</p></li><li class="listitem"><code class="literal">        </code>Sample a batch <div class="mediaobject"><img src="graphics/B08956_05_72.jpg" alt="Use of Wasserstein loss"/></div><p> from the uniform noise distribution</p></li><li class="listitem"><code class="literal">        </code><div class="mediaobject"><img src="graphics/B08956_05_073.jpg" alt="Use of Wasserstein loss"/></div><p>, compute the <code class="literal">        </code>discriminator gradients</p></li><li class="listitem"><code class="literal">        </code><div class="mediaobject"><img src="graphics/B08956_05_074.jpg" alt="Use of Wasserstein loss"/></div><p>, update the discriminator parameters</p></li><li class="listitem"><code class="literal">        </code><div class="mediaobject"><img src="graphics/B08956_05_075.jpg" alt="Use of Wasserstein loss"/></div><p>, clip discriminator weights</p></li><li class="listitem"><code class="literal">    end for</code></li><li class="listitem"><code class="literal">    </code>Sample a batch <div class="mediaobject"><img src="graphics/B08956_05_076.jpg" alt="Use of Wasserstein loss"/></div><p> from the uniform noise distribution</p></li><li class="listitem"><code class="literal">    </code><div class="mediaobject"><img src="graphics/B08956_05_077.jpg" alt="Use of Wasserstein loss"/></div><p>, compute the generator gradients</p></li><li class="listitem"><code class="literal">    </code><div class="mediaobject"><img src="graphics/B08956_05_078.jpg" alt="Use of Wasserstein loss"/></div><p>, update generator parameters</p></li><li class="listitem"><code class="literal">end while</code></li></ol></div><div class="mediaobject"><img src="graphics/B08956_05_03.jpg" alt="Use of Wasserstein loss"/><div class="caption"><p>Figure 5.1.3: Top: Training the WGAN discriminator requires fake data from the generator and real data from the true distribution. Bottom: Training the WGAN generator requires fake data from the generator pretending to be real.</p></div></div><p>Similar to GANs, WGAN alternately trains the discriminator and generator (through adversarial). However, in WGAN, the discriminator (also called the critic) trains <span class="emphasis"><em>n</em></span><sub>critic</sub> iterations (Lines 2 to 8) before training the generator for one iteration (Lines 9 to 11). This in contrast to GANs with an equal number of training iteration for both discriminator and generator. Training the discriminator means learning the parameters (weights and biases) of the discriminator. This requires sampling a batch from the real data (Line 3) and a batch from the fake data (Line 4) and computing the gradient of discriminator parameters (Line 5) after feeding the sampled data to the discriminator network. The discriminator parameters are optimized using RMSProp (Line 6). Both lines 5 and 6 are the optimization of <span class="emphasis"><em>Equation 5.1.21</em></span>. Adam was found to be unstable in WGAN.</p><p>Lastly, the Lipschitz constraint in the EM distance optimization is imposed by clipping the discriminator parameters (Line 7). Line 7 is the implementation of <span class="emphasis"><em>Equation 5.1.20</em></span>. After <span class="emphasis"><em>n</em></span><sub>critic</sub> iterations of <a id="id215" class="indexterm"/>discriminator training, the discriminator parameters are frozen. The generator training starts by sampling a batch of fake data (Line 9). The sampled data is labeled as real (1.0) trying to fool the discriminator network. The generator gradients are computed in Line 10 and optimized using the RMSProp in Line 11. Lines 10 and 11 perform gradients update to optimize <span class="emphasis"><em>Equation 5.1.22</em></span>.</p><p>After training the generator, the discriminator parameters are unfrozen, and another <span class="emphasis"><em>n</em></span><sub>critic</sub> discriminator training iterations start. We should take note that there is no need to freeze the generator parameters during discriminator training as the generator is only involved in the fabrication of data. Similar to GANs, the discriminator can be trained as a separate network. However, training the generator always requires the participation of the discriminator through the adversarial network since the loss is computed from the output of the generator network.</p><p>Unlike GAN, in WGAN real data are labeled 1.0 while fake data are labeled -1.0 as a workaround in computing the gradient in Line 5. Lines 5-6 and 10-11 perform gradient update to optimize <span class="emphasis"><em>Equations</em></span> <span class="emphasis"><em>5.1.21</em></span> and <span class="emphasis"><em>5.1.22</em></span> respectively. Each term in Lines 5 and 10 is modelled as:</p><div class="mediaobject"><img src="graphics/B08956_05_079.jpg" alt="Use of Wasserstein loss"/></div><p>     (Equation 5.1.23)</p><p>Where <span class="emphasis"><em>y</em></span><sub>label</sub> = 1.0 for the real data and <span class="emphasis"><em>y</em></span><sub>label</sub> = -1.0 for the fake data. We removed the superscript (i) for simplicity of the notation. For discriminator, WGAN increases <span class="inlinemediaobject"><img src="graphics/B08956_05_80.jpg" alt="Use of Wasserstein loss"/></span> to minimize the loss function when training using the real data. When training using fake data, WGAN decreases <span class="inlinemediaobject"><img src="graphics/B08956_05_081.jpg" alt="Use of Wasserstein loss"/></span> to minimize the loss function. For the generator, WGAN increases <span class="inlinemediaobject"><img src="graphics/B08956_05_082.jpg" alt="Use of Wasserstein loss"/></span> as to minimize the loss function when the fake data is labeled as real during training. Note that <span class="emphasis"><em>y</em></span><sub>label</sub> has no direct contribution in the loss function other than its sign. In Keras, <span class="emphasis"><em>Equation 5.1.23</em></span> is implemented as:</p><div class="informalexample"><pre class="programlisting">def wasserstein_loss(y_label, y_pred):
    return -K.mean(y_label * y_pred)</pre></div></div><div class="section" title="WGAN implementation using Keras"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec22"/>WGAN implementation using Keras</h2></div></div></div><p>To implement WGAN within <a id="id216" class="indexterm"/>Keras, we can reuse the <a id="id217" class="indexterm"/>DCGAN implementation of GANs, something we introduced in the previous chapter. The DCGAN builder and utility functions are implemented in <code class="literal">gan.py</code> in <code class="literal">lib</code> folder as a module.</p><p>The functions include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">generator()</code>: A generator model builder</li><li class="listitem" style="list-style-type: disc"><code class="literal">discriminator()</code>: Discriminator model builder</li><li class="listitem" style="list-style-type: disc"><code class="literal">train()</code>: DCGAN trainer</li><li class="listitem" style="list-style-type: disc"><code class="literal">plot_images()</code>: Generic generator outputs plotter</li><li class="listitem" style="list-style-type: disc"><code class="literal">test_generator()</code>: Generic generator test utility</li></ul></div><p>As shown in <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>5.1.1</em></span>, we can build a discriminator by simply calling:</p><div class="informalexample"><pre class="programlisting">discriminator = gan.discriminator(inputs, activation='linear')</pre></div><p>WGAN uses linear output activation. For the generator, we execute:</p><div class="informalexample"><pre class="programlisting">generator = gan.generator(inputs, image_size)</pre></div><p>The overall network model in Keras is similar to the one seen in <span class="emphasis"><em>Figure 4.2.1</em></span> for DCGAN.</p><p>
<span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>5.1.1</em></span> highlights the use of the RMSprop optimizer and Wasserstein loss function. The hyper-parameters in <span class="emphasis"><em>Algorithm</em></span> <span class="emphasis"><em>5.1.1</em></span> are used during training. <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>5.1.2</em></span> is the training function that closely follows the <span class="emphasis"><em>algorithm</em></span>. However, there is a minor tweak in the training of the discriminator. Instead of training the weights in a single combined batch of both real and fake data, we'll train with one batch of real data first and then a batch of fake data. This tweak will prevent the gradient from vanishing because of the opposite sign in the label of real and fake data and the small magnitude of weights due to clipping.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>The complete code is available on GitHub:</p><p>
<a class="ulink" href="https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras">https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras</a>
</p></div></div><p>
<span class="emphasis"><em>Figure 5.1.4</em></span> shows the evolution of the WGAN outputs on MNIST dataset.</p><p>Listing 5.1.1, <code class="literal">wgan-mnist-5.1.2.py</code>. The WGAN model instantiation and training. Both discriminator and generator use Wassertein 1 loss, <code class="literal">wasserstein_loss()</code>:</p><div class="informalexample"><pre class="programlisting">def build_and_train_models():
    # load MNIST dataset
    (x_train, _), (_, _) = mnist.load_data()

    # reshape data for CNN as (28, 28, 1) and normalize
    image_size = x_train.shape[1]
    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])
    x_train = x_train.astype('float32') / 255

    model_name = "wgan_mnist"
    # network parameters
    # the latent or z vector is 100-dim
    latent_size = 100
<span class="strong"><strong>    # hyper parameters from WGAN paper [2]</strong></span>
<span class="strong"><strong>    n_critic = 5</strong></span>
<span class="strong"><strong>    clip_value = 0.01</strong></span>
    batch_size = 64
    lr = 5e-5
    train_steps = 40000
    input_shape = (image_size, image_size, 1)

    # build discriminator model
    inputs = Input(shape=input_shape, name='discriminator_input')
<span class="strong"><strong>    # WGAN uses linear activation in paper [2]</strong></span>
<span class="strong"><strong>    discriminator = gan.discriminator(inputs, activation='linear')</strong></span>
<span class="strong"><strong>    optimizer = RMSprop(lr=lr)</strong></span>
<span class="strong"><strong>    # WGAN discriminator uses wassertein loss</strong></span>
<span class="strong"><strong>    discriminator.compile(loss=wasserstein_loss,</strong></span>
<span class="strong"><strong>                          optimizer=optimizer,</strong></span>
<span class="strong"><strong>                          metrics=['accuracy'])</strong></span>
    discriminator.summary()

    # build generator model
    input_shape = (latent_size, )
    inputs = Input(shape=input_shape, name='z_input')
    generator = gan.generator(inputs, image_size)
    generator.summary()

    # build adversarial model = generator + discriminator
    # freeze the weights of discriminator 
    # during adversarial training
    discriminator.trainable = False
    adversarial = Model(inputs,
                        discriminator(generator(inputs)),
                        name=model_name)
<span class="strong"><strong>    adversarial.compile(loss=wasserstein_loss,</strong></span>
<span class="strong"><strong>                        optimizer=optimizer,</strong></span>
<span class="strong"><strong>                        metrics=['accuracy'])</strong></span>
    adversarial.summary()

    # train discriminator and adversarial networks
    models = (generator, discriminator, adversarial)
    params = (batch_size,
              latent_size,
              n_critic,
              clip_value,
              train_steps,
              model_name)
    train(models, x_train, params)</pre></div><p>Listing 5.1.2, <code class="literal">wgan-mnist-5.1.2.py</code>. The training procedure for WGAN closely follows <span class="emphasis"><em>Algorithm</em></span> <span class="emphasis"><em>5.1.1</em></span>. The discriminator is trained <span class="emphasis"><em>n</em></span><sub>critic</sub> iterations per 1 generator training iteration:</p><div class="informalexample"><pre class="programlisting">def train(models, x_train, params):
    """Train the Discriminator and Adversarial Networks

    Alternately train Discriminator and Adversarial networks by batch.
    Discriminator is trained first with properly labeled real and fake images
    for n_critic times.
    Discriminator weights are clipped as a requirement of Lipschitz constraint.
    Generator is trained next (via Adversarial) with fake images
    pretending to be real.
    Generate sample images per save_interval

    # Arguments
        models (list): Generator, Discriminator, Adversarial models
        x_train (tensor): Train images
        params (list) : Networks parameters

    """
    # the GAN models
    generator, discriminator, adversarial = models
    # network parameters
    (batch_size, latent_size, n_critic,
            clip_value, train_steps, model_name) = params
    # the generator image is saved every 500 steps
    save_interval = 500
    # noise vector to see how the generator output 
    # evolves during training
    noise_input = np.random.uniform(-1.0, 1.0, size=[16,    
                                    latent_size])
    # number of elements in train dataset
    train_size = x_train.shape[0]
    # labels for real data
    real_labels = np.ones((batch_size, 1))
    for i in range(train_steps):
        # train discriminator n_critic times
        loss = 0
        acc = 0
        for _ in range(n_critic):
            # train the discriminator for 1 batch
            # 1 batch of real (label=1.0) and 
            # fake images (label=-1.0)
            # randomly pick real images from dataset
            rand_indexes = np.random.randint(0,
                                             train_size,         
                                             size=batch_size)
            real_images = x_train[rand_indexes]
            # generate fake images from noise using generator
            # generate noise using uniform distribution
            noise = np.random.uniform(-1.0,
                                      1.0,
                                      size=[batch_size,                    
                                      latent_size])
            fake_images = generator.predict(noise)

            # train the discriminator network
            # real data label=1, fake data label=-1
            # instead of 1 combined batch of real and fake images,
            # train with 1 batch of real data first, then 1 batch
            # of fake images.
            # this tweak prevents the gradient from vanishing 
            # due to opposite signs of real and
            # fake data labels (i.e. +1 and -1) and
            # small magnitude of weights due to clipping.
            real_loss, real_acc =                      
                         discriminator.train_on_batch(real_images,                                   
                                                      real_labels)
            fake_loss, fake_acc = 
                         discriminator.train_on_batch(fake_images,
                                                      real_labels)
            # accumulate average loss and accuracy
            loss += 0.5 * (real_loss + fake_loss)
            acc += 0.5 * (real_acc + fake_acc)

            # clip discriminator weights to satisfy 
            # Lipschitz constraint
            for layer in discriminator.layers:
                weights = layer.get_weights()
                weights = [np.clip(weight,
                                   -clip_value,
                                   clip_value) for weight in weights]
                layer.set_weights(weights)

        # average loss and accuracy per n_critic 
        # training iterations
        loss /= n_critic
        acc /= n_critic
        log = "%d: [discriminator loss: %f, acc: %f]" % (i, loss, acc)

        # train the adversarial network for 1 batch
        # 1 batch of fake images with label=1.0
        # since the discriminator weights are 
        # frozen in adversarial network
        # only the generator is trained
        # generate noise using uniform distribution
        noise = np.random.uniform(-1.0, 1.0, 
                                  size=[batch_size, latent_size])
        # train the adversarial network
        # note that unlike in discriminator training,
        # we do not save the fake images in a variable
        # the fake images go to the discriminator input 
        # of the adversarial for classification
        # fake images are labelled as real
        # log the loss and accuracy
        loss, acc = adversarial.train_on_batch(noise, real_labels)
        log = "%s [adversarial loss: %f, acc: %f]" % (log, loss, acc)
        print(log)
        if (i + 1) % save_interval == 0:
            if (i + 1) == train_steps:
                show = True
            else:
                show = False

            # plot generator images on a periodic basis
            gan.plot_images(generator,
                            noise_input=noise_input,
                            show=show,
                            step=(i + 1),
                            model_name=model_name)

    # save the model after training the generator
    # the trained generator can be reloaded for future 
    # MNIST digit generation
    generator.save(model_name + ".h5")</pre></div><div class="mediaobject"><img src="graphics/B08956_05_04.jpg" alt="WGAN implementation using Keras"/><div class="caption"><p>Figure 5.1.4: The sample outputs of WGAN vs. training steps. WGAN does not suffer mode collapse in all the outputs during training and testing.</p></div></div><p>WGAN is stable even under network configuration changes. For example, DCGAN is known to be unstable when batch normalization is inserted before the ReLU in the discriminator network. The <a id="id218" class="indexterm"/>same configuration is stable in <a id="id219" class="indexterm"/>WGAN.</p><p>Following figure shows us the outputs of both DCGAN and WGAN with batch normalization on the discriminator network:</p><div class="mediaobject"><img src="graphics/B08956_05_05.jpg" alt="WGAN implementation using Keras"/><div class="caption"><p>Figure 5.1.5: A comparison of the output of the DCGAN (Left) and WGAN (Right) when batch normalization is inserted before the ReLU activation in the discriminator network</p></div></div><p>Similar to the GAN training in the previous chapter, the trained model is saved on a file after 40,000 train steps. I would encourage you to run the trained generator model to see new synthesized MNIST digits images:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 wgan-mnist-5.1.2.py --generator=wgan_mnist.h5</strong></span>
</pre></div></div></div></div>
<div class="section" title="Least-squares GAN (LSGAN)"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Least-squares GAN (LSGAN)</h1></div></div></div><p>As<a id="id220" class="indexterm"/> discussed in the previous section, the original GAN is difficult to train. The problem arises when the GAN optimizes its loss function; it's actually optimizing the <span class="emphasis"><em>Jensen-Shannon</em></span> divergence, <span class="emphasis"><em>D</em></span><sub>JS</sub>. It is difficult to optimize <span class="emphasis"><em>D</em></span><sub>JS</sub> when there is little to no overlap between two distribution functions.</p><p>WGAN proposed to address the problem by using the EMD or Wasserstein 1 loss function which has a smooth differentiable function even when there is little or no overlap between the two distributions. However, WGAN is not concerned with the generated image quality. Apart from stability issues, there are still areas of improvement in terms of perceptive quality in the generated images of the original GAN. LSGAN theorizes that the twin problems can be solved simultaneously.</p><p>LSGAN proposes the least squares loss. <span class="emphasis"><em>Figure 5.2.1</em></span> demonstrates why the use of a sigmoid cross entropy loss in the GAN results in poorly generated data quality. Ideally, the fake samples distribution should be as close as possible to the true samples' distribution. However, for GANs, once<a id="id221" class="indexterm"/> the fake samples are already on the correct side of the decision boundary, the gradients vanish.</p><p>This prevents the generator from having enough motivation to improve the quality of the generated fake data. Fake samples far from the decision boundary will no longer attempt to move closer to the true samples' distribution. Using the least squares loss function, the gradients do not vanish as long as the fake samples distribution is far from the real samples' distribution. The generator will strive to improve its estimate of real density distribution even if the fake samples are already on the correct side of the decision boundary:</p><div class="mediaobject"><img src="graphics/B08956_05_06.jpg" alt="Least-squares GAN (LSGAN)"/><div class="caption"><p>Figure 5.2.1: Both real and fake samples distributions divided by respective decision boundaries: Sigmoid and Least squares</p></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Network</p>
</th><th style="text-align: left" valign="bottom">
<p>Loss Functions</p>
</th><th style="text-align: left" valign="bottom">
<p>Equation</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>GAN</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_05_083.jpg" alt="Least-squares GAN (LSGAN)"/></div>
<div class="mediaobject"><img src="graphics/B08956_05_084.jpg" alt="Least-squares GAN (LSGAN)"/></div>
</td><td style="text-align: left" valign="top">
<p>4.1.1</p>
<p>4.1.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LSGAN</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_05_085.jpg" alt="Least-squares GAN (LSGAN)"/></div>
<div class="mediaobject"><img src="graphics/B08956_05_086.jpg" alt="Least-squares GAN (LSGAN)"/></div>
</td><td style="text-align: left" valign="top">
<p>5.2.1</p>
<p>5.2.2</p>
</td></tr></tbody></table></div><p>Table 5.2.1: A comparison between the loss functions of GAN and LSGAN</p><p>The preceding table shows the<a id="id222" class="indexterm"/> comparison of the loss functions between GAN and LSGAN. Minimizing <span class="emphasis"><em>Equation 5.2.1</em></span> or the discriminator loss function implies that the MSE between real data classification and true label 1.0 should be close to zero. In addition, the MSE between the fake data classification and the true label 0.0 should be close to zero.</p><p>Similar to GANs, the LSGAN discriminator is trained to classify real from fake data samples. Minimizing <span class="emphasis"><em>Equation 5.2.2</em></span> means fooling the discriminator to think that the generated fake sample data are real with label 1.0.</p><p>Implementing LSGAN using the DCGAN code in the previous chapter as the basis requires few changes only. As shown in <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>5.2.1</em></span>, the discriminator sigmoid activation is removed. The discriminator is built by calling:</p><div class="informalexample"><pre class="programlisting">discriminator = gan.discriminator(inputs, activation=None)</pre></div><p>The generator is similar to the original DCGAN:</p><div class="informalexample"><pre class="programlisting">generator = gan.generator(inputs, image_size)</pre></div><p>Both the discriminator and adversarial loss functions are replaced by <code class="literal">mse</code>. All the network parameters are the same as in DCGAN. The network model of LSGAN in Keras is similar to <span class="emphasis"><em>Figure 4.2.1</em></span> except that there is no linear or output activation. The training process is similar to that seen in DCGAN and is provided by the utility function:</p><div class="informalexample"><pre class="programlisting">gan.train(models, x_train, params)</pre></div><p>Listing 5.2.1, <code class="literal">lsgan-mnist-5.2.1.py</code> shows how the discriminator and generator are the same in DCGAN except for the<a id="id223" class="indexterm"/> discriminator output activation and the use of MSE loss function:</p><div class="informalexample"><pre class="programlisting">def build_and_train_models():
    # MNIST dataset
    (x_train, _), (_, _) = mnist.load_data()

    # reshape data for CNN as (28, 28, 1) and normalize
    image_size = x_train.shape[1]
    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])
    x_train = x_train.astype('float32') / 255

    model_name = "lsgan_mnist"
    # network parameters
    # the latent or z vector is 100-dim
    latent_size = 100
    input_shape = (image_size, image_size, 1)
    batch_size = 64
    lr = 2e-4
    decay = 6e-8
    train_steps = 40000

    # build discriminator model
    inputs = Input(shape=input_shape, name='discriminator_input')
<span class="strong"><strong>    discriminator = gan.discriminator(inputs, activation=None)</strong></span>
    # [1] uses Adam, but discriminator converges 
    # easily with RMSprop
    optimizer = RMSprop(lr=lr, decay=decay)
<span class="strong"><strong>    # LSGAN uses MSE loss [2]</strong></span>
<span class="strong"><strong>    discriminator.compile(loss='mse',</strong></span>
<span class="strong"><strong>                            optimizer=optimizer,</strong></span>
<span class="strong"><strong>                            metrics=['accuracy'])</strong></span>
    discriminator.summary()

    # build generator model
    input_shape = (latent_size, )
    inputs = Input(shape=input_shape, name='z_input')
    generator = gan.generator(inputs, image_size)
    generator.summary()

    # build adversarial model = generator + discriminator
    optimizer = RMSprop(lr=lr*0.5, decay=decay*0.5)
    # freeze the weights of discriminator 
    # during adversarial training
    discriminator.trainable = False
    adversarial = Model(inputs,
                        discriminator(generator(inputs)),
                        name=model_name)
<span class="strong"><strong>    # LSGAN uses MSE loss [2]</strong></span>
<span class="strong"><strong>    adversarial.compile(loss='mse',</strong></span>
<span class="strong"><strong>                          optimizer=optimizer,</strong></span>
<span class="strong"><strong>                          metrics=['accuracy'])</strong></span>
    adversarial.summary()

    # train discriminator and adversarial networks
    models = (generator, discriminator, adversarial)
    params = (batch_size, latent_size, train_steps, model_name)
<span class="strong"><strong>    gan.train(models, x_train, params)</strong></span>
</pre></div><p>Following figure shows<a id="id224" class="indexterm"/> generated samples after training LSGAN using the MNIST dataset for 40,000 training steps. The output images have better perceptual quality compared to <span class="emphasis"><em>Figure 4.2.1</em></span> in DCGAN:</p><div class="mediaobject"><img src="graphics/B08956_05_07.jpg" alt="Least-squares GAN (LSGAN)"/><div class="caption"><p>Figure 5.2.2: Sample outputs of LSGAN vs. training steps</p></div></div><p>I encourage you to run the trained generator model to see the new synthesized MNIST digits images:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 lsgan-mnist-5.2.1.py --generator=lsgan_mnist.h5</strong></span>
</pre></div></div>
<div class="section" title="Auxiliary classifier GAN (ACGAN)"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec31"/>Auxiliary classifier GAN (ACGAN)</h1></div></div></div><p>ACGAN<a id="id225" class="indexterm"/> is similar in principle to the <span class="strong"><strong>Conditional GAN</strong></span> (<span class="strong"><strong>CGAN</strong></span>) that we discussed in the previous chapter. We're going to compare both CGANand ACGAN. For both CGAN and ACGAN, the generator inputs are noise and its label. The output is a fake image belonging to the input class label. For CGAN, the inputs to the discriminator are an image (fake or real) and its label. The output is the probability that the image is real. For ACGAN, the input to the discriminator is an image, whilst the output is the probability that the image is real and its class label. Following figure highlights the difference between CGAN and ACGAN  during generator training:</p><div class="mediaobject"><img src="graphics/B08956_05_08.jpg" alt="Auxiliary classifier GAN (ACGAN)"/><div class="caption"><p>Figure 5.3.1: CGAN vs. ACGAN generator training. The main difference is the input and output of the discriminator.</p></div></div><p>Essentially, in <a id="id226" class="indexterm"/>CGAN we feed the network with side information (label). In ACGAN, we try to reconstruct the side information using an auxiliary class decoder network. ACGAN argued that forcing the network to do additional tasks is known to improve the performance of the original task. In this case, the additional task is image classification. The original task is the generation of fake images.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Network</p>
</th><th style="text-align: left" valign="bottom">
<p>Loss Functions</p>
</th><th style="text-align: left" valign="bottom">
<p>Number</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>CGAN</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_05_087.jpg" alt="Auxiliary classifier GAN (ACGAN)"/></div>
<div class="mediaobject"><img src="graphics/B08956_05_088.jpg" alt="Auxiliary classifier GAN (ACGAN)"/></div>
</td><td style="text-align: left" valign="top">
<p>4.3.1</p>
<p>4.3.2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ACGAN</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_05_089.jpg" alt="Auxiliary classifier GAN (ACGAN)"/></div>
<div class="mediaobject"><img src="graphics/B08956_05_090.jpg" alt="Auxiliary classifier GAN (ACGAN)"/><div class="caption"><p>Table 5.3.1: A comparison between the loss functions of CGAN and ACGAN</p></div></div>
</td><td style="text-align: left" valign="top">
<p>5.3.1</p>
<p>5.3.2</p>
</td></tr></tbody></table></div><p>Preceding table shows the ACGAN loss functions as compared to CGAN. The ACGAN loss functions are the same as CGAN except for the additional classifier loss functions. Apart from the original task of identifying real from fake images (<span class="inlinemediaobject"><img src="graphics/B08956_05_091.jpg" alt="Auxiliary classifier GAN (ACGAN)"/></span>), <span class="emphasis"><em>Equation 5.3.1</em></span> of the discriminator has the additional task of correctly classifying real and fake images (<span class="inlinemediaobject"><img src="graphics/B08956_05_092.jpg" alt="Auxiliary classifier GAN (ACGAN)"/></span>). <span class="emphasis"><em>Equation</em></span> <span class="emphasis"><em>5.3.2</em></span> of the generator means that apart from trying to fool the discriminator with fake images (<span class="inlinemediaobject"><img src="graphics/B08956_05_093.jpg" alt="Auxiliary classifier GAN (ACGAN)"/></span>), it is asking the discriminator to correctly classify those fake images (<span class="inlinemediaobject"><img src="graphics/B08956_05_094.jpg" alt="Auxiliary classifier GAN (ACGAN)"/></span>).</p><p>Starting with the CGAN code, only the discriminator and the training function are modified to<a id="id227" class="indexterm"/> implement ACGAN. The discriminator and generator builder functions are also provided by <code class="literal">gan.py</code>. To see the changes made on the discriminator, following listing shows the builder function where the auxiliary decoder network that performs image classification and the dual outputs are highlighted.</p><p>Listing 5.3.1, <code class="literal">gan.py</code> shows how the discriminator model builder is the same as in DCGAN predicting if an image is real, the first output. An auxiliary decoder network is added to perform the image classification and produce the second output:</p><div class="informalexample"><pre class="programlisting">def discriminator(inputs,
                  activation='sigmoid',
                  num_labels=None,
                  num_codes=None):
    """Build a Discriminator Model

    Stack of LeakyReLU-Conv2D to discriminate real from fake
    The network does not converge with BN  so it is not used here
    unlike in [1]

    # Arguments
        inputs (Layer): Input layer of the discriminator (the image)
        activation (string): Name of output activation layer
        num_labels (int): Dimension of one-hot labels for ACGAN &amp; InfoGAN
        num_codes (int): num_codes-dim Q network as output 
                    if StackedGAN or 2 Q networks if InfoGAN
                    

    # Returns
        Model: Discriminator Model
    """
    kernel_size = 5
    layer_filters = [32, 64, 128, 256]

    x = inputs
    for filters in layer_filters:
        # first 3 convolution layers use strides = 2
        # last one uses strides = 1
        if filters == layer_filters[-1]:
            strides = 1
        else:
            strides = 2
        x = LeakyReLU(alpha=0.2)(x)
        x = Conv2D(filters=filters,
                   kernel_size=kernel_size,
                   strides=strides,
                   padding='same')(x)

    x = Flatten()(x)
    # default output is probability that the image is real
    outputs = Dense(1)(x)
    if activation is not None:
        print(activation)
        outputs = Activation(activation)(outputs)

<span class="strong"><strong>    if num_labels:</strong></span>
<span class="strong"><strong>        # ACGAN and InfoGAN have 2nd output</strong></span>
<span class="strong"><strong>        # 2nd output is 10-dim one-hot vector of label</strong></span>
<span class="strong"><strong>        layer = Dense(layer_filters[-2])(x)</strong></span>
<span class="strong"><strong>        labels = Dense(num_labels)(layer)</strong></span>
<span class="strong"><strong>        labels = Activation('softmax', name='label')(labels)</strong></span>
<span class="strong"><strong>        if num_codes is None:</strong></span>
<span class="strong"><strong>            outputs = [outputs, labels]</strong></span>
        else:
            # InfoGAN have 3rd and 4th outputs
            # 3rd output is 1-dim continous Q of 1st c given x
            code1 = Dense(1)(layer)
            code1 = Activation('sigmoid', name='code1')(code1)

            # 4th output is 1-dim continuous Q of 2nd c given x
            code2 = Dense(1)(layer)
            code2 = Activation('sigmoid', name='code2')(code2)

            outputs = [outputs, labels, code1, code2]
    elif num_codes is not None:
        # z0_recon is reconstruction of z0 normal distribution
        z0_recon =  Dense(num_codes)(x)
        z0_recon = Activation('tanh', name='z0')(z0_recon)
        outputs = [outputs, z0_recon]

    return Model(inputs, outputs, name='discriminator')</pre></div><p>The <a id="id228" class="indexterm"/>discriminator is then built by calling:</p><div class="informalexample"><pre class="programlisting">discriminator = gan.discriminator(inputs, num_labels=num_labels)</pre></div><p>The generator is the same as the one in ACGAN. To recall, the generator builder is shown in the following listing. We should note that both <span class="emphasis"><em>Listings</em></span> <span class="emphasis"><em>5.3.1</em></span> and <span class="emphasis"><em>5.3.2</em></span> are the same builder functions used by WGAN and <a id="id229" class="indexterm"/>LSGAN in the previous sections.</p><p>Listing 5.3.2, <code class="literal">gan.py</code> shows the generator model builder is the same as in CGAN:</p><div class="informalexample"><pre class="programlisting">def generator(inputs,
              image_size,
              activation='sigmoid',
              labels=None,
              codes=None):
    """Build a Generator Model

    Stack of BN-ReLU-Conv2DTranpose to generate fake images.
    Output activation is sigmoid instead of tanh in [1].
    Sigmoid converges easily.

    # Arguments
        inputs (Layer): Input layer of the generator (the z-vector)
        image_size (int): Target size of one side (assuming square image)
        activation (string): Name of output activation layer
        labels (tensor): Input labels
        codes (list): 2-dim disentangled codes for InfoGAN

    # Returns
        Model: Generator Model
    """
    image_resize = image_size // 4
    # network parameters
    kernel_size = 5
    layer_filters = [128, 64, 32, 1]

<span class="strong"><strong>    if labels is not None:</strong></span>
<span class="strong"><strong>        if codes is None:</strong></span>
<span class="strong"><strong>            # ACGAN labels</strong></span>
<span class="strong"><strong>            # concatenate z noise vector and one-hot labels</strong></span>
<span class="strong"><strong>            inputs = [inputs, labels]</strong></span>
        else:
            # infoGAN codes
            # concatenate z noise vector, one-hot labels 
            # and codes 1 &amp; 2
            inputs = [inputs, labels] + codes
        x = concatenate(inputs, axis=1)
    elif codes is not None:
        # generator 0 of StackedGAN
        inputs = [inputs, codes]
        x = concatenate(inputs, axis=1)
    else:
        # default input is just 100-dim noise (z-code)
        x = inputs

    x = Dense(image_resize * image_resize * layer_filters[0])(x)
    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)

    for filters in layer_filters:
        # first two convolution layers use strides = 2
        # the last two use strides = 1
        if filters &gt; layer_filters[-2]:
            strides = 2
        else:
            strides = 1
        x = BatchNormalization()(x)
        x = Activation('relu')(x)
        x = Conv2DTranspose(filters=filters,
                            kernel_size=kernel_size,
                            strides=strides,
                            padding='same')(x)

    if activation is not None:
        x = Activation(activation)(x)

    # generator output is the synthesized image x
    return Model(inputs, x, name='generator')</pre></div><p>In ACGAN, the<a id="id230" class="indexterm"/> generator is instantiated as:</p><div class="informalexample"><pre class="programlisting">generator = gan.generator(inputs, image_size, labels=labels)</pre></div><p>Following figure shows the network model of ACGAN in Keras:</p><div class="mediaobject"><img src="graphics/B08956_05_09.jpg" alt="Auxiliary classifier GAN (ACGAN)"/><div class="caption"><p>Figure 5.3.2: The Keras model of ACGAN</p></div></div><p>As shown in <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>5.3.3</em></span>, the <a id="id231" class="indexterm"/>discriminator and adversarial models are modified to accommodate the changes in the discriminator network. We now have two loss functions. The first is the original binary cross-entropy to train the discriminator in estimating the probability if the input image is real. The second is the image classifier predicting the class label. The output is a one-hot vector of 10 dimensions.</p><p>Referring to Listing 5.3.3, <code class="literal">acgan-mnist-5.3.1.py</code>, where highlighted are the changes implemented in the discriminator and adversarial models to accommodate the image classifier of the discriminator network. The two loss functions correspond to the two outputs of the discriminator:</p><div class="informalexample"><pre class="programlisting">def build_and_train_models():
    # load MNIST dataset
    (x_train, y_train), (_, _) = mnist.load_data()

    # reshape data for CNN as (28, 28, 1) and normalize
    image_size = x_train.shape[1]
    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])
    x_train = x_train.astype('float32') / 255

    # train labels
    num_labels = len(np.unique(y_train))
    y_train = to_categorical(y_train)

    model_name = "acgan_mnist"
    # network parameters
    latent_size = 100
    batch_size = 64
    train_steps = 40000
    lr = 2e-4
    decay = 6e-8
    input_shape = (image_size, image_size, 1)
    label_shape = (num_labels, )

    # build discriminator Model
    inputs = Input(shape=input_shape, name='discriminator_input')
    # call discriminator builder with 2 outputs, 
    # pred source and labels
<span class="strong"><strong>    discriminator = gan.discriminator(inputs, num_labels=num_labels)</strong></span>
<span class="strong"><strong>    # [1] uses Adam, but discriminator converges easily with RMSprop</strong></span>
<span class="strong"><strong>    optimizer = RMSprop(lr=lr, decay=decay)</strong></span>
<span class="strong"><strong>    # 2 loss fuctions: 1) probability image is real</strong></span>
<span class="strong"><strong>    # 2) class label of the image</strong></span>
<span class="strong"><strong>    loss = ['binary_crossentropy', 'categorical_crossentropy']</strong></span>
<span class="strong"><strong>    discriminator.compile(loss=loss,</strong></span>
<span class="strong"><strong>                          optimizer=optimizer,</strong></span>
<span class="strong"><strong>                          metrics=['accuracy'])</strong></span>
    discriminator.summary()

    # build generator model
    input_shape = (latent_size, )
    inputs = Input(shape=input_shape, name='z_input')
    labels = Input(shape=label_shape, name='labels')
    # call generator builder with input labels
    generator = gan.generator(inputs, image_size, labels=labels)
    generator.summary()

    # build adversarial model = generator + discriminator
    optimizer = RMSprop(lr=lr*0.5, decay=decay*0.5)
    # freeze the weights of discriminator 
    # during adversarial training
    discriminator.trainable = False
<span class="strong"><strong>    adversarial = Model([inputs, labels],</strong></span>
<span class="strong"><strong>                        discriminator(generator([inputs, labels])),</strong></span>
<span class="strong"><strong>                        name=model_name)</strong></span>
<span class="strong"><strong>    # same 2 loss fuctions: 1) probability image is real</strong></span>
<span class="strong"><strong>    # 2) class label of the image</strong></span>
<span class="strong"><strong>    adversarial.compile(loss=loss,</strong></span>
<span class="strong"><strong>                        optimizer=optimizer,</strong></span>
<span class="strong"><strong>                        metrics=['accuracy'])</strong></span>
    adversarial.summary()

    # train discriminator and adversarial networks
    models = (generator, discriminator, adversarial)
    data = (x_train, y_train)
    params = (batch_size, latent_size, train_steps, num_labels, model_name)
    train(models, data, params)</pre></div><p>In <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>5.3.4</em></span>, we highlight the changes implemented in the training routine. The main difference compared to <a id="id232" class="indexterm"/>CGAN code is that the output label must be supplied during discriminator and adversarial training.</p><p>As seen in Listing 5.3.4, <code class="literal">acgan-mnist-5.3.1.py</code>, the changes implemented in the train function are highlighted:</p><div class="informalexample"><pre class="programlisting">def train(models, data, params):
    """Train the discriminator and adversarial Networks

    Alternately train discriminator and adversarial networks by batch.
    Discriminator is trained first with real and fake images and
    corresponding one-hot labels.
    Adversarial is trained next with fake images pretending to be real and 
    corresponding one-hot labels.
    Generate sample images per save_interval.

    # Arguments
        models (list): Generator, Discriminator, Adversarial models
        data (list): x_train, y_train data
        params (list): Network parameters

    """
    # the GAN models
    generator, discriminator, adversarial = models
    # images and their one-hot labels
    x_train, y_train = data
    # network parameters
    batch_size, latent_size, train_steps, num_labels, model_name = params
    # the generator image is saved every 500 steps
    save_interval = 500
    # noise vector to see how the generator output 
    # evolves during training
    noise_input = np.random.uniform(-1.0, 
                                    1.0,
                                    size=[16, latent_size])
<span class="strong"><strong>    # class labels are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5</strong></span>
<span class="strong"><strong>    # the generator must produce these MNIST digits</strong></span>
<span class="strong"><strong>    noise_label = np.eye(num_labels)[np.arange(0, 16) % num_labels]</strong></span>
    # number of elements in train dataset
    train_size = x_train.shape[0]
    print(model_name,
          "Labels for generated images: ",
          np.argmax(noise_label, axis=1))

    for i in range(train_steps):
        # train the discriminator for 1 batch
        # 1 batch of real (label=1.0) and fake images (label=0.0)
        # randomly pick real images and corresponding labels 
        # from dataset 
        rand_indexes = np.random.randint(0, 
                                         train_size, 
                                         size=batch_size)
        real_images = x_train[rand_indexes]
<span class="strong"><strong>        real_labels = y_train[rand_indexes]</strong></span>
        # generate fake images from noise using generator
        # generate noise using uniform distribution
        noise = np.random.uniform(-1.0,
                                  1.0,
                                  size=[batch_size, latent_size])
<span class="strong"><strong>        # randomly pick one-hot labels</strong></span>
<span class="strong"><strong>        fake_labels = np.eye(num_labels)[np.random.choice(num_labels,</strong></span>
<span class="strong"><strong>                                                      batch_size)]</strong></span>
        # generate fake images
        fake_images = generator.predict([noise, fake_labels])
        # real + fake images = 1 batch of train data
        x = np.concatenate((real_images, fake_images))
<span class="strong"><strong>        # real + fake labels = 1 batch of train data labels</strong></span>
<span class="strong"><strong>        labels = np.concatenate((real_labels, fake_labels))</strong></span>
        # label real and fake images
        # real images label is 1.0
        y = np.ones([2 * batch_size, 1])
        # fake images label is 0.0
        y[batch_size:, :] = 0
        # train discriminator network, log the loss and accuracy
        # ['loss', 'activation_1_loss', 'label_loss', 
        # 'activation_1_acc', 'label_acc']
<span class="strong"><strong>        metrics  = discriminator.train_on_batch(x, [y, labels])</strong></span>
<span class="strong"><strong>        fmt = "%d: [disc loss: %f, srcloss: %f, lblloss: %f, srcacc: %f, lblacc: %f]"</strong></span>
<span class="strong"><strong>        log = fmt % (i, metrics[0], metrics[1], metrics[2], metrics[3], metrics[4])</strong></span>

        # train the adversarial network for 1 batch
        # 1 batch of fake images with label=1.0 and
        # corresponding one-hot label or class 
        # since the discriminator weights are frozen 
        # in adversarial network
        # only the generator is trained
        # generate noise using uniform distribution
        noise = np.random.uniform(-1.0,
                                  1.0,
                                  size=[batch_size, latent_size])
<span class="strong"><strong>        # randomly pick one-hot labels</strong></span>
<span class="strong"><strong>        fake_labels = np.eye(num_labels)[np.random.choice(num_labels,</strong></span>
<span class="strong"><strong>                                                      batch_size)]</strong></span>
        # label fake images as real
        y = np.ones([batch_size, 1])
        # train the adversarial network 
        # note that unlike in discriminator training, 
        # we do not save the fake images in a variable
        # the fake images go to the discriminator input 
        # of the adversarial
        # for classification
        # log the loss and accuracy
<span class="strong"><strong>        metrics  = adversarial.train_on_batch([noise, fake_labels],</strong></span>
<span class="strong"><strong>                                            [y, fake_labels])</strong></span>
<span class="strong"><strong>        fmt = "%s [advr loss: %f, srcloss: %f, lblloss: %f, srcacc: %f, lblacc: %f]"</strong></span>
<span class="strong"><strong>        log = fmt % (log, metrics[0], metrics[1], metrics[2], metrics[3], metrics[4])</strong></span>
        print(log)
        if (i + 1) % save_interval == 0:
            if (i + 1) == train_steps:
                show = True
            else:
                show = False

            # plot generator images on a periodic basis
            gan.plot_images(generator,
                        noise_input=noise_input,
                        noise_label=noise_label,
                        show=show,
                        step=(i + 1),
                        model_name=model_name)

    # save the model after training the generator
    # the trained generator can be reloaded for 
    # future MNIST digit generation
    generator.save(model_name + ".h5")</pre></div><p>In turned out that<a id="id233" class="indexterm"/> with the additional task, the performance improvement in ACGAN is significant compared to all GANs that we have discussed previously. ACGAN training is stable as shown in <span class="emphasis"><em>Figure 5.3.3</em></span> sample outputs of ACGAN for the following labels:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[0 1 2 3 </strong></span>
<span class="strong"><strong> 4 5 6 7 </strong></span>
<span class="strong"><strong> 8 9 0 1 </strong></span>
<span class="strong"><strong> 2 3 4 5]</strong></span>
</pre></div><p>Unlike CGAN, the sample outputs appearance does not vary widely during training. The MNIST digit image perceptive quality is also better. <span class="emphasis"><em>Figure 5.3.4</em></span> shows a side by side comparison of every MNIST digit produced by both CGAN and ACGAN. Digits 2-6 are of better quality in ACGAN than in CGAN.</p><p>I encourage you to run the trained generator model to see new synthesized MNIST digits images:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 acgan-mnist-5.3.1.py --generator=acgan_mnist.h5</strong></span>
</pre></div><p>Alternatively, a specific digit (for example, 3) to be generated can also be requested:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 acgan-mnist-5.3.1.py --generator=acgan_mnist.h5 --digit=3</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B08956_05_10.jpg" alt="Auxiliary classifier GAN (ACGAN)"/><div class="caption"><p>Figure 5.3.3: The sample outputs generated by the ACGAN as a function of train steps for labels [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5]</p></div></div><div class="mediaobject"><img src="graphics/B08956_05_11.jpg" alt="Auxiliary classifier GAN (ACGAN)"/><div class="caption"><p>Figure 5.3.4: A side by side comparison of outputs of CGAN and ACGAN conditioned with digits 0 to 9</p></div></div></div>
<div class="section" title="Conclusion"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec32"/>Conclusion</h1></div></div></div><p>In this chapter, we've presented various improvements in the original algorithm of GAN, first introduced in the previous chapter. WGAN proposed an algorithm to improve the stability of training by using the EMD or Wassertein 1 loss. LSGAN argued that the original cross-entropy function of GAN is prone to vanishing gradients, unlike least squares loss. LSGAN proposed an algorithm to achieve stable training and quality outputs. ACGAN convincingly improved the quality of the conditional generation of MNIST digits by requiring the discriminator to perform classification task on top of determining whether the input image is fake or real.</p><p>In the next chapter, we'll study how to control the attributes of generator outputs. Whilst CGAN and ACGAN are able to indicate the desired digits to produce; we have not analyzed GANs that can specify the attributes of outputs. For example, we may want to control the writing style of the MNIST digits such as roundness, tilt angle, and thickness. Therefore, the goal will be to introduce GANs with disentangled representations to control the specific attributes of the generator outputs.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec33"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Ian Goodfellow and others. <span class="emphasis"><em>Generative Adversarial Nets</em></span>. Advances in neural information processing systems, 2014(<a class="ulink" href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf</a>).</li><li class="listitem">Martin Arjovsky, Soumith Chintala, and Léon Bottou, <span class="emphasis"><em>Wasserstein GAN</em></span>. arXiv preprint, 2017(<a class="ulink" href="https://arxiv.org/pdf/1701.07875.pdf">https://arxiv.org/pdf/1701.07875.pdf</a>).</li><li class="listitem">Xudong Mao and others. <span class="emphasis"><em>Least Squares Generative Adversarial Networks</em></span>. 2017 IEEE International Conference on Computer Vision (ICCV). IEEE 2017(<a class="ulink" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf">http://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf</a>).</li><li class="listitem">Augustus Odena, Christopher Olah, and Jonathon Shlens. <span class="emphasis"><em>Conditional Image Synthesis with Auxiliary Classifier GANs</em></span>. ICML, 2017(<a class="ulink" href="http://proceedings.mlr.press/v70/odena17a/odena17a.pdf">http://proceedings.mlr.press/v70/odena17a/odena17a.pdf</a>).</li></ol></div></div></body></html>