["```py\nimport theano, numpy\n\nclass AXPBOp(theano.Op):\n    \"\"\"\n    This creates an Op that takes x to a*x+b.\n    \"\"\"\n    __props__ = (\"a\", \"b\")\n\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n        super(AXPBOp, self).__init__()\n\n    def make_node(self, x):\n        x = theano.tensor.as_tensor_variable(x)\n        return theano.Apply(self, [x], [x.type()])\n\n    def perform(self, node, inputs, output_storage):\n        x = inputs[0]\n        z = output_storage[0]\n        z[0] = self.a * x + self.b\n\n    def infer_shape(self, node, i0_shapes):\n        return i0_shapes\n    def grad(self, inputs, output_grads):\n        return [self.a * output_grads[0]]\n\nmult4plus5op = AXPBOp(4,5)\n\nx = theano.tensor.matrix()\ny = mult4plus5op(x)\nf = theano.function([x], y)\n\nres = f(numpy.random.rand(3,2))\n```", "```py\n>>> mult4plus5op2 = AXPBOp(4,5)\n\n>>> mult4plus5op == mult4plus5op2\nTrue\n\n>>> hash(mult4plus5op)\n-292944955210390262\n\n>>> hash(mult4plus5op2)\n-292944955210390262\n```", "```py\n>>> theano.printing.pprint(y)\nAXPBOp{a=4, b=5}.0\n\n>>> theano.printing.pydotprint(y)\n```", "```py\nitypes = [theano.tensor.dmatrix]\notypes = [theano.tensor.dmatrix]\n```", "```py\n>>> dy=theano.tensor.grad(y.sum(), x)\n\n>>> theano.printing.pprint(dy)\n'(TensorConstant{4} * fill(AXPBOp{a=4, b=5}(<TensorType(float32, matrix)>), fill(Sum{acc_dtype=float64}(AXPBOp{a=4, b=5}(<TensorType(float32, matrix)>)), TensorConstant{1.0})))'\n\n>>> df = theano.function([x], dy)\n\n>>> theano.printing.debugprint(df)\nAlloc [id A] ''   2\n |TensorConstant{(1, 1) of 4.0} [id B]\n |Shape_i{0} [id C] ''   1\n | |<TensorType(float32, matrix)> [id D]\n |Shape_i{1} [id E] ''   0\n   |<TensorType(float32, matrix)> [id D]\n```", "```py\n>>> y = mult4plus5op(2 * x) + 4 * x\n\n>>> f = theano.function([x], y)\n\n>>> theano.printing.debugprint(f)\nHostFromGpu(gpuarray) [id A] ''   6\n |GpuElemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)]<gpuarray> [id B] ''   5\n   |GpuFromHost<None> [id C] ''   4\n   | |AXPBOp{a=4, b=5} [id D] ''   3\n   |   |HostFromGpu(gpuarray) [id E] ''   2\n   |     |GpuElemwise{mul,no_inplace} [id F] ''   1\n   |       |GpuArrayConstant{[[ 2.]]} [id G]\n   |       |GpuFromHost<None> [id H] ''   0\n   |         |<TensorType(float32, matrix)> [id I]\n   |GpuArrayConstant{[[ 4.]]} [id J]\n   |GpuFromHost<None> [id H] ''   0\n```", "```py\nfrom theano.gpuarray.type import get_context\n\ndef make_node(self, x):\n    x = as_gpuarray_variable(x, self.context_name)\n\n    x_arg = pygpu.elemwise.arg('x', 'float32', read=True)\n    c_arg = pygpu.elemwise.arg('c', 'float32', read=True, write=True)\n    self.my_op = pygpu.elemwise.GpuElemwise(get_context(self.context_name), \"c = \" + str(self.a) + \" * x + \" + str(self.b), [x_arg, c_arg], convert_f16=True)\n\n    return Apply(self, [x], [x.type()])\n\ndef perform(self, node, inputs, output_storage):\n    x = inputs[0]\n    z = output_storage[0]\n    z[0] = pygpu.empty(x.shape, dtype=x.dtype, context=get_context(self.context_name))\n    self.my_op( x, z[0])\n```", "```py\nHostFromGpu(gpuarray) [id A] ''   4\n |GpuElemwise{Add}[(0, 1)]<gpuarray> [id B] ''   3\n   |GpuArrayConstant{[[ 4.]]} [id C]\n   |GpuAXPBOp{a=4, b=5, context_name='dev0'} [id D] ''   2\n     |GpuElemwise{Mul}[(0, 1)]<gpuarray> [id E] ''   1\n       |GpuArrayConstant{[[ 2.]]} [id F]\n       |GpuFromHost<dev0> [id G] ''   0\n         |<TensorType(float32, matrix)> [id H]\n```", "```py\ndef c_code_cache_version(self):\n    return (6, 0)\n\ndef c_support_code(self):\n    c_support_code = \"\"\"\n    bool same_shape(PyArrayObject* arr1, PyArrayObject* arr2)\n    {\n        if( PyArray_NDIM(arr1) != PyArray_NDIM(arr2)) {\n            return false;\n        }\n        for(int i = 0; i < PyArray_NDIM(arr2) ; i++) {\n            if (PyArray_DIMS(arr1)[0] == PyArray_DIMS(arr2)[0]) {\n                return false;\n            }\n        }\n        return true;\n    }\n    \"\"\"\n\n    return c_support_code\n\ndef c_support_code_apply(self, node, name):\n    dtype_x = node.inputs[0].dtype\n    dtype_z = node.outputs[0].dtype\n\n    a = self.a\n    b = self.b\n\n    c_support_code = \"\"\"\n    void elemwise_op_%(name)s(npy_%(dtype_x)s* x_ptr, npy_intp* x_str, int itemsize_x,\n        npy_%(dtype_z)s* z_ptr, npy_intp* z_str, int itemsize_z,\n        int nbDims, npy_intp* dims)\n    {\n        npy_intp stride_x = (npy_intp)(1);\n        npy_intp stride_z = (npy_intp)(1);\n        for (int i = 0; i < nbDims; i ++) {\n            stride_x = stride_x * x_str[i] / itemsize_x;\n            stride_z = stride_z * z_str[i] / itemsize_z;\n        }\n        for (int i=0; i < dims[0]; i++)\n            if (nbDims==1) {\n                z_ptr[i * z_str[0]/itemsize_z] = x_ptr[i * x_str[0] / itemsize_x] * ((npy_%(dtype_z)s) %(a)s) + ((npy_%(dtype_z)s)%(b)s);\n            } else {\n                elemwise_op_%(name)s( x_ptr + i * stride_x , x_str + 1, itemsize_x,\n                    z_ptr + i * stride_z , z_str + 1, itemsize_z,\n                    nbDims - 1, dims + 1 );\n            }\n    }\n    \"\"\"\n\n    return c_support_code % locals()\n\ndef c_code(self, node, name, inp, out, sub):\n    x = inp[0]\n    z = out[0]\n\n    dtype_x = node.inputs[0].dtype\n    dtype_z = node.outputs[0].dtype\n\n    itemsize_x = numpy.dtype(dtype_x).itemsize\n    itemsize_z = numpy.dtype(dtype_z).itemsize\n\n    typenum_z = numpy.dtype(dtype_z).num\n\n    fail = sub['fail']\n\n    c_code = \"\"\"\n    // Validate that the output storage exists and has the same\n    // dimension as x.\n    if (NULL == %(z)s || !(same_shape(%(x)s, %(z)s)))\n    {\n        /* Reference received to invalid output variable.\n        Decrease received reference's ref count and allocate new\n        output variable */\n        Py_XDECREF(%(z)s);\n        %(z)s = (PyArrayObject*)PyArray_EMPTY(PyArray_NDIM(%(x)s),\n                                            PyArray_DIMS(%(x)s),\n                                            %(typenum_z)s,\n                                            0);\n\n        if (!%(z)s) {\n            %(fail)s;\n        }\n    }\n\n    // Perform the elemwise operation\n    ((npy_%(dtype_z)s *)PyArray_DATA(%(z)s))[0] = 0;\n    elemwise_op_%(name)s((npy_%(dtype_x)s*)PyArray_DATA(%(x)s), PyArray_STRIDES(%(x)s), %(itemsize_x)s,\n                            (npy_%(dtype_z)s*)PyArray_DATA(%(z)s), PyArray_STRIDES(%(z)s), %(itemsize_z)s,\n                            PyArray_NDIM(%(x)s), PyArray_DIMS(%(x)s) );\n\n    \"\"\"\n\n    return c_code % locals()\n```", "```py\ndef gpu_kernels(self, node, name):\n    code = \"\"\"\nKERNEL void axpb(GLOBAL_MEM %(ctype)s *x, GLOBAL_MEM  %(ctype)s *z, ga_size n, ga_size m) {\nfor (ga_size i = LID_0; i < n; i += LDIM_0) {\n    for (ga_size j = LID_0; j < m; j += LDIM_0) {\n        z[i*m + j] = %(write_a)s( 2 * x[i*m + j] );\n    }\n}\n}\"\"\" % dict(ctype=pygpu.gpuarray.dtype_to_ctype(self.dtype),\n        name=name, write_a=write_w(self.dtype))\n    return [Kernel(\n            code=code, name=\"axpb\",\n            params=[gpuarray.GpuArray, gpuarray.GpuArray, gpuarray.SIZE, gpuarray.SIZE],\n            flags=Kernel.get_flags(self.dtype),\n            objvar='k_axpb_' + name)]\n\ndef c_code(self, node, name, inp, out, sub):\n    n, = inp\n    z, = out\n    dtype_n = node.inputs[0].dtype\n    fail = sub['fail']\n    ctx = sub['params']\n    typecode = pygpu.gpuarray.dtype_to_typecode(self.dtype)\n    sync = bool(config.gpuarray.sync)\n    kname = self.gpu_kernels(node, name)[0].objvar\n    s = \"\"\"\n    size_t dims[2] = {0, 0};\n    size_t ls, gs;\n    int err;\n    dims[0] = %(n)s->ga.dimensions[0];\n    dims[1] = %(n)s->ga.dimensions[1];\n    Py_CLEAR(%(z)s);\n    %(z)s = pygpu_zeros(2, dims,\n                        %(typecode)s,\n                        GA_C_ORDER,\n                        %(ctx)s, Py_None);\n    if (%(z)s == NULL) {\n        %(fail)s\n    }\n    ls = 1;\n    gs = 256;\n    err = axpb_call(1, &gs, &ls, 0, %(n)s->ga.data, %(z)s->ga.data, dims[0], dims[1]);\n    if (err != GA_NO_ERROR) {\n        PyErr_Format(PyExc_RuntimeError,\n                     \"gpuarray error: kEye: %%s. n%%lu, m=%%lu.\",\n                     GpuKernel_error(&%(kname)s, err),\n                     (unsigned long)dims[0], (unsigned long)dims[1]);\n        %(fail)s;\n    }\n    if(%(sync)d)\n        GpuArray_sync(&%(z)s->ga);\n    \"\"\" % locals()\n\n    return s\n```", "```py\nA new GPU computation kernel is defined under the name axpb, and it is a simple C code with special GPU types and two macros: KERNEL to designate the kernel function (hiding the CUDA __global__ declaration for kernels) and GLOBAL_MEM for the variables defined globally, available both on the CPU and the GPU (in opposition to variables inside the kernel function that, by default, are local to the thread executed on a GPU core).\n```", "```py\nint i = blockIdx.x*blockDim.x + threadIdx.x;\n```", "```py\nfrom keras import backend as K\nfrom keras.utils.conv_utils import convert_kernel\nfrom keras.models import Model\n\n# build your Keras model HERE\n# then\nmodel.load_weights('my_weights_tensorflow.h5')\n\nfor layer in model.layers:\n   if layer.__class__.__name__ in ['Convolution1D', 'Convolution2D']:\n      original_w = K.get_value(layer.W)\n      converted_w = convert_kernel(original_w)\n      K.set_value(layer.W, converted_w)\n\nmodel.save_weights('my_weights_theano.h5')\n```", "```py\ngsutil mb -l europe-west1 gs://keras_sentiment_analysis\ngsutil cp -r sem_eval2103.train gs://keras_sentiment_analysis/sem_eval2103.train\ngsutil cp -r sem_eval2103.dev gs://keras_sentiment_analysis/sem_eval2103.dev\ngsutil cp -r sem_eval2103.test gs://keras_sentiment_analysis/sem_eval2103.test\n```", "```py\n    from setuptools import setup, find_packages\n\n    setup(name='example5',\n      version='0.1',\n      packages=find_packages(),\n      description='keras on gcloud ml-engine',\n      install_requires=[\n          'keras',\n          'h5py',\n          'nltk'\n      ],\n      zip_safe=False)\n    ```", "```py\n      trainingInput:\n        scaleTier: CUSTOM\n        # standard_gpu provides 1 GPU. Change to complex_model_m_gpu for 4 GPUs\n        masterType: standard_gpu\n        runtimeVersion: \"1.0\"\n    ```", "```py\ngcloud ml-engine local train --module-name 7-google-cloud.bilstm \\\n  --package-path ./7-google-cloud  -- --job-dir ./7-google-cloud \\\n  -t sem_eval2103.train -d sem_eval2103.dev -v sem_eval2103.test\n```", "```py\nJOB_NAME=\"keras_sentiment_analysis_train_$(date +%Y%m%d_%H%M%S)\"\n\ngcloud ml-engine jobs submit training $JOB_NAME \\\n          --job-dir gs://keras_sentiment_analysis/$JOB_NAME \\\n          --runtime-version 1.0 \\\n          --module-name 7-google-cloud.bilstm  \\\n          --package-path ./7-google-cloud \\\n          --region europe-west1 \\\n          --config=7-google-cloud/cloudml-gpu.yaml \\\n          -- \\\n          -t gs://keras_sentiment_analysis/sem_eval2103.train \\\n          -d gs://keras_sentiment_analysis/sem_eval2103.dev \\\n          -v gs://keras_sentiment_analysis/sem_eval2103.test\n\ngcloud ml-engine jobs describe $JOB_NAME\n```"]