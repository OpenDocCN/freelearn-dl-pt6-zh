- en: Popular CNN Model Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, will introduce the ImageNet image database and also cover
    the architectures of the following popular CNN models:'
  prefs: []
  type: TYPE_NORMAL
- en: LeNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AlexNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VGG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GoogLeNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to ImageNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ImageNet is a database of over 15 million hand-labeled, high-resolution images
    in roughly 22,000 categories. This database is organized just like the WordNet
    hierarchy, where each concept is also called a **synset** (that is, **synonym
    set**). Each synset is a node in the ImageNet hierarchy. Each node has more than
    500 images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **ImageNet Large Scale Visual Recognition Challenge** (**ILSVRC**) was
    founded in 2010 to improve state-of-the-art technology for object detection and
    image classification on a large scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20756743-ac07-44cb-a804-05763571d81d.png)'
  prefs: []
  type: TYPE_IMG
- en: Following this overview of ImageNet, we will now take a look at various CNN
    model architectures.
  prefs: []
  type: TYPE_NORMAL
- en: LeNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2010, a challenge from ImageNet (known as **ILSVRC 2010**) came out with
    a CNN architecture, LeNet 5, built by Yann Lecun. This network takes a 32 x 32
    image as input, which goes to the convolution layers (**C1**) and then to the
    subsampling layer (**S2**). Today, the subsampling layer is replaced by a pooling
    layer. Then, there is another sequence of convolution layers (**C3**) followed
    by a pooling (that is, subsampling) layer (**S4**). Finally, there are three fully
    connected layers, including the **OUTPUT** layer at the end. This network was
    used for zip code recognition in post offices. Since then, every year various
    CNN architectures were introduced with the help of this competition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb2b54bf-cb43-4e1f-897c-862f85f06423.png)'
  prefs: []
  type: TYPE_IMG
- en: LeNet 5 – CNN architecture from Yann Lecun's article in 1998
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can conclude the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: The input to this network is a grayscale 32 x 32 image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture implemented is a CONV layer, followed by POOL and a fully connected
    layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CONV filters are 5 x 5, applied at a stride of 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AlexNet architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first breakthrough in the architecture of CNN came in the year 2012\. This
    award-winning CNN architecture is called **AlexNet**. It was developed at the
    University of Toronto by Alex Krizhevsky and his professor, Jeffry Hinton.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first run, a ReLU activation function and a dropout of 0.5 were used
    in this network to fight overfitting. As we can see in the following image, there
    is a normalization layer used in the architecture, but this is not used in practice
    anymore as it used heavy data augmentation. AlexNet is still used today even though
    there are more accurate networks available, because of its relative simple structure
    and small depth. It is widely used in computer vision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd1ba84c-d1ae-4f6f-87b6-0e641600eab4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'AlexNet is trained on the ImageNet database using two separate GPUs, possibly
    due to processing limitations with inter-GPU connections at the time, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1100c791-9be1-4e7b-8039-3172ad50606d.png)'
  prefs: []
  type: TYPE_IMG
- en: Traffic sign classifiers using AlexNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will use transfer learning for feature extraction and a
    German traffic sign dataset to develop a classifier. Used here is an AlexNet implementation
    byMichael Guerzhoy and Davi Frossard, and AlexNet weights are from the Berkeley
    vision and Learning center. The complete code and dataset can be downloaded from here.
  prefs: []
  type: TYPE_NORMAL
- en: 'AlexNet expects a 227 x 227 x 3 pixel image, whereas the traffic sign images
    are 32 x 32 x 3 pixels. In order to feed the traffic sign images into AlexNet,
    we''ll need to resize the images to the dimensions that AlexNet expects, that
    is, 227 x 227 x 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can do so with the help of the `tf.image.resize_images` method by TensorFlow.
    Another issue here is that AlexNet was trained on the ImageNet dataset, which
    has 1,000 classes of images. So, we will replace this layer with a 43-neuron classification
    layer. To do this, figure out the size of the output from the last fully connected
    layer; since this is a fully connected layer and so is a 2D shape, the last element
    will be the size of the output. `fc7.get_shape().as_list()[-1]` does the trick;
    combine this with the number of classes for the traffic sign dataset to get the
    shape of the final fully connected layer: `shape = (fc7.get_shape().as_list()[-1],
    43)`. The rest of the code is just the standard way to define a fully connected
    layer in TensorFlow. Finally, calculate the probabilities with `softmax`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: VGGNet architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The runner-up in the 2014 ImageNet challenge was VGGNet from the visual geometric
    group at Oxford University. This convolutional neural network is a simple and
    elegant architecture with a 7.3% error rate. It has two versions: VGG16 and VGG19.'
  prefs: []
  type: TYPE_NORMAL
- en: VGG16 is a 16-layer neural network, not counting the max pooling layer and the
    softmax layer. Hence, it is known as VGG16\. VGG19 consists of 19 layers. A pre-trained
    model is available in Keras for both Theano and TensorFlow backends.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key design consideration here is depth. Increases in the depth of the network
    were achieved by adding more convolution layers, and it was done due to the small
    3 x 3 convolution filters in all the layers. The default input size of an image
    for this model is 224 x 224 x 3\. The image is passed through a stack of convolution
    layers with a stride of 1 pixel and padding of 1\. It uses 3 x 3 convolution throughout
    the network. Max pooling is done over a 2 x 2 pixel window with a stride of 2,
    then another stack of convolution layers followed by three fully connected layers.
    The first two fully connected layers have 4,096 neurons each, and the third fully
    connected layers are responsible for classification with 1,000 neurons. The final
    layer is a softmax layer. VGG16 uses a much smaller 3 x 3 convolution window,
    compared to AlexNet''s much larger 11 x 11 convolution window. All hidden layers
    are built with the ReLU activation function. The architecture looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a1c03f6-5c37-4e49-9b3d-ef56e005b84d.png)'
  prefs: []
  type: TYPE_IMG
- en: VGG16 network architecture
  prefs: []
  type: TYPE_NORMAL
- en: Due to the small 3 x 3 convolution filter, the depth of VGGNet is increased.
    The number of parameters in this network is approximately 140 million, mostly
    from the first fully connected layer. In latter-day architectures, fully connected
    layers of VGGNet are replaced with **global average pooling** (**GAP**) layers
    in order to minimize the number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Another observation is that the number of filters increases as the image size
    decreases.
  prefs: []
  type: TYPE_NORMAL
- en: VGG16 image classification code example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Keras Applications module has pre-trained neural network models, along
    with its pre-trained weights trained on ImageNet. These models can be used directly for
    prediction, feature extraction, and fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first time it executes the preceding script, Keras will automatically download
    and cache the architecture weights to disk in the `~/.keras/models` directory.
    Subsequent runs will be faster.
  prefs: []
  type: TYPE_NORMAL
- en: GoogLeNet architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2014, ILSVRC, Google published its own network known as **GoogLeNet**. Its
    performance is a little better than VGGNet; GoogLeNet's performance is 6.7% compared
    to VGGNet's performance of 7.3%. The main attractive feature of GoogLeNet is that
    it runs very fast due to the introduction of a new concept called **inception
    module**, thus reducing the number of parameters to only 5 million; that's 12
    times less than AlexNet. It has lower memory use and lower power use too.
  prefs: []
  type: TYPE_NORMAL
- en: It has 22 layers, so it is a very deep network. Adding more layers increases
    the number of parameters and it is likely that the network overfits. There will
    be more computation, because a linear increase in filters results in a quadratic
    increase in computation. So, the designers use the inception module and GAP. The
    fully connected layer at the end of the network is replaced with a GAP layer because
    fully connected layers are generally prone to overfitting. GAP has no parameters
    to learn or optimize.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture insights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of choosing a particular filter size as in the previous architectures,
    the GoogLeNet designers applied all the three filters of sizes 1 x 1, 3 x 3, and
    5 x 5 on the same patch, with a 3 x 3 max pooling and concatenation into a single
    output vector.
  prefs: []
  type: TYPE_NORMAL
- en: The use of 1 x 1 convolutions decreases the dimensions wherever the computation
    is increased by the expensive 3 x 3 and 5 x 5 convolutions. 1 x 1 convolutions
    with the ReLU activation function are used before the expensive 3 x 3 and 5 x
    5 convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In GoogLeNet, inception modules are stacked one over the other. This stacking
    allows us to modify each module without affecting the later layers. For example,
    you can increase or decrease the width of any layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2636db13-c7fb-49c9-b20d-544287eacf55.png)'
  prefs: []
  type: TYPE_IMG
- en: GoogLeNet architecture
  prefs: []
  type: TYPE_NORMAL
- en: Deep networks also suffer from the fear of what is known as the **vanishing
    gradient** problem during backpropagation. This is avoided by adding auxiliary
    classifiers to intermediate layers. Also, during training, the intermediate loss
    was added to the total loss with a discounted factor of 0.3.
  prefs: []
  type: TYPE_NORMAL
- en: Since fully connected layers are prone to overfitting, it is replaced with a
    GAP layer. Average pooling does not exclude use of dropout, a regularization method
    for overcoming overfitting in deep neural networks. GoogLeNet added a linear layer
    after 60, a GAP layer to help others swipe for their own classifier using transfer
    learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Inception module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image is an example of an inception module:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac511227-54cd-4a2a-a9ef-077ec291ec0f.png)'
  prefs: []
  type: TYPE_IMG
- en: ResNet architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After a certain depth, adding additional layers to feed-forward convNets results
    in a higher training error and higher validation error. When adding layers, performance
    increases only up to a certain depth, and then it rapidly decreases. In the **ResNet**
    (**Residual Network**) paper, the authors argued that this underfitting is unlikely
    due to the vanishing gradient problem, because this happens even when using the
    batch normalization technique. Therefore, they have added a new concept called
    **residual block**. The ResNet team added connections that can skip layers:'
  prefs: []
  type: TYPE_NORMAL
- en: ResNet uses standard convNet and adds connections that skip a few convolution
    layers at a time. Each bypass gives a residual block.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7c96246-7f86-4e6c-89d0-22a25e655493.png)'
  prefs: []
  type: TYPE_IMG
- en: Residual block
  prefs: []
  type: TYPE_NORMAL
- en: In the 2015 ImageNet ILSVRC competition, the winner was ResNet from Microsoft,
    with an error rate of 3.57%. ResNet is a kind of VGG in the sense that the same
    structure is repeated again and again to make the network deeper. Unlike VGGNet,
    it has different depth variations, such as 34, 50, 101, and 152 layers. It has
    a whopping 152 layers compared to AlexNet 8, VGGNet's 19 layers, and GoogLeNet's
    22 layers. The ResNet architecture is a stack of residual blocks. The main idea
    is to skip layers by adding connections to the neural network. Every residual
    block has 3 x 3 convolution layers. After the last conv layer, a GAP layer is
    added. There is only one fully connected layer to classify 1,000 classes. It has
    different depth varieties, such as 34, 50, 101, or 152 layers for the ImageNet
    dataset. For a deeper network, say more than 50 layers, it uses the **bottleneck** features
    concept to improve efficiency. No dropout is used in this network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other network architectures to be aware of include:'
  prefs: []
  type: TYPE_NORMAL
- en: Network in Network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beyond ResNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FractalNet, an ultra-deep neural network without residuals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the different CNN architectures. These models
    are pre-trained existing models and differ in network architecture. Each of these
    networks is designed to solve a problem specific to its architecture. So, here
    we described their architectural differences.
  prefs: []
  type: TYPE_NORMAL
- en: We also understood how our own CNN architecture, as defined in the previous
    chapter, differs from these advanced ones.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how these pre-trained models can be used
    for transfer learning.
  prefs: []
  type: TYPE_NORMAL
