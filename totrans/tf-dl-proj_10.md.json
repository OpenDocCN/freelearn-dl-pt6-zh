["```py\nimport gym\nenv = gym.make('CartPole-v0')\nfor i_episode in range(20):\n      observation = env.reset()\n      for t in range(100):\n                  env.render()\n                  print(observation)\n                  # taking a random action\n                  action = env.action_space.sample()\n                  observation, reward, done, info = \\   \n                                             env.step(action)\n                  If done:\n                     print(\"Episode finished after %i \\\n                           timesteps\" % (t+1))\n                     break\n```", "```py\nprint(env.action_space)\nprint(env.observation_space)\nprint(env.observation_space.high)\nprint(env.observation_space.low)\n```", "```py\napt-get install -y python3-dev python-dev python-numpy libcupti-dev libjpeg-turbo8-dev make golang tmux htop chromium-browser git cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n```", "```py\ngit clone https://github.com/openai/gym\ncd gym\npip install -e .[all]\n```", "```py\nimport gym\nenv = gym.make('LunarLander-v2')\nenv.reset()\nenv.render()\n```", "```py\nconda create --name gym python=3.4 anaconda gcc=4.8.5\nsource activate gym\nconda install pip six libgcc swig\nconda install -c conda-forge opencv\npip install --upgrade tensorflow-gpu\ngit clone https://github.com/openai/gym\ncd gym\npip install -e .\nconda install -c https://conda.anaconda.org/kne pybox2d\n```", "```py\nimport gym\nfrom gym import wrappers\nimport numpy as np\nimport random, tempfile, os\nfrom collections import deque\nimport tensorflow as tf\n```", "```py\nclass Brain:\n    \"\"\"\n    A Q-Value approximation obtained using a neural network.\n    This network is used for both the Q-Network and the Target Network.\n    \"\"\"\n    def __init__(self, nS, nA, scope=\"estimator\",\n                 learning_rate=0.0001,\n                 neural_architecture=None,\n                 global_step=None, summaries_dir=None):\n        self.nS = nS\n        self.nA = nA\n        self.global_step = global_step\n        self.scope = scope\n        self.learning_rate = learning_rate\n        if not neural_architecture:\n            neural_architecture = self.two_layers_network\n        # Writes Tensorboard summaries to disk\n        with tf.variable_scope(scope):\n            # Build the graph\n            self.create_network(network=neural_architecture,              \n                                learning_rate=self.learning_rate)\n            if summaries_dir:\n                summary_dir = os.path.join(summaries_dir, \n                                          \"summaries_%s\" % scope)\n                if not os.path.exists(summary_dir):\n                    os.makedirs(summary_dir)\n                self.summary_writer = \\\n                               tf.summary.FileWriter(summary_dir)\n            else:\n                self.summary_writer = None\n```", "```py\ndef two_layers_network(self, x, layer_1_nodes=32, \n                                layer_2_nodes=32):\n\n    layer_1 = tf.contrib.layers.fully_connected(x, layer_1_nodes, \n                                        activation_fn=tf.nn.relu)\n    layer_2 = tf.contrib.layers.fully_connected(layer_1, \n                                          layer_2_nodes, \n                               activation_fn=tf.nn.relu)\n    return tf.contrib.layers.fully_connected(layer_2, self.nA, \n                                           activation_fn=None)\n```", "```py\n    def create_network(self, network, learning_rate=0.0001):\n\n        # Placeholders for states input\n        self.X = tf.placeholder(shape=[None, self.nS], \n                                 dtype=tf.float32, name=\"X\")\n        # The r target value\n        self.y = tf.placeholder(shape=[None, self.nA], \n                                 dtype=tf.float32, name=\"y\")\n        # Applying the choosen network\n        self.predictions = network(self.X)\n        # Calculating the loss\n        sq_diff = tf.squared_difference(self.y, self.predictions)\n        self.loss = tf.reduce_mean(sq_diff)\n        # Optimizing parameters using the Adam optimizer\n        self.train_op = tf.contrib.layers.optimize_loss(self.loss, \n                        global_step=tf.train.get_global_step(),                                      \n                        learning_rate=learning_rate, \n                        optimizer='Adam')\n        # Recording summaries for Tensorboard\n        self.summaries = tf.summary.merge([\n            tf.summary.scalar(\"loss\", self.loss),\n            tf.summary.scalar(\"max_q_value\", \n                             tf.reduce_max(self.predictions)),\n            tf.summary.scalar(\"mean_q_value\", \n                             tf.reduce_mean(self.predictions))])\n```", "```py\n    def predict(self, sess, s):\n        \"\"\"\n        Predicting q values for actions\n        \"\"\"\n        return sess.run(self.predictions, {self.X: s})\n\n    def fit(self, sess, s, r, epochs=1):\n        \"\"\"\n        Updating the Q* function estimator\n        \"\"\"\n        feed_dict = {self.X: s, self.y: r}\n        for epoch in range(epochs):\n            res = sess.run([self.summaries, self.train_op, \n                            self.loss, \n                            self.predictions,\n                            tf.train.get_global_step()], \n                            feed_dict)\n            summaries, train_op, loss, predictions, \n                                       self.global_step = res\n\n        if self.summary_writer:\n            self.summary_writer.add_summary(summaries,\nself.global_step)\n```", "```py\nclass Memory:\n    \"\"\"\n    A memory class based on deque, a list-like container with \n    fast appends and pops on either end (from the collections \n    package)\n    \"\"\"\n    def __init__(self, memory_size=5000):\n        self.memory = deque(maxlen=memory_size)\n\n    def __len__(self):\n        return len(self.memory)\n\n    def add_memory(self, s, a, r, s_, status):\n        \"\"\"\n        Memorizing the tuple (s a r s_) plus the Boolean flag status,\n        reminding if we are at a terminal move or not\n        \"\"\"\n        self.memory.append((s, a, r, s_, status))\n\n    def recall_memories(self):\n        \"\"\"\n        Returning all the memorized data at once\n        \"\"\"\n        return list(self.memory)\n```", "```py\nclass Agent:\n    def __init__(self, nS, nA, experiment_dir):\n        # Initializing\n        self.nS = nS\n        self.nA = nA\n        self.epsilon = 1.0  # exploration-exploitation ratio\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.9994\n        self.gamma = 0.99  # reward decay\n        self.learning_rate = 0.0001\n        self.epochs = 1  # training epochs\n        self.batch_size = 32\n        self.memory = Memory(memory_size=250000)\n\n        # Creating estimators\n        self.experiment_dir =os.path.abspath\\    \n                      (\"./experiments/{}\".format(experiment_dir))\n        self.global_step = tf.Variable(0, name='global_step', \n                                                trainable=False)\n        self.model = Brain(nS=self.nS, nA=self.nA, scope=\"q\",\n                           learning_rate=self.learning_rate,\n                           global_step=self.global_step,\n                           summaries_dir=self.experiment_dir)\n        self.target_model = Brain(nS=self.nS, nA=self.nA, \n                                             scope=\"target_q\",\n                             learning_rate=self.learning_rate,\n                                 global_step=self.global_step)\n\n        # Adding an op to initialize the variables.\n        init_op = tf.global_variables_initializer()\n        # Adding ops to save and restore all the variables.\n        self.saver = tf.train.Saver()\n\n        # Setting up the session\n        self.sess = tf.Session()\n        self.sess.run(init_op)\n```", "```py\ndef epsilon_update(self, t):\n    if self.epsilon > self.epsilon_min:\n        self.epsilon *= self.epsilon_decay\n```", "```py\n    def save_weights(self, filename):\n        \"\"\"\n        Saving the weights of a model\n        \"\"\"\n        save_path = self.saver.save(self.sess, \n                                    \"%s.ckpt\" % filename)\n        print(\"Model saved in file: %s\" % save_path)\ndef load_weights(self, filename):\n    \"\"\"\n    Restoring the weights of a model\n    \"\"\"\n    self.saver.restore(self.sess, \"%s.ckpt\" % filename)\n    print(\"Model restored from file\")\n```", "```py\n    def set_weights(self, model_1, model_2):\n        \"\"\"\n        Replicates the model parameters of one \n        estimator to another.\n        model_1: Estimator to copy the parameters from\n        model_2: Estimator to copy the parameters to\n        \"\"\"\n        # Enumerating and sorting the parameters \n        # of the two models\n        model_1_params = [t for t in tf.trainable_variables() \\\n                          if t.name.startswith(model_1.scope)]\n        model_2_params = [t for t in tf.trainable_variables() \\\n                         if t.name.startswith(model_2.scope)]\n        model_1_params = sorted(model_1_params, \n                                key=lambda x: x.name)\n        model_2_params = sorted(model_2_params, \n                                key=lambda x: x.name)\n        # Enumerating the operations to be done\n        operations = [coef_2.assign(coef_1) for coef_1, coef_2 \\\n                      in zip(model_1_params, model_2_params)]\n        # Executing the operations to be done\n        self.sess.run(operations)\n    def target_model_update(self):\n        \"\"\"\n        Setting the model weights to the target model's ones\n        \"\"\"\n        self.set_weights(self.model, self.target_model)\n```", "```py\n    def act(self, s):\n        \"\"\"\n        Having the agent act based on learned Q* function\n        or by random choice (based on epsilon)\n        \"\"\"\n        # Based on epsilon predicting or randomly \n        # choosing the next action\n        if np.random.rand() <= self.epsilon:\n            return np.random.choice(self.nA)\n        else:\n            # Estimating q for all possible actions\n            q = self.model.predict(self.sess, s)[0]\n            # Returning the best action\n            best_action = np.argmax(q)\n            return best_action\n```", "```py\n    def replay(self):\n        # Picking up a random batch from memory\n        batch = np.array(random.sample(\\\n                self.memory.recall_memories(), self.batch_size))\n        # Retrieving the sequence of present states\n        s = np.vstack(batch[:, 0])\n        # Recalling the sequence of actions\n        a = np.array(batch[:, 1], dtype=int)\n        # Recalling the rewards\n        r = np.copy(batch[:, 2])\n        # Recalling the sequence of resulting states\n        s_p = np.vstack(batch[:, 3])\n        # Checking if the reward is relative to \n        # a not terminal state\n        status = np.where(batch[:, 4] == False)\n        # We use the model to predict the rewards by \n        # our model and the target model\n        next_reward = self.model.predict(self.sess, s_p)\n        final_reward = self.target_model.predict(self.sess, s_p)\n\n        if len(status[0]) > 0:\n            # Non-terminal update rule using the target model\n            # If a reward is not from a terminal state, \n            # the reward is just a partial one (r0)\n            # We should add the remaining and obtain a \n            # final reward using target predictions\n            best_next_action = np.argmax(\\\n                             next_reward[status, :][0], axis=1)\n            # adding the discounted final reward\n            r[status] += np.multiply(self.gamma,\n                     final_reward[status, best_next_action][0])\n\n        # We replace the expected rewards for actions \n        # when dealing with observed actions and rewards\n        expected_reward = self.model.predict(self.sess, s)\n        expected_reward[range(self.batch_size), a] = r\n\n        # We re-fit status against predicted/observed rewards\n        self.model.fit(self.sess, s, expected_reward,\n                       epochs=self.epochs)\n```", "```py\nclass Environment:\n    def __init__(self, game=\"LunarLander-v2\"):\n        # Initializing\n        np.set_printoptions(precision=2)\n        self.env = gym.make(game)\n        self.env = wrappers.Monitor(self.env, tempfile.mkdtemp(), \n                               force=True, video_callable=False)\n        self.nS = self.env.observation_space.shape[0]\n        self.nA = self.env.action_space.n\n        self.agent = Agent(self.nS, self.nA, self.env.spec.id)\n\n        # Cumulative reward\n        self.reward_avg = deque(maxlen=100)\n```", "```py\n    def test(self):\n        self.learn(epsilon=0.0, episodes=100, \n                        trainable=False, incremental=False)\n\n    def train(self, epsilon=1.0, episodes=1000):\n        self.learn(epsilon=epsilon, episodes=episodes, \n                        trainable=True, incremental=False)\n\n    def incremental(self, epsilon=0.01, episodes=100):\n        self.learn(epsilon=epsilon, episodes=episodes, \n                        trainable=True, incremental=True)\n```", "```py\n    def learn(self, epsilon=None, episodes=1000, \n              trainable=True, incremental=False):\n        \"\"\"\n        Representing the interaction between the enviroment \n        and the learning agent\n        \"\"\"\n        # Restoring weights if required\n        if not trainable or (trainable and incremental):\n            try:\n                print(\"Loading weights\")\n                self.agent.load_weights('./weights.h5')\n            except:\n                print(\"Exception\")\n                trainable = True\n                incremental = False\n                epsilon = 1.0\n\n        # Setting epsilon\n        self.agent.epsilon = epsilon\n        # Iterating through episodes\n        for episode in range(episodes):\n            # Initializing a new episode\n            episode_reward = 0\n            s = self.env.reset()\n            # s is put at default values\n            s = np.reshape(s, [1, self.nS])\n\n            # Iterating through time frames\n            for time_frame in range(1000):\n                if not trainable:\n                    # If not learning, representing \n                    # the agent on video\n                    self.env.render()\n                # Deciding on the next action to take\n                a = self.agent.act(s)\n                # Performing the action and getting feedback\n                s_p, r, status, info = self.env.step(a)\n                s_p = np.reshape(s_p, [1, self.nS])\n\n                # Adding the reward to the cumulative reward\n                episode_reward += r\n\n                # Adding the overall experience to memory\n                if trainable:\n                    self.agent.memory.add_memory(s, a, r, s_p,\n                                                 status)\n\n                # Setting the new state as the current one\n                s = s_p\n\n                # Performing experience replay if memory length \n                # is greater than the batch length\n                if trainable:\n                    if len(self.agent.memory) > \\\n                           self.agent.batch_size:\n                        self.agent.replay()\n\n                # When the episode is completed, \n                # exiting this loop\n                if status:\n                    if trainable:\n                       self.agent.target_model_update()\n                    break\n\n            # Exploration vs exploitation\n            self.agent.epsilon_update(episode)\n\n            # Running an average of the past 100 episodes\n            self.reward_avg.append(episode_reward)\n            print(\"episode: %i score: %.2f avg_score: %.2f\"\n                  \"actions %i epsilon %.2f\" % (episode,\n                                        episode_reward,\n                           np.average(self.reward_avg),\n                                            time_frame,\n                                               epsilon)\n        self.env.close()\n\n        if trainable:\n            # Saving the weights for the future\n            self.agent.save_weights('./weights.h5')\n```", "```py\nlunar_lander = Environment(game=\"LunarLander-v2\")\n```", "```py\n    lunar_lander.train(epsilon=1.0, episodes=5000)\n```", "```py\ntensorboard --logdir=./experiments --port 6006\n```", "```py\nlunar_lander.incremental(episodes=25, epsilon=0.01)\n```", "```py\n lunar_lander.test()\n```"]