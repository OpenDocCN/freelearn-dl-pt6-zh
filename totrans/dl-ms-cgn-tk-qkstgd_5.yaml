- en: Working with Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to explore some more deep learning models with
    CNTK. Specifically, we're going to look at using neural networks for classifying
    image data. All that you've learned in the past chapters will come back in this
    chapter as we discuss how to train convolutional neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural network architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a convolutional neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to feed image data into a convolutional network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to improve network performance with data augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We assume you have a recent version of Anaconda installed on your computer
    and have followed the steps in [Chapter 1](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml),
    *Getting Started with CNTK*, to install CNTK on your computer. The sample code
    for this chapter can be found in our GitHub repository at: [https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch5](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch5).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll work on an example stored in a Jupyter notebook. To
    access the sample code, run the following commands inside an Anaconda prompt in
    the directory where you''ve downloaded the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We'll mention relevant notebooks in each of the sections so you can follow along
    and try out the different techniques yourself.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset for this chapter is not available in the GitHub repository. It would
    be too big to store there. Please open the `Prepare the dataset.ipynb` notebook
    and follow the instructions there to obtain the data for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2Wm6U49](http://bit.ly/2Wm6U49)'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural network architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we've learned how to use regular feed-forward network
    architectures to build neural networks. In a feed-forward neural network, we assume
    that there are interactions between the different input features. But we don't
    make any assumptions about the nature of these interactions. This is, however,
    not always the right thing to do.
  prefs: []
  type: TYPE_NORMAL
- en: When you work with complex data such as images, a feed-forward neural network
    won't do a very good job. This comes from the fact that we assume that there's
    an interaction between the inputs of our network. But we don't account for the
    fact that they are organized in a spatial way. When you look at the pixels in
    an image, there's a horizontal and vertical relationship between them. There's
    also a relationship between the colors in an image and the position of certain
    colored pixels in that image.
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional network is a special kind of neural network that makes the explicit
    assumption that we're dealing with data that has a spatial relationship to it.
    This makes it really good at recognizing images. But other spatially organized
    data will work too. Let's explore the architecture of a convolutional neural network
    used for image classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Network architecture used for image classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Convolutional networks used for image classification typically contain one
    or more convolution layers followed by pooling layers, and usually end in regular
    fully-connected layers to provide the final output as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bf0afaf-b58a-413f-ab2f-4caa7b6c68ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This image is from: https://en.wikipedia.org/wiki/File:Typical_cnn.png'
  prefs: []
  type: TYPE_NORMAL
- en: When you take a closer look at the structure of a convolutional network, you'll
    see that it starts with a set of **convolution** and **pooling** layers. You can
    consider this part a complex, trainable photo filter. The **convolution layers**
    filter out interesting details that are needed to classify the image, and the
    **pooling** **layers** summarize these features so that there are fewer data points
    to crunch near the end of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, you will find several sets of convolution layers and pooling layers
    in a neural network for image classification. This is done to be able to extract
    more complex details from the image. The first layer of the network extracts simple
    details, such as lines, from the image. The next set of layers then combines the
    output of the previous set of layers to learn more complex features, such as corners
    or curves. As you can imagine, the layers after that are used to learn increasingly
    more complex features.
  prefs: []
  type: TYPE_NORMAL
- en: Often, when you build a neural network, you want to classify what's in the image.
    This is where the classic dense layers play an important role. Typically, a model
    used for image recognition will end in one output layer and one or more dense
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at how to work with convolutional and pooling layers to create
    a convolutional neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Working with convolution layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you''ve seen what a convolutional network looks like, let''s look
    at the convolution layer that is used in the convolutional network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad85d578-6750-4eaf-8626-068c79382417.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This image is from: https://en.wikipedia.org/wiki/File:Conv_layer.png'
  prefs: []
  type: TYPE_NORMAL
- en: The convolution layer is the core building block of the convolutional network.
    You can consider the convolution layer a trainable filter that you can use to
    extract important details from the input and remove data that is considered noise.
    A convolution layer contains a set of weights that cover a small area (width and
    height) but cover all channels of the input given to the layer. When you create
    a convolution layer, you need to specify its depth in neurons. You'll find that
    most frameworks, including CNTK, talk about filters when talking about the depth
    of the layer.
  prefs: []
  type: TYPE_NORMAL
- en: When we perform a forward pass, we slide the filters of the layer across the
    input and perform a dot product operation between the input and the weights for
    each of the filters. The sliding motion is controlled by a stride setting. When
    you specify a stride of 1, you will end up with an output matrix that has the
    same width and height as the input, but with the same depth as the number of filters
    in the layer. You can set a different stride, which will reduce the width and
    height of the output matrix.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the input size and number of filters, you can configure the padding
    of the layer. Adding padding to a convolution layer will add a border of zeros
    around the processed input data. This may sound like a weird thing to do but can
    come in quite handy in some situations.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you look at the output size of a convolution layer, it will be determined
    based on the input size, the number of filters, the stride, and padding. The formula
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29379922-0057-4588-808c-60406285ff02.png)'
  prefs: []
  type: TYPE_IMG
- en: '*W* is the input size, *F* is the number of filters or depth of the layer,
    *P* the padding, and *S* the stride.'
  prefs: []
  type: TYPE_NORMAL
- en: Not all combinations of input size, filters, stride, and padding are valid,
    though. For example, when you have an input size *W* = 10 and a layer depth *F*
    = 3 and a stride *S* = 2, then you'll end up with an output volume of 5.5\. Not
    all inputs will perfectly map to this output size, so CNTK will raise an exception.
    This is where the padding setting comes in. By specifying padding, we can make
    sure that all inputs are mapped to output neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'The settings for input size and filters we''ve just discussed may feel a little
    abstract, but there''s sense to them. Setting a larger input size will cause the
    layer to capture coarser patterns from the input. Setting a smaller input size
    will make the layer better at detecting finer patterns. The depth or number of
    filters controls how many different patterns can be detected in the input image.
    At a high level, you could say that a convolutional filter with one filter detects
    one pattern; for example, horizontal lines. A layer with two filters can detect
    two different patterns: horizontal and vertical lines.'
  prefs: []
  type: TYPE_NORMAL
- en: Coming up with the right settings for a convolutional network can be quite a
    bit of work. Luckily, CNTK has settings that help make this process a little less
    complex.
  prefs: []
  type: TYPE_NORMAL
- en: Training a convolutional layer is done in the same way as a regular dense layer.
    This means that we'll perform a forward pass, calculate the gradients, and use
    the learner to come up with better values for the parameters in a backward pass.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution layers are often followed by a pooling layer to compress the features
    learned by the convolutional layer. Let's look at pooling layers next.
  prefs: []
  type: TYPE_NORMAL
- en: Working with pooling layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section we've looked at convolutional layers and how they can
    be used to extract details from pixel data. Pooling layers are used to summarize
    the extracted details. Pooling layers help reduce the volume of data so that it
    becomes easier to classify this data.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s important to understand that neural networks have a harder time to classify
    a sample when the sample has a lot of different input features. That''s why we
    use a combination of convolution layers and pooling layers to extract details
    and summarize them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14a89752-593c-4dcb-a9ad-ecbc785bfe33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This image is from: https://en.wikipedia.org/wiki/File:Max_pooling.png'
  prefs: []
  type: TYPE_NORMAL
- en: A pooling layer features a downsampling algorithm that you configure with an
    input size and stride. We will feed the output of each filter in the previous
    convolution layer into the pooling layer. The pooling layer moves across the slices
    of data and takes small windows equal to the configured input size. It takes the
    small areas of values and grabs the highest value from them as the output for
    that area. Just like with the convolution layer, it uses the stride to control
    how fast it moves across the input. For example, a size of 1 combined with a stride
    of 2 will reduce the data dimensions by half. By using only the highest input
    value, it discards 75% of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: Max-sampling, as this pooling technique is called, is not the only way in which
    a pooling layer can reduce the dimensionality of the input data. You can also
    use average-pooling. In this case, the average value of the input area is used
    as output in the pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: Note that pooling layers only reduce the size of the input along the width and
    height. The depth remains the same as before, so you can rest assured that features
    are only downsampled and not removed completely.
  prefs: []
  type: TYPE_NORMAL
- en: Since pooling layers have a fixed algorithm to downsample input data, there
    are no trainable parameters in them. This means that it takes no time to train
    pooling layers.
  prefs: []
  type: TYPE_NORMAL
- en: Other uses for convolutional networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''re focusing our efforts on using convolutional networks for image classification,
    but you can use this kind of neural network for many more scenarios, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Object detection in images. The CNTK website includes a nice example that shows
    how to build an object detection model: [https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN](https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detect faces in photos and predict the age of the person in the photo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caption images using a combination of convolutional and recurrent neural networks
    that we discuss in [Chapter 6](a5da9ef2-399a-4c30-b751-318d64939369.xhtml), *Working
    with Time Series Data*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict the distance to the bottom of a lake from sonar images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you start to combine convolutional networks for different tasks, you can
    build some pretty powerful applications; for example, a security camera that detects
    people in the videostream and warns the security guard about trespassers.
  prefs: []
  type: TYPE_NORMAL
- en: Countries such as China are investing heavily in this kind of technology. Convolutional
    networks are used in smart city applications to monitor crossings. Using a deep
    learning model, the authorities can detect accidents at traffic lights and reroute
    the traffic automatically so that the police have an easier job.
  prefs: []
  type: TYPE_NORMAL
- en: Building convolutional networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you've seen the basics behind convolutional networks and some common
    use cases for them, let's take a look at how to build one with CNTK.
  prefs: []
  type: TYPE_NORMAL
- en: We're going to build a model that can recognize handwritten digits in images.
    There's a free dataset available called the MNIST dataset that contains 60,000
    samples of handwritten digits. There's also a test set available with 10,000 samples
    for the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started and see what building a convolutional network looks like in
    CNTK. First, we'll look at how to put together the structure of the convolutional
    neural network, we then will take a look at how to train the parameters of a convolutional
    neural network. Finally, we'll explore how to improve the neural network by changing
    it's structure with different layer setups.
  prefs: []
  type: TYPE_NORMAL
- en: Building the network structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typically, when you build a neural network for recognizing patterns in images,
    you will use a combination of convolution and pooling layers. The end of the network
    should contain one or more hidden layers, ending with a softmax layer for classification
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build the network structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, import the required layers for the neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, import the activation functions for the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, import the `glorot_uniform initializer` function to initialize the convolutional
    layers later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, import the `input_variable` function to create input variables and
    the `default_options` function to make configuration of the neural network a little
    easier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new `input_variable` store the input images, they will contain `3`
    channels (red, green, and blue) and have a size of `28` by `28` pixels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create another `input_variable` to store the labels to predict.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, create the `default_options` for the network and use the `glorot_uniform`
    as the initialization function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, create a new `Sequential` layer set to structure the neural network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within the `Sequential` layer set, add a `Convolutional2D` layer with a `filter_shape`
    of `5` and a `strides` setting of `1` and set the number of filters to `8`. Enable
    `padding` so the image is padded to retain the original dimensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a `MaxPooling` layer with a `filter_shape` of `2` and a `strides` setting
    of `2` to compress the image by half.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add another `Convolution2D` layer with a `filter_shape` of 5 and a `strides`
    setting of 1, use 16 filters. Add `padding` to retain the size of the image produced
    by the previous pooling layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, add another `MaxPooling` layer with a `filter_shape` of 3 and a `strides`
    setting of 3 to reduce the image to a third.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, add a `Dense` layer with 10 neurons for the 10 possible classes the
    network can predict. Use a `log_softmax` activation function to turn the network
    into a classification model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We're using images of 28x28 pixels as the input for the model. This size is
    fixed, so when you want to make a prediction with this model, you need to provide
    the same size images as input.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this model is still very basic and will not produce perfect results,
    but it is a good start. Later on, we can start to tune it should we need to.
  prefs: []
  type: TYPE_NORMAL
- en: Training the network with images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have the structure of the convolutional neural network, let''s
    explore how to train it. Training a neural network that works with images requires
    more memory than most computers have available. This is where the minibatch sources
    from [Chapter 3](f7cd9148-99e8-427c-acf4-d74c3e52df58.xhtml), *Getting Data into
    Your Neural Network*, come into play. We''re going to set up a set of two minibatch
    sources to train and evaluate the neural network we''ve just created. Let''s first
    take a look at how to construct a minibatch source for images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, Import the `os` package to get access to some useful filesystem functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, import the necessary components to create a new `MinibatchSource`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new function `create_datasource` which takes the path to an input folder
    and a `max_sweeps` setting to control how often we can iterate over the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within the `create_datasource` function, locate the mapping.bin file within
    the source folder. This file will contain a mapping between the image on disk
    and its associated label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then create a set of stream definitions to read from the mapping.bin file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a `StreamDef` for the image file. Make sure to include the `transforms`
    keyword argument and initialize it with an empty array.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add another `StreamDef` for the labels field with 10 features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new `ImageDeserializer` and provide it the `mapping_file` and the `stream_definitions`
    variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, create a `MinibatchSource` and provide it with the deserializer and
    the `max_sweeps` setting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note, you can create the files necessary for training using the code in the
    `Preparing the dataset.ipynb` Python notebook. Make sure you have enough room
    on your hard drive to store the images. 1 GB of hard drive space is enough to
    store all samples for training and validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the `create_datasource` function, we can create two separate data
    sources to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: First, call the `create_datasource` function with the `mnist_train` folder to
    create the data source for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, call the `create_datasource` function with the `mnist_test` folder and
    set the `max_sweeps` to 1 to create the datasource for validating the neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you''ve prepared the images, it''s time to start training the neural network.
    We can use the `train` method on the `loss` function to kick off the training
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, import the Function decorator from the cntk package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, import the `cross_entropy_with_softmax` function from the losses module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, import the `classification_error` function from the metrics module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, import the `sgd` learner from the learners module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new function `criterion_factory` with two parameters, output and targets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mark the function with the `@Function` decorator to turn it into a CNTK function
    object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within the function, create a new instance of the `cross_entropy_with_softmax`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, create a new instance of the `classification_error` metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return both the loss and metric as a result of the function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After creating the `criterion_factory` function, initialize a new loss with
    it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, setup the `sgd` learner with the parameters of the model and a learning
    rate of 0.2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we''ve setup the loss and learner for the neural network, let''s look
    at how to train and validate the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First import the `ProgressPrinter` class from the `logging` module
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, import the `TestConfig` class from the `train` module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new instance of the `ProgressPrinter` so we can log the output of the
    training process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, create the `TestConfig` for the neural network using the `test_datasource`
    that we made earlier as input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new dictionary to map the data streams from the `train_datasource`
    to the input variables of the neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, call the `train` method on the `loss` and provide the `train_datasource`,
    the settings for the trainer, the `learner`, `input_map` and callbacks to use
    during training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you execute the python code, you will get back output that looks similar
    to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the loss decreases over time. It does take quite a long time to reach
    a low enough value for the model to be usable. Training an image classification
    model will take a very long time, so this is one of the cases where using GPU
    will make a big difference to the amount of time it takes to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Picking the right combination of layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous sections we've seen how to use convolutional and pooling layers
    to build a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: We've just seen that it takes quite a long time to train a model used for image
    recognition. Aside from the long training time, picking the right setup for a
    convolutional network is very hard and takes a long time. Often you will need
    hours of running experiments to find a network structure that works. This can
    be very demotivating for aspiring AI developers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lucky for us, there are several research groups working on finding the best
    architecture for neural networks used in image classification tasks. There are
    several different architectures that have been used successfully in competitions
    and real-life scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: VGG-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And there are several more. While we can't go into detail on how to build each
    of these architectures work, let's explore them on a functional level to see how
    they work so that you can make a more informed choice about which network architecture
    to try in your own application.
  prefs: []
  type: TYPE_NORMAL
- en: The VGG network architecture was invented by the Visual Geometry Group as a
    way to classify images in 1,000 different categories. This is quite hard to do,
    but the team managed to get an accuracy of 70.1%, which is quite good, considering
    how hard it is to differentiate between 1,000 different categories
  prefs: []
  type: TYPE_NORMAL
- en: The VGG network architecture uses stacks of convolution layers with an input
    size of 3x3\. The layers get an ever-increasing depth. Starting at layers with
    32 filters, continuing with 48 filters, all the way up to 512 filters. The reduction
    of the data volume is done using 2x2 pooling filters. The VGG network architecture
    was state-of-the-art when it was invented in 2015, as it had much better accuracy
    than the models invented previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other ways to build neural networks for image recognition, though.
    The ResNet architecture uses what''s called a micro-architecture. It still uses
    convolution layers, but this time they are arranged in blocks. The architecture
    is still very similar to other convolutional networks, but where the VGG network
    has long chain layers, the ResNet architecture has skip connections around blocks
    of convolution layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8cff3e4-456c-49b2-934b-f9e9cc603f31.png)'
  prefs: []
  type: TYPE_IMG
- en: ResNet architecture
  prefs: []
  type: TYPE_NORMAL
- en: This is where the term micro-architecture comes from. Each of the blocks is
    a micro-network capable of learning patterns from the input. Each block has several
    convolution layers and a residual connection. This connection bypasses the block
    of convolution layers, and the data coming from this residual connection is added
    to the output of the convolution layers. The idea behind this is that the residual
    connection shakes up the learning process in the network so that it learns better
    and faster.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the VGG network architecture, the ResNet architecture is deeper
    but easier to train since it has fewer parameters that you need to optimize. The
    VGG network architecture takes up 599 MB of memory, while the ResNet architecture
    takes only 102 MB.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final network architecture that we''ll explore is the Inception architecture.
    This architecture is also one from the micro-architecture category. Instead of
    the residual blocks that are used in the ResNet architecture, the inception network
    uses inception blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0d66db2-9de6-4355-a709-34d0e87c754d.png)'
  prefs: []
  type: TYPE_IMG
- en: Inception Network
  prefs: []
  type: TYPE_NORMAL
- en: The inception blocks in the Inception architecture use convolution layers with
    different input sizes of 1x1, 3x3, and 5x5, which are then concatenated along
    the channel axis. This generates a matrix that has the same width and height as
    the input but has more channels than the input. The idea is that when you do this,
    you have a much better spread of features extracted from the input and thus much
    better quality data to perform the classification task on. The Inception architecture
    depicted here is very shallow; the full version used normally can have more than
    two inception blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you start to work on the other convolutional network architectures you
    will quickly find that you need a lot more computational power to train them.
    Often, the dataset won''t fit into memory and your computer will just be too slow
    to train the model within a reasonable amount of time. This is where distributed
    training can help out. If you''re interested in training models using multiple
    machines, you should definitely take a look at this chapter in the CNTK manual:
    [https://docs.microsoft.com/en-us/cognitive-toolkit/Multiple-GPUs-and-machines](https://docs.microsoft.com/en-us/cognitive-toolkit/Multiple-GPUs-and-machines).'
  prefs: []
  type: TYPE_NORMAL
- en: Improving model performance with data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks used for image recognition not only are difficult to set up
    and train, they also require a lot of data to train. Also, they tend to overfit
    on the images used during training. For example, when you only use photos of faces
    in an upright position, your model will have a hard time recognizing faces that
    are rotated in another direction.
  prefs: []
  type: TYPE_NORMAL
- en: To help overcome problems with rotation and shifts in certain directions, you
    can use image augmentation. CNTK supports specific transforms when creating a
    minibatch source for images.
  prefs: []
  type: TYPE_NORMAL
- en: We've included an additional notebook for this chapter that demonstrates how
    to use the transformations. You can find the sample code for this section in the
    `Recognizing hand-written digits with augmented data.ipynb` file in the samples
    for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several transformations that you can use. For example, you can randomly
    crop images used for training with just a few lines of code. Other transformations
    you can use are scale and color. You can find more information about these transformations
    on the CNTK website: [https://cntk.ai/pythondocs/cntk.io.transforms.html](https://cntk.ai/pythondocs/cntk.io.transforms.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the function used to create the minibatch source earlier in this chapter,
    we can change the list of transforms by including a cropping transform as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We've enhanced the function to include a set of image transforms. When we're
    training, we will randomly crop the image so we get more variations of the image.
    This changes the dimensions of the image, however, so we need to also include
    a scale transformation to make sure that it fits the size expected by the input
    layer of our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Using these transforms during training will increase the variation in the training
    data, which reduces the chance that your neural network gets stuck on images that
    have a slightly different color, rotation, or size.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware, though, that these transforms don't generate new samples. They simply
    change the data before it is fed into the trainer. You will want to increase the
    maximum number of epochs to allow for enough random samples to be generated with
    these transforms applied. How many extra epochs of training you need will depend
    on the size of your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: It's also important to keep in mind that the dimensions of the input layer and
    intermediate layers have a large impact on the capabilities of the convolutional
    network. Larger images will naturally work better when you want to detect small
    objects. Scaling images back to a much smaller size will make the smaller object
    disappear or lose too much detail to be recognizable by the network.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional networks that support larger images will, however, take a lot
    more computation power to optimize, so it will take longer to train them and it
    will be harder to get optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, you will need to balance a combination of image size, layer dimensions,
    and what data augmentation you use to get optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've looked at using neural networks to classify images. It's
    very different from working with normal data. Not only do we need a lot more training
    data to get the right result, we also need a different architecture to work with
    images that is better suited for the job.
  prefs: []
  type: TYPE_NORMAL
- en: We've seen how convolution layers and pooling layers can be used to essentially
    create an advanced photo filter that extracts important details from the data
    and summarize these details to reduce the dimensionality of the input to a manageable
    size.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have used the advanced properties of the convolution filters and pooling
    filters, it's back to business as usual with dense layers to produce a classification
    network.
  prefs: []
  type: TYPE_NORMAL
- en: It can be quite hard to come up with a good structure for an image classification
    model, so it's always a good idea to check out one of the existing architectures
    before venturing into image classification. Also, using the right kind of augmentation
    techniques can help quite a bit to get better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Working with images is just one of the scenarios where deep learning is powerful.
    In the next chapter, we'll look at how to use deep learning to train models on
    time-series data, such as stock exchange information, or course information for
    things such as Bitcoin. We'll learn how to use sequences in CNTK and how to build
    a neural network that can reason over time. See you in the next chapter.
  prefs: []
  type: TYPE_NORMAL
