<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Encoding Inputs</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will be covering the following recipes:</p>
<ul>
<li>Need for encoding</li>
<li>Encoding an image</li>
<li>Encoding for recommender systems</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>A typical image is comprised thousands of pixels; text is also comprised thousands of unique words, and the number of distinct customers of a company could be in the millions. Given this, all three—user, text, and images<span>—</span>would have to be represented as a vector in thousands of dimensional planes. The drawback of representing a vector in such a high dimensional space is that we will not able to calculate the similarity of vectors efficiently.</p>
<p>Representing an image, text, or user in a lower dimension helps us in grouping entities that are very similar. Encoding is a way to perform unsupervised learning to represent an input in a lower dimension with minimal loss of information while retaining the information about images that are similar.</p>
<p>In this chapter, we will be learning about the following:</p>
<ul>
<li>Encoding an image to a much a lower dimension
<ul>
<li>Vanilla autoencoder</li>
<li>Multilayer autoencoder</li>
<li>Convolutional autoencoder</li>
</ul>
</li>
<li>Visualizing encodings</li>
<li>Encoding users and items in recommender systems</li>
<li>Calculating the similarity between encoded entities</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Need for encoding</h1>
                </header>
            
            <article>
                
<p>Encoding is typically used where the number of dimensions in a vector is huge. Encoding helps turn a large vector into a vector that has far fewer dimensions without losing much information from the original vector. In the following sections, let's explore the need for encoding images, text, and recommender systems.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Need for encoding in text analysis</h1>
                </header>
            
            <article>
                
<p>To understand the need for encoding in text analysis, let's consider the following scenario. Let's go through the following two sentences:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1374 image-border" src="Images/295eeeca-8cb4-4c90-b9a8-c303c76c4fdf.png" style="width:10.75em;height:5.42em;" width="157" height="79"/></p>
<p>In traditional text analysis, the preceding two sentences are one-hot encoded, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1375 image-border" src="Images/26bdf572-bf24-4ad1-9c8d-30c1cc2c3814.png" style="width:31.92em;height:9.42em;" width="476" height="140"/></p>
<p>Note that there are five unique words in the two sentences.</p>
<p>The preceding one-hot encoded versions of the words result in an encoded version of sentences as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1376 image-border" src="Images/87b77716-a1e5-4266-b56f-37c4292347a9.png" style="width:31.00em;height:5.17em;" width="476" height="79"/></p>
<p class="mce-root"/>
<p>In the preceding scenario, we can see that the Euclidian distance between the two sentences is greater than zero, as the encodings of <strong>like</strong> and <strong>enjoy</strong> are different. However, intuitively, we know that the words enjoy and like are very similar to each other. Further, the distance between (<strong>I</strong>, <strong>Chess</strong>) is the same as (<strong>like</strong>, <strong>enjoy</strong>).</p>
<p>Note that, given that there are five unique words across the two sentences, we represent each word in a five-dimensional space. In an encoded version, we represent a word in a lower dimension (let's say, three-dimensions) in such a way that words that are similar will have less distance between them when compared to words that are not similar.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Need for encoding in image analysis</h1>
                </header>
            
            <article>
                
<p>To understand the need for encoding in image analysis, let's consider the scenario where we group images; however, the labels of images are not present. For further clarification, let's consider the following images of the same label in the MNIST dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/a63ab262-0216-4f6e-82c4-5a3d9254d6f0.png" width="325" height="140"/></p>
<p>Intuitively, we know that both the preceding images correspond to the same label. However, when we take the Euclidian distance between the preceding two images, the distance is greater than zero, as different pixels are highlighted in the preceding two images.</p>
<p>You should notice the following issue in storing the information of an image:</p>
<p>While the image comprises a total of 28 x 28 = 784 pixels, <span>the </span>majority of the columns are black and thus no information is composed in them, resulting in them occupying more space while storing information than is needed.</p>
<p> Using autoencoders, we represent the preceding two images in a lower dimension in such a way that the distance between the two encoded versions is now much smaller and at the same time ensuring that the encoded version does not lose much information from the original image.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Need for encoding in recommender systems</h1>
                </header>
            
            <article>
                
<p>To understand the need for encoding in recommender systems, let's consider the scenario of movie recommendations for customers. Similar to text analysis, if we were to one-hot encode each movie/customer, we would end up with multiple thousand-dimensional vectors for each movie (as there are thousands of movies). Encoding users in a much lower dimension based on <span>the </span>viewing habits of customers, which results in grouping movies based on the similarity of movies, could help us map movies that a user is more likely to watch.</p>
<p>A similar concept can also be applied to e-commerce recommendation engines, as well as recommending products to a customer in a supermarket.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Encoding an image</h1>
                </header>
            
            <article>
                
<p>Image encoding can be performed in multiple ways. In the following sections, we will contrast the performance of vanilla autoencoders, multilayer autoencoders, and convolutional autoencoders. The term auto-encoding refers to encoding in such a way that the original input is recreated with a far fewer number of dimensions in an image.</p>
<p>An autoencoder takes an image as input and encodes the input image into a lower dimension in such a way that we can reconstruct the original image by using only the encoded version of the input image. Essentially, you can think of the encoded version of similar images as having similar encoded values.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Before we define our strategy, let's get a feel for how autoencoders work:</p>
<ol>
<li>We'll define a toy dataset that has one vector with 11 values</li>
<li>We'll represent the 11 values in a lower dimension (two-dimensions):
<ul>
<li>The information present in input data is preserved as much as possible while lowering the dimensions</li>
<li><span><span>The vector in low dimensional space is called an <strong>embedding</strong>/<strong>encoded</strong> <strong>vector</strong>, <strong>bottleneck</strong> <strong>feature</strong>/<strong>vector</strong>, or a <strong>compressed representation</strong></span></span></li>
<li>The 11 values are converted into two values by performing a matrix multiplication of input values with a random weight matrix that is 11 x 2 in dimensions</li>
<li>The lower dimension vector represents bottleneck features. Bottleneck features are features that are required to reconstruct the original image</li>
</ul>
</li>
<li><span>We'll r</span>econstruct the lower dimension bottleneck feature vector to obtain the output vector:
<ul>
<li>The two-dimension feature vector is multiplied by a matrix that is 2 x 11 in shape to obtain an output that is 1 x 11 in shape. Matrix multiplication of 1 x 2 with 2 x 11 vectors gives an output that is 1 x 11 in shape.</li>
</ul>
</li>
<li>We'll calculate the sum of squared difference between the input vector and the output vector</li>
<li>We vary the randomly initialized weight vectors to minimize the sum of squared difference between the input and output vectors</li>
<li><span>The resulting encoded vector would be a lower dimensional vector that represents an 11-dimensional vector in two-dimensional space</span></li>
</ol>
<p>While leveraging neural networks, you can consider the encoded vector as a hidden layer that connects the input and output layer.</p>
<p>Additionally, for the neural network, the input and output layer values are exactly the same and the hidden layer has a lower dimension than the input layer.</p>
<p>In this recipe, we'll learn about multiple autoencoders:</p>
<ul>
<li>Vanilla autoencoder</li>
<li>Multilayer autoencoder</li>
<li>Convolutional autoencoder</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>In the following sections, we will implement multiple variations of autoencoders in Python (the code file is available as</span> <kbd>Auto_encoder.ipynb</kbd> <span>in GitHub).</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Vanilla autoencoder</h1>
                </header>
            
            <article>
                
<p>A vanilla autoencoder looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1085 image-border" src="Images/d67047e4-6d9e-4cd9-bccf-fdd5f74f2ea5.png" style="width:29.25em;height:23.17em;" width="778" height="618"/></p>
<p>As displayed in the preceding diagram, a Vanilla autoencoder reconstructs the input with a minimal number of hidden layers and hidden units in its network.</p>
<p>To understand how a vanilla autoencoder works, let's go through the following recipe, where we reconstruct MNIST images using a lower-dimensional encoded version of the original image (the code file is available as <kbd>Auto_encoder.ipynb</kbd> in GitHub):</p>
<ol>
<li>Import <span>the </span>relevant packages:</li>
</ol>
<pre style="padding-left: 60px">import tensorflow as tf<br/>import keras<br/>import numpy as np<br/>from keras.datasets import mnist<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import Dropout<br/>from keras.layers import Flatten<br/>from keras.layers.convolutional import Conv2D<br/>from keras.layers.convolutional import MaxPooling2D<br/>from keras.utils import np_utils</pre>
<ol start="2">
<li>Import the dataset:</li>
</ol>
<pre style="padding-left: 60px">(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()</pre>
<ol start="3">
<li>Reshape and scale the dataset:</li>
</ol>
<pre style="padding-left: 60px">X_train = X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2])<br/>X_test = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2])<br/>X_train = X_train/255<br/>X_test = X_test/255</pre>
<ol start="4">
<li>Construct the network architecture:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Dense(32, input_dim=784, activation='relu'))<br/>model.add(Dense(784, activation='relu'))<br/>model.summary()</pre>
<p style="padding-left: 60px">A summary of model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/20ebaedb-619c-4ea0-a329-6281c3dc9837.png" style="width:32.58em;height:11.75em;" width="517" height="187"/></p>
<p style="padding-left: 60px">In the preceding code, we are representing a 784-dimensional input in a 32-dimensional encoded version.</p>
<ol start="5">
<li>Compile and fit the model:</li>
</ol>
<pre style="padding-left: 60px">model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])<br/>model.fit(X_train, X_train, validation_data=(X_test, X_test),epochs=10, batch_size=1024, verbose=1)</pre>
<p style="padding-left: 60px">Note that we are using the mean squared error loss function, as the pixel values are continuous. Additionally, the input and output arrays are just the same—<kbd>X_train</kbd>.</p>
<ol start="6">
<li>Print a reconstruction of the first four input images:</li>
</ol>
<pre style="padding-left: 60px">import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>plt.subplot(221)<br/>plt.imshow(model.predict(X_test[0,:].reshape(1,784)).reshape(28,28), cmap=plt.get_cmap('gray'))<br/>plt.axis('off')<br/>plt.subplot(222)<br/>plt.imshow(model.predict(X_test[1,:].reshape(1,784)).reshape(28,28), cmap=plt.get_cmap('gray'))<br/>plt.axis('off')<br/>plt.subplot(223)<br/>plt.imshow(model.predict(X_test[2,:].reshape(1,784)).reshape(28,28), cmap=plt.get_cmap('gray'))<br/>plt.axis('off')<br/>plt.subplot(224)<br/>plt.imshow(model.predict(X_test[3,:].reshape(1,784)).reshape(28,28), cmap=plt.get_cmap('gray'))<br/>plt.axis('off')<br/>plt.show()</pre>
<p>The reconstructured MNIST digits are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/5f536303-198e-479f-a4d6-b3d57a3b67a4.png" style="width:22.17em;height:17.08em;" width="305" height="235"/></p>
<p class="mce-root"/>
<p>To understand how well the autoencoder worked, let's compare the preceding predictions with the original input images:</p>
<p>The original MNIST digits are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/17100fd3-9b8d-42bd-8eab-da2d28d86814.png" style="width:21.92em;height:17.00em;" width="307" height="238"/></p>
<p>From the preceding images, we can see that the reconstructed images are blurred when compared to the original input image.</p>
<p>To get around the issue of blurring, let's build multilayer autoencoders that are deep (thereby resulting in more parameters) and thus potentially a better representation of the original image.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multilayer autoencoder</h1>
                </header>
            
            <article>
                
<p>A multilayer autoencoder looks as follows, where there are more number of hidden layers connecting the input layer to output layer:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1086 image-border" src="Images/1f3ee687-42fa-453a-b0a0-93c0bf13c546.png" style="width:31.83em;height:15.00em;" width="1077" height="507"/></p>
<p class="mce-root"/>
<p><span>Essentially, a multilayer autoencoder reconstructs the input with more hidden layers in its network.</span></p>
<p>To build a multilayer autoencoder, we will repeat the same steps that we had in the previous section, up until <em>step 3</em>. However, <em>step 4</em>, where the network architecture is defined, will be modified to include multilayers, as follows:</p>
<pre>model = Sequential()<br/>model.add(Dense(100, input_dim=784, activation='relu'))<br/>model.add(Dense(32,activation='relu'))<br/>model.add(Dense(100,activation='relu'))<br/>model.add(Dense(784, activation='relu'))<br/>model.summary()</pre>
<p>A summary of model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/2314656c-b2fc-4c71-a031-206d5f870de8.png" style="width:34.17em;height:16.83em;" width="510" height="251"/></p>
<p>In the preceding network, our first hidden layer has 100 units, the second hidden layer (which is the embedded version of the image) is 32-dimensional, and the third hidden layer is 100-dimensional in shape.</p>
<p>Once the network architecture is defined, we compile and run it, as follows:</p>
<pre>model.compile(loss='mean_squared_error', optimizer='adam')<br/>model.fit(X_train, X_train, validation_data=(X_test, X_test),epochs=25, batch_size=1024, verbose=1)</pre>
<p class="mce-root"/>
<p>The predictions of the preceding model are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/ff4c5081-bf2f-4346-9439-33d59b9a2bd4.png" style="width:19.25em;height:15.17em;" width="291" height="229"/></p>
<p>Note that the preceding predictions are still a little blurred compared to the original images.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convolutional autoencoder</h1>
                </header>
            
            <article>
                
<p>So far, we have explored vanilla and multilayer autoencoders. In this section, we will see how convolutional autoencoders work in reconstructing <span>the </span>original images from a lower-dimensional vector.</p>
<p>Convolutional autoencoders look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1087 image-border" src="Images/3cd826e0-e037-416a-992e-7ab29296eb34.png" style="width:31.92em;height:15.42em;" width="1075" height="520"/></p>
<p><span>Essentially, a convolutional autoencoder reconstructs the input with more hidden layers in its network where the hidden layers consist of convolution, pooling, and upsampling the downsampled image.</span></p>
<p class="mce-root"/>
<p>Similar to a multilayer autoencoder, a convolutional autoencoder differs from other types of autoencoder in its model architecture. In the following code, we will define the model architecture for the convolutional autoencoder while every other step remains similar to the vanilla autoencoder up until <em>step 3</em>.</p>
<p>The only differences between the <kbd>X_train</kbd> and <kbd>X_test</kbd> shapes are defined as follows:</p>
<pre>(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()<br/><br/>X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)<br/>X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[2],1)<br/>X_train = X_train/255<br/>X_test = X_test/255</pre>
<p>Note that, in the preceding step, we are reshaping the image so that it can be passed to a <kbd>conv2D</kbd> method:</p>
<ol>
<li>Define the model architecture:</li>
</ol>
<pre style="padding-left: 30px">model = Sequential()<br/>model.add(Conv2D(32, (3,3), input_shape=(28, 28,1), activation='relu',padding='same',name='conv1'))<br/>model.add(MaxPooling2D(pool_size=(2, 2),name='pool1'))<br/>model.add(Conv2D(16, (3,3), activation='relu',padding='same',name='conv2'))<br/>model.add(MaxPooling2D(pool_size=(2, 2),name='pool2'))<br/>model.add(Conv2D(8, (3,3), activation='relu',padding='same',name='conv3'))<br/>model.add(MaxPooling2D(pool_size=(2, 2),name='pool3'))<br/>model.add(Conv2D(32, (3,3), activation='relu',padding='same',name='conv4'))<br/>model.add(MaxPooling2D(pool_size=(2, 2),name='pool4'))<br/>model.add(Flatten(name='flatten'))<br/>model.add(Reshape((1,1,32)))<br/>model.add(Conv2DTranspose(8, kernel_size = (3,3), activation='relu'))<br/>model.add(Conv2DTranspose(16, kernel_size = (5,5), activation='relu'))<br/>model.add(Conv2DTranspose(32, kernel_size = (8,8), activation='relu'))<br/>model.add(Conv2DTranspose(32, kernel_size = (15,15), activation='relu'))<br/>model.add(Conv2D(1, (3, 3), activation='relu',padding='same'))<br/>model.summary()</pre>
<p style="padding-left: 60px">In the preceding code, we have defined a convolutional architecture where we reshaped the input image so that it has a 32-dimensional embedded version in the middle of its architecture and finally upsample it so that we are able to reconstruct it.</p>
<p style="padding-left: 60px">A summary of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/008c615f-9a99-4eb9-9a39-141aa90aeda8.jpg" style="width:34.08em;height:42.42em;" width="455" height="566"/></p>
<p class="mce-root"/>
<ol start="2">
<li>Compile and fit the model</li>
</ol>
<pre>from keras.optimizers import Adam<br/>adam = Adam(lr=0.001)<br/>model.compile(loss='mean_squared_error', optimizer='adam')<br/>model.fit(X_train, X_train, validation_data=(X_test, X_test),epochs=10, batch_size=1024, verbose=1)</pre>
<p>Once we make predictions on the first four test data points, the reconstructed images look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/ede7d605-daf3-4166-a7ae-c42a38c8a464.png" style="width:19.33em;height:15.08em;" width="303" height="236"/></p>
<p>Note that the reconstruction <span>is</span><span> </span><span>now slightly better than the previous two reconstructions (using Vanilla and multilayer autoencoders) of the test images.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Grouping similar images</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we represented each image in a much lower dimension with the intuition that images that are similar will have similar embeddings and images that are not similar will have dissimilar embeddings. However, we have not yet looked at the similarity measure or examined embeddings in detail.</p>
<p>In this section, we will try and plot embeddings in a 2D space. We can reduce the 32-dimensional vector to a two-dimensional space by using a technique called <strong>t-SNE</strong>. (More about t-SNE can be found here: <a href="http://www.jmlr.org/papers/v9/vandermaaten08a.html" target="_blank">http://www.jmlr.org/papers/v9/vandermaaten08a.html</a>.)</p>
<p>This way, our feeling that similar images will have similar embeddings can be proved, as similar images should be clustered together in the two-dimensional plane.</p>
<p>In the following code, we will represent embeddings of all the test images in a two-dimensional plane:</p>
<ol>
<li>Extract the 32-dimensional vector of each of the 10,000 images in <span>the </span>test:</li>
</ol>
<pre style="padding-left: 60px">from keras.models import Model<br/>layer_name = 'flatten'<br/>intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)<br/>intermediate_output = intermediate_layer_model.predict(X_test)</pre>
<ol start="2">
<li>Perform t-SNE to generate a two-dimensional vector:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.manifold import TSNE<br/>tsne_model = TSNE(n_components=2, verbose=1, random_state=0)<br/>tsne_img_label = tsne_model.fit_transform(intermediate_output)<br/>tsne_df = pd.DataFrame(tsne_img_label, columns=['x', 'y'])<br/>tsne_df['image_label'] = y_test</pre>
<ol start="3">
<li>Plot the visualization of the t-SNE dimensions for the test image embeddings:</li>
</ol>
<pre style="padding-left: 60px">from ggplot import *<br/>chart = ggplot(tsne_df, aes(x='x', y='y', color='factor(image_label)'))+ geom_point(size=70,alpha=0.5)<br/>chart</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">A visualization of embeddings in two dimensional space is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/9f76bf7a-b13e-4f60-85bd-a035f6b14dd5.png" width="790" height="489"/></p>
<p>Note that, in the preceding chart, we see that, more often than not, clusters are formed among images that correspond to the same label.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Encoding for recommender systems</h1>
                </header>
            
            <article>
                
<p>So far, in the previous sections, we have encoded an image. In this section, we will encode users and movies in a movie-related dataset. The reason for this is that there could be millions of users as customers and thousands of movies in a catalog. Thus, we are not in a position to one-hot encode such data straight away. Encoding comes in handy in such a scenario. One of the most popular techniques that's used in encoding for recommender systems is matrix factorization. In the next section, we'll understand how it works and generate embeddings for users and movies.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The thinking behind encoding users and movies is as follows:</p>
<p>If two users are similar in terms of liking certain movies, the vectors that represent the two users should be similar. In the same manner, if two movies are similar (potentially, they belong to the same genre or have the same cast), they should have similar vectors.</p>
<p>The strategy that we'll adopt to encode movies, so that we recommend a new set of movies based on the historical set of movies watched by a user, is as follows:</p>
<ol>
<li>Import the dataset that contains information of the users and the rating they gave to different movies that they watched<a href="https://grouplens.org/datasets/movielens/100k/" target="_blank"/></li>
<li>Assign IDs to both users and movies</li>
<li>Convert users and movies into 32-dimensional vectors</li>
<li>Use <span>the </span>functional API in Keras to perform the dot product of the 32-dimensional vectors of movies and users:
<ul>
<li>If there are 100,000 users and 1,000 movies, the movie matrix will be 1,000 x 32 dimensions and the user matrix will be 100,000 x 32 dimensions</li>
<li>The dot product of the two will be 100,000 x 1,000 in dimension</li>
</ul>
</li>
<li>Flatten the output and pass it through a dense layer, before connecting to the output layer, which has a linear activation and has output values ranging from 1 to 5</li>
<li>Fit the model</li>
<li>Extract the embedding weights of movies</li>
<li>Extract the embedding weights of users</li>
<li>Movies that are similar to a given movie of interest can be found by calculating the pairwise similarity of the movie of interest with every other movie in the dataset</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In the following code, we will come up with with a vector for a user and a movie in a typical recommender system <span>(The code file is available</span> as <kbd>Recommender_systems.ipynb</kbd> i<span>n GitHub)</span>:</p>
<ol>
<li>Import the dataset. The recommended dataset is available in code in GitHub.</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import pandas as pd<br/>from keras.layers import Input, Embedding, Dense, Dropout, merge, Flatten, dot<br/>from keras.models import Model<br/>from keras.optimizers import Adam<br/>ratings = pd.read_csv('...') # Path to the user-movie-ratings file</pre>
<p class="CDPAlignCenter CDPAlign"><img src="Images/922112db-3673-419c-ae17-295a587c0051.jpg" style="width:27.83em;height:6.58em;" width="381" height="90"/></p>
<ol start="2">
<li>Convert <span>the </span>user and movies into a categorical variable. In the following code, we create two new variables—<kbd>User2</kbd> and <kbd>Movies2</kbd><span>—which</span> are categorical:</li>
</ol>
<pre style="padding-left: 60px">ratings['User2']=ratings['User'].astype('category')<br/>ratings['Movies2']=ratings['Movies'].astype('category')</pre>
<ol start="3">
<li>Assign a unique ID to each user and movie:</li>
</ol>
<pre style="padding-left: 60px">users = ratings.User.unique()<br/>movies = ratings.Movies.unique()<br/>userid2idx = {o:i for i,o in enumerate(users)}<br/>moviesid2idx = {o:i for i,o in enumerate(movies)}<br/>idx2userid = {i:o for i,o in enumerate(users)}<br/>idx2moviesid = {i:o for i,o in enumerate(movies)}</pre>
<ol start="4">
<li>Add the unique IDs as new columns to our original table:</li>
</ol>
<pre style="padding-left: 60px">ratings['Movies2'] = ratings.Movies.apply(lambda x: moviesid2idx[x])<br/>ratings['User2'] = ratings.User.apply(lambda x: userid2idx[x])</pre>
<ol start="5">
<li>Define embeddings for each user ID and unique ID:</li>
</ol>
<pre style="padding-left: 60px">n_users = ratings.User.nunique()<br/>n_movies = ratings.Movies.nunique()</pre>
<p style="padding-left: 60px">In the preceding code, we are extracting the total number of unique users and unique movies in the dataset:</p>
<pre style="padding-left: 60px">def embedding_input(name,n_in,n_out):<br/>  inp = Input(shape=(1,),dtype='int64',name=name)<br/>  return inp, Embedding(n_in,n_out,input_length=1)(inp)</pre>
<p style="padding-left: 60px">In the preceding code, we are defining a function that takes an ID as input and converts it into an embedding vector that is <kbd>n_out</kbd> in dimensions for the total of <kbd>n_in</kbd> values:</p>
<pre style="padding-left: 60px">n_factors = 100<br/>user_in, u = embedding_input('user_in', n_users, n_factors)<br/>article_in, a = embedding_input('article_in', n_movies, n_factors)</pre>
<p style="padding-left: 60px">In the preceding code, we are extracting 100 dimensions for each unique user and also for each unique movie.</p>
<ol start="6">
<li>Define the model:</li>
</ol>
<pre style="padding-left: 60px">x = dot([u,a], axes=1)<br/>x=Flatten()(x)<br/>x = Dense(500, activation='relu')(x)<br/>x = Dense(1)(x)<br/>model = Model([user_in,article_in],x)<br/>adam = Adam(lr=0.01)<br/>model.compile(adam,loss='mse')<br/>model.summary()</pre>
<p class="mce-root"/>
<p>A summary of model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/f135011c-a121-4c89-868a-a4d1f89a3baa.jpg" width="765" height="401"/></p>
<ol start="7">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 60px">model.fit([ratings.User2,ratings.Movies2], ratings.rating, epochs=50,batch_size=128)</pre>
<ol start="8">
<li>Extract the vectors of each user or movie:</li>
</ol>
<pre style="padding-left: 60px"># Extracting user vectors<br/>model.get_weights()[0]<br/><br/># Extracting movie vectors<br/>model.get_weights()[1]</pre>
<p>As we thought earlier, movies that are similar should have similar vectors.</p>
<p>Typically, while identifying the similarity between embeddings, we use a measure named cosine similarity (there's more information on how cosine similarity is calculated in the next chapter).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For a randomly selected movie that is located in the 574<sup>th</sup> position, cosine similarity is calculated as follows:</p>
<pre style="padding-left: 60px">from sklearn.metrics.pairwise import cosine_similarity<br/>np.argmax(cosine_similarity(model.get_weights()[1][574].reshape(1,-1),model.get_weights()[1][:574].reshape(574,100)))</pre>
<p>From the preceding code, we can calculate the ID that is most similar to the movie located in the 574<sup><span>th</span></sup> location of the categorical movie column.</p>
<p>Once we look into the movie ID list, we should see that the most similar movies to the given movie, indeed happen to be similar, intuitively.</p>


            </article>

            
        </section>
    </div>



  </body></html>