- en: Population-Scale Clustering and Ethnicity Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding variations in genome sequences assists us in identifying people
    who are predisposed to common diseases, curing rare diseases, and finding the
    corresponding population group of individuals from a larger population group.
    Although classical machine learning techniques allow researchers to identify groups
    (that is, clusters) of related variables, the accuracy and effectiveness of these
    methods diminish for large and high-dimensional datasets such as the whole human
    genome.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, **Deep Neural Networks** (**DNNs**) form the core of **deep
    learning** (**DL**) and provide algorithms to model complex, high-level abstractions
    in data. They can better exploit large-scale datasets to build complex models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we apply the K-means algorithm to large-scale genomic data
    from the 1000 Genomes project analysis aimed at clustering genotypic variants
    at the population scale. Finally, we train an H2O-based DNN model and a Spark-based
    random forest model for predicting geographic ethnicity. The theme of this chapter
    is *give me your genetic variants data and I will tell your ethnicity*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, we will configure H2O so that the same setting can be used in
    upcoming chapters too. Concisely, we will learn the following topics throughout
    this end-to-end project:'
  prefs: []
  type: TYPE_NORMAL
- en: Population-scale clustering and geographic ethnicity prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 1000 Genomes project, a deep catalog of human genetic variants
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms and tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using K-means for population-scale clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using H2O for ethnicity prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using random forest for ethnicity prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Population scale clustering and geographic ethnicity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Next-generation genome sequencing** (**NGS**) reduces overhead and time for
    genomic sequencing, leading to big data production in an unprecedented way. In
    contrast, analyzing this large-scale data is computationally expensive and increasingly
    becomes the key bottleneck. This increase in NGS data in terms of number of samples
    overall and features per sample demands solutions for massively parallel data
    processing, which imposes extraordinary challenges on machine learning solutions
    and bioinformatics approaches. The use of genomic information in medical practice
    requires efficient analytical methodologies to cope with data from thousands of
    individuals and millions of their variants.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important tasks is the analysis of genomic profiles to attribute
    individuals to specific ethnic populations, or the analysis of nucleotide haplotypes
    for disease susceptibility. The data from the 1000 Genomes project serves as the
    prime source to analyze genome-wide **single nucleotide polymorphisms** (**SNPs**)
    at scale for the prediction of the individual's ancestry with regards to continental
    and regional origins.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning for genetic variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Research has revealed that population groups from Asia, Europe, Africa, and
    America can be separated based on their genomic data. However, it is more challenging
    to accurately predict the haplogroup and the continent of origin, that is, geography,
    ethnicity, and language. Other research shows that the Y chromosome lineage can
    be geographically localized, forming the evidence for (geographically) clustering
    the human alleles of the human genotypes.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the clustering of individuals is correlated with geographic origin and
    ancestry. Since race depends on ancestry as well, the clusters are also correlated
    with the more traditional concepts of race, but the correlation is not perfect
    since genetic variation occurs according to probabilistic principles. Therefore,
    it does not follow a continuous distribution in different races and rather overlaps
    across or spills into different populations.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the identification of ancestry, or even race, may prove to be useful
    for biomedical reasons, but any direct assessment of disease-related genetic variation
    will ultimately yield more accurate and beneficial information.
  prefs: []
  type: TYPE_NORMAL
- en: The datasets provided by various genomics projects, such as **The Cancer Genome
    Atlas** (**TCGA**), **International Cancer Genome Consortium (ICGC)**, **1000
    Genomes Projects**, and **Personal Genome Project** (**PGP**), dispose of large-scale
    data. For fast processing of such data, ADAM and Spark-based solutions have been
    proposed and are now widely used in genomics data analytics research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark forms the most efficient data-processing framework and, in addition,
    provides primitives for in-memory cluster computing, for example, for querying
    the user data repeatedly. This makes Spark an excellent candidate for machine
    learning algorithms that outperform the Hadoop-based MapReduce framework. By using
    the genetic variants dataset from the 1000 Genomes project, we will try to answer
    the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How is human genetic variation distributed geographically among different population
    groups?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we use the genomic profile of individuals to attribute them to specific
    populations or derive disease susceptibility from their nucleotide haplotype?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the individual's genomic data suitable to predict geographic origin (that
    is, the population group for an individual)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this project, we addressed the preceding questions in a scalable and more
    efficient way. Particularly, we examined how we applied Spark and ADAM for large-scale
    data processing, H2O for K-means clustering of the whole population to determine
    inter- and intra-population groups, and MLP-based supervised learning by tuning
    more hyperparameters to more accurately predict the population group for an individual
    according to the individual's genomic data. Do not worry at this point; we will
    provide the technical details on working with these technologies in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: However, before getting started, let's take a brief journey to the 1000 Genomes
    Project dataset to provide you with some justification on why interoperating these
    technologies is really important.
  prefs: []
  type: TYPE_NORMAL
- en: 1000 Genomes Projects dataset description
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data from the 1000 Genomes project is a very large catalog of human genetic
    variants. The project aims to determine genetic variants with frequencies higher
    than 1% in the populations studied. The data has been made openly available and
    freely accessible through public data repositories to scientists worldwide. Also,
    the data from the 1000 Genomes project is widely used to screen variants discovered
    in exome data from individuals with genetic disorders and in cancer genome projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The genotype dataset in **Variant Call Format** (**VCF**) provides the data
    of human individuals (that is, samples) and their genetic variants, and in addition,
    the global allele frequencies as well as the ones for the super populations. The
    data denotes the population''s region for each sample which is used for the predicted
    category in our approach. Specific chromosomal data (in VCF format) may have additional
    information denoting the super-population of the sample or the sequencing platform
    used. For multiallelic variants, each alternative **allele frequency** (**AF**)
    is presented in a comma-separated list, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The AF is calculated as the quotient of **Allele Count** (**AC**) and **Allele
    Number** (**AN**) and NS is the total number of samples with data, whereas `_AF`
    denotes the AF for a specific region.
  prefs: []
  type: TYPE_NORMAL
- en: The 1000 Genomes Project started in 2008; the consortium consisted of more than
    400 life scientists and phase 3 finished in September 2014 covering `2,504` individuals
    from 26 populations (that is, ethnic backgrounds) in total. In total, over 88
    million variants (84.7 million **single nucleotide polymorphisms** (**SNPs**),
    3.6 million short insertions/deletions (indels), and 60,000 structural variants)
    have been identified as high-quality haplotypes.
  prefs: []
  type: TYPE_NORMAL
- en: In short, 99.9% of the variants consist of SNPs and short indels. Less important
    variants—including SNPs, indels, deletions, complex short substitutions, and other
    structural variant classes—have been removed for quality control. As a result,
    the third phase release leaves 84.4 million variants.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the 26 populations has about 60-100 individuals from Europe, Africa,
    America (South and North), and Asia (South and East). The population samples are
    grouped into super-population groups according to their predominant ancestry:
    East Asian (**CHB**, **JPT**, **CHS**, **CDX**, and **KHV**), European (**CEU**,
    **TSI**, **FIN**, **GBR**, and **IBS**), African (**YRI**, **LWK**, **GWD**, **MSL**,
    **ESN**, **ASW**, and **ACB**), American (**MXL**, **PUR**, **CLM**, and **PEL**),
    and South Asian (**GIH**, **PJL**, **BEB**, **STU**, and **ITU**). For details,
    refer to *Figure 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c184827-d03e-4b8d-a33b-7a1084dbf66b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Geographic ethnic groups from 1000 Genomes project''s release 3 (source
    http://www.internationalgenome.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: The released datasets provide the data for 2,504 healthy adults (18 years and
    older, third project phase); only reads with at least 70 **base pairs** (**bp**)
    have been used until more advanced solutions are available. All genomic data from
    all samples were combined to attribute all variants to a region. However, note
    that specific haplotypes may not occur in the genomes of a particular region;
    that is, the multi-sample approach allows attributing variants to an individual's
    genotype even if the variants are not covered by sequencing reads from that sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, overlapping reads are provided and the single sample genomes
    have not necessarily been consolidated. All individuals were sequenced using both
    of these:'
  prefs: []
  type: TYPE_NORMAL
- en: Whole-genome sequencings (*mean depth = 7.4x*, where *x* is the number of reads,
    on average, that are likely to be aligned at a given reference *bp*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Targeted exome sequencing (*mean depth = 65.7x*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, individuals and their first-degree relatives such as an adult
    offspring were genotyped using high-density SNP microarrays. Each genotype comprises
    all 23 chromosomes and a separate panel file denotes the sample and population
    information. *Table 1* gives an overview of the different releases of the 1000
    Genomes project:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1 – Statistics of the 1000 Genomes project''s genotype dataset** **(source:**
    [http://www.internationalgenome.org/data](http://www.internationalgenome.org/data)**)**'
  prefs: []
  type: TYPE_NORMAL
- en: '| **1000 genome release** | **Variants** | **Individual** | **Populations**
    | **File format** |'
  prefs: []
  type: TYPE_TB
- en: '| Phase 3 | Phase 3 | 2,504 | 26 | VCF |'
  prefs: []
  type: TYPE_TB
- en: '| Phase 1 | 37.9 million | 1,092 | 14 | VCF |'
  prefs: []
  type: TYPE_TB
- en: '| Pilot | 14.8 million | 179 | 4 | VCF |'
  prefs: []
  type: TYPE_TB
- en: The AF in the five super-population groups, **EAS=East Asian**, **EUR=European**,
    **AFR=African**, **AMR=American**, **SAS=South Asian** populations are calculated
    from allele numbers (AN, range= [0, 1]).
  prefs: []
  type: TYPE_NORMAL
- en: See the details of the panel file at [ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel).
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms, tools, and techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large-scale data from release 3 of the 1000 Genomes project contributes to 820
    GB of data. Therefore, ADAM and Spark are used to pre-process and prepare the
    data (that is, training, testing, and validation sets) for the MLP and K-means
    models in a scalable way. Sparkling water transforms the data between H2O and
    Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Then, K-means clustering, the MLP (using H2O) are trained. For the clustering
    and classification analysis, the genotypic information from each sample is required
    using the sample ID, variation ID, and the count of the alternate alleles where
    the majority of variants that we used were SNPs and indels.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we should know the minimum info about each tool used such as ADAM, H2O,
    and some background information on the algorithms such as K-means, MLP for clustering,
    and classifying the population groups.
  prefs: []
  type: TYPE_NORMAL
- en: H2O and Sparkling water
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'H2O is an AI platform for machine learning. It offers a rich set of machine
    learning algorithms and a web-based data processing UI that comes as both open
    sources as well as commercial. Using H2O, it''s possible to develop machine learning
    and DL applications with a wide range of languages, such as Java, Scala, Python,
    and R:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96b2598b-a253-48cd-8213-de1386bb9314.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The H2O compute engine and available features (source: https://h20.ai/)'
  prefs: []
  type: TYPE_NORMAL
- en: It also has the ability to interface with Spark, HDFS, SQL, and NoSQL databases.
    In short, H2O works with R, Python, and Scala on Hadoop/Yarn, Spark, or laptop.
    On the other hand, Sparkling water combines the fast, scalable ML algorithms of
    H2O with the capabilities of Spark. It drives the computation from Scala/R/Python
    and utilizes the H2O flow UI. In short, Sparkling *water = H2O + Spark*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout the next few chapters, we will explore and the wide rich features
    of H2O and Sparkling water; however, I believe it would be useful to provide a
    diagram of all of the functional areas that it covers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dbb845b1-18f8-431e-97d1-195b6b8d84ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A glimpse of available algorithms and the supported ETL techniques
    (source: https://h20.ai/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a list of features and techniques curated from the H2O website. It
    can be used for wrangling data, modeling using the data, and scoring the resulting
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: Process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scoring tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalized linear models** (**GLM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregate, filter, bin, and derive columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient boosting machine** (**GBM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slice, log transform, and anonymize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hit ratio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variable creation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA/PCA score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodel scoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and validation sampling plan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows how to provide a clear method of describing the
    way in which H2O Sparkling water can be used to extend the functionality of Apache
    Spark. Both H2O and Spark are open source systems. Spark MLlib contains a great
    deal of functionality, while H2O extends this with a wide range of extra functionalities,
    including DL. It offers tools to transform, model, and score the data, as we can
    find in Spark ML. It also offers a web-based user interface to interact with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b10c6dbb-b830-491c-9d73-d0b27bdde21c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Sparkling water extends H2O and interoperates with Spark (source:
    https://h20.ai/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows how H2O integrates with Spark. As we already know,
    Spark has master and worker servers; the workers create executors to do the actual
    work. The following steps occur to run a Sparkling water-based application:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark's submit command sends the Sparkling water JAR to the Spark master
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Spark master starts the workers and distributes the JAR file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Spark workers start the executor JVMs to carry out the work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Spark executor starts an H2O instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The H2O instance is embedded with the Executor JVM, and so it shares the JVM
    heap space with Spark. When all of the H2O instances have started, H2O forms a
    cluster, and then the H2O flow web interface is made available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10b221b6-aa8f-4e8f-ab55-a9da2d90afc3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: How Sparkling water fits into the Spark architecture (source: http://blog.cloudera.com/blog/2015/10/how-to-build-a-machine-learning-app-using-sparkling-water-and-apache-spark/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure explains how H2O fits into the Spark architecture and
    how it starts, but what about data sharing? Now the question would be: how does
    data pass between Spark and H2O? The following diagram explains this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b0ab4ad-f630-4cc0-aa39-d8253355e303.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Data passing mechanism between Spark and H2O'
  prefs: []
  type: TYPE_NORMAL
- en: To get a clearer view of the preceding figure, a new H2O RDD data structure
    has been created for H2O and Sparkling water. It is a layer based at the top of
    an H2O frame, each column of which represents a data item and is independently
    compressed to provide the best compression ratio.
  prefs: []
  type: TYPE_NORMAL
- en: ADAM for large-scale genomics data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analyzing DNA and RNA sequencing data requires large-scale data processing to
    interpret the data according to its context. Excellent tools and solutions have
    been developed at academic labs, but often fall short on scalability and interoperability.
    By this means, ADAM is a genomics analysis platform with specialized file formats
    built using Apache Avro, Apache Spark and Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: However, large-scale data processing solutions such as ADAM-Spark can be applied
    directly to the output data from a sequencing pipeline, that is, after quality
    control, mapping, read preprocessing, and variant quantification using single
    sample data. Some examples are DNA variants for DNA sequencing, read counts for
    RNA sequencing, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'See more at [http://bdgenomics.org/](http://bdgenomics.org/) and the related
    publication: Massie, Matt and Nothaft, Frank et al., ADAM: Genomics Formats and
    Processing Patterns for Cloud Scale Computing, UCB/EECS-2013-207, EECS Department,
    University of California, Berkeley.'
  prefs: []
  type: TYPE_NORMAL
- en: In our study, ADAM is used to achieve the scalable genomics data analytics platform
    with support for the VCF file format so that we can transform genotype-based RDD
    into a Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning is a type of machine learning algorithm used for grouping
    related data objects and finding hidden patterns by inferencing from unlabeled
    datasets—that is, training sets consisting of input data without labels.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see a real-life example. Suppose you have a large collection of non-pirated
    and totally legal MP3 files in a crowded and massive folder on your hard drive.
    Now, what if you could build a predictive model that helps you automatically group
    together similar songs and organize them into your favorite categories, such as
    country, rap, and rock?
  prefs: []
  type: TYPE_NORMAL
- en: This is an act of assigning an item to a group so that an MP3 is added to the
    respective playlist in an unsupervised way. For classification, we assume that
    you are given a training dataset of correctly labeled data. Unfortunately, we
    do not always have that luxury when we collect data in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we would like to divide a huge collection of music into
    interesting playlists. How can we possibly group together songs if we do not have
    direct access to their metadata? One possible approach is a mixture of various
    ML techniques, but clustering is often at the heart of the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd15ee54-52ae-47a9-8dcf-646723794d3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Clustering data samples at a glance'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the main objective of unsupervised learning algorithms is to
    explore unknown/hidden patterns in input data that is unlabeled. Unsupervised
    learning, however, also comprehends other techniques to explain the key features
    of the data in an exploratory way to find the hidden patterns. To overcome this
    challenge, clustering techniques are used widely to group unlabeled data points
    based on certain similarity measures in an unsupervised way.
  prefs: []
  type: TYPE_NORMAL
- en: Population genomics and clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering analysis is about dividing data samples or data points and putting
    them into corresponding homogeneous classes or clusters. Thus, a simple definition
    of clustering can be thought of as the process of organizing objects into groups
    whose members are similar in some way, as shown in.
  prefs: []
  type: TYPE_NORMAL
- en: This way, a cluster is a collection of objects that have some similarity between
    them and are dissimilar to the objects belonging to other clusters. If collections
    of genetic variants are given, clustering algorithms put these objects into a
    group based on similarity—that is, population groups or super-population groups.
  prefs: []
  type: TYPE_NORMAL
- en: How does K-means work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A clustering algorithm, such as K-means, locates the centroid of the group of
    data points. However, to make clustering accurate and effective, the algorithm
    evaluates the distance between each point from the centroid of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Eventually, the goal of clustering is to determine intrinsic grouping in a
    set of unlabeled data. For example, the K-means algorithm tries to cluster related
    data points within the predefined **three** (that is, *k = 3*) clusters as shown
    in *Figure 8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9b8ab25-2f24-4635-b875-9710eb439de4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The results of a typical clustering algorithm and a representation
    of the cluster centers'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, using a combined approach of Spark, ADAM and H2O are capable of
    processing large amounts of variant data points. Suppose, we have n data points
    (x[i], i=1, 2… n, example, genetic variants) that need to be partitioned into
    *k* clusters. Then K-means assigns a cluster to each data point and aiming to
    find the positions *μ[i]*, *i=1...k* of the clusters that minimize the distance
    from the data points to the cluster. Mathematically, K-means tries to achieve
    the goal by solving an equation—that is, an optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb521b9d-0f9d-49ed-b2f7-e642e98ee905.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, *c[i]* is the set of data points that assigned to
    cluster *i* and *d(x,μ[i])=∥x−μ[i]∥[2]²* is the Euclidean distance to be calculated.
    The algorithm computes this distance between data points and the center of the
    k clusters by minimizing the **Within-Cluster Sum of Squares** (that is, **WCSS**),
    where *c[i]* is the set of points belonging to cluster *i*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can understand that the overall clustering operation using K-means
    is not a trivial one but an NP-hard optimization problem. Which also means that
    K-means algorithm not only tries to find the global minima but also often is stuck
    in different solutions. The K-means algorithm proceeds by alternating between
    two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster assignment step**: Assign each observation to the cluster whose mean
    yields the least **WCSS**. The sum of squares is the squared Euclidean distance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centroid update step**: Calculate the new means to be the centroids of the
    observations in the new clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a nutshell, the overall approach of K-means training can be described in
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aab181a6-0476-41d9-822d-bb2c0feeab04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Overall approach of the K-means algorithm process'
  prefs: []
  type: TYPE_NORMAL
- en: DNNs for geographic ethnicity prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Multilayer Perceptron** (**MLP**) is an example of a DNN that is a feed-forward
    neural network; that is, there are only connections between the neurons from different
    layers. There is one (pass through) input layer, one or more layers of **linear
    threshold units** (**LTUs**) (called **hidden layers**), and one final layer of
    LTUs (called the **output layer**).'
  prefs: []
  type: TYPE_NORMAL
- en: Each layer, excluding the output layer, involves a bias neuron and is fully
    connected to the next layer, forming a fully connected bipartite graph. The signal
    flows exclusively from the input to the output, that is, one-directional (**feed-forward**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Until recently, an MLP was trained using the back-propagation training algorithm,
    but now the optimized version (that is, Gradient Descent) uses a reverse-mode
    auto diff; that is, the neural networks are trained with SGD using back-propagation
    as a gradient computing technique. Two layers of abstraction are used in DNN training
    for solving classification problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient computation**: Using back-propagation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization level**: Using SGD, ADAM, RMSPro, and Momentum optimizers to
    compute the gradient computed earlier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each training cycle, the algorithm feeds the data into the network and computes
    the state and output for every neuron in the consecutive layers. The approach
    then measures the output error over the network, that is, the gap between the
    expected output and the current output, and the contribution from each neuron
    in the last hidden layer towards the neuron's output error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Iteratively, the output error is propagated back to the input layer through
    all hidden layers and the error gradient is calculated across all connection weights
    during backward propagation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6998423-d881-4f26-a9e3-c868dc81c503.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: A modern MLP consisting of input layer, ReLU, and softmax'
  prefs: []
  type: TYPE_NORMAL
- en: For a multiclass classification task, the output layer is typically determined
    by a shared softmax function (see *Figure 2* for more) in contrast to individual
    activation functions, and each output neuron provides the estimated probability
    for the corresponding class.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we will be using tree ensembles such as random forest for the
    classification. At this moment, I believe we can skip the basic introduction of
    RF since we have covered it in detail in [Chapter 1](4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml),
    *Analyzing Insurance Severity Claims*, [Chapter 2](4e196881-40c8-4eb9-b2b3-e332a49adc1a.xhtml),
    *Analyzing and Predicting Telecommunication Churn*, and [Chapter 3](51e66c26-e12b-4764-bbb7-444986c05870.xhtml),
    *High-Frequency Bitcoin Price Prediction from Historical Data*. Well, it is time
    for the being stared. Nonetheless, it is always good to have your programming
    environment ready before getting your hands dirty.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring programming environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we describe how to configure our programming environment so
    that we can interoperate with Spark, H2O, and Adam. Note that using H2O on a laptop
    or desktop is quite resource intensive. Therefore, make sure that your laptop
    has at least 16 GB of RAM and enough storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anyway, I am going to make this project a Maven project on Eclipse. However,
    you can try to define the same dependencies in SBT too. Let us define the properties
    tag on a `pom.xml` file for a Maven-friendly project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can the latest version of the Spark 2.2.1 version (any 2.x version
    or even higher should work fine):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we need to declare the dependencies for H2O and Sparkling water that match
    the version specified in the properties tag. Later versions might also work, and
    you can try:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s define ADAM and its dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When I tried this on a Windows machine, additionally I had to install `joda-time`
    dependencies. Let us do it (but depending your platform, it might not be needed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Once you create a Maven project in Eclipse (manually from the IDE or using `$
    mvn install)`, all the required dependencies will be downloaded! We are ready
    to code now!
  prefs: []
  type: TYPE_NORMAL
- en: 'Wait! How about seeing the UI of H2O on the browser? For this, we have to manually
    download the H2O JAR somewhere in our computer and run it as a regular `.jar`
    file. In short, it''s a three-way process:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the **Latest Stable Release** H[2]O from [https://www.h2o.ai/download/](https://www.h2o.ai/download/).
    Then unzip it; it contains everything you need to get started.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From your terminal/command prompt, run the `.jar` using `java -jar h2o.jar`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Point your browser to `http://localhost:54321`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/bbe1f494-edb7-4737-980a-1d485ebda72a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The UI of H2O FLOW'
  prefs: []
  type: TYPE_NORMAL
- en: This shows the available features of the latest version (that is, h2o-3.16.0.4
    as of 19 January 2018) of H2O. However, I am not going to explain everything here,
    so let's stop exploring because I believe for the time being this much knowledge
    about H2O and Sparking water will be enough.
  prefs: []
  type: TYPE_NORMAL
- en: Data pre-processing and feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I already stated that all the 24 VCF files contribute 820 GB of data. Therefore,
    I decided to use the genetic variant of chromosome Y only one two make the demonstration
    clearer. The size is around 160 MB, which is not meant to pose huge computational
    challenges. You can download all the VCF files as well as the panel file from
    [ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us get started. We start by creating `SparkSession`, the gateway for the
    Spark application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then let''s show Spark the path of both VCF and the panel file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We process the panel file using Spark to access the target population data
    and identify the population groups. We first create a set of the populations that
    we want to predict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we need to create a map of sample ID → population so that we can filter
    out the samples we are not interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the panel file produces the sample ID of all individuals, population
    groups, ethnicities, super population groups, and the genders shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed1be322-c2e8-4982-a8f6-50c9546d37ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Contents of a sample panel file'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then load the ADAM genotypes and filter the genotypes so that we''re left with
    only those in the populations we''re interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The next job would be converting the `Genotype` objects into our own `SampleVariant`
    objects to try to conserve memory. Then, the `genotype` object is converted into
    a `SampleVariant` object that contains only the data we need for further processing:
    the sample ID, which uniquely identifies a particular sample; a variant ID, which
    uniquely identifies a particular genetic variant; and a count of alternate alleles
    (only when the sample differs from the reference genome).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The signature that prepares a sample variant is given here; it takes `sampleID`,
    `variationId`, and the `alternateCount`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Alright! Let us find `variantID` from the `genotype` file. A `varitantId` is
    a `String` type consisting of the name, start, and the end position in the chromosome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the `variantID`, we should hunt for the alternate count. In the
    `genotype` file, the objects that do not have an allele reference are roughly
    genetic alternates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we construct a simple variant object. For this, we need to intern sample
    IDs as they will be repeated a lot in a VCF file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Excellent! We have been able to construct simple variants. Now, the next challenging
    task is to prepare `variantsRDD` before we are able to create the `variantsBySampleId`
    RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we have to group the variants by sample ID so that we can process the
    variants sample by sample. After that, we can get the total number of samples
    to be used to find variants that are missing for some samples. Lastly, we have
    to group the variants by variant ID and filter out those variants that are missing
    from some samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s make a map of variant ID → count of samples with an alternate count
    of greater than zero. Then we filter out those variants that are not in our desired
    frequency range. The objective here is simply to reduce the number of dimensions
    in the dataset to make it easier to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The total number of samples (or individuals) has been determined before grouping
    them based on their variant IDs and filtering out variants without support by
    the samples to simplify the data pre-processing and to better cope with the very
    large number of variants (in total 84.4 million).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13* shows a conceptual view of a genotype variants collection in the
    1000 Genomes project and exposes the feature extraction process from the same
    data to train our K-means and MLP models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5006da23-8d31-45d6-a5a6-dfeb88d14337.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Conceptual view of the genotype variants collection in the 1000
    Genomes project'
  prefs: []
  type: TYPE_NORMAL
- en: 'The specified range is arbitrary and was chosen because it includes a reasonable
    number of variants, but not too many. To be more specific, for each variant, the
    frequency for alternate alleles have been calculated, and variants with less than
    12 alternate alleles have been excluded, leaving about 3 million variants in the
    analysis (for 23 chromosome files):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we has `filteredVariantsBySampleId`, the next task is to sort the variants
    for each sample ID. Each sample should now have the same number of sorted variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'All items in the RDD should now have the same variants in the same order. The
    final task is to use `sortedVariantsBySampleId` to construct an RDD of `Row` containing
    the region and the alternate count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, we can just use the first one to construct our header for the training
    data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Well done! Up to this point, we have our RDD and the header `StructType`. So
    now, we can play with both H2O and the Spark deep/machine learning algorithm with
    minimal adjustment/conversion. The overall flow of this end-to-end project can
    be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47f8c70a-7a52-495c-bbc2-afbab44c1ab1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The pipeline of the overall approach'
  prefs: []
  type: TYPE_NORMAL
- en: Model training and hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have `rowRDD` and the header, the next task is to construct the rows
    of our Schema DataFrame from the variants using the header and `rowRDD`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/9e459b56-504f-4465-a5c5-ebd78266d610.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: A snapshot of the training dataset containing features and the label
    (that is, Region) columns'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding DataFrame, only a few columns, including the label, are shown
    so that it fits on the page.
  prefs: []
  type: TYPE_NORMAL
- en: Spark-based K-means for population-scale clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a previous section, we have seen how the K-means work. So we can directly
    dive into the implementation. Since the training will be unsupervised, we need
    to drop the label column (that is, `Region`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/34b50280-9728-4f15-8048-2b5e56d2f801.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: A snapshot of the training dataset for K-means without the label
    (that is, Region)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have seen in [Chapters 1](4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml), *Analyzing
    Insurance Severity Claims* and [Chapter 2](4e196881-40c8-4eb9-b2b3-e332a49adc1a.xhtml), *Analyzing
    and Predicting Telecommunication Churn* that Spark expects two columns (that is,
    features and label) for supervised training, and for unsupervised training, it expects only
    a single column containing the features. Since we dropped the label column, we
    now need to amalgamate the entire variable column into a single `features` column.
    So for this, we will again use the `VectorAssembler()` transformer. At first,
    let''s select the columns to be embedded into a vector space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we instantiate the `VectorAssembler()` transformer, specifying the input
    columns and the output column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a26cbeac-f5c1-423c-984d-1b5417fda8b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: A snapshot of the feature vectors for the K-means'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our dataset is very highly dimensional, we can use some dimensionality
    algorithms such as PCA. So let''s do it by instantiating a `PCA()` transformer
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we transform the assembled DataFrame (that is, assembled) and the top
    50 principle components. You can adjust the number though. Finally, to avoid the
    ambiguity, we renamed the `pcaFeatures` column to `features`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/6b822396-5778-4322-9013-44a867bc6a2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: A snapshot of the top 50 principal components as the most important
    features'
  prefs: []
  type: TYPE_NORMAL
- en: 'Excellent! Everything went smoothly. Finally, we are ready to train the K-means
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'So let''s evaluate clustering by computing the **Within-Set Sum of Squared
    Errors** (**WSSSE**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Determining the number of optimal clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The beauty of clustering algorithms such as K-means is that they do the clustering
    on the data with an unlimited number of features. They are great tools to use
    when you have raw data and would like to know the patterns in that data. However,
    deciding on the number of clusters prior doing the experiment might not be successful
    and may sometimes lead to an overfitting or underfitting problem.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, one common thing to all three algorithms (that is, K-means,
    bisecting K-means, and Gaussian mixture) is that the number of clusters must be
    determined in advance and supplied to the algorithm as a parameter. Hence, informally,
    determining the number of clusters is a separate optimization problem to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will use a heuristic approach based on the Elbow method. We start from
    K = 2 clusters, then we run the K-means algorithm for the same dataset by increasing
    K and observing the value of the cost function WCSS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'At some point, a big drop in cost function can be observed, but then the improvement
    became marginal with the increasing value of `k`. As suggested in cluster analysis
    literature, we can pick the `k` after the last big drop of WCSS as an optimal
    one. Now, let''s see the WCSS values for a different number of clusters between
    2 and 20 for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Now let us discuss how to take advantage of the Elbow method for determining
    the number of clusters. As shown next, we calculated the cost function, WCSS,
    as a function of a number of clusters for the K-means algorithm applied to Y chromosome
    genetic variants from the selected population groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be observed that a somewhat **big drop** occurs when `k = 9` (which
    is not a drastic drop though). Therefore, we choose the number of clusters to
    be 10, as shown in *Figure 10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4260d5c6-46f4-4302-9948-ab8cbd4190b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19: Number of clusters as a function of WCSS
  prefs: []
  type: TYPE_NORMAL
- en: Using H2O for ethnicity prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to this point, we have seen how to cluster genetic variants. We have also
    used the Elbow method and found the number of optimal `k`, the tentative number
    cluster. Now we should explore another task that we planned at the beginning—that
    is, ethnicity prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous K-means section, we prepared a Spark DataFrame named `schemaDF`.
    That one cannot be used with H2O. However, an additional conversion is necessary.
    We use the `asH2OFrame()` method to convert the Spark DataFrame into an H2O frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, one important thing you should remember while using H2O is that if you
    do not convert the label column into categorical, it will treat the classification
    task as regression. To get rid of this, we can use the `toCategoricalVec()` method
    from H2O. Since H2O frames are resilient, we can further update the same frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now our H2O frame is ready to train an H2O-based DL model (which is DNN, or
    to be more specific, a deep MLP). However, before we start the training, let''s
    randomly split the DataFrame into 60% training, 20% test, and 20% validation data
    using the H2O built-in `FrameSplitter()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Fantastic! Our train, test, and validation sets are ready, so let us set the
    parameters for our DL model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding setting, we have specified an MLP having three hidden layers
    with 128, 256 and 512 neurons respectively. So altogether, there are five layers
    including the input and the output layer. The training will iterate up to 200
    epoch. Since we have used too many neurons in the hidden layer, we should use
    the dropout to avoid overfitting. To avoid achieve a better regularization, we
    used the l1 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding setting also states that we will train the model using the training
    set, and additionally the validation set will be used to validate the training.
    Finally, the response column is `Region`. On the other hand, the seed is used
    to ensure reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'So all set! Now let''s train the DL model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on your hardware configuration, it might take a while. Therefore,
    it is time to  rest and get some coffee maybe! Once we have the trained model,
    we can see the training error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, the training was not that great! Nevertheless, we should try
    with different combination of hyperparameters. The error turns out to be high
    though, but let us not worry too much and evaluate the model, compute some model
    metrics, and evaluate model quality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Not so high accuracy! However, you should try with other VCF files and by tuning
    the hyperparameters too. For example, after reducing the neurons in the hidden
    layers and with l2 regularization and 100 epochs, I had about 20% improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Another improvement clue is here. Apart from these hyperparameters, another
    advantage of using H2O-based DL algorithms is that we can take the relative variable/feature
    importance. In previous chapters, we have seen that when using a Random Forest
    algorithm in Spark, it is also possible to compute the variable importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the idea is that if your model does not perform well, it would be
    worth dropping less important features and doing the training again. Now, it is
    possible to find the feature importance during supervised training. I have observed
    this feature importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba65fb17-4d3b-43ad-ae89-0b2780157865.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20: Relative feature importance using H2O
  prefs: []
  type: TYPE_NORMAL
- en: Now the question would be why don't you drop them and try training again and
    observe if the accuracy has increased or not? Well, I leave it up to the readers.
  prefs: []
  type: TYPE_NORMAL
- en: Using random forest for ethnicity prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we have seen how to use H2O for ethnicity prediction.
    However, we could not achieve better prediction accuracy. Therefore, H2O is not
    mature enough to compute all the necessary performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: So why don't we try Spark-based tree ensemble techniques such as Random Forest
    or GBTs? Because we have seen that in most cases, RF shows better predictive accuracy,
    so let us try with that one.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the K-means section, we''ve already prepared the Spark DataFrame named `schemaDF`.
    Therefore, we can simply transform the variables into feature vectors that we
    described before. Nevertheless, for this, we need to exclude the label column.
    We can do it using the `drop()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you can further reduce the dimensionality and extract the most
    principal components using PCA or any other feature selector algorithm. However,
    I will leave it up to you. Since Spark expects the label column to be numeric,
    we have to convert the ethnic group name into numeric. We can use `StringIndexer()`
    for this. It is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we randomly split the dataset for training and testing. In our case, let''s
    use 75% for the training and the rest for the testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this this a small dataset, considering this fact, we can `cache` both
    the train and test set for faster access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s create a `paramGrid` for searching through decision tree''s `maxDepth`
    parameter for the best model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we set up the 10-fold cross validation for an optimized and stable model.
    This will reduce the chances of overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, now we are ready for the training. So let''s train the random forest
    model with the best hyperparameters setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the cross-validated and the best model, why don't we evaluate
    the model using the test set. Why not? First, we compute the prediction DataFrame
    for each instance. Then we use the `MulticlassClassificationEvaluator()` to evaluate
    the performance since this is a multiclass classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we compute performance metrics such as `accuracy`, `precision`,
    `recall`, and `f1` measure. Note that using RF classifier, we can get `weightedPrecision`
    and the `weightedRecall`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/40c0900b-2076-47ed-beb4-0bdc16c67382.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 21: Raw prediction probability, true label, and the predicted label using
    random forest
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s compute the classification `accuracy`, `precision`, `recall`, `f1`
    measure and error on test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print the performance metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Yes, it turns out to be a better performer. This is bit unexpected since we
    hoped to have better predictive accuracy from a DL model, but we did not. As I
    already stated, we can still try with other parameters of H2O. Anyway, we can
    now see around 25% improvement using random forest. However, probably, it can
    still be improved.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to interoperate with a few big data tools such as
    Spark, H2O, and ADAM for handling a large-scale genomics dataset. We applied the
    Spark-based K-means algorithm to genetic variants data from the 1000 Genomes project
    analysis, aiming to cluster genotypic variants at the population scale.
  prefs: []
  type: TYPE_NORMAL
- en: Then we applied an H2O-based DL algorithm and Spark-based Random Forest models
    to predict geographic ethnicity. Additionally, we learned how to install and configure
    H2O for DL. This knowledge will be used in later chapters. Finally and importantly,
    we learned how to use H2O to compute variable importance in order to select the
    most important features in a training set.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how effectively we can use the **Latent Dirichlet
    Allocation** (**LDA**) algorithm for finding useful patterns in data. We will
    compare other topic modeling algorithms and the scalability power of LDA. In addition,
    we will utilize **Natural Language Processing** (**NLP**) libraries such as Stanford
    NLP.
  prefs: []
  type: TYPE_NORMAL
