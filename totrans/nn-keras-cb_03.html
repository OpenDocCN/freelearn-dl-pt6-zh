<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Applications of Deep Feedforward Neural Networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will be covering the following recipes:</p>
<ul>
<li class="mce-root">Predicting credit default</li>
<li class="mce-root">Predicting house prices</li>
<li class="mce-root">Categorizing news articles into topics</li>
<li>Classifying common audio</li>
<li class="mce-root">Predicting stock prices</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapters, we learned about building a neural network and the various parameters that need to be tweaked to ensure that the model built generalizes well. Additionally, we learned about how neural networks can be leveraged to perform image analysis using MNIST data.</p>
<p>In this chapter, we will learn how neural networks can be used for prediction on top of the following:</p>
<ul>
<li>Structured dataset
<ul>
<li>Categorical output prediction</li>
<li>Continuous output prediction</li>
</ul>
</li>
</ul>
<ul>
<li>Text analysis</li>
<li>Audio analysis</li>
</ul>
<p>Additionally, we will also be learning about the following:</p>
<ul>
<li>Implementing a custom loss function</li>
<li>Assigning higher weights for certain classes of output over others</li>
<li>Assigning higher weights for certain rows of a dataset over others</li>
<li>Leveraging a functional API to integrate multiple sources of data</li>
</ul>
<p>We will learn about all the preceding by going through the following recipes:</p>
<ul>
<li>Predicting a credit default</li>
<li>Predicting house prices</li>
<li>Categorizing news articles</li>
<li>Predicting stock prices</li>
<li>Classifying common audio</li>
</ul>
<p>However, you should note that these applications are provided only for you to understand how neural networks can be leveraged to analyze a variety of input data. Advanced ways of analyzing text, audio, and time-series data will be provided in later chapters about the  Convolutional Neural Network and the Recurrent Neural Network.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Predicting credit default</h1>
                </header>
            
            <article>
                
<p>In the financial services industry, one of the major sources of losing out on revenues is the default of certain customers. However, a very small percentage of the total customers default. Hence, this becomes a problem of classification and, more importantly, identifying rare events.</p>
<p>In this case study, we will analyze a dataset that tracks certain key attributes of a customer at a given point in time and tries to predict whether the customer is likely to default.</p>
<p>Let's consider the way in which you might operationalize the predictions from the model we build. Businesses might want to have a special focus on the customers who are more likely to default—potentially giving them alternative payment options or  a way to reduce the credit limit, and so on.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The strategy that we'll adopt to predict default of a customer is as follows:</p>
<ul>
<li><strong>Objective</strong>: Assign a high probability to the customers who are more likely to default.</li>
<li><strong>Mea</strong><strong>surement</strong> <strong>criterion</strong>: Maximize the number of customers who have actually defaulted when we consider only the top 10% of members by decreasing the default probability.</li>
</ul>
<p>The strategy we will be adopting to assign a probability of default for each member will be as follows:</p>
<ul>
<li>Consider the historic data of all members.</li>
<li>Understand the variables that can help us to identify a customer who is likely to default:
<ul>
<li>Income-to-debt ratio is a very good indicator of whether a member is likely to default.</li>
<li>We will be extracting a few other variables similar to that.</li>
</ul>
</li>
<li>In the previous step, we created the input variables; now, let's go ahead and create the dependent variable:
<ul>
<li>We will extract the members who have actually defaulted in the next 2 years by first going back in history and then looking at whether members defaulted in the next 2 years</li>
<li>It is important to have a time lag, as it might not give us any levers to change the outcome if we do not have a time gap between when a member is likely to default and the date of prediction.</li>
</ul>
</li>
<li>Given that the  outcome is binary, we will minimize the binary cross-entropy loss.</li>
<li>The model shall have a hidden layer that connects the input layer and the output layer.</li>
<li>We shall calculate the number of the top 10% probability members who have actually defaulted, in the test dataset.</li>
</ul>
<p>Note that we assume that test data is representative here, as we are not in a position to assess the performance of a model on unseen dataset without productionalizing the model. We shall assume that the model's performance on an unseen dataset is a good indicator of how well the model will perform on future data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>We'll code up the strategy as follows <span>(Please refer to the <kbd>Credit default prediction.ipynb</kbd> file in GitHub while implementing the code)</span>:</p>
<ol>
<li><span>Import the relevant packages and the dataset:</span></li>
</ol>
<pre style="padding-left: 90px">import pandas as pd<br/>data = pd.read_csv('...') # Please add path to the file you downloaded</pre>
<p style="padding-left: 60px">The first three rows of the dataset we downloaded are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1234 image-border" src="Images/f2c32cdc-9ecb-4a70-89f9-1460501a5bc3.png" style="width:29.92em;height:7.25em;" width="408" height="99"/></p>
<p style="padding-left: 60px">The <span><span>preceding screenshot</span></span> is a subset of variables in the original dataset. The variable named <kbd>Defaultin2yrs</kbd> is the output variable that we need to predict, based on the rest of the variables present in the dataset.</p>
<ol start="2">
<li>Summarize the dataset to understand the variables better:</li>
</ol>
<pre style="padding-left: 90px">data.describe()</pre>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">Once you look at the output you will notice the following:</p>
<ul>
<li>Certain variables have a small range (<kbd>age</kbd>), while others have a much bigger range (<kbd>Income</kbd>).</li>
<li>Certain variables have missing values (<kbd>Income</kbd>).</li>
<li>Certain variables have outlier values <span>(</span><kbd>Debt_income_ratio</kbd><span>)</span>. In the next steps, we will go ahead and correct all the issues flagged previously.</li>
<li>Impute missing values in a variable with the variable's median value:</li>
</ul>
<pre style="padding-left: 90px">vars = data.columns[1:]<br/>import numpy as np<br/>for var in vars:<br/>     data[var]= np.where(data[var].isnull(),data[var].median(),data[var])</pre>
<p style="padding-left: 60px">In the preceding code, we excluded the first variable, as it is the variable that we are trying to predict, and then we imputed the missing values in the rest of the variables (provided the variable does have a missing value).</p>
<ol start="3">
<li>Cap each variable to its corresponding 95<sup>th</sup> percentile value so that we do not have outliers in our input variables:</li>
</ol>
<pre style="padding-left: 60px">for var in vars:<br/>     x=data[var].quantile(0.95)<br/>     data[var+"outlier_flag"]=np.where(data[var]&gt;x,1,0)<br/>     data[var]=np.where(data[var]&gt;x,x,data[var])</pre>
<p style="padding-left: 60px">In the preceding code, we have identified the 95<sup>th</sup> percentile value of each variable, created a new variable that has a value of one if the row contains an outlier in the given variable, and zero otherwise. Additionally, we have capped the variable values to the 95<sup>th</sup> percentile value of the original value.</p>
<ol start="4">
<li>Once we summarize the modified data, we notice that except for the <kbd>Debt_income_ratio</kbd> variable every other variable does not seem to have outliers anymore. Hence, let's constrain <kbd>Debt_income_ratio</kbd> <span>further </span>to have a limited range of output, by capping it at the 80<sup>th</sup> percentile value:</li>
</ol>
<pre style="padding-left: 90px">data['Debt_income_ratio_outlier']=np.where(data['Debt_incomeratio']&gt;1,1,0)<br/>data['Debt_income_ratio']=np.where(data['Debt_income_ratio']&gt;1,1,data['Debt_income_ratio'])</pre>
<ol start="5">
<li>Normalize all variables to the same scale for a value between zero and one:</li>
</ol>
<pre style="padding-left: 90px">for var in vars:<br/>     data[var]= data[var]/data[var].max()</pre>
<p style="padding-left: 60px">In the preceding code, we are limiting all the variables to a similar range of output, which is between zero and one, by dividing each input variable value with the input variable column's maximum value.</p>
<ol start="6">
<li>Create the input and the output dataset:</li>
</ol>
<pre style="padding-left: 90px">X = data.iloc[:,1:]<br/>Y = data['Defaultin2yrs']</pre>
<ol start="7">
<li>Split the datasets into train and test datasets:</li>
</ol>
<pre style="padding-left: 90px">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state= 42)</pre>
<p style="padding-left: 60px">In the preceding step, we use the <kbd>train_test_split</kbd> method to split the input and output arrays into train and test datasets where the test dataset has 30% of the total number of data points in the input and the corresponding output arrays.</p>
<ol start="8">
<li>Now that the datasets are created, let's define the neural network model, as follows:</li>
</ol>
<pre style="padding-left: 90px">from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import Dropout<br/>from keras.utils import np_utils<br/>model = Sequential()<br/>model.add(Dense(1000, input_dim=X_train.shape[1], activation='relu'))<br/>model.add(Dense(1, activation='sigmoid'))<br/>model.summary()</pre>
<p style="padding-left: 60px">A summary of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1235 image-border" src="Images/faf052a6-457c-4e04-9b97-131f47334ab3.png" style="width:40.00em;height:12.67em;" width="518" height="164"/></p>
<p style="padding-left: 60px">In the previous architecture, we connect the input variables to a hidden layer that has 1,000 hidden units.</p>
<ol start="9">
<li>Compile the model. We shall employ binary cross entropy, as the output variable has only two classes. Additionally, we will specify that <kbd>optimizer</kbd> is an <kbd>adam</kbd> optimization:</li>
</ol>
<pre style="padding-left: 90px">model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</pre>
<ol start="10">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 90px">history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=1024, verbose=1)</pre>
<p style="padding-left: 60px">The variation of training and test loss, accuracy over increasing epochs is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1236 image-border" src="Images/e6314265-8680-40d4-a66a-75d2da8f9876.png" style="width:32.25em;height:25.75em;" width="387" height="309"/></p>
<ol start="11">
<li>Make predictions on the test dataset:</li>
</ol>
<pre style="padding-left: 90px">pred = model.predict(X_test)</pre>
<ol start="12">
<li>Check for the number of actual defaulters that are captured in the top 10% of the test dataset when ranked in order of decreasing probability:</li>
</ol>
<pre style="padding-left: 90px">test_data = pd.DataFrame([y_test]).T<br/>test_data['pred']=pred<br/>test_data = test_data.reset_index(drop='index')<br/>test_data = test_data.sort_values(by='pred',ascending=False)<br/>print(test_data[:4500]['Defaultin2yrs'].sum())</pre>
<p style="padding-left: 30px">In the preceding code, we concatenated the predicted values with actual values and then sorted the dataset by probability. We checked the actual number of defaulters that are captured in the top 10% of the test dataset (which is the first 4,500 rows).</p>
<p>We should note that there are 1,580 actual defaulters that we have captured by going through the 4,500 high-probability customers. This is a good prediction, as on average only 6% of the total customers default. Hence, in this case, ~35% of customers who have a high probability of default actually defaulted.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we have learned about the following concepts:</p>
<ul>
<li><strong>Imputin</strong><span class="underline"><strong>g</strong></span> <strong>m</strong><strong>issing</strong> <strong>values</strong>: We have learned that one of the ways to impute the missing values of a variable is by replacing the missing values with the median of the corresponding variable. Other ways to deal with the missing values is by replacing them with the mean value, and also by replacing the missing value with the mean of the variable's value in the rows that are most similar to the row that contains a missing value (this technique is called i<strong>dentifying the K-Nearest Neighbours</strong>).</li>
<li><strong>Capping the outlier values</strong>: We have also learned that one way to cap the outliers is by replacing values that are above the 95<sup>th</sup> percentile value with the 95<sup>th</sup> percentile value. The reason we performed this exercise is to ensure that the input variable does not have all the values clustered around a small value (when the variable is scaled by the maximum value, which is an outlier).</li>
<li><strong>Scaling dataset</strong>: Finally, we scaled the dataset so that it can then be passed to a neural network.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Assigning weights for classes</h1>
                </header>
            
            <article>
                
<p>When we assign equal weightage to the rows that belong to a defaulter and the rows that belong to a non-defaulter, potentially the model can fine-tune for the non-defaulters. In this section, we will look into ways of assigning a higher weightage so that our model classifies defaulters better.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In the previous section, we assigned the same weightage for each class; that is, the categorical cross entropy loss is the same if the magnitude of difference between actual and predicted is the same, irrespective of whether it is for the prediction of a default or not a default.</p>
<p>To understand the scenario further, let's consider the following example:</p>
<table style="border-collapse: collapse;width: 100%;border-color: #000000" border="1">
<tbody>
<tr>
<td style="width: 12.1857%"><strong>Scenario</strong></td>
<td style="width: 29.6905%"><strong>Probability of default</strong></td>
<td style="width: 21.7827%"><strong>Actual value of default</strong></td>
<td style="width: 34.9871%"><strong>Cross entropy loss</strong></td>
</tr>
<tr>
<td style="width: 12.1857%">1</td>
<td style="width: 29.6905%"><em>0.2</em></td>
<td style="width: 21.7827%"><em>1</em></td>
<td style="width: 34.9871%"><em>1*log(0.2)</em></td>
</tr>
<tr>
<td style="width: 12.1857%">2</td>
<td style="width: 29.6905%"><em>0.8</em></td>
<td style="width: 21.7827%"><em>0</em></td>
<td style="width: 34.9871%"><em>(1-0)*log(1-0.8)</em></td>
</tr>
</tbody>
</table>
<p>In the preceding scenario, the cross-entropy loss value is just the same, irrespective of the actual value of default.</p>
<p>However, we know that our objective is to capture as many actual defaulters as possible in the top 10% of predictions when ranked by probability.</p>
<p>Hence, let's go ahead and assign a higher weight of loss (a weight of <em>100</em>) when the actual value of default is <em>1</em> and a lower weightage (a weight of <em>1</em>) when the actual value of default is <em>0</em>.</p>
<p>The previous scenario now changes as follows:</p>
<table style="border-collapse: collapse;width: 100%;border-color: #000000" border="1">
<tbody>
<tr>
<td style="width: 12%"><strong>Scenario</strong></td>
<td style="width: 28%"><strong>Probability of default</strong></td>
<td style="width: 22.1567%"><strong>Actual value of default</strong></td>
<td style="width: 33.8433%"><strong>Cross entropy loss</strong></td>
</tr>
<tr>
<td style="width: 12%">1</td>
<td style="width: 28%"><em>0.2</em></td>
<td style="width: 22.1567%"><em>1</em></td>
<td style="width: 33.8433%"><em>100*1*log(0.2)</em></td>
</tr>
<tr>
<td style="width: 12%">2</td>
<td style="width: 28%"><em>0.8</em></td>
<td style="width: 22.1567%"><em>0</em></td>
<td style="width: 33.8433%"><em>1*(1-0)*log(1-0.8)</em></td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now, if we notice the cross entropy loss, it is much higher when the predictions are wrong when the actual value of default is <em>1</em> compared to the predictions when the actual value of default is <em>0</em>.</p>
<p>Now that we have understood the intuition of assigning weightages to classes, let's go ahead and assign weights to output classes in the credit default dataset.</p>
<p><span>All the steps performed to build the dataset and model remain the same as in the previous section, except for the model-fitting process.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The model fitting process is done by following these steps (Please refer to <kbd>Credit default prediction.ipynb</kbd> file in GitHub while implementing the code):</p>
<pre>history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=1024, verbose=1,class_weight = {0:1,1:100})</pre>
<p>Note that, in the preceding code snippet, we created a dictionary with the weights that correspond to the distinct classes in output that is then passed as an input to the <kbd>class_weight</kbd> parameter.</p>
<p><span>The preceding step ensures that we assign a weightage of </span><kbd>100</kbd><span> to calculating the loss value when the actual outcome is </span><kbd>1</kbd><span> and a weightage of </span><kbd>1</kbd><span> when calculating the loss value when the actual outcome is </span><kbd>0</kbd><span>.</span></p>
<p>The variation of accuracy and loss values over increasing epochs is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1237 image-border" src="Images/319b3dc9-49bd-46b1-b089-16157ec61b44.png" style="width:34.00em;height:26.42em;" width="398" height="310"/></p>
<p>Note that the accuracy values are much lower in this iteration, as we are predicting more number of data points to have a value 1 than in the scenario of equal weightage to both classes.</p>
<p>Once the model is fitted, let's proceed and check for the number of actual defaulters that are captured in the top 10% of predictions, as follows:</p>
<pre>pred = model.predict(X_test)<br/>test_data = pd.DataFrame([y_test[:,1]]).T<br/>test_data['pred']=pred[:,1]<br/>test_data = test_data.reset_index(drop='index')<br/>test_data = test_data.sort_values(by='pred',ascending=False)<br/>test_data.columns = ['Defaultin2yrs','pred']<br/>print(test_data[:4500]['Defaultin2yrs'].sum())</pre>
<p>You notice that compared to the previous scenario of 1,580 customers being captured in the the top 10%, we have 1,640 customers captured in the top 10% in this scenario, and thus a better outcome for the objective we have set where we have captured 36% of all defaulters in top 10% of high probable customers in this scenario, when compared to 35% in the previous scenario.</p>
<div class="packt_infobox">It is not always necessary that accuracy improves as we increase class weights. Assigning class weights is a mechanism to give higher weightage to the prediction of our interest.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Predicting house prices</h1>
                </header>
            
            <article>
                
<p>In the previous case study, we had an output that was categorical. In this case study, we shall look into an output that is continuous in nature, by trying to predict the price of a house where 13 variables that are likely to impact the house price are provided as input.</p>
<p>The objective is to minimize the error by which we predict the price of a house.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Given that the objective is to minimize error, let's define the error that we shall be minimizing—we should ensure that a positive error and a negative error do not cancel out each other. Hence, we shall minimize the absolute error. An alternative of this is to minimize the squared error.</p>
<p>Now that we have fine-tuned our objective, let's define our strategy of solving this problem:</p>
<ul>
<li>Normalize the input dataset so that all variables range between zero to one.</li>
<li>Split the given data to train and test datasets.</li>
<li>Initialize the hidden layer that connects the input of 13 variables to the output of one variable.</li>
<li>Compile the model with the Adam optimizer, and define the loss function to minimize as the mean absolute error value.</li>
<li>Fit the model.</li>
<li>Make a prediction on the test dataset.</li>
<li>Calculate the error in the prediction on the test dataset.</li>
</ul>
<p>Now that we have defined our approach, let's go ahead and perform it in code in the next section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Import the relevant dataset (Please refer to the <kbd>Predicting house price.ipynb</kbd> file in GitHub while implementing the code and for the recommended dataset):</li>
</ol>
<pre style="padding-left: 90px">from keras.datasets import boston_housing<br/>(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()</pre>
<ol start="2">
<li>Normalize the input and output dataset so that all variables have a range from zero to one:</li>
</ol>
<pre style="padding-left: 90px">import numpy as np<br/>train_data2 = train_data/np.max(train_data,axis=0)<br/>test_data2 = test_data/np.max(train_data,axis=0)<br/>train_targets = train_targets/np.max(train_targets)<br/>test_targets = test_targets<span>/np.max(train_targets)</span></pre>
<p style="padding-left: 60px">Note that we have normalized the test dataset with the maximum value in the train dataset itself, as we should not be using any of the values from the test dataset in the model-building process. Additionally, note that we have normalized both the input and the output values.</p>
<ol start="3">
<li>Now that the input and output datasets are prepared, let's proceed and define the model:</li>
</ol>
<pre style="padding-left: 90px">from keras.models import Sequential<br/>from keras.layers import Dense, Dropout<br/>from keras.utils import np_utils<br/>from keras.regularizers import l1<br/>model = Sequential()<br/>model.add(Dense(64, input_dim=13, activation='relu', kernel_regularizer = l1(0.1)))<br/>model.add(Dense(1, activation='relu', kernel_regularizer = l1(0.1)))<br/>model.summary()</pre>
<p style="padding-left: 60px">A summary of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1238 image-border" src="Images/096001a6-4571-48cb-9e30-93b7c2c7ef5f.png" style="width:33.67em;height:10.75em;" width="515" height="165"/></p>
<p style="padding-left: 60px">Note that we performed <span> </span><kbd>L1</kbd><span> </span> regularization in the model-building process so that the model does not overfit on the training data (as the number of data points in the training data is small).</p>
<ol start="4">
<li>Compile the model to minimize the mean absolute error value:</li>
</ol>
<pre style="padding-left: 90px">model.compile(loss='mean_absolute_error', optimizer='adam')</pre>
<ol start="5">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 90px">history = model.fit(train_data2, train_targets, validation_data=(test_data2, test_targets), epochs=100, batch_size=32, verbose=1)</pre>
<ol start="6">
<li>Calculate the mean absolute error on the test dataset:</li>
</ol>
<pre style="padding-left: 90px">np.mean(np.abs(model.predict(test_data2) - test_targets))*50</pre>
<p>We should note that the mean absolute error is <em>~6.7</em> units.</p>
<p>In the next section, we will vary the loss function and add custom weights to see whether we can improve upon the mean absolute error values.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Defining the custom loss function</h1>
                </header>
            
            <article>
                
<p>In the previous section, we used the predefined mean absolute error <kbd>loss</kbd> function to perform the optimization. In this section, we will learn about defining a custom loss function to perform optimization.</p>
<p>The custom loss function that we shall build is a modified mean squared error value, where the error is the difference between the square root of the actual value and the square root of the predicted value.</p>
<p>The custom loss function is defined as follows:</p>
<pre>import keras.backend as K<br/>def loss_function(y_true, y_pred):<br/>    return K.square(K.sqrt(y_pred)-K.sqrt(y_true))</pre>
<p>Now that we have defined the <kbd>loss</kbd> function, we will be reusing the same input and output datasets that we prepared in previous section, and we will also be using the same model that we defined earlier.</p>
<p>Now, let's compile the model:</p>
<pre>model.compile(loss=loss_function, optimizer='adam')</pre>
<p>In the preceding code, note that we defined the <kbd>loss</kbd> value as the custom loss function that we defined earlier—<kbd>loss_function</kbd>.</p>
<pre>history = model.fit(train_data2, train_targets, validation_data=(test_data2, test_targets), epochs=100, batch_size=32, verbose=1)</pre>
<p>Once we fit the model, we will note that the mean absolute error is <em>~6.5</em> units, which is slightly less than the previous iteration where we used the <kbd>mean_absolute_error</kbd> loss function.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Categorizing news articles into topics</h1>
                </header>
            
            <article>
                
<p>In the previous case studies, we analyzed datasets that were structured, that is, contained variables and their corresponding values. In this case study, we will be working on a dataset that has text as input, and the expected output is one of the 46 possible topics that the text is related to.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To understand the intuition of performing text analysis, let's consider the Reuters dataset, where each news article is classified into one of the 46 possible topics.</p>
<p>We will adopt the following strategy to perform our analysis:</p>
<ul>
<li>Given that a dataset could contain thousands of unique words, we will shortlist the words that we shall consider.</li>
<li> For this specific exercise, we shall consider the top 10,000 most frequent words.</li>
<li>An alternative approach would be to consider the words that cumulatively constitute 80% of all words within a dataset. This ensures that all the rare words are excluded.</li>
<li>Once the words are shortlisted, we shall one-hot-encode the article based on the constituent frequent words.</li>
<li>Similarly, we shall one-hot-encode the output label.</li>
<li>Each input now is a 10,000-dimensional vector, and the output is a 46-dimensional vector:</li>
<li>We will divide the dataset into train and test datasets. However, in code, you will notice that we will be using the in-built dataset of <kbd>reuters</kbd> in Keras that has built-in function to identify the top <kbd>n</kbd> frequent words and split the dataset into train and test datasets.</li>
<li>Map the input and output with a hidden layer in between.</li>
<li>We will perform softmax at the output layer to obtain the probability of the input belonging to one of the 46 classes.</li>
<li>Given that we have multiple possible outputs, we shall employ a categorical cross entropy loss function.</li>
<li>We shall compile and fit the model to measure its accuracy on a test dataset.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>We'll code up the strategy defined previously as follows (please refer to the <kbd>Categorizing news articles into topics.ipynb</kbd> file in GitHub while implementing the code):</p>
<ol>
<li>Import the dataset :</li>
</ol>
<pre style="padding-left: 90px">from keras.datasets import reuters<br/>(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)</pre>
<p style="padding-left: 60px">In the preceding code snippet, we loaded data from the <kbd>reuters</kbd> dataset that is available  in Keras. Additionally, we consider only the <kbd>10000</kbd> most frequent words in the dataset.</p>
<ol start="2">
<li>Inspect the dataset:</li>
</ol>
<pre style="padding-left: 90px">train_data[0]</pre>
<p style="padding-left: 60px">A sample of the loaded training dataset is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1239 image-border" src="Images/40f57912-d7fe-41a3-8840-03871083ff15.png" style="width:4.83em;height:10.33em;" width="58" height="124"/></p>
<p style="padding-left: 60px">Note that the numbers in the preceding output represent the index of words that are present in the output.</p>
<ol start="3">
<li>We can extract the index of values as follows:</li>
</ol>
<pre style="padding-left: 90px">word_index = reuters.get_word_index()</pre>
<ol start="4">
<li>Vectorize the input. We will convert the text into a vector in the following way:
<ul>
<li>One-hot-encode the input words—resulting in a total of <kbd>10000</kbd> columns in the input dataset.</li>
<li>If a word is present in the given text, the column corresponding to the word index shall have a value of one and every other column shall have a value of zero.</li>
<li>Repeat the preceding step for all the unique words in a text. If a text has two unique words, there will be a total of two columns that have a value of one, and every other column will have a value of zero:</li>
</ul>
</li>
</ol>
<pre style="padding-left: 120px">import numpy as np<br/>def vectorize_sequences(sequences, dimension=10000):<br/>     results = np.zeros((len(sequences), dimension))<br/>     for i, sequence in enumerate(sequences):<br/>         results[i, sequence] = 1.<br/>     return results</pre>
<p style="padding-left: 60px">In the preceding function, we initialized a variable that is a zero matrix and imputed it with a value of one, based on the index values present in the input sequence.</p>
<p style="padding-left: 60px">In the following code, we are converting the words into IDs.</p>
<pre style="padding-left: 60px">x_train = vectorize_sequences(train_data)<br/>x_test = vectorize_sequences(test_data)</pre>
<ol start="5">
<li>One-hot-encode the output:</li>
</ol>
<pre style="padding-left: 60px">from keras.utils.np_utils import to_categorical<br/>one_hot_train_labels = to_categorical(train_labels)<br/>one_hot_test_labels = to_categorical(test_labels)</pre>
<p style="padding-left: 60px">The preceding code converts each output label into a vector that is <kbd>46</kbd> in length, where one of the <kbd>46</kbd> values is one and the rest are zero, depending on the label's index value.</p>
<ol start="6">
<li>Define the model and compile it:</li>
</ol>
<pre style="padding-left: 60px">from keras.models import Sequential<br/>from keras.layers import Dense<br/>model = Sequential()<br/>model.add(Dense(64, activation='relu', input_shape=(10000,)))<br/>model.add(Dense(64, activation='relu'))<br/>model.add(Dense(46, activation='softmax'))<br/>model.summary()<br/>model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1240 image-border" src="Images/849f6764-c6d9-4993-8ffd-7319ebba6bf7.png" style="width:37.50em;height:14.50em;" width="508" height="197"/></p>
<p style="padding-left: 60px">Note that while compiling, we defined <kbd>loss</kbd> as <kbd>categorical_crossentropy</kbd> as the output in this case is categorical (multiple classes in output).</p>
<ol start="7">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 60px">history = model.fit(X_train, y_train,epochs=20,batch_size=512,validation_data=(X_test, y_test))</pre>
<p style="padding-left: 60px">The preceding code results in a model that has 80% accuracy in classifying the input text into the right topic, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1241 image-border" src="Images/2ef23cd1-8063-4ac8-8614-e647abd39c7f.png" style="width:33.33em;height:27.83em;" width="400" height="334"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classifying common audio</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we have understood the strategy to perform modeling on a structured dataset and also on unstructured text data.</p>
<p>In this section, we will be learning about performing a classification exercise where the input is raw audio.</p>
<p>The strategy we will be adopting is that we will be extracting features from the input audio, where each audio signal is represented as a vector of a fixed number of features.</p>
<p>There are multiple ways of extracting features from an audio—however, for this exercise, we will be extracting the <span><strong>Mel Frequency Cepstral Coefficients</strong> (<strong>MFCC</strong>) corresponding to the audio file.</span></p>
<p>Once we extract the features, we shall perform the classification exercise in a way that is very similar to how we built a model for MNIST dataset classification—where we had hidden layers connecting the input and output layers.</p>
<p>In the following section, we will be performing classification on top of an audio dataset where there are ten possible classes of output.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The strategy that we defined previously is coded as follows (Please refer to the <kbd>Audio classification.ipynb</kbd> file in GitHub while implementing the code):</p>
<ol>
<li>Import the dataset:</li>
</ol>
<pre style="padding-left: 90px">import pandas as pd<br/>data = pd.read_csv('/content/train.csv')</pre>
<ol start="2">
<li>Extract features for each audio input:</li>
</ol>
<pre style="padding-left: 90px">ids = data['ID'].values<br/><span>def extract_feature(file_name):</span><br/><span>    X, sample_rate = librosa.load(file_name)</span><br/><span>    stft = np.abs(librosa.stft(X))</span><br/><span>    mfccs = np.mean(librosa.feature.mfcc(y=X,sr=sample_rate, n_mfcc=40).T,axis=0)</span><br/><span>    return mfccs</span></pre>
<p style="padding-left: 60px">In the preceding code, we defined a function that takes <kbd>file_name</kbd> as input, extracts the <kbd>40</kbd> MFCC corresponding to the audio file, and returns the same.</p>
<ol start="3">
<li>Create the input and the output dataset:</li>
</ol>
<pre style="padding-left: 90px">x = []<br/>y = []<br/>for i in range(len(ids)):     <br/>     try:<br/>         filename = '/content/Train/'+str(ids[i])+'.wav'<br/>         y.append(data[data['ID']==ids[i]]['Class'].values)<br/>         x.append(extract_feature(filename))<br/>     except:<br/>         continue<br/>x = np.array(x)</pre>
<p style="padding-left: 60px">In the preceding code, we loop through one audio file at a time, extracting its features and storing it in the input list. Similarly, we will be storing the output class in the output list. Additionally, we will convert the output list into a categorical value that is one-hot-encoded:</p>
<pre style="padding-left: 90px">y2 = []<br/>for i in range(len(y)):<br/>     y2.append(y[i][0])<br/>y3 = np.array(pd.get_dummies(y2))</pre>
<p style="padding-left: 60px">The <kbd>pd.get_dummies</kbd> method works very similar to the <kbd>to_categorical</kbd> method we used earlier; however, <kbd>to_categorical</kbd> does not work on text classes (it works on numeric values only, which get converted to one-hot-encoded values).</p>
<ol start="4">
<li>Build the model and compile it:</li>
</ol>
<pre style="padding-left: 90px">model = Sequential()<br/>model.add(Dense(1000, input_shape = (40,), activation = 'relu'))<br/>model.add(Dense(10,activation='sigmoid'))<br/>from keras.optimizers import Adam<br/>adam = Adam(lr=0.0001)<br/>model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc'])</pre>
<p style="padding-left: 90px">The summary of the preceding model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1242 image-border" src="Images/23de187e-b380-4973-9db6-ef8af7130d91.png" style="width:37.00em;height:11.75em;" width="507" height="162"/></p>
<ol start="5">
<li>Create the train and test datasets and then fit the model:</li>
</ol>
<pre>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(x, y3, test_size=0.30,random_state=10)<br/>model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose = 1)</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">Once the model is fitted, you will notice that the model has 91% accuracy in classifying audio in the right class.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Stock price prediction</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we learned about performing audio, text, and structured data analysis using neural networks. In this section, we will learn about performing a time-series analysis using a case study of predicting a stock price.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To predict a stock price, we will perform the following steps:</p>
<ol>
<li>Order the dataset from the oldest to the newest date.</li>
<li>Take the first five stock prices as input and the sixth stock price as output.</li>
<li>Slide it across so that in the next data point the second to the sixth data points are input and the seventh data point is the output, and so on, till we reach the final data point.</li>
<li>Given that it is a continuous number that we are predicting, the <kbd>loss</kbd> function this time shall be the mean squared error value.</li>
</ol>
<p>Additionally, we will also try out the scenario where we integrate the text data into the historic numeric data to predict the next day's stock price.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The above strategy is coded as follows (please refer to <kbd>Chapter 3  - stock price prediction.ipynb</kbd> file in GitHub while implementing the code and for the recommended dataset):</p>
<ol>
<li><span> </span>Import the relevant packages and the dataset:</li>
</ol>
<pre style="padding-left: 90px">import pandas as pd<br/>data2 = pd.read_csv('/content/stock_data.csv')</pre>
<ol start="2">
<li>Prepare the dataset where the input is the previous five days' stock price value and the output is the stock price value on the sixth day:</li>
</ol>
<pre style="padding-left: 90px">x= []<br/>y = []<br/>for i in range(data2.shape[0]-5):<br/> x.append(data2.loc[i:(i+4)]['Close'].values)<br/> y.append(data2.loc[i+5]['Close'])<br/>import numpy as np<br/>x = np.array(x)<br/>y = np.array(y)</pre>
<ol start="3">
<li>Prepare the train and test datasets, build the model, compile it, and fit it:</li>
</ol>
<pre style="padding-left: 90px">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30,random_state=10)</pre>
<p style="padding-left: 90px">Build the model and compile it:</p>
<pre style="padding-left: 90px"><br/>from keras.layers import Dense<br/>from keras.models import Sequential, Model<br/>model = Sequential()<br/>model.add(Dense(100, input_dim = 5, activation = 'relu'))<br/>model.add(Dense(1,activation='linear'))<br/>model.compile(optimizer='adam', loss='mean_squared_error')</pre>
<p style="padding-left: 90px">The previous code results in a summary of model as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1246 image-border" src="Images/19ad486e-b866-4549-8fc7-a842143634a8.png" style="width:37.58em;height:11.92em;" width="505" height="160"/></p>
<pre style="padding-left: 90px">model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), verbose = 1)</pre>
<p>Once we fit the model, we should note that the mean squared error value <em>~$360</em> in predicting the stock price or ~$18 in predicting the stock price.</p>
<p>Note that there is a pitfall in predicting a stock price this way. However, that will be dealt with in the chapter on RNN applications.</p>
<p>For now, we will focus on learning how neural networks can be useful in a variety of different scenarios.</p>
<p class="mce-root"/>
<p>In the next section, we will understand the ways in which we can integrate the numeric data with the text data of news headlines in a single model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Leveraging a functional API</h1>
                </header>
            
            <article>
                
<p>In this section, we will continue to improve the accuracy of the stock price prediction by integrating historical price points data with the most-recent headlines of the company for which we are predicting the stock price. </p>
<p>The strategy that we will adopt to integrate data from multiple sources—structured (historical price) data and unstructured (headline) data is as follows:</p>
<ul>
<li>We will convert the unstructured text into a structured format in a manner that is similar to the way we categorized news articles into topics.</li>
<li>We will pass the structured format of text through a neural network and extract the hidden layer output.</li>
<li>Finally, we pass the hidden layer output to the output layer, where the output layer has one node.</li>
<li>In a similar manner, we pass the input historical price data through the neural network to extract the hidden layer values, which then get passed to the output layer that has one unit in output.</li>
<li>We multiply the output of each of the individual neural network operations to extract the final output.</li>
<li>The squared error value of the final output shall now be minimized.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The previous strategy is coded as follows:</p>
<ol>
<li>Let's fetch the headline data from the API provided by the Guardian, as follows:</li>
</ol>
<pre style="padding-left: 90px">from bs4 import BeautifulSoup<br/>import urllib, json<br/><br/>dates = []<br/>titles = []<br/>for i in range(100):<br/>     try:<br/>         url = 'https://content.guardianapis.com/search?from-date=2010-01-01&amp;section=business&amp;page-size=200&amp;order-by=newest&amp;page='+str(i+1)+'&amp;q=amazon&amp;api-key=207b6047-a2a6-4dd2-813b-5cd006b780d7'<br/>         response = urllib.request.urlopen(url)<br/>         encoding = response.info().get_content_charset('utf8')<br/>         data = json.loads(response.read().decode(encoding))<br/>     for j in range(len(data['response']['results'])):<br/>         dates.append(data['response']['results'][j]['webPublicationDate'])<br/>         titles.append(data['response']['results'][j]['webTitle']) <br/>     except:<br/>         break</pre>
<ol start="2">
<li>Once <kbd>titles</kbd> and <kbd>dates</kbd> are extracted, we shall preprocess the data to convert the <kbd>date</kbd> values to a <kbd>date</kbd> format, as follows:</li>
</ol>
<pre style="padding-left: 90px">import pandas as pd<br/>data = pd.DataFrame(dates, titles)<br/>data['date']=data['date'].str[:10]<br/>data['date']=pd.to_datetime(data['date'], format = '%Y-%m-%d')<br/>data = data.sort_values(by='date')<br/>data_final = data.groupby('date').first().reset_index()</pre>
<ol start="3">
<li>Now that we have the most recent headline for every date on which we are trying to predict the stock price, we will integrate the two data sources, as follows:</li>
</ol>
<pre style="padding-left: 90px">data2['Date'] = pd.to_datetime(data2['Date'],format='%Y-%m-%d')<br/>data3 = pd.merge(data2,data_final, left_on = 'Date', right_on = 'date', how='left')</pre>
<ol start="4">
<li>Once the datasets are merged, we will go ahead and normalize the text data so that we remove the following:
<ul>
<li>Convert all words in a text into lowercase so that the words like <kbd>Text</kbd> and <kbd>text</kbd> are treated the same.</li>
<li>Remove punctuation so that words such as <kbd>text.</kbd> and <kbd>text</kbd> are treated the same.</li>
<li>Remove stop words such as <kbd>a</kbd>, <kbd>and</kbd>, <kbd>the</kbd>, which do not add much context to the text:</li>
</ul>
</li>
</ol>
<pre style="padding-left: 90px">import nltk<br/>import re<br/>nltk.download('stopwords')<br/>stop = nltk.corpus.stopwords.words('english')<br/>def preprocess(text):<br/>     text = str(text)<br/>     text=text.lower()<br/>     text=re.sub('[^0-9a-zA-Z]+',' ',text)<br/>     words = text.split()<br/>     words2=[w for w in words if (w not in stop)]<br/>     words4=' '.join(words2)<br/>     return(words4)<br/>data3['title'] = data3['title'].apply(preprocess)</pre>
<ol start="5">
<li>Replace all the null values in the <kbd>title</kbd> column with a hyphen <kbd>-</kbd>:</li>
</ol>
<pre style="padding-left: 90px">data3['title']=np.where(data3['title'].isnull(),'-','-'+data3['title'])</pre>
<p style="padding-left: 90px">Now that we have preprocessed the text data, let's assign an ID to each word. Once we have finished this assignment, we can perform text analysis in a way that is very similar to what we did in the <em>Categorizing news articles into topics</em> section, as follows:</p>
<pre style="padding-left: 90px">docs = data3['title'].values<br/><br/>from collections import Counter<br/>counts = Counter()<br/>for i,review in enumerate(docs):<br/>     counts.update(review.split())<br/>words = sorted(counts, key=counts.get, reverse=True)<br/>vocab_size=len(words)<br/>word_to_int = {word: i for i, word in enumerate(words, 1)}</pre>
<ol start="6">
<li>Given that we have encoded all the words, let's replace them with their corresponding text in the original text:</li>
</ol>
<pre style="padding-left: 90px">encoded_docs = []<br/>for doc in docs:<br/>     encoded_docs.append([word_to_int[word] for word in doc.split()])<br/><br/>def vectorize_sequences(sequences, dimension=vocab_size):<br/>     results = np.zeros((len(sequences), dimension+1))<br/>     for i, sequence in enumerate(sequences):<br/>         results[i, sequence] = 1.<br/>     return results<br/>vectorized_docs = vectorize_sequences(encoded_docs)</pre>
<p style="padding-left: 90px">Now that we have encoded the texts, we understand the way in which we will integrate the two data sources.</p>
<ol start="7">
<li>First, we shall prepare the training and test datasets, as follows:</li>
</ol>
<pre style="padding-left: 90px">x1 = np.array(x)<br/>x2 = np.array(vectorized_docs[5:])<br/>y = np.array(y)<br/><br/>X1_train = x1[:2100,:]<br/>X2_train = x2[:2100, :]<br/>y_train = y[:2100]<br/>X1_test = x1[2100:,:]<br/>X2_test = x2[2100:,:]<br/>y_test = y[2100:]</pre>
<p style="padding-left: 90px">Typically, we would use a functional API when there are multiple inputs or multiple outputs expected. In this case, given that there are multiple inputs, we will be leveraging a functional API.</p>
<ol start="8">
<li>Essentially, a functional API takes out the sequential process of building the model and is performed as follows. Take the input of the vectorized documents and extract the output from it:</li>
</ol>
<pre style="padding-left: 90px">input1 = Input(shape=(2406,))<br/>input1_hidden = (Dense(100, activation='relu'))(input1)<br/>input1_output = (Dense(1, activation='tanh'))(input1_hidden)</pre>
<p style="padding-left: 90px">In the preceding code, note that we have not used the sequential modeling process but defined the various connections using the <kbd>Dense</kbd> layer.</p>
<div class="packt_infobox">Note that the input has a shape of <kbd>2406</kbd>, as there are <kbd>2406</kbd> unique words that remain after the filtering process.</div>
<ol start="9">
<li>Take the input of the previous <kbd>5</kbd> stock prices and build the model:</li>
</ol>
<pre style="padding-left: 90px">input2 = Input(shape=(5,))<br/>input2_hidden = (Dense(100, activation='relu'))(input2)<br/>input2_output = (Dense(1, activation='linear'))(input2_hidden)</pre>
<ol start="10">
<li>We will multiply the output of the two inputs:</li>
</ol>
<pre style="padding-left: 90px">from keras.layers import multiply<br/>out = multiply([model, model2])</pre>
<ol start="11">
<li>Now that we have defined the output, we will build the model as follows:</li>
</ol>
<pre style="padding-left: 90px">model = Model([input1, input2], out)<br/>model.summary()</pre>
<p style="padding-left: 90px">Note that, in the preceding step, we used the <kbd>Model</kbd> layer to define the input (passed as a list) and the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1247 image-border" src="Images/67dff3a9-1e83-4591-9096-e379635afd33.png" style="width:38.25em;height:21.83em;" width="615" height="351"/></p>
<p style="padding-left: 60px">A visualization of the preceding output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1248 image-border" src="Images/e66fdee9-4b95-4101-b450-29712e649413.png" style="width:34.83em;height:18.50em;" width="483" height="256"/></p>
<ol start="12">
<li>Compile and fit the model:</li>
</ol>
<pre style="padding-left: 90px">model.compile(optimizer='adam', loss='mean_squared_error')<br/>model.fit(x=[X2_train, X1_train], y=y_train, epochs=100,batch_size = 32, validation_data = ([X2_test, X1_test], y_test))</pre>
<p>The preceding code results in a mean squared error of <em>~5000</em> and clearly shows that the model overfits, as the training dataset loss is much lower than the test dataset loss.</p>
<p>Potentially, the overfitting is a result of a very high number of dimensions in the vectorized text data. We will look at how we can improve upon this in <a href="7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml">Chapter 11</a>, <em>Building a Recurrent Neural Network</em>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Defining weights for rows</h1>
                </header>
            
            <article>
                
<p>In the <em>Predicting house prices</em> recipe, we learned about defining a custom loss function. However, we are not in a position yet to assign a higher weightage for certain rows over others. (We did a similar exercise for  a credit default prediction case study where we assigned higher weightage to one class over the other; however, that was a classification problem, and the current problem that we are solving is a continuous variable-prediction problem.)</p>
<p class="mce-root">In this section, we will define weights for each row and then pass them to the <kbd>custom_loss</kbd> function that we will define.</p>
<p class="mce-root">We will continue working on the same dataset that we analyzed in the <em>Stock price prediction</em> recipe.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li class="mce-root">To perform specifying weightages at a row level, we will modify our train and test datasets in such a way that the first <kbd>2100</kbd> data points after ordering the dataset are in the train dataset and the rest are in the test dataset:</li>
</ol>
<pre style="padding-left: 90px">X_train = x[:2100,:,:]<br/>y_train = y[:2100]<br/>X_test = x[2100:,:,:]<br/>y_test = y[2100:]</pre>
<ol start="2">
<li>A row in input shall have a higher weight if it occurred more recently and less weightage otherwise:</li>
</ol>
<pre style="padding-left: 90px">weights = np.arange(X_train.shape[0]).reshape((X_train.shape[0]),1)/2100</pre>
<p style="padding-left: 60px">The preceding code block assigns lower weightage to initial data points and a higher weightage to data points that occurred more recently.</p>
<p style="padding-left: 60px">Now that we have defined the weights for each row, we will include them in the custom loss function. Note that in this case our custom loss function shall include both the predicted and actual values of output as well as the weight that needs to be assigned to each row.</p>
<ol start="3">
<li class="mce-root">The partial method enables us to pass more variables than just the actual and predicted values to the custom loss function:</li>
</ol>
<pre style="padding-left: 90px">import keras.backend as K<br/>from functools import partial</pre>
<ol start="4">
<li>To pass <kbd>weights</kbd> to the <kbd>custom_loss</kbd> function, we shall be using the partial function to pass both <kbd>custom_loss</kbd> and <kbd>weights</kbd> as a parameter in step 7. In the code that follows, we are defining the <span> </span><kbd>custom_loss</kbd><span>  function:</span></li>
</ol>
<pre style="padding-left: 90px">def custom_loss_4(y_true, y_pred, weights):<br/>     return K.square(K.abs(y_true - y_pred) * weights)</pre>
<ol start="5">
<li>Given that the model we are building has two inputs, input variables and weights corresponding to each row, we will first define the <kbd>shape</kbd> input of the two as follows:</li>
</ol>
<pre style="padding-left: 90px">input_layer = Input(shape=(5,1))<br/>weights_tensor = Input(shape=(1,))</pre>
<ol start="6">
<li>Now that we have defined the inputs, let's initialize <kbd>model</kbd> that accepts the two inputs as follows:</li>
</ol>
<pre style="padding-left: 90px">inp1 = Dense(1000, activation='relu')(input_layer)<br/>out = Dense(1, activation='linear')(i3)<br/>model = Model([input_layer, weights_tensor], out)</pre>
<ol start="7">
<li>Now that we have initialized <kbd>model</kbd>, we will define the optimization function as follows:</li>
</ol>
<pre style="padding-left: 90px">cl4 = partial(custom_loss_4, weights=weights_tensor)</pre>
<p style="padding-left: 60px">In the preceding scenario, we specify that we need to minimize the <kbd>custom_loss_4</kbd> function and also that we provide an additional variable (<kbd>weights_tensor</kbd>) to the custom loss function.</p>
<ol start="8">
<li>Finally, before fitting the model, we will also provide <kbd>weights</kbd> for each row corresponding to the test dataset. Given that we are predicting these values, it is of no use to provide a low weightage to certain rows over others, as the test dataset is not provided to model. However, we will only specify this to make a prediction using the model we defined (which accepts two inputs):</li>
</ol>
<pre style="padding-left: 90px">test_weights = np.ones((156,1))</pre>
<ol start="9">
<li>Once we specify the <kbd>weights</kbd>  of test data, we will go ahead and fit the model as follows:</li>
</ol>
<pre style="padding-left: 90px">model = Model([input_layer, weights_tensor], out)<br/>model.compile(adam, cl4)<br/>model.fit(x=[X_train, weights], y=y_train, epochs=300,batch_size = 32, validation_data = ([X_test, test_weights], y_test))</pre>
<p style="padding-left: 60px">The preceding results in a test dataset loss that is very different to what we saw in the previous section. We will look at the reason for this in more detail in the <a href="7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml" target="_blank">Chapter 11</a>, <em>Building a Recurrent Neural Network</em> chapter.</p>
<div class="mce-root packt_tip">You need to be extremely careful while implementing the preceding model, as it has a few pitfalls. However, in general, it is advised to implement models to predict stock price movements only after sufficient due diligence.</div>


            </article>

            
        </section>
    </div>



  </body></html>