["```py\n# Libraries used\nlibrary(keras)\nlibrary(deepviz)\nlibrary(readtext)\n```", "```py\n# Reading Reuters train data\nsetwd(\"~/Desktop/C50/C50train\")\ntemp = list.files(pattern=\"*.*\")\nk <- 1; tr <- list(); trainx <- list(); trainy <- list()\nfor (i in 1:length(temp)) {for (j in 1:50) \n         { trainy[k] <- temp[i]\n         k <- k+1}\nauthor <- temp[i]\nfiles <- paste0(\"~/Desktop/C50/C50train/\", author, \"/*\")\ntr <- readtext(files)\ntrainx <- rbind(trainx, tr)}\ntrainx <- trainx$text\n```", "```py\n# Text file 901\ntrainx[901]\n[1] \"Drug discovery specialist Chiroscience Group plc said on Monday it is testing two anti-cancer compounds before deciding which will go forward into human trials before the end of the year.\\nBoth are MMP inhibitors, the same novel class of drug as British Biotech Plc's potential blockbuster Marimastat, which are believed to stop cancer cells from spreading.\\nIn an interview, chief executive John Padfield said Chiroscience hoped to have its own competitor to Marimastat in early trials next year and Phase III trials in 1998.\"\n\n# Author\ntrainy[901]\n[[1]]\n[1] \"JonathanBirt\"\n```", "```py\n# Reuters test data\nsetwd(\"~/Desktop/C50/C50test\")\ntemp = list.files(pattern=\"*.*\")\nk <- 1; tr <- list(); testx <- list(); testy <- list()\nfor (i in 1:length(temp)) {for (j in 1:50) \n         { testy[k] <- temp[i]\n         k <- k+1}\n         author <- temp[i]\n         files <- paste0(\"~/Desktop/C50/C50test/\", author, \"/*\")\n         tr <- readtext(files)\n         testx <- rbind(testx, tr)}\ntestx <- testx$text\n```", "```py\n# Tokenization\ntoken <- text_tokenizer(num_words = 500) %>%    \n         fit_text_tokenizer(trainx)\n\n# Text to sequence of integers\ntrainx <- texts_to_sequences(token, trainx)\ntestx <- texts_to_sequences(token, testx)\n\n# Examples\ntrainx[[7]]\n[1] 98   4  41  5  4  2  4  425  5  20  4  9  4  195  5  157  1  18\n[19] 87  3  90  3  59 1 169 346  2  29  52 425   6  72 386 110 331  24\n[37] 5   4  3  31  3  22   7  65  33 169 329  10 105  1 239  11   4  31\n[55] 11 422  8  60 163 318  10  58 102   2 137 329 277  98 58 287  20  81\n[73] 3 142  9   6  87   3  49  20 142   2 142   6   2  60  13   1 470   8\n[91] 137 190  60   1  85 152   5  6 211  1  3  1  85  11  2 211 233  51\n[109] 233 490  7 155   3 305   6  4  86  3  70  4  3 157  52 142   6 282\n[127] 233  4 286  11 485  47  11   9  1 386 497  2  72  7  33   6  3  1\n[145] 60   3 234  23  32  72 485   7 203   6  29 390  5   3  19  13  55 184\n[163] 53  10   1  41  19 485 119  18   6  59  1 169   1  41  10  17 458  91\n[181] 6  23  12   1   3   3  10 491   2  14   1   1 194 469 491  2  1   4\n[199] 331 112 485 475  16  1 469  1 331  14   2 485 234  5 171 296  1  85\n[217] 11 135 157  2 189  1  31  24   4   5 318 490 338   6 147 194  24 347\n[235] 386  23  24  32 117 286 161  6 338  25   4  32  2  9  1  38  8 316\n[253] 60 153  27 234 496 457 153  20 316  2 254 219 145 117  25  46  27  7\n[271] 228  34 184  75 11 418  52 296   1 194 469 180 469  6  1 268  6 250\n[289] 469  29 90  6  15  58 175  32  33 229  37 424  36  51  36  3 169  15\n[307] 1  7 175  1 319 207  5   4\n\ntrainx[[901]]\n[1]  74 356 7  9 199  12  11  61 145 31  22 399 79 145  1 133  3  1  28 203\n[21] 29  1 319  3  18 101 470 31  29  2  20  5  33 369 116 134  7  2  25 17\n[41] 303  2  5 222 100  28   6   5\n```", "```py\n# Integers per article for train data\nz <- NULL\nfor (i in 1:2500) {z[i] <- print(length(trainx[[i]]))}\nsummary(z)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   31.0   271.0   326.0   326.8   380.0   918.0 \n\n# Intergers per article for text data\nz <- NULL\nfor (i in 1:2500) {z[i] <- print(length(testx[[i]]))}\nsummary(z)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   39.0   271.0   331.0   329.1   384.0  1001.0 \n```", "```py\n# Train and test labels to integers\ntrainy <- as.factor(unlist(trainy))\ntrainy <- as.integer(trainy) -1 \ntesty <- as.factor(unlist(testy))\ntesty <- as.integer(testy) -1\n\n# Saving original labels\ntrainy_org <- trainy\ntesty_org <- testy\n```", "```py\n# Padding and truncation\ntrainx <- pad_sequences(trainx, maxlen = 300) \ntestx <- pad_sequences(testx, maxlen = 300)\ndim(trainx) \n[1] 2500  300\n```", "```py\n# Example of truncation\ntrainx[7,]\n  [1]   5 157   1  18  87   3  90   3  59   1 169 346   2  29  52 425\n [17]   6  72 386 110 331  24   5   4   3  31   3  22   7  65  33 169\n [33] 329  10 105   1 239  11   4  31  11 422   8  60 163 318  10  58\n [49] 102   2 137 329 277  98  58 287  20  81   3 142   9   6  87   3\n [65]  49  20 142   2 142   6   2  60  13   1 470   8 137 190  60   1\n [81]  85 152   5   6 211   1   3   1  85  11   2 211 233  51 233 490\n [97]   7 155   3 305   6   4  86   3  70   4   3 157  52 142   6 282\n[113] 233   4 286  11 485  47  11   9   1 386 497   2  72   7  33   6\n[129]   3   1  60   3 234  23  32  72 485   7 203   6  29 390   5   3\n[145]  19  13  55 184  53  10   1  41  19 485 119  18   6  59   1 169\n[161]   1  41  10  17 458  91   6  23  12   1   3   3  10 491   2  14\n[177]   1   1 194 469 491   2   1   4 331 112 485 475  16   1 469   1\n[193] 331  14   2 485 234   5 171 296   1  85  11 135 157   2 189   1\n[209]  31  24   4   5 318 490 338   6 147 194  24 347 386  23  24  32\n[225] 117 286 161   6 338  25   4  32   2   9   1  38   8 316  60 153\n[241]  27 234 496 457 153  20 316   2 254 219 145 117  25  46  27   7\n[257] 228  34 184  75  11 418  52 296   1 194 469 180 469   6   1 268\n[273]   6 250 469  29  90   6  15  58 175  32  33 229  37 424  36  51\n[289]  36   3 169  15   1   7 175   1 319 207   5   4\n\n# Example of padding\ntrainx[901,]\n  [1]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [17]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [33]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [49]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [65]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [81]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [97]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[113]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[129]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[145]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[161]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[177]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[193]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[209]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[225]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[241]   0   0   0   0   0   0   0   0   0   0   0   0  74 356   7   9\n[257] 199  12  11  61 145  31  22 399  79 145   1 133   3   1  28 203\n[273]  29   1 319   3  18 101 470  31  29   2  20   5  33 369 116 134\n[289]   7   2  25  17 303   2   5 222 100  28   6   5\n```", "```py\n# Data partition\ntrainx_org <- trainx  \ntestx_org <- testx\nset.seed(1234)\nind <- sample(2, nrow(trainx), replace = T, prob=c(.8, .2))\ntrainx <- trainx_org[ind==1, ]\nvalidx <- trainx_org[ind==2, ]\ntrainy <- trainy_org[ind==1]\nvalidy <- trainy_org[ind==2]\n```", "```py\n# OHE\ntrainy <- to_categorical(trainy, 50)\nvalidy <- to_categorical(validy, 50)\ntesty <- to_categorical(testy, 50)\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential() %>%\n         layer_embedding(input_dim = 500, \n                         output_dim = 32, \n                         input_length = 300) %>%\n         layer_conv_1d(filters = 32, \n                  kernel_size = 5, \n                  padding = \"valid\",\n                  activation = \"relu\",\n                  strides = 1) %>%\n         layer_max_pooling_1d(pool_size = 4) %>%\n         layer_lstm(units = 32) %>%\n         layer_dense(units = 50, activation = \"softmax\") \n\n# Model summary\nsummary(model)\n___________________________________________________________________________\nLayer (type)                     Output Shape                  Param #     \n===========================================================================\nembedding (Embedding)            (None, 300, 32)               16000       \n___________________________________________________________________________\nconv1d (Conv1D)                  (None, 296, 32)               5152        \n___________________________________________________________________________\nmax_pooling1d (MaxPooling1D)     (None, 74, 32)                0           \n___________________________________________________________________________\nlstm (LSTM)                      (None, 32)                    8320        \n___________________________________________________________________________\ndense (Dense)                    (None, 50)                    1650        \n===========================================================================\nTotal params: 31,122\nTrainable params: 31,122\nNon-trainable params: 0\n___________________________________________________________________________\n```", "```py\n# Compile model\nmodel %>% compile(optimizer = \"adam\",  \n         loss = \"categorical_crossentropy\",\n         metrics = c(\"acc\"))\n```", "```py\n# Fitting the model\nmodel_one <- model %>% fit(trainx, trainy,\n         epochs = 30,\n         batch_size = 32,\n         validation_data = list(validx, validy))\n\n# Loss and accuracy plot\nplot(model_one)\n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(trainx, trainy)\n$loss\n[1] 1.45669\n$acc\n[1] 0.5346288\n```", "```py\n# Prediction and confusion matrix\npred <- model %>%   predict_classes(trainx_org)\ntab <- table(Predicted=pred, Actual=trainy_org)\n(accuracy <- 100*diag(tab)/colSums(tab))\n 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \n82 40 30 10 54 46 54 82  8 56 46 36 76 18 52 90 50 56  8 66 80 24 30 46 32 \n25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \n46 88 62 22 64 76  2 74 88 72 74 76 86 70 60 86 38 32  0 48  6 24 76  8 22 \n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(testx, testy)\n$loss\n[1] 2.460835\n$acc\n[1] 0.2508\n```", "```py\n# Prediction and confusion matrix\npred1 <- model %>%   predict_classes(testx)\ntab1 <- table(Predicted=pred1, Actual=testy_org)\n(accuracy <- 100*diag(tab1)/colSums(tab1))\n 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \n22 28  2  2 28 14 14 20  6 28 24  8 28  8 46 84 14 36 10 50 40 12  4 22  4 \n25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 \n18 54 38 12 34 46  0 52 26 48 40 26 84 46 18 24 26 10  0 46  0  4 38  0 10\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential() %>%\n         layer_embedding(input_dim = 1500, \n                         output_dim = 32, \n                         input_length = 400) %>%\n         layer_conv_1d(filters = 32, \n                  kernel_size = 5, \n                  padding = \"valid\",\n                  activation = \"relu\",\n                  strides = 1) %>%\n         layer_max_pooling_1d(pool_size = 4) %>%\n         layer_dropout(0.25) %>%\n         layer_lstm(units = 32) %>%\n         layer_dense(units = 50, activation = \"softmax\") \n\n# Compiling the model\nmodel %>% compile(optimizer = \"adam\",  \n         loss = \"categorical_crossentropy\",\n         metrics = c(\"acc\"))\n\n# Fitting the model\nmodel_two <- model %>% fit(trainx, trainy,\n         epochs = 30,\n         batch_size = 16,\n         validation_data = list(validx, validy))\n\n# Plot of loss and accuracy\nplot(model_two)\n```", "```py\n# Loss and accuracy for train data\nmodel %>% evaluate(trainx, trainy)\n$loss\n[1] 0.3890106\n$acc\n[1] 0.9133034\n\n# Loss and accuracy for test data\nmodel %>% evaluate(testx, testy)\n$loss\n[1] 2.710119\n$acc\n[1] 0.308\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential() %>%\n          layer_embedding(input_dim = 1500,\n                          output_dim = 32,\n                          input_length = 400) %>%\n          layer_conv_1d(filters = 64,\n                   kernel_size = 4,\n                   padding = \"valid\",\n                   activation = \"relu\",\n                   strides = 1) %>%\n          layer_max_pooling_1d(pool_size = 4) %>%\n          layer_dropout(0.25) %>%\n          layer_lstm(units = 32) %>%\n          layer_dense(units = 50, activation = \"softmax\")\n\n# Compiling the model\n model %>% compile(optimizer = \"adam\",  \n          loss = \"categorical_crossentropy\",\n          metrics = c(\"acc\"))\n\n # Fitting the model\n model_three <- model %>% fit(trainx, trainy,\n          epochs = 30,\n          batch_size = 8,\n          validation_data = list(validx, validy))\n\n# Loss and accuracy plot\nplot(model_three)\n```", "```py\n# Loss and accuracy for train data\nmodel %>% evaluate(trainx, trainy)\n$loss\n[1] 0.1093387\n$acc\n[1] 0.9880419\n\n# Loss and accuracy for test data\nmodel %>% evaluate(testx, testy)\n[1] 3.262691\n$acc\n[1] 0.337\n```"]