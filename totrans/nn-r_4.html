<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Perceptron Neural Network Modeling – Basic Models</h1>
                
            
            <article>
                
<p class="calibre2">So far, we have seen the basics of neural networks and how the learning portion works. In this chapter, we take a look at one of the basic and simple forms of neural network architecture, the perceptron.</p>
<p class="calibre2">A <strong class="calibre1">perceptron</strong> is defined as a basic building block of a neural network. In machine learning, a perceptron is an algorithm for supervised learning of binary classifiers. They classify an output as binary: <kbd class="calibre13">TRUE</kbd>/<kbd class="calibre13">FALSE</kbd> or <kbd class="calibre13">1</kbd>/<kbd class="calibre13">0</kbd>.</p>
<p class="calibre2">This chapter helps understand the following topics:</p>
<ul class="calibre16">
<li class="calibre17">Explanation of the perceptron</li>
<li class="calibre17">Linear separable classifier</li>
<li class="calibre17">Simple perceptron implementation function</li>
<li class="calibre17"><strong class="calibre1">Multi-Layer Perceptrons</strong> (<strong class="calibre1">MLPs</strong><span>)</span></li>
</ul>
<p class="calibre2"><span>By the end of the chapter, we will understand the basic concepts of perceptrons and how they are used in neural network algorithm. We will discover the linear separable classifier. We will learn a simple perceptron implementation function in R environment. We will know how to train and model an MLP.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Perceptrons and their applications</h1>
                
            
            <article>
                
<p class="calibre2"><span>A perceptron can be understood as anything that takes multiple inputs and produces one output. It is the simplest form of a neural network. The perceptron was proposed by Frank Rosenblatt in 1958 as an entity with an input and output layer and a learning rule based on minimizing the error. This learning function called <strong class="calibre1">error backpropagation</strong> alters connective weights (synapses) based on the actual output of the network with respect to a given input, as the difference between the actual output and the desired output.</span></p>
<p class="calibre2">The enthusiasm was enormous and the cybernetics industry was born. But later, scientists Marvin Minsky and Seymour Papert (1969) demonstrated the limits of the perceptron. Indeed, a perceptron is able to recognize, after a suitable training, only linearly separable functions. For example, the XOR logic function cannot be implemented by a perceptron.</p>
<p class="calibre2">The following image showns Frank Rosenblatt at the Cornell Aeronautical Laboratory (1957-1959), while working on the Mark I Perceptron classifier:</p>
<div class="cdpaligncenter"><img class="image-border19" src="../images/00080.jpeg"/></div>
<p class="calibre2">Potentially, a multilevel network of percetters could solve more complex problems, but the increasing computational complexity of training made this path impracticable. Only in recent times have we started to consider the utility of this operational entity.</p>
<p class="calibre2"><span>In the single form, a perceptron has one neuron unit accepting inputs and producing a set of outputs.</span></p>
<p class="calibre2"><span>For example, let us take a look at the following image:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00081.jpeg"/></div>
<p class="calibre2">Here <strong class="calibre1">x<sub class="calibre25">1</sub>, x<sub class="calibre25">2</sub>,.., x<sub class="calibre25">n</sub></strong> are the set of inputs and <strong class="calibre1">x<sub class="calibre25">0</sub></strong> is the bias. <strong class="calibre1">x<sub class="calibre25">0</sub></strong> is set to <strong class="calibre1">1</strong>. The output <strong class="calibre1">y</strong> is the sum product of <strong class="calibre1">w<sub class="calibre25">i</sub>x<sub class="calibre25">i</sub></strong>. The <strong class="calibre1">signum</strong> function is applied after the sum product has been executed.</p>
<p class="calibre2">It separates the output as:</p>
<ul class="calibre16">
<li class="calibre17">If <strong class="calibre1">y&gt;0</strong>, the output is <strong class="calibre1">1</strong></li>
<li class="calibre17">If <strong class="calibre1">y&lt;=0</strong>, the output is <strong class="calibre1">-1</strong></li>
</ul>
<p class="calibre2">The bias is constant and is associated with weight <strong class="calibre1">w<sub class="calibre25">0</sub></strong>. This perceptron functions as a linear separator, splitting the output into one category, <strong class="calibre1">-1</strong> or <strong class="calibre1">+1</strong>.</p>
<p class="calibre2">Note that here this is no backpropagation and the weight update updates through steps we will soon see. There is a threshold setup which determines the value of the output. The output here is binary (either <strong class="calibre1">-1</strong> or <strong class="calibre1">+1</strong>), which can be set as zero or one.</p>
<p class="calibre2">Hence, a perceptron is a simple classification function that directly makes its prediction. The core of the functionality lives in the weights and how we update the weights to the best possible prediction of <strong class="calibre1">y</strong>.</p>
<p class="calibre2">This case is a <strong class="calibre1">simple perceptron</strong> or basic perceptron, and the outputs are binary in nature: <em class="calibre14">0/1</em> <em class="calibre14">true/false</em> <em class="calibre14">+1/-1</em>.</p>
<p class="calibre2">There is another type of perceptron called the <strong class="calibre1">multi-class perceptron</strong>, which can classify many possible labels for an animal, such as <span>dog, cat, or bird.</span></p>
<p class="calibre2">In the following figure is shown a simple perceptron architecture versus multi-class perceptron architecture:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00082.jpeg"/></div>
<p class="calibre2">By modifying the weighting vector, we can modify the output of a perceptron to improve the learning or storage properties. For example, we can try to instruct a perceptron such that given an input <em class="calibre14">x</em>, output <em class="calibre14">y</em> is as close as possible to a given a priori chosen <em class="calibre14">y</em> actual value. The computational capabilities of a single perceptron are, however, limited, and the performance that can be obtained depends heavily on both the input choice and the choice of function that you want to implement.</p>
<p class="calibre2">In fact, inputs can be limited to a subset of all the possible inputs, or be randomly extracted according to a certain predetermined probability distribution. To a lesser extent, the performance of such a system also depends on how the distance between the actual outputs and the expected outputs is quantified.</p>
<p class="calibre2">Once you have identified the problem of learning, you can try to find the optimal weight assignment for the given problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Simple perceptron – a linear separable classifier</h1>
                
            
            <article>
                
<p class="calibre2">As we saw, a simple perceptron is a single layer neural unit which is a linear classifier. It is a neuron capable of producing only two output patterns, which can be synthesized in <em class="calibre14">active</em> or <em class="calibre14">inactive</em>. Its decision rule is implemented by a <em class="calibre14">threshold</em> behavior: if the sum of the activation patterns of the individual neurons that make up the input layer, weighted for their weights, exceeds a certain threshold, then the output neuron will adopt the output pattern <em class="calibre14">active</em>. Conversely, the output neuron will remain in the <em class="calibre14">inactive</em> state.</p>
<p class="calibre2">As mentioned, the output is the sum of <em class="calibre14">weights*inputs</em> and a function applied on top of it; output is <em class="calibre14">+1 (y&gt;0)</em> or <em class="calibre14">-1(y&lt;=0)</em>, as shown in the following figure:</p>
<div class="calibre28"><img src="../images/00083.jpeg" class="calibre70"/></div>
<p class="calibre2"> </p>
<p class="calibre2">We can see the linear interaction here; the output <em class="calibre14">y</em> is linearly dependent on the inputs.</p>
<p class="calibre2">As with most neural network models, it is possible to realize a learning function based on the modification of synaptic connective weights, even in perceptors. At the beginning of the training phase, weights <em class="calibre14">w</em> of perceptron synaptic connections assume completely random values. For training, we have a number of examples with its relative, correct, classification. The network is presented in turn, the different cases to be classified and the network processes each time its response (greater than the threshold or less than the threshold). If the classification is correct (network output is the same as expected), the training algorithm does not make any changes. On the contrary, if the classification is incorrect, the algorithm changes the synaptic weights in an attempt to improve the classification performance of the network.</p>
<p class="calibre2">The single perceptron is an online learner. The weight updates happen through the following steps:</p>
<ol class="calibre19">
<li value="1" class="calibre17">Get <em class="calibre14">x</em> and output label <em class="calibre14">y</em>.</li>
<li value="2" class="calibre17">Update <em class="calibre14">w</em> for <em class="calibre14">f(x)</em>.</li>
<li value="3" class="calibre17">If <em class="calibre14">f(x)=y</em>, mark as completed; else, fix it</li>
<li value="4" class="calibre17">Now adjust score based on error:</li>
</ol>
<p class="calibre71"><em class="calibre14">f(x)= sign(sum of weights*inputs)</em>, the errors are possible</p>
<p class="calibre71">if <em class="calibre14">y=+1</em> and <em class="calibre14">f(x)=-1, w*x</em> is too small, make it bigger</p>
<p class="calibre71">if <em class="calibre14">y=-1</em> and <em class="calibre14">f(x)=+1, w*x</em> is too large make it smaller</p>
<ol start="5" class="calibre19">
<li value="5" class="calibre17">Apply the following rules:</li>
</ol>
<p class="calibre71">make <em class="calibre14">w=w-x</em> if <em class="calibre14">f(f)=+1</em> and <em class="calibre14">y=-1</em></p>
<p class="calibre71">make <em class="calibre14">w=w+x</em> if <em class="calibre14">f(f)=-1</em> and <em class="calibre14">y=+1</em></p>
<p class="calibre71"><em class="calibre14">w=w</em> if <em class="calibre14">f(x)=y</em></p>
<p class="calibre71">Or simply, <em class="calibre14">w=w+yx</em> if <em class="calibre14">f(x)!=y</em></p>
<ol start="6" class="calibre19">
<li value="6" class="calibre17">Repeat steps 3 to 5, until <em class="calibre14">f(x) = y</em>.</li>
</ol>
<p class="calibre2">The perceptron is guaranteed to satisfy all our data, but only for a binary classifier with a single neuron. In step 5, we brought a term called <strong class="calibre1">learning rate</strong>. This helps our model converge. In step 5, <em class="calibre14">w</em> is written as: <em class="calibre14">w=w+αyx if f(x) != y</em>, where <em class="calibre14">α</em> is the learning rate chosen.</p>
<p class="calibre2">The bias is also updated as <em class="calibre14">b=b+ αy</em> if <em class="calibre14">f(x) != y</em>. The <em class="calibre14">b</em> is actually our <em class="calibre14">w<sub class="calibre25">0</sub></em>.</p>
<p class="calibre2">If the Boolean function is a linear threshold function (that is, if it is linearly separable), then the local perceptron rule can find a set of weights capable of achieving it in a finite number of steps.</p>
<p class="calibre2">This theorem, known as the <strong class="calibre1">perceptron theorem</strong>, is also applicable in the case of the global rule, which modifies the vector of synaptic weights <em class="calibre14">w</em>, not at a single input vector, but depending on the behavior of the perceptron on the whole set of input vectors.</p>
<p class="calibre2">We just mentioned the linearly separable function, but what is meant by this term? We will understand it in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Linear separation</h1>
                
            
            <article>
                
<p class="calibre2">When a set of output values can be split by a straight line, the output values are said to be linearly separable. Geometrically, this condition describes the situation in which there is a hyperplane that separates, in the vector space of inputs, those that require positive output from those that require a negative output, as shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00084.jpeg"/></div>
<p class="calibre2">Here, one side of the separator are those predicted to belong to one class whilst those on the other side are predicted to belong to a different class. The decision rule of the Boolean neuron corresponds to the breakdown of the input features space, operated by a hyperplane.</p>
<p class="calibre2">If, in addition to the output neuron, even the input of the neural network is Boolean, then using the neural network to perform a classification is equivalent to determining a Boolean function of the input vector. This function takes the value 1 where it exceeds the threshold value, 0 otherwise. For example, with two input and output Boolean neurons, it is possible to represent, in an extremely intuitive way, the <em class="calibre14">AND</em> and <em class="calibre14">OR</em> functions.</p>
<p class="calibre2">Indeed, the <em class="calibre14">AND</em> gate and <em class="calibre14">OR</em> gate are linearly separable. Let's test it in practice by first listing the possible cases in a table and then representing them on a two-dimensional plane.</p>
<p class="calibre2">Let's first do this for the <em class="calibre14">AND</em> function. In the following table are listed all the <span>possible cases with the logical results:</span></p>
<table class="calibre72">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">x1</strong></td>
<td class="calibre8"><strong class="calibre1">x2</strong></td>
<td class="calibre8"><strong class="calibre1">y</strong> (<strong class="calibre1">AND gate</strong>)</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">1</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">0</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">0</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">0</em></td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">The following figure shows all the four cases in a <span>two-dimensional plane:</span></p>
<div class="cdpaligncenter"><img class="image-border20" src="../images/00085.jpeg"/></div>
<p class="calibre2">All points above the hyperplane are assumed to be <em class="calibre14">1/TRUE</em>, while the ones below are assumed to be <em class="calibre14">0/FALSE</em>.</p>
<p class="calibre2">Let's do it now for the <em class="calibre14">OR</em> function. <span>In the following table are listed all the</span> <span>possible cases with the logical results:</span></p>
<table class="calibre72">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">x1</strong></td>
<td class="calibre8"><strong class="calibre1">x2</strong></td>
<td class="calibre8"><strong class="calibre1">y</strong> (<strong class="calibre1">OR gate</strong>)</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">1</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">1</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">1</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">0</em></td>
</tr>
</tbody>
</table>
<p class="calibre2"><span>The following figure shows all the four cases in a</span> <span>two-dimensional plane:</span></p>
<div class="cdpaligncenter"><img class="image-border21" src="../images/00086.jpeg"/></div>
<p class="calibre2"><span>In this case also, all the points above the hyperplane are assumed to be <em class="calibre14">1/TRUE</em>, while the ones below are assumed to be <em class="calibre14">0/FALSE</em>.</span></p>
<p class="calibre2">However, some Boolean functions cannot be replicated through a network structure, such as that seen up to here. The <em class="calibre14">XOR</em> and identity functions, for example, are not separable: to isolate them, two lines would be needed, which can be implemented only through the use of a more complex network structure.</p>
<p class="calibre2"><span>In the following table are listed all the</span> <span>possible cases with the logical results, for the <em class="calibre14">XOR</em> function:</span></p>
<table class="calibre72">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">x1</strong></td>
<td class="calibre8"><strong class="calibre1">x2</strong></td>
<td class="calibre8"><strong class="calibre1">y (XOR gate)</strong></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">0</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">1</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">1</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">0</em></td>
</tr>
</tbody>
</table>
<p class="calibre2"><span>In the following figure are shown all the four cases in a</span> <span>two-dimensional plane:</span></p>
<div class="cdpaligncenter"><img class="image-border22" src="../images/00087.jpeg"/></div>
<p class="calibre2">As anticipated, such a function requires two lines to group all possible cases.</p>
<p class="calibre2">After understanding the basics of perceptron theory, we can study a practical case.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The perceptron function in R</h1>
                
            
            <article>
                
<p class="calibre2">In the previous sections, we understood the fundamental concepts underlying the use of a perceptron as a classifier. The time has come to put into practice what has been studied so far. We will do it by analyzing an example in which we will try to classify the floral species on the basis of the size of the petals and sepals of an Iris. As you will recall, the <kbd class="calibre13">iris</kbd> dataset has already been used in <a href="part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 3</a>, <em class="calibre14">Deep Learning Using Multilayer Neural Networks</em>. The reason for its re-use is not only due to the quality of the data contained in it that allows the reader to easily understand the concepts outlined, but also, and more importantly, to be able to compare the different algorithms.</p>
<p class="calibre2">As you will recall, the dataset contains 50 samples from each of three species of Iris (Iris <kbd class="calibre13">setosa</kbd>, Iris <kbd class="calibre13">virginica</kbd>, and Iris <kbd class="calibre13">versicolor</kbd>). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.</p>
<p class="calibre2">The following variables are contained:</p>
<ul class="calibre16">
<li class="calibre17">Sepal length in cm</li>
<li class="calibre17">Sepal width in cm</li>
<li class="calibre17">Petal length in cm</li>
<li class="calibre17">Petal width in cm</li>
<li class="calibre17">Class: <kbd class="calibre13">setosa</kbd>, <kbd class="calibre13">versicolour</kbd>, <kbd class="calibre13">virginica</kbd></li>
</ul>
<p class="calibre2">In the example presented, we will try to classify the <kbd class="calibre13">setosa</kbd> and <kbd class="calibre13">versicolor</kbd> species through linear separation.</p>
<p class="calibre2">Let us implement a perceptron function in R for the <kbd class="calibre13">iris</kbd> dataset. The code is presented next:</p>
<pre class="calibre24"><strong class="calibre1">######################################################################</strong><br class="title-page-name"/><strong class="calibre1">###Chapter 4 - Introduction to Neural Networks - using R    ##########</strong><br class="title-page-name"/><strong class="calibre1">###Simple Perceptron implementation function in R - iris dataset  ####</strong><br class="title-page-name"/><strong class="calibre1">######################################################################</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">data(iris)</strong><br class="title-page-name"/><strong class="calibre1">head(iris, n=20)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">iris_sub=iris[1:100, c(1, 3, 5)] </strong><br class="title-page-name"/><strong class="calibre1">names(iris_sub)=c("sepal", "petal", "species") </strong><br class="title-page-name"/><strong class="calibre1">head(iris_sub) </strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">library(ggplot2) </strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">ggplot(iris_sub, aes(x = sepal, y = petal)) + </strong><br class="title-page-name"/><strong class="calibre1"> geom_point(aes(colour=species, shape=species), size = 3) +</strong><br class="title-page-name"/><strong class="calibre1"> xlab("Sepal length") +</strong><br class="title-page-name"/><strong class="calibre1"> ylab("Petal length") +</strong><br class="title-page-name"/><strong class="calibre1"> ggtitle("Species vs Sepal and Petal lengths")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">euclidean.norm = function(x) {sqrt(sum(x * x))}</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">distance.from.plane = function(z,w,b) {</strong><br class="title-page-name"/><strong class="calibre1"> sum(z*w) + b</strong><br class="title-page-name"/><strong class="calibre1">}</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">classify.linear = function(x,w,b) {</strong><br class="title-page-name"/><strong class="calibre1"> distances = apply(x, 1, distance.from.plane, w, b)</strong><br class="title-page-name"/><strong class="calibre1"> return(ifelse(distances &lt; 0, -1, +1))</strong><br class="title-page-name"/><strong class="calibre1">}</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">perceptron = function(x, y, learning.rate=1) {</strong><br class="title-page-name"/><strong class="calibre1"> w = vector(length = ncol(x)) # initialize weights</strong><br class="title-page-name"/><strong class="calibre1"> b = 0 # Initialize bias</strong><br class="title-page-name"/><strong class="calibre1"> k = 0 # count updates</strong><br class="title-page-name"/><strong class="calibre1"> R = max(apply(x, 1, euclidean.norm))</strong><br class="title-page-name"/><strong class="calibre1"> mark.complete = TRUE </strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1"> while (mark.complete) {</strong><br class="title-page-name"/><strong class="calibre1">    mark.complete=FALSE </strong><br class="title-page-name"/><strong class="calibre1">    yc = classify.linear(x,w,b)</strong><br class="title-page-name"/><strong class="calibre1">    for (i in 1:nrow(x)) {</strong><br class="title-page-name"/><strong class="calibre1">       if (y[i] != yc[i]) {</strong><br class="title-page-name"/><strong class="calibre1">          w = w + learning.rate * y[i]*x[i,]</strong><br class="title-page-name"/><strong class="calibre1">          b = b + learning.rate * y[i]*R^2</strong><br class="title-page-name"/><strong class="calibre1">          k = k+1</strong><br class="title-page-name"/><strong class="calibre1">          mark.complete=TRUE</strong><br class="title-page-name"/><strong class="calibre1">       }</strong><br class="title-page-name"/><strong class="calibre1">    }</strong><br class="title-page-name"/><strong class="calibre1"> }</strong><br class="title-page-name"/><strong class="calibre1"> s = euclidean.norm(w)</strong><br class="title-page-name"/><strong class="calibre1"> return(list(w=w/s,b=b/s,updates=k))</strong><br class="title-page-name"/><strong class="calibre1">}</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">x = cbind(iris_sub$sepal, iris_sub$petal)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">y = ifelse(iris_sub$species == "setosa", +1, -1)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">p = perceptron(x,y)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">plot(x,cex=0.2)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">points(subset(x,Y==1),col="black",pch="+",cex=2)</strong><br class="title-page-name"/><strong class="calibre1">points(subset(x,Y==-1),col="red",pch="-",cex=2)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">intercept = - p$b / p$w[[2]]</strong><br class="title-page-name"/><strong class="calibre1">slope = - p$w[[1]] /p$ w[[2]]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">abline(intercept,slope,col="green")</strong></pre>
<p class="calibre2">Now, let us go through the code line-by-line. Following the style in the rest of this book, we will present a portion of the code first as follows and then explain it in detail:</p>
<pre class="calibre24"><strong class="calibre1">data(iris)</strong><br class="title-page-name"/><strong class="calibre1">head(iris, n=20)</strong></pre>
<p class="calibre2"><span>The first command loads the <kbd class="calibre13">iris</kbd> dataset, which is contained in the datasets library, and saves it in a given dataframe. Then we use the <kbd class="calibre13">head</kbd> function to display the first <kbd class="calibre13">20</kbd> lines of the dataset. Remember, the <kbd class="calibre13">head</kbd> function returns the first or last parts of a vector, matrix, table, dataframe, or function. In this case, we specify the number of lines that must be displayed (<kbd class="calibre13">n=20</kbd>). The following is the result:</span></p>
<pre class="calibre24"><strong class="calibre1">&gt; head(iris, n=20)</strong><br class="title-page-name"/><strong class="calibre1">   Sepal.Length Sepal.Width Petal.Length Petal.Width Species</strong><br class="title-page-name"/><strong class="calibre1">1           5.1         3.5          1.4         0.2  setosa</strong><br class="title-page-name"/><strong class="calibre1">2           4.9         3.0          1.4         0.2  setosa</strong><br class="title-page-name"/><strong class="calibre1">3           4.7         3.2          1.3         0.2  setosa</strong><br class="title-page-name"/><strong class="calibre1">4           4.6         3.1          1.5         0.2  setosa</strong><br class="title-page-name"/><strong class="calibre1">5           5.0         3.6          1.4         0.2  setosa</strong><br class="title-page-name"/><strong class="calibre1">6           5.4         3.9          1.7         0.4  setosa</strong><br class="title-page-name"/><strong class="calibre1">7           4.6         3.4          1.4         0.3  setosa</strong><br class="title-page-name"/><strong class="calibre1">8           5.0         3.4          1.5         0.2  setosa</strong><br class="title-page-name"/><strong class="calibre1">9           4.4         2.9          1.4         0.2  setosa</strong><br class="title-page-name"/><strong class="calibre1">10          4.9         3.1          1.5         0.1  setosa</strong><br class="title-page-name"/><strong class="calibre1">11          5.4         3.7          1.5         0.2  setosa</strong><br class="title-page-name"/><strong class="calibre1">12          4.8         3.4          1.6         0.2  setosa</strong><br class="title-page-name"/><strong class="calibre1">13          4.8         3.0          1.4         0.1  setosa</strong><br class="title-page-name"/><strong class="calibre1">14          4.3         3.0          1.1         0.1  setosa</strong><br class="title-page-name"/><strong class="calibre1">15          5.8         4.0          1.2         0.2  setosa</strong><br class="title-page-name"/><strong class="calibre1">16          5.7         4.4          1.5         0.4  setosa</strong><br class="title-page-name"/><strong class="calibre1">17          5.4         3.9          1.3         0.4  setosa</strong><br class="title-page-name"/><strong class="calibre1">18          5.1         3.5          1.4         0.3  setosa</strong><br class="title-page-name"/><strong class="calibre1">19          5.7         3.8          1.7         0.3  setosa</strong><br class="title-page-name"/><strong class="calibre1">20          5.1         3.8          1.5         0.3  setosa</strong></pre>
<p class="calibre2">Let's go back to the code. We will get the binary output by extracting only <em class="calibre14">100</em> rows of the <kbd class="calibre13">iris</kbd> dataset, and extracting only <kbd class="calibre13">sepal</kbd> length and <kbd class="calibre13">petal</kbd> length with <kbd class="calibre13">species</kbd>:</p>
<div class="title-page-name">
<pre class="calibre24"><strong class="calibre1">iris_sub=iris[1:100, c(1, 3, 5)] </strong><br class="title-page-name"/><strong class="calibre1">names(iris_sub)=c("sepal", "petal", "species") </strong><br class="title-page-name"/><strong class="calibre1">head(iris_sub)</strong> </pre></div>
<p class="calibre2">Here, only the first <kbd class="calibre13">100</kbd> rows of the <kbd class="calibre13">iris</kbd> dataset are taken and columns <kbd class="calibre13">1</kbd>,<kbd class="calibre13">3</kbd>, and <kbd class="calibre13">5</kbd> are chosen. This is because the first <kbd class="calibre13">100</kbd> lines contain the data for the two species (<kbd class="calibre13">setosa</kbd> and <kbd class="calibre13">versicolor</kbd>) we are interested in, in the following example. The three columns are <kbd class="calibre13">sepal.length(x1)</kbd>, <kbd class="calibre13">petal.length(x2)</kbd>, and <kbd class="calibre13">species(y - output)</kbd>.</p>
<div class="title-page-name">
<pre class="calibre24"><strong class="calibre1">library(ggplot2) </strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">ggplot(iris_sub, aes(x = sepal, y = petal)) + </strong><br class="title-page-name"/><strong class="calibre1"> geom_point(aes(colour=species, shape=species), size = 3) +</strong><br class="title-page-name"/><strong class="calibre1"> xlab("Sepal length") +</strong><br class="title-page-name"/><strong class="calibre1"> ylab("Petal length") +</strong><br class="title-page-name"/><strong class="calibre1"> ggtitle("Species vs Sepal and Petal lengths")</strong></pre></div>
<p class="calibre2">First we load the <kbd class="calibre13">ggplot2</kbd> library, and then we use <kbd class="calibre13">ggplot()</kbd> to get the scatterplot of the distribution of species with respect to <kbd class="calibre13">sepal.length</kbd> and <kbd class="calibre13">petal.length</kbd>. Of course, the library should have been installed beforehand.</p>
<div class="packt_tip"><span class="calibre62">Remember, to install a library that is not present in the initial distribution of R, you must use the</span> <kbd class="calibre61">install.package</kbd> <span class="calibre62">function. This is the main function to install packages. It takes a vector of names and a destination library, downloads the packages from the repositories and installs them.</span></div>
<p class="calibre2">The objective of the <kbd class="calibre13">perceptron</kbd> function is to find a linear separation of the <kbd class="calibre13">setosa</kbd> and <kbd class="calibre13">versicolor</kbd> species. The following figure shows <span><strong class="calibre1">Sepal length</strong></span> versus <strong class="calibre1">Petal length</strong> for the two s<span>pecies of Iris flower:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00088.jpeg"/></div>
<p class="calibre2">As can be seen, the two species are placed in distinct areas of the plane, so linear separation is possible. At this point, we need to define functions to do the perceptron processing:</p>
<div class="title-page-name">
<pre class="calibre24"><strong class="calibre1">euclidean.norm = function(x) {sqrt(sum(x * x))}</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">distance.from.plane = function(z,w,b) {</strong><br class="title-page-name"/><strong class="calibre1"> sum(z*w) + b</strong><br class="title-page-name"/><strong class="calibre1">}</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">classify.linear = function(x,w,b) {</strong><br class="title-page-name"/><strong class="calibre1"> distances = apply(x, 1, distance.from.plane, w, b)</strong><br class="title-page-name"/><strong class="calibre1"> return(ifelse(distances &lt; 0, -1, +1))</strong><br class="title-page-name"/><strong class="calibre1">}</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">perceptron = function(x, y, learning.rate=1) {</strong><br class="title-page-name"/><strong class="calibre1"> w = vector(length = ncol(x)) # initialize weights</strong><br class="title-page-name"/><strong class="calibre1"> b = 0 # Initialize bias</strong><br class="title-page-name"/><strong class="calibre1"> k = 0 # count updates</strong><br class="title-page-name"/><strong class="calibre1"> R = max(apply(x, 1, euclidean.norm))</strong><br class="title-page-name"/><strong class="calibre1"> mark.complete = TRUE </strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1"> while (mark.complete) {</strong><br class="title-page-name"/><strong class="calibre1">    mark.complete=FALSE </strong><br class="title-page-name"/><strong class="calibre1">    yc = classify.linear(x,w,b)</strong><br class="title-page-name"/><strong class="calibre1">    for (i in 1:nrow(x)) {</strong><br class="title-page-name"/><strong class="calibre1">       if (y[i] != yc[i]) {</strong><br class="title-page-name"/><strong class="calibre1">          w = w + learning.rate * y[i]*x[i,]</strong><br class="title-page-name"/><strong class="calibre1">          b = b + learning.rate * y[i]*R^2</strong><br class="title-page-name"/><strong class="calibre1">          k = k+1</strong><br class="title-page-name"/><strong class="calibre1">          mark.complete=TRUE</strong><br class="title-page-name"/><strong class="calibre1">       }</strong><br class="title-page-name"/><strong class="calibre1">    }</strong><br class="title-page-name"/><strong class="calibre1"> }</strong><br class="title-page-name"/><strong class="calibre1"> s = euclidean.norm(w)</strong><br class="title-page-name"/><strong class="calibre1"> return(list(w=w/s,b=b/s,updates=k))</strong><br class="title-page-name"/><strong class="calibre1">}</strong></pre></div>
<p class="calibre2">We define the <kbd class="calibre13">perceptron</kbd> function as discussed in the algorithm for perceptron training. We apply <kbd class="calibre13">learning.rate</kbd> as <kbd class="calibre13">1</kbd> and try to update the weights in each loop. Once we have the output and the function <em class="calibre14">(weights*inputs)</em> equal, we stop the training and move out. The updated weights are returned by the function. The objective of the function is to get a set of optimal weights needed for the model as follows:</p>
<div class="title-page-name">
<pre class="calibre24"><strong class="calibre1">x = cbind(iris_sub$sepal, iris_sub$petal)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">y = ifelse(iris_sub$species == "setosa", +1, -1)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">p = perceptron(x,y)</strong></pre></div>
<p class="calibre2">With the first line, we set the <kbd class="calibre13">x</kbd> inputs as the <kbd class="calibre13">sepal</kbd> and <kbd class="calibre13">petal</kbd> lengths. <kbd class="calibre13">sepal.length</kbd> and <kbd class="calibre13">petal.length</kbd> form the input matrix. In the second line, we set <span>label output as positive for <kbd class="calibre13">setosa</kbd> and the rest as negative.</span> The output is either <kbd class="calibre13">setosa</kbd> or not (<kbd class="calibre13">+1</kbd> or <kbd class="calibre13">-1</kbd>). In the third line, we run the <kbd class="calibre13">perceptron</kbd> function.</p>
<p class="calibre2">We call the <kbd class="calibre13">perceptron</kbd> function with <kbd class="calibre13">x</kbd> and <kbd class="calibre13">y</kbd>, which gives the optimal weights for the perceptron as shown in the following code sample:</p>
<div class="title-page-name">
<pre class="calibre24"><strong class="calibre1">plot(x,cex=0.2)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">points(subset(x,Y==1),col="black",pch="+",cex=2)</strong><br class="title-page-name"/><strong class="calibre1">points(subset(x,Y==-1),col="red",pch="*",cex=2)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">intercept = - p$b / p$w[[2]]</strong><br class="title-page-name"/><strong class="calibre1">slope = - p$w[[1]] /p$ w[[2]]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">abline(intercept,slope,col="green")</strong></pre></div>
<p class="calibre2">The previous lines of code plot <kbd class="calibre13">x</kbd> and <kbd class="calibre13">y</kbd>, highlighting <kbd class="calibre13">setosa</kbd> and <kbd class="calibre13">versicolor</kbd> as <kbd class="calibre13">+</kbd> and <kbd class="calibre13">*</kbd> points in the graph. We then find the intercept and slope of the <kbd class="calibre13">p</kbd> variable (<span>perceptron</span>), returned by the perceptron. Plotting the linear separation line gives the following graph:</p>
<div class="cdpaligncenter"><img class="image-border23" src="../images/00089.gif"/></div>
<p class="calibre2">To summarize, we have implemented the perceptron using R code and found optimal weights. The linear separation has been achieved using the perceptron.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Multi-Layer Perceptron</h1>
                
            
            <article>
                
<p class="calibre2">We saw that the <em class="calibre14">AND</em> and <em class="calibre14">OR</em> gate outputs are linearly separable and perceptron can be used to model this data. However, not all functions are separable. In fact, there are very few and their proportion to the total of achievable functions tends to zero as the number of bits increases. Indeed, as we anticipated, if we take the <em class="calibre14">XOR</em> gate, the linear separation is not possible. The crosses and the zeros are in different locations and we cannot put a line to separate them, as shown in the following figure:</p>
<p class="calibre2"> </p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00090.jpeg"/></div>
<p class="calibre2">We could think of parsing more perceptrons. The resulting structure could thus learn a greater number of functions, all of which belong to the subset of linearly separable functions.<br class="title-page-name"/>
In order to achieve a wider range of functions, intermediate transmissions must be introduced into the perceptron between the input layer and the output layer, allowing for some kind of internal representation of the input. The resulting perceptron is called MLP.</p>
<p class="calibre2">We have already seen this as feed forward networks in <a href="part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 1</a>, <em class="calibre14">Neural Network and Artificial Intelligence Concepts</em>. M<span>LP consists of at least three layers of nodes: input, hidden, and output nodes. Except for the input nodes, each node is a neuron using a non-linear activation function</span><span>. MLP uses a supervised learning technique and back propagation for training. The multiple layers and non-linear nature distinguishes MLP from simple perceptrons. MLP is specifically used when the data is not linearly separable.</span></p>
<p class="calibre2">For example, an MLP, such as that shown in the following figure, is able to realize the <strong class="calibre1">XOR</strong> function, which we have previously seen cannot be achieved through a simple perceptron:</p>
<div class="cdpaligncenter"><img class="image-border24" src="../images/00091.jpeg"/></div>
<p class="calibre2"><strong class="calibre1">XOR</strong> is achieved using a three layer network and is a combination of <strong class="calibre1">OR</strong> and <strong class="calibre1">AND</strong> perceptrons. The output layer contains one neuron which gives the <strong class="calibre1">XOR</strong> output. A configuration of this kind allows the two neurons to specialize each on a particular logic function. For example, in the case of <strong class="calibre1">XOR,</strong> the two neurons can respectively carry out the <strong class="calibre1">AND</strong> and <strong class="calibre1">OR</strong> logic functions.</p>
<div class="packt_tip">The term MLP does not refer to a single perceptron that has multiple layers. Rather, it contains many perceptrons that are organized into layers. An alternative is an MLP network.</div>
<p class="calibre2">Applications of MLP are:</p>
<ul class="calibre16">
<li class="calibre17">MLPs are extremely useful for complex problems in research.</li>
<li class="calibre17">MLPs are universal function approximators and they can <span>be used to create mathematical models by regression analysis.</span> <span><span>MLPs also make good classifier algorithms.</span></span></li>
<li class="calibre17">MLPs are used in diverse fields, such as speech recognition, image recognition, and language translation. They form the basis for deep learning.</li>
</ul>
<p class="calibre2">We will now implement an MLP using the R package SNNS.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">MLP R implementation using RSNNS</h1>
                
            
            <article>
                
<p class="calibre2">The package <kbd class="calibre13">RSNNS</kbd> is taken from CRAN for this example of <kbd class="calibre13">mlp()</kbd> model build. The SNNS is a library written in C++ and contains many standard implementations of neural networks. This <kbd class="calibre13">RSNNS</kbd> package wraps the SNNS functionality to make it available from within R. Using the <kbd class="calibre13">RSNNS</kbd> low-level interface, all the algorithmic functionality and flexibility of SNNS can be accessed. The package contains a high-level interface for most commonly used neural network topologies and learning algorithms, which integrate seamlessly into R. A brief description of the <span><kbd class="calibre13">RSNNS</kbd></span> package, extracted from the official documentation, is shown in the following table:</p>
<table class="calibre40">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">RSNNS package</strong></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Description</strong>:</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2">The SNNS is a library containing many standard implementations of neural networks. This package wraps the SNNS functionality to make it available from within R. Using the <kbd class="calibre13">RSNNS</kbd> low-level interface, all the algorithmic functionality and flexibility of SNNS can be accessed. Furthermore, the package contains a convenient high-level interface, so that the most common neural network topologies and learning algorithms integrate seamlessly into R.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Details</strong>:</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><span>Package:</span> <kbd class="calibre13">RSNNS</kbd><br class="title-page-name"/>
<span>Type: Package</span><br class="title-page-name"/>
<span>Version: 0.4-9</span><br class="title-page-name"/>
<span>Date: 2016-12-16</span><br class="title-page-name"/>
<span>License: LGPL (&gt;=2)</span></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Authors</strong>:</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><em class="calibre14">Christoph Bergmeir</em><br class="title-page-name"/>
<em class="calibre14">José M. Benítez</em></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Usage</strong>:</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">mlp(x, y,</kbd><br class="title-page-name"/>
<kbd class="calibre13">size = c(5),</kbd><br class="title-page-name"/>
<kbd class="calibre13">maxit = 100,</kbd><br class="title-page-name"/>
<kbd class="calibre13">initFunc = "Randomize_Weights",</kbd><br class="title-page-name"/>
<kbd class="calibre13">initFuncParams = c(-0.3, 0.3),</kbd><br class="title-page-name"/>
<kbd class="calibre13">learnFunc = "Std_Backpropagation",</kbd><br class="title-page-name"/>
<kbd class="calibre13">learnFuncParams = c(0.2, 0),</kbd><br class="title-page-name"/>
<kbd class="calibre13">updateFunc = "Topological_Order",</kbd><br class="title-page-name"/>
<kbd class="calibre13">updateFuncParams = c(0),</kbd><br class="title-page-name"/>
<kbd class="calibre13">hiddenActFunc = "Act_Logistic",</kbd><br class="title-page-name"/>
<kbd class="calibre13">shufflePatterns = TRUE,</kbd><br class="title-page-name"/>
<kbd class="calibre13">linOut = FALSE,</kbd><br class="title-page-name"/>
<kbd class="calibre13">outputActFunc = if (linOut) "Act_Identity" else "Act_Logistic",</kbd><br class="title-page-name"/>
<kbd class="calibre13">inputsTest = NULL,</kbd><br class="title-page-name"/>
<kbd class="calibre13">targetsTest = NULL,</kbd><br class="title-page-name"/>
<kbd class="calibre13">pruneFunc = NULL,</kbd><br class="title-page-name"/>
<kbd class="calibre13">pruneFuncParams = NULL, ...)</kbd></td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">We use the <kbd class="calibre13">mlp()</kbd> function which creates an MLP and trains it. Training is usually performed by backpropagation.</p>
<p class="calibre2">The most commonly used parameters are listed in the following table:</p>
<table class="calibre73">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">x</kbd></p>
</td>
<td class="calibre8">A matrix with training inputs for the network</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">y</kbd></td>
<td class="calibre8">The corresponding targets values</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">size</kbd></td>
<td class="calibre8">Number of units in the hidden layers</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">maxit</kbd></td>
<td class="calibre8">Maximum iterations to learn</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">hiddenActFunc</kbd></td>
<td class="calibre8">The activation function of all hidden units</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">outputActFunc</kbd></td>
<td class="calibre8">
<p class="calibre2">The activation function of all output units</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">inputsTest</kbd></td>
<td class="calibre8">
<p class="calibre2">A matrix with inputs to test the network</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">targetsTest</kbd></td>
<td class="calibre8">
<p class="calibre2">The corresponding targets for the test input</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2"><span>Let's see the code for building a SNNS MLP using the full Iris dataset:<br class="title-page-name"/></span></p>
<pre class="calibre24"><strong class="calibre1">###################################################################</strong><br class="title-page-name"/><strong class="calibre1">###Chapter 4 - Introduction to Neural Networks - using R ##########</strong><br class="title-page-name"/><strong class="calibre1">###Simple RSNNS implementation function in R - iris dataset #######</strong><br class="title-page-name"/><strong class="calibre1">###################################################################</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">data(iris)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">library("RSNNS")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">iris = iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">irisValues = iris[,1:4]</strong><br class="title-page-name"/><strong class="calibre1">irisTargets = decodeClassLabels(iris[,5])</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">iris = splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)</strong><br class="title-page-name"/><strong class="calibre1">iris = normTrainingAndTestSet(iris)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">model = mlp(iris$inputsTrain, </strong><br class="title-page-name"/><strong class="calibre1">       iris$targetsTrain, </strong><br class="title-page-name"/><strong class="calibre1">       size=5, </strong><br class="title-page-name"/><strong class="calibre1">       learnFuncParams=c(0.1),</strong><br class="title-page-name"/><strong class="calibre1">       maxit=50, </strong><br class="title-page-name"/><strong class="calibre1">       inputsTest=iris$inputsTest, </strong><br class="title-page-name"/><strong class="calibre1">       targetsTest=iris$targetsTest)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">summary(model)</strong><br class="title-page-name"/><strong class="calibre1">weightMatrix(model)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">par(mfrow=c(2,2))</strong><br class="title-page-name"/><strong class="calibre1">plotIterativeError(model)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">predictions = predict(model,iris$inputsTest)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">plotRegressionError(predictions[,2], iris$targetsTest[,2])</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">confusionMatrix(iris$targetsTrain,fitted.values(model))</strong><br class="title-page-name"/><strong class="calibre1">confusionMatrix(iris$targetsTest,predictions)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">par(mfrow=c(1,2))</strong><br class="title-page-name"/><strong class="calibre1">plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])</strong><br class="title-page-name"/><strong class="calibre1">plotROC(predictions[,2], iris$targetsTest[,2])</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">confusionMatrix(iris$targetsTrain, </strong><br class="title-page-name"/><strong class="calibre1">       encodeClassLabels(fitted.values(model),</strong><br class="title-page-name"/><strong class="calibre1">       method="402040", </strong><br class="title-page-name"/><strong class="calibre1">       l=0.4, </strong><br class="title-page-name"/><strong class="calibre1">       h=0.6))<br class="title-page-name"/></strong><strong class="calibre1">###################################################################</strong><strong class="calibre1"> </strong><strong class="calibre1"><br class="title-page-name"/></strong></pre>
<p class="calibre2">Let's go through the code step-by-step.</p>
<p class="calibre2"><span>This command loads the iris dataset, which is contained in the datasets library, and saves it in a given dataframe. Considering the many times we have used it, I do not think it's necessary to add anything. This loads the <kbd class="calibre13">RSNNS</kbd> library for the program:</span></p>
<pre class="calibre24"><br class="title-page-name"/><strong class="calibre1"> install.packages("RSNNS") </strong><br class="title-page-name"/><strong class="calibre1"> library("RSNNS")</strong></pre>
<div class="packt_tip"><span class="calibre62">Remember, to install a library that is not present in the initial distribution of R, you must use the</span> <kbd class="calibre61">install.package</kbd> <span class="calibre62">function. This is the main function to install packages. It takes a vector of names and a destination library, downloads the packages from the repositories and installs them.</span></div>
<p class="calibre2"><span>In our case, we must use the command</span> <kbd class="calibre13">install.packages("RSNNS")</kbd><span>.</span> The install package is required only the first time, to install the <kbd class="calibre13">RSNNS</kbd> package from CRAN.</p>
<pre class="calibre24"><strong class="calibre1">iris = iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]</strong></pre>
<p class="calibre2">In this preceding line, the <kbd class="calibre13">iris</kbd> dataset is shuffled within rows. This operation makes the order of the rows in the dataset random. In fact, in the original dataset, the observations are ordered by floral species: the first <em class="calibre14">50</em> occurrences of the <kbd class="calibre13">setosa</kbd> species, followed by the <em class="calibre14">50</em> occurrences of the <kbd class="calibre13">versicolor</kbd> species, and finally the <em class="calibre14">50</em> occurrences of the virginica species. After this operation, the rows happen randomly. To verify this, we print the first <kbd class="calibre13">20</kbd> lines of the modified dataset:</p>
<pre class="calibre24"><strong class="calibre1">&gt; head(iris, n=20)</strong><br class="title-page-name"/><strong class="calibre1">    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species</strong><br class="title-page-name"/><strong class="calibre1">75           6.4         2.9          4.3         1.3 versicolor</strong><br class="title-page-name"/><strong class="calibre1">112          6.4         2.7          5.3         1.9  virginica</strong><br class="title-page-name"/><strong class="calibre1">54           5.5         2.3          4.0         1.3 versicolor</strong><br class="title-page-name"/><strong class="calibre1">36           5.0         3.2          1.2         0.2     setosa</strong><br class="title-page-name"/><strong class="calibre1">14           4.3         3.0          1.1         0.1     setosa</strong><br class="title-page-name"/><strong class="calibre1">115          5.8         2.8          5.1         2.4  virginica</strong><br class="title-page-name"/><strong class="calibre1">125          6.7         3.3          5.7         2.1  virginica</strong><br class="title-page-name"/><strong class="calibre1">27           5.0         3.4          1.6         0.4     setosa</strong><br class="title-page-name"/><strong class="calibre1">8            5.0         3.4          1.5         0.2     setosa</strong><br class="title-page-name"/><strong class="calibre1">41           5.0         3.5          1.3         0.3     setosa</strong><br class="title-page-name"/><strong class="calibre1">85           5.4         3.0          4.5         1.5 versicolor</strong><br class="title-page-name"/><strong class="calibre1">64           6.1         2.9          4.7         1.4 versicolor</strong><br class="title-page-name"/><strong class="calibre1">108          7.3         2.9          6.3         1.8  virginica</strong><br class="title-page-name"/><strong class="calibre1">65           5.6         2.9          3.6         1.3 versicolor</strong><br class="title-page-name"/><strong class="calibre1">66           6.7         3.1          4.4         1.4 versicolor</strong><br class="title-page-name"/><strong class="calibre1">98           6.2         2.9          4.3         1.3 versicolor</strong><br class="title-page-name"/><strong class="calibre1">39           4.4         3.0          1.3         0.2     setosa</strong><br class="title-page-name"/><strong class="calibre1">84           6.0         2.7          5.1         1.6 versicolor</strong><br class="title-page-name"/><strong class="calibre1">2            4.9         3.0          1.4         0.2     setosa</strong><br class="title-page-name"/><strong class="calibre1">142          6.9         3.1          5.1         2.3  virginica</strong></pre>
<p class="calibre2">The numbers in the first column are the row numbers of the original dataset. How can we notice the shuffling flawed perfectly. To compare with the original sequence, see the previous example:</p>
<pre class="calibre24"><strong class="calibre1">irisValues = iris[,1:4]</strong><br class="title-page-name"/><strong class="calibre1">irisTargets = decodeClassLabels(iris[,5])</strong></pre>
<p class="calibre2">The independent and target variables are set up and assigned to <kbd class="calibre13">irisValues</kbd> and <kbd class="calibre13">irisTargets</kbd> respectively:</p>
<pre class="calibre24"><strong class="calibre1">iris = splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)</strong><br class="title-page-name"/><strong class="calibre1">iris = normTrainingAndTestSet(iris)</strong></pre>
<p class="calibre2">In the first line, the training data and the test data is split up through the <kbd class="calibre13">splitForTrainingAndTest()</kbd> function. This function splits the input and target values to a training and a testing set. A test set is taken from the end of the data. If the data is to be shuffled, this should be done before calling this function. In particular, the data is split as follows: <em class="calibre14">85</em> percent for training and <em class="calibre14">15</em> percent for testing. In the second line, the data is normalized. To do this, the <kbd class="calibre13">normTrainingAndTestSet()</kbd> function is used. This function normalizes the training and test set in the following way: The <kbd class="calibre13">inputsTrain</kbd> member is normalized using <kbd class="calibre13">normalizeData</kbd> with the parameters given in type. The normalization parameters obtained during this normalization are then used to normalize the <kbd class="calibre13">inputsTest</kbd> member. If the <kbd class="calibre13">dontNormTargets</kbd> argument is not set, then the targets are normalized in the same way:</p>
<pre class="calibre24"><strong class="calibre1">model = mlp(iris$inputsTrain, </strong><br class="title-page-name"/><strong class="calibre1">   iris$targetsTrain, </strong><br class="title-page-name"/><strong class="calibre1">   size=5, </strong><br class="title-page-name"/><strong class="calibre1">   learnFuncParams=c(0.1),</strong><br class="title-page-name"/><strong class="calibre1">   maxit=50, </strong><br class="title-page-name"/><strong class="calibre1">   inputsTest=iris$inputsTest, </strong><br class="title-page-name"/><strong class="calibre1">   targetsTest=iris$targetsTest)</strong></pre>
<p class="calibre2">The <kbd class="calibre13">mlp()</kbd> function is called with the training dataset to build the model. This function creates an MLP and trains it. MLPs are fully connected feed-forward networks, and probably the most common network architecture in use. Training is usually performed by error backpropagation or a related procedure. The test dataset is also passed to provide the test results:</p>
<pre class="calibre24"><strong class="calibre1">summary(model)</strong><br class="title-page-name"/><strong class="calibre1">weightMatrix(model)</strong></pre>
<p class="calibre2">These lines of code allow us to extract useful information from the newly created model. The <kbd class="calibre13">summary()</kbd> function prints out a summary of the network. The printed information can be either all information of the network in the original SNNS file format, or the information given by <kbd class="calibre13">extractNetInfo</kbd>. This behavior is controlled with the parameter <kbd class="calibre13">origSnnsFormat</kbd>, while the <kbd class="calibre13">weightMatrix()</kbd> function extracts the weight matrix of an <kbd class="calibre13">rsnns</kbd> object. The following figure shows a screenshot of the summary results:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00092.jpeg"/></div>
<p class="calibre2">Now we measure the performance of the algorithm in model training:</p>
<pre class="calibre24"><strong class="calibre1">plotIterativeError(model)</strong></pre>
<p class="calibre2">The <kbd class="calibre13">plotIterativeError()</kbd> function plots the iterative training and test error of the net of the model. The results are shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00093.jpeg"/></div>
<p class="calibre2">The previous figure showns the iterative fit error as a black line and the iterative test error as a red line. As can be seen, both lines have a strongly decreasing trend, demonstrating that the algorithm quickly converges.<br class="title-page-name"/>
After properly training the model, it is time to use it to make predictions:</p>
<pre class="calibre24"><strong class="calibre1">predictions = predict(model,iris$inputsTest)</strong></pre>
<p class="calibre2">In this case, we have used the <kbd class="calibre13">predict()</kbd> function. This is a generic function for predictions from the results of various model fitting functions. The function invokes particular methods which depend on the class of the first argument. We have both the predictions and the actual data; we just have to compare them through the regression error calculus:</p>
<pre class="calibre24"><strong class="calibre1">plotRegressionError(predictions[,2], iris$targetsTest[,2])</strong></pre>
<p class="calibre2">To plot the regression error, we have used the <kbd class="calibre13">plotRegressionError()</kbd> function. This function shows target values on the <em class="calibre14">X</em> axis and fitted/predicted values on the <em class="calibre14">Y</em> axis. The optimal fit would yield a line through zero with gradient one. This optimal line is shown in black in the following figure. A linear fit to the actual data is shown in red. The following figure shows the <span>regression error for the model which we previously trained:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00094.gif"/></div>
<p class="calibre2"><span>Let's now evaluate the model performance in predicting data by computing the confusion matrix:</span></p>
<pre class="calibre24"><strong class="calibre1">confusionMatrix(iris$targetsTrain,fitted.values(model))</strong><br class="title-page-name"/><strong class="calibre1">confusionMatrix(iris$targetsTest,predictions)</strong></pre>
<p class="calibre2">To <span>compute the confusion matrix, we have used the <kbd class="calibre13">confusionMatrix()</kbd> function.</span></p>
<div class="packt_tip">Remember, the confusion matrix shows how many times a pattern with the real class <kbd class="calibre61">x</kbd> was classified as class <kbd class="calibre61">y</kbd>. A perfect method should result in a diagonal matrix. All values not on the diagonal are errors of the method.</div>
<p class="calibre2">In the first line of the code, we calculated the confusion matrix for the data used in the training (which is <em class="calibre14">85</em> percent of the data), while in the second line, we calculated the confusion matrix for the data used in the test (which is the remaining <em class="calibre14">15</em> percent of data). The results are as follows:</p>
<pre class="calibre24"><strong class="calibre1">&gt; confusionMatrix(iris$targetsTrain,fitted.values(model))</strong><br class="title-page-name"/><strong class="calibre1">       predictions</strong><br class="title-page-name"/><strong class="calibre1">targets  1  2  3</strong><br class="title-page-name"/><strong class="calibre1">      1 45  0  0</strong><br class="title-page-name"/><strong class="calibre1">      2  0 34  3</strong><br class="title-page-name"/><strong class="calibre1">      3  0  1 44</strong><br class="title-page-name"/><strong class="calibre1">&gt; confusionMatrix(iris$targetsTest,predictions)</strong><br class="title-page-name"/><strong class="calibre1">       predictions</strong><br class="title-page-name"/><strong class="calibre1">targets  1  2  3</strong><br class="title-page-name"/><strong class="calibre1">      1  5  0  0</strong><br class="title-page-name"/><strong class="calibre1">      2  0 13  0</strong><br class="title-page-name"/><strong class="calibre1">      3  0  0  5</strong></pre>
<p class="calibre2">As can be seen, there were four mistakes in the training phase, that only concerned the <kbd class="calibre13">versicolor</kbd> and <kbd class="calibre13">virginica</kbd> species. Remember, we obtained <span>the same result</span> in the example presented in <a href="part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 3</a>, <em class="calibre14">Deep Learning Using Multilayer Neural Networks</em>. In the test, however, we did not make any mistakes. I would say very good results, although the processed data is actually small. We graphically evaluate these results:</p>
<pre class="calibre24"><strong class="calibre1">par(mfrow=c(1,2))</strong><br class="title-page-name"/><strong class="calibre1">plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])</strong><br class="title-page-name"/><strong class="calibre1">plotROC(predictions[,2], iris$targetsTest[,2])</strong></pre>
<p class="calibre2">To evaluate network performance, we have plotted the receiver operating characteristic. The previous commands plot the ROC for both the phases (training and testing).<br class="title-page-name"/>
The ROC is a metric used to check the quality of classifiers. For each class of a classifier, ROC applies threshold values across the interval <em class="calibre14">[0,1]</em> to outputs. The ROC curve is a plot of the TPR versus the FPR, as the threshold is varied. A perfect test would show points in the upper-left corner, with <em class="calibre14">100</em> percent sensitivity and <em class="calibre14">100</em> percent specificity. The better the lines approach the upper-left corner, the better is the network performance. The following figure shows the ROC curves <span>for both the phases (training to the left and test to the right):</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00095.gif"/></div>
<p class="calibre2">As already mentioned, in the training phase there were errors that are absent in the test.</p>
<div class="packt_tip">Note, we used the <kbd class="calibre61">par()</kbd> function to display both the charts in a single window. In it we have set to display the graphs as a matrix with a row and two columns.</div>
<p class="calibre2">There is no <kbd class="calibre13">plot</kbd> function within <kbd class="calibre13">RSNNS</kbd>, hence we use a <kbd class="calibre13">plot</kbd> function from GitHub to plot the following MLP for the neural network we just built. There are three classes of output and there are four input nodes:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00096.jpeg"/></div>
<p class="calibre2">We have seen a simple implementation of an <kbd class="calibre13">iris</kbd> dataset neural network using <kbd class="calibre13">RSNNS</kbd>. The same <kbd class="calibre13">mlp()</kbd> function can be used for any MLP neural network architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we introduced you the concept of perceptrons, which are the basic building blocks of a neural network. We also saw multi-layer perceptrons and an implementation using <kbd class="calibre13">RSNNS</kbd>. The simple perceptron is useful only for a linear separation problem and cannot be used where the output data is not linearly separable. These limits are exceeded by the use of the MLP algorithm.</p>
<p class="calibre2">We understood the basic concepts of perceptron and how they are used in neural network algorithms. We discovered the linear separable classifier and the functions this concept applies to. We learned a simple perceptron implementation function in R environment and then we learnt how to train and model an MLP.</p>
<p class="calibre2">In the next chapter, we will understand how to train, test, and evaluate a dataset using the neural network model. We will learn how to visualize the neural network model in R environment. We will cover concepts like early stopping, avoiding overfitting, generalization of neural network, and scaling of neural network parameters.</p>


            </article>

            
        </section>
    </body></html>