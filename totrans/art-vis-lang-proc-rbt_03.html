<html><head></head><body>
		<div id="_idContainer070" class="Content">
			<h1 id="_idParaDest-53"><em class="italics"><a id="_idTextAnchor071"/>Chapter 3</em></h1>
		</div>
		<div id="_idContainer071" class="Content">
			<h1 id="_idParaDest-54"><a id="_idTextAnchor072"/>Fundamentals of Natural Language Processing</h1>
		</div>
		<div id="_idContainer072" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Classify different areas of natural language processing</li>
				<li class="bullets">Analyze basic natural language processing libraries in Python</li>
				<li class="bullets">Predict the topics in a set of texts</li>
				<li class="bullets">Develop a simple language model</li>
			</ul>
			<p>This chapter covers different fundamentals and areas of natural language processing, along with its libraries in Python.</p>
		</div>
		<div id="_idContainer107" class="Content">
			<h2 id="_idParaDest-55"><a id="_idTextAnchor073"/>Introduction</h2>
			<p><strong class="keyword">Natural Language Processing</strong> (<strong class="keyword">NLP</strong>) is an area of <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>) with the goal of enabling computers to understand and manipulate human language in order to perform useful tasks. Within this area, there are two sections: <strong class="keyword">Natural Language Understanding</strong> (<strong class="keyword">NLU</strong>) and <strong class="keyword">Natural Language Generation</strong> (<strong class="keyword">NLG</strong>).</p>
			<p>In recent years, AI has changed the way machines interact with humans. AI helps people solve complex equations by performing tasks such as recommending a movie according to your tastes (recommender systems). Thanks to the high performance of GPUs and the huge amount of data available, it's possible to create intelligent systems that are capable of learning and behaving like humans.</p>
			<p>There are many libraries that aim to help with the creation of these systems. In this chapter, we will review the most famous Python libraries to extract and clean information from raw text. You may consider this task complex, but a complete understanding and interpretation of the language is a difficult task in itself. For example, the sentence "Cristiano Ronaldo scores three goals" would be hard for a machine to understand because it would not know who Cristiano Ronaldo is or what is meant by the number of goals. </p>
			<p>One of the most popular topics in NLP is <strong class="bold">Question Answering</strong> (<strong class="bold">QA</strong>). This discipline also consists of <strong class="bold">Information Retrieva</strong>l (<strong class="bold">IR</strong>). These systems construct answers by querying a database for knowledge or information, but they are capable of extracting answers from a collection of natural language documents. That is how a search engine such as Google works. </p>
			<p>In the industry today, NLP is becoming more and more popular. The latest NLP trends are online advertisement matching, sentiment analysis, automated translation, and chatbots.</p>
			<p>Conversational agents, popularly known as chatbots, are the next challenge for NLP. They can hold real conversation and many companies use them to get feedback about their products or to create a new advertising campaign, by analyzing the behavior and opinions of clients through the chatbot. Virtual assistants are a great example of NLP and they have already been introduced to the market. The most famous are Siri, Amazon's Alexa, and Google Home. In this book, we will create a chatbot to control a virtual robot that is able to understand what we want the robot to do.</p>
			<h3 id="_idParaDest-56"><a id="_idTextAnchor074"/>Natural Language Processing</h3>
			<p>As mentioned before, NLP is an AI field that takes care of understanding and processing human language. NLP is located at the intersection between AI, computer science, and linguistics. The main aim of this area is to make computers understand statements or words written in human languages:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/C13550_03_01.jpg" alt="Figure 3.1: Representation of NLP within AI, linguistics, and computer science"/>
				</div>
			</div>
			<h6>Figure 3.1: Representation of NLP within AI, linguistics, and computer science</h6>
			<p>Linguistic science focuses on the study of human language, trying to characterize and explain the different approaches of language. </p>
			<p>A language can be defined as a set of rules and a set of symbols. Symbols are combined and used to broadcast information and are structured by rules. Human language is special. We cannot simply picture it as naturally formed symbols and rules; depending on the context, the meaning of words can change.</p>
			<p>NLP is becoming more popular and can solve many difficult problems. The amount of text data available is very large, and it is impossible for a human to process all that data. In Wikipedia, the average number of new articles per day is 547, and in total, there are more than 5,000,000 articles. As you can imagine, a human cannot read all that information. </p>
			<p>There are three challenges faced by NLP. The first challenge is collecting all the data, the second is classifying it, and the final one is extracting the relevant information. </p>
			<p>NLP solves many tedious tasks, such as spam detection in emails, <strong class="bold">part-of-speech</strong> (<strong class="bold">POS</strong>) tagging, and named entity recognition. With deep learning, NLP can also solve voice-to-text problems. Although NLP shows a lot of power, there are some cases such as working without having a good solution from the dialog between a human and a machine, QA systems summarization and machine translation.</p>
			<h3 id="_idParaDest-57"><a id="_idTextAnchor075"/>Parts of NLP</h3>
			<p>As mentioned before, NLP can be divided into two groups: NLU and NLG.</p>
			<p><strong class="bold">Natural Language Understanding</strong></p>
			<p>This section of NLP relates to the understanding and analysis of human language. It focusses on the comprehension of text data, and processing it to extract relevant information. NLU provides direct human-computer interaction and performs tasks related to the comprehension of language.</p>
			<p>NLU covers the hardest of AI challenges, and that is the interpretation of text. The main challenge of NLU is understanding dialog. </p>
			<h4>Note</h4>
			<p class="callout"> NLP uses a set of methods for generating, processing, and understanding language. NLU uses functions to understand the meaning of a text. </p>
			<p>Previously, a conversation was represented as a tree, but this approach cannot cover many dialog cases. To cover more cases, more trees would be required, one for each context of the conversation, leading to the repeating of many sentences:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/C13550_03_02.jpg" alt="Figure 3.2: Representation of a dialogue using trees"/>
				</div>
			</div>
			<h6>Figure 3.2: Representation of a dialogue using trees</h6>
			<p>This approach is outdated and inefficient because is based on fixed rules; it's essentially an if-else structure. But now, NLU has contributed another approach. A conversation can be represented as a Venn diagram where each set is a context of the conversation:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/C13550_03_03.jpg" alt="Figure 3.3: Representation of a conversation using a Venn diagram"/>
				</div>
			</div>
			<h6>Figure 3.3: Representation of a conversation using a Venn diagram</h6>
			<p>As you can see in the previous figures, the NLU approach improves the structure of understanding a conversation, because it is not a fixed structure that contains if-else conditions. The main goal of NLU is to interpret the meaning of human language and deal with the contexts of a conversation, solving ambiguities and managing data. </p>
			<p><strong class="bold">Natural Language Generation</strong></p>
			<p>NLG is the process of producing phrases, sentences, and paragraphs with meaning and structure. It is an area of NLP that does not deal with understanding text. </p>
			<p>To generate natural language, NLG methods need relevant data.</p>
			<p>NLG has three components:</p>
			<ul>
				<li><strong class="bold">Generator</strong>: Responsible for including the text within an intent to have it related with the context of the situation</li>
				<li><strong class="bold">Components and levels of representations</strong>: Gives structure to the generated text</li>
				<li><strong class="bold">Application</strong>: Saves relevant data from the conversation to follow a logical thread</li>
			</ul>
			<p>Generated text must be in a human-readable format. The advantages of NLG are that you can make your data accessible and you can create summaries of reports rapidly.</p>
			<h3 id="_idParaDest-58"><a id="_idTextAnchor076"/>Levels of NLP</h3>
			<p>Human language has different levels of representation. Each representation level is more complex than the previous level. As we ascend through the levels, it gets more difficult to understand the language.</p>
			<p>The two first levels depend on the data type (audio or text), in which we have the following:</p>
			<ul>
				<li><strong class="bold">Phonological analysis</strong>: If the data is speech, first, we need to analyze the audio to have sentences.</li>
				<li><strong class="bold">OCR/tokenization</strong>: If we have text, we need to recognize the characters and form words using computer vision (OCR). If not, we will need to tokenize the text (that is, split the sentence into units of text).<h4>Note</h4><p class="callout">The OCR process is the identification of characters in an image. Once it generates words, they are processed as raw text. </p></li>
				<li><strong class="bold">Morphological analysis</strong>: Focused on the words of a sentence and analyzing its morphemes.</li>
				<li><strong class="bold">Syntactic analysis</strong>: This level focuses on the grammatical structure of a sentence. That means understanding different parts of a sentence, such as the subject or the predicate.</li>
				<li><strong class="bold">Semantic representation</strong>: A program does not understand a single word; it can know the meaning of a word by knowing how the word is used in a sentence. For example, "cat" and "dog" could mean the same for an algorithm because they can be used in the same way. Understanding sentences in this way is called word-level meaning.</li>
				<li><strong class="bold">Discourse processing</strong>: Analyzing and identifying connected sentences in a text and their relationships. By doing this, an algorithm could understand what the topic of the text is.</li>
			</ul>
			<p>NLP shows great potential in today's industry, but there are some exceptions. Using deep learning concepts, we can work with some of these exceptions to get better results. Some of these problems will be reviewed in <em class="italics">Chapter 4</em>, <em class="italics">Neural Networks with NLP</em>. The advantage of text processing techniques and the improvement of recurrent neural networks are the reasons why NLP is becoming increasingly important.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor077"/>NLP in Python</h2>
			<p>Python has become very popular in recent years, by combining the power of general-purpose programming languages with the use of specific domain languages, such as MATLAB and R (designed for mathematics and statistics). It has different libraries for data loading, visualization, NLP, image processing, statistics, and more. Python has the most powerful libraries for text processing and machine learning algorithms. </p>
			<h3 id="_idParaDest-60"><a id="_idTextAnchor078"/>Natural Language Toolkit (NLTK)</h3>
			<p>NLTK is the most common kit of tools for working with human language data in Python. It includes a set of libraries and programs for processing natural language and statistics. NLTK is commonly used as a learning tool and for carrying out research.</p>
			<p>This library provides interfaces and methods for over 50 corpora and lexical resources. NLTK is capable of classifying text and performing other functions, such as tokenization, stemming (extracting the stem of a word), tagging (identifying the tag of a word, such as person, city…), and parsing (syntax analysis).</p>
			<h3 id="_idParaDest-61"><a id="_idTextAnchor079"/>Exercise 10: Introduction to NLTK</h3>
			<p>In this exercise, we will review the most basic concepts about the NLTK library. As we said before, this library is one of the most widely used tools for NLP. It can be used to analyze and study text, disregarding useless information. These techniques can be applied to any text data, for example, to extract the most important keywords from a set of tweets or to analyze an article in a newspaper: </p>
			<h4>Note </h4>
			<p class="callout">All the exercises in this chapter will be executed in Google Colab.</p>
			<ol>
				<li>Open up your Google Colab interface.</li>
				<li>Create a folder for the book.</li>
				<li>Here, we are going to process a sentence with basic methods of the NLTK library. First of all, let's import the necessary methods (<strong class="inline">stopwords</strong>, <strong class="inline">word_tokenize</strong>, and <strong class="inline">sent_tokenize</strong>):<p class="snippet">from nltk.corpus import stopwords</p><p class="snippet">from nltk.tokenize import word_tokenize</p><p class="snippet">from nltk.tokenize import sent_tokenize </p><p class="snippet">import nltk</p><p class="snippet">nltk.download('punkt')</p></li>
				<li>Now we create a sentence and apply the methods:<p class="snippet">example_sentence = "This course is great. I'm going to learn deep learning; Artificial Intelligence is amazing and I love robotics..."</p><p class="snippet">sent_tokenize(example_sentence) # Divide the text into sentences</p><div id="_idContainer076" class="IMG---Figure"><img src="image/C13550_03_04.jpg" alt=""/></div><h6>Figure 3.4: Sentence divided into a sub-sentence</h6><p class="snippet">word_tokenize(example_sentence)</p><div id="_idContainer077" class="IMG---Figure"><img src="image/C13550_03_05.jpg" alt="Fig 3.5: Tokenizing a sentence into words"/></div><h6>Figure 3.5: Tokenizing a sentence into words</h6><h4>Note</h4><p class="callout"><strong class="inline">Sent_tokenize</strong> returns a list of different sentences. One of the disadvantages of NLTK is that <strong class="inline">sent_tokenize</strong> does not analyze the semantic structure of the whole text; it just splits the text by the dots.</p></li>
				<li>With the sentence tokenized sentence by words, let's subtract the stop words. The stop words are a set of words without relevant information about the text. Before using <strong class="inline">stopwords</strong>, we will need to download it:<p class="snippet">nltk.download('stopwords')</p></li>
				<li>Now, we set the language of our <strong class="inline">stopwords</strong> as English:<p class="snippet">stop_words = set(stopwords.words("english")) </p><p class="snippet">print(stop_words)</p><p>The output is as follows:</p><div id="_idContainer078" class="IMG---Figure"><img src="image/C13550_03_06.jpg" alt="Figure 3.6: Stopwords set as English"/></div><h6>Figure 3.6: Stopwords set as English</h6></li>
				<li>Process the sentence, deleting <strong class="inline">stopwords</strong>:<p class="snippet">print(word_tokenize(example_sentence))</p><p class="snippet">print([w for w in word_tokenize(example_sentence.lower()) if w not in stop_words]) </p><p>The output is as follows:</p><div id="_idContainer079" class="IMG---Figure"><img src="image/C13550_03_07.jpg" alt="Figure 3.7: Sentence without stop words"/></div><h6>Figure 3.7: Sentence without stop words</h6></li>
				<li>We can now modify the set of <strong class="inline">stopwords</strong> and check the output:<p class="snippet">stop_words = stop_words - set(('this', 'i', 'and')) </p><p class="snippet">print([w for w in word_tokenize(example_sentence.lower()) if w not in stop_words]) </p><div id="_idContainer080" class="IMG---Figure"><img src="image/C13550_03_08.jpg" alt="Figure 3.8: Setting stop words"/></div><h6>Figure 3.8: Setting stop words</h6></li>
				<li>Stemmers remove morphological affixes from words. Let's define a stemmer and process our sentence. <strong class="inline">Porter stemmer</strong> is an algorithm for performing this task:<p class="snippet">from nltk.stem.porter import *    # importing a stemmer</p><p class="snippet">stemmer = PorterStemmer()    # importing a stemmer     </p><p class="snippet">print([stemmer.stem(w) for w in  word_tokenize(example_sentence)])</p><p>The output is as follows:</p><div id="_idContainer081" class="IMG---Figure"><img src="image/C13550_03_09.jpg" alt="Figure 3.9: Setting stop words"/></div><h6>Figure 3.9: Setting stop words</h6></li>
				<li>Finally, let's classify each word by its type. To do this, we will use a POS tagger:<p class="snippet">nltk.download('averaged_perceptron_tagger')</p><p class="snippet">t = nltk.pos_tag(word_tokenize(example_sentence)) #words with each tag</p><p class="snippet">t</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/C13550_03_10.jpg" alt="Figure 3.10: POS tagger"/>
				</div>
			</div>
			<h6>Figure 3.10: POS tagger</h6>
			<h4>Note</h4>
			<p class="callout">The averaged perceptron tagger is an algorithm trained to predict the category of a word.</p>
			<p>As you may have noticed in this exercise, NLTK can easily process a sentence. Also, it can analyze a huge set of text documents without any problem. It supports many languages and the tokenization process is faster than that for similar libraries, and it has many methods for each NLP problem.</p>
			<h3 id="_idParaDest-62"><a id="_idTextAnchor080"/>spaCy</h3>
			<p>spaCy is another library for NLP in Python. It does look similar to NLTK, but you will see some differences in the way it works.</p>
			<p>spaCy was developed by Matt Honnibal and is designed for data scientists to clean and normalize text easily. It's the quickest library in terms of preparing text data for a machine learning model. It includes built-in word vectors and some methods for comparing the similarity between two or more texts (these methods are trained with neural networks).</p>
			<p>Its API is easy to use and more intuitive than NLTK. Often, in NLP, spaCy is compared to NumPy. It provides methods and functions for performing tokenization, lemmatization, POS tagging, NER, dependency parsing, sentence and document similarity, text classification, and more.</p>
			<p>As well as having linguistic features, it also has statistical models. This means you can predict some linguistic annotations, such as whether a word is a verb or a noun. Depending on the language you want to make predictions in, you will need to change a module. Within this section are Word2Vec models, which we will discuss in <em class="italics">Chapter 4</em>, <em class="italics">Neural Networks with NLP.</em></p>
			<p>spaCy has many advantages, as we said before, but there are some cons too; for instance, it supports only 8 languages (NLTK supports 17 languages), the tokenization process is slow (and this time-consuming process could be critical on a long corpus), and overall, it is not flexible (that is, it just provides API methods without the possibility of modifying any parameters).</p>
			<p>Before starting with the exercise, let's review the architecture of spaCy. The most important data structures of spaCy are the Doc and the Vocab. </p>
			<p>The Doc structure is the text you are loading; it is not a string. It is composed of a sequence of tokens and their annotations. The Vocab structure is a set of lookup tables, but what are lookup tables and why is the structure important? Well, a lookup table in computation is an array indexing an operation that replaces a runtime. spaCy centralizes information that is available across documents. This means that it is more efficient, as this saves memory. Without these structures, the computational speed of spaCy would be slower. </p>
			<p>However, the structure of Doc is different to Vocab because Doc is a container of data. A Doc object owns the data and is composed of a sequence of tokens or spans. There are also a few lexemes, which are related to the Vocab structure because they do not have context (unlike the token container).</p>
			<h4>Note</h4>
			<p class="callout">A lexeme is a unit of lexical meaning without having inflectional endings. The area of study for this is morphological analysis. </p>
			<p>The figure 3.11 shows us the spaCy architecture.</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/C13550_03_11.jpg" alt="Figure 3.11: spaCy architecture"/>
				</div>
			</div>
			<h6>Figure 3.11: spaCy architecture</h6>
			<p>Depending on the language model you are loading, you will have a different pipeline and a different Vocab.</p>
			<h3 id="_idParaDest-63"><a id="_idTextAnchor081"/>Exercise 11: Introduction to spaCy</h3>
			<p>In this exercise, we will do the same transformations that we performed in <em class="italics">Exercise 10</em>, <em class="italics">Introduction to NLTK</em>, and to the same sentence as in that exercise but with the spaCy API. This exercise will help you to understand and learn about the differences between these libraries:</p>
			<ol>
				<li value="1">Open up your Google Colab interface.</li>
				<li>Create a folder for the book.</li>
				<li>Then, import the package to use all its features:<p class="snippet">import spacy</p></li>
				<li>Now we are going to initialize our <strong class="inline">nlp</strong> object. This object is a part of the spaCy methods. By executing this line of code, we are loading the model inside the parenthesis:<p class="snippet">import en_core_web_sm</p><p class="snippet">nlp = spacy.load('en')</p></li>
				<li>Let's take the same sentence as in <em class="italics">Exercise 10</em>, <em class="italics">Introduction to NLTK,</em> and create the Doc container:<p class="snippet">example_sentence = "This course is great. I'm going to learn deep learning; Artificial Intelligence is amazing and I love robotics..."</p><p class="snippet">doc1 = nlp(example_sentence)</p></li>
				<li>Now, print <strong class="inline">doc1</strong>, its format, the 5th and 11th token, and a span between the 5th and the 11th token. You will see this:<p class="snippet">print("Doc structure: {}".format(doc1))</p><p class="snippet">print("Type of doc1:{}".format(type(doc1)))</p><p class="snippet">print("5th and 10th Token of the Doc: {}, {}".format(doc1[5], doc1[11]))</p><p class="snippet">print("Span between the 5th token and the 10th: {}".format(doc1[5:11]))</p><p>The output is as follows:</p><div id="_idContainer084" class="IMG---Figure"><img src="image/C13550_03_12.jpg" alt="Figure 3.12: Output of a spaCy document"/></div><h6>Figure 3.12: Output of a spaCy document</h6></li>
				<li>As we saw in Figure 3.5, documents are composed of tokens and spans. First, we are going to see the spans of <strong class="inline">doc1</strong>, and then its tokens.<p>Print the spans:</p><p class="snippet">for s in doc1.sents:</p><p class="snippet">    print(s)</p><p>The output is as follows:</p><div id="_idContainer085" class="IMG---Figure"><img src="image/C13550_03_13.jpg" alt="Figure 3.13: Printing the spans of doc1"/></div><h6>Figure 3.13: Printing the spans of doc1</h6><p>Print the tokens:</p><p class="snippet">for i in doc1:</p><p class="snippet">    print(i)</p><p>The output is as follows:</p><div id="_idContainer086" class="IMG---Figure"><img src="image/C13550_03_14.jpg" alt=""/></div><h6>Figure 3.14: Printing the tokens of doc1</h6></li>
				<li>Once we have the document divided into tokens, the stop words can be removed.<p>First, we need to import them:</p><p class="snippet">from spacy.lang.en.stop_words import STOP_WORDS</p><p class="snippet">print("Some stopwords of spaCy: {}".format(list(STOP_WORDS)[:10]))</p><p class="snippet">type(STOP_WORDS)</p><p>The output is as follows:</p><div id="_idContainer087" class="IMG---Figure"><img src="image/C13550_03_15.jpg" alt="Figure 3.15: 10 stop words in spaCy"/></div><h6>Figure 3.15: 10 stop words in spaCy</h6><p>But the token container has the <strong class="inline">is_stop</strong> attribute:</p><p class="snippet">for i in doc1[0:5]:</p><p class="snippet">    print("Token: {} | Stop word: {}".format(i, i.is_stop)</p><p>The output is as follows:</p><div id="_idContainer088" class="IMG---Figure"><img src="image/C13550_03_16.jpg" alt="Figure 3.16: The is_stop attribute of tokens&#13;&#10;"/></div><h6>Figure 3.16: The is_stop attribute of tokens</h6></li>
				<li>To add new stop words, we must modify the <strong class="inline">vocab</strong> container:<p class="snippet">nlp.vocab["This"].is_stop = True doc1[0].is_stop</p><p>The output here would be as follows:</p><p>True</p></li>
				<li>To perform speech tagging, we initialize the token container:<p class="snippet">for i in doc1[0:5]:</p><p class="snippet">    print("Token: {} | Tag: {}".format(i.text, i.pos_))</p><p>The output is as follows:</p><div id="_idContainer089" class="IMG---Figure"><img src="image/C13550_03_17.jpg" alt="Figure 3.17: The .pos_ attribute of tokens"/></div><h6>Figure 3.17: The .pos_ attribute of tokens</h6></li>
				<li>The document container has the <strong class="inline">ents</strong> attribute, with the entity of the tokens. To have more entities in our document, let's declare a new one:<p class="snippet">doc2 = nlp("I live in Madrid and I am working in Google from 10th of December.")</p><p class="snippet">for i in doc2.ents:</p><p class="snippet">    print("Word: {} | Entity: {}".format(i.text, i.label_))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/C13550_03_18.jpg" alt="Figure 3.18: The .label_ attribute of tokens"/>
				</div>
			</div>
			<h6>Figure 3.18: The .label_ attribute of tokens</h6>
			<h4>Note</h4>
			<p class="callout">As you can see in this exercise, spaCy is much easier to use than NLTK, but NLTK provides more methods to perform different operations on text. spaCy is perfect for production. That means, in the least amount of time, you can perform basic processes on text.</p>
			<p>The exercise has ended! You can now pre-process a text using NLTK or spaCy. Depending on the task you want to perform, you will be able to choose one of these libraries to clean your data.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor082"/>Topic Modeling</h2>
			<p>Within NLU, which is a part of NLP, one of the many tasks that can be performed is extracting the meaning of a sentence, a paragraph, or a whole document. One approach to understanding a document is through its topics. For example, if a set of documents is from a newspaper, the topics might be politics or sports. With topic modeling techniques, we can obtain a bunch of words representing various topics. Depending on your set of documents, you will then have different topics represented by different words. The goal of these techniques is to know the different types of documents in your corpus.</p>
			<h3 id="_idParaDest-65"><a id="_idTextAnchor083"/>Term Frequency – Inverse Document Frequency (TF-IDF)</h3>
			<p><strong class="bold">TF-IDF</strong> is a commonly used NLP model for extracting the most important words from a document. To perform this classification, the algorithm will assign a weight to each word. The idea of this method is to ignore words without relevance to the meaning of a global concept, (which means the overall topic of a text), so those terms will be down-weighted (which means that they will be ignored). Down-weighing them will allow us to find the keywords of that document (the words with the greatest weights).</p>
			<p>Mathematically, the algorithm to find the weight of a term in a document is as follows:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/C13550_03_19.jpg" alt="Figure 3.19: TF-IDF formula"/>
				</div>
			</div>
			<h6>Figure 3.19: TF-IDF formula</h6>
			<ul>
				<li><em class="italics">Wi,j</em>: Weight of the term, i, in the document, j</li>
				<li><em class="italics">tf,j</em>: Number of occurrences of i in j</li>
				<li><em class="italics">df,j</em>: Number of documents containing i</li>
				<li><em class="italics">N</em>: Total number of documents</li>
			</ul>
			<p>The result is the number of times a term appears in that document, multiplied by the log of the total number of documents, divided by the number of documents that contain the term.</p>
			<h3 id="_idParaDest-66"><a id="_idTextAnchor084"/>Latent Semantic Analysis (LSA)</h3>
			<p>LSA is one of the foundational techniques of topic modeling. It analyzes the relationship between a set of documents and their terms, and produces a set of concepts related to them.</p>
			<p>LSA is a step ahead when compared to TF-IDF. In a large set of documents, the TF-IDF matrix has very noisy information and many redundant dimensions, so the LSA algorithm performs dimensionality reduction.</p>
			<p>This reduction is performed with Singular Value Decomposition (SVD). SVD factorizes a matrix, M, into the product of three separate matrices:</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/C13550_03_20.jpg" alt="Figure 3.20: Singular Value Decomposition"/>
				</div>
			</div>
			<h6>Figure 3.20: Singular Value Decomposition</h6>
			<ul>
				<li><em class="italics">A</em>: This is the input data matrix.</li>
				<li><em class="italics">m</em>: This is the number of documents.</li>
				<li><em class="italics">n</em>: This is the number of terms.</li>
				<li><em class="italics">U</em>: Left singular vectors. Our document-topic matrix.</li>
				<li><em class="italics">S</em>: Singular values. Represents the strength of each concept. This is a diagonal matrix.</li>
				<li><em class="italics">V</em>: Right singular vectors. Represents terms' vectors in terms of topics.<h4>Note</h4><p class="callout">This method is more efficient on a large set of documents, but there are better algorithms to perform this task such as LDA or PLSA.</p></li>
			</ul>
			<h3 id="_idParaDest-67"><a id="_idTextAnchor085"/>Exercise 12: Topic Modeling in Python</h3>
			<p>In this exercise, TF-IDF and LSA will be coded in Python using a specific library. By the end of this exercise, you will be able to perform these techniques to extract the weights of a term in a document:</p>
			<ol>
				<li value="1">Open up your Google Colab interface.</li>
				<li>Create a folder for the book.</li>
				<li>To generate the TF-IDF matrix, we could code the formula in Figure 3.19, but we are going to use one of the most famous libraries for machine learning algorithms in Python, scikit-learn:<p class="snippet">from sklearn.feature_extraction.text import TfidfVectorizer</p><p class="snippet">from sklearn.decomposition import TruncatedSVD</p></li>
				<li>The corpus we are going to use for this exercise will be simple, with just four sentences:<p class="snippet">corpus = [</p><p class="snippet">     'My cat is white',</p><p class="snippet">     'I am the major of this city',</p><p class="snippet">     'I love eating toasted cheese',</p><p class="snippet">     'The lazy cat is sleeping',</p><p class="snippet">]</p></li>
				<li>With the <strong class="inline">TfidfVectorizer</strong> method, we can convert the collection of documents in our corpus to a matrix of TF-IDF features:<p class="snippet">vectorizer = TfidfVectorizer()</p><p class="snippet">X = vectorizer.fit_transform(corpus)</p></li>
				<li>The <strong class="inline">get_feature_names()</strong> method shows the extracted features. <h4>Note </h4><p class="callout">To understand the <strong class="inline">TfidfVectorizer</strong> function better, visit the Scikit Learn documentation - <a href="https://bit.ly/2S6lwWP">https://bit.ly/2S6lwWP</a></p><p class="snippet">vectorizer.get_feature_names()</p><p>The output is as follows:</p><div id="_idContainer093" class="IMG---Figure"><img src="image/C13550_03_21.jpg" alt="Figure 3.21: Feature names of the corpus"/></div><h6>Figure 3.21: Feature names of the corpus</h6></li>
				<li>X is a sparse matrix. To see its content, we can use the <strong class="inline">todense()</strong> function:<p class="snippet">X.todense()</p><p>The output is as follows:</p><div id="_idContainer094" class="IMG---Figure"><img src="image/C13550_03_22.jpg" alt="Figure 3.22: TF-IDF matrix of the corpus"/></div><h6>Figure 3.22: TF-IDF matrix of the corpus</h6></li>
				<li>Now let's perform dimensionality reduction with LSA. The <strong class="inline">TruncatedSVD</strong> method uses SVD to transform the input matrix. In this exercise, we'll use <strong class="inline">n_components=10</strong>. From now on, you have to use <strong class="inline">n_components=100</strong> (it has better results in larger corpuses):<p class="snippet">lsa = TruncatedSVD(n_components=10,algorithm='randomized',n_iter=10,random_state=0)</p><p class="snippet">lsa.fit_transform(X)</p><p>The output is as follows:</p><div id="_idContainer095" class="IMG---Figure"><img src="image/C13550_03_23.jpg" alt="Figure 23: Dimensionality reduction with LSA"/></div><h6>Figure 23: Dimensionality reduction with LSA</h6></li>
				<li><strong class="inline">attribute .components_</strong> shows the weight of each <strong class="inline">vectorizer.get_feature_names()</strong>. Notice that the LSA matrix has a range of 4x16, we have 4 documents in our corpus (concepts), and the vectorizer has 16 features (terms):<p class="snippet">lsa.components_</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/C13550_03_24.jpg" alt="Figure 3.24: The desired TF-IDF matrix output &#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.24: The desired TF-IDF matrix output</h6>
			<p>The exercise has ended successfully! This was a preparatory exercise for <em class="italics">Activity 3</em>, <em class="italics">Process a Corpus</em>. Do check the seventh step of the exercise – it will give you the key to complete the activity ahead. I encourage you to read the scikit-learn documentation and learn how to see the potential of these two methods. Now you know how to create the TF-IDF matrix. This matrix could be huge, so to manage the data better, the LSA algorithm performs dimensionality reduction on the weight of each term in the document.</p>
			<h3 id="_idParaDest-68"><a id="_idTextAnchor086"/>Activity 3: Process a Corpus</h3>
			<p>In this activity, we will process a really small corpus to clean the data and extract the keywords and concepts using LSA.</p>
			<p>Imagine this scenario: the newspaper vendor in your town has published a competition. It consists of predicting the category of an article. This newspaper does not have a structural database, which means it has only raw data. They provide a small set of documents, and they need to know whether the article is political, scientific, or sports-related:</p>
			<h4>Note</h4>
			<p class="callout">You can choose between spaCy and the NLTK library to do the activity. Both solutions will be valid if the keywords are related at the end of the LSA algorithm.</p>
			<ol>
				<li value="1">Load the corpus documents and store them in a list.<h4>Note</h4><p class="callout">The corpus documents can be found on GitHub, <a href="https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson03/Activity03/dataset">https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson03/Activity03/dataset</a></p></li>
				<li>Pre-process the text with spaCy or NLTK.</li>
				<li>Apply the LSA algorithm.</li>
				<li>Show the first five keywords related to each concept:<p>Keywords: moon, apollo, earth, space, nasa</p><p>Keywords: yard, touchdown, cowboys, prescott, left</p><p>Keywords: facebook, privacy, tech, consumer, data</p><h4>Note</h4><p class="callout">The output keywords probably will not be the same as yours. If your keywords are not related then check the solution.</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/C13550_03_25.jpg" alt="Figure 3.25: Output example of the most relevant words in a concept (f1)"/>
				</div>
			</div>
			<h6>Figure 3.25: Output example of the most relevant words in a concept (f1)</h6>
			<h4>Note</h4>
			<p class="callout">The solution for this activity is available on page 306.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor087"/>Language Modeling</h2>
			<p>So far, we have reviewed the most basic techniques for pre-processing text data. Now we are going to dive deep into the structure of natural language – language models. We can consider this topic an introduction to machine learning in NLP.</p>
			<h3 id="_idParaDest-70"><a id="_idTextAnchor088"/>Introduction to Language Models</h3>
			<p>A statistical <strong class="keyword">Language Model</strong> (<strong class="keyword">LM</strong>) is the probability distribution of a sequence of words, which means, to assign a probability to a particular sentence. For example, LMs could be used to calculate the probability of an upcoming word in a sentence. This involves making some assumptions about the structure of the LM and how it will be formed. An LM is never totally correct with its output, but using one is often necessary.</p>
			<p>LMs are used in many more NLP tasks. For example, in machine translation, it is important to know what sentence precedes the next. LMs are also used for speech recognition, to avoid ambiguity, for spelling corrections, and for summarization.</p>
			<p>Let's see how an LM is mathematically represented:</p>
			<ul>
				<li>P(W) = P(w1, w2,w3,w4,…wn)</li>
			</ul>
			<p><em class="italics">P(W)</em> is our LM and <em class="italics">wi</em> are the words included in <em class="italics">W</em>, and as we mentioned before, we can use it to compute the probability of an upcoming word in this way:</p>
			<ul>
				<li>P(w5|w1,w2,w3,w4)</li>
			</ul>
			<p>This (w1, w2, w3, w4) states what the probability of <em class="italics">w5</em> (the upcoming word) could be in a given sequence of words. </p>
			<p>Looking at this example, P (w5|w1, w2, w3, w4), we can assume this:</p>
			<ul>
				<li>P(actual word | previous words)</li>
			</ul>
			<p>Depending on the number of previous words we are looking at to obtain the probability of the actual word, there are different models we can use. So, now we are going to introduce some important concepts regarding such models.</p>
			<h3 id="_idParaDest-71"><a id="_idTextAnchor089"/>The Bigram Model</h3>
			<p>The bigram model is a sequence of two consecutive words. For example, in the sentence "My cat is white," there are these bigrams:</p>
			<p>My cat</p>
			<p>Cat is</p>
			<p>Is white</p>
			<p>Mathematically, a bigram has this form:</p>
			<ul>
				<li>Bigram model: P(wi|wi-1)</li>
			</ul>
			<h3 id="_idParaDest-72"><a id="_idTextAnchor090"/>N-gram Model</h3>
			<p>If we change the length of the previous word, we obtain the N-gram model. It works just like the bigram model but considers more words than the previous set.</p>
			<p>Using the previous example of "My cat is white," this is what we can obtain:</p>
			<ul>
				<li><strong class="bold">Trigram</strong><p>My cat is</p><p>Cat is white</p></li>
				<li><strong class="bold">4-gram</strong></li>
				<li>My cat is white</li>
			</ul>
			<p><strong class="bold">N-Gram Problem</strong></p>
			<p>At this point, you could think the n-gram model is more accurate than the bigram model because the n-gram model has access to additional "previous knowledge." However, n-gram models are limited to a certain extent, because of long-distance dependencies. An example would be, "After thinking about it a lot, I bought a television," which we compute as:</p>
			<ul>
				<li>P(television| after thinking about it a lot, I bought a)</li>
			</ul>
			<p>The sentence "After thinking about it a lot, I bought a television" is probably the only sequence of words with this structure in our corpus. If we change the word "television" for another word, for example "computer," the sentence "After thinking about it a lot, I bought a computer" is also valid, but in our model, the following would be the case:</p>
			<ul>
				<li>P(computer| after thinking about it a lot, I bought a) = 0</li>
			</ul>
			<p>This sentence is valid, but our model is not accurate, so we need to be careful with the use of n-gram models.</p>
			<h3 id="_idParaDest-73"><a id="_idTextAnchor091"/>Calculating Probabilities</h3>
			<p><strong class="bold">Unigram Probability</strong></p>
			<p>The unigram is the simplest case for calculating probabilities. It counts the number of times a word appears in a set of documents. Here is the formula for this:</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/C13550_03_26.jpg" alt="Figure 3.26: Unigram probability estimation"/>
				</div>
			</div>
			<h6>Figure 3.27: Unigram probability estimation</h6>
			<ul>
				<li><em class="italics">c(wi)</em> is the number of times<em class="italics"> </em></li>
				<li><em class="italics">wi</em> appears in the whole corpus. The size of the corpus is just how many tokens are in it.</li>
			</ul>
			<p><strong class="bold">Bigram Probability</strong></p>
			<p>To estimate bigram probability, we are going to use maximum likelihood estimation:</p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/C13550_03_27.jpg" alt="Figure 3.27: Bigram probability estimation"/>
				</div>
			</div>
			<h6>Figure 3.27: Bigram probability estimation</h6>
			<p>To understand this formula better, let's look at an example.</p>
			<p>Imagine our corpus is composed of these three sentences:</p>
			<p>My name is Charles.</p>
			<p>Charles is my name.</p>
			<p>My dog plays with the ball.</p>
			<p>The size of the corpus is 14 words, and now we are going to estimate the probability of the sequence "my name":</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/C13550_03_28.jpg" alt="Figure 3.28: Example of bigram estimation"/>
				</div>
			</div>
			<h6>Figure 3.28: Example of bigram estimation</h6>
			<p><strong class="bold">The Chain Rule</strong></p>
			<p>Now we know the concepts of bigrams and n-grams, we need to know how we can obtain those probabilities.</p>
			<p>If you have basic statistics knowledge, you might think the best option is to apply the chain rule and join each probability. For example, in the sentence "My cat is white," the probability is as follows:</p>
			<ul>
				<li>P(my cat is white) = p(white|my cat is) p(is|my cat) p(cat|my) p(my)</li>
			</ul>
			<p>It seems to be possible with this sentence, but if we had a much longer sentence, long-distance dependency problems would appear and the result of the n-gram model could be incorrect.</p>
			<p><strong class="bold">Smoothing</strong></p>
			<p>So far, we have a probabilistic model, and if we want to estimate the parameters of our model, we can use the maximum likelihood of estimation.</p>
			<p>One of the big problems of LMs is insufficient data. Our data is limited, so there will be many unknown events. What does this mean? It means we'll end up with an LM that gives a probability of 0 to unseen words.  </p>
			<p>To solve this problem, we are going to use a smoothing method. With this smoothing method, every probability estimation result will be greater than zero. The method we are going to use is add-one smoothing:</p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/C13550_03_29.jpg" alt="Figure 3.29: Add-one smoothing in bigram estimation"/>
				</div>
			</div>
			<h6>Figure 3.29: Add-one smoothing in bigram estimation</h6>
			<p><em class="italics">V</em> is the number of distinct tokens in our corpus.</p>
			<h4>Note</h4>
			<p class="callout">There are more smoothing methods with better performance; this is the most basic method.</p>
			<p><strong class="bold">Markov Assumption</strong></p>
			<p>Markov assumption is very useful for estimating the probabilities of a long sentence. With this method, we can solve the problem of long-distance dependencies. Markov assumption simplifies the chain rule to estimate long sequences of words. Each estimation only depends on the previous step:</p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/C13550_03_30.jpg" alt="Figure 3.30: Markov assumption"/>
				</div>
			</div>
			<h6>Figure 3.30: Markov assumption</h6>
			<p>We can also have a second-order Markov assumption, which depends on two previous terms, but we are going to use first-order Markov assumption:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/C13550_03_31.jpg" alt="Figure 3.31: Example of Markov"/>
				</div>
			</div>
			<h6>Figure 3.31: Example of Markov</h6>
			<p>If we apply this to the whole sentence, we get this:</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/C13550_03_32.jpg" alt="Figure 3.32: Example of Markov for a whole sentence"/>
				</div>
			</div>
			<h6>Figure 3.32: Example of Markov for a whole sentence</h6>
			<p>Decomposing the sequence of words in the aforementioned way will output the probabilities more accurately.</p>
			<h3 id="_idParaDest-74"><a id="_idTextAnchor092"/>Exercise 13: Create a Bigram Model</h3>
			<p>In this exercise, we are going to create a simple LM with unigrams and bigrams. Also, we will compare the results of creating the LM both without add-one smoothing and with it. One application of the n-gram is, for example, in keyboard apps. They can predict your next word. That prediction could be done with a bigram model:</p>
			<ol>
				<li value="1">Open up your Google Colab interface.</li>
				<li>Create a folder for the book.</li>
				<li>Declare a small, easy training corpus:<p class="snippet">import numpy as np</p><p class="snippet">corpus = [</p><p class="snippet">     'My cat is white',</p><p class="snippet">     'I am the major of this city',</p><p class="snippet">     'I love eating toasted cheese',</p><p class="snippet">     'The lazy cat is sleeping',</p><p class="snippet">]</p></li>
				<li>Import the required libraries and load the model:<p class="snippet">import spacy</p><p class="snippet">import en_core_web_sm</p><p class="snippet">from spacy.lang.en.stop_words import STOP_WORDS</p><p class="snippet">nlp = en_core_web_sm.load()</p></li>
				<li>Tokenize it with spaCy. To be faster in doing the smoothing and the bigrams, we are going to create three lists:<p><strong class="inline">Tokens</strong>: All tokens of the corpus</p><p><strong class="inline">Tokens_doc</strong>: List of lists with the tokens of each corpus</p><p><strong class="inline">Distinc_tokens</strong>: All tokens removing duplicates:</p><p class="snippet">tokens = []</p><p class="snippet">tokens_doc = []</p><p class="snippet">distinc_tokens = []</p><p>Let's create a first loop to iterate over the sentences in our corpus. The <strong class="inline">doc</strong> variable will contain a sequence of the sentences' tokens:</p><p class="snippet">for c in corpus:</p><p class="snippet">    doc = nlp(c)</p><p class="snippet">    tokens_aux = []</p><p>Now we are going to create a second loop to iterate through the tokens to push them into the corresponding list. The <strong class="inline">t</strong> variable will be each token of the sentence:</p><p class="snippet">    for t in doc:</p><p class="snippet">        tokens_aux.append(t.text)</p><p class="snippet">        if t.text not in tokens:</p><p class="snippet">            distinc_tokens.append(t.text) # without duplicates </p><p class="snippet">        tokens.append(t.text)</p><p class="snippet">    tokens_doc.append(tokens_aux)</p><p class="snippet">    tokens_aux = []</p><p class="snippet">    print(tokens)</p><p class="snippet">    print(distinc_tokens)</p><p class="snippet">    print(tokens_doc)</p></li>
				<li>Create the unigram model and test it:<p class="snippet">def unigram_model(word):</p><p class="snippet">    return tokens.count(word)/len(tokens)</p><p class="snippet">unigram_model("cat")</p><p>Result = 0.1388888888888889</p></li>
				<li>Add the smoothing and test it with the same word:<p class="snippet">def unigram_model_smoothing(word):</p><p class="snippet">    return (tokens.count(word) + 1)/(len(tokens) + len(distinc_tokens))</p><p class="snippet">unigram_model_smoothing("cat")</p><p>Result = 0.1111111111111111</p><h4>Note</h4><p class="callout">The problem with this smoothing method is that every unseen word has the same probability.</p></li>
				<li>Create the bigram model:<p class="snippet">def bigram_model(word1, word2):</p><p class="snippet">    hit = 0</p></li>
				<li>We need to iterate through all of the tokens in the documents to try to find the number of times that <strong class="inline">word1</strong> and <strong class="inline">word2</strong> appear together:<p class="snippet">    for d in tokens_doc:</p><p class="snippet">        for t,i in zip(d, range(len(d))): # i is the length of d  </p><p class="snippet">            if i &lt;= len(d)-2:</p><p class="snippet">                if word1 == d[i] and word2 == d[i+1]:</p><p class="snippet">                    hit += 1</p><p class="snippet">    print("Hits: ",hit)</p><p class="snippet">    return hit/tokens.count(word1)</p><p class="snippet">bigram_model("I","am")</p><p>The output is as follows:</p><div id="_idContainer105" class="IMG---Figure"><img src="image/C13550_03_33.jpg" alt="Figure 3.33: Output showing the times word1 and word2 appear together in the document"/></div><h6>Figure 3.33: Output showing the times word1 and word2 appear together in the document</h6></li>
				<li>Add the smoothing to the bigram model:<p class="snippet">def bigram_model_smoothing(word1, word2):</p><p class="snippet">    hit = 0</p><p class="snippet">    for d in tokens_doc:</p><p class="snippet">        for t,i in zip(d, range(len(d))):</p><p class="snippet">            if i &lt;= len(d)-2:</p><p class="snippet">                if word1 == d[i] and word2 == d[i+1]:</p><p class="snippet">                    hit += 1</p><p class="snippet">    return (hit+1)/(tokens.count(word1)+len(distinc_tokens))</p><p class="snippet">bigram_model("I","am")</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/C13550_03_34.jpg" alt="Figure 3.34: Output after adding smoothing to the model"/>
				</div>
			</div>
			<h6>Figure 3.34: Output after adding smoothing to<a id="_idTextAnchor093"/> the model</h6>
			<p>Congratulations! You have completed the last exercise of this chapter. In the next chapter, you will see that this LM approach is a fundamental deep NLP approach. You can now take a huge corpus and create your own LM.</p>
			<h4>Note</h4>
			<p class="callout">Applying the Markov assumption, the final probability will round the 0. I recommend using log() and adding each component. Also, check the precision bits of your code (float16 &lt; float32 &lt; float64).</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor094"/>Summary</h2>
			<p>NLP is becoming more and more important in AI. Industries analyze huge quantities of raw text data, which is unstructured. To understand this data, we use many libraries to process it. NLP is divided into two groups of methods and functions: NLG to generate natural language, and NLU to understand it.</p>
			<p>Firstly, it is important to clean text data, since there will be a lot of useless, irrelevant information. Once the data is ready to be processed, through a mathematical algorithm such as TF-IDF or LSA, a huge set of documents can be understood. Libraries such as NLTK and spaCy are useful for doing this task. They provide methods to remove the noise in data. A document can be represented as a matrix. First, TF-IDF can give a global representation of a document, but when a corpus is big, the better option is to perform dimensionality reduction with LSA and SVD. scikit-learn provides algorithms for processing documents, but if documents are not pre-processed, the result will not be accurate. Finally, the use of language models could be necessary, but they need to be formed of a valid training set of documents. If the set of documents is good, the language model should be able to generate language.</p>
			<p>In the next chapter, we will introduce <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>). We will be looking at some advanced models of these RNNs and will accordingly be one step ahead in building our robot.</p>
		</div>
	</body></html>