["```py\n            public static final String DATA_URL = \n            \"http://ai.stanford.edu/~amaas/data/sentiment/*\"; \n\n    ```", "```py\n            public static final String DATA_PATH = FilenameUtils.concat\n            (System.getProperty(\"java.io.tmpdir\"),local_file_path); \n\n    ```", "```py\n            public static final String WORD_VECTORS_PATH =    \n            \"/PATH_TO_YOUR_VECTORS/GoogleNews-vectors-negative300.bin\"; \n\n    ```", "```py\n            if( !archiveFile.exists() )\n            { \n             System.out.println(\"Starting data download (80MB)...\"); \n             FileUtils.copyURLToFile(new URL(DATA_URL), archiveFile); \n             System.out.println(\"Data (.tar.gz file) downloaded to \" +  \n             archiveFile.getAbsolutePath()); \n\n             extractTarGz(archizePath, DATA_PATH); \n            }\n            else \n            {       \n             System.out.println(\"Data (.tar.gz file) already exists at \" +  \n             archiveFile.getAbsolutePath()); \n             if( !extractedFile.exists())\n               { \n                extractTarGz(archizePath, DATA_PATH); \n               } \n             else \n               { \n                System.out.println(\"Data (extracted) already exists at \" +   \n                extractedFile.getAbsolutePath()); \n               } \n             } \n             } \n\n    ```", "```py\n          int batchSize = 50;      \n          int vectorSize = 300; \n          int nEpochs = 5;  \n          int truncateReviewsToLength = 300; \n     MultiLayerConfiguration conf = new             \n          NeuralNetConfiguration.Builder()\n     .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_\n             DESCENT)\n     .iterations(1)\n            .updater(Updater.RMSPROP) \n            .regularization(true).l2(1e-5) \n            .weightInit(WeightInit.XAVIER) \n            .gradientNormalization(GradientNormalization\n            .ClipElementWiseAbsoluteValue).gradientNormalizationThreshold\n            (1.0) \n            .learningRate(0.0018) \n            .list() \n            .layer(0, new GravesLSTM.Builder()\n                  .nIn(vectorSize)\n                  .nOut(200) \n                  .activation(\"softsign\")\n                  .build()) \n            .layer(1, new RnnOutputLayer.Builder()\n                  .activation(\"softmax\") \n                  .lossFunction(LossFunctions.LossFunction.MCXENT)\n                  .nIn(200)\n                  .nOut(2)\n                  .build()) \n            .pretrain(false)\n            .backprop(true)\n            .build(); \n\n          MultiLayerNetwork net = new MultiLayerNetwork(conf); \n          net.init(); \n          net.setListeners(new ScoreIterationListener(1)); \n\n    ```", "```py\n          DataSetIterator train = new AsyncDataSetIterator(new    \n          SentimentExampleIterator(DATA_PATH,wordVectors,\n          batchSize,truncateReviewsToLength,true),1);\n          DataSetIterator test = new AsyncDataSetIterator(new          \n          SentimentExampleIterator(DATA_PATH,wordVectors,100,\n          truncateReviewsToLength,false),1); \n          for( int i=0; i<nEpochs; i++ )\n          { \n            net.fit(train); \n            train.reset(); \n            System.out.println(\"Epoch \" + i + \" complete. Starting    \n            evaluation:\"); \n\n    ```", "```py\n            Evaluation evaluation = new Evaluation(); \n            while(test.hasNext())\n            { \n              DataSet t = test.next(); \n              INDArray features = t.getFeatureMatrix(); \n              INDArray lables = t.getLabels(); \n              INDArray inMask = t.getFeaturesMaskArray(); \n              INDArray outMask = t.getLabelsMaskArray(); \n              INDArray predicted =  \n              net.output(features,false,inMask,outMask); \n              evaluation.evalTimeSeries(lables,predicted,outMask); \n            } \n          test.reset(); \n\n          System.out.println(evaluation.stats()); \n          } \n\n    ```"]