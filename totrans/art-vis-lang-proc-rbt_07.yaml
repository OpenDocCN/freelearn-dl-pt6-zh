- en: '*Chapter 7*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章*'
- en: Build a Text-Based Dialogue System (Chatbot)
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基于文本的对话系统（聊天机器人）
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将能够：
- en: Define the terms GloVe, Word2Vec and Embeddings
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义GloVe、Word2Vec和嵌入的术语
- en: Develop your own Word2Vec
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发你自己的Word2Vec
- en: Select tools to create conversational agents
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择工具来创建对话代理
- en: Predict the intent of a conversation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测对话的意图
- en: Create a conversational agent
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个对话代理
- en: This chapter covers an introduction to terms such as GloVe, Word2Vec, and embeddings
    and tools that will help you create a conversational agent.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了GloVe、Word2Vec、嵌入等术语以及将帮助你创建对话代理的工具。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: One of the latest trends in deep NLP is the creation of conversational agents,
    also knowns as chatbots. A chatbot is a **text-based dialogue system** that understands
    human language and can hold a real conversation with people. Many companies use
    these systems to interact with its customers to obtain information and feedback,
    for example, opinions on a new product launch.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自然语言处理（NLP）中的最新趋势之一是创建对话代理，也叫聊天机器人。聊天机器人是一个**基于文本的对话系统**，它能够理解人类语言，并能够与人进行真实的对话。许多公司使用这些系统与客户互动，获取信息和反馈，例如，对新产品发布的意见。
- en: Chatbots are used as assistants, for example, Siri, Alexa, and Google Home.
    These can give us real-time information about the weather or traffic.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人被用作助手，例如，Siri、Alexa和Google Home。这些助手可以提供实时的天气或交通信息。
- en: At this point, the question is how can bots understand us? In the previous chapters,
    we have reviewed language models and how they work. However, the most important
    thing in language models (LMs) is the position of a word in a sentence. Each word
    has a certain probability of appearing in a sentence, depending on the words already
    in that sentence. But the probability distribution approach is not a good fit
    for this task. In this case, we need to understand the meaning, not predict the
    next word, after which, the model will understand the meaning of a word in a given
    corpus.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的问题是，机器人如何理解我们？在前几章中，我们回顾了语言模型及其工作原理。然而，在语言模型（LMs）中，最重要的事情是单词在句子中的位置。每个单词在句子中出现的概率是有一定的，取决于句子中已经出现的单词。但概率分布方法并不适用于这个任务。在这种情况下，我们需要理解单词的意义，而不是预测下一个单词，之后模型将能够理解给定语料库中单词的意义。
- en: A word in itself doesn't make sense unless it is placed within a context or
    in a corpus. It is important to understand the meaning of a sentence and this
    dictated by its structure (that is, the position of the words in it). The model
    will then predict the meaning of words by looking at which words are near to it.
    But firstly, how is it possible to represent this mathematically?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个单词本身没有意义，除非它被放在一个上下文或语料库中。理解一个句子的意义是很重要的，而这由句子的结构决定（也就是单词在句子中的位置）。模型将通过查看哪些单词接近它来预测单词的意义。但首先，如何用数学方式来表示这个呢？
- en: 'In *Chapter 4*, *Neural Networks with NLP*, we looked at representing a word
    using a one-hot encoded vector, which is a vector with 1s and 0s. However, this
    representation does not provide us with the actual meaning of a word. Let''s take
    a look at an example:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章*《*神经网络与自然语言处理*》中，我们讨论了如何使用一个one-hot编码向量来表示一个单词，这个向量由1和0组成。然而，这种表示方式并没有提供单词的实际含义。我们来看一个例子：
- en: Dog  [1,0,0,0,0,0]
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狗  [1,0,0,0,0,0]
- en: Cat  [0,0,0,0,1,0]
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猫  [0,0,0,0,1,0]
- en: A dog and a cat are animals, but their representation in 1s and 0s does not
    give us any information about the meaning of those words.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 狗和猫是动物，但它们在1和0中的表示并没有提供任何关于这些单词含义的信息。
- en: But what would happen if these vectors gave us a similarity between two words
    based on their meaning? Two words with a similar meaning would be placed near
    to each other in a plane, as opposed to two words without any such relation. For
    example, the name of a country and its capital are related.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果这些向量根据单词的含义为我们提供了两个单词之间的相似性会发生什么呢？具有相似含义的两个单词将被放置在平面上彼此靠近，而没有任何关系的单词则不会。例如，一个国家的名称和它的首都是相关的。
- en: Having this approach, a set of sentences can be related to a conversational
    intention or a specific topic (also known as intent, this term will be used throughout
    this chapter). Using this system, we would be able to maintain a sensible conversational
    dialogue with a human.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方法，一组句子可以与对话意图或特定主题相关联（也称为意图，这个术语将在本章中反复使用）。使用这种系统，我们将能够与人类进行合理的对话交流。
- en: The intent of a conversation is the topic of the dialogue. For example, if you
    were talking about a match between Real Madrid and Barcelona, the intent of the
    conversation would be football.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对话的意图是对话的主题。例如，如果你正在谈论皇家马德里与巴塞罗那的比赛，那么对话的意图就是足球。
- en: Later in this chapter, we will review the fundamental concepts of the representation
    of a word as a vector, and how to create such vectors and use them to recognize
    the intent of a conversation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章稍后，我们将回顾将单词表示为向量的基本概念，以及如何创建这样的向量并用它们来识别对话的意图。
- en: Word Representation in Vector Space
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量空间中的词表示
- en: This section will cover the different architectures for computing a continuous
    vector representation of words from a corpus. These representations will depend
    on the similarity of words, in terms of meaning. Also, there will be an introduction
    to a new Python library (**Gensim**) to do this task.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍从语料库中计算单词的连续向量表示的不同架构。这些表示将取决于单词在意义上的相似性。此外，还将介绍一个新的 Python 库 (**Gensim**)
    来完成这项任务。
- en: Word Embeddings
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Word embeddings are a collection of techniques and methods to map words and
    sentences from a corpus and output them as vectors or real numbers. Word embeddings
    generate a representation of each word in terms of the context in which the word
    appears. The main task of word embeddings is to perform a dimension reduction
    from a space with one dimension per word to a continuous vector space.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是一种将语料库中的单词和句子映射并输出为向量或实数的技术和方法的集合。词嵌入通过表示单词出现的上下文来生成每个单词的表示。词嵌入的主要任务是将每个单词的空间维度从一个维度降到一个连续的向量空间。
- en: 'To better understand what that means, let''s have a look at an example. Imagine
    we have two similar sentences, such as these:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这是什么意思，让我们看一个例子。假设我们有两句话，它们相似，例如：
- en: I am good.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我很好。
- en: I am great.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我很好。
- en: 'Now, encoding these sentences as one-hot vectors, we have something like this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将这些句子编码为独热向量，我们得到类似这样的表示：
- en: I  [1,0,0,0]
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我  [1,0,0,0]
- en: Am  [0,1,0,0]
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Am  [0,1,0,0]
- en: Good  [0,0,1,0]
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Good  [0,0,1,0]
- en: Great  [0,0,0,1]
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Great  [0,0,0,1]
- en: We know the previous two sentences are similar (in terms of their meaning),
    because "great" and "good" have a similar meaning. But how could we measure the
    similarity of these two words? We have two vectors representing the words, so
    let's compute the cosine similarity.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道前两个句子是相似的（在意义上），因为“great”和“good”有相似的意思。但我们如何衡量这两个单词的相似度呢？我们有两个向量表示这两个单词，那么让我们计算余弦相似度。
- en: Cosine Similarity
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: 'Cosine similarity measures the similarity between two vectors. As the name
    suggests, this method will state the cosine of the angle between two sentences.
    Its formula is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度衡量两个向量之间的相似度。顾名思义，这种方法将表示两个句子之间角度的余弦值。其公式如下：
- en: '![Figure 7.1: Formula for cosine similarity](img/C13550_07_01.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1：余弦相似度公式](img/C13550_07_01.jpg)'
- en: 'Figure 7.1: Formula for cosine similarity'
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.1：余弦相似度公式
- en: Figure 7.1 shows the formula for cosine similarity. A and B are the vectors.
    Following the previous example, if we compute the similarity between "good" and
    "great", the result is 0\. This is because one-hot encoded vectors are independent
    and there is no projection along the same dimension (that means there is only
    a single 1 in a dimension, and the rest are 0s).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 展示了余弦相似度的公式。A 和 B 是向量。按照之前的示例，如果我们计算“good”和“great”之间的相似度，结果是 0。这是因为独热编码向量是独立的，并且在相同的维度上没有投影（这意味着某一维度上只有一个
    1，其余是 0）。
- en: 'Figure 7.2 explains this concept:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 解释了这个概念：
- en: '![Figure 7.2: Dimension without projection](img/C13550_07_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2：无投影维度](img/C13550_07_02.jpg)'
- en: 'Figure 7.2: Dimension without projection'
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.2：无投影维度
- en: Word embeddings solve this problem. There are many techniques to represent word
    embeddings. But all these techniques are in unsupervised learning algorithms.
    One of the most famous methods is the Word2Vec model, which is going to be explained
    next.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入解决了这个问题。有许多技术可以表示词嵌入。但所有这些技术都属于无监督学习算法。其中最著名的方法之一是 Word2Vec 模型，接下来将进行解释。
- en: Word2Vec
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2Vec
- en: The main goal of Word2Vec is to produce word embeddings. It processes a corpus
    and then assigns a vector to each unique word in the corpus. This vector, however,
    does not work like the one-hot vector method. For example, if we have a corpus
    with 10,000 words, we would have 10,000 dimensions in our one-hot encoded vectors,
    but Word2Vec can perform dimension reduction, typically of several hundred dimensions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 的主要目标是生成词嵌入。它处理语料库，然后为语料库中的每个唯一单词分配一个向量。然而，这个向量不像 one-hot 向量方法那样工作。例如，如果我们的语料库中有
    10,000 个单词，我们的 one-hot 编码向量就会有 10,000 个维度，但 Word2Vec 可以进行降维，通常降到几百维。
- en: 'The core idea of Word2Vec is that a word''s meaning is represented by the words
    that are frequently near to it. When a word appears in a sentence, its context
    is formed by the set of words it has nearby. This set of words are within a fixed-size
    window:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 的核心思想是一个单词的含义由经常出现在它附近的单词来表示。当一个单词出现在句子中时，它的上下文由它附近的单词集构成。这些单词集位于一个固定大小的窗口内：
- en: '![Figure 7.3: Context words of wx](img/C13550_07_03.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3：*wx* 的上下文词](img/C13550_07_03.jpg)'
- en: 'Figure 7.3: Context words of wx'
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.3：*wx* 的上下文词
- en: Figure 7.3 shows an example of the context words for *wx*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 显示了 *wx* 的上下文词的示例。
- en: The concept of Word2Vec was created by Tomas Mikolov in 2013\. He proposed a
    framework for learning word vectors. The method works by iterating through a corpus,
    taking a set of words with a central word (in Figure 7.3, it is *wx*) and context
    words (in figure 7.3, the words shown inside the black rectangular box). The vectors
    of these words keep updating until the corpus ends.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 的概念由 Tomas Mikolov 在 2013 年提出。他提出了一个学习词向量的框架。该方法通过遍历语料库来工作，取出一组包含中心词（在图
    7.3 中为 *wx*）和上下文词（在图 7.3 中，黑色矩形框内显示的词）的一组单词。这些词的向量会不断更新，直到语料库结束。
- en: 'There are two methods for performing Word2Vec:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种执行 Word2Vec 的方法：
- en: '**Skip-Gram model**: In this model, the input is the word placed in the center
    and thereafter it predicts the context of words.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Skip-Gram 模型**：在此模型中，输入是放置在中心的单词，之后预测其上下文词。'
- en: '**CBOW model**: The input of this model are the vectors of the context words,
    and the output is the central word.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CBOW 模型**：该模型的输入是上下文词的向量，输出是中心词。'
- en: '![Figure 7.4: CBOW and skip-gram model representation](img/C13550_07_04.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4：CBOW 和 Skip-gram 模型表示](img/C13550_07_04.jpg)'
- en: 'Figure 7.4: CBOW and skip-gram model representation'
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.4：CBOW 和 Skip-gram 模型表示
- en: Both of these models output good results, but the skip-gram model works well
    with a small amount of data. We will not go into these models in more depth to
    generate our Word2Vec, but we will be using the Gensim library, which is explained
    in this chapter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种模型都能产生不错的结果，但 Skip-gram 模型在数据量较小的情况下表现较好。我们不会深入讨论这些模型如何生成我们的 Word2Vec，但我们会使用
    Gensim 库，本章将介绍它。
- en: Problems with Word2Vec
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2Vec 的问题
- en: 'Word2Vec has many advantages for representing words within a vector space.
    It improves the performance of the task and can capture complex word meanings.
    But it is not perfect, and does present some problems:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 在将单词表示为向量空间中的点时有许多优点。它提高了任务的性能，并能够捕捉复杂的单词含义。但它并不完美，存在一些问题：
- en: 'Inefficiently using statistics: It captures co-occurrences of words one at
    a time. The problem here is that words that do not occur together within a trained
    corpus tend to get closer in the plane (this can cause ambiguity) because there''s
    no way to represent their relationships.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低效地使用统计数据：它一次捕捉单词的共现。这里的问题是，那些在训练语料库中没有共同出现的单词往往会在平面上变得更接近（这可能会导致歧义），因为没有办法表示它们之间的关系。
- en: The need to modify the parameters of the model, that is, if the corpus size
    changes. Doing this, the model will be trained again, and this consumes a lot
    of time.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要修改模型的参数，即当语料库的大小发生变化时，模型需要重新训练，而这会消耗大量的时间。
- en: Before diving deep in how to solve these problems with Word2Vec, we are going
    to introduce Gensim, a library for creating Word2Vec models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨如何用 Word2Vec 解决这些问题之前，我们将介绍 Gensim，这是一个用于创建 Word2Vec 模型的库。
- en: Gensim
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Gensim
- en: Gensim is a Python library that provides different NLP methods. It is not like
    NLTK or spaCy; those libraries are focused on the pre-processing and analysis
    of data. Gensim provides us with methods to process raw text (which is unstructured).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim 是一个 Python 库，提供了不同的 NLP 方法。它不同于 NLTK 或 spaCy；那些库侧重于数据的预处理和分析，而 Gensim
    提供了处理原始文本（即无结构文本）的方法。
- en: 'These are the advantages of Gensim:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim 的优点如下：
- en: Gensim can be used with a huge corpus. It has memory independence, which means
    the corpus will not need to be stored in the RAM of your computer. Also, it has
    memory sharing to store the trained models.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gensim可以用于超大语料库。它具有内存独立性，这意味着语料库不需要存储在计算机的RAM中。同时，它具有内存共享功能来存储训练好的模型。
- en: It can provide efficient vector space algorithms, such as Word2Vec, Doc2Vec,
    LSI, LSA, and so on.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以提供高效的向量空间算法，如Word2Vec、Doc2Vec、LSI、LSA等。
- en: Its API is easy to learn.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的API易于学习。
- en: 'These are the disadvantages of Gensim:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是Gensim的缺点：
- en: It does not provide methods to pre-process text, and it has to be used with
    NLTK or spaCy to obtain a full NLP pipeline.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它没有提供文本预处理的方法，必须与NLTK或spaCy一起使用，以获得完整的自然语言处理管道。
- en: 'Exercise 24: Creation of a Word Embedding'
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 24：创建词嵌入
- en: In this exercise, we are going to create our word embedding using a small corpus
    and use Gensim. Once our model is trained, we will print it on a two-dimensional
    graph to check the distribution of the words.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用一个小语料库和Gensim创建我们的词嵌入。一旦模型训练完成，我们将把它打印在一个二维图表上，以检查单词的分布情况。
- en: 'Gensim provides the possibility to change some parameters to perform training
    well on our data. Some useful parameters are as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim提供了更改一些参数的可能性，以便在我们的数据上进行良好的训练。一些有用的参数如下：
- en: '`Num_features`: Represents the dimensionality of the vectors (more dimensions
    equals more accuracy, but is more computationally expensive). In our case, we
    are going to set this parameter to **2** (vectors of 2 dimensions).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Num_features`：表示向量的维度（更多维度意味着更高的准确性，但计算成本更高）。在我们的例子中，我们将此参数设置为**2**（2维向量）。'
- en: '`Window_size`: Represents the size of the fixed window to contain the context
    of words. In our case, the corpus is small, so the size here is set to **1**.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Window_size`：表示固定窗口的大小，用于包含词语的上下文。在我们的例子中，语料库较小，所以此处大小设置为**1**。'
- en: '`Min_word_count`: The minimum set word count threshold.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Min_word_count`：最小的词频阈值。'
- en: '`Workers`: The threads of your computer running in parallel. In our case, one
    worker will be good for the size of our corpus.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Workers`：计算机并行运行的线程。在我们的例子中，一个工作线程对于语料库的大小就足够了。'
- en: 'Let''s begin with the exercise:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从练习开始：
- en: 'Import the libraries. We are going to use the Gensim model, Word2Vec:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库。我们将使用Gensim模型，Word2Vec：
- en: '[PRE0]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define a small random corpus:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个小的随机语料库：
- en: '[PRE1]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we will tokenize each sentence with `spaCy`. The concept of `spaCy` was
    covered in *Chapter 3*, *Fundamentals of Natural Language Processing*:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将使用`spaCy`对每个句子进行分词。`spaCy`的概念在*第3章*，*自然语言处理基础*中有讲解：
- en: '[PRE2]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now let''s define a few variables to create the Word2vec model:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们定义一些变量来创建Word2Vec模型：
- en: '[PRE3]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create the model using the Word2Vec method with a seed of 0 (this seed is just
    a value to initialize the weights of the model; using the same seed to obtain
    the same results is recommended):'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Word2Vec方法创建模型，种子为0（这个种子只是用来初始化模型的权重；推荐使用相同的种子以获得相同的结果）：
- en: '[PRE4]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we will build the vocabulary from our corpus. First, we need to have a
    vocabulary to train our model:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将从语料库中构建词汇表。首先，我们需要一个词汇表来训练模型：
- en: '[PRE5]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Train the model. The parameters here are the sentences of the corpus: total
    words and epochs. In this case, 1 epoch will be good:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。这里的参数是语料库的句子：总词数和训练轮次（在此案例中，1轮训练就足够）：
- en: '[PRE6]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we can see how the model works when computing the similarity of two words:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以看到当计算两个词语的相似度时，模型是如何工作的：
- en: '[PRE7]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 7.5: The computed result stating the similarity of two words](img/C13550_07_05.jpg)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.5：计算结果，表示两个词语的相似度](img/C13550_07_05.jpg)'
- en: 'Figure 7.5: The computed result stating the similarity of two words'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.5：计算结果，表示两个词语的相似度
- en: 'Now, to print the model, define a variable with the words of our corpus and
    an array with the vector of each word:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了打印模型，定义一个包含我们语料库中单词的变量，并定义一个数组来存储每个单词的向量：
- en: '[PRE8]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create a `DataFrame` with this data using pandas:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas创建一个包含此数据的`DataFrame`：
- en: '[PRE9]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Figure 7.6: Coordinates of our vectors](img/C13550_07_06.jpg)'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.6：我们的向量坐标](img/C13550_07_06.jpg)'
- en: 'Figure 7.6: Coordinates of our vectors'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.6：我们的向量坐标
- en: 'Create a figure with the location of each word in a plane:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个图形，表示每个单词在平面中的位置：
- en: '[PRE10]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Figure 7.7: Location of items in our Word2Vec model](img/C13550_07_07.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7：我们Word2Vec模型中项目的位置](img/C13550_07_07.jpg)'
- en: 'Figure 7.7: Location of items in our Word2Vec model'
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.7：我们Word2Vec模型中项目的位置
- en: As you can see in figure 7.7, words can be represented in two dimensions. If
    you have a smaller corpus, to find out the similarity of two words in terms of
    meaning, you just need to measure the distances of those two words.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如图7.7所示，词语可以在二维空间中表示。如果你有一个较小的语料库，要了解两个词在意义上的相似性，只需要测量这两个词的距离。
- en: Now you know how to train your own Word2Vec model!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道如何训练自己的Word2Vec模型了！
- en: Global Vectors (GloVe)
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Global Vectors (GloVe)
- en: Global Vectors is a model for word representation. It works just like the Word2Vec
    model but adds some new features in order to be much more efficient.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Global Vectors是一个词表示模型。它的工作方式和Word2Vec模型类似，但增加了一些新特性，使其更加高效。
- en: Before beginning with this model, it will be beneficial to think of other ways
    to create a word vector.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用这个模型之前，思考其他创建词向量的方法是很有益的。
- en: The statistics of word occurrences in a corpus is the first source of information
    we can find to use in unsupervised algorithms, so it is possible to capture the
    co-occurrence counts directly. To obtain this information, we do not need a processed
    method; just having the text data will be enough.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库中词语出现的统计数据是我们可以用来在无监督算法中利用的第一手信息，因此可以直接捕捉共现计数。为了获得这些信息，我们不需要经过处理的方法；只要有文本数据就足够了。
- en: 'Creating a co-occurrence matrix, X, along with a fixed-size window, we can
    obtain a new representation of words. For example, imagine this corpus:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个共现矩阵X，并且设置一个固定大小的窗口，我们可以得到词语的新表示。例如，假设这个语料库：
- en: I am Charles.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我是Charles。
- en: I am amazing.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我很棒。
- en: I love apples.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我喜欢苹果。
- en: 'A window-based co-occurrence matrix is as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 基于窗口的共现矩阵如下所示：
- en: '![Figure 7.8: A window-based co-occurrence matrix](img/C13550_07_08.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8: 基于窗口的共现矩阵](img/C13550_07_08.jpg)'
- en: 'Figure 7.8: A window-based co-occurrence matrix'
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 7.8: 基于窗口的共现矩阵'
- en: The co-occurrence matrix is easy to understand, counting how many times a word
    appears next to another word in the corpus.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 共现矩阵很容易理解，它统计了一个词在语料库中与另一个词相邻出现的次数。
- en: For example, in the first row, with the word "I", the word "am" has the value
    2, because there are 2 occurrences of "I am."
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在第一行，词语"I"的情况下，词语"am"的值为2，因为"I am"出现了两次。
- en: This representation improves the one-hot encoding and can capture semantic and
    syntactic information, but it does have certain problems, such as the size of
    the model, the sparsity of the vocabulary, and the model is less robust overall.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示方式改进了独热编码，并能捕捉语义和句法信息，但也存在一些问题，比如模型的大小、词汇的稀疏性，并且模型的整体鲁棒性较差。
- en: 'But in this case, these problems can be solved by reducing the dimension of
    the matrix using **SVD** (which was explained in *Chapter 3*, *Fundamentals of
    Natural Language Processing*) with the following formula:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 但在这种情况下，可以通过使用**SVD**（在*第3章*，*自然语言处理基础*中解释）来减少矩阵的维度，解决这些问题，公式如下：
- en: A = USVT
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A = USVT
- en: The results of doing this are good and the representation of the words do make
    sense, but this would still be problematic with a large corpus.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的结果是好的，词的表示确实有意义，但对于大规模语料库而言，仍然会有问题。
- en: 'The GloVe approach solves the problem of Word2Vec models in the following ways:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe方法通过以下方式解决了Word2Vec模型的问题：
- en: The overall time taken to train the model is reduced if the corpus has a change
    in it.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果语料库发生变化，训练模型所需的总体时间将减少。
- en: Statistics are used efficiently. It behaves better with words that do not appear
    many times in the corpus. This was a problem with Word2Vec, that is, unusual words
    have similar vectors.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计数据得到了高效使用。它在处理语料库中不常出现的词语时表现更好。这是Word2Vec中的一个问题，即不常见的词语具有相似的向量。
- en: GloVe combines the preceding two approaches to achieve fast training. It is
    scalable to a huge corpus and can achieve better performance with small vectors.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe结合了前两种方法，以实现快速训练。它能够扩展到庞大的语料库，并且可以用小向量实现更好的性能。
- en: Note
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This model was created by Stanford University and is an open source project.
    You can find more documentation at [https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson07/Exercise25-26/utils](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson07/Exercise25-26/utils).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由斯坦福大学创建，是一个开源项目。你可以在[https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson07/Exercise25-26/utils](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson07/Exercise25-26/utils)找到更多文档。
- en: In the next exercise, you will learn how to work with GloVe.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，您将学习如何使用GloVe。
- en: 'Exercise 25: Using a Pretrained GloVe to See the Distribution of Words in a
    Plane'
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习25：使用预训练的GloVe查看平面中词语的分布
- en: 'In this exercise, you will learn how to use GloVe and how to plot a region
    of a model. We will use the Gensim library once again:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，您将学习如何使用GloVe以及如何绘制模型的区域。我们将再次使用Gensim库：
- en: Note
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'To obtain the model, you will need to download the file from the `utils` folder
    on GitHub (which is the 50-dimensional model):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取模型，您需要从GitHub上的`utils`文件夹下载文件（这是50维的模型）：
- en: '[https://github.com/TrainingByPackt/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Chapter%207/utils](https://github.com/TrainingByPackt/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Chapter%207/utils)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/TrainingByPackt/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Chapter%207/utils](https://github.com/TrainingByPackt/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Chapter%207/utils)'
- en: Open up your Google Colab interface.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开您的Google Colab界面。
- en: Create a folder for the book, download the `utils` folder from GitHub, and upload
    it to the folder.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为本书创建一个文件夹，从GitHub下载`utils`文件夹并将其上传到该文件夹中。
- en: 'Import drive and mount it as follows:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入驱动器并按如下方式挂载：
- en: '[PRE11]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once you have mounted your drive for the first time, you will have to enter
    the authorization code by clicking on the URL mentioned by Google and pressing
    the **Enter** key on your keyboard:![Figure 7.9: Image displaying the Google Colab
    authorization step](img/C13550_07_09.jpg)'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您第一次挂载了您的驱动器，您需要通过点击Google提供的URL并按下键盘上的**Enter**键来输入授权码：![图7.9：显示Google Colab授权步骤的图片](img/C13550_07_09.jpg)
- en: 'Figure 7.9: Image displaying the Google Colab authorization step'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.9：显示Google Colab授权步骤的图片
- en: 'Now that you have mounted the drive, you need to set the path of the directory:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您已经挂载了驱动器，您需要设置目录的路径：
- en: '[PRE12]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The path mentioned in step 5 may change as per your folder setup on Google Drive.
    However, the path will always begin with `cd /content/drive/My Drive/`.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第5步中提到的路径可能会根据您在Google Drive上的文件夹设置而有所不同。然而，该路径始终以`cd /content/drive/My Drive/`开头。
- en: The `utils` folder must be present in the path you are setting up.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`utils`文件夹必须出现在您正在设置的路径中。'
- en: 'Import the libraries:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE13]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Use the `glove2word2vec` function provided by Gensim to create the `word2vec`
    model:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Gensim提供的`glove2word2vec`函数创建`word2vec`模型：
- en: '[PRE14]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The `glove.6B.50d.txt` file in this case has been placed within the `utils`
    folder. If you choose to place it elsewhere, the path will change accordingly.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本例中，`glove.6B.50d.txt`文件已被放置在`utils`文件夹中。如果您选择将其放在其他地方，路径将相应变化。
- en: 'Initialize the model using the file generated by the `glove2word2vec` function:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`glove2word2vec`函数生成的文件初始化模型：
- en: '[PRE15]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With GloVe, you can measure the similarity of a pair of words. Check whether
    the model works by computing the similarity between two words and printing a word
    vector:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用GloVe，您可以衡量一对词语的相似度。通过计算两个词语之间的相似度并打印出词向量来检查模型是否有效：
- en: '[PRE16]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Figure 7.10: Similarity of woman and queen](img/C13550_07_10.jpg)'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图7.10：woman和queen的相似度](img/C13550_07_10.jpg)'
- en: 'Figure 7.10: Similarity of woman and queen'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.10：woman和queen的相似度
- en: 'In *Exercise 24*, *Creation of a Word Embedding*, we created our own vectors,
    but here, vectors are already created. To see the representation vector of a word,
    we just have to do the following:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*练习24*，*创建词嵌入*中，我们创建了自己的向量，但在这里，向量已经被创建。要查看一个词的表示向量，我们只需要做以下操作：
- en: '[PRE17]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Figure 7.11: “Woman” vector representation (50 dimensions)](img/C13550_07_11.jpg)'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图7.11：“Woman”向量表示（50维）](img/C13550_07_11.jpg)'
- en: 'Figure 7.11: "Woman" vector representation (50 dimensions)'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.11：“Woman”向量表示（50维）
- en: 'We can also see the words most similar to other words. As you can see in steps
    4 and 5, GloVe have many functionalities related to word representation:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以看到与其他词语最相似的词语。正如在步骤4和5中所示，GloVe有许多与词表示相关的功能：
- en: '[PRE18]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Figure 7.12: Words most similar to woman](img/C13550_07_12.jpg)'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图7.12：与woman最相似的词语](img/C13550_07_12.jpg)'
- en: 'Figure 7.12: Words most similar to woman'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.12：与woman最相似的词语
- en: 'Now we are going to use Singular Value Decomposition (SVD) to visualize high-dimensional
    data to plot the words most similar to woman. Import the necessary libraries:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用奇异值分解（SVD）来可视化高维数据，以绘制与woman最相似的词语。导入必要的库：
- en: '[PRE19]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Initialize an array of 50 dimensions and append the vector of woman. To perform
    this dimensional reduction, we are going to create a matrix, and its rows will
    be the vector of each word:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个 50 维的数组并附加“woman”向量。为了进行这种维度缩减，我们将创建一个矩阵，其行将是每个单词的向量：
- en: '[PRE20]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Figure 7.13: Matrix values with the word “dog”](img/C13550_07_13.jpg)'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.13：包含“dog”单词的矩阵值](img/C13550_07_13.jpg)'
- en: 'Figure 7.13: Matrix values with the word "woman"'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.13：包含“woman”单词的矩阵值
- en: 'Now, we have the word `dog` in the matrix and we need to append every vector
    of the similar words. Add the rest of the vectors to the matrix:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们在矩阵中有了“dog”这个单词，我们需要附加每个相似单词的向量。将其余的向量添加到矩阵中：
- en: '[PRE21]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This matrix is something like this:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个矩阵大致如下所示：
- en: '![Figure 7.14: Matrix with the most similar vectors of the “woman” vector](img/C13550_07_14.jpg)'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.14：包含与“woman”向量最相似的矩阵](img/C13550_07_14.jpg)'
- en: 'Figure 7.14: Matrix with the most similar vectors of the "woman" vector'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.14：包含与“woman”向量最相似的矩阵
- en: Once we have all the vectors in the matrix, let's initialize the TSNE method.
    It is a function of Sklearn;
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们拥有了矩阵中的所有向量，就让我们初始化 TSNE 方法。它是 Sklearn 的一个函数；
- en: '[PRE22]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Transform the matrix into vectors of two dimensions and create a DataFrame
    with pandas to store them:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将矩阵转换为二维向量，并使用 pandas 创建一个 DataFrame 来存储它们：
- en: '[PRE23]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Figure 7.15: Coordinates of our vectors in two dimensions](img/C13550_07_15.jpg)'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.15：我们向量在二维中的坐标](img/C13550_07_15.jpg)'
- en: 'Figure 7.15: Coordinates of our vectors in two dimensions'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.15：我们向量在二维中的坐标
- en: 'Create a plot to see the words in a plane:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个图表来查看平面上的单词：
- en: '[PRE24]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Figure 7.16: Distribution of the words most similar to woman](img/C13550_07_16.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.16：与“woman”最相似的单词分布](img/C13550_07_16.jpg)'
- en: 'Figure 7.16: Distribution of the words most similar to woman'
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.16：与“woman”最相似的单词分布
- en: Here, we reduced the dimensionality of the vectors to get the output in a two-dimensional
    graph. Here, we can see the similarity relationship between the words.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过降低向量的维度，将其输出为二维图形。在此，我们可以看到单词之间的相似关系。
- en: You have finished exercise 25! You can now choose between using your own word2vec
    model or a GloVe model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经完成了第 25 号练习！现在你可以选择使用自己的 word2vec 模型或 GloVe 模型。
- en: Dialogue Systems
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对话系统
- en: As we mentioned before, chatbots are becoming more and more popular. They can
    help humans 24/7, answering questions or just holding a conversation. Dialogue
    systems can understand topics, give reasonable answers, and detect sentiments
    in a conversation (such as positive, neutral, or negative sentiment) with a human.
    The main goal of these systems is to hold a natural dialogue by imitating a human.
    This capability to behave or think like a human is one of the most important factors
    in ensuring a good user experience in the conversation. The Loebner Prize is a
    chatbot contest in which chatbots are tested using many different sentences and
    questions, and the most human-like system wins. One of the most popular conversational
    agents is the Mitsuku chatbot ([https://www.pandorabots.com/mitsuku/](https://www.pandorabots.com/mitsuku/)).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，聊天机器人正变得越来越流行。它们可以帮助人类 24/7，解答问题或进行对话。对话系统能够理解话题，给出合理的回答，并在与人类的对话中检测情感（如积极、消极或中立情感）。这些系统的主要目标是通过模仿人类来进行自然对话。这种像人类一样思考或行为的能力是确保对话中良好用户体验的一个重要因素。洛布纳奖是一个聊天机器人竞赛，参赛的聊天机器人需要接受多种不同句子和问题的测试，最像人类的系统获胜。最受欢迎的对话代理之一是
    Mitsuku 聊天机器人（[https://www.pandorabots.com/mitsuku/](https://www.pandorabots.com/mitsuku/)）。
- en: 'Chatbots are commonly used as a text service to give information to users.
    For example, one of the most popular conversational agents in Spain is Lola, which
    can give you your zodiac information ([https://1millionbot.com/chatbot-lola/](https://1millionbot.com/chatbot-lola/)).
    You just need to send a message and wait a few seconds to receive the data. But
    in 2011, Apple developed Siri, a virtual assistant that understands speech, and
    now, we have Amazon''s Alexa and Google Assistant too. Depending on the input
    type of a system, they can be classified into two groups: **spoken dialogue systems**
    and **text-based dialogue systems**, which are explained later in the chapter.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人通常作为文本服务为用户提供信息。例如，在西班牙，最受欢迎的对话代理之一是 Lola，它可以为你提供星座信息（[https://1millionbot.com/chatbot-lola/](https://1millionbot.com/chatbot-lola/)）。你只需发送一条消息，等待几秒钟即可收到数据。但在
    2011 年，苹果公司开发了 Siri，一款能理解语音的虚拟助手，现在我们也有了亚马逊的 Alexa 和谷歌助手。根据系统的输入类型，它们可以分为两类：**语音对话系统**和**基于文本的对话系统**，这些将在本章后面进行解释。
- en: This is not the only way to classify conversational agents. Depending on the
    type of knowledge they have, they can be divided into goal-oriented and open-domain.
    We will also review these classifications later in this chapter.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是唯一的对话代理分类方式。根据它们所拥有的知识类型，它们可以分为面向目标和开放领域两类。我们将在本章后面回顾这些分类。
- en: Actually, there are many tools for creating your own chatbot in a few minutes.
    But in this chapter, you will learn how to create the required system knowledge
    from scratch.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，有许多工具可以在几分钟内创建你自己的聊天机器人。但在本章中，你将学习如何从零开始创建所需的系统知识。
- en: Tools for Developing Chatbots
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发聊天机器人的工具
- en: 'Chatbots help a lot of many upcoming companies. But to create a chatbot, do
    you need to have knowledge of deep NLP? Well, thanks to these tools, a person
    without any NLP knowledge can create a chatbot in a matter of hours:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人对许多新兴公司帮助巨大。但是，创建聊天机器人时，你是否需要具备深度 NLP 的知识？幸运的是，借助这些工具，即使没有任何 NLP 知识的人，也可以在几个小时内创建一个聊天机器人：
- en: '**Dialogflow**: This easily creates a natural-language conversation. Dialogflow
    is a Google-owned developer that provides voice and conversational interfaces.
    This system uses Google''s machine learning expertise to find the appropriate
    intents in a dialogue with a user and is deployed on Google Cloud Platform. It
    supports more than 14 languages and multiple platforms.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dialogflow**: 它可以轻松创建自然语言对话。Dialogflow 是由 Google 拥有的开发工具，提供语音和对话界面。该系统利用
    Google 的机器学习技术来找出与用户对话中的适当意图，并且部署在 Google Cloud Platform 上。它支持 14 种以上的语言和多个平台。'
- en: '**IBM Watson**: Watson Assistant provides a user-friendly interface to create
    conversational agents. It works just like Dialogflow but it is deployed on IBM
    Cloud and it''s backed by IBM Watson knowledge. Watson also provides several tools
    to analyze data generated by conversations.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IBM Watson**: Watson Assistant 提供了一个用户友好的界面来创建对话代理。它的工作原理与 Dialogflow 类似，但它部署在
    IBM Cloud 上，并且由 IBM Watson 知识库支持。Watson 还提供了多个工具来分析对话生成的数据。'
- en: '**LUIS**: Language Understanding (LUIS) is a Microsoft machine learning-based
    service for building natural-language apps. This bot framework is hosted on the
    Azure cloud and uses Microsoft knowledge.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LUIS**: 语言理解（LUIS）是微软基于机器学习的服务，用于构建自然语言应用程序。这个机器人框架托管在 Azure 云上，并使用微软的知识库。'
- en: The aforementioned tools are a complex NLP system. In this chapter, we are going
    to look at a basic method for identifying the intent of a message using a pretrained
    GloVe. The latest chatbot trends are voice assistants. These tools allow you to
    implement a chatbot controlled by voice. There are many ways to classify a conversational
    agent.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 上述工具是一个复杂的 NLP 系统。在本章中，我们将介绍一种使用预训练的 GloVe 来识别消息意图的基本方法。最新的聊天机器人趋势是语音助手。这些工具使你能够实现一个由语音控制的聊天机器人。有许多方法可以对话代理进行分类。
- en: Types of Conversational Agents
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对话代理的类型
- en: Conversational agents can be classified into several groups, depending on the
    type of input-output data and their knowledge limits. When a company orders the
    creation of a chatbot, the first step is to analyze what its communication channel
    (text or voice) will be and what the topics of the conversation will be (limited
    knowledge or without restriction).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对话代理可以根据输入输出数据类型和知识限制进行多种分组。当一家公司委托创建聊天机器人时，第一步是分析其沟通渠道（文本或语音）以及对话的主题（有限知识或无限制）。
- en: Now, we are going to explain many types of groups and the features of each one.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将解释许多类型的分组及其各自的特点。
- en: Classification by Input-Output Data Type
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 按输入输出数据类型分类
- en: 'A voice-controlled virtual assistant is not like a basic chatbot, which we
    use text to communicate with. Depending on the input-output type, we can divide
    them into two groups:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 语音控制的虚拟助手不同于基础聊天机器人，我们使用文本进行沟通。根据输入输出类型，我们可以将它们分为两类：
- en: '**Spoken Dialogue System (SDS)**: These models are designed to be voice-controlled,
    without chat interfaces or keyboards, but with a microphone and speakers. These
    systems are harder to work with than a normal chatbot because they are composed
    of different modules:'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音对话系统 (SDS)**: 这些模型旨在通过语音控制，而没有聊天界面或键盘，而是通过麦克风和扬声器来工作。这些系统比普通聊天机器人更难处理，因为它们由不同的模块组成：'
- en: '![Figure 7.17: Structure of an SDS model](img/C13550_07_17.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.17：SDS 模型结构](img/C13550_07_17.jpg)'
- en: 'Figure 7.17: Structure of an SDS model'
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.17：SDS 模型结构
- en: Figure 7.17 shows the modules of an SDS. An SDS has a higher error probability,
    because speech-to-text systems need to transform the voice of a human into text,
    and this can fail. Once speech is converted into text, the conversational agent
    identifies the intent of the conversation and returns a response. Before the agent
    returns a response, the answer is converted to voice.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text-Based Dialogue System**: In contrast with an SDS, text-based dialogue
    systems are based on a chat interface, where the user interacts with the chatbot
    using a keyboard and a screen. In this chapter, we will be creating a text-based
    dialogue chatbot.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification by System Knowledge
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If the chatbot is able to successfully respond to every kind of message using
    its knowledge or if it is limited to a set of specific questions, these conversational
    agents can be divided as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '**Closed-Domain or Goal-Oriented (GO)**: The model has been trained to identify
    a set of intents. The chatbot will only understand sentences related to these
    topics. If the conversational agent does not identify the intent (intent was explained
    in the introduction to this chapter), it will return a predefined sentence.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open-Domain**: Not all chatbots have a set of defined intents. If the system
    can answer every type of sentence using NLG techniques and other data sources,
    it is classified as an open-domain model. The architecture of these systems is
    harder to build than a GO model.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a third class of conversational agent, based on its knowledge, that
    is, the **Hybrid Domain**. It is a combination of the models mentioned previously,
    therefore, depending on the sentence, the chatbot will have a predefined response
    (associated intent with many responses) or not.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creation of a Text-Based Dialogue System
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we already know the different classes of a conversational agent and
    how they can pick or generate a response. There are many other ways to build a
    conversational agent, and NLP provides many different approaches to achieve this
    objective. For example, **seq2seq** (sequence-to-sequence) models are able to
    find an answer when given a question. Also, deep language models can generate
    responses based on a corpus, that is, if a chatbot has a conversational corpus,
    it can follow a conversation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to build a chatbot using Stanford's GloVe. In
    *Exercise 26*, *Create Your First Conversational Agent*, you will find a brief
    introduction to the technique we are going to use, and in the activity, we will
    create a conversational agent to control a robot.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '**Scope definition and Intent Creation**'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Our conversational agent will be a text-based dialogue system and goal-oriented.
    Therefore, we will interact with our chatbot using the keyboard and it will only
    understand sentences related to intents created by us.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Before starting with the intent creation, we need to know what the main goal
    of our chatbot is (maintain a general dialogue, control a device, and obtain information)
    and what different types of sentences the users may ask.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Once we have analyzed the possible conversations of our chatbot, we can create
    the intents. Each intent will be a text file with several training sentences.
    These training sentences are possible interactions of a user with our chatbot.
    It is really important to define these sentences well because the chatbot could
    match the wrong intent if there are two similar sentences with different intents.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A good previous analysis of the possible conversations of our chatbot will make
    intent definition easier. It is obvious the chatbot will not understand all the
    sentences a user may say, but it must be able to recognize the meaning of sentences
    related to our intents.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'The system will also have a file with the same name as the intent file, but
    instead of containing the training sentences, it will have responses related to
    the intent:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18: Folder structure of the system](img/C13550_07_18.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.18: Folder structure of the system'
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In figure 7.18, we can see the structure of our chatbot. The extension of the
    intents and responses files are `.txt`, but you can also save them as `.json`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '**GloVe for Intent Detection**'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of this chapter, the fundamentals of word embeddings, word
    to vectors, and global vectors were reviewed. GloVe represents each word with
    a real-valued vector, and these vectors can be used as features in a variety of
    applications. But for this case – building a conversational agent – we are going
    to use complete sentences to train our chatbot, not just words.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: The chatbot needs to understand that an entire sentence is represented by a
    set of words as a vector. This representation of a sequence as a vector is called
    **seq2vec**. Internally, the conversational agent will compare the user's sentence
    with each intent training phrase to find the most similar meaning.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: At this point, there are vectors representing sequences, and these sequences
    are in a file related to an intent. If the same process mentioned previously is
    used to join all the sequence vectors into one, we will have a representation
    of the intent. The main idea is to not just represent a sentence; it is to represent
    a whole document in a vector, and this is called **Doc2vec**. With this approach,
    when the user interacts with the chatbot, it will find the intent of that user
    phrase.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'The final structure of our system will look as shown in figure 7.19:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19: Final folder structure](img/C13550_07_19.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.19: Final folder structure'
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The file named `main.py` will contain the different methods to analyze the
    input sentence using the GloVe model located in `/data`, creating the document
    vectors to perform the match between the user''s sentence and the intent:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20: Doc2Vec transformation](img/C13550_07_20.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.20: Doc2Vec transformation'
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Figure 7.20 shows the process of transforming a set of sentences in a vector,
    representing a document. In the example, the **A.txt** file is an intent with
    three sentences. Each sentence has three words, so each sentence has three vectors.
    Combining the vectors, we obtain a representation of each set of words, after
    which, we get the document vector.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: The approach of converting sentences into vectors allows a comparison of a sequence
    of vectors within a document vector without any problem. When the user interacts
    with the chatbot, the user phrase will be transformed as seq2vec and then it will
    be compared with each document vector to find the most similar one.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 26: Create Your First Conversational Agent'
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Perform exercise 26 in the same folder that you performed exercise 25 in.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you will create a chatbot to understand basic conversation.
    This exercise will cover the intent and response definition, the transformation
    of words into a vector, representing a document, and matching the user's sentence
    with the intent.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting the exercise, please take a look at the folder structure in
    Google Colab, as shown in figure 7.21:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21: Structure of Exercise 3](img/C13550_07_21.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.21: Structure of Exercise 26'
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `Exercise26.ipynb` file is the `main.py` file that we came across before,
    and within the `utils` folder, you will find the folder structure presented as
    mentioned in the previous exercise:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.22: Structure of Exercise 3 (2)](img/C13550_07_22.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.22: Structure of Exercise 26 (2)'
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The folder responses have the files with the phrases that the chatbot can output
    when the user interacts with it. Training is where intents are defined within
    sentences. To obtain the vectors of each word, we are going to use Stanford''s
    GloVe with five dimensions:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to define the intents and the responses for each intent. This
    is an introduction exercise, so let''s define three intents: welcome, how-are-you,
    and farewell, and create some related sentences (separated by commas).'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"Welcome" training sentences: Hi friend, Hello, Hi, Welcome.'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"Farewell" training sentences: Bye, Goodbye, See you, Farewell, Have a good
    day.'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"How are you" training sentences: How are you? What is going on? Are you okay?'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once we have the intents created, we will need the responses. Create three files
    with the same name as the intent files and add responses.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"Welcome" responses: Hello! Hi.'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"How are you?" responses: I''m good! Very good my friend :)'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"Farewell" responses: See you! Goodbye!'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import drive and mount it as follows:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Once you have mounted your drive for the first time, you will have to enter
    the authorization code by clicking on the URL mentioned by Google and pressing
    the **Enter** key on your keyboard:![Figure 7.23: The Google Colab authorization
    step](img/C13550_07_23.jpg)'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 7.23: The Google Colab authorization step'
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now that you have mounted the drive, you need to set the path of the directory:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Import the necessary libraries:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'With spaCy, we are going to tokenize the sentences and erase the punctuation
    marks. Now, create a function that tokenizes every sentence of a document. In
    this exercise, we will create the Doc2vec from the word vectors by combining all
    these vectors into one. That is why we are going to tokenize the whole document,
    returning an array with all the tokens. It is good practice to erase the stopwords
    too, but in this exercise it is not necessary. The input of this function is an
    array of sentences:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Load the GloVe model:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create two lists with the names of the intent files and the response files:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Create a function that returns a vector of 100 dimensions representing a document.
    The input of this function will be a list with the tokens of a document. We need
    to initialize an empty vector with 100 dimensions. What this function will perform
    is adding every vector word and then dividing it by the length of the tokenized
    document:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, we are ready to read each intent file (located in the training folder),
    tokenizing them, and creating an array with every document vector:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![Figure 7.24: Documents represented as vectors](img/C13550_07_24.jpg)'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.24: Documents represented as vectors'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With a function of `sklearn` called `cosine_similarity`, create a function
    that finds the most similar intent, comparing a sentence vector with each document
    vector:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s test our chatbot. Tokenize the input of the user and use the last function
    (`select_intent`) to obtain the related intent:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![Figure 7.25: Predicted document intent](img/C13550_07_25.jpg)'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.25: Predicted document intent'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Create a function that gives a response to the user:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Check for the output with the test sentence:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output will look like this:'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.26: Response of intent how_are_you](img/C13550_07_26.jpg)'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.26: Response of intent how_are_you'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Check whether the system works with many test sentences.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You have completed exercise 26! You are ready to build a conversational agent
    to control our virtual robot. As you saw in exercise 26 (step 2), you need a good
    definition of intents. If you try to add the same sentence in two different intents,
    the system could fail.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 7: Create a Conversational Agent to Control a Robot'
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will create a chatbot with many intents. To perform this
    activity, we will use Stanford's GloVe, as in *Exercise 26*, *Create Your First
    Conversational Agent*. We will learn how to create a program that waits for a
    user sentence, and when the user interacts with the chatbot, it will return a
    response.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario: You work in a company developing a security system. This security
    system will be a robot equipped with a camera to see the environment and wheels
    to move forward or backward. This robot will be controlled via text, so you can
    type orders and the robot will perform different actions.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'The robot can perform the following actions:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move forward.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Move backward.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Rotations:'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 45º to the right.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 45º to the left.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Identify what the robot can see. This activity is performed in the same way
    as in *Exercise 26*, *Create Your First Conversational Agent*. To avoid rewriting
    code, the `chatbot_intro.py` file has four basic methods:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Pre_processing`: To tokenize sentences'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Doc_vector`: To create document vectors'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Select_intent`: To find the most similar intent introduced in a sentence'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Send_response`: To send a sentence located in the response folder'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Knowing these methods, the core work is done, so the most important thing is
    the design of the intents.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We need to develop four different activities, but the rotation activity has
    two different types. We are going to define five intents, one per action (two
    for rotation). You can use these sentences, but you are free to add more training
    sentences or more actions:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Backward**:'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Move back
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Going backward
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Backward
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go back
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Moving backward
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Environment**:'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What can you see?
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Environment information
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Take a picture
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tell me what you are seeing?
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What do you have in front of you?
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Forward**:'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Advance
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Move forward
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go to the front
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Start moving
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Forward
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Left**:'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Turn to the left
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go left
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Look to the left
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Turn left
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Left
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Right**:'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Turn to the right
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go right
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Look to the right
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Turn right
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Right
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can find the files in the activity/training folder:'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.27: Training sentence files](img/C13550_07_27.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.27: Training sentence files'
  id: totrans-345
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity is available on page 323.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Conversational agents, also knowns as chatbots, are text-based dialogue systems
    that understand human language in order to hold a "real" conversation with people.
    To achieve a good understanding of what a human is saying, chatbots need to classify
    dialogue into intents, that is, a set of sentences representing a meaning. Conversational
    agents can be classified into several groups, depending on the type of input-output
    data and knowledge limits. This representation of meaning is not easy. To have
    sound knowledge supporting a chatbot, a huge corpus is needed. Finding the best
    way to represent a word is a challenge, and one-hot encoding is useless. The main
    problem with one-hot encoding is the size of the encoded vectors. If we have a
    corpus of 88,000 words, then the vectors will have a size of 88,000, and without
    any relationship between the words. This is where the concept of word embeddings
    enters the picture.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings are a collection of techniques and methods to map words and
    sentences from a corpus into vectors or real numbers. Word embeddings generate
    a representation of each word in terms of the context in which a word appears.
    To generate word embeddings, we can use Word2Vec. Word2Vec processes a corpus
    and assigns a vector to each unique word in the corpus, and it can perform dimension
    reduction, typically of several hundred dimensions.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'The core idea of Word2Vec is that a word''s meaning is given by the words that
    are frequently found near to it. When a word appears in a sentence, its context
    is formed by the set of words it has nearby. Word2Vec can be implemented using
    two types of algorithm: skip-gram and CBOW. The idea of Word2Vec is to represent
    words which is useful, but in terms of efficiency, it has problems. GloVe combines
    Word2Vec and the statistical information of a corpus. GloVe joins these two approaches
    to achieve fast training, scalable to huge corpora, and achieve better performance
    with small vectors. With GloVe, we are capable of giving knowledge to our chatbot,
    combined with training sentences defining our set of intents.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec的核心思想是，一个词的意义由其附近经常出现的词语所决定。当一个词出现在句子中时，它的上下文是由其附近的词语集合形成的。Word2Vec可以通过两种算法实现：skip-gram和CBOW。Word2Vec的思想是表示词语，这是有用的，但在效率方面存在问题。GloVe将Word2Vec和语料库的统计信息结合起来。GloVe将这两种方法结合，达到快速训练、可扩展到大规模语料库，并通过小向量实现更好的性能。通过GloVe，我们能够为聊天机器人提供知识，并结合定义我们意图集的训练句子。
- en: '*Chapter 8*, *Object Recognition to Guide the Robot Using CNNs*, will introduce
    you to object recognition using different pretrained models. Furthermore, it will
    look at the latest trend in computer vision – the recognition of objects using
    boxes identifying what is in every part of a picture.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '*第8章*，*使用CNN进行物体识别以引导机器人*，将向你介绍如何使用不同的预训练模型进行物体识别。此外，它还将探讨计算机视觉中的最新趋势——通过框架识别图片中每个部分的物体。'
