<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Performing Anomaly Detection on Unsupervised Data</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this chapter, we will perform anomaly detection with the <strong>Modified National Institute of Standards and Technology</strong> (<strong>MNIST</strong>) dataset using a simple autoencoder without any pretraining. We will identify the outliers in the given MNIST data. Outlier digits can be considered as most untypical or not normal digits. We will encode the MNIST data and then decode it back in the output layer. Then, we will calculate the reconstruction error for the MNIST data.<br/></span></p>
<p class="mce-root"><span>T</span>he MNIST<span> sample that closely resembles a digit value will have low reconstruction error. We will then sort them based on the reconstruction errors and then display the best samples and the worst samples (outliers) using the JFrame window. The autoencoder is constructed using a feed-forward network. Note that we are not performing any pretraining. We can process feature inputs in an autoencoder and we won't require MNIST labels at any stage.</span><span><br/></span></p>
<p class="mce-root"><span>In this chapter, we will cover the following recipes:<br/></span></p>
<ul>
<li>Extracting and preparing MNIST data</li>
<li>Constructing dense layers for input</li>
<li>Constructing output layers</li>
<li>Training with MNIST images</li>
<li>Evaluating and sorting the results based on the anomaly score</li>
<li>Saving the resultant model</li>
</ul>
<p>Let's begin.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>The code for this chapter can be found here: <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/08_Performing_Anomaly_detection_on_unsupervised%20data/sourceCode/cookbook-app/src/main/java/MnistAnomalyDetectionExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/08_Performing_Anomaly_detection_on_unsupervised%20data/sourceCode/cookbook-app/src/main/java/MnistAnomalyDetectionExample.java</a>.</p>
<p>The JFrame-specific implementation can be found here:<br/>
<a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/08_Performing_Anomaly_detection_on_unsupervised%20data/sourceCode/cookbook-app/src/main/java/MnistAnomalyDetectionExample.java#L134">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/08_Performing_Anomaly_detection_on_unsupervised%20data/sourceCode/cookbook-app/src/main/java/MnistAnomalyDetectionExample.java#L134</a>.</p>
<p>After cloning our GitHub repository, navigate to the<kbd> Java-Deep-Learning-Cookbook/08_Performing_Anomaly_detection_on_unsupervised data/sourceCode</kbd> directory. Then, import the <kbd>cookbook-app</kbd> project as a Maven project by importing <kbd>pom.xml</kbd>.</p>
<p>Note that we use the MNIST dataset from here: <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>.</p>
<p>However, we don't have to download the dataset for this chapter: DL4J has a custom implementation that allows us to fetch MNIST data automatically. We will be using this in this chapter. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting and preparing MNIST data</h1>
                </header>
            
            <article>
                
<p>Unlike supervised image classification use cases, we will perform an anomaly detection task on the MNIST dataset. On top of that, we are using an unsupervised model, which means that we will not be using any type of label to perform the training process. To start the ETL process, we will extract this unsupervised MNIST data and prepare it so that it is usable for neural network training. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create iterators for the MNIST data using <kbd>MnistDataSetIterator</kbd>: </li>
</ol>
<pre style="padding-left: 60px">DataSetIterator iter = new MnistDataSetIterator(miniBatchSize,numOfExamples,binarize);</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Use <kbd>SplitTestAndTrain</kbd> to split the base iterator into train/test iterators:</li>
</ol>
<pre style="padding-left: 60px">DataSet ds = iter.next();<br/> SplitTestAndTrain split = ds.splitTestAndTrain(numHoldOut, new Random(12345));</pre>
<ol start="3">
<li>Create lists to store the feature sets from the train/test iterators:</li>
</ol>
<pre style="padding-left: 60px">List&lt;INDArray&gt; featuresTrain = new ArrayList&lt;&gt;();<br/> List&lt;INDArray&gt; featuresTest = new ArrayList&lt;&gt;();<br/> List&lt;INDArray&gt; labelsTest = new ArrayList&lt;&gt;();</pre>
<ol start="4">
<li>Populate the values into the feature/label lists that were previously created:</li>
</ol>
<pre style="padding-left: 60px">featuresTrain.add(split.getTrain().getFeatures());<br/> DataSet dsTest = split.getTest();<br/> featuresTest.add(dsTest.getFeatures());<br/> INDArray indexes = Nd4j.argMax(dsTest.getLabels(),1);<br/> labelsTest.add(indexes);</pre>
<ol start="5">
<li>Call <kbd>argmax()</kbd> for every iterator instance to convert the labels to one dimensional data if it's multidimensional:</li>
</ol>
<pre style="padding-left: 60px">while(iter.hasNext()){<br/> DataSet ds = iter.next();<br/> SplitTestAndTrain split = ds.splitTestAndTrain(80, new Random(12345)); // 80/20 split (from miniBatch = 100)<br/> featuresTrain.add(split.getTrain().getFeatures());<br/> DataSet dsTest = split.getTest();<br/> featuresTest.add(dsTest.getFeatures());<br/> INDArray indexes = Nd4j.argMax(dsTest.getLabels(),1);<br/> labelsTest.add(indexes);<br/> }<strong><br/></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>I</span>n step 1, w<span>e have used <kbd>MnistDataSetIterator</kbd> to extract and load MNIST data in one place. DL4J comes with this specialized iterator to load MNIST data without having to worry about downloading the data on your own. You might notice that MNIST data on the official website follows the </span><kbd>ubyte</kbd><span> format. This is certainly not the desired format, and we need to extract all the images separately to load them properly on the neural network.</span></p>
<p class="mce-root"/>
<p><span>Therefore, it is very convenient to have an MNIST iterator implementation such as </span><kbd>MnistDataSetIterator</kbd><span> in DL4J. It simplifies the typical task of handling MNIST data in the </span><kbd>ubyte</kbd><span> format. </span>MNIST data has a total of 60,000 training digits, 10,000 test digits, and 10 labels. Digit images have a dimension of 28 x 28, the shape of the data is in a flattened format: [<kbd>minibatch</kbd>, 784]. <kbd>MnistDataSetIterator</kbd> internally uses the <kbd>MnistDataFetcher</kbd> and <kbd>MnistManager</kbd> classes to fetch the MNIST data and load them into the proper format. In step 1, <kbd>binarize</kbd><span>: </span><kbd>true</kbd><span> or </span><kbd>false</kbd><span> indicates whether to binarize the MNIST data.</span></p>
<p><span>Note t</span>hat in step 2<span>,</span> <kbd>numHoldOut</kbd><span> indicates the number of samples to be held for training. If </span><kbd>miniBatchSize</kbd><span> is </span><kbd>100</kbd><span> and </span><span><kbd>numHoldOut</kbd> is <kbd>80</kbd>, then the remaining 20 samples are meant for testing and evaluation. </span><span>We can use </span><kbd>DataSetIteratorSplitter</kbd><strong> </strong><span>instead of </span><kbd>SplitTestAndTrain</kbd><span> for splitting of data, as mentioned</span> in step 2. </p>
<p>In step 3, <span>we created lists to maintain the features and labels with respect to training and testing. W</span><span>e need them for the training and evaluation stages, respectively. We also created a list to store labels from the test set to map the outliers with labels during the test and evaluation phases. </span><span>These lists are populated once in every occurrence of a batch. For example, in the case of <kbd>featuresTrain</kbd> or <kbd>featuresTest</kbd>, a batch of features (after data splitting) is represented by an</span><span> </span><kbd>INDArray</kbd><span> </span><span>item. W</span><span>e have also used an</span> <kbd>argMax()</kbd> <span>function from ND4J. This </span><span>converts the labels array into a one-dimensional array. MNIST labels from </span><kbd>0</kbd><span> to </span><kbd>9</kbd><span> effectively need just one-dimensional space for representation. </span></p>
<p>In the following code, <kbd>1</kbd><span><strong> </strong>denotes the dimension:</span></p>
<pre>Nd4j.argMax(dsTest.getLabels(),1);</pre>
<p><span>Also, note that we use the labels for mapping outliers to labels and not for training.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constructing dense layers for input</h1>
                </header>
            
            <article>
                
<p>The core of the neural network design is the layer architecture. For autoencoders, we need to design dense layers that do encoding at the front and decoding at the other end. Basically, we are reconstructing the inputs in this way. Accordingly, we need to make our layer design.</p>
<p><span>Let's start configuring our autoencoder using the default settings and then proceed further by defining the necessary input layers for our autoencoder. Remember that the number of incoming connections to the neural network will be equal to the number of outgoing connections from the neural network.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Use <kbd>MultiLayerConfiguration</kbd> to construct the autoencoder network:</li>
</ol>
<pre style="padding-left: 60px">NeuralNetConfiguration.Builder configBuilder = new NeuralNetConfiguration.Builder();<br/> configBuilder.seed(12345);<br/> configBuilder.weightInit(WeightInit.XAVIER);<br/> configBuilder.updater(new AdaGrad(0.05));<br/> configBuilder.activation(Activation.RELU);<br/> configBuilder.l2(l2RegCoefficient);<br/> NeuralNetConfiguration.ListBuilder builder = configBuilder.list();</pre>
<ol start="2">
<li>Create input layers using <kbd>DenseLayer</kbd>:</li>
</ol>
<pre style="padding-left: 60px">builder.layer(new DenseLayer.Builder().nIn(784).nOut(250).build());<br/> builder.layer(new DenseLayer.Builder().nIn(250).nOut(10).build());<strong><br/></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, while configuring generic neural network parameters, we set the default learning rate as shown here:</p>
<pre>configBuilder.updater(new AdaGrad(learningRate));</pre>
<p>The <kbd>Adagrad</kbd> optimizer is based on how frequently a parameter gets updated during training. <kbd>Adagrad</kbd> is based on a vectorized learning rate. <span>The learning rate will be small when there are many updates received. This is crucial for high-dimensional problems. Hence, this optimizer can be a good fit for our autoencoder use case.</span></p>
<p><span>We are performing dimensionality reduction at the input layers in an autoencoder architecture. This is also known as encoding the data. We want to ensure that the same set of features are decoded from the encoded data. We calculate reconstruction errors to measure how close we are compared to the real feature set before encoding. In</span> step 2, we ar<span>e trying to encode the data from a higher dimension (<kbd>784</kbd>) to a lower dimension (<kbd>10</kbd>). </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constructing output layers</h1>
                </header>
            
            <article>
                
<p>As a final step, we need to decode the data back from the encoded state. Are we able to reconstruct the input just the way it is? If yes, then it's all good. Otherwise, we need to calculate an associated reconstruction error. <span>Remember that the incoming connections to the output layer should be the same as the outgoing connections from the preceding layer. </span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create an output layer using <kbd>OutputLayer</kbd>:</li>
</ol>
<pre style="padding-left: 60px">OutputLayer outputLayer = new OutputLayer.Builder().nIn(250).nOut(784)<br/> .lossFunction(LossFunctions.LossFunction.MSE)<br/> .build();</pre>
<ol start="2">
<li>Add <kbd>OutputLayer</kbd> to the layer definitions:</li>
</ol>
<pre style="padding-left: 60px">builder.layer(new OutputLayer.Builder().nIn(250).nOut(784)<br/> .lossFunction(LossFunctions.LossFunction.MSE)<br/> .build());</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>We have mentioned the <strong>mean square error</strong><span> (</span><strong>MSE</strong><span>) as the error function associated with the output layer. </span><kbd>lossFunction</kbd>, which is used in autoencoder architecture, is MSE in most cases. MSE is optimal in calculating how close the reconstructed input is to the original input. ND4J has an implementation for MSE, which is <kbd>LossFunction.MSE</kbd>.</p>
<p><span>In the output layer, we get the reconstructed input in their original dimensions.</span> <span>We will then use an error function to calculate the reconstruction error.</span> In step 1, we're constructing an output layer that calculates the reconstruction error for anomaly detection. It is important to keep the incoming and outgoing connections the same at the input and output layers, respectively. Once the output layer definition is created, we need to add it to a stack of layer configurations that is maintained to create the neural network configuration. In step 2, <span>we added the output layer to the previously maintained neural network configuration builder. In order to follow an intuitive approach, we have created configuration builders first, unlike the straightforward approach here:</span><span> <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/08_Performing_Anomaly_detection_on_unsupervised%20data/sourceCode/cookbook-app/src/main/java/MnistAnomalyDetectionExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/08_Performing_Anomaly_detection_on_unsupervised%20data/sourceCode/cookbook-app/src/main/java/MnistAnomalyDetectionExample.java</a>.</span></p>
<p><span>You can obtain a configuration instance by calling the <kbd>build()</kbd> method on the</span> <kbd>Builder</kbd> <span>instance. </span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training with MNIST images</h1>
                </header>
            
            <article>
                
<p>Once the layers are constructed and the neural network is formed, we can initiate the training session. During the training session, we reconstruct the input multiple times and evaluate the reconstruction error. In previous recipes, we completed the autoencoder network configuration by defining the input and output layers as required. Note that we are going to train the network with its own input features, not the labels. Since we use an autoencoder for anomaly detection, we encode the data and then decode it back to measure the reconstruction error. Based on that, we list the most probable anomalies in MNIST data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Choose the correct training approach. Here is what is expected to happen during the training instance:</li>
</ol>
<pre style="padding-left: 60px"> Input -&gt; Encoded Input -&gt; Decode -&gt; Output</pre>
<p style="padding-left: 60px">So, we need to train output against input (output ~ input, in an ideal case).</p>
<ol start="2">
<li>Train every feature set using the <kbd>fit()</kbd> method: </li>
</ol>
<pre style="padding-left: 60px">int nEpochs = 30;<br/> for( int epoch=0; epoch&lt;nEpochs; epoch++ ){<br/> for(INDArray data : featuresTrain){<br/> net.fit(data,data);<br/> }<br/> }<strong><br/></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The <kbd>fit()</kbd> method accepts both features and labels as attributes for the first and second attributes, respectively. We reconstruct the MNIST features against themselves. In other words, we are trying to recreate the features once they are encoded and check how much they vary from actual features. <span><span>We measure the reconstruction error during training and bother only about the feature values. So, the output is validated against the input and resembles how an autoencoder functions. So</span></span>, step 1 is cruci<span><span>al for the evaluation stage as well.</span></span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span><span>Refer to this block of code:</span></span></p>
<pre>for(INDArray data : featuresTrain){<br/> net.fit(data,data);<br/>}</pre>
<p>That's the reason why we train the autoencoder against its own features (inputs) as we call <kbd>fit()</kbd> in this way: <kbd>net.fit(data,data)</kbd> in step 2.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating and sorting the results based on the anomaly score</h1>
                </header>
            
            <article>
                
<p>We need to calculate the reconstruction error for all the feature sets. Based on that, we will find the outlier data for all the MNIST digits (0 to 9). Finally, we will display the outlier data in the JFrame window. We also need feature va<span>lues from a test set for the evaluation. We also need label values from the test set, not for evaluation, but for mapping anomalies with labels. Then, we can plot outlier data against each label. The labels are only used for plotting outlier data in JFrame against respective labels. In this recipe, we evaluate the trained autoencoder model for MNIST anomaly detection, and then sort the results and display them.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Compose a map that relates each MNIST digit to a list of (score, feature) pairs:</li>
</ol>
<pre style="padding-left: 60px">Map&lt;Integer,List&lt;Pair&lt;Double,INDArray&gt;&gt;&gt; listsByDigit = new HashMap&lt;&gt;();</pre>
<ol start="2">
<li>Iterate through each and every test feature, calculate the reconstruction error, make a score-feature pair for the purpose of displaying the sample with a low reconstruction error:</li>
</ol>
<pre style="padding-left: 60px">for( int i=0; i&lt;featuresTest.size(); i++ ){<br/> INDArray testData = featuresTest.get(i);<br/> INDArray labels = labelsTest.get(i);<br/> for( int j=0; j&lt;testData.rows(); j++){<br/> INDArray example = testData.getRow(j, true);<br/> int digit = (int)labels.getDouble(j);<br/> double score = net.score(new DataSet(example,example));<br/> // Add (score, example) pair to the appropriate list<br/> List digitAllPairs = listsByDigit.get(digit);<br/> digitAllPairs.add(new Pair&lt;&gt;(score, example));<br/> }<br/> }</pre>
<ol start="3">
<li>Create a custom comparator to sort the map:</li>
</ol>
<pre style="padding-left: 60px">Comparator&lt;Pair&lt;Double, INDArray&gt;&gt; sortComparator = new Comparator&lt;Pair&lt;Double, INDArray&gt;&gt;() {<br/> @Override<br/> public int compare(Pair&lt;Double, INDArray&gt; o1, Pair&lt;Double, INDArray&gt; o2) {<br/> return Double.compare(o1.getLeft(),o2.getLeft());<br/> }<br/> };</pre>
<ol start="4">
<li>Sort the map using <kbd>Collections.sort()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">for(List&lt;Pair&lt;Double, INDArray&gt;&gt; digitAllPairs : listsByDigit.values()){<br/> Collections.sort(digitAllPairs, sortComparator);<br/> }</pre>
<ol start="5">
<li>Collect the best/worst data to display in a JFrame window for visualization:</li>
</ol>
<pre style="padding-left: 60px">List&lt;INDArray&gt; best = new ArrayList&lt;&gt;(50);<br/> List&lt;INDArray&gt; worst = new ArrayList&lt;&gt;(50);<br/> for( int i=0; i&lt;10; i++ ){<br/> List&lt;Pair&lt;Double,INDArray&gt;&gt; list = listsByDigit.get(i);<br/> for( int j=0; j&lt;5; j++ ){<br/> best.add(list.get(j).getRight());<br/> worst.add(list.get(list.size()-j-1).getRight());<br/> }<br/> }</pre>
<ol start="6">
<li>Use a custom JFrame implementation for visualization, such as <kbd>MNISTVisualizer</kbd>, to visualize the results:</li>
</ol>
<pre style="padding-left: 60px">//Visualize the best and worst digits<br/> MNISTVisualizer bestVisualizer = new MNISTVisualizer(imageScale,best,"Best (Low Rec. Error)");<br/> bestVisualizer.visualize();<br/> MNISTVisualizer worstVisualizer = new MNISTVisualizer(imageScale,worst,"Worst (High Rec. Error)");<br/> worstVisualizer.visualize();</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>U</span>sing step 1 and step 2, <span>for every MNIST digit, we maintain a list of </span>(score, feature)<span> </span><span>pairs. W</span>e <span>composed a map that relates each MNIST digit to this list of pairs. In the end, we just have to sort it to find the best/worst cases.</span></p>
<p>Also, we used the <kbd>score()</kbd> function to calculate the reconstruction error:</p>
<pre>double score = net.score(new DataSet(example,example));</pre>
<p>During the evaluation, we reconstruct the test features and measure how much it differs from actual feature values. A high reconstruction error indicates the presence of a high percentage of outliers.</p>
<p>After step 4, we should see JFrame visualization for <span>reconstruction errors, </span>as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1180 image-border" src="assets/48f96597-f613-4198-8ebf-2e049ba79953.png" style="width:34.25em;height:36.08em;"/></p>
<p class="mce-root"/>
<p class="mce-root CDPAlignLeft CDPAlign">Visualization is JFrame dependent. Basically, what we do is take the <em>N</em> best/worst pairs from the previously created map in step 1. We make a list of best/worst data and pass it to our JFrame visualization logic to display the outlier in the JFrame window. The JFrame window on the right side represents the outlier data. We are leaving the JFrame implementation aside as it is beyond the scope for this book. For the complete JFrame im<span>plementation, refer to GitHub source mentioned in the <em>Technical requirements</em> section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Saving the resultant model</h1>
                </header>
            
            <article>
                
<p>Model persistence is very important as it enables the reuse of neural network models without having to train more than once. <span>Once the autoencoder is trained to perform outlier detection, we can save the model to the disk for later use. We explained the </span><kbd>ModelSerializer</kbd><span> </span><span>class in a previous chapter. We use this to save the autoencoder model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Use <kbd>ModelSerializer</kbd> to persist the model:</li>
</ol>
<pre style="padding-left: 60px">File modelFile = new File("model.zip");<br/> ModelSerializer.writeModel(multiLayerNetwork,file, saveUpdater);</pre>
<ol start="2">
<li>Add a normalizer to the persisted model:</li>
</ol>
<pre style="padding-left: 60px">ModelSerializer.addNormalizerToModel(modelFile,dataNormalization);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>We officially target the DL4J version 1.0.0-beta 3 in this chapter. We used <kbd>ModelSerializer</kbd> to save the models to disk. If you use the new version, 1.0.0-beta 4, there is another recommended way to save the model by using the <kbd>save()</kbd> method offered by <kbd>MultiLayerNetwork</kbd>:</p>
<pre>File locationToSave = new File("MyMultiLayerNetwork.zip");<br/>   model.save(locationToSave, saveUpdater);</pre>
<p>Use <kbd>saveUpdater = true</kbd> if you want to train the network in the future.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>To restore the network model, call the <kbd>restoreMultiLayerNetwork()</kbd><span> </span>method:</p>
<pre>ModelSerializer.restoreMultiLayerNetwork(new File("model.zip"));</pre>
<p>Additionally, if you use the latest version, 1.0.0-beta 4, you can use the<span> </span><kbd>load()</kbd><span> </span>method offered by<span> </span><kbd>MultiLayerNetwork</kbd>:</p>
<pre>MultiLayerNetwork restored = MultiLayerNetwork.load(locationToSave, saveUpdater);</pre>


            </article>

            
        </section>
    </body></html>