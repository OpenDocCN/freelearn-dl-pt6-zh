- en: Advanced NLP Recipes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will go through the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an NLP pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving the text similarity problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarizing text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resolving anaphora
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disambiguating word sense
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring advanced sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a conversational assistant or chatbot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how to process input text, identify parts of speech, and
    extract important information (named entities). We've learned a few computer science
    concepts also, such as grammars, parsers, and so on. In this chapter, we will
    dig deeper into advanced topics in **natural language processing** (**NLP**),
    which need several techniques to properly understand and solve them.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an NLP pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In computing, a pipeline can be thought of as a multi-phase data flow system
    where the output from one component is fed to the input of another component.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the things that happen in a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Data is flowing all the time from one component to another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The component is a black box that should worry about the input data and output
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A well-defined pipeline takes care of the following things.
  prefs: []
  type: TYPE_NORMAL
- en: The input format of the data that is flowing through each of the components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output format of the data that is coming out of each of the components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making sure that data flow is controlled between components by adjusting the
    velocity of data inflow and outflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if you are familiar with Unix/Linux systems and have some exposure
    to working on a shell, you'd have seen the | operator, which is the shell's abstraction
    of a data pipe. We can leverage the | operator to build pipelines in the Unix
    shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example in Unix (for a quick understanding): how do I find the
    number of files in a given directory ?'
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this, we need the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: We need a component (or a command in the Unix context) that reads the directory
    and lists all the files in it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need another component (or a command in the Unix context) that reads the
    lines and prints the count of lines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, we have the solutions to these two requirements. Which are :'
  prefs: []
  type: TYPE_NORMAL
- en: The `ls` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `wc` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we can build a pipeline where we take the output from `ls` and feed it to
    `wc`, we are done.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of Unix commands, `ls -l  | wc -l` is a simple pipeline that counts
    the files in a directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this knowledge, let''s get back to the NLP pipeline requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Input data acquisition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breaking the input data into words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the POS of words in the input data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting the named entities from the words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the relationships between named entities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, let's try to build the simplest possible pipeline; it acquires
    data from a remote RSS feed and then prints the identified named entities in each
    document.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have Python installed, along with the `nltk`, `queue`, `feedparser`,
    and `uuid` libraries.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open Atom editor (or your favorite programming editor).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new file called `PipelineQ.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following source code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c2691bf7-2c41-45ec-b6fb-a508383a6e71.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the program using the Python interpreter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see this output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2432e7f0-57ca-4b03-99d9-b1ac745705af.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build this pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'These five instructions import five Python libraries into the current program:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nltk`: Natural language toolkit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threading`: A threading library used to create lightweight tasks within a
    single program'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`queue`: A queue library that can be used in a multi-threaded program'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feedparser`: An RSS feed parsing library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`uuid`: An RFC-4122-based uuid version 1, 3, 4, 5-generating library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating a new empty list to keep track of all the threads in the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This instruction creates a list of two queues in a variable `queue`?
  prefs: []
  type: TYPE_NORMAL
- en: 'Why do we need two queues:'
  prefs: []
  type: TYPE_NORMAL
- en: The first queue is used to store tokenized sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second queue is used to store all the POS analyzed words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This instruction defines a new function, `extractWords()`, which reads a sample
    RSS feed from the internet and stores the words, along with a unique identifier
    for this text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We are defining a sample URL (entertainment news) from the India Times website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction invokes the `parse()` function of the `feedparser` library.
    This `parse()` function downloads the content of the URL and converts it into
    a list of news items. Each news item is a dictionary with title and summary keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We are taking the first five entries from the RSS feed and storing the current
    item in a variable called `entry`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The title of the current RSS feed item is stored in a variable called `text`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction skips the titles that contain sensitive words. Since we are
    reading the data from the internet, we have to make sure that the data is properly
    sanitized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Break the input text into words using the `word_tokenize()` function and store
    the result into a variable called `words`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a dictionary called `data` with two key-value pairs, where we are storing
    the UUID and input words under the UUID and input keys respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction stores the dictionary in the first queue, `queues[0]`. The
    second argument is set to true, which indicates that if the queue is full, pause
    the thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'A well-designed pipeline understands that it should control the inflow and
    outflow of the data according to the component''s computation capacity. If not,
    the entire pipeline collapses. This instruction prints the current RSS item that
    we are processing along with its unique ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction defines a new function called `extractPOS()`, which reads
    from the first queue, processes the data, and saves the POS of the words in the
    second queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an infinite loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'These instructions check whether the first queue is empty. When the queue is
    empty, we stop processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to make this program robust, pass the feedback from the first queue.
    This is left as an exercise to the reader. This is the else part, which indicates
    there is some data in the first queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Take the first item from the queue (in FIFO order):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the parts of speech in the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the first queue, mentioning that we are done with processing the item
    that is just extracted by this thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the POS tagged word list in the second queue so that the next phase in
    the pipeline will execute things. Here also, we are using true for the second
    parameter, which will make sure that the thread will wait if there is no free
    space in the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction defines a new function, `extractNE()`, which reads from the
    second queue, processes the POS tagged words, and prints the named entities on
    screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an infinite loop instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If the second queue is empty, then we exit the infinite loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction picks an element from the second queue and stores it in a data variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction marks the completion of data processing on the element that
    was just picked from the second queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction extracts the named entities from the `postags` variable and
    stores it in a variable called `chunks`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: These instructions do the following
  prefs: []
  type: TYPE_NORMAL
- en: Print the UUID from the data dictionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterate over all chunks that are identified
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are using a try/except block because not all elements in the tree have the `label()`
    function (they are tuples when no NE is found)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we call a `print()` function, which prints a newline on screen
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This instruction defines a new function, `runProgram`, which does the pipeline
    setup using threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'These three instructions create a new thread with `extractWords()` as the function,
    start the thread and add the thread object (`e`) to the list called `threads`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'These instructions create a new thread with `extractPOS()` as the function,
    start the thread, and add the thread object (`p`) to the list variable `threads`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'These instructions create a new thread using `extractNE()` for the code, start
    the thread, and add the thread object (`n`) to the list `threads`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'These two instructions release the resources that are allocated to the queues
    once all the processing is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'These two instructions iterate over the threads list, store the current thread
    object in a variable, `t`, call the `join()` function to mark the completion of
    the thread, and release resources allocated to the thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the section of the code that is invoked when the program is run with
    the main thread. The `runProgram()` is called, which simulates the entire pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Solving the text similarity problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The text similarity problem deals with the challenge of finding how close given
    text documents are. Now, when we say close, there are many dimensions in which
    we can say they are closer or far:'
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment/emotion dimension
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sense dimension
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mere presence of certain words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many algorithms available for this; all of them vary in the degree
    of complexity, the resources needed, and the volume of data we are dealing with.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will use the TF-IDF algorithm to solve the similarity problem.
    So first, let''s understand the basics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Term frequency (TF)**: This technique tries to find the relative importance
    (or frequency) of the word in a given document'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we are talking about relative importance, we generally normalize the frequency
    with respect to the total words that are present in the document to compute the
    TF value of a word.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inverse document frequency (IDF)** : This technique makes sure that words
    that are frequently used (a, the, and so on) should be given lower weight when
    compared to the words that are rarely used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since both TF and IDF values are decomposed to numbers (fractions), we will
    do a multiplication of these two values for each term against every document and
    build *M* vectors of *N* dimensions (where *N* is the total number of documents
    and *M* are the unique words in all the documents).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have these vectors, we need to find the cosine similarity using the
    following formula on these vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4acc3f1a-3759-4c0a-acbe-2e297e6f41be.png)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have Python installed, along with the `nltk` and `scikit` libraries.
    Having some understanding of mathematics is helpful.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open atom editor (or your favorite programming editor).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new file called `Similarity.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following source code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/21458fab-7f37-42a8-8e06-5e6a227a7eed.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the program using the Python interpreter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f9774ed3-f05c-43c4-9fb6-49a8bba75d10.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we are solving the text similarity problem. These four instructions
    import the necessary libraries that are used in the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We are defining a new class, `TextSimilarityExample`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction defines a new constructor for the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This instruction defines sample sentences on which we want to find the similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We are defining the TF of all the words in a given sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This function does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Converts the sentence to lower case and extracts all the words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finds the frequency distribution of these words using the nltk `FreqDist` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterates over all the dictionary keys, builds the normalized floating values,
    and stores them in a dictionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the dictionary that contains the normalized score for each word in the
    sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are defining an IDF that finds the IDF value for all the words in all the
    documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This function does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: We define a local function called `idf()`, which is the formula to find the
    IDF of a given word
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We iterate over all the statements and convert them to lowercase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find how many times each word is present across all the documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build the IDF value for all words and return the dictionary containing these
    IDF values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are now defining a `TF_IDF` (TF multiplied by IDF) for all the documents
    against a given search string.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what we are doing here:'
  prefs: []
  type: TYPE_NORMAL
- en: Break the search string into tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build `IDF()` for all sentences in the `self.statements` variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterate over all sentences and find the TF for all words in this sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter and use only the words that are present in the input search string and
    build vectors that consist of *tf*idf* values against each document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return the list of vectors for each word in the search query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This function displays the contents of vectors on screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Now, in order to find the similarity, as we discussed initially, we need to
    find the Cosine similarity on all the input vectors. We can do all the math ourselves.
    But this time, let's try to use scikit to do all the computations for us.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In the previous functions, we learned how to build TF and IDF values and finally
    get the TF x IDF values for all the documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what we are doing here:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a new function: `cosineSimilarity()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a new vectorizer object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a matrix of TF-IDF values for all the documents that we are interested
    in, using the `fit_transform()` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Later we compare each document with all other documents and see how close they
    are to each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the `demo()` function and it runs all the other functions we have defined
    before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Let's see what we are doing here
  prefs: []
  type: TYPE_NORMAL
- en: We take the first statement as our input query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We build vectors using our own handwritten `TF_IDF()` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We display our TF x IDF vectors for all sentences on screen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We print the cosine similarity computed for all the sentences using the `scikit`
    library by invoking the `cosineSimilarity()` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are creating a new object for the `TextSimilarityExample()` class and then
    invoking the `demo()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Identifying topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to do document classification. Beginners
    might think document classification and topic identification are the same, but
    there is a slight difference.
  prefs: []
  type: TYPE_NORMAL
- en: Topic identification is the process of discovering topics that are present in
    the input document set. These topics can be multiple words that occur uniquely
    in a given text.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example. When we read arbitrary text that contains a mention of
    Sachin Tendulkar, score, win we can understand that the sentence is describing
    cricket. But we may be wrong as well.
  prefs: []
  type: TYPE_NORMAL
- en: In order to find all these types of topics in a given input text, we use the Latent
    Dirichlet allocation algorithm (we could use TF-IDF as well, but since we have
    already explored it in a previous recipe, let's see how LDA works in identifying
    the topic).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have Python installed, along with the `nltk`, `gensim`, and `feedparser`
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open atom editor (or your favorite programming editor).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new file called `IdentifyingTopic.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following source code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0b4aa55c-b327-4ad5-aa01-d0e95e7a21bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the program using the Python interpreter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c3ba63c9-117a-4bb1-a6a1-0d78dfb1869f.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how the topic identification program works. These five instructions
    import the necessary libraries into the current program.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction defines a new class, `IdentifyingTopicExample`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction defines a new function, `getDocuments()`, whose responsibility
    is to download few documents from the internet using `feedparser`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Download all the documents mentioned in the URL and store the list of dictionaries
    into a variable called `feed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Empty the list to keep track of all the documents that we are going to analyze
    further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Take the top five documents from the `feed` variable and store the current
    news item into a variable called `entry`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the news summary into a variable called `text`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'If the news article contains any sensitive words, skip those:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the document in the `documents` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the current document on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Display an informational message to the user that we have collected *N* documents
    from the given `url`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: This instruction defines a new function, `cleanDocuments()`, whose responsibility
    is to clean the input text (since we are downloading it from the internet, it
    can contain any type of data).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We are interested in extracting words that are in the English alphabet. So,
    this tokenizer is defined to break the text into tokens, where each token consists
    of letters from a to z and A-Z. By doing so, we can be sure that punctuation and
    other bad data doesn't come into the processing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the stop words of English in a variable, `en_stop`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a empty list called `cleaned`, which is used to store all the cleaned
    and tokenized documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate over all the documents we have collected using the `getDocuments()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the document to lowercase to avoid treating the same word differently
    because they are case sensitive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Break the sentence into words. The output is a list of words stored in a variable
    called `words`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Ignore all the words from the sentence if they belong to the English stop word
    category and store all of them in the `non_stopped_words` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Store the sentence that is tokenized and cleaned in a variable called `self.cleaned` (class
    member).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Show a diagnostic message to the user that we have finished cleaning the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction defines a new function, `doLDA`, which runs the LDA analysis
    on the cleaned documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we directly process the cleaned documents, we create a dictionary from
    these documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The input corpus is defined as a bag of words for each cleaned sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a model on the corpus with the number of topics defined as `2` and set
    the vocabulary size/mapping using the `id2word` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Print two topics, where each topic should contain four words on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the function that does all the steps in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: When the current program is invoked as the main program, create a new object
    called `topicExample` from the `IdentifyingTopicExample()` class and invoke the `run()` function
    on the object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Summarizing text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this information overload era, there is so much information that is available
    in print/text form. Its humanly impossible for us to consume all this data. In
    order to make the consumption of this data easier, we have been trying to invent
    algorithms that can help simplify large text into a summary (or a gist) that we
    can easily digest.
  prefs: []
  type: TYPE_NORMAL
- en: By doing this, we will save time and also make things easier for the network.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use the gensim library, which has built-in support for
    this summarization using the TextRank algorithm ([https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have Python installed, along with the `bs4` and `gensim` libraries.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open atom editor (or your favorite programming editor).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new file called `Summarize.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following source code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a95816f9-4168-4eb3-85ae-7e56797626ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the program using the Python interpreter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b7fa82ee-3668-4db0-98c8-b5d5b9571fc8.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how we our summarization program works.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'These three instructions import the necessary libraries into the current program:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gensim.summarization.summarize`: Text-rank-based summarization algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bs4`: A `BeautifulSoup` library for parsing HTML documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests`: A library to download HTTP resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are defining a dictionary called URLs whose keys are the title of the paper
    that is auto generated and the value is the URL to the paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate through all the keys of the dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the URL of the current paper in a variable called `url`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Download the content of the url using the `requests` library''s `get()` method
    and store the response object into a variable, `r`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `BeautifulSoup()` to parse the text from the `r` object using the HTML
    parser and store the return object in a variable called `soup`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Strip out all the HTML tags and extract only the text from the document into
    the variable `data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Find the position of the text `Introduction` and skip past towards end of string,
    to mark is as starting offset from which we want to extract the substring.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the second position in the document, exactly at the beginning of the related
    work section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, extract the introduction of the paper, which is between these two offsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the URL and the title of the paper on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the `summarize()` function on the text, which returns shortened text as
    per the text rank algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Print an extra newline for more readability of the screen output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Resolving anaphora
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many natural languages, while forming sentences, we avoid the repeated use
    of certain nouns with pronouns to simplify the sentence construction.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Ravi is a boy. He often donates money to the poor.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, there are two statements:'
  prefs: []
  type: TYPE_NORMAL
- en: Ravi is a boy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He often donates money to the poor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we start analyzing the second statement, we cannot make a decision about
    who is donating the money  without knowing about the first statement. So, we should
    associate He with Ravi to get the complete sentence meaning. All this reference
    resolution happens naturally in our mind.
  prefs: []
  type: TYPE_NORMAL
- en: If we observe the previous example carefully, first the subject is present;
    then the pronoun comes up. So the direction of the flow is from left to right.
    Based on this flow, we can call these types of sentences anaphora.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take another example:'
  prefs: []
  type: TYPE_NORMAL
- en: He was already on his way to airport. Realized Ravi
  prefs: []
  type: TYPE_NORMAL
- en: This is another class of example where the direction of expression is the reverse
    order (first the pronoun and then the noun). Here too, He is associated with Ravi.
    These types of sentences are called as Cataphora.
  prefs: []
  type: TYPE_NORMAL
- en: 'The earliest available algorithm for this anaphora resolution dates back to
    the 1970; Hobbs has presented a paper on this. An online version of this paper
    is available here: [https://www.isi.edu/~hobbs/pronoun-papers.html](https://www.isi.edu/~hobbs/pronoun-papers.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will try to write a very simple Anaphora resolution algorithm
    using what we have learned just now.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have python installed, along with the `nltk` library and `gender`
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: You can use `nltk.download()` to download the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open atom editor (or your favorite programming editor).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new file called `Anaphora.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following source code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2f596d1e-74a7-4788-bcf8-0db0cf82e547.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the program using the Python interpreter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7d6cf7f5-a322-479c-b628-5d50d524c98d.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let see how our simple Anaphora resolution algorithm works.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'These four instructions import the necessary modules and functions that are
    used in the program. We are defining a new class called `AnaphoraExample`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'We are defining a new constructor for this class, which doesn''t take any parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: These two instructions load all the male and female names from the `nltk.names` corpus
    and tag them as male/female before storing them in two lists called male/female.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction creates a unique list of males and females. `random.shuffle()` ensures
    that all of the data in the list is randomized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction invokes the `feature()` function on the gender and stores
    all the names in a variable called `training`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'We are creating a `NaiveBayesClassifier` object called `_classifier` using
    the males and females features that are stored in a variable called `training`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'This function defines the simplest possible feature, which categorizes the
    given name as male or female just by looking at the last letter of the name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes a word as an argument and tries to detect the gender as
    male or female using the classifier we have built:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the main function that is of interest to us, as we are going to detect
    anaphora on the sample sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'These are four examples with mixed complexity expressed in anaphora form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction iterates over all the sentences by taking one sentence at
    a time to a local variable called `sent`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction tokenizes, assigns parts of speech, extracts chunks (named
    entities), and returns the chunk tree to a variable called `chunks`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'This variable is used to store all the names and pronouns that help us resolve
    anaphora:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction shows the current sentence that is being processed on the
    user''s screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction flattens the tree chunks to a list of items expressed in IOB
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'We are traversing through all chunked sentences that are in IOB format (tuple
    with three elements):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'If the POS of the word is `NNP` and IOB letter for this word is `B-PERSON` or `O`,
    then we mark this word as a `Name`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'If the POS of the word is `CC`, then also we will add this to the `stack` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'If the POS of the word is `PRP`, then we will add this to the `stack` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally we print the stack on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: We are creating a new object called `anaphora` from `AnaphoraExample()` and
    invoking the `learnAnaphora()` function on the anaphora object. Once this function
    execution completes, we see the list of words for every sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: Disambiguating word sense
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we learned how to identify POS of the words, find named
    entities, and so on. Just like a word in English behaves as both a noun and a
    verb, finding the sense in which a word is used is very difficult for computer
    programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a few examples to understand this sense portion:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sentence** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| *She is my date* | Here the sense of the word *date* is not the calendar
    date but expresses a human relationship. |'
  prefs: []
  type: TYPE_TB
- en: '| *You have taken too many leaves to skip cleaning leaves in the garden* |
    Here the word *leaves* has multiple senses:'
  prefs: []
  type: TYPE_NORMAL
- en: The first word *leave* means taking a break
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second one actually refers to tree leaves
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Like this, there are many combinations of senses possible in sentences.
  prefs: []
  type: TYPE_NORMAL
- en: One of the challenges we have faced for senses identification is to find a proper
    nomenclature to describe these senses. There are many English dictionaries available
    that describe the behavior of words and all possible combinations of those. Of
    them all, WordNet is the most structured, preferred, and widely accepted source
    of sense usage.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see examples of senses from the WordNet library and
    use the built-in `nltk` library to find out the sense of words.
  prefs: []
  type: TYPE_NORMAL
- en: Lesk is the oldest algorithm that was coined to tackle this sense detection.
    You will see, however, that this one too is not accurate in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have Python installed, along with the `nltk` library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open atom editor (or your favorite programming editor).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new file called `WordSense.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following source code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/25897007-6789-40c0-b6b6-1ea605c35b4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the program using the Python interpreter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/984afb10-d504-4899-b396-6d0a1d9d7cdb.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how our program works. This instruction imports the `nltk` library
    into the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: We are defining a function with the name `understandWordSenseExamples()`, which
    uses the WordNet corpus to showcase the possible senses of the words that we are
    interested in.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: These are the three words with different senses of expression. They are stored
    as a list in a variable called `words`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'These instructions do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Iterate over all the words in the list by storing the current word in a variable
    called `word`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invoke the `synsets()` function from the `wordne`t module and store the result
    in the `syns` variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take the first three synsets from the list, iterate through them, and take the
    current one in a variable called `syn`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invoke the `examples()` function on the `syn` object and take the first two
    examples as the iterator. The current value of the iterator is available in the
    variable example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Print the word, synset's name, and example sentence finally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a new function, `understandBuiltinWSD()`, to explore the NLTK built-in
    lesk algorithm's performance on sample sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: Define a new variable called `maps`, a list of tuples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'Each tuple consists of three elements:'
  prefs: []
  type: TYPE_NORMAL
- en: The sentence we want to analyze
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word in the sentence for which we want to find the sense
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The POS of the word
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In these two instructions, we are traversing through the `maps` variable, taking
    the current tuple into variable `m`, invoking the `nltk.wsd.lesk()` function,
    and displaying the formatted results on screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: When the program is run, call the two functions that show the results on the
    user's screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: Performing sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback is one of the most powerful measures for understanding relationships.
    Humans are very good at understanding feedback in verbal communication as the
    analysis happens unconsciously. In order to write computer programs that can measure
    and find the emotional quotient, we should have some good understanding of the
    ways these emotions are expressed in these natural languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sentence** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| *I am very happy* | Indicates a happy emotion |'
  prefs: []
  type: TYPE_TB
- en: '| *She is so :(* | We know there is an iconic sadness expression here |'
  prefs: []
  type: TYPE_TB
- en: With the increased use of text, icons, and emojis in written natural language
    communication, it's becoming increasingly difficult for computer programs to understand
    the emotional meaning of a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try to write a program to understand the facilities nltk provides to build
    our own algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have Python installed, along with the `nltk` library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open atom editor (or your favorite programming editor).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new file called `Sentiment.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following source code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/842ffb61-bece-46e2-9d4f-1db5e6c5ca33.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the program using the Python interpreter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8487c0f4-10ac-4de5-b4f2-f61f307f889f.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how our sentiment analysis program works. These instructions import
    the `nltk` module and `sentiment_analyzer` module respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: We are defining a new function, `wordBasedSentiment()`, which we will use to
    learn how to do sentiment analysis based on the words that we already know and
    which mean something important to us.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: We are defining a list of three words that are special to us as they represent
    some form of happiness. These words are stored in the `positive_words` variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: This is the sample text that we are going to analyze; the text is stored in
    a variable called `text`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: We are calling the `extract_unigram_feats()` function on the text using the words
    that we have defined. The result is a dictionary of input words that indicate
    whether the given words are present in the text or not.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: This instruction displays the dictionary on the user's screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: This instruction defines a new function that we will use to understand whether
    some pairs of words occur in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: This instruction defines a list of two-word tuples. We are interested in finding
    if these pairs of words occur together in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: This is the sentence we are interested in processing and finding the features
    of.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: We are calling the `extract_bigram_feats()` on the input sentence against the
    sets of words in the `word_sets` variable. The result is a dictionary that tells
    whether these pairs of words are present in the sentence or not.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: This instruction displays the dictionary on screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: We are defining a new function, `markNegativity()`, which helps us understand
    how we can find negativity in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: Next is the sentence on which we want to run the negativity analysis. It's stored
    in a variable, `text`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: We are calling the `mark_negation()` function on the text. This returns a list
    of all the words in the sentence along with a special suffix `_NEG` for all the
    words that come under the negative sense. The result is stored in the `negation` variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: This instruction displays the list negation on screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: When the program is run, these functions are called and we see the output of
    three functions in the order they are executed (top-down).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: Exploring advanced sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are seeing that more and more businesses are going online to increase their
    target customer base and the customers are given the ability to leave feedback
    via various channels. It's becoming more and more important for businesses to
    understand the emotional response of their customers with respect to the businesses
    they run.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will write our own sentiment analysis program based on what
    we have learned in the previous recipe. We will also explore the built-in vader
    sentiment analysis algorithm, which helps evaluate in finding the sentiment of
    complex sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have Python installed, along with the `nltk` library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open atom editor (or your favorite programming editor).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new file called `AdvSentiment.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following source code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b2a2b51b-c606-442c-8639-148cea230656.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the program using the Python interpreter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/49f760f3-9ef7-4291-a3a7-f727caab0996.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's see how our sentiment analysis program works. These four instructions
    import the necessary modules that we are going to use as part of this program.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'Defining a new function, `mySentimentAnalyzer()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: This instruction defines a new subfunction, `score_feedback()`, which takes
    a sentence as input and returns the score for the sentence in terms of `-1` negative, `0` neutral,
    and `1` positive.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: Since we are just experimenting, we are defining the three words using which
    we are going to find the sentiment. In real-world use cases, we might use these
    from the corpus of a larger dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: This instruction breaks the input sentence into words. The list of words is
    fed to the `mark_negation()` function to identify the presence of any negativity
    in the sentence. Join the result from `mark_negation()` to the string and see
    if the `_NEG` suffix is present; then set the score as `-1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we are using `extract_unigram_feats()` on the input text against `positive_words` and
    storing the dictionary into a variable called `analysis`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: The value of score is decided to be `1` if there is a presence of the positive
    word in the input text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally this `score_feedback()` function returns the computed score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: These are the four reviews that we are interested in processing using our algorithm
    to print the score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: These instructions extract the sentences from the variable feedback by splitting
    on newline (`\n`) and calling the `score_feedback()` function on this text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: The result will be the score and sentence on the screen. This instruction defines
    the `advancedSentimentAnalyzer()` function, which will be used to understand the
    built-in features of NLTK sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: We are defining five sentences to analyze. you'll note that we are also using
    emoticons (icons) to see how the algorithm works.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: This instruction creates a new object for `SentimentIntensityAnalyzer()` and
    stores the object in the variable `senti`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: 'These instructions do the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Iterate over all the sentences and store the current one in the variable `sentence`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display the currently processed sentence on screen
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invoke the `polarity_scores()` function on this sentence; store the result in
    a variable called `kvp`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traverse through the dictionary `kvp` and print the key (negativity, neutral,
    positivity, or compound types) and the score computed for these types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the current program is invoked, call these two functions to display the results
    on screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: Creating a conversational assistant or chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conversational assistants or chatbots are not very new. One of the foremost
    of this kind is ELIZA, which was created in the early 1960s and is worth exploring.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to successfully build a conversational engine, it should take care
    of the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the target audience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the natural language in which communication happens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the intent of the user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Come up with responses that can answer the user and give further clues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLTK has a module, `nltk.chat`, which simplifies building these engines by providing
    a generic framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the available engines in NLTK:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Engines** | **Modules** |'
  prefs: []
  type: TYPE_TB
- en: '| Eliza | `nltk.chat.eliza` Python module |'
  prefs: []
  type: TYPE_TB
- en: '| Iesha | `nltk.chat.iesha` Python module |'
  prefs: []
  type: TYPE_TB
- en: '| Rude | `nltk.chat.rudep` Python module |'
  prefs: []
  type: TYPE_TB
- en: '| Suntsu | `nltk.chat.suntsu` module |'
  prefs: []
  type: TYPE_TB
- en: '| Zen | `nltk.chat.zen` module |'
  prefs: []
  type: TYPE_TB
- en: In order to interact with these engines we can just load these modules in our
    Python program and invoke the `demo()` function.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will show us how to use built-in engines and also write our own
    simple conversational engine using the framework provided by the `nltk.chat` module.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have Python installed, along with the `nltk` library. Having an understanding
    of regular expressions also helps.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open atom editor (or your favorite programming editor).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new file called `Conversational.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following source code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1d727273-2841-4170-af3a-992ef49b141c.png)'
  prefs: []
  type: TYPE_IMG
- en: Save the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the program using the Python interpreter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f142875f-b0c3-4faf-b7e7-58ca26b1cbaa.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's try to understand what we are trying to achieve here. This instruction
    imports the `nltk` library into the current program.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction defines a new function called `builtinEngines` that takes
    a string parameter, `whichOne`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: These if, elif, else instructions are typical branching instructions that decide
    which chat engine's `demo()` function is to be invoked depending on the argument
    that is present in the `whichOne` variable. When the user passes an unknown engine
    name, it displays a message to the user that it's not aware of this engine.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: It's a good practice to handle all known and unknown cases also; it makes our
    programs more robust in handling unknown situations.
  prefs: []
  type: TYPE_NORMAL
- en: This instruction defines a new function called `myEngine()`; this function does
    not take any parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: This is a single instruction where we are defining a nested tuple data structure
    and assigning it to chat pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s pay close attention to the data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: We are defining a tuple of tuples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each subtuple consists of two elements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first member is a regular expression (this is the user's question in regex
    format)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The second member of the tuple is another set of tuples (these are the answers)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We are defining a subfunction called `chat()` inside the `myEngine()` function.
    This is permitted in Python. This `chat()` function displays some information
    to the user on the screen and calls the nltk built-in `nltk.chat.util.Chat()` class
    with the chatpairs variable. It passes `nltk.chat.util.reflections` as the second
    argument. Finally we call the `chatbot.converse()` function on the object that's
    created using the `chat()` class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: 'This instruction calls the `chat()` function, which shows a prompt on the screen
    and accepts the user''s requests. It shows responses according to the regular
    expressions that we have built before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: These instructions will be called when the program is invoked as a standalone
    program (not using import).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: 'They do these two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Invoke the built-in engines one after another (so that we can experience them)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once all the five built-in engines are excited, they call our `myEngine()`,
    where our customer engine comes into play
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
