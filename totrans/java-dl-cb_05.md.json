["```py\n<dependency>\n <groupId>org.deeplearning4j</groupId>\n <artifactId>deeplearning4j-nlp</artifactId>\n <version>1.0.0-beta3</version>\n </dependency>\n```", "```py\nFile file = new File(\"raw_sentences.txt\");\nSentenceIterator iterator = new BasicLineIterator(file);\n```", "```py\nFile file = new File(\"raw_sentences.txt\");\nSentenceIterator iterator = new LineSentenceIterator(file);\n```", "```py\nList<String> sentences= Arrays.asList(\"sample text\", \"sample text\", \"sample text\");\nSentenceIterator iter = new CollectionSentenceIterator(sentences); \n```", "```py\nSentenceIterator iter = new FileSentenceIterator(new File(\"/home/downloads/sentences.txt\"));\n```", "```py\n<dependency>\n <groupId>org.deeplearning4j</groupId>\n <artifactId>deeplearning4j-nlp-uima</artifactId>\n <version>1.0.0-beta3</version>\n </dependency>\n```", "```py\nSentenceIterator iterator = UimaSentenceIterator.create(\"path/to/your/text/documents\"); \n```", "```py\nSentenceIterator iter = UimaSentenceIterator.create(\"path/to/your/text/documents\");\n```", "```py\niterator.setPreProcessor(new SentencePreProcessor() {\n @Override\n public String preProcess(String sentence) {\n return sentence.toLowerCase();\n }\n });\n```", "```py\nSentenceIterator iterator = new UimaSentenceIterator(path,AnalysisEngineFactory.createEngine( AnalysisEngineFactory.createEngineDescription(TokenizerAnnotator.getDescription(), SentenceAnnotator.getDescription())));\n```", "```py\nTokenizerFactory tokenFactory = new DefaultTokenizerFactory();\ntokenFactory.setTokenPreProcessor(new CommonPreprocessor());\n\n```", "```py\nWord2Vec model = new Word2Vec.Builder()\n .minWordFrequency(wordFrequency)\n .layerSize(numFeatures)\n .seed(seed)\n .epochs(numEpochs)\n .windowSize(windowSize)\n .iterate(iterator)\n .tokenizerFactory(tokenFactory)\n .build();\n\n```", "```py\nmodel.fit();\n```", "```py\nCollection<String> words = model.wordsNearest(\"season\",10); \n```", "```py\nweek\ngame\nteam\nyear\nworld\nnight\ntime\ncountry\nlast\ngroup\n```", "```py\ndouble cosSimilarity = model.similarity(\"season\",\"program\");\nSystem.out.println(cosSimilarity);\n\n```", "```py\n0.2720930874347687\n\n```", "```py\nNd4j.setDataType(DataBuffer.Type.DOUBLE);\n```", "```py\nWordVectorSerializer.writeWordVectors(model.lookupTable(),new File(\"words.txt\"));\n```", "```py\nPair<InMemoryLookupTable,VocabCache> vectors = WordVectorSerializer.loadTxt(new File(\"words.txt\"));\nVocabCache cache = vectors.getSecond();\nINDArray weights = vectors.getFirst().getSyn0(); \n\n```", "```py\n List<String> cacheList = new ArrayList<>();\n for(int i=0;i<cache.numWords();i++){\n cacheList.add(cache.wordAtIndex(i));\n }\n```", "```py\nBarnesHutTsne tsne = new BarnesHutTsne.Builder()\n .setMaxIter(100)\n .theta(0.5)\n .normalize(false)\n .learningRate(500)\n .useAdaGrad(false)\n .build();\n\n```", "```py\ntsne.fit(weights);\ntsne.saveAsFile(cacheList,\"tsne-standard-coords.csv\");\n```", "```py\nWordVectorSerializer.writeWord2VecModel(model, \"model.zip\");\n```", "```py\nWord2Vec word2Vec = WordVectorSerializer.readWord2VecModel(\"model.zip\");\n```", "```py\nFile file = new File(\"GoogleNews-vectors-negative300.bin.gz\");\nWord2Vec model = WordVectorSerializer.readWord2VecModel(file);\n```", "```py\nmodel.wordsNearest(\"season\",10))\n```", "```py\nWord2Vec model = new Word2Vec.Builder()\n .iterate(iterator)\n .tokenizerFactory(tokenizerFactory)\n .minWordFrequency(5)\n .layerSize(200)\n .seed(42)\n .windowSize(5)\n .build();\n```", "```py\nWordVectors wordVectors = WordVectorSerializer.loadStaticModel(new File(WORD_VECTORS_PATH));\n\n```", "```py\n Map<String,List<File>> reviewFilesMap = new HashMap<>();\n reviewFilesMap.put(\"Positive\", Arrays.asList(filePositive.listFiles()));\n reviewFilesMap.put(\"Negative\", Arrays.asList(fileNegative.listFiles()));\n LabeledSentenceProvider sentenceProvider = new FileLabeledSentenceProvider(reviewFilesMap, rndSeed); \n```", "```py\nCnnSentenceDataSetIterator iterator = new CnnSentenceDataSetIterator.Builder(CnnSentenceDataSetIterator.Format.CNN2D)\n .sentenceProvider(sentenceProvider)\n .wordVectors(wordVectors) //we mention word vectors here\n .minibatchSize(minibatchSize)\n .maxSentenceLength(maxSentenceLength) //words with length greater than this will be ignored.\n .useNormalizedWordVectors(false)\n .build();\n\n```", "```py\nComputationGraphConfiguration.GraphBuilder builder = new NeuralNetConfiguration.Builder()\n .weightInit(WeightInit.RELU)\n .activation(Activation.LEAKYRELU)\n .updater(new Adam(0.01))\n .convolutionMode(ConvolutionMode.Same) //This is important so we can 'stack' the results later\n .l2(0.0001).graphBuilder();\n\n```", "```py\nbuilder.addLayer(\"cnn3\", new ConvolutionLayer.Builder()\n .kernelSize(3,vectorSize) //vectorSize=300 for google vectors\n .stride(1,vectorSize)\n .nOut(100)\n .build(), \"input\");\n builder.addLayer(\"cnn4\", new ConvolutionLayer.Builder()\n .kernelSize(4,vectorSize)\n .stride(1,vectorSize)\n .nOut(100)\n .build(), \"input\");\n builder.addLayer(\"cnn5\", new ConvolutionLayer.Builder()\n .kernelSize(5,vectorSize)\n .stride(1,vectorSize)\n .nOut(100)\n .build(), \"input\");\n```", "```py\nbuilder.addVertex(\"merge\", new MergeVertex(), \"cnn3\", \"cnn4\", \"cnn5\")\n\n```", "```py\nComputationGraphConfiguration config = builder.build();\n ComputationGraph net = new ComputationGraph(config);\n  net.init();\n```", "```py\nfor (int i = 0; i < numEpochs; i++) {\n net.fit(trainIterator);\n }\n```", "```py\nEvaluation evaluation = net.evaluate(testIter);\nSystem.out.println(evaluation.stats());\n```", "```py\nINDArray features = ((CnnSentenceDataSetIterator)testIterator).loadSingleSentence(contents);\n INDArray predictions = net.outputSingle(features);\n List<String> labels = testIterator.getLabels();\n System.out.println(\"\\n\\nPredictions for first negative review:\");\n for( int i=0; i<labels.size(); i++ ){\n System.out.println(\"P(\" + labels.get(i) + \") = \" + predictions.getDouble(i));\n }\n```", "```py\nLabelAwareIterator labelAwareIterator = new FileLabelAwareIterator.Builder()\n .addSourceFolder(new ClassPathResource(\"label\").getFile()).build();  \n```", "```py\nTokenizerFactory tokenizerFactory = new DefaultTokenizerFactory();\ntokenizerFactory.setTokenPreProcessor(new CommonPreprocessor()); \n\n```", "```py\nParagraphVectors paragraphVectors = new ParagraphVectors.Builder()\n .learningRate(learningRate)\n .minLearningRate(minLearningRate)\n .batchSize(batchSize)\n .epochs(epochs)\n .iterate(labelAwareIterator)\n .trainWordVectors(true)\n .tokenizerFactory(tokenizerFactory)\n .build();\n\n```", "```py\nparagraphVectors.fit();\n\n```", "```py\nClassPathResource unClassifiedResource = new ClassPathResource(\"unlabeled\");\n FileLabelAwareIterator unClassifiedIterator = new FileLabelAwareIterator.Builder()\n .addSourceFolder(unClassifiedResource.getFile())\n .build();\n```", "```py\nInMemoryLookupTable<VocabWord> lookupTable = (InMemoryLookupTable<VocabWord>)paragraphVectors.getLookupTable();\n\n```", "```py\nwhile (unClassifiedIterator.hasNextDocument()) {\n//Calculate the domain vector of each document.\n//Calculate the cosine similarity of the domain vector with all \n//the given labels\n //Display the results\n }\n\n```", "```py\nLabelledDocument labelledDocument = unClassifiedIterator.nextDocument();\n List<String> documentAsTokens = tokenizerFactory.create(labelledDocument.getContent()).getTokens();\n\n```", "```py\nVocabCache vocabCache = lookupTable.getVocab();\n\n```", "```py\nAtomicInteger cnt = new AtomicInteger(0);\n for (String word: documentAsTokens) {\n if (vocabCache.containsWord(word)){\n cnt.incrementAndGet();\n }\n }\n INDArray allWords = Nd4j.create(cnt.get(), lookupTable.layerSize());\n\n```", "```py\ncnt.set(0);\n for (String word: documentAsTokens) {\n if (vocabCache.containsWord(word))\n allWords.putRow(cnt.getAndIncrement(), lookupTable.vector(word));\n }\n\n```", "```py\nINDArray documentVector = allWords.mean(0);\n\n```", "```py\nList<String> labels = labelAwareIterator.getLabelsSource().getLabels();\n List<Pair<String, Double>> result = new ArrayList<>();\n for (String label: labels) {\n INDArray vecLabel = lookupTable.vector(label);\n if (vecLabel == null){\n throw new IllegalStateException(\"Label '\"+ label+\"' has no known vector!\");\n }\n double sim = Transforms.cosineSim(documentVector, vecLabel);\n result.add(new Pair<String, Double>(label, sim));\n }\n```", "```py\n for (Pair<String, Double> score: result) {\n log.info(\" \" + score.getFirst() + \": \" + score.getSecond());\n }\n```"]