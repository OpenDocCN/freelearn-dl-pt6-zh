- en: Options Trading Using Q-learning and Scala Play Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As human beings, we learn from experiences. We have not become so charming by
    accident. Years of positive compliments as well as negative criticism, have all
    helped shape us into who we are today. We learn how to ride a bike by trying out
    different muscle movements until it just clicks. When you perform actions, you
    are sometimes rewarded immediately. This is all about **Reinforcement learning**
    (**RL**).
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is all about designing a machine learning system driven by criticisms
    and rewards. We will see how to apply RL algorithms for a predictive model on
    real-life datasets.
  prefs: []
  type: TYPE_NORMAL
- en: From the trading point of view, an option is a contract that gives its owner
    the right to buy (call option) or sell (put option) a financial asset (underlying)
    at a fixed price (the strike price) at or before a fixed date (the expiry date).
  prefs: []
  type: TYPE_NORMAL
- en: We will see how to develop a real-life application for such options trading
    usingan RL algorithm called **QLearning**. To be more precise, we will solve the
    problem of computing the best strategy in options trading, and we want to trade
    certain types of options given some market conditions and trading data.
  prefs: []
  type: TYPE_NORMAL
- en: The IBM stock datasets will be used to design a machine learning system driven
    by criticisms and rewards. We will start from RL and its theoretical background
    so that the concept is easier to grasp. Finally, we will wrap up the whole application
    as a web app using Scala Play Framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concisely, we will learn the following topics throughout this end-to-end project:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Q-learning—an RL algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Options trading—what is it all about?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of technologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Q-learning for options trading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrapping up the application as a web app using Scala Play Framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement versus supervised and unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whereas supervised and unsupervised learning appear at opposite ends of the
    spectrum, RL exists somewhere in the middle. It is not supervised learning because
    the training data comes from the algorithm deciding between exploration and exploitation.
    In addition, it is not unsupervised because the algorithm receives feedback from
    the environment. As long as you are in a situation where performing an action
    in a state produces a reward, you can use RL to discover a good sequence of actions
    to take the maximum expected rewards.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of an RL agent will be to maximize the total reward that it receives
    in the end. The third main subelement is the `value` function. While rewards determine
    an immediate desirability of the states, values indicate the long-term desirability
    of states, taking into account the states that may follow and the available rewards
    in these states. The `value` function is specified with respect to the chosen
    policy. During the learning phase, an agent tries actions that determine the states
    with the highest value, because these actions will get the best number of rewards
    in the end.
  prefs: []
  type: TYPE_NORMAL
- en: Using RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Figure 1* shows a person making decisions to arrive at their destination.
    Moreover, suppose that on your drive from home to work, you always choose the
    same route. However, one day your curiosity takes over and you decide to try a
    different path, hoping for a shorter commute. This dilemma of trying out new routes
    or sticking to the best-known route is an example of exploration versus exploitation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e85f2dfa-b4f1-4f3c-bf90-bc66c9d4f1ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An agent always tries to reach the destination by passing through
    the route'
  prefs: []
  type: TYPE_NORMAL
- en: RL techniques are being used in many areas. A general idea that is being pursued
    right now is creating an algorithm that does not need anything apart from a description
    of its task. When this kind of performance is achieved, it will be applied virtually
    everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: Notation, policy, and utility in RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may notice that RL jargon involves incarnating the algorithm into taking
    actions in situations to receive rewards. In fact, the algorithm is often referred
    to as an agent that acts with the environment. You can just think of it is an
    intelligent hardware agent that is sensing with sensors and interacting with the
    environment using its actuators. Therefore, it should not be a surprise that much
    of RL theory is applied in robotics. Now, to extend our discussion further, we
    need to know a few terminologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Environment**: An environment is any system having states and mechanisms
    to transition between different states. For example, the environment for a robot
    is the landscape or facility it operates in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agent**: An agent is an automated system that interacts with the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State**: The state of the environment or system is the set of variables or
    features that fully describe the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Goal**: A goal is a state that provides a higher discounted cumulative reward
    than any other state. A high cumulative reward prevents the best policy from being
    dependent on the initial state during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action**: An action defines the transition between states, where an agent
    is responsible for performing, or at least recommending, an action. Upon execution
    of an action, the agent collects a reward (or punishment) from the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy**: The policy defines the action to be performed and executed for
    any state of the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward**: A reward quantifies the positive or negative interaction of the
    agent with the environment. Rewards are essentially the training set for the learning
    engine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Episode *(***also known as **trials**): This defines the number of steps
    necessary to reach the goal state from an initial state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will discuss more on policy and utility later in this section. *Figure 2*
    demonstrates the interplay between **states**, **actions**, and **rewards**. If
    you start at state **s[1]**, you can perform action **a[1]** to obtain a reward
    **r (s[1], a[1])**. Arrows represent **actions**, and **states** are represented
    by circles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e880ac5-2e4b-44a2-9c91-38786d5942b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An agent performing an action on a state produces a reward'
  prefs: []
  type: TYPE_NORMAL
- en: A robot performs actions to change between different states. But how does it
    decide which action to take? Well, it is all about using a different or concrete
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: Policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In RL lingo, we call a strategy **policy**. The goal of RL is to discover a
    good strategy. One of the most common ways to solve it is by observing the long-term
    consequences of actions in each state. The short-term consequence is easy to calculate:
    it''s just the reward. Although performing an action yields an immediate reward,
    it is not always a good idea to greedily choose the action with the best reward.
    That is a lesson in life too, because the most immediate best thing to do may
    not always be the most satisfying in the long run. The best possible policy is
    called the optimal policy, and it is often the holy grail of RL, as shown in *Figure
    3*, which shows the optimal action, given any state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bca50b75-f45d-4aa0-ab22-714d10131b3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A policy defines an action to be taken in a given state'
  prefs: []
  type: TYPE_NORMAL
- en: We have seen one type of policy where the agent always chooses the action with
    the greatest immediate reward, called **greedy policy**. Another simple example
    of a policy is arbitrarily choosing an action, called **random policy**. If you
    come up with a policy to solve a, RL problem, it is often a good idea to double-check
    that your learned policy performs better than both the random and the greedy policies.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we will see how to develop another robust policy called **policy
    gradients**, where a neural network learns a policy for picking actions by adjusting
    its weights through gradient descent using feedback from the environment. We will
    see that, although both the approaches are used, policy gradient is more direct
    and optimistic.
  prefs: []
  type: TYPE_NORMAL
- en: Utility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The long-term reward is called a **utility**. It turns out that if we know
    the utility of performing an action upon a state, then it is easy to solve RL.
    For example, to decide which action to take, we simply select the action that
    produces the highest utility. However, uncovering these utility values is difficult.
    The utility of performing an action *a* at a state *s* is written as a function, *Q(s,
    a)*, called the **utility function**. This predicts the expected immediate reward,
    and rewards following an optimal policy given the state-action input, as shown
    in *Figure 4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86800cd6-c7ec-4a55-9148-2d4afb66e6b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Using a utility function'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most RL algorithms boil down to just three main steps: infer, do, and learn.
    During the first step, the algorithm selects the best action (*a*) given a state
    (*s*) using the knowledge it has so far. Next, it perform the action to find out
    the reward (*r*) as well as the next state (*s''*). Then it improves its understanding
    of the world using the newly acquired knowledge *(s, r, a, s'')*. However,  as
    I think you will agree, this is just a naive way to calculate the utility.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the question is: what could be a more robust way to compute it? We can
    calculate the utility of a particular state-action pair *(s, a)* by recursively
    considering the utilities of future actions. The utility of your current action
    is influenced not only by the immediate reward but also the next best action,
    as shown in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5215bab5-b9d6-40f5-a5c9-12c83fa6cea8.png)'
  prefs: []
  type: TYPE_IMG
- en: '*s''* denotes the next state, and *a''* denotes the next action. The reward
    of taking action *a* in state *s* is denoted by *r(s, a)*. Here, *γ* is a hyperparameter
    that you get to choose, called the discount factor. If *γ* is *0*, then the agent
    chooses the action that maximizes the immediate reward. Higher values of *γ* will
    make the agent give more importance to considering long-term consequences. In
    practice, we have more such hyperparameter to be considered. For example, if a
    vacuum cleaner robot is expected to learn to solve tasks quickly, but not necessarily
    optimally, we may want to set a faster learning rate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, if a robot is allowed more time to explore and exploit, we may
    tune down the learning rate. Let us call the learning rate *α* and change our
    utility function as follows (note that when *α = 1*, both the equations are identical):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ec4cadc-c023-415b-9ed9-e50bcc6078c1.png)'
  prefs: []
  type: TYPE_IMG
- en: In summary, an RL problem can be solved if we know this *Q(s, a)* function.
    Here comes an algorithm called Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: A simple Q-learning implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q-learning is an algorithm that can be used in financial and market trading
    applications, such as options trading. One reason is that the best policy is generated
    through training. that is, RL defines the model in Q-learning over time and is
    constantly updated with any new episode. Q-learning is a method for optimizing
    (cumulated) discounted reward, making far-future rewards less prioritized than
    near-term rewards; Q-learning is a form of model-free RL. It can also be viewed
    as a method of asynchronous **dynamic programming** (**DP**).
  prefs: []
  type: TYPE_NORMAL
- en: It provides agents with the capability of learning to act optimally in Markovian
    domains by experiencing the consequences of actions, without requiring them to
    build maps of the domains. In short, Q-learning qualifies as an RL technique because
    it does not strictly require labeled data and training. Moreover, the Q-value
    does not have to be a continuous, differentiable function.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Markov decision processes provide a mathematical framework
    for modeling decision-making in situations where outcomes are partly random and
    partly under the control of a decision-maker. Therein, the probability of the
    random variables at a future point of time depends only on the information at
    the current point in time and not on any of the historical values. In other words,
    the probability is independent of historical states.
  prefs: []
  type: TYPE_NORMAL
- en: Components of the Q-learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This implementation is highly inspired by the Q-learning implementation from
    a book, written by Patrick R. Nicolas, *Scala for Machine Learning - Second Edition*,
    Packt Publishing Ltd., September 2017\. Thanks to the author and Packt Publishing
    Ltd. The source code is available at [https://github.com/PacktPublishing/Scala-for-Machine-Learning-Second-Edition/tree/master/src/main/scala/org/scalaml/reinforcement](https://github.com/PacktPublishing/Scala-for-Machine-Learning-Second-Edition/tree/master/src/main/scala/org/scalaml/reinforcement).
  prefs: []
  type: TYPE_NORMAL
- en: 'Interested readers can take a look at the the original implementation at the
    extensed version of course can be downloaded from Packt repository or GitHub repo
    of this book. The key components of implementation of the Q-learning algorithm
    are a few classes—`QLearning`, `QLSpace`, `QLConfig`, `QLAction`, `QLState`, `QLIndexedState`,
    and `QLModel`—as described in the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '`QLearning`: Implements training and prediction methods. It defines a data
    transformation of type `ETransform` using a configuration of type `QLConfig`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QLConfig`: This parameterized class defines the configuration parameters for
    the Q-learning. To be more specific, it is used to hold an explicit configuration
    from the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QLAction`**:** This is a class that defines actions between on source state
    and multiple destination states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QLPolicy`: This is an enumerator used to define the type of parameters used
    to update the policy during the training of the Q-learning model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QLSpace`: This has two components: a sequence of states of type `QLState`
    and the identifier, `id`, of one or more goal states within the sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QLState`: Contains a sequence of `QLAction` instances that help in the transition
    from one state to another. It is also used as a reference for the object or instance
    for which the state is to be evaluated and predicted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QLIndexedState`: This class returns an indexed state that indexes a state
    in the search toward the goal state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QLModel`: This is used to generate a model through the training process. Eventually,
    it contains the best policy and the accuracy of a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that, apart from the preceding components, an optional constraint function
    limits the scope of the search for the next most rewarding action from the current
    state. The following diagram shows the key components of the Q-learning algorithm
    and their interaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52b17c4f-9ee2-41f5-8530-ebf971a10444.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Components of the QLearning algorithm and their interaction'
  prefs: []
  type: TYPE_NORMAL
- en: States and actions in QLearning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `QLAction` class specifies the transition from one state to another state.
    It takes two parameters—that is, from and to. Both of them have their own integer
    identifiers that need to be greater than 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from`: A source of the action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`to`: Target of the action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The signature is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `QLState` class defines the state in the Q-learning. It takes three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id`: An identifier that uniquely identifies a state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`actions`: A list of actions for the transition from this state to other states,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`instance`: A state may have properties of type `T`, independent from the state
    transition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the signature of the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the `toString()` method is used for textual representation
    of a state in Q-learning. The state is defined by its ID and the list of actions
    it may potentially trigger.
  prefs: []
  type: TYPE_NORMAL
- en: The state might not have any actions. This is usually the case with the goal
    or absorbing state. In this case, the list is empty. The parameterized instance
    is a reference to the object for which the state is computed.
  prefs: []
  type: TYPE_NORMAL
- en: Now we know the state and action to perform. However, the `QLearning` agent
    needs to know the search space of the form (States *x* Actions). The next step
    consists of creating the graph or search space.
  prefs: []
  type: TYPE_NORMAL
- en: The search space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The search space is the container responsible for any sequence of states. The
    `QLSpace` class defines the search space (States *x* Actions) for the Q-learning
    algorithm, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7238d9c-0a36-4840-b783-0d97adf1b688.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: State transition matrix with QLData (Q-value, reward, probability)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The search space can be provided by the end user with a list of states and
    actions, or automatically created by providing the number of states by taking
    the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`States`: The sequence of all possible states defined in the Q-learning search
    space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`goalIds`: A list of identifiers of states that are goals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let us see the implementation of the class. It is rather a large code block.
    So let us start from the constructor that generates a map named `statesMap`. It
    retrieves the state using its `id` and the array of goals, `goalStates`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then it creates a map of the states as an immutable Map of state ID and state
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a policy and a state of an action, the next task is to compute
    the maximum value given a state and policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we need to know the number of states by accessing the number
    of states in the search space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the `init` method selects an initial state for training episodes. The
    state is randomly selected if the `state0` argument is invalid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `nextStates` method retrieves the list of states resulting from
    the execution of all the actions associated with that state. The search space
    `QLSpace` is created by the factory method `apply` defined in the `QLSpace` companion
    object, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, how do you know whether the current state is a goal state? Well,
    the `isGoal()` method does the trick.
  prefs: []
  type: TYPE_NORMAL
- en: 'It accepts a parameter called `state`*,* which is *a* state that is tested
    against the goal and returns `Boolean: true` if this state is a goal state; otherwise, it
    returns false:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The apply method creates a list of states using the instances set, the goals,
    and the constraining function `constraints` as input. Each state creates its list
    of actions. The actions are generated from this state to any other states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The function constraints limit the scope of the actions that can be triggered
    from any given state, as shown in figure X.
  prefs: []
  type: TYPE_NORMAL
- en: The policy and action-value
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `QLData `class encapsulates the attributes of the policy in the Q-learning
    algorithm by creating a `QLData` record or instance with a given reward, probability,
    and Q-value that is computed and updated during training. The probability variable
    is used to model the intervention condition for an action to be executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the action does not have any external constraint, the probability is 1 (that
    is, the highest), and it is zero otherwise (that is, the action is not allowed
    anyhow). The signature is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, the Q-Value updated during training using the Q-learning
    formula, but the overall value is computed for an action by using its reward,
    adjusted by its probability, and then returning the adjusted value. Then the `value()`
    method selects the attribute of an element of the Q-learning policy using its
    type. It takes the `varType` of the attribute (that is, `REWARD`, `PROBABILITY`,
    and `VALUE`) and returns the value of this attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the `toString()` method helps to represent the value, reward, and
    the probability. Now that we know how the data will be manipulated, the next task
    is to create a simple schema that initializes the reward and probability associated
    with each action. The following Scala case is a class named `QLInput`; it inputs
    to the Q-learning search space (`QLSpace`) and policy (`QLPolicy`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding signature, the constructor creates an action input to Q-learning.
    It takes four parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from`, the identifier for the source state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`to`, the identifier for the target or destination state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward`, which is the credit or penalty to transition from the state with
    id `from` to the state with id `to`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prob, the probability of transition from the state `from` to the state `to`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding class, the `from` and `to` arguments are used for a specific
    action, but the last two arguments are the reward collected at the completion
    of the action and its probability, respectively. Both the actions have a reward
    and a probability of 1 by default. In short, we only need to create an input for
    actions that have either a higher reward or a lower probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of states and the sequence of input define the policy of type `QLPolicy`,
    which is a data container. An action has a Q-value (also known as **action-value**),
    a reward, and a probability. The implementation defines these three values in
    three separate matrices—*Q* for the action values, *R* for rewards, and *P* for
    probabilities—in order to stay consistent with the mathematical formulation. Here
    is the workflow for this class:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the policy using the input probabilities and rewards (see the `qlData` variable).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the number of states from the input size (see the `numStates` variable).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the Q value for an action from state `from` to state `to` (see the `setQ`
    method) and get the Q-value using the `get()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the Q-value for a state transition action from state `from` to state
    `to` (see the Q method).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the estimate for a state transition action from state `from` to state
    `to` (see the `EQ` method), and return the value in a `double`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the reward for a state transition action from state `from` to state `to` (see
    the R method).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the probability for a state transition action from state `from` to
    state `to` (see the `P` method).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the minimum and maximum value for `Q` (see the `minMaxQ` method).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Retrieve the pair (index source state, index destination state) whose transition
    is a positive value. The index of the state is converted to a Double (see the `EQ:
    Vector[DblPair]` method).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the textual description of the reward matrix for this policy using the first
    `toString()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Textual representation of any one of the following: Q-value, reward, or probability
    matrix using the second `toString()` method.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Validate the `from` and `to` value using the `check()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now let us see the class definition consisting of the preceding workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: QLearning model creation and training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `QLearning` class encapsulates the Q-learning algorithm, more specifically
    the action-value updating equation. It is a data transformation of type `ETransform`
    (we will see this later on) with an explicit configuration of type `QLConfig`.
    This class is a generic parameterized class that implements the `QLearning` algorithm.
    The Q-learning model is initialized and trained during the instantiation of the
    class so it can be in the correct state for runtime prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the class instances have only two states: successfully trained and
    failed training (we''ll see this later).'
  prefs: []
  type: TYPE_NORMAL
- en: The implementation does not assume that every episode (or training cycle) will
    be successful. At the completion of training, the ratio of labels over the initial
    training set is computed. The client code is responsible for evaluating the quality
    of the model by testing the ratio (see the model evaluation section).
  prefs: []
  type: TYPE_NORMAL
- en: 'The constructor takes the configuration of the algorithm (that is, `config`),
    the search space (that is, `qlSpace`), and the policy (that is, `qlPolicy`) parameters
    and creates a Q-learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The model is automatically created effectively if the minimum coverage is reached
    (or trained) during instantiation of the class, which is essentially a Q-learning
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `train()` method is applied to each episode with randomly generated
    initial states. Then it computes the coverage (based on the `minCoverage` configuration
    value supplied by the `conf` object) as the number of episodes for each the goal
    state was reached:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, the `heavyLiftingTrain(state0: Int)` method does
    the heavy lifting at each episode (or epoch). It triggers the search by selecting
    either the initial state state 0 or a random generator *r* with a new seed, if
    `state0` is < 0.'
  prefs: []
  type: TYPE_NORMAL
- en: At first, it gets all the states adjacent to the current state, and then it
    selects the most rewarding of the list of adjacent states. If the next most rewarding
    state is a goal state, we are done. Otherwise, it recomputes the policy value
    for the state transition using the reward matrix (that is, `QLPolicy.R`).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the recomputation, it applies the Q-learning updating formula by updating
    the Q-Value for the policy; then it invokes the search method with the new state
    and incremented iterator. Let''s see the body of this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As a list of policies and training coverage is given, let us get the trained
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the preceding model is trained using the input data (see the class
    `QLPolicy`) used for training the Q-learning algorithm using the inline `getInput()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to do one of the most important steps that will be used in our
    options trading application. Therefore, we need to retrieve the model for Q-learning
    as an option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The overall application fails if the model is not defined (see the `validateConstraints()`
    method for validation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Then, a recursive computation of the next most rewarding state is performed
    using Scala tail recursion. The idea is to search among all states and recursively
    select the state with the most awards given for the best policy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, the `nextState()` method retrieves the eligible
    states that can be transitioned to from the current state. Then it extracts the
    state, `qState`, with the most rewarding policy by incrementing the iteration
    counter. Finally, it returns the states if there are no more states or if the
    method does not converge within the maximum number of allowed iterations supplied
    by the `config.episodeLength` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tail recursion**: In Scala, tail recursion is a very effective construct
    used to apply an operation to every item of a collection. It optimizes the management
    of the function stack frame during recursion. The annotation triggers a validation
    of the condition necessary for the compiler to optimize function calls.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the configuration of the Q-learning algorithm, `QLConfig`, specifies:'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate, `alpha`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discount rate, `gamma`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum number of states (or length) of an episode, `episodeLength`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of episodes (or epochs) used in training, `numEpisodes`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum coverage required to select the best policy, `minCoverage`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are almost done, except that the validation is not completed. However,
    let us first see the companion object for the configuration of the Q-learning
    algorithm. This singleton defines the constructor for the `QLConfig` class and
    validates its parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Excellent! We have seen how to implement the `QLearning` algorithm in Scala.
    However, as I said, the implementation is based on openly available sources, and
    the training may not always converge. One important consideration for such an
    online model is validation. A commercial application (or even a fancy Scala web
    app, which we will be covering in the next section) may require multiple types
    of validation mechanisms regarding the states transition, reward, probability,
    and Q-value matrices.
  prefs: []
  type: TYPE_NORMAL
- en: QLearning model validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One critical validation is to verify that the user-defined constraints function
    does not create a dead-end in the search or training of Q-learning. The function
    constraints establish the list of states that can be accessed from a given state
    through actions. If the constraints are too tight, some of the possible search
    paths may not reach the goal state. Here is a simple validation of the constraints
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Making predictions using the trained model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we can select the state with the most awards given for the best policy
    recursively (see the `nextState` method in the following code), an online training
    method for the Q-learning algorithm can be performed for options trading, for
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, once the Q-learning model is trained using the supplied data, the next
    state can be predicted using the Q-learning model by overriding the data transformation
    method (`PipeOperator`, that is, `|`) with a transformation of a state to a predicted
    goal state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: I guess that's enough of a mouthful, though it would have been good to evaluate
    the model. But evaluating on a real-life dataset, it would be even better, because
    running and evaluating a model's performance on fake data is like buying a new
    car and never driving it. Therefore, I would like to wrap up the implementation
    part and move on to the options trading application using this Q-learning implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Developing an options trading web app using Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The trading algorithm is the process of using computers programmed to follow
    a defined set of instructions for placing a trade in order to generate profits
    at a speed and frequency that is impossible for a human trader. The defined sets
    of rules are based on timing, price, quantity, or any mathematical model.
  prefs: []
  type: TYPE_NORMAL
- en: Problem description
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Through this project, we will predict the price of an option on a security
    for *N* days in the future according to the current set of observed features derived
    from the time of expiration, the price of the security, and volatility. The question
    would be: what model should we use for such an option pricing model? The answer
    is that there are actually many; Black-Scholes stochastic **partial differential
    equations** (**PDE**) is one of the most recognized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematical finance, the Black-Scholes equation is necessarily a PDE overriding
    the price evolution of a European call or a European put under the Black-Scholes
    model. For a European call or put on an underlying stock paying no dividends,
    the equation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/beeab1f3-57f6-47fa-9358-4a3b47f69bcc.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *V* is the price of the option as a function of stock price *S* and time
    *t*, *r* is the risk-free interest rate, and *σ* *σ* (displaystyle sigma) is the
    volatility of the stock. One of the key financial insights behind the equation
    is that anyone can perfectly hedge the option by buying and selling the underlying
    asset in just the right way without any risk. This hedge implies that there is
    only one right price for the option, as returned by the Black-Scholes formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a January maturity call option on an IBM with an exercise price of
    $95\. You write a January IBM put option with an exercise price of $85\. Let us
    consider and focus on the call options of a given security, IBM. The following
    chart plots the daily price of the IBM stock and its derivative call option for
    May 2014, with a strike price of $190:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7cce8043-97c9-41b7-a0e2-bd401127cfe0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7: IBM stock and call $190 May 2014 pricing in May-Oct 2013
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what will be the profit and loss be for this position if IBM is selling
    at $87 on the option maturity date? Alternatively, what if IBM is selling at $100?
    Well, it is not easy to compute or predict the answer. However, in options trading,
    the price of an option depends on a few parameters, such as time decay, price,
    and volatility:'
  prefs: []
  type: TYPE_NORMAL
- en: Time to expiration of the option (time decay)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The price of the underlying security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The volatility of returns of the underlying asset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A pricing model usually does not consider the variation in trading volume in
    terms of the underlying security. Therefore, some researchers have included it
    in the option trading model. As we have described, any RL-based algorithm should
    have an explicit state (or states), so let us define the state of an option using
    the following four normalized features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time decay** (`timeToExp`): This is the time to expiration once normalized
    in the range of (0, 1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relative volatility** (`volatility`): within a trading session, this is the
    relative variation of the price of the underlying security. It is different than
    the more complex volatility of returns defined in the Black-Scholes model, for
    example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volatility relative to volume** (`vltyByVol`): This is the relative volatility
    of the price of the security adjusted for its trading volume.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relative** **difference between the current price and the strike price**
    (`priceToStrike`): This measures the ratio of the difference between the price
    and the strike price to the strike price.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following graph shows the four normalized features that can be used for
    the IBM option strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96deb7b5-fd00-41aa-8eed-a514786ef05b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8: Normalized relative stock price volatility, volatility relative to
    trading volume, and price relative to strike price for the IBM stock
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let us look at the stock and the option price dataset. There are two files
    `IBM.csv` and `IBM_O.csv` contain the IBM stock prices and option prices, respectively.
    The stock price dataset has the date, the opening price, the high and low price,
    the closing price, the trade volume, and the adjusted closing price. A shot of
    the dataset is given in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28f78eae-73f9-450c-a168-a30ec57f50f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: IBM stock data'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, `IBM_O.csv` has 127 option prices for IBM Call 190 Oct 18,
    2014\. A few values are 1.41, 2.24, 2.42, 2.78, 3.46, 4.11, 4.51, 4.92, 5.41,
    6.01, and so on. Up to this point, can we develop a predictive model using a `QLearning`,
    algorithm that can help us answer the previously mentioned question: Can it tell
    us the how IBM can make maximum profit by utilizing all the available features?'
  prefs: []
  type: TYPE_NORMAL
- en: Well, we know how to implement the `QLearning`, and we know what option trading
    is. Another good thing is that the technologies that will be used for this project
    such as Scala, Akka, Scala Play Framework, and RESTful services are already discussed
    in [Chapter 3](51e66c26-e12b-4764-bbb7-444986c05870.xhtml), *High-Frequency Bitcoin
    Price Prediction from Historical Data*. Therefore, it may be possible. Then we
    try it to develop a Scala web project that helps us maximize the profit.
  prefs: []
  type: TYPE_NORMAL
- en: Implementating an options trading web application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of this project is to create an options trading web application that
    creates a QLearning model from the IBM stock data. Then the app will extract the
    output from the model as a JSON object and show the result to the user. *Figure
    10*, shows the overall workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edad61e5-68f9-4f3a-bd72-b09377596bf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10: Workflow of the options trading Scala web
  prefs: []
  type: TYPE_NORMAL
- en: 'The compute API prepares the input for the Q-learning algorithm, and the algorithm
    starts by extracting the data from the files to build the option model. Then it
    performs operations on the data such as normalization and discretization. It passes
    all of this to the Q-learning algorithm to train the model. After that, the compute
    API gets the model from the algorithm, extracts the best policy data, and puts
    it onto JSON to be returned to the web browser. Well, the implementation of the
    options trading strategy using Q-learning consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Describing the property of an option
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the function approximation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying the constraints on the state transition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an option property
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Considering the market volatility, we need to be a bit more realistic, because
    any longer-term prediction is quite unreliable. The reason is that it would fall
    outside the constraint of the discrete Markov model. So, suppose we want to predict
    the price for next two days—that is, *N= 2*. That means the price of the option
    two days in the future is the value of the reward profit or loss. So, let us encapsulate
    the following four parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`timeToExp`: Time left until expiration as a percentage of the overall duration
    of the option'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volatility normalized Relative volatility of the underlying security for a given
    trading session
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vltyByVol`: Volatility of the underlying security for a given trading session
    relative to a trading volume for the session'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`priceToStrike`: Price of the underlying security relative to the Strike price
    for a given trading session'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `OptionProperty` class defines the property of a traded option on a security.
    The constructor creates the property for an option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Creating an option model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we need to create an `OptionModel` to act as the container and the factory
    for the properties of the option. It takes the following parameters and creates
    a list of option properties, `propsList`, by accessing the data source of the
    four features described earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: The symbol of the security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strike price for `option`, `strikePrice`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source of the `data`, `src`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum time decay or time to expiration, `minTDecay`. Out-of-the-money
    options expire worthlessly, and in-the-money options have a very different price
    behavior as they get closer to the expiration. Therefore, the last `minTDecay`
    trading sessions prior to the expiration date are not used in the training process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The number of steps (or buckets), `nSteps`, is used in approximating the values
    of each feature. For instance, an approximation of four steps creates four buckets:
    (0, 25), (25, 50), (50, 75), and (75, 100).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then it assembles `OptionProperties` and computes the normalized minimum time
    to the expiration of the option. Then it computes an approximation of the value
    of options by discretization of the actual value in multiple levels from an array
    of options prices; finally it returns a map of an array of levels for the option
    price and accuracy. Here is the constructor of the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside this class implementation, at first, a validation is done using the
    `check()` method, by checking the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`strikePrice`: A positive price is required'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minExpT`: This has to be between 2 and 16'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nSteps`: Requires a minimum of two steps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s the invocation of this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The signature of the preceding method is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the preceding constraint is satisfied, the list of option properties,
    named `propsList`, is created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, the factory uses the `zipWithIndex` Scala method
    to represent the index of the trading sessions. All feature values are normalized
    over the interval (0, 1), including the time decay (or time to expiration) of
    the `normDecay` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `quantize()` method of the `OptionModel` class converts the normalized
    value of each option property of features into an array of bucket indices. It
    returns a map of profit and loss for each bucket keyed on the array of bucket
    indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The method also creates a mapper instance to index the array of buckets. An
    accumulator, `acc`, of type `NumericAccumulator` extends the `Map[Int, (Int, Double)]`
    and computes this tuple *(number of occurrences of features on each bucket, sum
    of the increase or decrease of the option price)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `toArrayInt` method converts the value of each option property (`timeToExp`,
    `volatility`, and so on) into the index of the appropriate bucket. The array of
    indices is then encoded to generate the id or index of a state. The method updates
    the accumulator with the number of occurrences and the total profit and loss for
    a trading session for the option. It finally computes the reward on each action
    by averaging the profit and loss on each bucket. The signature of the `encode()`,
    `toArrayInt()` is given in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Finally, and most importantly, if the preceding constraints are satisfied (you
    can modify these constraints though) and once the instantiation of the `OptionModel`
    class generates a list of `OptionProperty` elements if the constructor succeeds; otherwise, it
    generates an empty list.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it altogether
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because we have implemented the Q-learning algorithm, we can now develop the
    options trading application using Q-learning. However, at first, we need to load
    the data using the `DataSource` class (we will see its implementation later on).
    Then we can create an option model from the data for a given stock with default
    strike and minimum expiration time parameters, using `OptionModel`, which defines
    the model for a traded option, on a security. Then we have to create the model
    for the profit and loss on an option given the underlying security.
  prefs: []
  type: TYPE_NORMAL
- en: The profit and loss are adjusted to produce positive values. It instantiates
    an instance of the Q-learning class, that is, a generic parameterized class that
    implements the Q-learning algorithm. The Q-learning model is initialized and trained
    during the instantiation of the class, so it can be in the correct state for the
    runtime prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the class instances have only two states: successfully trained and
    failed training Q-learning value action. Then the model is returned to get processed
    and visualized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let us create a Scala object and name it `QLearningMain`. Then, inside
    the `QLearningMain` object, define and initialize the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Name`: Used to indicate the reinforcement algorithm''s name (for our case,
    it''s Q-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`STOCK_PRICES`: File that contains the stock data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OPTION_PRICES`: File that contains the available option data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`STRIKE_PRICE`: Option strike price'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MIN_TIME_EXPIRATION`: Minimum expiration time for the option recorded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QUANTIZATION_STEP`: Steps used in discretization or approximation of the value
    of the security'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ALPHA`: Learning rate for the Q-learning algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DISCOUNT` (gamma): Discount rate for the Q-learning algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MAX_EPISODE_LEN`: Maximum number of states visited per episode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NUM_EPISODES`: Number of episodes used during training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MIN_COVERAGE`: Minimum coverage allowed during the training of the Q-learning
    model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NUM_NEIGHBOR_STATES`: Number of states accessible from any other state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`REWARD_TYPE`: Maximum reward or Random'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tentative initializations for each parameter are given in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the `run()` method accepts as input the reward type (`Maximum reward` in
    our case), quantized step (in our case, `QUANTIZATION_STEP`), alpha (the learning
    rate, `ALPHA` in our case) and gamma (in our case, it''s `DISCOUNT`, the discount
    rate for the Q-learning algorithm). It displays the distribution of values in
    the model. Additionally, it displays the estimated Q-value for the best policy
    on a Scatter plot (we will see this later). Here is the workflow of the preceding
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it extracts the stock price from the `IBM.csv` file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then it creates an option model `createOptionModel` using the stock prices and
    quantization, `quantizeR` (see the `quantize` method for more and the main method
    invocation later)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The option prices are extracted from the `IBM_o.csv` file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, another model, `model`, is created using the option model to evaluate
    it on the option prices, `oPrices`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the estimated Q-Value (that is, *Q-value = value * probability*) is
    displayed 0n a Scatter plot using the `display` method
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By amalgamating the preceding steps, here''s the signature of the `run()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now here is the signature of the `createOptionModel()` method that creates
    an option model using (see the `OptionModel` class):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Then the `createModel()` method creates a model for the profit and loss on an
    option given the underlying security. Note that the option prices are quantized
    using the `quantize()` method defined earlier. Then the constraining method is
    used to limit the number of actions available to any given state. This simple
    implementation computes the list of all the states within a radius of this state.
    Then it identifies the neighboring states within a predefined radius.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it uses the input data to train the Q-learning model to compute the
    minimum value for the profit, a loss so the maximum loss is converted to a null
    profit. Note that the profit and loss are adjusted to produce positive values.
    Now let us see the signature of this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that if the preceding invocation cannot create an option model, the code
    fails to show a message that the model creation failed. Nonetheless, remember
    that the `minCoverage` used in the following line is important, considering the
    small dataset we used (because the algorithm will converge very quickly):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Although we''ve already stated that it is not assured that the model creation
    and training will be successful, a Naïve clue would be using a very small `minCoverage`
    value between `0.0` and `0.22`. Now, if the preceding invocation is successful,
    then the model is trained and ready for making prediction. If so, then the display
    method is used to display the estimated *Q-value = value * probability* in a Scatter
    plot. Here is the signature of the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Hang on and do not lose patience! We are finally ready to see a simple `rn`
    and inspect the result. So let us do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding output shows the transition from one state to another, and for
    the **0.1** coverage, the `QLearning` model had 15,750 transitions for 126 states
    to reach goal state 37 with optimal rewards. Therefore, the training set is quite
    small and only a few buckets have actual values. So we can understand that the
    size of the training set has an impact on the number of states. `QLearning` will
    converge too fast for a small training set (like what we have for this example).
  prefs: []
  type: TYPE_NORMAL
- en: However, for a larger training set, `QLearning` will take time to converge;
    it will provide at least one value for each bucket created by the approximation.
    Also, by seeing those values, it is difficult to understand the relation between
    Q-values and states.
  prefs: []
  type: TYPE_NORMAL
- en: 'So what if we can see the Q-values per state? Why not! We can see them on a
    scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca965320-2e1b-4071-8bcd-12deb0e638da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Q-value per state'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let us display the profile of the log of the Q-value (`QLData.value`) as
    the recursive search (or training) progress for different episodes or epochs.
    The test uses a learning rate *α = 0.1* and a discount rate *γ = 0.9* (see more
    in the deployment section):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d881e918-f728-4520-826c-62a84b78c31a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Profile of the logarithmic Q-Value for different epochs during Q-learning
    training'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding chart illustrates the fact that the Q-value for each profile is
    independent of the order of the epochs during training. However, the number of
    iterations to reach the goal state depends on the initial state selected randomly
    in this example. To get more insights, inspect the output on your editor or access
    the API endpoint at `http://localhost:9000/api/compute` (see following). Now,
    what if we display the distribution of values in the model and display the estimated
    Q-value for the best policy on a Scatter plot for the given configuration parameters?
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/97ed3adc-210c-4522-bf60-22f75a28d47d.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13: Maximum reward with quantization 32 with the QLearning'
  prefs: []
  type: TYPE_NORMAL
- en: 'The final evaluation consists of evaluating the impact of the learning rate
    and discount rate on the coverage of the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94fc5941-bbad-41f5-97ae-f9219bba71d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Impact of the learning rate and discount rate on the coverage of
    the training'
  prefs: []
  type: TYPE_NORMAL
- en: The coverage decreases as the learning rate increases. This result confirms
    the general rule of using *learning rate* *< 0.2*. A similar test to evaluate
    the impact of the discount rate on the coverage is inconclusive. We could have
    thousands of such configuration parameters with different choices and combinations.
    So, what if we can wrap the whole application as a Scala web app similar to what
    we did in [Chapter 3](51e66c26-e12b-4764-bbb7-444986c05870.xhtml), *High-Frequency
    Bitcoin Price Prediction from Historical Data*? I guess it would not be that bad
    an idea. So let us dive into it.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping up the options trading app as a Scala web app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea is to get the trained model and construct the best policy JSON output
    for the maximum reward case. `PlayML` is a web app that uses the options trading
    Q-learning algorithm to provide a compute API endpoint that takes the input dataset
    and some options to calculate the q-values and returns them in JSON format to
    be modeled in the frontend.
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapped up Scala web ML app has the following directory structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9c7cd08-9159-4dad-8f7b-d2a920c6fc22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Scala ML web app directory structure'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding structure, the app folder has both the original QLearning
    implementation (see the `ml` folder) and some additional backend code. The `controller` subfolder has
    a Scala class named `API.scala`, used as the Scala controller for controlling
    the model behavior from the frontend. Finally, `Filters.scala` acts as the `DefaultHttpFilters`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06994b75-70b4-434d-87e1-40d0b40a16c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: The ml directory structure'
  prefs: []
  type: TYPE_NORMAL
- en: The `conf` folder has the Scala web app configuration file, `application.conf`,
    containing the necessary configurations. All the dependencies are defined in the
    `build.sbt` file, as shown in the following code*:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The `lib` folder has some `.jar` files used as external dependencies defined
    in the `build.sbt` file*.* The `public` folder has the static pages used in the
    UI. Additionally, the data files `IBM.csv` and `IBM_O.csv` are also there. Finally,
    the target folder holds the application as a packaged (if any).
  prefs: []
  type: TYPE_NORMAL
- en: The backend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the backend, I encapsulated the preceding Q-learning implementation and
    additionally created a Scala controller that controls the model behavior from
    the frontend. The structure is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the preceding code carefully; it has more or less the same structure
    as the `QLearningMain.scala` file. There are only two important things here, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute is done as an Action that takes the input from the UI and computes the
    value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then the result is returned as a JSON object using the `JsObject()` method to
    be shown on the UI (see the following)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The frontend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The app consists of two main parts: the API endpoint, built with the play framework,
    and the frontend single-page application, built with `Angular.js`. The frontend
    app sends the data to the API to get computed and then shows the results using
    `chart.js`. Here are the steps that we need for this:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the form
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communicate with the API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Populate the view with coverage data and charts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The algorithm''s JSON output should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: All the config parameters are returned
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GOAL_STATE_INDEX`, the maximum Profit Index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COVERAGE`, the ratio of training trials or epochs that reach a predefined
    goal state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COVERAGE_STATES`, the size of the quantized option values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`COVERAGE_TRANSITIONS`, the number of states squared'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Q_VALUE`, the q-value of all the states'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OPTIMAL`, the states with the most reward returned if the reward type isn''t
    random'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The frontend code** initiates the `Angular.js` app with the `chart.js` module
    as follows (see in the `PlayML/public/assets/js/main.js` file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the run button action prepares the form data to be sent to the API and
    sends the data to the backend. Next, it passes the returned data to the result
    variable to be used in the frontend. Then, it clears the charts and recreates
    them; if an optimal is found, it initializes the optimal chart. Finally, if the
    Q-value is found initialize, the q-value chart is getting initialized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The preceding frontend code is then embedded in the HTML (see `PlayML/public/index.html`)
    to get the UI to be accessed on the Web as a fancy app at `http://localhost:9000/`.
    Feel free to edit the content according to your requirement. We will see the details
    soon.
  prefs: []
  type: TYPE_NORMAL
- en: Running and Deployment Instructions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As was already stated in [Chapter 3](51e66c26-e12b-4764-bbb7-444986c05870.xhtml),
    *High-Frequency Bitcoin Price Prediction from Historical Data*, you need Java
    1.8+ and SBT as the dependencies. Then follow these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the app. I named the code `PlayML.zip`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unzip the file and you will get the folder `ScalaML`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go to the PlayML project folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run `$ sudo sbt run` to download all the dependencies and run the app.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then the application can be accessed at `http://localhost:9000/`, where we
    can upload the IBM stock and option prices and, of course, provide other config
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db3fee1d-a5a3-495e-99f3-1019e92ac422.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: The UI of options trading using QLearning'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you upload the stock price and option price data and click on the run
    button, a graph will be generated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e0609eb-4652-44ce-84ef-b70bbc57c84a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: QLearning reaches goal state 81 with a coverage of 0.2 for 126 states
    and 15,750 transitions'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the API endpoint can be accessed at [http://localhost:9000/api/compute](http://localhost:9000/api/compute).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb375ce1-4039-41c1-97e4-7c6924a92171.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: The API endpoint (abridged)'
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can easily deploy your application as a standalone server by setting the
    application HTTP port to 9000, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that you probably need root permissions to bind a process to this port.
    Here is a short workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `$ sudo sbt dist` to build application binary. The output can be found at
    `PlayML /target/universal/APP-NAME-SNAPSHOT.zip`. In our case, it's `playml-1.0.zip`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, to run the application, unzip the file and then run the script in the
    `bin` directory:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you need to configure your web server to map to the app port configuration.
    Nevertheless, you can easily deploy your application as a standalone server by
    setting the application HTTP port to `9000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: However, if you plan to host several applications on the same server or load-balance
    several instances of your application for scalability or fault tolerance, you
    can use a frontend HTTP server. Note that using a frontend HTTP server will rarely
    give you better performance than using a Play server directly.
  prefs: []
  type: TYPE_NORMAL
- en: However, HTTP servers are very good at handling HTTPS, conditional GET requests,
    and static assets, and many services assume that a frontend HTTP server is part
    of your architecture. Additional information can be found at [https://www.playframework.com/documentation/2.6.x/HTTPServer](https://www.playframework.com/documentation/2.6.x/HTTPServer).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to develop a real-life application called options
    trading using a RL algorithm called Q-learning. The IBM stock datasets were used
    to design a machine learning system driven by criticisms and rewards. Additionally,
    we learned some theoretical background. Finally, we learned how to wrap up a Scala
    desktop application as a web app using Scala Play Framework and deploy it in production.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see two examples of building very robust and accurate
    predictive models for predictive analytics using H2O on a bank marketing dataset.
    For this example, we will be using bank marketing datasets. The data is related
    to direct marketing campaigns of a Portuguese banking institution. The marketing
    campaigns were based on phone calls. The goal of this end-to-end project will
    be to predict that the client will subscribe a term deposit.
  prefs: []
  type: TYPE_NORMAL
