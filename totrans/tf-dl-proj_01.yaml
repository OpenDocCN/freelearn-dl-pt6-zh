- en: Recognizing traffic signs using Convnets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the first project of the book, we''ll try to work on a simple model where
    deep learning performs very well: traffic sign recognition. Briefly, given a color
    image of a traffic sign, the model should recognize which signal it is. We will
    explore the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: How the dataset is composed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which deep network to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to pre-process the images in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train and make predictions with an eye on performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we'll try to predict some traffic signs using their images, we will use
    a dataset built for the same purpose. Fortunately, researchers of Institute für
    Neuroinformatik, Germany, created a dataset containing almost 40,000 images, all
    different and related to 43 traffic signs. The dataset we will use is part of
    a competition named **German Traffic Sign Recognition Benchmark** (**GTSRB**),
    which attempted to score the performance of multiple models for the same goal.
    The dataset is pretty old—2011! But it looks like a nice and well-organized dataset
    to start our project from.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used in this project is freely available at [http://benchmark.ini.rub.de/Dataset/GTSRB_Final_Training_Images.zip](http://benchmark.ini.rub.de/Dataset/GTSRB_Final_Training_Images.zip).
  prefs: []
  type: TYPE_NORMAL
- en: Before you start running the code, please download the file and unpack it in
    the same directory as the code. After decompressing the archive, you'll have a
    new folder, named GTSRB, containing the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the book would like to thank those who worked on the dataset
    and made it open source.
  prefs: []
  type: TYPE_NORMAL
- en: Also, refer [http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)
    to learn more about CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Speed limit 20 km/h":'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fccc98ce-8fa7-42b6-b84a-0c099ce0f4a6.png)'
  prefs: []
  type: TYPE_IMG
- en: '"go straight or turn right":'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0af10cb0-4f00-4137-bf30-68fefc3df1df.png)'
  prefs: []
  type: TYPE_IMG
- en: '"roundabout":'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45a70e63-353a-485d-8006-2662616623cc.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the signals don't have a uniform brightness (some are very dark
    and some others are very bright), they're different in size, the perspective is
    different, they have different backgrounds, and they may contain pieces of other
    traffic signs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is organized in this way: all the images of the same label are
    inside the same folder. For example, inside the path `GTSRB/Final_Training/Images/00040/`,
    all the images have the same label, `40`. For the images with another label, `5`,
    open the folder `GTSRB/Final_Training/Images/00005/`. Note also that all the images
    are in PPM format, a lossless compression format for images with many open source
    decoders/encoders.'
  prefs: []
  type: TYPE_NORMAL
- en: The CNN network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For our project, we will use a pretty simple network with the following architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed38d09a-6f57-4af7-bf66-8224e098e188.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this architecture, we still have the choice of:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of filters and kernel size in the 2D convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kernel size in the Max pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of units in the Fully Connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The batch size, optimization algorithm, learning step (eventually, its decay
    rate), activation function of each layer, and number of epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first operation of the model is reading the images and standardizing them.
    In fact, we cannot work with images of variable sizes; therefore, in this first
    step, we'll load the images and reshape them to a predefined size (32x32). Moreover,
    we will one-hot encode the labels in order to have a 43-dimensional array where
    only one element is enabled (it contains a 1), and we will convert the color space
    of the images from RGB to grayscale. By looking at the images, it seems obvious
    that the information we need is not contained in the color of the signal but in
    its shape and design.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now open a Jupyter Notebook and place some code to do that. First of
    all, let''s create some final variables containing the number of classes (43)
    and the size of the images after being resized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will write a function that reads all the images given in a path, resize
    them to a predefined shape, convert them to grayscale, and also one-hot encode
    the label. In order to do that, we''ll use a named tuple named `dataset`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Thanks to the skimage module, the operation of reading, transforming, and resizing
    is pretty easy. In our implementation, we decided to convert the original color
    space (RGB) to lab, then retaining only the luminance component. Note that another
    good conversion here is YUV, where only the "Y" component should be retained as
    a grayscale image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the preceding cell gives this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'One note about the output format: the shape of the observation matrix *X* has
    four dimensions. The first indexes the observations (in this case, we have almost
    40,000 of them); the other three dimensions contain the image (which is 32 pixel,
    by 32 pixels grayscale, that is, one-dimensional). This is the default shape when
    dealing with images in TensorFlow (see the code `_tf_format` function).'
  prefs: []
  type: TYPE_NORMAL
- en: As for the label matrix, the rows index the observation, while the columns are
    the one-hot encoding of the label.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to have a better understanding of the observation matrix, let''s print
    the feature vector of the first sample, together with its label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/244954aa-b5cb-45a5-a0be-8b4576ff5a95.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the image, that is, the feature vector, is 32x32\. The label
    contains only one `1` in the first position.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now print the last sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/255c7721-64e7-411f-8020-b03bab04a1db.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The feature vector size is the same (32x32), and the label vector contains one
    `1` in the last position.
  prefs: []
  type: TYPE_NORMAL
- en: These are the two pieces of information we need to create the model. Please,
    pay particular attention to the shapes, because they're crucial in deep learning
    while working with images; in contrast to classical machine learning observation
    matrices, here the *X* has four dimensions!
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step of our preprocessing is the train/test split. We want to train
    our model on a subset of the dataset, and then measure the performance on the
    leftover samples, that is, the test set. To do so, let''s use the function provided
    by `sklearn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we''ll use 75% of the samples in the dataset for training
    and the remaining 25% for testing. In fact, here''s the output of the previous
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Train the model and make predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing to have is a function to create minibatches of training data.
    In fact, at each training iteration, we''d need to insert a minibatch of samples
    extracted from the training set. Here, we''ll build a function that takes the
    observations, labels, and batch size as arguments and returns a minibatch generator.
    Furthermore, to introduce some variability in the training data, let''s add another
    argument to the function, the possibility to shuffle the data to have different
    minibatches of data for each generator. Having different minibatches of data in
    each generator will force the model to learn the in-out connection and not memorize
    the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To test this function, let''s print the shapes of minibatches while imposing
    `batch_size=10000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'That prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Unsurprisingly, the 29,406 samples in the training set are split into two minibatches
    of 10,000 elements, with the last one of `9406` elements. Of course, there are
    the same number of elements in the label matrix too.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s now time to build the model, finally! Let''s first build the blocks that
    will compose the network. We can start creating the fully connected layer with
    a variable number of units (it''s an argument), without activation. We''ve decided
    to use Xavier initialization for the coefficients (weights) and 0-initialization
    for the biases to have the layer centered and scaled properly. The output is simply
    the multiplication of the input tensor by the weights, plus the bias. Please take
    a look at the dimensionality of the weights, which is defined dynamically, and
    therefore can be used anywhere in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now create the fully connected layer with activation; specifically,
    here we will use the leaky ReLU. As you can see, we can build this function using
    the previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s create a convolutional layer that takes as arguments the input
    data, kernel size, and number of filters (or units). We will use the same activations
    used in the fully connected layer. In this case, the output passes through a leaky
    ReLU activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to create a `maxpool_layer`. Here, the size of the window and
    the strides are both squares (quadrates):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The last thing to define is the dropout, used for regularizing the network.
    Pretty simple thing to create, but remember that dropout should only be used when
    training the network, and not when predicting the outputs; therefore, we need
    to have a conditional operator to define whether to apply dropouts or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it''s time to put it all together and create the model as previously
    defined. We''ll create a model composed of the following layers:'
  prefs: []
  type: TYPE_NORMAL
- en: 2D convolution, 5x5, 32 filters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2D convolution, 5x5, 64 filters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flattenizer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fully connected later, 1,024 units
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dropout 40%
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fully connected layer, no activation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Softmax output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: And now, let's write the function to train the model on the training set and
    test the performance on the test set. Please note that all of the following code
    belongs to the function `train_model` function; it's broken down in to pieces
    just for simplicity of explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function takes as arguments (other than the training and test sets and
    their labels) the learning rate, the number of epochs, and the batch size, that
    is, number of images per training batch. First things first, some TensorFlow placeholders
    are defined: one for the minibatch of images, one for the minibatch of labels,
    and the last one to select whether to run for training or not (that''s mainly
    used by the dropout layer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define the output, metric score, and optimizer. Here, we decided
    to use the `AdamOptimizer` and the cross entropy with `softmax(logits)` as loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, here''s the code for training the model with minibatches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After the training, it''s time to test the model on the test set. Here, instead
    of sending a minibatch, we will use the whole test set. Mind it! `is_training` should
    be set as `False` since we don''t want to use the dropouts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And, as a final operation, let''s print the classification report and plot
    the confusion matrix (and its `log2` version) to see the misclassifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s run the function with some parameters. Here, we will run the
    model with a learning step of 0.001, 256 samples per minibatch, and 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This is followed by the classification report per class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we managed to reach a precision of `0.99` on the test set; also,
    recall and f1 score have the same score. The model looks stable since the loss
    in the test set is similar to the one reported in the last iteration; therefore,
    we're not over-fitting nor under-fitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'And the confusion matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1133d1f1-0f4e-49f0-9102-dae691a5c4f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the `log2` version of preceding screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b1e724e-aa03-4b01-a9d2-22f38c6089d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Follow-up questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Try adding/removing some CNN layers and/or fully connected layers. How does
    the performance change?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This simple project is proof that dropouts are necessary for regularization.
    Change the dropout percentage and check the overfitting-underfitting in the output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, take a picture of multiple traffic signs in your city, and test the trained
    model in real life!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to recognize traffic signs using a convolutional
    neural network, or CNN. In the next chapter, we'll see something more complex
    that can be done with CNNs.
  prefs: []
  type: TYPE_NORMAL
