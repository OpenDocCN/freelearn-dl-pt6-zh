- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: AI for Autonomous Vehicles – Build a Self-Driving Car
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自主车辆的人工智能——构建一辆自动驾驶汽车
- en: I'm really pumped up for you to start this new chapter. It's probably the most
    challenging, and most fun, adventure we'll have in this book. You're literally
    about to build a self-driving car from scratch, on a 2D map, using the powerful
    deep Q-learning model. I think that's incredibly exciting!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我真是非常激动，期待你开始这个新章节。这可能是本书中最具挑战性，同时也是最有趣的冒险。你将从零开始，使用强大的深度 Q 学习模型，构建一辆 2D 地图上的自动驾驶汽车。我觉得这真是令人兴奋！
- en: Think fast; what's our first step?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 快点想想，第一步是什么？
- en: If you answered "building the environment," you're absolutely right. I hope
    that's getting so familiar to you that you answered before I even finished the
    question. Let's start by building an environment in which a car can learn how
    to drive by itself.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回答了“构建环境”，你完全正确。我希望这个答案已经让你感到如此熟悉，以至于我还没问完你就回答出来了。让我们从构建一个汽车可以自己学习如何驾驶的环境开始。
- en: Building the environment
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建环境
- en: This time, we have much more to define than just the states, actions, and rewards.
    Building a self-driving car is a seriously complex problem. Now, I'm not going
    to ask you to go to your garage and turn yourself into a hybrid AI mechanic; you're
    simply going to build a virtual self-driving car that moves around a 2D map.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们需要定义的内容远不止状态、动作和奖励。构建一辆自动驾驶汽车是一个非常复杂的问题。现在，我不会要求你去车库，把自己变成一位混合型 AI 机械师；你只需在一个
    2D 地图上构建一个虚拟的自动驾驶汽车，让它四处移动。
- en: 'You''ll build this 2D map inside a Kivy web app. Kivy is a free and open source
    Python framework, used for the development of applications like games, or really
    any kind of mobile app. Check out the website here: [https://kivy.org/#home](https://kivy.org/#home).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在 Kivy 网络应用中构建这个 2D 地图。Kivy 是一个免费的开源 Python 框架，用于开发类似游戏的应用，或者任何种类的移动应用。你可以在这里查看网站：[https://kivy.org/#home](https://kivy.org/#home)。
- en: The whole environment for this project is built with Kivy, from start to finish.
    The development of the map and the virtual car has nothing to do with AI, so we
    won't go line by line through the code that implements it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的整个环境都是用 Kivy 构建的，从头到尾。地图和虚拟汽车的开发与 AI 无关，因此我们不会逐行讲解实现它的代码。
- en: However, I am going to describe the features of the map. For those of you curious
    to know about exactly how the map is built, I've provided a fully commented Python
    file in the GitHub named `map_commented.py` that builds the environment from scratch
    with a full explanation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我将描述一下地图的功能。对于那些好奇地图是如何构建的朋友，我已经在 GitHub 上提供了一个完全注释过的 Python 文件，名为 `map_commented.py`，它从头开始构建这个环境，并提供了详细的解释。
- en: 'Before we look at all the features, let''s have a look at this map with the
    little virtual car inside:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看所有功能之前，让我们先看一下这个地图，上面有一辆小虚拟车：
- en: '![](img/B14110_10_01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_01.png)'
- en: 'Figure 1: The map'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：地图
- en: The first thing you'll notice is a black screen, which is the Kivy user interface.
    You build your games or apps inside this interface. As you might guess, it's actually
    the container of the whole environment.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先会注意到的是一个黑色的屏幕，那就是 Kivy 用户界面。你在这个界面内构建你的游戏或应用。正如你可能猜到的，它实际上是整个环境的容器。
- en: You can see something weird inside, a white rectangle with three colored dots
    in front of it. Well, that's the car! My apologies for not being a better artist,
    but it's important to keep things simple. The white little rectangle is the shape
    of the car, and the three little dots are the sensors of the car. Why do we need
    sensors? Because on this map, we will have the option to build roads, delimited
    by sand, which the car will have to avoid going through.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到里面有些奇怪的东西，一个白色的矩形，前面有三个彩色的圆点。嗯，那就是汽车！抱歉，我不是个更好的艺术家，但保持简单是很重要的。那个白色的小矩形就是汽车的形状，而那三个小圆点是汽车的传感器。为什么我们需要传感器呢？因为在这个地图上，我们将有选项构建被沙地隔开的道路，而汽车必须避免通过这些沙地。
- en: To put some sand on the map, simply keep pressing left with your mouse and draw
    whatever you want. It doesn't have to just be roads; you can add some obstacles
    as well. In any case, the car will have to avoid going through the sand.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要在地图上放一些沙子，只需持续按住鼠标左键并绘制你想要的任何内容。它不一定只是道路；你还可以添加一些障碍物。无论如何，汽车必须避免通过沙地。
- en: 'If you remember that everything works from the rewards, I''m sure you already
    know how to make that happen; it''s by penalizing the self-driving car with a
    bad reward when it goes onto the sand. We''ll take care of that later. In the
    meantime, let''s have a look at one of my nice drawings of roads with sand:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得一切都来自于奖励，我相信你已经知道如何实现这一点；那就是当自动驾驶汽车驶上沙地时，通过给予它一个不好的奖励来进行惩罚。我们稍后会处理这个问题。与此同时，让我们看一下我绘制的带有沙子的道路示意图：
- en: '![](img/B14110_10_02.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_02.png)'
- en: 'Figure 2: Map with a drawn road'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：带有绘制道路的地图
- en: The sensors are there to detect the sand, so the car can avoid it. The blue
    sensor covers an area at the left of the car, the red sensor covers an area at
    the front of the car, and the yellow sensor covers an area at the right of the
    car.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器用来检测沙子，以便汽车能够避免。蓝色传感器覆盖汽车左侧的区域，红色传感器覆盖汽车前方的区域，黄色传感器覆盖汽车右侧的区域。
- en: 'Finally, there are three buttons to click on at the bottom left corner of the
    screen, which are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在屏幕的左下角有三个按钮可以点击，它们是：
- en: '**clear**: Removes all the sand drawn on the map'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**clear**: 移除地图上绘制的所有沙子'
- en: '**save**: Saves the weights (parameters) of the AI'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**save**: 保存AI的权重（参数）'
- en: '**load**: Loads the last saved weights'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**load**: 加载最后保存的权重'
- en: Now we've had a look at our little map, let's move on to defining our goals.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过了这张小地图，接下来让我们定义一下我们的目标。
- en: Defining the goal
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义目标
- en: We understand that our goal is to build a self-driving car. Good. But how are
    we going to formalize that goal, in terms of AI and reinforcement learning? Your
    intuition should hopefully make you think about the rewards we're going to set.
    I agree—we're going to give a high reward to our car if it manages to self-drive.
    But how can we tell that it's managing to self-drive?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解我们的目标是构建一辆自动驾驶汽车。很好。那么我们如何在AI和强化学习的框架下对这个目标进行形式化定义呢？希望你的直觉已经让你想到了我们将设定的奖励。我同意——如果我们的汽车成功实现自动驾驶，我们会给予它一个高奖励。但是，我们怎么判断它已经成功实现了自动驾驶呢？
- en: 'We''ve got plenty of ways to evaluate this. For example, we could simply draw
    some obstacles on the map, and train our self-driving car to move around the map
    without hitting the obstacles. That''s a simple challenge, but we could try something
    a little more fun. Remember the road I drew earlier? How about we train our car
    to go from the upper left corner of the map, to the bottom right corner, through
    any road we build between these two spots? That''s a real challenge, and that''s
    what we''ll do. Let''s imagine that the map is a city, where the upper left corner
    is the Airport, and the bottom right corner is Downtown:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有很多方法可以评估这个问题。例如，我们可以在地图上画一些障碍物，并训练我们的自动驾驶汽车在不碰到障碍物的情况下绕过它们。这是一个简单的挑战，但我们可以尝试一些更有趣的挑战。还记得我之前画的那条路吗？那我们来训练汽车从地图的左上角到右下角，沿着我们在这两个点之间建立的任何道路行驶。这才是一个真正的挑战，这就是我们要做的。假设这张地图是一个城市，左上角是机场，右下角是市中心：
- en: '![](img/B14110_10_03.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_03.png)'
- en: 'Figure 3: The two destinations – Airport and Downtown'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：两个目的地——机场和市中心
- en: 'Now we can clearly formulate a goal; to train the self-driving car to make
    round trips between the Airport and Downtown. As soon as it reaches the Airport,
    it will then have to go to Downtown, and as soon as it reaches Downtown, it will
    then have to go to the Airport. More than that, it should be able to make these
    round trips along any road connecting these two locations. It should also be able
    to cope with any obstacles along that road it has to avoid. Here is an example
    of another, more challenging road:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以清楚地制定一个目标；训练自动驾驶汽车在机场和市中心之间往返。当它到达机场时，就必须前往市中心；而当它到达市中心时，又必须返回机场。更重要的是，它应该能够沿着任何连接这两个地点的道路完成这些往返。它还应该能够应对道路上任何需要避免的障碍。这里是另一条更具挑战性的道路示例：
- en: '![](img/B14110_10_04.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_04.png)'
- en: 'Figure 4: A more challenging road'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：更具挑战性的道路
- en: 'If you think that road look too easy, here''s a more challenging example; this
    time with not only a more difficult road but also many obstacles:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为这条路看起来太简单，这里有一个更具挑战性的例子；这次不仅是更难的道路，而且还有许多障碍：
- en: '![](img/B14110_10_05.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_05.png)'
- en: 'Figure 5: An even more challenging road'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：一条更具挑战性的道路
- en: 'As a final example, I want to share this last map, designed by one of my students,
    which could belong in the movie *Inception*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个例子，我想分享这张由我的一位学生设计的地图，它可能出现在电影*盗梦空间*中：
- en: '![](img/B14110_10_06.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_06.png)'
- en: 'Figure 6: The most challenging road ever'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：有史以来最具挑战性的道路
- en: If you look closely, it's still a path that goes from Airport to Downtown and
    vice versa, just much more challenging. The AI we create will be able to cope
    with any of these maps.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细看，它仍然是从机场到市中心来回的道路，只不过现在变得更具挑战性了。我们创建的 AI 将能够应对这些地图中的任何一种。
- en: I hope you find that as exciting as I do! Keep that level of energy up, because
    we have quite a lot of work to do.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能和我一样感到兴奋！保持这种能量，因为我们还有很多工作要做。
- en: Setting the parameters
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置参数
- en: 'Before you define the input states, the output actions, and the rewards, you
    must set all of the parameters of the map and the car that will be part of your
    environment. The inputs, outputs, and rewards are all functions of these parameters.
    Let''s list them all, using the same names as in the code, so that you can easily
    understand the file `map.py`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在你定义输入状态、输出动作和奖励之前，必须先设置所有关于地图和汽车的参数，这些参数将成为你环境的一部分。输入、输出和奖励都是这些参数的函数。让我们列出它们，使用和代码中相同的名称，这样你就能轻松理解文件
    `map.py`：
- en: '**angle**: The angle between the *x*-axis of the map and the axis of the car'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**angle**：地图的 *x*-轴与汽车轴之间的角度'
- en: '**rotation**: The last rotation made by the car (we will see later that when
    playing an action, the car makes a rotation)'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**rotation**：汽车做的最后一次旋转（我们稍后会看到，执行一个动作时，汽车会进行旋转）'
- en: '**pos = (self.car.x, self.car.y)**: The position of the car (`self.car.x` is
    the *x*-coordinate of the car, `self.car.y` is the *y*-coordinate of the car)'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**pos = (self.car.x, self.car.y)**：汽车的位置（`self.car.x` 是汽车的 *x*-坐标，`self.car.y`
    是汽车的 *y*-坐标）'
- en: '**velocity = (velocity_x, velocity_y)**: The velocity vector of the car'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**velocity = (velocity_x, velocity_y)**：汽车的速度向量'
- en: '**sensor1 = (sensor1_x, sensor1_y)**: The position of the first sensor'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**sensor1 = (sensor1_x, sensor1_y)**：第一个传感器的位置'
- en: '**sensor2 = (sensor2_x, sensor2_y)**: The position of the second sensor'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**sensor2 = (sensor2_x, sensor2_y)**：第二个传感器的位置'
- en: '**sensor3 = (sensor3_x, sensor3_y)**: The position of the third sensor'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**sensor3 = (sensor3_x, sensor3_y)**：第三个传感器的位置'
- en: '**signal1**: The signal received by sensor 1'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**signal1**：传感器 1 接收到的信号'
- en: '**signal2**: The signal received by sensor 2'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**signal2**：传感器 2 接收到的信号'
- en: '**signal3**: The signal received by sensor 3'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**signal3**：传感器 3 接收到的信号'
- en: Now let's slow down; we've got to define how these signals are computed. The
    signals are a measure of the density of sand around their sensor. How are you
    going to compute that density? You start by introducing a new variable, called
    `sand`, which you initialize as an array that has as many cells as our graphic
    interface has pixels. Simply put, the `sand` array is the black map itself and
    the pixels are the cells of the array. Then, each cell of the `sand` array will
    get a 1 if there is sand, and a 0 if there is not.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们放慢一下进度；我们得定义这些信号是如何计算的。这些信号是传感器周围沙子密度的度量。你打算如何计算这种密度呢？你首先引入一个新变量，叫做 `sand`，它初始化为一个数组，数组的单元格数量与我们的图形界面的像素数量相同。简单来说，`sand`
    数组就是黑色地图本身，像素则是数组的单元格。然后，如果某个位置有沙子，`sand` 数组中的相应单元格会被赋值为 1，如果没有沙子，则赋值为 0。
- en: 'For example, here the `sand` array has only 1s in its first few rows, and the
    rest is all 0s:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里 `sand` 数组的前几行只有 1，其余部分全是 0：
- en: '![](img/B14110_10_07.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_10_07.png)'
- en: 'Figure 7: The map with only sand in the first rows'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：只有前几行有沙子的地图
- en: I know the border is a little wobbly—like I said, I'm no great artist—and that
    just means those rows of the `sand` array would have 1s where the sand is and
    0s where there's no sand.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道边界有点晃动——就像我说的，我不是伟大的艺术家——这只是意味着 `sand` 数组的那些行会在有沙子的地方显示 1，而没有沙子的地方显示 0。
- en: Now that you have this `sand` array it's very easy to compute the density of
    sand around each sensor. You surround your sensor by a square of 20 by 20 cells
    (which the sensor reads from the `sand` array), then you count the number of ones
    in these cells, and finally you divide that number by the total number of cells
    in that square, that is, 20 x 20 = 400 cells.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了这个 `sand` 数组，计算每个传感器周围的沙子密度变得非常简单。你将传感器周围放置一个 20x20 单元格的正方形（传感器从 `sand`
    数组中读取这些单元格），然后计算这些单元格中 1 的数量，最后将这个数字除以该正方形中的总单元格数，即 20 x 20 = 400 个单元格。
- en: 'Since the `sand` array only contains 1s (where there''s sand) and 0s (where
    there''s no sand), we can very easily count the number of 1s by simply summing
    the cells of the `sand` array in this 20 by 20 square. That gives us exactly the
    density of sand around each sensor, and that''s what''s computed at lines 81,
    82, and 83 in the `map.py` file:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`sand`数组只包含1（表示有沙子）和0（表示没有沙子），我们可以通过简单地对这个20x20平方中的`sand`数组单元格求和，轻松地计算出1的数量。这就给出了每个传感器周围的沙子密度，而这正是在`map.py`文件中的第81、82和83行计算的内容：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we''ve covered how the signals are computed, let''s continue with
    the rest of the parameters. The last parameters, which I''ve highlighted in the
    list below, are important because they''re the last pieces that we need to reveal
    the final input state vector. Here they are:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讲解了信号是如何计算的，让我们继续讨论其余的参数。下面我标出的一些最后的参数很重要，因为它们是我们需要的最后几块拼图，以揭示最终的输入状态向量。它们是：
- en: '**goal_x**: The *x*-coordinate of the goal (which can either be the Airport
    or Downtown)'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**goal_x**：目标的*x*坐标（可以是机场或市中心）'
- en: '**goal_y**: The *y*-coordinate of the goal (which can either be the Airport
    or Downtown)'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**goal_y**：目标的*y*坐标（可以是机场或市中心）'
- en: '**xx = (goal_x - self.car.x)**: The difference of *x*-coordinates between the
    goal and the car'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**xx = (goal_x - self.car.x)**：目标和汽车之间的*x*坐标差'
- en: '**yy = (goal_y - self.car.y)**: The difference of *y*-coordinates between the
    goal and the car'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**yy = (goal_y - self.car.y)**：目标和汽车之间的*y*坐标差'
- en: '**orientation**: The angle that measures the direction of the car with respect
    to the goal'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**orientation**：测量汽车相对于目标方向的角度'
- en: 'Let''s slow down again for a moment. We need to know how orientation is computed;
    it''s the angle between the axis of the car (the `velocity` vector from our first
    list of parameters) and the axis that joins the goal and the center of the car.
    The goal has the coordinates (`goal_x`, `goal_y`) and the center of the car has
    the coordinates (`self.car.x`, `self.car.y`). For example, if the car is heading
    perfectly toward the goal, then orientation = 0°. If you''re curious as to how
    we can compute the angle between the two axes in Python, here''s the code that
    gets the `orientation` (lines 126, 127, and 128 in the `map.py` file):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再慢下来一下。我们需要知道如何计算方向；它是汽车的轴（来自我们参数列表中的`velocity`向量）与连接目标和汽车中心的轴之间的角度。目标的坐标是（`goal_x`，`goal_y`），汽车中心的坐标是（`self.car.x`，`self.car.y`）。例如，如果汽车正朝目标完全前进，那么方向
    = 0°。如果你对如何在Python中计算这两个轴之间的角度感兴趣，这里是获取`orientation`的代码（`map.py`文件中的第126、127和128行）：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Good news—we're finally ready to define the main pillars of the environment.
    I'm talking, of course, about the input states, the actions, and the rewards.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息——我们终于准备好定义环境的主要支柱了。我说的当然是输入状态、动作和奖励。
- en: 'Before I define them, try to guess what they''re going to be. Check out all
    the preceding parameters again, and remember the goal: making round trips between
    two locations, the Airport and Downtown, while avoiding any obstacles along the
    road. The solution''s in the next section.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我定义它们之前，试着猜猜它们是什么。再检查一下所有前面的参数，并记住目标：在两个地点之间来回旅行，即机场和市中心，同时避免道路上的任何障碍。解决方案在下一节。
- en: The input states
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入状态
- en: 'What do you think the input states are? You might have answered "the position
    of the car." In that case, the input state would be a vector of two elements,
    the coordinates of the car: `self.car.x` and `self.car.y`.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为输入状态是什么？你可能回答了“汽车的位置”。在这种情况下，输入状态将是一个包含两个元素的向量，汽车的坐标：`self.car.x`和`self.car.y`。
- en: That's a good start. From the intuition and foundation techniques of deep Q-learning
    you learned in *Chapter 9*, *Going Pro with Artificial Brains – Deep Q-Learning*,
    you know that when you're doing deep Q-learning, the input state doesn't have
    to be a single element as in Q-learning. In fact, in deep Q-learning the input
    state can be a vector of many elements, allowing you to supply many sources of
    information to your AI to help it predict smart actions to play.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不错的开始。从你在*第9章*《成为人工智能专家 - 深度Q学习》中学到的深度Q学习的直觉和基础技术来看，你知道在做深度Q学习时，输入状态不一定像Q学习那样是单一的元素。实际上，在深度Q学习中，输入状态可以是多个元素的向量，从而允许你为AI提供多个信息来源，帮助它预测智能的动作来执行。
- en: 'The input state can even be bigger than a simple vector: it can be an image!
    In that case, the AI model is called **deep convolutional Q-learning**. It''s
    the same as deep Q-learning, except that you add a convolutional neural network
    at the entrance of the neural network that allows your AI (machine) to visualize
    images. We''ll cover this technique in *Chapter 12*, *Deep Convolution Q-Learning*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输入状态甚至可以比一个简单的向量更大：它可以是一个图像！在这种情况下，AI模型被称为**深度卷积Q学习**。它和深度Q学习相同，只不过在神经网络的入口处加入了卷积神经网络，让你的AI（机器）能够处理图像。我们将在*第12章*，*深度卷积Q学习*中介绍这项技术。
- en: We can do better than just supplying the car position coordinates. They tell
    us where the self-driving car is located, but there's another parameter that's
    better, simpler, and more directly related to the goal. I'm talking about the
    `orientation` variable. The orientation is a single input that directly tells
    us if we are pointed in the right direction, toward the goal. If we have that
    orientation, we don't need the car position coordinates at all to navigate toward
    the goal; we can just change the orientation by a certain angle to point the car
    more in the direction of the goal. The actions that the AI performs will be what
    changes that orientation. We'll discuss those in the next section.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做得比仅仅提供汽车位置坐标更好。位置坐标告诉我们自动驾驶汽车的位置，但有一个更好的参数，它更简单，并且与目标更直接相关。我说的是`orientation`变量。方向是一个单一的输入，它直接告诉我们是否朝着正确的方向前进，朝着目标。如果我们有了这个方向，我们就不再需要汽车的位置坐标来导航到目标；我们只需要改变方向一定角度，就可以让汽车更朝着目标的方向行驶。AI执行的操作将是改变这个方向的动作。我们将在下一节讨论这些操作。
- en: 'We have the first element of our input state: the orientation.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有了输入状态的第一个元素：方向。
- en: But that's not enough. Remember that we also have another goal, or, should I
    say, constraint. Our car needs to stay on the road and avoid any obstacles along
    that road.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 但这还不够。记住，我们还有另一个目标，或者说是约束。我们的车需要保持在道路上，并避开道路上的任何障碍物。
- en: In the input state, we need information telling the AI whether it is about to
    move off the road or hit an obstacle. Try and work it out for yourself—do we have
    a way to get this information?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入状态中，我们需要一些信息来告诉AI它是否即将驶出道路或撞到障碍物。试着自己推理一下——我们有办法获取这些信息吗？
- en: 'The solution is the sensors. Remember that our car has three sensors giving
    us signals about how much sand is around them. The blue sensor tells us if there''s
    any sand at the left of the car, the red sensor tells us if there is any sand
    in front of the car, and the yellow sensor tells us if there is any sand at the
    right of the car. The signals of these sensors are already coded into three variables:
    `signal1`, `signal2`, and `signal3`. These signals will tell the AI if it''s about
    to hit some obstacle or about to get out of the road, since the road is delimited
    by sand.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案就是传感器。记住，我们的车有三个传感器，给我们提供关于周围沙子多少的信号。蓝色传感器告诉我们车左边是否有沙子，红色传感器告诉我们车前方是否有沙子，黄色传感器告诉我们车右边是否有沙子。这些传感器的信号已经被编码成三个变量：`signal1`、`signal2`和`signal3`。这些信号将告诉AI它是否即将撞到障碍物或驶出道路，因为道路是由沙子界定的。
- en: That's the rest of the information you need for your input state. With these
    four elements, `signal1`, `signal2`, `signal3`, and `orientation`, you have everything
    you need to be able to drive from one location to another, while staying on the
    road, and without hitting any obstacles.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你所需的输入状态的其余信息。通过这四个元素，`signal1`、`signal2`、`signal3`和`orientation`，你拥有了足够的信息，能够从一个位置驾驶到另一个位置，同时保持在道路上，并避免撞到任何障碍物。
- en: 'In conclusion, here''s what the input state is going to be at each time:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，下面是每次的输入状态：
- en: Input state = (`orientation`, `signal1`, `signal2`, `signal3`)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输入状态 = (`orientation`, `signal1`, `signal2`, `signal3`)
- en: 'And that''s exactly what''s coded at line 129 in the `map.py` file:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 而这正是`map.py`文件中第129行的编码内容：
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`state` is the variable name given to the input state.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`state`是给输入状态指定的变量名。'
- en: Don't worry too much about the code syntax difference between `signal`, `self.signal`,
    and `self.car.signal`; they're all the same. The reason we use these different
    variables is because the AI is coded with classes (as in **Object Oriented Programming**
    (**OOP**)), which allows us to create several self-driving cars on the same map.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 不必太担心`signal`、`self.signal`和`self.car.signal`之间的代码语法差异；它们是一样的。我们使用这些不同的变量是因为AI是用类（如**面向对象编程**（**OOP**））编写的，这样我们就可以在同一张地图上创建多个自动驾驶汽车。
- en: If you do want to have several self-driving cars on your map, for example, if
    you want them racing, then you can distinguish the cars better thanks to `self.car.signal`.
    For example, if you have two cars, you can name the two objects `car1` and `car2`
    so that you can distinguish the first sensor signals of the two cars, by using
    `self.car1.signal1` and `self.car2.signal1`. In this chapter, we just have one
    car, so whether we use `signal1`, `car.signal1` or `self.car.signal1`, we get
    the same thing.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在地图上拥有多辆自动驾驶汽车，例如，如果你想让它们进行比赛，那么你可以通过`self.car.signal`更好地区分这些车。例如，如果你有两辆车，你可以将这两个对象命名为`car1`和`car2`，然后通过使用`self.car1.signal1`和`self.car2.signal1`来区分这两辆车的第一个传感器信号。在本章中，我们只有一辆车，因此无论是使用`signal1`、`car.signal1`还是`self.car.signal1`，结果都是一样的。
- en: We've covered the input state; now let's tackle the actions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过输入状态；现在让我们来处理动作。
- en: The output actions
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出动作
- en: 'I''ve already briefly mentioned or suggested what the actions are going to
    be. Given our input state, it''s easy to guess. Naturally, since you''re building
    a self-driving car, you might think that the actions should be: move forward,
    turn left, or turn right. You''d be absolutely right! That''s exactly what the
    actions are going to be.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经简要提到或暗示了这些动作会是什么。根据我们的输入状态，猜测这些动作是很容易的。自然地，因为你正在构建一辆自动驾驶汽车，你可能会认为动作应该是：前进、左转或右转。你完全正确！这正是这些动作会是的内容。
- en: Not only is this intuitive, but it aligns extremely well with our choice of
    input states. They contain the `orientation` variable that tells us if we're aimed
    in the right direction toward the goal. Simply put, if the `orientation` input
    tells us our car is pointed in the right direction, we perform the action of moving
    forward. If the `orientation` input tells us that the goal is on the right of
    our car, we perform the action of turning right. Finally, if the `orientation`
    tells us that the goal is on the left of our car, we perform the action of turning
    left.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅直观，而且与我们选择的输入状态极其吻合。输入状态包含了`orientation`变量，它告诉我们汽车是否朝着正确的方向驶向目标。简单来说，如果`orientation`输入告诉我们我们的车指向正确的方向，我们就执行前进的动作。如果`orientation`输入告诉我们目标在车的右侧，我们就执行右转的动作。最后，如果`orientation`告诉我们目标在车的左侧，我们就执行左转的动作。
- en: 'At the same time, if any of the signals spot some sand around the car, the
    car will turn left or right to avoid it. The three possible actions of move forward,
    turn left, and turn right make logical sense with the goal, constraint, and input
    states we have, and we can define them as the three following rotations:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，如果任何信号检测到车周围有沙子，汽车会向左或向右转弯以避开它。前进、左转和右转这三种可能的动作与我们所设定的目标、约束和输入状态是逻辑一致的，我们可以将它们定义为以下三个旋转：
- en: '*rotations* = [turn 0° (that is, move forward), turn 20° to the left, turn
    20° to the right]'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*旋转* = [转动0°（即前进）、向左转20°、向右转20°]'
- en: The choice of 20° is quite arbitrary. You could very well choose 10°, 30°, or
    40°. I'd avoid more than 40°, because then your car would have twitchy, fidgety
    movements, and wouldn't look like a smoothly moving car.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选择20°是相当任意的。你完全可以选择10°、30°或40°。我建议避免选择超过40°，因为那样你的车子会有不稳定、抖动的动作，看起来就不像是一辆平稳行驶的车了。
- en: However, the actions the ANN outputs will not be 0°, 20°, and -20°; they will
    be 0, 1 and 2.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，人工神经网络输出的动作不会是0°、20°和-20°，它们将是0、1和2。
- en: '*actions* = [0, 1, 2]'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*动作* = [0, 1, 2]'
- en: It's always better to use simple categories like those when you're dealing with
    the output of an artificial neural network. Since 0, 1, and 2 will be the actions
    the AI returns, how do you think we end up with the rotations?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理人工神经网络的输出时，使用像这样的简单类别总是更好的。由于0、1和2将是AI返回的动作，那么你认为我们是如何得到旋转角度的呢？
- en: 'You''ll use a simple mapping, called `action2rotation` in our code, which maps
    the actions 0, 1, 2 to the respective rotations of 0°, 20°, -20°. This is exactly
    what''s coded on lines 34 and 131 of the `map.py` file:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用一个简单的映射，代码中叫做`action2rotation`，它将动作0、1、2映射到相应的旋转角度0°、20°、-20°。这正是`map.py`文件中第34行和第131行的代码：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, let's move on to the rewards. This one's going to be fun, because this
    is where you decide how you want to reward or punish your car. Try to figure out
    how by yourself first, and then take a look at the solution in the following section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续讨论奖励。这个部分会很有趣，因为在这里你决定如何奖励或惩罚你的汽车。先尝试自己思考一下，然后再看看下一节中的解决方案。
- en: The rewards
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 奖励
- en: 'To define the system of rewards, we have to answer the following questions:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义奖励系统，我们必须回答以下问题：
- en: In which cases do we give the AI a good reward? How good for each case?
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在哪些情况下我们会给AI一个好奖励？每种情况下的奖励有多好？
- en: In which cases do we give the AI a bad reward? How bad for each case?
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在哪些情况下我们会给AI一个坏奖励？每种情况下的奖励有多坏？
- en: 'To answer these questions, we must simply remember what the goal and constraints are:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，我们只需要记住目标和约束是什么：
- en: The goal is to make round trips between the Airport and Downtown.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标是进行机场和市区之间的往返。
- en: The constraints are to stay on the road and avoid obstacles if any. In other
    words, the constraint is to stay away from the sand.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 约束是保持在道路上，并避免任何障碍物。换句话说，约束是远离沙地。
- en: 'Hence, based on this goal and constraints, the answers to our preceding questions are:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于这个目标和约束，我们之前问题的答案是：
- en: We give the AI a good reward when it gets closer to the destination.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当AI向目标靠近时，我们会给予它一个好奖励。
- en: We give the AI a bad reward when it gets further away from the destination.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当AI离目标越来越远时，我们会给予它一个坏奖励。
- en: We give the AI a bad reward if it's about to drive onto some sand.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果AI即将驶入沙地，我们会给予它一个坏奖励。
- en: That's it! That should work, because these good and bad rewards have a direct
    effect on the goal and constraints.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！这样应该能奏效，因为这些好坏奖励直接影响目标和约束。
- en: To answer the second part of each question, how good and how bad the reward
    should be for each case, we'll play the tough card; it's often more effective.
    The tough card consists of punishing the car more when it makes mistakes than
    we reward it when it does well. In other words, the bad reward is going to be
    stronger than the good reward.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答每个问题的第二部分，即对于每种情况，奖励应该有多好或多坏，我们将采取强硬手段；这通常更有效。强硬手段包括在汽车犯错时给予比表现好时更多的惩罚。换句话说，坏奖励将比好奖励更强烈。
- en: This works well in reinforcement learning, but that doesn't mean you should
    do the same with your dog or your kids. When you're dealing with a biological
    system, the other way around (high good reward and small bad reward) is a much
    more effective way to train or educate. Just food for thought.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在强化学习中效果很好，但这并不意味着你应该用同样的方法训练你的狗或孩子。当你面对一个生物系统时，反过来（高好奖励和小坏奖励）是一种更有效的训练或教育方式。仅供参考。
- en: 'On that note, here are the rewards we''ll give in each case:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这一点，以下是每种情况下我们会给予的奖励：
- en: The AI gets a bad reward of -1 if it drives onto some sand. Nasty!
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果AI驶入沙地，它会得到-1的坏奖励。真讨厌！
- en: The AI gets a bad reward of -0.2 if it moves away from the destination.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果AI远离目标，它会得到-0.2的坏奖励。
- en: The AI gets a good reward of 0.1 if it moves closer to the destination.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果AI向目标靠近，它会得到一个0.1的好奖励。
- en: The reason we attribute the worst reward (-1) to the case when the car drives
    onto some sand makes sense. Driving onto sand is what we absolutely want to avoid.
    The sand on the map represents obstacles in real life; in real life, you would
    train your self-driving car not to hit any obstacle, so as to avoid any accident.
    To do so, we penalize the AI with a highly bad reward when it does hit an obstacle
    during its training.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将最差奖励（-1）赋给汽车驶入沙地的情况，这是合理的。驶入沙地是我们绝对希望避免的。地图上的沙地代表现实生活中的障碍物；在现实生活中，你会训练自动驾驶汽车避开任何障碍物，以避免事故发生。为了实现这一点，当AI在训练过程中撞到障碍物时，我们会给予它极其严重的坏奖励。
- en: 'How''s that translated that into code? That''s easy; you just take your `sand`
    array and check if the car has just moved onto a cell that contains a 1\. If it
    does, that means the car has moved onto some sand and must therefore get a bad
    reward of -1\. That''s exactly what''s coded here at lines 138, 139, and 140 of
    the `map.py` file (including an update of the car velocity vector, which not only
    updates the speed by slowing the car down to 1, but also updates the direction
    of the car by a certain angle, `self.car.angle`):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何将其转化为代码呢？很简单；你只需要检查`sand`数组，看汽车是否刚刚驶入包含1的格子。如果是，那就意味着汽车驶入了沙地，因此必须获得-1的坏奖励。这正是`map.py`文件中第138、139和140行的代码（包括更新汽车速度向量，不仅通过将汽车速度减慢到1来更新速度，还通过一定角度更新汽车方向`self.car.angle`）。
- en: '[PRE4]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then for the other reward attributions, you just have to complete the `if` condition
    preceding with an `else`, which will say what happens in the case where the car
    has not driven onto some sand.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他的奖励分配，你只需要在前面的`if`条件后面加一个`else`，这将说明在汽车没有驶入沙地的情况下会发生什么。
- en: 'In that case, you start a new `if` and `else` condition, saying that if the
    car has moved away from the destination, you give it a bad reward of `-0.2`, and,
    if the car has moved closer to the destination, you give it a good reward of `0.1`.
    The way you measure if the car is getting away from or closer to the goal is by
    comparing two distances put into two separate variables: `last_distance`, which
    is the previous distance between the car and the destination at time *t*-1, and
    `distance`, which is the current distance between the car and the destination
    at time *t*. If you put all that together, you get the following code, which completes
    the preceding lines of code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你开始一个新的 `if` 和 `else` 条件，表示如果汽车远离目标，你给予一个坏的奖励 `-0.2`，如果汽车更接近目标，则给予一个好的奖励
    `0.1`。衡量汽车是否远离或接近目标的方法是通过比较两个距离，这两个距离分别存储在两个变量中：`last_distance`，表示在时刻 *t*-1 时汽车与目标之间的距离，以及
    `distance`，表示在时刻 *t* 时汽车与目标之间的当前距离。如果将这些组合在一起，你会得到以下代码，完成前面的代码行：
- en: '[PRE5]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To keep the car from trying to veer off the map, lines 147 to 158 of the `map.py`
    file punish the AI with a bad reward of `-1` if the self-driving car gets within
    `10` pixels of any of the map's 4 borders of the map. Finally, lines 160 to 162
    of the `map.py` file update the goal, switching it from the Airport to Downtown,
    or vice versa, anytime the car gets within 100 pixels of the current goal.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止汽车尝试驶出地图，`map.py` 文件的第147行到158行会惩罚AI，如果自动驾驶汽车距离地图的任意4条边界 `10` 像素以内，它会被赋予一个坏奖励
    `-1`。最后，`map.py` 文件的第160行到162行会在汽车距离当前目标100像素以内时，更新目标，将其从机场切换到市区，反之亦然。
- en: AI solution refresher
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI解决方案回顾
- en: Let's refresh our memory by reminding ourselves of the steps of the deep Q-learning
    process, while adapting them to our self-driving car application.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回顾深度Q学习过程的步骤，来刷新一下记忆，同时将其适应到我们的自动驾驶汽车应用中。
- en: 'Initialization:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化：
- en: The memory of the experience replay is initialized to an empty list, called
    **memory** in the code.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经验重放的记忆被初始化为空列表，在代码中称为 **memory**。
- en: The maximum size of the memory is set, called **capacity** in the code.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记忆的最大容量被设置，在代码中称为 **capacity**。
- en: 'At each time *t*, the AI repeats the following process, until the end of the
    epoch:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时刻 *t*，AI重复以下过程，直到本轮结束：
- en: The AI predicts the Q-values of the current state *S[t]*. Therefore, since three
    actions can be played (0 <-> 0°, 1 <-> 20°, or 2 <-> -20°), it gets three predicted
    Q-values.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI预测当前状态 *S[t]* 的Q值。因此，由于可以执行三种动作（0 <-> 0°，1 <-> 20°，或2 <-> -20°），它得到了三个预测的Q值。
- en: The AI performs an action selected by the Softmax method (see *Chapter 5*, *Your
    First AI Model – Beware the Bandits!*):![](img/B14110_10_002.png)
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI执行一个通过Softmax方法选定的动作（参见 *第5章*，*你的第一个AI模型 – 当心土匪!*）：![](img/B14110_10_002.png)
- en: The AI receives a reward ![](img/B14110_10_003.png), which is one of -1, -0.2
    or +0.1.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI收到一个奖励 ![](img/B14110_10_003.png)，奖励值可能是-1、-0.2 或 +0.1。
- en: The AI reaches the next state ![](img/B14110_10_004.png), which is composed
    of the next three signals from the three sensors, plus the orientation of the
    car.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI到达下一个状态 ![](img/B14110_10_004.png)，该状态由三个传感器的下一个信号以及汽车的方向组成。
- en: The AI appends the transition ![](img/B14110_10_005.png) to the memory.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI将转换 ![](img/B14110_10_005.png) 添加到记忆中。
- en: 'The AI takes a random batch ![](img/B14110_10_006.png) of transitions. For
    all the transitions ![](img/B14110_10_007.png) of the random batch ![](img/B14110_10_008.png):'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI获取一个随机批次 ![](img/B14110_10_006.png) 的转换。对于随机批次中的所有转换 ![](img/B14110_10_007.png)
    ![](img/B14110_10_008.png)：
- en: 'The AI gets the predictions: ![](img/B14110_10_009.png)'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI获取预测值：![](img/B14110_10_009.png)
- en: 'The AI gets the targets: ![](img/B14110_10_010.png)'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI获取目标值：![](img/B14110_10_010.png)
- en: The AI computes the loss between the predictions and the targets over the whole
    batch ![](img/B14110_10_011.png):![](img/B14110_10_012.png)
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI计算整个批次中预测值与目标值之间的损失 ![](img/B14110_10_011.png):![](img/B14110_10_012.png)
- en: Finally, the AI backpropagates this loss error into the neural network, and
    through stochastic gradient descent updates the weights according to how much
    they contributed to the loss error.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，AI将这个损失误差反向传播到神经网络中，并通过随机梯度下降法根据每个权重对损失误差的贡献来更新权重。
- en: Implementation
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: Now it's time for the implementation! The first thing you need is a professional
    toolkit, because you're not going to build an artificial brain with simple Python
    libraries. What you need is an advanced framework, which allows fast computation
    for the training of neural networks.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候开始实现了！你首先需要一套专业的工具包，因为你不可能仅仅用简单的Python库来构建一个人工大脑。你需要的是一个高级框架，它能够快速计算神经网络的训练过程。
- en: Today, the best frameworks to build and train AIs are **TensorFlow** (by Google)
    and **PyTorch** (by Facebook). How should you choose between the two? They're
    both great to work with and equally powerful. They both have dynamic graphs, which
    allow the fast computation of the gradients of complex functions needed to train
    the model during backpropagation with mini-batch gradient descent. Really, it
    doesn't matter which framework you choose; both work very well for our self-driving
    car. As far as I'm concerned, I have slightly more experience with PyTorch, so
    I'm going to opt for PyTorch and that's how the example in this chapter will continue
    to play out.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，构建和训练AI的最佳框架是**TensorFlow**（由Google开发）和**PyTorch**（由Facebook开发）。你应该如何在这两者之间做出选择？它们都非常适合使用，且功能强大。它们都有动态计算图，可以快速计算训练模型时反向传播和小批量梯度下降所需的复杂函数的梯度。实际上，选择哪一个框架并不重要；两者都非常适合我们的自动驾驶汽车。就我个人而言，我在PyTorch方面稍有更多经验，因此我将选择PyTorch，本章中的示例也将继续使用PyTorch进行演示。
- en: 'To take a step back, our self-driving car implementation is composed of three
    Python files:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 回过头来看，我们的自动驾驶汽车实现由三个Python文件组成：
- en: '`car.kv`, which contains the Kivy objects (rectangle shape of the car and the
    three sensors)'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`car.kv`，包含Kivy对象（汽车的矩形形状和三个传感器）'
- en: '`map.py`, which builds the environment (map, car, input states, output actions,
    rewards)'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`map.py`，用于构建环境（地图、汽车、输入状态、输出动作、奖励）'
- en: '`deep_q_learning.py`, which builds and trains the AI through deep Q-learning'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`deep_q_learning.py`，用于通过深度Q学习来构建和训练AI'
- en: We've already covered the major elements of `map.py`, and now we're about to
    tackle `deep_q_learning.py`, where you'll not only build an artificial neural
    network, but also implement the deep Q-learning training process. Let's get started!
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了`map.py`的主要元素，现在我们将开始处理`deep_q_learning.py`，在这里你不仅要构建一个人工神经网络，还要实现深度Q学习训练过程。让我们开始吧！
- en: Step 1 – Importing the libraries
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一步 – 导入库
- en: 'As usual, you start by importing the libraries and modules you need to build
    your AI. These include:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，你需要通过导入必要的库和模块来开始构建AI。这些包括：
- en: '`os`: The operating system library, used to load the saved AI models.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`os`：操作系统库，用于加载保存的AI模型。'
- en: '`random`: Used to sample some random transitions from the memory for experience
    replay.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`random`：用于从记忆中抽取一些随机转移进行经验回放。'
- en: '`torch`: The main library from PyTorch, which will be used to build our neural
    network with tensors, as opposed to simple matrices like `numpy` arrays. While
    a matrix is a 2-D array, a tensor can be a *n*-dimensional array, with more than
    just a single number in its cells. Here''s a diagram so you can clearly understand
    the difference between a matrix and a tensor:![](img/B14110_10_013.png)'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch`：PyTorch的主要库，将用于通过张量构建我们的神经网络，而不是像`numpy`数组那样使用简单的矩阵。矩阵是二维数组，而张量可以是*n*维数组，其单元格中不仅包含一个数字。以下是一个图示，帮助你清楚地理解矩阵和张量之间的区别：![](img/B14110_10_013.png)'
- en: '`torch.nn`: The `nn` module from the torch library, used to build the fully
    connected layers in the artificial neural network of our AI.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch.nn`：PyTorch库中的`nn`模块，用于构建我们AI的人工神经网络中的全连接层。'
- en: '`torch.nn.functional`: The `functional` sub-module from the `nn` module, used
    to call the activation functions (rectifier and Softmax), as well as the loss
    function for backpropagation.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch.nn.functional`：`nn`模块中的`functional`子模块，用于调用激活函数（修正线性单元和Softmax），以及用于反向传播的损失函数。'
- en: '`torch.optim`: The `optim` module from the torch library, used to call the
    Adam optimizer, which computes the gradients of the loss with respect to the weights
    and updates those weights in directions that reduce the loss.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch.optim`：PyTorch库中的`optim`模块，用于调用Adam优化器，它计算损失函数相对于权重的梯度，并在减小损失的方向上更新这些权重。'
- en: '`torch.autograd`: The `autograd` module from the torch library, used to call
    the `Variable` class, which associates each tensor and its gradient into the same
    variable.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch.autograd`：PyTorch库中的`autograd`模块，用于调用`Variable`类，该类将每个张量及其梯度关联到同一个变量中。'
- en: 'That makes up your first code section:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这构成了你的第一段代码：
- en: '[PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Step 2 – Creating the architecture of the neural network
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第2步 – 创建神经网络的架构
- en: This code section is where you really become the architect of the brain in your
    AI. You're about to build the input layer, the fully connected layers, and the
    output layer, while choosing some activation functions that will forward-propagate
    the signal inside the brain.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这一段代码是你真正开始构建AI大脑的地方。你将要构建输入层、全连接层和输出层，并选择一些激活函数，用于在大脑内部进行前向传播信号。
- en: First, you build this brain inside a class, which we are going to call `Network`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将把这个大脑构建在一个类里面，我们将这个类称为`Network`。
- en: What is a class? Let's explain that before we explain why you're using one.
    A class is an advanced structure in Python that contains the instructions of an
    object we want to build. Taking the example of your neural network (the object),
    these instructions include how many layers you want, how many neurons you want
    inside each layer, which activation function you choose, and so on. These parameters
    define your artificial brain and are all gathered in what we call the `__init__()`
    method, which is what we always start with when building a class. But that's not
    all—a class can also contain tools, called methods, which are functions that either
    perform some operations or return something. Your `Network` class will contain
    one method, which forward-propagates the signal inside the neural network and
    returns the predicted Q-values. Call this method `forward`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是类？在我们解释为什么使用类之前，先来解释一下什么是类。类是Python中的一种高级结构，它包含了我们想要构建的对象的指令。以神经网络（即对象）为例，这些指令包括你想要多少层、每一层里面有多少个神经元、你选择了哪种激活函数，等等。这些参数定义了你的人工大脑，并且都聚集在我们称之为`__init__()`的方法中，这也是我们在构建类时总是从这里开始的。但这还不是全部——类还可以包含工具，称为方法，这些方法是执行某些操作或返回某些东西的函数。你的`Network`类将包含一个方法，用于在神经网络中进行前向传播并返回预测的Q值。我们将这个方法命名为`forward`。
- en: 'Now, why use a class? That''s because building a class allows you to create
    as many objects (also called instances) as you want, and easily switch from one
    to another by just changing the arguments of the class. For example, your `Network`
    class contains two arguments: `input_size` (the number of inputs) and `nb_actions`
    (the number of actions). If you ever want to build an AI with more inputs (besides
    the signals and the orientation) or more outputs (you could add an action that
    brakes the car), you''ll do it in a flash thanks to the advanced structure of
    the class. It''s super practical, and if you''re not already familiar with classes
    you''ll have to get familiar with them. Nearly all AI implementations are done
    with classes.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么使用类呢？这是因为构建类可以让你创建任意多个对象（也叫实例），并且只需改变类的参数就能轻松切换。举个例子，你的`Network`类包含两个参数：`input_size`（输入的数量）和`nb_actions`（动作的数量）。如果你将来想要构建一个拥有更多输入（除了信号和方向）或更多输出（例如添加一个能够刹车的动作）的AI，得益于类的高级结构，你可以快速实现。这非常实用，如果你还不熟悉类的概念，你将需要尽快掌握它们。几乎所有的AI实现都使用类。
- en: 'That was just a short technical aside to make sure I don''t lose anybody on
    the way. Now let''s build this class. As there are many important elements to
    explain in the code, and since you''re probably new to PyTorch, I''ll show you
    the code first and then explain it line by line from the `deep_q_learning.py`
    file:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个简短的技术说明，确保我在讲解过程中没有让任何人迷失。现在，让我们开始构建这个类。由于代码中有许多重要的元素需要解释，并且你可能对PyTorch不太熟悉，所以我会先展示代码，然后逐行解释来自`deep_q_learning.py`文件的内容：
- en: '[PRE7]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Line 15**: You introduce the `Network` class. In the parenthesis of this
    class, you can see `nn.Module`. That means you''re calling the `Module` class,
    which is an existing class taken from the `nn` module, in order to get all the
    properties and tools of the `Module` class and use them inside your `Network`
    class. This trick of calling another existing class inside a new class is called
    **inheritance**.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**第15行**：你引入了`Network`类。在这个类的括号内，你可以看到`nn.Module`。这意味着你调用了`Module`类，它是从`nn`模块中提取的一个现有类，用来获取`Module`类的所有属性和工具，并在你的`Network`类中使用它们。这个在新类中调用另一个现有类的技巧叫做**继承**。'
- en: '**Line 17**: You start with the `__init__()` method, which defines all the
    parameters (number of inputs, number of outputs, and so on) of your artificial
    neural network. You can see three arguments: `self`, `input_size`, and `nb_action.self`
    refer to the object, that is, to the future instance of the class that will be
    created after the class is done. Any time you see `self` before a variable, and
    separated by a dot (like `self.variable`), that means the variable belongs to
    the object. That should clear up any mystery about `self`!'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**第17行**：你从`__init__()`方法开始，它定义了人工神经网络的所有参数（输入数量、输出数量等）。你可以看到三个参数：`self`、`input_size`和`nb_action`。`self`指代对象，即类创建后将要生成的未来实例。每当你看到`self`出现在变量前面，并且通过点（`.`）与变量分隔时，意味着该变量属于该对象。这应该能解开关于`self`的一切谜团！'
- en: Then, `input_size` is the number of inputs in your input state vector (thus
    4), and `nb_action` is the number of output actions (thus 3). What's important
    to understand is that the arguments (other than self) of the `__init__()` method
    are the ones you will enter when creating the future object, which is the future
    artificial brain of your AI.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`input_size`是输入状态向量中的输入数量（因此是4），`nb_action`是输出动作的数量（因此是3）。重要的是要理解，`__init__()`方法中的参数（除了`self`）是你在创建未来对象时会输入的参数，也就是未来你的AI人工大脑。
- en: '**Line 18**: You use the `super()` function to activate the inheritance (explained
    in Line 15), inside the `__init__()` method.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**第18行**：你使用`super()`函数来激活继承（如第15行所述），该函数位于`__init__()`方法内。'
- en: '**Line 19**: Here you introduce the first object variable, `self.input_size`,
    set equal to the argument `input_size` (which will later be entered as `4`, since
    the input state has 4 elements).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**第19行**：这里你引入了第一个对象变量`self.input_size`，并将其设置为与参数`input_size`相等（稍后将输入为`4`，因为输入状态有4个元素）。'
- en: '**Line 20**: You introduce the second object variable, `self.nb_action`, set
    equal to the argument `nb_action` (which will later be entered as `3`, since there
    are three actions that can be performed).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**第20行**：你引入了第二个对象变量`self.nb_action`，并将其设置为与参数`nb_action`相等（稍后将输入为`3`，因为可以执行三个动作）。'
- en: '**Line 21**: You introduce the third object variable, `self.fc1`, which is
    the first full connection between the input layer (composed of the input state)
    and the hidden layer. That first full connection is created as an object of the
    `nn.Linear` class, which takes two arguments: the first one is the number of elements
    in the left layer (the input layer), so `input_size` is the right argument to
    use, and the second one is the number of hidden neurons in the right layer (the
    hidden layer). Here, you choose to have 30 neurons, and therefore the second argument
    is `30`. The choice of 30 is purely arbitrary, and the self-driving car could
    work well with any other numbers.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**第21行**：你引入了第三个对象变量`self.fc1`，它是输入层（由输入状态组成）与隐藏层之间的第一个全连接。这个第一个全连接作为`nn.Linear`类的对象创建，它接受两个参数：第一个是左侧层（输入层）中的元素数量，因此应该使用`input_size`作为参数，第二个是右侧层（隐藏层）中的隐藏神经元数量。在这里，你选择了30个神经元，因此第二个参数是`30`。选择30只是一个任意的决定，自动驾驶汽车也可以在其他数量下正常工作。'
- en: '**Line 22**: You introduce the fourth object variable, `self.fc2`, which is
    the second full connection between the hidden layer (composed of 30 hidden neurons)
    and the output layer. It could have been a full connection with a new hidden layer,
    but your problem is not complex enough to need more than one hidden layer, so
    you''ll just have one hidden layer in your artificial brain. Just like before,
    that second full connection is created as an object of the `nn.Linear` class,
    which takes two arguments: the first one is the number of elements in the left
    layer (the hidden layer), therefore `30`, and the second one is the number of
    hidden neurons in the right layer (the output layer), therefore `3`.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**第22行**：你引入了第四个对象变量`self.fc2`，它是隐藏层（由30个隐藏神经元组成）与输出层之间的第二个全连接。它本来也可以是与新隐藏层的全连接，但你的问题不复杂到需要多个隐藏层，因此你在人工大脑中只会有一个隐藏层。和之前一样，这个第二个全连接作为`nn.Linear`类的对象创建，它接受两个参数：第一个是左侧层（隐藏层）中的元素数量，因此是`30`，第二个是右侧层（输出层）中的隐藏神经元数量，因此是`3`。'
- en: '**Line 24**: You start building the first and only method of the class, the
    `forward` method, which will propagate the signal from the input layer to the
    output layer, after which it will return the predicted Q-values. This `forward`
    method takes two arguments: `self`, because you''ll use the object variables inside
    the `forward` method, and `state`, the input state vector composed of four elements
    (orientation plus the three signals).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 25**: You forward propagate the signal from the input layer to the hidden
    layer while activating the signal with a rectifier activation function, also called
    **ReLU** (**Rectified Linear Unit**). You do this in two steps. First, the forward
    propagation from the input layer to the hidden layer is done by calling the first
    full connection `self.fc1` with the input state vector `state` as input: `self.fc1(state)`.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'That returns the hidden layer. And then we call the `relu` function with that
    hidden layer as input to break the linearity of the signal the following way:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_08.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The Rectifier activation function'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the ReLU layer is to break linearity by creating non-linear operations
    along the fully connected layers. You'll want to have that, because you're trying
    to solve a nonlinear problem. Finally, `F.relu(self.fc1(state))` returns `x`,
    the hidden layer with a nonlinear signal.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 26**: You forward-propagate the signal from the hidden layer to the
    output layer containing the Q-values. In the same way as the previous line, this
    is done by calling the second full connection `self.fc2` with the hidden layer
    `x` as input: `self.fc2(x)`. That returns the Q-values, which you name `q_values`.
    Here, no activation function is needed because you''ll select the action to play
    with Softmax, later, in another class.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 27**: Finally, here, the `forward` method returns the Q-values.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at what you've just created!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_09.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The neural network (the brain) of our AI'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '`self.fc1` are all the blue connection lines between the **Input Layer** and
    the **Hidden Layer**.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '`self.fc2` are all the blue connection lines between the **Hidden Layer** and
    the **Output Layer**.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: That should help you visualize the full connections better. Great job!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Implementing experience replay
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time for the next step! You'll now build another class, which builds the memory
    object for experience replay (as seen in *Chapter 5*, *Your First AI Model – Beware
    the Bandits!*). Call this class `ReplayMemory`. Let's have a look at the code
    first and then I'll explain everything line by line from the `deep_q_learning.py`
    file.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Line 31**: You introduce the `ReplayMemory` class. This time you don''t need
    to inherit from any other class, so just input `object` in the parenthesis of
    the class.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 33**: As always, you start with the `__init__()` method, which only
    takes two arguments: `self`, the object, and `capacity`, the maximum size of the
    memory.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 34**: You introduce the first object variable, `self.capacity`, set
    equal to the argument `capacity`, which will be entered later when creating an
    object of the class.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 35**: You introduce the second object variable, `self.memory`, initialized
    as an empty list.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 37**: You start building the first tool of the class, the `push` method,
    which takes a transition as input and adds it to the memory. However, if adding
    that transition exceeds the memory''s capacity, the `push` method also deletes
    the first element of the memory. The `event` argument you can see is the transition
    to be added.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 38**: Using the `append` function, you add the transition to the memory.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 39**: You start an `if` condition that checks if the length of the memory
    (meaning its number of transitions) is larger than the capacity.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 40**: If that is indeed the case, you delete the first element of the
    memory.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 42**: You start building the second tool of the class, the `sample`
    method, which samples some random transitions from the experience replay memory.
    It takes `batch_size` as input, which is the size of the batches of transitions
    with which you''ll train your neural network.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember how it works: instead of forward-propagating single input states into
    the neural network and updating the weights after each transition resulting from
    the input state, you forward-propagate small batches of input states and update
    the weights after backpropagating the same whole batches of transitions with mini-batch
    gradient descent. That''s different from stochastic gradient descent (weight update
    every single input) and batch gradient descent (weight update every batch of inputs)
    as explained in *Chapter 9*, *Going Pro with Artificial Brains – Deep Q-Learning*:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_10.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Batch gradient descent versus stochastic gradient descent'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 43**: You sample some random transitions from the memory and put them
    into a batch of size `batch_size`. For example, if `batch_size = 100`, you sample
    100 random transitions from the memory. The sampling is done with the `sample()`
    function from the random library. Then, `zip(*list)` is used to regroup the states,
    actions, and rewards into separate batches of the same size (`batch_size`), in
    order to put the sampled transitions into the format expected by PyTorch (the
    `Variable` format, which comes next in Line 44).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'This is probably a good time to take a step back. Let''s see what Line 43 gives
    you:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_11.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The batches of last states, actions, rewards, and next states'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 44**: Using the `map()` function, wrap each sample into a `torch Variable`
    object (as `Variable()` is actually a class), so that each tensor inside the samples
    is associated to a gradient. Simply put, you can see a `torch Variable` as an
    advanced structure that encompasses a tensor and a gradient.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: This is the beauty of PyTorch. These `torch Variables` are all in a dynamic
    graph which allows fast computation of the gradient of complex functions. Those
    fast computations are required for the weight updates happening during backpropagation
    with mini-batch gradient descent. Inside the `Variable` class we see `torch.cat(x,0)`.
    That's just a concatenation trick, along the vertical axis, to put the samples
    in the format expected by the `Variable` class.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important thing to remember is this: when training a neural network
    with PyTorch, we always work with `torch Variables`, as opposed to just tensors.
    You can find more details about this in the PyTorch documentation.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – Implementing deep Q-learning
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You've made it! You're finally about to start coding the whole deep Q-learning
    process. Again, you'll wrap all of it into a class, this time called `Dqn`, as
    in deep Q-network. This is your final run before the finish line. Let's smash
    this.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, the class is quite long so I''ll show and explain the lines of code
    method by method from the `deep_q_learning.py` file. Here''s the first one, the
    `__init__()` method:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Line 48**: You introduce the `Dqn` class. You don''t need to inherit from
    any other class so just input `object` in the parenthesis of the class.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 50**: As always, you start with the `__init__()` method, which this
    time takes four arguments:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '`self`: The object'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`input_size`: The number of inputs in the input state vector (that is, 4)'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`nb_action`: The number of actions (that is, 3)'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`gamma`: The discount factor in the temporal difference formula'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Line 51**: You introduce the first object variable, `self.gamma`, set equal
    to the argument `gamma` (which will be entered later when you create an object
    of the `Dqn` class).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 52**: You introduce the second object variable, `self.model`, an object
    of the `Network` class you built before. This object is your neural network; in
    other words, the brain of our AI. When creating this object, you input the two
    arguments of the `__init__()` method in the `Network` class, which are `input_size`
    and `nb_action`. You''ll enter their real values (respectively `4` and `3`) later,
    when creating an object of the `Dqn` class.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 53**: You introduce the third object variable, `self.memory`, as an
    object of the `ReplayMemory` class you built before. This object is the experience
    replay memory. Since the `__init__` method of the `ReplayMemory` class only expects
    one argument, the `capacity`, that''s exactly what you input here as `100,000`.
    In other words, you''re creating a memory of size 100,000, which means that instead
    of remembering just the last transition, the AI will remember the last 100,000
    transitions.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 54**: You introduce the fourth object variable, `self.optimizer`, as
    an object of the `Adam` class, which is an existing class built in the `torch.optim`
    module. This object is the optimizer, which updates the weights through mini-batch
    gradient descent during backpropagation. In the arguments, keep most of the default
    parameter values (you can check them in the PyTorch documentation) and only enter the
    model parameters (the `params` argument), which you access with `self.model.parameters`,
    one of the attributes of the `nn.Module` class from which the `Network` class
    inherits.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 55**: You introduce the fifth object variable, `self.last_state`, which
    will be the last state in each (last state, action, reward, next state) transition.
    This last state is initialized as an object of the `Tensor` class from the torch
    library, into which you only have to enter the `input_size` argument. Then `.unsqueeze(0)`
    is used to create an additional dimension at index 0, which will correspond to
    the batch. This allows us to do something like this, matching each last state
    to the appropriate batch:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_12.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Adding a dimension for the batch'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 56**: You introduce the sixth object variable, `self.last_action`, initialized
    as `0`, which is the last action played at each iteration.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 57**: We introduce the last object variable, `self.last_reward`, initialized
    as `0`, which is the last reward received after playing the last action `self.last_action`,
    in the last state `self.last_state`.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you''re all good for the `__init__` method. Let''s move on to the next
    code section with the next method: the `select_action` method, which selects the
    action to play at each iteration using Softmax.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Line 59**: You start defining the `select_action` method, which takes as
    input an input state vector (orientation, signal 1, signal 2, signal 3), and returns
    as output the selected action to play.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 60**: You get the probabilities of the three actions thanks to the Softmax
    function taken from the `torch.nn.functional` module. This Softmax function takes
    the Q-values as input, which are exactly returned by `self.model(Variable(state))`.
    Remember, `self.model` is an object of the `Network` class, which has the `forward`
    method, which takes as input an input state tensor wrapped into a `torch` `Variable`,
    and returns as output the Q-values for the three actions.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '**Geek note**: Usually we would specify that we call the `forward` method this
    way – `self.model.forward(Variable(state))` – but since `forward` is the only
    method of the `Network` class, it is sufficient to just call `self.model`.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplying the Q-values by a number (here `100`) inside `softmax` is a good
    trick to remember: it allows you to regulate the Exploration versus Exploitation.
    The lower that number is, the more you''ll explore, and therefore the longer it
    will take to get optimized actions. Here, the problem''s not too complex, so choose
    a large number (`100`) in order to have confident actions and a smooth trajectory
    to the goal. You''ll clearly see the difference if you remove `*100` from the
    code. Simply put, with the `*100`, you''ll see a car sure of itself; without the
    `*100`, you''ll see a car fidgeting.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 61**: You take a random draw from the distribution of actions created
    by the `softmax` function at line 60, by calling the `multinomial()` function
    from your probabilities `probs`.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 62**: You return the selected action to perform, which you access in
    `action.data[0,0]`. The returned `action` has an advanced tensor structure, and
    the action index (0, 1, or 2) that you''re interested in is located in the `data`
    attribute of the action tensor at the first cell of indexes [0,0].'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s move on to the next code section, the `learn` method. This one is pretty
    interesting because it''s where the heart of deep Q-learning beats. It''s in this
    method that we compute the temporal difference, and accordingly the loss, and
    update the weights with our optimizer in order to reduce that loss. That''s why
    this method is called `learn`, because it is here that the AI learns to perform
    better and better actions that increase the accumulated reward. Let''s continue:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Line 64**: You start by defining the `learn()` method, which takes as inputs
    the batches of the four elements composing a transition (input state, action,
    reward, next state):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_states`: A batch of input states.'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`batch_actions`: A batch of actions played.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`batch_rewards`: A batch of the rewards received.'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`batch_next_states`: A batch of the next states reached.'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before I explain Lines 65, 66, and 67, let''s take a step back and see what
    you''ll have to do. As you know, the goal of this `learn` method is to update
    the weights in directions that reduce the back-propagated loss at each iteration
    of the training. First let''s remind ourselves of the formula for the loss:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_014.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: 'Inside the formula for the loss, we clearly recognize the outputs (predicted
    Q-values) and the targets:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_015.png)![](img/B14110_10_016.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, to compute the loss, you proceed this way over the next four lines
    of code:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 65**: You collect the batch of outputs, ![](img/B14110_10_017.png).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 66**: You compute the ![](img/B14110_10_018.png) part of the targets,
    which you call `batch_next_outputs`.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 67**: You get the batch of targets.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 68**: Since you have the outputs and targets, you''re ready to get the
    loss.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Now let's do this in detail.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 65**: You collect the batch of outputs ![](img/B14110_10_019.png), meaning
    the predicted Q-values of the input states and the actions played in the batch.
    Getting them takes several steps. First, you call `self.model(batch_states)`,
    which, as seen in Line 60, returns the Q-values of each input state in `batch_states`
    and for all the three actions 0, 1, and 2\. To help you visualize it better, it
    returns something like this:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_13.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: What is returned by self.model(batch_states)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'You only want the predicted Q-values for the selected actions from the batch
    of outputs, which are found in the batch of actions `batch_actions`. That''s exactly
    what the `.gather(1, batch_actions.unsqueeze(1)).squeeze(1)` trick does: for each
    input state of the batch, it picks the Q-value that corresponds to the action
    that was selected in the batch of actions. To help visualize this better, let''s
    suppose the batch of actions is the following:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_14.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Batch of actions'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Then you would get the following batch of outputs composed of the red Q-values:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_15.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Batch of outputs'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: I hope this is clear; I'm doing my best not to lose you along the way.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 66**: Now you get the ![](img/B14110_10_020.png) part of the target.
    Call this `batch_next_outputs`; you get it in two steps. First, call `self.model(batch_next_states)`
    to get the predicted Q-values for each next state of the batch of next states
    and for each of the three actions. Then, for each next state of the batch, take
    the maximum of the three Q-values using `.detach().max(1)[0]`. That gives you
    the batch of the ![](img/B14110_10_021.png) values part of the targets.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 67**: Since you have the batch of rewards ![](img/B14110_10_022.png)
    (it''s part of the arguments), and since you just got the batch of the ![](img/B14110_10_023.png)
    values part of the targets at Line 66, then you''re ready to get the batch of
    targets:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_024.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: That's exactly what you do at Line 67, by summing `batch_rewards` and `batch_next_outputs`
    multiplied by `self.gamma`, one of the object variables in the `Dqn` class. Now
    you have both the batch of outputs and the batch of targets, so you're ready to
    get the loss.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 68**: Let''s remind ourselves of the formula for the loss:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_025.png)![](img/B14110_10_026.png)![](img/B14110_10_027.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: Therefore, in order to get the loss, you just have to get the sum of the squared
    differences between our targets and outputs in the batch. That's exactly what
    the `smooth_l1_loss` function will do. Taken from the `torch.nn.functional` module,
    it takes as inputs the two batches of outputs and targets and returns the loss
    as given in the preceding formula. In the code, call this loss `td_loss` as in
    **temporal difference loss**.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Excellent progress! Now that you have the loss, representing the error between
    the predictions and the targets, you're ready to backpropagate this loss into
    the neural network and update our weights to reduce this loss through mini-batch
    gradient descent. That's why the next step to take here is to use your optimizer,
    which is the tool that will perform the updates to the weights.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 69**: You first initialize the gradients, by calling the `zero_grad()`
    method from your `self.optimizer` object (`zero_grad` is a method of the `Adam`
    class), which will basically set all the gradients of the weights to zero.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 70**: You backpropagate the loss error `td_loss` into the neural network
    by calling the `backward()` function from `td_loss`.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 71**: You perform the weights updates by calling the `step()` method
    from your `self.optimizer` object (`step` is a method of the `Adam` class).'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You've built yourself a tool in the `Dqn` class that will train
    your car to drive better. You've done the toughest part. Now all you have left
    to do is to wrap things up into a last method, called `update`, which will simply
    update the weights after reaching a new state.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Now, in case you are thinking, "but isn't what I've already done with the `learn`
    method?," well, you're right; but you need to make an extra function that will
    update the weights at the right time. The right time to update the weights is
    right after our AI reaches a new state. Simply put, this final `update` method
    you're about to implement will connect the dots between the `learn` method and
    the dynamic environment.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s the finish line! Are you ready? Here''s the code:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Line 73**: You introduce the `update()` method, which takes as input the
    new state reached and the new reward received right after playing an action. This
    new state entered here will be the `state` variable you can see in Line 129 of
    the `map.py` file and this new reward will be the `reward` variable you can see
    in Lines 138 to 145 of the `map.py` file. This `update` method performs some operations
    including the weights updates and, in the end, returns the new action to perform.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 74**: You first convert the new state into a torch tensor and unsqueeze
    it to create an additional dimension (placed first in index 0) corresponding to
    the batch. To ease future operations, you also make sure that all the elements
    of the new state (orientation plus the three signals) are converted into floats
    by adding `.float()`.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 75**: Using the `push()` method from your memory object, add a new transition
    to the memory. This new transition is composed of:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '`self.last_state`: The last state reached before reaching that new state'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`self.last_action`: The last action played that led to that new state'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`self.last_reward`: The last reward received after performing that last action'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`new_state`: The new state that was just reached'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the elements of this new transition are converted into torch tensors.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 76**: Using the `select_action()` method from your `Dqn` class, perform
    a new action from the new state just reached.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 77**: Check if the size of the memory is larger than 100\. In `self.memory.memory`,
    the first `memory` is the object created at Line 53 and the second `memory` is
    the variable object introduced at Line 35.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 78**: If that''s the case, sample 100 transitions from the memory, using
    the `sample()` method from your `self.memory` object. This returns four batches
    of size 100:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_states`: The batch of current states (current at the time of the transition).'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`batch_actions`: The batch of actions performed in the current states.'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`batch_rewards`: The batch of rewards received after playing the actions of `batch_actions`
    in the current states of `batch_states`.'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`batch_next_states`: The batch of next states reached after playing the actions
    of `batch_actions` in the current states of `batch_states`.'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Line 79**: Still in the `if` condition, proceed to the weights updates using
    the `learn()` method called from the same `Dqn` class, with the four previous
    batches as inputs.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 80**: Update the last state reached, `self.last_state`, which becomes
    `new_state`.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 81**: Update the last action performed, `self.last_action`, which becomes
    `new_action`.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 82**: Update the last reward received, `self.last_reward`, which becomes
    `new_reward`.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 83**: Return the new action performed.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: That's it for the `update()` method! I hope you can see how we connected the
    dots. Now, to connect the dots even better, let's see where and how you call that
    `update` method in the `map.py` file.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: First, before calling that `update()` method, you have to create an object of
    the `Dqn` class, which here is called `brain`. That's exactly what you do in Line
    33 of the `map.py` file.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The arguments entered here are the three arguments we see in the `__init__()`
    method of the `Dqn` class:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '`4` is the number of elements in the input state (`input_size`).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`3` is the number of possible actions (`nb_action`).'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0.9` is the discount factor (`gamma`).'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, from this `brain` object, you call on the `update()` method in Line 130
    of the `map.py` file, right after reaching a new state, called `state` in the
    code:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Going back to your `Dqn` class, you need two extra methods:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: The `save()` method, which saves the weights of the AI's network after their
    last updates. This method will be called as soon as you click the **save** button
    while running the map. The weights of your AI will be then saved and put into
    a file named `last_brain.pth`, which will automatically be populated in the folder
    that contains your Python files. That's what allows you to have a pre-trained
    AI.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `load()` method, which loads the saved weights in the `last_brain.pth` file.
    This method will be called as soon as you click the **load** button while running
    the map. It allows you to start the map with a pre-trained self-driving car, without
    having to wait for it to train.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These last two methods aren't AI-related, so we won't spend time explaining
    each line of their code. Still, it's good for you to be able to recognize these
    two tools in case you want to use them for another AI model that you build with
    PyTorch.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how they''re implemented:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Congratulations!
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: That's right! You've finished this 100 lines of code implementation of the AI
    inside our self-driving car. That's quite an accomplishment, especially when coding
    deep Q-learning for the first time. You really can be proud to have gone this
    far.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: After all this hard work, you definitely deserve to have some fun, and I think
    it'll be the most fun to watch the result of your hard work. In other words, you're
    about to see your self-driving car in action! I remember I was so excited the
    first time I ran this. You'll feel it too; it's pretty cool!
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The demo
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I have some good news and some bad news.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ll start with the bad news: we can''t run the `map.py` file with a simple
    plug and play on Google Colab. The reason for that is that Kivy is very tricky
    to install through Colab. So, we''ll go for the classic method of running a Python
    file: through the terminal.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that once we install Kivy and PyTorch through the terminal,
    you'll have a fantastic demo!
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s install everything we need to run our self-driving car. Here''s what
    we have to install, in the following order:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '**Anaconda**: A free and open source distribution of Python that offers an
    easy way to install packages thanks to the `conda` command. This is what we''ll
    use to install PyTorch and Kivy.'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Virtual environment with Python 3.6**: Anaconda is installed with Python
    3.7 or higher; however, that 3.7 version is not compatible with Kivy. We''ll create
    a virtual environment in which we install Python 3.6, a version compatible with
    both Kivy and our implementation. Don''t worry if that sounds intimidating, I''ll
    give you all the details you need to set this up.'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**PyTorch**: Then, inside the virtual environment, we''ll install PyTorch,
    the AI framework used to build our deep Q-network. We''ll install a specific version
    of PyTorch that''s compatible with our implementation, so that everyone can be
    on the same page and run it with no issues. PyTorch upgrades sometimes include
    changes in the names of the modules, which can make an old implementation incompatible
    with the newest PyTorch versions. Here, we know we have the right PyTorch version
    for our implementation.'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Kivy**: To finish, still inside the virtual environment, we''ll install Kivy,
    the open source Python framework on which we will run our map.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's start with Anaconda.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Installing Anaconda
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On Google, or your favorite browser, go to [www.anaconda.com](https://www.anaconda.com/).
    On the Anaconda website, click **Download** on the upper right corner of the screen.
    Scroll down and you''ll find the Python versions to download:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_16.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Installing Anaconda – Step 2'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: At the top, make sure that your system (Windows, macOS, or Linux) is correctly
    selected. If it is, click the **Download** button in the Python 3.7 version box.
    This will download Anaconda with Python 3.7.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Then double-click the downloaded file and keep clicking **Continue** and **Agree**
    to install, until the end. If you're prompted to choose who or how to install
    it for, choose **install for me only**.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Creating a virtual environment with Python 3.6
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that Anaconda''s installed, you can create a virtual environment, named
    `selfdrivingcar`, with Python 3.6 installed. To do this you need to open a terminal and
    enter some commands. Here''s how to open it for the three systems:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: For Linux users, just press `Ctrl` + `Alt` + `T`.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For Mac users, press `Cmd` + `Space`, and then in the Spotlight Search enter
    `Terminal`.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For Windows users, click the Windows button at the lower left corner of your screen,
    find `anaconda` in the list of programs, and click to open Anaconda prompt. A
    black window will open; that's the terminal you'll use to install the packages.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inside the terminal, enter the following command:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Just like so:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_17.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
- en: This command creates a virtual environment called `selfdrivingcar` with Python
    3.6 and other packages installed.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'After pressing `Enter`, you''ll get this in a few seconds:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_18.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: 'Press `y` to proceed. This will download and extract the packages. After a
    few seconds, you''ll get this, which marks the end of the installation:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_19.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: Then we're going to activate the `selfdrivingcar` virtual environment, meaning
    we're going to get inside it in order to install PyTorch and Kivy within the `selfdrivingcar`
    virtual environment.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see just preceding, to activate the environment, we will enter the
    following command:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Enter that command, and then you''ll get inside the virtual environment:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_20.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
- en: Now we can see `(selfdrivingcar)` before my computer's name, `hadelins-macbook-pro`,
    which means we are inside the `selfdrivingcar` virtual environment.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: We're ready for the next steps, which are the installation of PyTorch and Kivy
    inside this virtual environment. Don't close your terminal, or when you open it
    again you'll be back in the main environment.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we''re going to install PyTorch inside the virtual environment by entering
    the following command:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Just like so:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_21.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
- en: 'After a few seconds, we get this:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_22.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
- en: Press `y` again, and then press `Enter`.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few seconds, PyTorch is installed:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_23.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: Installing Kivy
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now let''s proceed to Kivy. In the same virtual environment, we''re going to
    install Kivy by entering the following command:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/B14110_10_24.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: 'Again, we get this:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_25.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
- en: Enter `y` again, and after a few seconds more, Kivy is installed.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_26.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
- en: 'Now I have some terrific news for you: you''re ready to run the self-driving
    car! To do that, we need to run our code in the terminal, still inside our virtual
    environment.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: If you already closed your terminal, then when you open it again enter the `conda
    activate selfdrivingcar` command in order to get back inside the virtual environment.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s run the code! If you haven''t already, download the whole repository
    by clicking the **Clone or download** button on the GitHub page:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: ([https://github.com/PacktPublishing/AI-Crash-Course](https://github.com/PacktPublishing/AI-Crash-Course))
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_27.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: The GitHub repository'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'Then unzip it and move the unzipped folder to your desktop, just like so:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_28.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
- en: 'Now go into `Chapter 10` and select and copy all the files inside:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_29.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
- en: 'Then, because we''re only interested in these files right now, and to simplify
    the command lines in the terminal, paste these files inside the main `AI-Crash-Course-master`
    folder and remove all the rest, which we don''t need, so that you eventually end
    up with this:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_30.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
- en: 'Now we''re going to access this folder from the terminal. Since we put the
    repository folder in the desktop, we will find it in a flash. Back into the terminal,
    enter `ls` (l as in lion) to see in which folder you are in your machine:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_31.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
- en: 'I can see that I''m in my main root folder, which contains the `Desktop` folder.
    It should usually be the case for you too. So now we''re going to go into the
    `Desktop` folder by entering the following command:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/B14110_10_32.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
- en: 'Enter `ls` again and check that you indeed see the `AI-Crash-Course-master`
    folder:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_33.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
- en: 'Then go into the `AI-Crash-Course-master` folder by entering the following
    command:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/B14110_10_34.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
- en: Perfect! Now we're in the right spot! By entering `ls` again, you can see all
    the files of the repo, including the `map.py` file, which is the one we have to
    run to see our self-driving car in action!
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_35.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
- en: If by any chance you had trouble getting to this point, that may be because
    your main root folder doesn't contain your `Desktop` folder. If that's the case,
    just put the `AI-Crash-Course-master` repo folder inside one of the folders that
    you see when entering the `ls` command in the terminal, and redo the same process.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'What you have to do is just find and enter the `AI-Crash-Course-master` folder
    with the `cd` commands. That''s it! Don''t forget to make sure your `AI-Crash-Course-master`
    folder only contains the self-driving car files:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_36.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: Now you're only one command line away from running your self-driving car. I
    hope you're excited to see the results of your hard work; I know exactly how you
    feel, I was in your shoes not so long ago!
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 'So, without further ado, let''s enter the final command, right now. It''s this:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/B14110_10_37.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
- en: 'As soon as you enter it, the map with the car will pop up just like so:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_38.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: The map'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first minute or so, your self-driving car will explore its actions
    by performing nonsense movements; you might see it spinning around. After each
    100 movements, the weights inside the neural network of the AI get updated, and
    the car improves its actions to get higher rewards. And suddenly, maybe after
    another 30 seconds or so, you should see your car making round trips between the
    Airport and Downtown, which I highlighted here again:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_39.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: The destinations'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Now have some fun! Draw some obstacles on the map to see if the car avoids them.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'On my side I have just drawn this, and after a few more minutes of training,
    I can clearly see the car avoiding the obstacles:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_40.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: Road with obstacles'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'And you can have even more fun! By, for example, drawing a road like so:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_10_41.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: The road of the demo'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: After a few minutes of training, the car becomes able to self-drive along that
    road, while making many road trips between the Airport and Downtown.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'Quick question for you: how did you program the car to travel between the destinations?'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'You did it by giving a small positive reward to the AI when the car gets closer
    to the goal. That''s programmed in rows 144 and 145 inside the `map.py` file:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Congratulations to you for completing this massive chapter on this not-so-basic
    self-driving car application! I hope you had fun, and that you feel proud to have
    mastered such an advanced model in deep reinforcement learning.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we learned how to build a deep Q-learning model to drive a
    self-driving car. As inputs it took the information from the three sensors and
    its current orientation. As outputs it decided the Q-values for each of the actions
    of going straight, turning left, or turning right. As for the rewards, we punished
    it badly for hitting the sand, punished it slightly for going in the wrong direction,
    and rewarded it slightly for going in the right direction. We made the AI implementation
    in PyTorch and used Kivy for the graphics. To run all of this we used the Anaconda
    environment.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Now take a long break, you deserve it! I'll see you in the next chapter for
    our next AI challenge, where this time we will solve a real-world business problem
    with cost implications running into the millions.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
