- en: Advanced Applications of Deep Learning in NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following advanced recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Automated text generation from Shakespeare's writings using LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Questions and answers on episodic data using memory networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language modeling to predict the next best word using recurrent neural networks
    – LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative chatbot development using deep learning recurrent networks – LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning techniques are being utilized well to solve some open-ended problems.
    This chapter discusses these types of problems, in which a simple *yes* or *no* would
    be difficult to say. We are hopeful that you will enjoy going through these recipes
    to obtain the viewpoint of what cutting-edge works are going on in this industry
    at the moment, and try to learn some basic building blocks of the same with relevant
    coding snippets.
  prefs: []
  type: TYPE_NORMAL
- en: Automated text generation from Shakespeare's writings using LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use deep **recurrent neural networks** (**RNN**) to
    predict the next character based on the given length of a sentence. This way of
    training a model can generate automated text continuously, which imitates the
    writing style of the original writer with enough training on the number of epochs
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *Project Gutenberg* eBook of the complete works of William Shakespeare''s
    dataset is used to train the network for automated text generation. Data can be
    downloaded from [http://www.gutenberg.org/](http://www.gutenberg.org/) for the
    raw file used for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is used to create a dictionary of characters to indices
    and vice-versa mapping, which we will be using to convert text into indices at
    later stages. This is because deep learning models cannot understand English and
    everything needs to be mapped into indices to train these models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/daf470df-74a1-4e55-b293-16f7fcfba4f5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before training the model, various preprocessing steps are involved to make
    it work. The following are the major steps involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing**: Prepare *X* and *Y* data from the given entire story text
    file and converting them into indices vectorized format.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deep learning model training and validation**: Train and validate the deep
    learning model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Text generation**: Generate the text with the trained model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following lines of code describe the entire modeling process of generating
    text from Shakespeare''s writings. Here we have chosen character length. This
    needs to be considered as `40` to determine the next best single character, which
    seems to be very fair to consider. Also, this extraction process jumps by three
    steps to avoid any overlapping between two consecutive extractions, to create
    a dataset more fairly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot depicts the total number of sentences considered,
    `193798`, which is enough data for text generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcf0bdeb-2033-4410-ab4e-9decd697a6db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next code block is used to convert the data into a vectorized format for
    feeding into deep learning models, as the models cannot understand anything about
    text, words, sentences and so on. Initially, total dimensions are created with
    all zeros in the NumPy array and filled with relevant places with dictionary mappings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The deep learning model is created with RNN, more specifically Long Short-Term
    Memory networks with `128` hidden neurons, and the output is in the dimensions
    of the characters. The number of columns in the array is the number of characters.
    Finally, the `softmax` function is used with the `RMSprop` optimizer. We encourage
    readers to try with other various parameters to check out how results vary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/34c322b4-4201-4882-94da-07bdfff39e8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As mentioned earlier, deep learning models train on number indices to map input
    to output (given a length of 40 characters, the model will predict the next best
    character). The following code is used to convert the predicted indices back to
    the relevant character by determining the maximum index of the character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The model will be trained over `30` iterations with a batch size of `128`.
    And also, the diversity has been changed to see the impact on the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown in the next screenshot to compare the first iteration
    (`Iteration 1`) and final iteration (`Iteration 29`). It is apparent that with
    enough training, the text generation seems to be much better than with `Iteration
    1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad139862-1a1c-4394-af42-5ee640ba9f18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Text generation after `Iteration 29` is shown in this image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0bb264f-1530-4235-8eb9-bcf4281467c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Though the text generation altogether seems to be a bit magical, we have generated
    text using Shakespeare's writings, proving that with the right training and handling,
    we can imitate any writer's style of writing.
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers on episodic data using memory networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use deep RNN to create a model to work on a question-and-answer
    system based on episodic memory. It will extract the relevant answers for a given
    question by reading a story in a sequential manner. For further reading, refer
    to the paper *Dynamic Memory Networks for Natural Language Processing* by Ankit
    Kumar et. al. ([https://arxiv.org/pdf/1506.07285.pdf](https://arxiv.org/pdf/1506.07285.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Facebook's bAbI data has been used for this example, and the same can be downloaded
    from [http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz](http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz).
    It consists of about 20 types of tasks, among which we have taken the first one,
    a single supporting-fact-based question-and-answer system.
  prefs: []
  type: TYPE_NORMAL
- en: 'After unzipping the file, go to the `en-10k` folder and use the files starting
    with `qa1_single supporting-fact` for both the train and test files. The following
    code is used for extraction of stories, questions, and answers in this particular
    order to create the data required for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After extraction, it seems that about 10k observations were created in the
    data for both train and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a777c1c7-bf50-4b1a-97c5-6dee55c5a679.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After extraction of basic datasets, we need to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing**: Create a dictionary and map the story, question and answers
    to vocab to map into vector format.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model development and validation**: Train the deep learning models and test
    on the validation data sample.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Predicting outcomes based on the trained model**: Trained models are utilized
    for predicting outcomes on test data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After train and test data creation, the remaining methodology is described as
    follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create a dictionary for vocabulary, in which for every word
    from the story, question and answer data mapping is created. Mappings are used
    to convert words into integer numbers and subsequently into vector space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot depicts all the words in the vocabulary. It has only
    `22` words, including the `PAD` word, which has been created to fill blank spaces
    or zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6567905e-5112-4afc-9a3e-136b5e303f04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code is used to determine the maximum length of words. By knowing
    this, we can create a vector of maximum size, which can incorporate all lengths
    of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The maximum length of words for story is `14`, and for questions it is `4`.
    For some of the stories and questions, the length could be less than the maximum
    length; those words will be replaced with `0` (or `PAD` word). The reason? This
    padding of extra blanks will make all the observations of equal length. This is
    for computation efficiency, or else it will be difficult to map different lengths,
    or creating parallelization in GPU for computation will be impossible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c1ebfd-8183-4b30-b28c-f9bba117674b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following snippets of code does import various functions from respective classes
    which we will be using in the following section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Word-to-vectorized mapping is being performed in the following code after considering
    the maximum lengths for story, question, and so on, while also considering vocab
    size, all of which we have computed in the preceding segment of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The application of `data_vectorization` is shown in this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image describes the dimensions of train and test data for story,
    question, and answer segments accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8133b229-445b-4f66-a8e7-9a239f8070d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Parameters are initialized in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The core building blocks of the model are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'By reading the model summary in following image, you can see how blocks are
    connected and the see total number of parameters required to be trained to tune
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa0bd335-47c0-4da5-8e42-dbb492fd5af2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following code does perform model fitting on train data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The model accuracy has significantly improved from the first iteration (*train
    accuracy = 19.35%* and *validation accuracy = 28.98%*) to the 40^(th) (*train
    accuracy = 82.22%* and *validation accuracy = 84.51%*), which can be shown in
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d13ea6d1-b563-4a0d-af7b-0acd4db56e73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following code does plot both training & validation accuracy change with respective
    to change in epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The change in accuracy with the number of iterations is shown in the following
    image. It seems that the accuracy has improved marginally rather than drastically
    after `10` iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b00044e5-cedb-4f5a-8c4a-7028cec8a801.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following code, results are predicted which is finding probability for
    each respective class and also applying `argmax` function for finding the class
    where the probability is maximum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: After training the model enough and achieving better accuracies on validation
    data such as 84.51%, it is time to verify with actual test data to see how much
    the predicted answers are in line with the actual answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Out of ten randomly drawn questions, the model was unable to predict the correct
    question only once (for the sixth question; the actual answer is `bedroom` and
    the predicted answer is `office`). This means we have got 90% accuracy  on the
    sample. Though we may not be able to generalize the accuracy value, this gives
    some idea to reader about the prediction ability of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29f8f2b6-ff9e-4e9e-abb5-33338550761e.png)'
  prefs: []
  type: TYPE_IMG
- en: Language modeling to predict the next best word using recurrent neural networks
    LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicting the next word based on some typed words has many real-word applications.
    An example would be to suggest the word while typing it into the Google search
    bar. This type of feature does improve user satisfaction in using search engines.
    Technically, this can be called **N-grams** (if two consecutive words are extracted,
    it will be called **bi-grams**). Though there are so many ways to model this,
    here we have chosen deep RNNs to predict the next best word based on *N-1* pre-words.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alice in Wonderland data has been used for this purpose and the same data can
    be downloaded from [http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt](http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt).
    In the initial data preparation stage, we have extracted N-grams from continuous
    text file data, which looks like a story file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'N-grams are selected with the following `N` value. In the following code, we
    have chosen `N` as `3`, which means each piece of data has three words consecutively.
    Among them, two pre-words (bi-grams) used to predict the next word in each observation.
    Readers are encouraged to change the value of `N` and see how the model predicts
    the words:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: With the increase in N-grams to 4, 5, and 6 or so, we need to provide
    enough amount of incremental data to compensate for the curse of dimensionality.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After extracting basic data observations, we need to perform the following
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing**: In the preprocessing step, words are converted to vectorized
    form, which is needed for working with the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model development and validation**: Create a convergent-divergent model to
    map the input to the output, followed by training and validation data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prediction of next best word**: Utilize the trained model to predict the
    next best word on test data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vectorization of the given words (*X* and *Y* words) to vector space using `CountVectorizer`
    from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After converting the data into vectorized form, we can see that the column
    value remains the same, which is the vocabulary length (2559 of all possible words):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35884aee-14dc-47da-ab5f-70fc2de54929.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code is the heart of the model, consisting of convergent-divergent
    architecture that reduces and expands the shape of the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This screenshot depicts the complete architecture of the model, consisting
    of a convergent-divergent structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52e02c04-33fd-4dea-9ab5-ddb15528aba3.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is trained on data with 100 epochs. Even after a significant improvement
    in the train accuracy (from 5.46% to 63.18%), there is little improvement in the
    validation accuracy (6.63% to 10.53%). However, readers are encouraged to try
    various settings to improve the validation accuracy further:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5db198b-1382-4fe9-914c-0a66d3ef6c1b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Less validation accuracy provides a hint that the model might not predict the
    word very well. The reason could be the very-high-dimensional aspect of taking
    the word rather than the character level (character dimensions are 26, which is
    much less than the 2559 value of words). In the following screenshot, we have
    predicted about two times out of `10`. However, it is very subjective to say whether
    it is a yes or no. Sometimes, the word predicted could be close but not the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78c49c83-741f-4a96-ab85-88af03bcfd8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Generative chatbot using recurrent neural networks (LSTM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative chatbots are very difficult to build and operate. Even today, most
    workable chatbots are retrieving in nature; they retrieve the best response for
    the given question based on semantic similarity, intent, and so on. For further
    reading, refer to the paper *Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation* by Kyunghyun Cho et. al. ([https://arxiv.org/pdf/1406.1078.pdf](https://arxiv.org/pdf/1406.1078.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The A.L.I.C.E Artificial Intelligence Foundation dataset `bot.aiml`  **Artificial
    Intelligence Markup Language** (**AIML**), which is customized syntax such as
    XML file has been used to train the model. In this file, questions and answers
    are mapped. For each question, there is a particular answer. Complete `.aiml`
    files are available at *aiml-en-us-foundation-alice.v1-9* from [https://code.google.com/archive/p/aiml-en-us-foundation-alice/downloads](https://code.google.com/archive/p/aiml-en-us-foundation-alice/downloads).
    Unzip the folder to see the `bot.aiml` file and open it using Notepad. Save as
    `bot.txt` to read in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'AIML files have unique syntax, similar to XML. The `pattern` word is used to
    represent the question and the `template` word for the answer. Hence, we are extracting
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The question and answers are joined to extract the total vocabulary used in
    the modeling, as we need to convert all words/characters into numeric representation.
    The reason is the same as mentioned before—deep learning models can't read English
    and everything is in numbers for the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/113ecf8f-6dc5-4f66-86b2-577e574ddafd.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After extracting the question-and-answer pairs, the following steps are needed
    to process the data and produce the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing**: Convert the question-and-answer pairs into vectorized format,
    which will be utilized in model training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model building and validation**: Develop deep learning models and validate
    the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prediction of answers from trained model**: The trained model will be used
    to predict answers for given questions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The question and answers are utilized to create the vocabulary of words to
    index mapping, which will be utilized for converting words into vector mappings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cdec47c6-3e7d-49ca-88a0-b276eb4a0e61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Encoding and decoding functions are used to convert text to indices and indices
    to text respectively. As we know, Deep learning models work on numeric values
    rather than text or character data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is used to vectorize the question and answers with the given
    maximum length for both questions and answers. Both might be different lengths.
    In some pieces of data, the question length is greater than answer length, and
    in a few cases, it''s length is less than answer length. Ideally, the question
    length is good to catch the right answers. Unfortunately in this case, question
    length is much less than the answer length, which is a very bad example to develop
    generative models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is an important part of the chatbot. Here we have used recurrent
    networks, repeat vector, and time-distributed networks. The repeat vector used
    to match dimensions of input to output values. Whereas time-distributed networks
    are used to change the column vector to the output dimension''s vocabulary size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following model summary describes the change in flow of model size across
    the model. The input layer matches the question''s dimension and the output matches
    the answer''s dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a06e5d6b-d4e4-4ae4-ad8e-c214afa531bc.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are a bit tricky in the following screenshot even though the accuracy
    is significantly higher. The chatbot model might produce complete nonsense, as
    most of the words are padding here. The reason? The number of words in this data
    is less:'
  prefs: []
  type: TYPE_NORMAL
- en: '>![](img/fb81b2d5-3b8b-41ce-910c-25ee240b59be.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot depicts the sample output on test data. The output
    does not seem to make sense, which is an issue with generative models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc37d6c6-52a3-4209-ab28-5c9323203e98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our model did not work well in this case, but still some areas of improvement
    are possible going forward with generative chatbot models. Readers can give it
    a try:'
  prefs: []
  type: TYPE_NORMAL
- en: Have a dataset with lengthy questions and answers to catch signals well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a larger architecture of deep learning models and train over longer iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make question-and-answer pairs more generic rather than factoid-based, such
    as retrieving knowledge and so on, where generative models fail miserably
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
