- en: Use Cases of Neural Networks – Advanced Topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With **Artificial Neural Networks** (**ANN**), let's try to simulate typical
    brain activities such as image perception, pattern recognition, language understanding,
    sense-motor coordination, and so on. ANN models are composed of a system of nodes,
    equivalent to neurons of a human brain, which are interconnected by weighted links,
    equivalent to synapses between neurons. The output of the network is modified
    iteratively from link weights to convergence.
  prefs: []
  type: TYPE_NORMAL
- en: This final chapter presents ANN applications from different use cases and how
    neural networks can be used in the AI world. We will see some use cases and their
    implementation in R. You can adapt the same set of programs for other real work
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow integration with R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras integration with R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handwritten digit recognition using `MNIST` dataset with `H2O`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building LSTM with mxnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering data using auto encoders with `H2O`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**) using `H2O`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breast cancer detection using the `darch` package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have understood the advanced concepts of
    the learning process and their implementation in the R environment. We will apply
    different types of algorithms to implement a neural network. We will review how
    to train, test, and deploy a model. We will look again at how to perform a correct
    valuation procedure. We will also cover more of deep learning in our use cases
    as deep learning is the latest thing that is based on advanced neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow integration with R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is an open source numerical computing library provided by Google
    for machine intelligence. It hides all of the programming required to build deep
    learning models and gives the developers a black box interface to program. The
    Keras API for TensorFlow provides a high-level interface for neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Python is the **de facto** programming language for deep learning, but R is
    catching up. Deep learning libraries are now available with R and a developer
    can easily download TensorFlow or Keras similar to other R libraries and use them.
  prefs: []
  type: TYPE_NORMAL
- en: In TensorFlow, nodes in the graph represent mathematical operations, while the
    graph edges represent the multidimensional data arrays (tensors) communicated
    between them. TensorFlow was originally developed by the Google Brain Team within
    Google's machine intelligence research for machine learning and deep neural networks
    research, but it is now available in the public domain. TensorFlow exploits GPU
    processing when configured appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generic use cases for TensorFlow are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Image recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voice/sound recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-based processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handwriting Recognition** (**HWR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will see how we can bring TensorFlow libraries into R. This
    will open up a huge number of possibilities with deep learning using TensorFlow
    in R. In order to use TensorFlow, we must first install Python. If you don't have
    a Python installation on your machine, it's time to get it.
  prefs: []
  type: TYPE_NORMAL
- en: Python is a dynamic **Object-Oriented Programming** (**OOP**) language that
    can be used for many types of software development. It offers strong support for
    integration with other languages and programs, is provided with a large standard
    library, and can be learned within a few days. Many Python programmers can confirm
    a substantial increase in productivity and feel that it encourages the development
    of higher quality code and maintainability. Python runs on Windows, Linux/Unix,
    macOS X, OS/2, Amiga, Palm Handhelds, and Nokia phones. It also works on Java
    and .NET virtual machines. Python is licensed under the OSI-approved open source
    license; its use is free, including for commercial products.
  prefs: []
  type: TYPE_NORMAL
- en: Python was created in the early 1990s by Guido van Rossum at Stichting Mathematisch
    Centrum in the Netherlands as a successor of a language called **ABC**. Guido
    remains Python's principal author, although it includes many contributions from
    others.
  prefs: []
  type: TYPE_NORMAL
- en: If you do not know which version to use, there is an (English) document that
    could help you choose. In principle, if you have to start from scratch, we recommend
    choosing Python 3, and if you need to use third-party software packages that may
    not be compatible with Python 3, we recommend using Python 2.7\. All information
    about the available versions and how to install Python is given at [https://www.python.org/](https://www.python.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'After properly installing the Python version of our machine, we have to worry
    about installing TensorFlow. We can retrieve all library information and available
    versions of the operating system from the following link: [https://www.tensorflow.org/](https://www.tensorflow.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, in the install section, we can find a series of guides that explain how
    to install a version of TensorFlow that allows us to write applications in Python.
    Guides are available for the following operating systems:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing TensorFlow on Ubuntu
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing TensorFlow on macOS X
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing TensorFlow on Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing TensorFlow from sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, to install Tensorflow on Windows, we must choose one of the following
    types:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow with CPU support only
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow with GPU support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To install TensorFlow, start a terminal with privileges as administrator. Then
    issue the appropriate `pip3` install command in that terminal. To install the
    CPU-only version, enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A series of code lines will be displayed on the video to keep us informed of
    the execution of the installation procedure, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00137.gif)'
  prefs: []
  type: TYPE_IMG
- en: At this point, we can return to our favorite environment; I am referring to
    the R development environment. We will need to install the interface to TensorFlow.
    The R interface to TensorFlow lets you work productively using the high-level
    Keras and Estimator APIs, and when you need more control, it provides full access
    to the core TensorFlow API. To install the R interface to TensorFlow, we will
    use the following procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the `tensorflow` R package from CRAN as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, use the **install_tensorflow()** function to install TensorFlow (for
    a proper installation procedure, you must have administrator privileges):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can confirm that the installation succeeded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This will provide you with a default installation of TensorFlow suitable for
    use with the `tensorflow` R package. Read on if you want to learn about additional
    installation options, including installing a version of TensorFlow that takes
    advantage of NVIDIA GPUs if you have the correct CUDA libraries installed. In
    the following code, we can check the success of the installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Keras integration with R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, RStudio loads the CPU version of TensorFlow. Once Keras is loaded,
    we have a powerful set of deep learning libraries that can be utilized by R programmers
    to execute neural networks and deep learning. To install Keras for R, use this
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we load the `keras` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we check whether keras is installed correctly by loading the `MNIST`
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: MNIST HWR using R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Handwriting Recognition (HWR) is a very commonly used procedure in modern technology.
    The image of the written text can be detected offline from a piece of paper by
    optical scanning (**optical character recognition** (**OCR**)) or intelligent
    word recognition. Alternatively, pen tip movements can be detected online (for
    example, from a pen-computer surface, a task that is generally easier since there
    are more clues available). Technically, recognition of handwriting is the ability
    of a computer to receive and interpret a handwritten intelligible input from sources
    such as paper documents, photos, touchscreens, and other devices.
  prefs: []
  type: TYPE_NORMAL
- en: HWR is performed through various techniques that generally require OCR. However,
    a complete script recognition system also manages formatting, carries out correct
    character segmentation, and finds the most plausible words.
  prefs: []
  type: TYPE_NORMAL
- en: '**Modified National Institute of Standards and Technology** (**MNIST**) is
    a large database of handwritten digits. It has a set of 70,000 examples of data.
    It is a subset of NIST''s larger dataset. The digits are of 28x28 pixel resolution
    and are stored in a matrix of 70,000 rows and 785 columns; 784 columns form each
    pixel value from the 28x28 matrix and one value is the actual digit. The digits
    have been size-normalized and centered in a fixed-size image.'
  prefs: []
  type: TYPE_NORMAL
- en: The digit images in the MNIST set were originally selected and experimented
    with by Chris Burges and Corinna Cortes using bounding-box normalization and centering.
    Yann LeCun's version uses centering by center of mass within in a larger window.
    The data is available on Yann LeCun's website at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each image is created as 28x28\. Here is a sample of images of *0-8* from the
    MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00138.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: MNIST has a sample of several handwritten digits. This dataset can be fed for
    our training to an R program and our code can recognize any new handwritten digit
    that is presented as data for prediction. This is a case where the neural network
    architecture functions as a computer vision system for an AI application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the distribution of the `MNIST` dataset available
    on LeCun''s website:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Digit** | **Count** |'
  prefs: []
  type: TYPE_TB
- en: '| *0* | *5923* |'
  prefs: []
  type: TYPE_TB
- en: '| *1* | *6742* |'
  prefs: []
  type: TYPE_TB
- en: '| *2* | *5958* |'
  prefs: []
  type: TYPE_TB
- en: '| *3* | *6131* |'
  prefs: []
  type: TYPE_TB
- en: '| *4* | *5842* |'
  prefs: []
  type: TYPE_TB
- en: '| *5* | *5421* |'
  prefs: []
  type: TYPE_TB
- en: '| *6* | *5918* |'
  prefs: []
  type: TYPE_TB
- en: '| *7* | *6265* |'
  prefs: []
  type: TYPE_TB
- en: '| *8* | *5851* |'
  prefs: []
  type: TYPE_TB
- en: '| *9* | *5949* |'
  prefs: []
  type: TYPE_TB
- en: We will not use the `h2o` package for deep learning to train and test the `MNIST`
    dataset. We will split the dataset of 70,000 rows into 60,000 training rows and
    10,000 test rows. Then, we'll find the accuracy of the model. The model can then
    be used to predict any incoming dataset of 28x28 pixel handwritten digits containing
    numbers between zero and nine. Finally, we will reduce the file size to 100 rows
    for demo training processing on two datasets in `.csv` format, named `mnist_train_100.csv`
    and `mnist_test_10.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our sample R code, we use a 100-row training dataset and a 10-row test
    dataset. The R code is presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s go through the code to learn how to apply the `h2o` package to
    solve a HWR problem. We''ve already properly introduced the `h2o` package in [Chapter
    3](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4), *Deep Learning Using
    Multilayer Neural Networks*. The `h2o` is included and initiated through the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are displayed in the R prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The training file is opened using a handle. It is set to have 100 rows to simplify
    the demo work. The complete dataset can be downloaded from the URL suggested before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This command sets the working directory where we will have inserted the dataset
    for the next reading.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This piece of code first loads the training dataset of `MNIST`, reducing the
    file size to 100 rows for demo training processing. Then we use the `attach()`
    function to attach the database to the R search path. This means that the database
    is searched by R when evaluating a variable, so objects in the database can be
    accessed by simply giving their names. Finally, we use the `names()` function
    to set the names of the dataset. The same thing we will do for the dataset to
    be used in the testing phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we create a 28x28 matrix with pixel color values by taking the
    tenth row of the dataset, which contains the number zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what we''ve got by plotting an object `image`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following is shown the image of the handwritten digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00139.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s create a mirror image of the handwritten digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, view the image to verify the operation just made:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following is shown the mirror image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00140.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s do the same for the first six rows in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the images of the handwritten digits contained in the first six rows
    of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00141.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Reset the plot options back to default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The next command lets us do some explanatory analysis of the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This command finds the count of each number in the training matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Above are displayed the number of occurrences of each digit in the dataset.
    It''s time to build and train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to produce the summaries of the results of the `model` fitting function,
    we will use the `summary()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows some of the results obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00142.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can understand the evolution of the algorithm used, by checking the performance
    of the training model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have a properly trained `model`, so we can use it to make
    predictions. In our case, we will use it to recognize handwritten digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have used `model`, we need to format the actual and expected matrices
    to verify the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Enter the names of the variables inserted into the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, check the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'As can be seen from the analysis of the table just proposed, for the test data,
    the model has predicted 60 percent (six out of ten) correctly. This accuracy is
    only for the small training dataset. The model can be improved further by:'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the training dataset count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tweaking the parameters of the `h20.deeplearning` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allocating more memory to the `h2o` JVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expanding the test dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM using the iris dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Continuing with the LSTM architecture for RNN introduced in [Chapter 6](part0111.html#39REE0-263fb608a19f4bb5955f37a7741ba5c4),
    *Recurrent and Convolutional Neural Networks*, we present the `iris` dataset processing
    using the `mxnet` LSTM function. The function expects all inputs and outputs as
    numeric. It is particularly useful for processing text sequences, but here we
    will train an LSTM model on the `iris` dataset. The input values are `petal.length`,
    `petal.width`, `sepal.length`, and `sepal.width`. The output variable is `Species`,
    which is converted to a numeric value between one and three. The `iris` dataset
    has been detailed in [Chapter 4](part0088.html#2JTHG0-263fb608a19f4bb5955f37a7741ba5c4),
    *Perceptron Neural Network Modeling – Basic Models*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The program requires `mxnet`, which needs to be installed. `mxnet` for R is
    available for both CPUs and GPUs and for the following OSes: Linux, macOS, and
    Windows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will only indicate the installation procedures for Windows machines and
    CPU versions. Refer to the following URL for information on installation procedures
    for other architectures: [https://mxnet.incubator.apache.org/get_started/install.html](https://mxnet.incubator.apache.org/get_started/install.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `mxnet` on a computer with a CPU processor, we use the prebuilt
    binary package. We can install the package directly on the R console through the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following packages are installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see the installation of the `mxnet` package, install in addition
    to several packages. So, we already have everything we need to proceed. This `mxnet`
    library contains the `mx.lstm` function we are going to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, the internal dataset `iris` is loaded and the `x` and
    `y` variables are set with independent and target variables, respectively. The
    Species variable is converted to a number between one and three:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Just an explanation, with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We asked R to select from the `iris` dataset, which consists of 150 lines and
    five columns, only lines one to four, leaving out the fifth. This procedure will
    also be performed for multiples of five, so in the end, we will omit every multiple
    row of five from our selection. We will also omit the fifth column. At the end,
    we will get 120 rows and four columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now set the input and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we set the dataframe we will use for the test, by selecting only the lines
    we had previously omitted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The `mx.lstm` function is called with the input and output values so that the
    model is trained with the LSTM on the RNN with the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print the results to compare the model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: From the comparison between the test data and those obtained from the forecast
    it can be noticed that the best results were obtained for the versicolor species.
    From the results obtained, it is clear that the model needs to be improved because
    the forecasts it is able to perform are not at the level of those obtained in
    the models we obtained in the previous examples.
  prefs: []
  type: TYPE_NORMAL
- en: Working with autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen autoencoders in the deep learning chapter for unsupervised learning.
    Autoencoders utilize neural networks to perform non-linear dimensionality reduction.
    They represent data in a better way, by finding latent features in it using universal
    function approximators. Autoencoders try to combine or compress input data in
    a different way.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample representation using MLP is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00143.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: PCA using H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the greatest difficulties encountered in multivariate statistical analysis
    is the problem of displaying a dataset with many variables. Fortunately, in datasets
    with many variables, some pieces of data are often closely related to each other.
    This is because they actually contain the same information, as they measure the
    same quantity that governs the behavior of the system. These are therefore redundant
    variables that add nothing to the model we want to build. We can then simplify
    the problem by replacing a group of variables with a new variable that encloses
    the information content.
  prefs: []
  type: TYPE_NORMAL
- en: PCA generates a new set of variables, among them uncorrelated, called principal
    components; each main component is a linear combination of the original variables.
    All principal components are orthogonal to each other, so there is no redundant
    information. The principal components as a whole constitute an orthogonal basis
    for the data space. The goal of PCA is to explain the maximum amount of variance
    with the fewest number of principal components. It is a form of multidimensional
    scaling. It is a linear transformation of the variables into a lower dimensional
    space that retains the maximum amount of information about the variables. A principal
    component is therefore a combination of the original variables after a linear
    transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we use `h2o` to achieve PCA. The `prcomp()` function
    is used find the principal components of a set of input features. This is unsupervised
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's go through the code to understand how to apply the `h2o` package
    to apply PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can proceed with loading the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This command loads the library into the R environment. The following function
    initiates the `h2o` engine with a maximum memory size of `2` GB and two parallel
    cores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The following messages are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We follow the directions on the R prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The `h20.init` function initiates the `h2o` engine with a maximum memory size
    of `2` GB and two parallel cores. The following commands load the data into the
    R environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The first instruction generates the path that contains the file to upload.
    To upload a file in a directory local to your `h2o` instance, use `h2o.uploadFile()`,
    which can also upload data local to your `h2o` instance in addition to your R
    session. In the parentheses, specify the `h2o` reference object in R and the complete
    URL or normalized file path for the file. Let''s see now that it''s inside:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s print a brief summary of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00144.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To perform PCA on the given dataset, we will use the `prcomp()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s print a brief `summary` of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following figure, we see a summary of the PCA model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00145.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To better understand the results, we can make a scree plot of the percent variability
    explained by each principal component. The percent variability explained is contained
    in the model importance variables from the PCA model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows a scree plot of the percent variability explained
    by each principal component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00146.gif)'
  prefs: []
  type: TYPE_IMG
- en: The bar plot shows the proportion of variance for each principal component;
    as you can see, the first two components have about 70 percent of the variance.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders using H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An autoencoder is an ANN used for learning without efficient coding control.
    The purpose of an autoencoder is to learn coding for a set of data, typically
    to reduce dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: Architecturally, the simplest form of autoencoder is an advanced and non-recurring
    neural network very similar to the MLP, with an input level, an output layer,
    and one or more hidden layers that connect them, but with the layer outputs having
    the same number of input level nodes for rebuilding their inputs.
  prefs: []
  type: TYPE_NORMAL
- en: In the following is proposed an example of autoencoder using `h2o` on a `movie`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used in this example is a set of movies and genre taken from [https://grouplens.org/datasets/movielens](https://grouplens.org/datasets/movielens).
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the movies.csv file, which has three columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`movieId`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`title`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`genres`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are 164,979 rows of data for clustering. We will use `h2o.deeplearning`
    to have the `autoencoder` parameter fix the clusters. The objective of the exercise
    is to cluster the movies based on genre, which can then be used to recommend similar
    movies or same-genre movies to the users. The program uses `h20.deeplearning`,
    with the `autoencoder` parameter set to `T`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s go through the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'These commands load the library in the R environment and set the working directory
    where we will have inserted the dataset for the next reading. Then we load the
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'To visualize the type of data contained in the dataset, we analyze a preview
    of one of these variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the first `20` rows of the `movie` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00147.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we build and train `model`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s analyze some of the information contained in `model`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an extract from the results of the `summary()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00148.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next command, we use the `h2o.deepfeatures()` function to extract the
    non-linear feature from an `h2o` dataset using an H2O deep learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, the first six rows of the features extracted from the
    model are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot a diagram where we want to see how the model grouped the movies
    through the results obtained from the analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The plot of the movies, once clustering is done, is shown next. We have plotted
    only 100 movie titles due to space issues. We can see some movies being closely
    placed, meaning they are of the same genre. The titles are clustered based on
    distances between them, based on genre.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00149.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Given the large number of titles, the movie names cannot be distinguished, but
    what appears to be clear is that the model has grouped the movies into three distinct
    groups.
  prefs: []
  type: TYPE_NORMAL
- en: Breast cancer detection using darch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use the `darch` package, which is used for deep architectures
    and **Restricted Boltzmann Machines** (**RBM**). The `darch` package is built
    on the basis of the code from G. E. Hinton and R. R. Salakhutdinov (available
    under MATLAB code for **Deep Belief Nets** (**DBN**)). This package is for generating
    neural networks with many layers (deep architectures) and training them with the
    method introduced by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: This method includes a pre-training with the contrastive divergence method and
    fine-tuning with commonly known training algorithms such as backpropagation or
    conjugate gradients. Additionally, supervised fine-tuning can be enhanced with
    maxout and dropout, two recently developed techniques used to improve fine-tuning
    for deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: The basis of the example is classification based on a set of inputs. To do this,
    we will use the data contained in the dataset named BreastCancer.csv that we just
    used in [Chapter 5](part0096.html#2RHM00-263fb608a19f4bb5955f37a7741ba5c4), *Training
    and Visualizing a Neural Network in R*. This data has been taken from the UCI
    Repository Of Machine Learning. The dataset is periodically updated as soon as
    Dr. Wolberg reports his clinical cases. The data is of breast cancer patients
    with a classification of benign or malignant tumor based on a set of ten independent
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: To get the data, we draw on the large collection of data available in the UCI
    Machine Learning Repository at [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).
  prefs: []
  type: TYPE_NORMAL
- en: 'Details of the data are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of instances**: 699 (as of 15 July 1992)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of attributes**: 10 plus the class attribute'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attribute information**: The class attribute has been moved to the last column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The description of the attributes is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: To understand the `darch` function, we first set up an XOR gate and then use
    it for training and verification. The `darch` function uses output data and input
    attributes to build the model, which can be tested internally by `darch` itself.
    In this case, we achieve 0 percent error and 100 percent accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use the breast cancer data to build the `darch` model and then check
    the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin analyzing the code line by line, explaining in detail all the features
    applied to capture the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The first two lines of the initial code are used to load the libraries needed
    to run the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them. This function should be
    used only once and not every time you run the code.
  prefs: []
  type: TYPE_NORMAL
- en: The `mlbench` library contains a collection of artificial and real-world machine
    learning benchmark problems, including, for example, several datasets from the
    UCI repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `darch` library is a package for deep architectures and RBM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'With this command, we upload the dataset named `BreastCancer`, as mentioned,
    in the `mlbench` library. Let''s see now that it''s inside:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: With this command, we see a brief summary by using the `summary()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the `summary()` function is a generic function used to produce result
    summaries of the results of various model fitting functions. The function invokes
    particular methods that depend on the class of the first argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the function has been applied to a dataframe and the results
    are listed in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00150.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The `summary()` function returns a set of statistics for each variable. In particular,
    it is useful to highlight the result provided for the `Class` variable, which
    contains the diagnosis of the cancer mass. In this case, `458` cases of `benign`
    class and `241` cases of `malignant` class were detected. Another feature to highlight
    is the `Bare.nuclei` variable. For this variable, `16` cases of missing values
    were detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'To remove missing values, we can use the `na.omit()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we build and train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'To evaluate the `model` performance, we can plot the raw network error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot of error versus epoch is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00151.gif)'
  prefs: []
  type: TYPE_IMG
- en: We get the minimum error at 34 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We finally have the network trained and ready for use; now we can use it to
    make our predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'We used the entire set of data at our disposal to make our forecast using the
    model. All we have to do is compare the results obtained with the model predictions
    and the data available in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are really good! Only two wrong classifications! I would say that
    we can be content with the fact that they started from `683` observations. To
    better understand what the errors were, we build a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Although in a simple way, the matrix tells us that we only made two errors
    equally distributed between the two values of the class. For more information
    on the confusion matrix, we can use the `CrossTable()` function contained in the
    `gmodels` package. As always, before loading the book, you need to install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion matrix obtained by using the `CrossTable()` function is shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00152.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we had anticipated in the classification, our model has only two errors:
    *FP* and *FN*. Then calculate the accuracy; as indicated in [Chapter 2](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4),
    *Learning Processes in Neural Networks*, it is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00153.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s calculate the accuracy in R environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned before, the classifier has achieved excellent results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter, we saw some use cases with neural networks and deep learning.
    This should form the basis of your future work on neural networks. The usage is
    common in most cases, with changes in the dataset involved for the model during
    training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw the following examples in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating TensorFlow and Keras with R, which opens up vast set of use cases
    to be built using R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a digit recognizer through classification using H2O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the LSTM function with MxNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA using H2O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an autoencoder using H2O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usage of `darch` for classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R is a very flexible and a major statistical programming language for data scientists
    across the world. A grasp of neural networks with R will help the community evolve
    further and increase the usage of R for deep learning and newer use cases.
  prefs: []
  type: TYPE_NORMAL
