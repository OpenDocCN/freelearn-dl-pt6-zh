<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Text Analysis Using Word Vectors</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In the previous chapter, we learned about encoding an image or encoding users or movies for recommender systems, where the items that are similar have similar vectors. In this chapter, we will be discussing how to encode text data.</span></p>
<p>You will be learning about the following topics:</p>
<ul>
<li>Building a word vector from scratch in Python</li>
<li>Building a word vector using skip-gram and CBOW models</li>
<li>Performing vector arithmetic using pre-trained word vectors</li>
<li>Creating a document vector</li>
<li>Building word vectors using fastText</li>
<li>Building word vectors using GloVe</li>
<li>Building sentiment classification using word vectors</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>In the traditional approach of solving text-related problems, we would one-hot encode the word. However, if the dataset has thousands of unique words, the resulting one-hot-encoded vector would have thousands of dimensions, which is likely to result in computation issues. Additionally, similar words will not have similar vectors in this scenario. Word2Vec is an approach that helps us to achieve similar vectors for similar words.</p>
<p><span>To understand how Word2Vec is useful, let's explore the following problem.</span></p>
<p>Let's say we have two input sentences:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/2ffc33e5-9913-45da-96d2-64b6c54b31b6.png" style="width:11.67em;height:8.25em;" width="146" height="103"/></p>
<p>Intuitively, we know that <strong>enjoy</strong> and <strong>like</strong> are similar words. However, in traditional text mining, when we one-hot encode the words, our output looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/06066bdf-95ff-41d5-85cb-e1789acc850e.png" style="width:42.33em;height:12.33em;" width="631" height="184"/></p>
<p>Notice that one-hot encoding results in each word being assigned a column. The major issue with one-hot encoding such as this is that the Eucledian distance between <strong>I</strong> and <strong>enjoy</strong> is the same as the Eucledian distance between <strong>enjoy</strong> and <strong>like</strong>.</p>
<p>However, intuitively, we know that the distance between <strong>enjoy</strong> and <strong>like</strong> should be lower than the distance between <strong>I</strong> and <strong>enjoy</strong>, as <strong>enjoy</strong> and <strong>like</strong> are similar to each other.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a word vector from scratch in Python</h1>
                </header>
            
            <article>
                
<p>The principle based on which we'll build a word vector is <em>related words will have similar words surrounding them</em>.</p>
<p>For example: the words <em>queen</em> and <em>princess</em> will have similar words (related to a <em>kingdom</em>) around them more frequently. In a way, the context (surrounding words) of these words would be similar.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Our dataset (of two sentences) looks as follows when we take the surrounding words as input and the remaining (middle) word as output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d5a3f1d0-5066-49a1-80a8-70c53b0979e8.png" style="width:37.25em;height:17.83em;" width="545" height="261"/></p>
<p>Notice that we are using the middle word as output and the remaining words as input. A vectorized form of this input and output looks as follows (recall the way in which we converted a sentence into a vector in the <em>Need for encoding in text analysis</em> section in <a href="18e82d39-d5b2-40fe-ae2f-df222c2e1ffe.xhtml" target="_blank">Chapter 9</a>, <em>Encoding Input</em>):</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/17715c08-bfa2-4f12-a2dd-c8b30b67a2cc.png" width="628" height="194"/></p>
<p>Notice that the vectorized form of input in the first row is <em>{0, 1, 1, 1, 0}</em>, as the input word index is <em>{1, 2, 3}</em>, and the output is <em>{1, 0, 0, 0, 0}</em> as the output word's index is <em>{1}</em>.</p>
<p>In such a scenario, our hidden layer has three neurons associated with it. Our neural network would look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1628 image-border" src="Images/536560c0-abd1-4c24-8e19-5e09c079e68f.png" style="width:30.25em;height:29.17em;" width="954" height="918"/></p>
<p>The dimensions of each layer are as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Layer</strong></p>
</td>
<td>
<p><strong>Shape of weights</strong></p>
</td>
<td>
<p><strong>Commentary</strong></p>
</td>
</tr>
<tr>
<td>
<p>Input layer</p>
</td>
<td>
<p>1 x 5</p>
</td>
<td>
<p>Each row is multiplied by five weights.</p>
</td>
</tr>
<tr>
<td>
<p>Hidden layer</p>
</td>
<td>
<p>5 x 3</p>
</td>
<td>
<p>There are five input weights each to the three neurons in the hidden layer.</p>
</td>
</tr>
<tr>
<td>
<p>Output of hidden layer</p>
</td>
<td>
<p>1 x 3</p>
</td>
<td>
<p>This is the matrix multiplication of the input and the hidden layer.</p>
</td>
</tr>
<tr>
<td>
<p>Weights from hidden to output</p>
</td>
<td>
<p>3 x 5</p>
</td>
<td>
<p>Three output hidden units are mapped to five output columns (as there are five unique words).</p>
</td>
</tr>
<tr>
<td>
<p>Output layer</p>
</td>
<td>
<p>1 x 5</p>
</td>
<td>
<p>This is the matrix multiplication between the output of the hidden layer and the weights from the hidden to the output layer.</p>
</td>
</tr>
</tbody>
</table>
<p>Note that we would not be applying activation on top of the hidden layer while building a word vector.</p>
<p>The output layer's values are not restricted to a specific range. Hence, we pass them through the softmax function so that we arrive at the probability of words. Furthermore, we minimize the cross-entropy loss to arrive at the optimal weight values across the network. Now, the word vector of a given word is the hidden-layer unit values when the input is the one-hot encoded version of the word (not the input sentence).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Now that we know how word vectors are generated, let's code up the process of generating word vectors (the code file is available as <kbd>Word_vector_generation.ipynb</kbd> in GitHub):</p>
<ol>
<li>Define the sentences of interest:</li>
</ol>
<pre style="padding-left: 60px">docs = ["I enjoy playing TT", "I like playing TT"]</pre>
<p style="padding-left: 60px">From the preceding, we should expect the word vectors of <kbd>enjoy</kbd> and <kbd>like</kbd> to be similar, as the words around <kbd>enjoy</kbd> and <kbd>like</kbd> are exactly the same.</p>
<ol start="2">
<li>Let's now create the one-hot encoded version of each sentence:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.feature_extraction.text import CountVectorizer<br/>vectorizer = CountVectorizer(min_df=0, token_pattern=r"\b\w+\b")<br/>vectorizer.fit(docs)</pre>
<p style="padding-left: 60px">Note that vectorizer defines the parameters that convert a document into a vector format. Additionally, we pass in more parameters so that words such as <kbd>I</kbd> do not get filtered out in the <kbd>CountVectorizer</kbd>.</p>
<p style="padding-left: 60px">Furthermore, we will fit our documents to the defined vectorizer.</p>
<ol start="3">
<li>Transform the documents into a vector format:</li>
</ol>
<pre style="padding-left: 60px">vector = vectorizer.transform(docs)</pre>
<ol start="4">
<li>Validate the transformations performed:</li>
</ol>
<pre style="padding-left: 60px">print(vectorizer.vocabulary_)<br/>print(vector.shape)<br/>print(vector.toarray())</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1457 image-border" src="Images/5a5b6dc6-d127-48fa-b65e-75253bde3f66.png" style="width:30.83em;height:5.33em;" width="425" height="74"/></p>
<p style="padding-left: 60px">Note that <kbd>vocabulary_</kbd> returns the index of various words, and that converting the <kbd>toarray</kbd> vector returns the one-hot encoded version of sentences.</p>
<ol start="5">
<li>Create the input and the output dataset:</li>
</ol>
<pre style="padding-left: 60px">x = []<br/>y = []<br/>for i in range(len(docs)):<br/>     for j in range(len(docs[i].split())):<br/>         t_x = []<br/>         t_y = []<br/>         for k in range(4):<br/>             if(j==k):<br/>                 t_y.append(docs[i].split()[k])<br/>                 continue<br/>             else:<br/>                 t_x.append(docs[i].split()[k])<br/>         x.append(t_x)<br/>         y.append(t_y)<br/><br/>x2 = []<br/>y2 = []<br/>for i in range(len(x)):<br/>     x2.append(' '.join(x[i]))<br/>     y2.append(' '.join(y[i]))</pre>
<p style="padding-left: 60px">From the preceding code, we have created the input and output datasets. Here is the input dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/0e9852c5-b6c2-45ca-a41f-8ad7a8f4a1d2.jpg" style="width:11.25em;height:9.67em;" width="163" height="141"/></p>
<p style="padding-left: 60px">And here is the output dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1464 image-border" src="Images/7269b7b9-b223-4ba1-b9cf-0103abd330da.png" style="width:35.33em;height:1.83em;" width="475" height="25"/></p>
<ol start="6">
<li>Transform the preceding input and output words into vectors:</li>
</ol>
<pre style="padding-left: 60px">vector_x = vectorizer.transform(x2)<br/>vector_x.toarray()<br/>vector_y = vectorizer.transform(y2)<br/>vector_y.toarray()</pre>
<p style="padding-left: 60px">Here is the input array:</p>
<p class="CDPAlignCenter CDPAlign"/>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1466 image-border" src="Images/4ac45077-8dbc-41aa-b6b8-5ad81fbd5a88.png" style="width:14.75em;height:11.25em;" width="214" height="164"/></p>
<p style="padding-left: 60px">Here is the output array:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1467 image-border" src="Images/98d56a01-487d-4a36-8ae1-f040f3cf8a1e.png" style="width:14.92em;height:11.25em;" width="216" height="163"/></p>
<p class="mce-root"/>
<ol start="7">
<li>Define the neural network model that maps the input and output vector with a hidden layer that has three units:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Dense(3, activation='linear', input_shape=(5,)))<br/>model.add(Dense(5,activation='sigmoid'))</pre>
<ol start="8">
<li>Compile and fit the model:</li>
</ol>
<pre style="padding-left: 60px">model.compile(loss='binary_crossentropy',optimizer='adam')<br/><br/>model.fit(vector_x, vector_y, epochs=1000, batch_size=4,verbose=1)</pre>
<ol start="9">
<li>Extract the word vectors by fetching the intermediate layer values where the inputs are the vectors of each individual word (not a sentence):</li>
</ol>
<pre style="padding-left: 60px">from keras.models import Model<br/>layer_name = 'dense_5'<br/>intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)</pre>
<p style="padding-left: 60px">In the preceding code, we are extracting the output from the layer we are interested in: a layer named <kbd>dense_5</kbd> in the model we initialized.</p>
<p style="padding-left: 60px">In the code below, we are extracting the output of intermediate layer when we pass the one-hot-encoded version of the word as input:</p>
<pre>for i in range(len(vectorizer.vocabulary_)):<br/>     word = list(vectorizer.vocabulary_.keys())[i]<br/>     word_vec = vectorizer.transform([list(vectorizer.vocabulary_.keys())[i]]).toarray()<br/>     print(word, intermediate_layer_model.predict(word_vec))</pre>
<p>The word vectors of individual words are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1468 image-border" src="Images/d0f17447-2ac1-4512-b2be-0c821dd11b4e.png" style="width:36.50em;height:7.92em;" width="438" height="95"/></p>
<p>Note that the words <kbd>enjoy</kbd> and <kbd>like</kbd> are more correlated to each other than others are and hence a better representation of word vectors.</p>
<div class="packt_tip">The name could be different for the model you run, as we did not specify the layer name in our model build. Also, the layer name changes for every new run of model initialization when we do not explicitly specify the model name in the layer.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Measuring the similarity between word vectors</h1>
                </header>
            
            <article>
                
<p>The similarity between word vectors could be measured using multiple metrics—<span>here are</span> two of the more common ones:</p>
<ul>
<li>Cosine similarity</li>
<li>Eucledian distance</li>
</ul>
<p>The cosine similarity between two different vectors, <em>A</em> and <em>B</em>, is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/cf0c3808-dfe4-473c-be3a-b0fd82932a7e.png" style="width:29.17em;height:4.00em;" width="4810" height="660"/></p>
<p>In the example in previous section, the cosine similarity between <em>enjoy</em> and <em>like</em> is calculated as follows:</p>
<p><em>enjoy = (-1.43, -0.94, -2.49)</em></p>
<p><em>like     = (<span>-1.43, -0.94, -2.66</span>)</em></p>
<p>Here is the similarity between the <em>enjoy</em> and <em>like</em> vectors:</p>
<p><em>(<span>-1.43</span>*<span>-1.43</span> + <span>-0.94</span>*<span>-0.94</span> +<span>-2.49</span>*<span>-2.66</span>)/ sqrt((<span>-1.43)</span><sup>2</sup> + (<span>-0.94)</span><sup>2</sup> + (<span>-2.49)</span><sup>2</sup>)* sqrt(<span>(</span><span>-1.43)</span><span>^2 + (</span><span>-0.94)</span><span>^2</span> + (<span>-2.66)</span>^2) = 0.99</em></p>
<p>The Eucledian distance between two different vectors, <em>A</em> and <em>B</em>, is calculated as follows:</p>
<p><em>distance = sqrt(A-B)^2</em></p>
<p><em>= sqrt((<span>-1.43</span> - (<span>-1.43)</span>)^2 + (<span>-0.94</span> - (<span>-0.94)</span>)^2 + (<span>-2.49</span> - (<span>-2.66)</span>)^2)</em></p>
<p><em>= 0.03</em></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a word vector using the skip-gram and CBOW models</h1>
                </header>
            
            <article>
                
<p>In the previous recipe, we built a word vector. In this recipe, we'll build skip-gram and CBOW models using the <kbd>gensim</kbd> library.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The method that we have adopted to build a word vector in this recipe is called a <strong>continuous bag of words</strong> (<strong>CBOW</strong>) model. The reason it is called as CBOW is explained as follows:</p>
<p>Let's use this sentence as an example: <em>I enjoy playing TT</em>.</p>
<p>Here's how the CBOW model handles this sentence:</p>
<ol>
<li>Fix a window of certain size—let's say 1.
<ul>
<li>By specifying the window size, we are specifying the number of words that will be considered to the right as well as to the left of the given word.</li>
</ul>
</li>
<li>Given the window size, the input and output vectors would look as follows:</li>
</ol>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Input words</strong></p>
</td>
<td>
<p><strong>Output word</strong></p>
</td>
</tr>
<tr>
<td>
<p><em>{I, playing}</em></p>
</td>
<td>
<p><em>{enjoy}</em></p>
</td>
</tr>
<tr>
<td>
<p><em>{enjoy,TT}</em></p>
</td>
<td>
<p><em>{playing}</em></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Another approach to building a word vector is the skip-gram model, where the preceding step is reversed, as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p> <strong>Input words</strong></p>
</td>
<td>
<p><strong>Output word</strong></p>
</td>
</tr>
<tr>
<td>
<p><em>{enjoy}</em></p>
</td>
<td>
<p><em>{I, playing}</em></p>
</td>
</tr>
<tr>
<td>
<p><em>{playing}</em></p>
</td>
<td>
<p><em>{enjoy, TT}</em></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The approach to arrive at the hidden layer values of a word remains the same as we discussed in previous section regardless of whether it is a skip-gram model or a CBOW model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it</h1>
                </header>
            
            <article>
                
<p>Now that we understand the backend of how a word vector gets built, let's build word vectors using the skip-gram and CBOW models. To build the model, we will be using the airline sentiment dataset, where tweet texts are given and the sentiments corresponding to the tweets are provided. To generate word vectors, we will be using the <kbd>gensim</kbd> package, as follows (the code file is available as <kbd>word2vec.ipynb</kbd> in GitHub):</p>
<ol>
<li>Install the <kbd>gensim</kbd> package:</li>
</ol>
<pre style="padding-left: 60px"><strong>$pip install gensim</strong></pre>
<ol start="2">
<li>Import the relevant packages:</li>
</ol>
<pre style="padding-left: 60px">import gensim<br/>import pandas as pd</pre>
<ol start="3">
<li>Read the airline tweets sentiment dataset, which contains comments (text) related to airlines and their corresponding sentiment. The dataset can be obtained from <a href="https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2016/03/Airline-Sentiment-2-w-AA.csv">https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2016/03/Airline-Sentiment-2-w-AA.csv</a>:<a href="https://www.figure-eight.com/data-for-everyone/"/></li>
</ol>
<pre style="padding-left: 60px">data=pd.read_csv('https://www.dropbox.com/s/8yq0edd4q908xqw/airline_sentiment.csv')<br/>data.head()                   </pre>
<p style="padding-left: 60px">A sample of the dataset looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/7cdad718-62b5-41b1-8158-947dd9313b85.png" style="width:32.92em;height:12.00em;" width="466" height="170"/></p>
<ol start="4">
<li>Preprocess the preceding text to do the following:
<ul>
<li>Normalize every word to lower case.</li>
<li>Remove punctuation and retain only numbers and alphabets.</li>
<li>Remove stop words:</li>
</ul>
</li>
</ol>
<pre style="padding-left: 60px">import re<br/>import nltk<br/>from nltk.corpus import stopwords<br/>nltk.download('stopwords')<br/>stop = set(stopwords.words('english'))<br/>def preprocess(text):<br/>    text=text.lower()<br/>    text=re.sub('[^0-9a-zA-Z]+',' ',text)<br/>    words = text.split()<br/>    words2 = [i for i in words if i not in stop]<br/>    words3=' '.join(words2)<br/>    return(words3)<br/>data['text'] = data['text'].apply(preprocess)</pre>
<ol start="5">
<li>Split sentences into a list of tokens so that they can then be passed to <kbd>gensim</kbd>. The output of the first sentence should look as follows:</li>
</ol>
<pre style="padding-left: 60px">data['text'][0].split()</pre>
<p style="padding-left: 60px">The above code splits the sentence by space and thus looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/20e6f8c6-be19-41b0-a33e-6c2574611a1d.png" width="521" height="36"/></p>
<p style="padding-left: 60px">We will loop through all the text we have and append it in a list, as follows:</p>
<pre style="padding-left: 60px">list_words=[]<br/>for i in range(len(data)):<br/>     list_words.append(data['text'][i].split())</pre>
<p style="padding-left: 60px">Let's inspect the first three lists within the list of lists:</p>
<pre style="padding-left: 60px">list_words[:3]</pre>
<p style="padding-left: 60px">The list of lists of the first three sentences is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/b06668c0-b3d5-4475-afce-4f836f7cf292.png" style="width:39.67em;height:17.08em;" width="515" height="222"/></p>
<ol start="6">
<li>Build the <kbd>Word2Vec</kbd> model:</li>
</ol>
<pre style="padding-left: 60px">from gensim.models import Word2Vec</pre>
<p style="padding-left: 60px">Define the vector size, context window size to look into, and the minimum count of a word for it to be eligible to have a word vector, as follows:</p>
<pre style="padding-left: 60px">model = Word2Vec(size=100,window=5,min_count=30, sg=0, alpha = 0.025)</pre>
<p style="padding-left: 60px">In the preceding code, <kbd>size</kbd> represents the size (dimension) of word vectors, window represents the context size of words that would be considered, <kbd>min_count</kbd> specifies the minimum frequency based on which a word is considered, <kbd>sg</kbd> represents whether skip-gram would be used (when <kbd>sg=1</kbd>) or CBOW (when <kbd>sg = 0</kbd>) would be used, and alpha represents the learning rate of the model.</p>
<p style="padding-left: 60px">Once the model is defined, we will pass our list of lists to build a vocabulary, as follows:</p>
<pre style="padding-left: 60px">model.build_vocab(list_words)</pre>
<p style="padding-left: 60px">Once the vocabulary is built, the final words that would be left after filtering out the words that occur fewer than 30 times in the whole corpus can be found as follows:</p>
<pre style="padding-left: 60px">model.wv.vocab.keys()</pre>
<p class="mce-root"/>
<ol start="7">
<li>Train the model by specifying the total number of examples (lists) that need to be considered and the number of epochs to be run, as follows:</li>
</ol>
<pre>model.train(list_words, total_examples=model.corpus_count, epochs=100)</pre>
<p style="padding-left: 60px">In the preceding code, <kbd>list_words</kbd> (the list of words) is the input, <kbd>total_examples</kbd> represents the total number of lists to be considered, and epochs is the number of epochs to be run.</p>
<p style="padding-left: 60px">Alternatively, you can also train the model by specifying the <kbd>iter</kbd> parameter in the <kbd>Word2Vec</kbd> method, as follows:</p>
<pre style="padding-left: 60px">model = Word2Vec(list_words,size=100,window=5,min_count=30, iter = 100)</pre>
<ol start="8">
<li>Extract the word vectors of a given word (<kbd>month</kbd>), as follows:</li>
</ol>
<pre style="padding-left: 60px">model['month']</pre>
<p style="padding-left: 60px">The word vector corresponding to the word "month" is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/074b3142-43d7-44e8-9b3d-0875022063eb.png" style="width:37.83em;height:24.58em;" width="500" height="325"/></p>
<p style="padding-left: 60px">The similarity between two words can be calculated as follows:</p>
<pre style="padding-left: 60px">model.similarity('month','year')<br/>0.48</pre>
<p style="padding-left: 60px">The words that are most similar to a given word is calculated as follows:</p>
<pre style="padding-left: 60px">model.most_similar('month')</pre>
<p style="padding-left: 60px">The most similar words of the word <kbd>month</kbd> are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d45f1891-0a22-4ffe-b943-338221155ccd.png" style="width:19.25em;height:12.92em;" width="250" height="168"/></p>
<p style="padding-left: 60px">Note that, while these similarities look low and some of the most similar words do not look intuitive, it will be more realistic once we train on a huge dataset than the 11,000-tweet dataset that we have.</p>
<p style="padding-left: 60px">In the preceding scenario, let's see the output of most similar words to the word "month", when we run the model for a few number of epochs:</p>
<pre style="padding-left: 60px">model = Word2Vec(size=100,window=5,min_count=30, sg=0)<br/>model.build_vocab(list_words)<br/>model.train(list_words, total_examples=model.corpus_count, epochs=5)<br/>model.most_similar('month')</pre>
<p style="padding-left: 60px">The most similar words to the word "month" are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/ccf6b20e-4013-4fc5-aea4-643a97f97cb9.png" width="251" height="168"/></p>
<p class="mce-root"/>
<p>We can see that if we have few epochs, the most similar words to the word <kbd>month</kbd> are not intuitive while the results are intuitive when there are many epochs, particularly as the weights are not fully optimized for few epochs.</p>
<div class="packt_tip">The same operations can be replicated for skip-gram by replacing the value of the <kbd>sg</kbd> parameter with <kbd>1</kbd>.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Performing vector arithmetic using pre-trained word vectors</h1>
                </header>
            
            <article>
                
<p>In the previous section, one of the limitations that we saw is that the number of sentences is too small for us to build a model that is robust (we saw that the correlation of month and year is around 0.4 in the previous section, which is relatively low, as they belong to the same type of words).</p>
<p>To overcome this scenario, we will use the word vectors trained by Google. The pre-trained word vectors from Google include word vectors for a vocabulary of 3,000,000 words and phrases that were trained on words from Google News dataset. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Download the pre-trained word vectors from Google News <span>(the code file is available as </span><kbd>word2vec.ipynb</kbd><span> in GitHub)</span>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz</strong></pre>
<p style="padding-left: 60px">Unzip the downloaded file:</p>
<pre style="padding-left: 60px"><strong>$gunzip '/content/GoogleNews-vectors-negative300.bin.gz'</strong></pre>
<p style="padding-left: 60px">This command unzips the <kbd>bin</kbd> file, which is the saved version of the model.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Load the model:</li>
</ol>
<pre style="padding-left: 60px">from gensim.models import KeyedVectors<br/>filename = '/content/GoogleNews-vectors-negative300.bin'<br/>model = KeyedVectors.load_word2vec_format(filename, binary=True)</pre>
<ol start="3">
<li>Load the  most similar words to the given word, <kbd>month</kbd>:</li>
</ol>
<pre style="padding-left: 60px">model.most_similar('month')</pre>
<p style="padding-left: 60px">The words that are most similar to <kbd>month</kbd> are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/df73a48b-f995-4ad3-98a2-97506f9bad8d.png" width="260" height="170"/></p>
<ol start="4">
<li>We will perform vector arithmetic; that is, we will try to answer the following analogy: woman is to man as what is to king? Check out the following code:</li>
</ol>
<pre style="padding-left: 60px">result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)<br/>print(result)</pre>
<p style="padding-left: 60px">The output of above arithmetic is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/568bc06c-e395-4066-895d-74c8d070fd04.png" width="234" height="26"/></p>
<p>In this scenario, the word vector of <kbd>woman</kbd> is subtracted from the word vector of <kbd>man</kbd> and added to the word vector of <kbd>king</kbd> <span>–</span> resulting in a vector that is closest to the word <kbd>queen</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating a document vector</h1>
                </header>
            
            <article>
                
<p>To understand the reason for having a document vector, let's go through the following intuition.</p>
<p>The word <em>bank</em> is used in the context of finance and also in the context of a river. How do we identify whether the word <em>bank</em> in the given sentence or document is related to the topic of a river or the topic of finance?</p>
<p>This problem could be solved by adding a document vector, which works in a similar way to word-vector generation but with the addition of a one-hot encoded version of the paragraph ID, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1135 image-border" src="Images/d1f47a77-220b-459a-9c00-2e9d68fb81d9.png" style="width:29.33em;height:16.92em;" width="881" height="507"/></p>
<p>In the preceding scenario, the paragraph ID encompasses the delta that is not captured by just the words. For example, in the sentence <em>on the bank of river</em> where <em>on the bank of</em> is the input and <em>river</em> is the output, the words <em>on, the</em>, and <em>of</em> do not contribute to the prediction as they are frequently-occurring words, while the word <em>bank</em> confuses the output prediction to be either river or America. The document ID of this particular document/sentence will help to identify whether the document is related to the topic of rivers or to the topic of finance. This model is called the <strong>Distributed Memory Model of Paragraph Vectors</strong> (<strong>PV-DM</strong>).</p>
<p>For example, if the number of documents is 100, the one-hot encoded version of the paragraph ID will be 100-dimensional. Similarly, if the number of unique words that meet the minimum frequency of a word is 1,000, the one-hot encoded version of the words is 1,000 in size. When the hidden-layer size (which is the word vector size) is 300, the total number of parameters would be 100 * 300 + 1,000 * 300 = 330,000</p>
<p class="mce-root"/>
<p>The document vector would be the value of the hidden layer when the one-hot encoded versions of all input words are 0 (that is, the effect of the words is neutralized and only the effect of the document/paragraph ID is considered).</p>
<p>Similar to the way in which input and output switch between the skip-gram and CBOW models, even for a document vector, the output and input can be switched as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1136 image-border" src="Images/4cc8cbac-3d82-4fe8-ad28-0b5ac55e1631.png" style="width:30.33em;height:19.42em;" width="1088" height="698"/></p>
<p>This representation of the model is called a <strong>paragraph vector with a distributed bag of words</strong> (<strong>PVDBOW</strong>).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The strategy that we'll adopt to build a document vector is as follows:</p>
<ul>
<li>Preprocess the input sentences to remove punctuation as well as lowercasing for all words, and remove the stop words (words that occur very frequently and do not add context to sentence, for example, <em>and</em> and <em>the</em>)</li>
<li>Tag each sentence with its sentence ID.
<ul>
<li>We are assigning an ID for each sentence.</li>
</ul>
</li>
<li>Use the Doc2Vec method to extract vectors for document IDs as well as words.
<ul>
<li>Train the Doc2Vec method over a high number of epochs, so that the model is trained.</li>
</ul>
</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Now that we have the intuition of how a document vector gets generated and a strategy in place to build a document vector, let's generate the document vectors of the airline tweets dataset <span>(the code file is available</span> as <kbd>word2vec.ipynb</kbd> in G<span>itHub)</span>:</p>
<ol>
<li>Import the relevant packages:</li>
</ol>
<pre style="padding-left: 60px">from gensim.models.doc2vec import Doc2Vec, TaggedDocument<br/>from nltk.tokenize import word_tokenize</pre>
<ol start="2">
<li>Preprocess the tweets' text:</li>
</ol>
<pre style="padding-left: 60px">import re<br/>import nltk<br/>from nltk.corpus import stopwords<br/>nltk.download('stopwords')<br/>stop = set(stopwords.words('english'))<br/>def preprocess(text):<br/>    text=text.lower()<br/>    text=re.sub('[^0-9a-zA-Z]+',' ',text)<br/>    words = text.split()<br/>    words2 = [i for i in words if i not in stop]<br/>    words3=' '.join(words2)<br/>    return(words3)<br/>data['text'] = data['text'].apply(preprocess)</pre>
<ol start="3">
<li>Create a dictionary of tagged documents where the document ID is generated along with the text (tweet):</li>
</ol>
<pre style="padding-left: 60px">import nltk<br/>nltk.download('punkt')<br/>tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data['text'])]</pre>
<p style="padding-left: 60px">The tagged document data looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/cfc4df1f-d5d2-4bbd-94a5-b9e08757d5c3.png" width="628" height="32"/></p>
<p style="padding-left: 60px">In the preceding code, we are extracting a list of all the constituent words in a sentence (document).</p>
<ol start="4">
<li>Initialize a model with parameters, as follows:</li>
</ol>
<pre style="padding-left: 60px">max_epochs = 100<br/>vec_size = 300<br/>alpha = 0.025<br/>model = Doc2Vec(size=vec_size,<br/>                alpha=alpha,<br/>                min_alpha=0.00025,<br/>                min_count=30,<br/>                dm =1)</pre>
<p style="padding-left: 60px">In the preceding code snippet, <kbd>size</kbd> represents the vector size of the document, <kbd>alpha</kbd> represents the learning rate, <kbd>min_count</kbd> represents the minimum frequency for a word to be considered, and <kbd>dm = 1</kbd> represents the PV-DM</p>
<p style="padding-left: 60px">Build a vocabulary:</p>
<pre style="padding-left: 60px">model.build_vocab(tagged_data)</pre>
<ol start="6">
<li>Train the model for a high number of epochs on the tagged data:</li>
</ol>
<pre style="padding-left: 60px">model.train(tagged_data,epochs=100,total_examples=model.corpus_count)</pre>
<ol start="7">
<li>The training process would generate vectors for words as well as for the document/paragraph ID.</li>
</ol>
<p style="padding-left: 60px">Word vectors can be fetched similarly to how we fetched them in the previous section, as follows:</p>
<pre style="padding-left: 60px">model['wife']</pre>
<p style="padding-left: 60px">Document vectors can be fetched as follows:</p>
<pre style="padding-left: 60px">model.docvecs[0]</pre>
<p style="padding-left: 60px">The preceding code snippet generates snippets for the document vectors for the first document.</p>
<ol start="8">
<li>Extract the most similar document to a given document ID:</li>
</ol>
<pre style="padding-left: 60px">similar_doc = model.docvecs.most_similar('457')<br/>print(similar_doc)</pre>
<p class="CDPAlignCenter CDPAlign"><img src="Images/45742cdd-3b4c-44a5-8459-8e4833b21286.png" width="613" height="28"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 90px">In the preceding code snippet, we are extracting the document ID that is most similar to the document ID number 457, which is 827.</p>
<p style="padding-left: 90px">Let's look into the text of documents 457 and 827:</p>
<pre style="padding-left: 60px">data['text'][457]</pre>
<p class="CDPAlignCenter CDPAlign"><img src="Images/87db2662-06c1-414d-a69d-25c9b5e0bacd.png" width="188" height="27"/></p>
<pre style="padding-left: 60px">data['text'][827]</pre>
<p class="CDPAlignCenter CDPAlign"><img src="Images/2c6c1158-93c8-4324-8de6-169ac90b85d4.png" width="146" height="24"/></p>
<p>If we inspect the vocabulary of the model, we would see that apart from the word <kbd>just</kbd>, all the other words occur between the two sentences—hence it is obvious that document ID <kbd>457</kbd> is most similar to document ID <kbd>827</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building word vectors using fastText</h1>
                </header>
            
            <article>
                
<p>fastText is a library created by the Facebook Research Team for the efficient learning of word representations and sentence classification.</p>
<p>fastText differs from word2vec in the sense that word2vec treats every single word as the smallest unit whose vector representation is to be found, but fastText assumes a word to be formed by a n-grams of character; for example, sunny is composed of <em>[sun, sunn, sunny]</em>,<em>[sunny, unny, nny]</em>, and so on, where we see a subset of the original word of size <em>n</em>, where <em>n</em> could range from <em>1</em> to the length of the original word.</p>
<p>Another reason for the use of fastText would be that the words do not meet the minimum frequency cut-off in the skip-gram or CBOW models. For example, the word <em>appended</em> would not be very different than <em>append</em>. However, if <em>append</em> occurs frequently, and in the new sentence we have the word <em>appended</em> instead of <em>append</em>, we are not in a position to have a vector for <em>appended</em>. The n-gram consideration of fastText comes in handy in such a scenario.</p>
<p>Practically, fastText uses skip-gram/CBOW models, however, it augments the input dataset so that the unseen words are also taken into consideration.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The strategy that we'll adopt to extract word vectors using fastText is as follows:</p>
<ol>
<li>Use the fastText method in the gensim library</li>
<li>Preprocess the input data</li>
<li>Break each input sentence into a list of lists</li>
<li>Build a vocabulary on top of the input list of lists</li>
<li>Train the model with the preceding input data over multiple epochs</li>
<li>Calculate the similarity between words</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In the following code, let's look at how to generate word vectors using fastText <span>(the code file is availa</span>ble as <kbd>word2vec.ipynb</kbd> in G<span>itHub)</span>:</p>
<ol>
<li>Import the relevant packages:</li>
</ol>
<pre style="padding-left: 60px">from gensim.models.fasttext import FastText</pre>
<ol start="2">
<li>Preprocess and prepare the dataset into a list of lists, just like we did for the word2vec models:</li>
</ol>
<pre style="padding-left: 60px">import re<br/>import nltk<br/>from nltk.corpus import stopwords<br/>nltk.download('stopwords')<br/>stop = set(stopwords.words('english'))<br/>def preprocess(text):<br/>    text=text.lower()<br/>    text=re.sub('[^0-9a-zA-Z]+',' ',text)<br/>    words = text.split()<br/>    words2 = [i for i in words if i not in stop]<br/>    words3=' '.join(words2)<br/>    return(words3)<br/>data['text'] = data['text'].apply(preprocess)</pre>
<p style="padding-left: 60px">In the preceding code, we are preprocessing the input text. Next, let's convert the input text into a list of lists:</p>
<pre style="padding-left: 60px">list_words=[]<br/>for i in range(len(data)):<br/>     list_words.append(data['text'][i].split())</pre>
<p class="mce-root"/>
<ol start="3">
<li>Define the model (specify the number of vectors per word) and build a vocabulary:</li>
</ol>
<pre style="padding-left: 60px">ft_model = FastText(size=100)<br/>ft_model.build_vocab(list_words)</pre>
<ol start="4">
<li>Train the model:</li>
</ol>
<pre style="padding-left: 60px">ft_model.train(list_words, total_examples=ft_model.corpus_count,epochs=100)</pre>
<ol start="5">
<li>Check the word vectors of a word that is not present in the vocabulary of the model. For example, the word <kbd>first</kbd> is present in the vocabulary; however, the word <kbd>firstli</kbd> is not present in the vocabulary. In such a scenario, check the similarity between the word vectors for <kbd>first</kbd> and <kbd>firstli</kbd>:</li>
</ol>
<pre style="padding-left: 60px">ft_model.similarity('first','firstli')</pre>
<p>The output of the preceding code snippet is 0.97, which indicates a very high correlation between the two words.</p>
<p>Thus, we can see that fastText word vectors help us to generate word vectors for words that are not present in the vocabulary.</p>
<div class="packt_tip">The preceding method could also be leveraged to correct the spelling mistakes, if any, within our corpus of data, as the incorrectly-spelled words are likely to occur rarely, and the most similar word with the highest frequency is more likely to be the correctly-spelled version of the misspelled word.</div>
<p>Spelling corrections can be performed using vector arithmetic, as follows:</p>
<pre>result = ft_model.most_similar(positive=['exprience', 'prmise'], negative=['experience'], topn=1)<br/>print(result)</pre>
<p>Note that in the preceding code, the positive words have a spelling mistake, while the negative word does not. The output of the code is <kbd>promise</kbd>. So this potentially corrects our spelling mistake.</p>
<p>Additionally, it can also be performed as follows:</p>
<pre>ft_model.most_similar('exprience', topn=1)</pre>
<div class="CDPAlignLeft CDPAlign"><em>[('experience', 0.9027844071388245)]</em></div>
<p>However, note that this does not work when there are multiple spelling mistakes.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building word vectors using GloVe</h1>
                </header>
            
            <article>
                
<p>Similar to the way word2vec generates word vectors, <strong>GloVe</strong> (short for <strong>Global Vectors for Word Representation</strong>), also generates word vectors but using a different method. In this section, we will explore how <span>GloVe </span>works and then get into the implementation details of <span>GloVe</span>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>GloVe aims to achieve two goals:</p>
<ul>
<li>Creating word vectors that capture meaning in vector space</li>
<li>Taking advantage of global count statistics instead of only local information</li>
</ul>
<p>GloVe learns word vectors by looking at the cooccurrence matrix of words and optimizing for a loss function. The working details of the GloVe algorithm can be understood from the following example:</p>
<p>Let's consider a scenario where there are two sentences, as follows:</p>
<table style="border-collapse: collapse;width: 19.2572%" border="1">
<tbody>
<tr>
<td style="width: 100%">
<p class="CDPAlignLeft CDPAlign"><strong>Sentences</strong></p>
</td>
</tr>
<tr>
<td style="width: 100%">
<p class="CDPAlignLeft CDPAlign">This is test</p>
</td>
</tr>
<tr>
<td style="width: 100%">
<p class="CDPAlignLeft CDPAlign">This is also a</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Let's try to build a word-cooccurrence matrix. There is a total of five unique words within our toy dataset of sentences, and from there the word-cooccurrence matrix looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1471 image-border" src="Images/adf142cf-c7c0-4105-83bb-56260c16ab48.png" style="width:21.50em;height:9.75em;" width="305" height="139"/></p>
<p>In the preceding table, the words <em>this</em> and <em>is</em> occur together in the two rows of the dataset and hence have a cooccurrence value of 2. Similarly, the words <em>this</em> and <em>test</em> occur together only once in the dataset and hence have a cooccurrence value of 1.</p>
<p>However, in the preceding matrix, we have not taken the distance between the two words into consideration. The intuition for considering the distance between the two words is that the farther the cooccurring words are from each other, the less relevant they might be for the cooccurrence.</p>
<p>We will introduce a new metric—<em>offset</em>, which penalizes for having a high distance between the given word and the cooccurring word. For example, <em>test</em> occurs at a distance of 2 from <em>this</em> in the first sentence, so we will divide the cooccurrence number by a value of 2.</p>
<p>The transformed cooccurrence matrix now looks as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p> </p>
</td>
<td>
<p><strong>this</strong></p>
</td>
<td>
<p><strong>is</strong></p>
</td>
<td>
<p><strong>test</strong></p>
</td>
<td>
<p><strong>also</strong></p>
</td>
<td>
<p><strong>a</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>this</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>0.5</p>
</td>
<td>
<p>0.5</p>
</td>
<td>
<p>0.33</p>
</td>
</tr>
<tr>
<td>
<p><strong>is</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0.5</p>
</td>
</tr>
<tr>
<td>
<p><strong>test</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>also</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p><strong>a</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now that we have built the matrix, let's bring in one additional parameter: the <em>context</em> of the words to be considered. For example, if the window size is 2, the cooccurrence value corresponding to the words <em>this</em> and <em>a</em> would be a value of 0 as the distance between the two words is greater than 2. The transformed cooccurrence matrix when the context window size is 2 looks as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p> </p>
</td>
<td>
<p><strong>this</strong></p>
</td>
<td>
<p><strong>is</strong></p>
</td>
<td>
<p><strong>test</strong></p>
</td>
<td>
<p><strong>also</strong></p>
</td>
<td>
<p><strong>a</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>this</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>0.5</p>
</td>
<td>
<p>0.5</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>is</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0.5</p>
</td>
</tr>
<tr>
<td>
<p><strong>test</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>also</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p><strong>a</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>Now that we have arrived at a modified cooccurrence matrix, we randomly initialize the word vectors of each word with a dimension of 2 in this instance. The randomly-initialized weights and bias values of each word, where each word has a vector size of 3, look as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Word</strong></p>
</td>
<td>
<p><strong>Weights 1</strong></p>
</td>
<td>
<p><strong>Weights 2</strong></p>
</td>
<td>
<p><strong>Weights 3</strong></p>
</td>
<td>
<p><strong>Bias</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>this</strong></p>
</td>
<td>
<p>-0.64</p>
</td>
<td>
<p>0.82</p>
</td>
<td>
<p>-0.08</p>
</td>
<td>
<p>0.16</p>
</td>
</tr>
<tr>
<td>
<p><strong>is</strong></p>
</td>
<td>
<p>-0.89</p>
</td>
<td>
<p>-0.31</p>
</td>
<td>
<p>0.79</p>
</td>
<td>
<p>-0.34</p>
</td>
</tr>
<tr>
<td>
<p><strong>test</strong></p>
</td>
<td>
<p>-0.01</p>
</td>
<td>
<p>0.14</p>
</td>
<td>
<p>0.82</p>
</td>
<td>
<p>-0.35</p>
</td>
</tr>
<tr>
<td>
<p><strong>also</strong></p>
</td>
<td>
<p>-0.1</p>
</td>
<td>
<p>-0.67</p>
</td>
<td>
<p>0.89</p>
</td>
<td>
<p>0.26</p>
</td>
</tr>
<tr>
<td>
<p><strong>a</strong></p>
</td>
<td>
<p>-0.1</p>
</td>
<td>
<p>-0.84</p>
</td>
<td>
<p>0.35</p>
</td>
<td>
<p>0.36</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Given that the preceding weights and biases are randomly initialized, we modify the weights to optimize the loss function. In order to do that, let's define the loss function of interest, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7d0d0021-4a99-46aa-9ec9-6a8121574215.png" style="width:22.33em;height:4.00em;" width="3400" height="610"/></p>
<p>In the preceding equation, <em>w<sub>i</sub></em> represents the word vector of the <em>i</em><sup>th</sup> word, and <em>w<sub>j</sub></em> represents the word vector of <em>j</em><sup>th</sup> word; <em>b<sub>i</sub></em> and <em>b<sub>j</sub></em> are the biases that correspond to the <em>i</em><sup>th</sup> and <em>j</em><sup>th</sup> words, respectively. <em>X<sub>ij</sub></em> represents the values in the final cooccurrence value that we defined earlier.</p>
<p>For example, the value of <em>X<sub>ij</sub></em> where <em>i</em> is the word <em>this</em> and <em>j</em> is the word <em>also</em> is 0.5</p>
<p>When the value of <em>X<sub>ij</sub></em> is 0, the value of f(<em>x<sub>ij</sub></em>) is 0; otherwise, it is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e3cdf189-d305-4b20-88c4-6c4ba834f4b1.png" style="width:19.33em;height:3.42em;" width="2720" height="480"/></p>
<p>In the preceding equation, alpha is empirically found to be 0.75, <em>x<sub>max</sub></em> is 100, and <em>x</em> is the value of <em>x<sub>ij</sub></em>.</p>
<p class="mce-root"/>
<p>Now that the equation is defined, let's apply that to our matrix, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1472 image-border" src="Images/d9c693ff-588a-47e5-b951-5503216d9586.png" style="width:82.00em;height:32.25em;" width="984" height="387"/></p>
<p>The first table represents the word-cooccurrence matrix and the randomly-initialized weights and biases.</p>
<p>The second table represents the loss-value calculation, where we calculate the overall weighted loss value.</p>
<p>We optimize the weights and biases until the overall weighted loss value is the least.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Now that we know how word vectors are generated using GloVe, let's implement the same in Python <span>(the code file is available</span> as <kbd>word2vec.ipynb</kbd><span> in GitHub)</span>:</p>
<ol>
<li>Install GloVe:</li>
</ol>
<pre style="padding-left: 60px"><strong>$pip install glove_python</strong></pre>
<ol start="2">
<li>Import the relevant packages:</li>
</ol>
<pre style="padding-left: 60px">from glove import Corpus, Glove</pre>
<ol start="3">
<li>Preprocess the dataset the way we preprocessed in word2vec, skip-gram, and CBOW algorithms, as follows:</li>
</ol>
<pre style="padding-left: 60px">import re<br/>import nltk<br/>from nltk.corpus import stopwords<br/>import pandas as pd<br/>nltk.download('stopwords')<br/>stop = set(stopwords.words('english'))<br/>data = pd.read_csv('https://www.dropbox.com/s/8yq0edd4q908xqw/airline_sentiment.csv?dl=1')<br/>def preprocess(text):<br/>    text=text.lower()<br/>    text=re.sub('[^0-9a-zA-Z]+',' ',text)<br/>    words = text.split()<br/>    words2 = [i for i in words if i not in stop]<br/>    words3=' '.join(words2)<br/>    return(words3)<br/>data['text'] = data['text'].apply(preprocess)<br/>list_words=[]<br/>for i in range(len(data)):<br/>      list_words.append(data['text'][i].split())</pre>
<ol start="4">
<li>Create a corpus and fit it with a vocabulary:</li>
</ol>
<pre style="padding-left: 60px">corpus.fit(list_words, window=5)</pre>
<p style="padding-left: 60px">The dictionary of the corpus can be found as follows:</p>
<pre style="padding-left: 60px">corpus.dictionary</pre>
<p style="padding-left: 60px">The unique words and their corresponding word IDs are obtained as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/a15b2447-73bd-42bd-97e8-5bb1d630f4e9.png" width="168" height="151"/></p>
<p style="padding-left: 60px">The preceding screenshot represents the key values of the words and their corresponding index.</p>
<p style="padding-left: 60px">The following code snippet gives us the cooccurrence matrix:</p>
<pre style="padding-left: 60px">corpus.matrix.todense()</pre>
<p class="CDPAlignCenter CDPAlign"><img src="Images/0248814c-3205-47f8-b94d-79788deb21df.png" style="width:30.25em;height:10.08em;" width="369" height="123"/></p>
<ol start="5">
<li>Let's define the model parameters, that is, the number of dimensions of a vector, the learning rate, and the number of epochs to be run, as follows:</li>
</ol>
<pre style="padding-left: 60px">glove = Glove(no_components=100, learning_rate=0.025)<br/>glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)</pre>
<ol start="6">
<li>Once the model is fit, the weights and biases of word vectors can be found as follows:</li>
</ol>
<pre style="padding-left: 60px">glove.word_biases.tolist()<br/>glove.word_vectors.tolist()</pre>
<ol start="7">
<li>The word vector for a given word can be determined as follows:</li>
</ol>
<pre style="padding-left: 60px">glove.word_vectors[glove.dictionary['united']]</pre>
<ol start="8">
<li>The most similar words for a given word can be determined as follows:</li>
</ol>
<pre style="padding-left: 60px">glove.most_similar('united')</pre>
<p style="padding-left: 60px">The output of most similar words to "united" is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/f88b5cb8-7fc0-4a9f-97cc-13fdc15c6c0e.png" width="268" height="72"/></p>
<p>Note that the words that are the most similar to the word <kbd>united</kbd> are the words that belong to other airlines.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building sentiment classification using word vectors</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we learned how to generate word vectors using multiple models. In this section, we will learn how to build a sentiment classifier for a given sentence. We will continue using the airline sentiment tweet dataset for this exercise.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Generate word vectors like the way we extracted in previous recipes <span>(the code file is available</span> as <kbd>word2vec.ipynb</kbd> <span>in GitHub)</span>:</p>
<ol>
<li>Import the packages and download the dataset:</li>
</ol>
<pre style="padding-left: 60px">import re<br/>import nltk<br/>from nltk.corpus import stopwords<br/>import pandas as pd<br/>nltk.download('stopwords')<br/>stop = set(stopwords.words('english'))<br/>data=pd.read_csv('https://www.dropbox.com/s/8yq0edd4q908xqw/airline_sentiment.csv?dl=1')</pre>
<ol start="2">
<li>Preprocess the input text:</li>
</ol>
<pre style="padding-left: 60px">def preprocess(text):<br/> text=text.lower()<br/> text=re.sub('[^0-9a-zA-Z]+',' ',text)<br/> words = text.split()<br/> words2 = [i for i in words if i not in stop]<br/> words3=' '.join(words2)<br/> return(words3)<br/>data['text'] = data['text'].apply(preprocess)</pre>
<ol start="3">
<li>Extract a list of lists across all the sentences in the dataset:</li>
</ol>
<pre style="padding-left: 60px">t=[]<br/>for i in range(len(data)):<br/> t.append(data['text'][i].split())</pre>
<p class="mce-root"/>
<ol start="4">
<li>Build a CBOW model, where the context window <kbd>size</kbd> is <kbd>5</kbd> and the vector length is 100:</li>
</ol>
<pre style="padding-left: 60px">from gensim.models import Word2Vec<br/>model = Word2Vec(size=100,window=5,min_count=30, sg=0)</pre>
<ol start="5">
<li>Specify the vocabulary to model and then train it:</li>
</ol>
<pre style="padding-left: 60px">model.build_vocab(t)<br/>model.train(t, total_examples=model.corpus_count, epochs=100)</pre>
<ol start="6">
<li>Extract the average vector of a given tweet:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>features= []<br/>for i in range(len(t)):<br/>      t2 = t[i]<br/>      z = np.zeros((1,100))<br/>      k=0<br/>      for j in range(len(t2)):<br/>            try:<br/>              z = z+model[t2[j]]<br/>              k= k+1<br/>            except KeyError:<br/>              continue<br/>      features.append(z/k)</pre>
<p style="padding-left: 60px">We are taking the average of the word vectors for all the words present in the input sentence. Additionally, there will be certain words that are not in the vocabulary (words that occur less frequently) and would result in an error if we try to extract their vectors. We've deployed <kbd>try</kbd> and <kbd>catch</kbd> errors for this specific scenario.</p>
<ol start="7">
<li>Preprocess features to convert them into an array, split the dataset, into train and test datasets and reshape the datasets so that they can be passed to model:</li>
</ol>
<pre style="padding-left: 60px">features = np.array(features)<br/><br/>from sklearn.cross_validation import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(features, data['airline_sentiment'], test_size=0.30,random_state=10)<br/>X_train = X_train.reshape(X_train.shape[0],100)<br/>X_test = X_test.reshape(X_test.shape[0],100)</pre>
<ol start="8">
<li>Compile and build the neural network to predict the sentiment of a tweet:</li>
</ol>
<pre style="padding-left: 60px">from keras.layers import Dense, Activation<br/>from keras.models import Sequential<br/>from keras.utils import to_categorical<br/>from keras.layers.embeddings import Embedding<br/>model = Sequential()<br/>model.add(Dense(1000,input_dim = 100,activation='relu'))<br/>model.add(Dense(1))<br/>model.add(Activation('sigmoid'))<br/>model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>model.summary()</pre>
<p style="padding-left: 60px">The summary of model defined above is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/f192b1ad-1bfb-48b4-aec9-fe09290cf0ad.png" style="width:36.75em;height:16.25em;" width="473" height="209"/></p>
<p style="padding-left: 60px">In the preceding model, we have a 1,000-dimensional hidden layer that connects the 100 inputted average word vector values to the output, which has a value of 1 (1 or 0 for a positive or negative sentiment, respectively):</p>
<pre style="padding-left: 60px">model.fit(X_train, y_train, batch_size=128, nb_epoch=5, validation_data=(X_test, y_test),verbose = 1)</pre>
<p style="padding-left: 60px">We can see that the accuracy of our model is ~90% in predicting the sentiment of a tweet.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="9">
<li>Plot the confusion matrix of predictions:</li>
</ol>
<pre style="padding-left: 60px">pred = model.predict(X_test)<br/>pred2 = np.where(pred&gt;0.5,1,0)<br/>from sklearn.metrics import confusion_matrix<br/>confusion_matrix(y_test, pred2)</pre>
<p style="padding-left: 60px">The output of confusion matrix is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/2c7b07d2-adcd-4ee5-bcaa-b8bfd5201169.png" width="157" height="43"/></p>
<p style="padding-left: 60px">From the above, we see that in 2,644 sentences, we predicted them to be positive and they are actually positive. 125 sentences were predicted to be negative and happened to be positive. 209 sentences were predicted to be positive and happened to be negative and finally, 485 sentences were predicted negative and were actually negative.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>While we implemented the sentiment classification using the CBOW model and an average of all the word vectors that are present in the tweets, the other ways we could have proceeded are as follows:</p>
<ul>
<li>Use the skip-gram model.</li>
<li>Use the doc2vec model to build a model using document vectors.</li>
<li>Use the fastText-model-based word vectors.</li>
<li>Use the GloVe-based word vectors.</li>
<li>Use pre-trained models' word vector values.</li>
</ul>
<p>While these methods work in a similar fashion, one of the limitations of the preceding model is that it does not take word order into consideration. There are more sophisticated algorithms that solve the problem of word order, which will be discussed in the next chapter.</p>


            </article>

            
        </section>
    </div>



  </body></html>