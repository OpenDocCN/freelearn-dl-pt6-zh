["```py\nfrom nltk.corpus import comtrans\nprint(comtrans.aligned_sents('alignment-de-en.txt')[0])\n```", "```py\n<AlignedSent: 'Wiederaufnahme der S...' -> 'Resumption of the se...'>\n```", "```py\nprint(comtrans.aligned_sents()[0].words)\nprint(comtrans.aligned_sents()[0].mots)\n```", "```py\n['Wiederaufnahme', 'der', 'Sitzungsperiode']\n['Resumption', 'of', 'the', 'session']\n```", "```py\nprint(comtrans.aligned_sents()[0].alignment)\n```", "```py\n0-0 1-1 1-2 2-3\n```", "```py\nimport pickle\nimport re\nfrom collections import Counter\nfrom nltk.corpus import comtrans\n```", "```py\ndef retrieve_corpora(translated_sentences_l1_l2='alignment-de-en.txt'):\n    print(\"Retrieving corpora: {}\".format(translated_sentences_l1_l2))\n    als = comtrans.aligned_sents(translated_sentences_l1_l2)\n    sentences_l1 = [sent.words for sent in als]\n    sentences_l2 = [sent.mots for sent in als]\n    return sentences_l1, sentences_l2\n```", "```py\nsen_l1, sen_l2 = retrieve_corpora()\nprint(\"# A sentence in the two languages DE & EN\")\nprint(\"DE:\", sen_l1[0])\nprint(\"EN:\", sen_l2[0])\nprint(\"# Corpora length (i.e. number of sentences)\")\nprint(len(sen_l1))\nassert len(sen_l1) == len(sen_l2)\n```", "```py\nRetrieving corpora: alignment-de-en.txt\n# A sentence in the two languages DE & EN\nDE: ['Wiederaufnahme', 'der', 'Sitzungsperiode']\nEN: ['Resumption', 'of', 'the', 'session']\n# Corpora length (i.e. number of sentences)\n33334\n```", "```py\ndef clean_sentence(sentence):\n    regex_splitter = re.compile(\"([!?.,:;$\\\"')( ])\")\n    clean_words = [re.split(regex_splitter, word.lower()) for word in sentence]\n    return [w for words in clean_words for w in words if words if w]\n```", "```py\nclean_sen_l1 = [clean_sentence(s) for s in sen_l1]\nclean_sen_l2 = [clean_sentence(s) for s in sen_l2]\nprint(\"# Same sentence as before, but chunked and cleaned\")\nprint(\"DE:\", clean_sen_l1[0])\nprint(\"EN:\", clean_sen_l2[0])\n```", "```py\nDE: ['wiederaufnahme', 'der', 'sitzungsperiode']\nEN: ['resumption', 'of', 'the', 'session']\n```", "```py\ndef filter_sentence_length(sentences_l1, sentences_l2, min_len=0, max_len=20):\n    filtered_sentences_l1 = []\n    filtered_sentences_l2 = []\n    for i in range(len(sentences_l1)):\n        if min_len <= len(sentences_l1[i]) <= max_len and \\\n                 min_len <= len(sentences_l2[i]) <= max_len:\n            filtered_sentences_l1.append(sentences_l1[i])\n            filtered_sentences_l2.append(sentences_l2[i])\n    return filtered_sentences_l1, filtered_sentences_l2\n```", "```py\nfilt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(clean_sen_l1, \n          clean_sen_l2)\nprint(\"# Filtered Corpora length (i.e. number of sentences)\")\nprint(len(filt_clean_sen_l1))\nassert len(filt_clean_sen_l1) == len(filt_clean_sen_l2)\n```", "```py\n# Filtered Corpora length (i.e. number of sentences)\n14788\n```", "```py\n_PAD = \"_PAD\"\n_GO = \"_GO\"\n_EOS = \"_EOS\"\n_UNK = \"_UNK\"\n_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\nPAD_ID = 0\nGO_ID = 1\nEOS_ID = 2\nUNK_ID = 3\nOP_DICT_IDS = [PAD_ID, GO_ID, EOS_ID, UNK_ID]\n```", "```py\nimport data_utils\n\ndef create_indexed_dictionary(sentences, dict_size=10000, storage_path=None):\n    count_words = Counter()\n    dict_words = {}\n    opt_dict_size = len(data_utils.OP_DICT_IDS)\n    for sen in sentences:\n        for word in sen:\n            count_words[word] += 1\n\n    dict_words[data_utils._PAD] = data_utils.PAD_ID\n    dict_words[data_utils._GO] = data_utils.GO_ID\n    dict_words[data_utils._EOS] = data_utils.EOS_ID\n    dict_words[data_utils._UNK] = data_utils.UNK_ID\n\n    for idx, item in enumerate(count_words.most_common(dict_size)):\n        dict_words[item[0]] = idx + opt_dict_size\n    if storage_path:\n        pickle.dump(dict_words, open(storage_path, \"wb\"))\n    return dict_words\n```", "```py\ndef sentences_to_indexes(sentences, indexed_dictionary):\n    indexed_sentences = []\n    not_found_counter = 0\n    for sent in sentences:\n        idx_sent = []\n        for word in sent:\n            try:\n                idx_sent.append(indexed_dictionary[word])\n            except KeyError:\n                idx_sent.append(data_utils.UNK_ID)\n                not_found_counter += 1\n        indexed_sentences.append(idx_sent)\n\n    print('[sentences_to_indexes] Did not find {} words'.format(not_found_counter))\n    return indexed_sentences\n```", "```py\ndict_l1 = create_indexed_dictionary(filt_clean_sen_l1, dict_size=15000, storage_path=\"/tmp/l1_dict.p\")\ndict_l2 = create_indexed_dictionary(filt_clean_sen_l2, dict_size=10000, storage_path=\"/tmp/l2_dict.p\")\nidx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\nidx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\nprint(\"# Same sentences as before, with their dictionary ID\")\nprint(\"DE:\", list(zip(filt_clean_sen_l1[0], idx_sentences_l1[0])))\n```", "```py\n# Same sentences as before, with their dictionary ID\nDE: [('wiederaufnahme', 1616), ('der', 7), ('sitzungsperiode', 618)]\nEN: [('resumption', 1779), ('of', 8), ('the', 5), ('session', 549)]\n```", "```py\ndef extract_max_length(corpora):\n    return max([len(sentence) for sentence in corpora])\n```", "```py\nmax_length_l1 = extract_max_length(idx_sentences_l1)\nmax_length_l2 = extract_max_length(idx_sentences_l2)\nprint(\"# Max sentence sizes:\")\nprint(\"DE:\", max_length_l1)\nprint(\"EN:\", max_length_l2)\n```", "```py\n# Max sentence sizes:\nDE: 20\nEN: 20\n```", "```py\ndef prepare_sentences(sentences_l1, sentences_l2, len_l1, len_l2):\n    assert len(sentences_l1) == len(sentences_l2)\n    data_set = []\n    for i in range(len(sentences_l1)):\n        padding_l1 = len_l1 - len(sentences_l1[i])\n        pad_sentence_l1 = ([data_utils.PAD_ID]*padding_l1) + sentences_l1[i]\n        padding_l2 = len_l2 - len(sentences_l2[i])\n        pad_sentence_l2 = [data_utils.GO_ID] + sentences_l2[i] + [data_utils.EOS_ID] + ([data_utils.PAD_ID] * padding_l2)\n        data_set.append([pad_sentence_l1, pad_sentence_l2])\n    return data_set\n```", "```py\ndata_set = prepare_sentences(idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2)\nprint(\"# Prepared minibatch with paddings and extra stuff\")\nprint(\"DE:\", data_set[0][0])\nprint(\"EN:\", data_set[0][1])\nprint(\"# The sentence pass from X to Y tokens\")\nprint(\"DE:\", len(idx_sentences_l1[0]), \"->\", len(data_set[0][0]))\nprint(\"EN:\", len(idx_sentences_l2[0]), \"->\", len(data_set[0][1]))\n```", "```py\n# Prepared minibatch with paddings and extra stuff\nDE: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1616, 7, 618]\nEN: [1, 1779, 8, 5, 549, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n# The sentence pass from X to Y tokens\nDE: 3 -> 20\nEN: 4 -> 22\n```", "```py\nimport time\nimport math\nimport sys\nimport pickle\nimport glob\nimport os\nimport tensorflow as tf\nfrom seq2seq_model import Seq2SeqModel\nfrom corpora_tools import *\n\npath_l1_dict = \"/tmp/l1_dict.p\"\npath_l2_dict = \"/tmp/l2_dict.p\"\nmodel_dir = \"/tmp/translate \"\nmodel_checkpoints = model_dir + \"/translate.ckpt\"\n```", "```py\ndef build_dataset(use_stored_dictionary=False):\n    sen_l1, sen_l2 = retrieve_corpora()\n    clean_sen_l1 = [clean_sentence(s) for s in sen_l1]\n    clean_sen_l2 = [clean_sentence(s) for s in sen_l2]\n    filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(clean_sen_l1, clean_sen_l2)\n\n    if not use_stored_dictionary:\n        dict_l1 = create_indexed_dictionary(filt_clean_sen_l1, dict_size=15000, storage_path=path_l1_dict)\n        dict_l2 = create_indexed_dictionary(filt_clean_sen_l2, dict_size=10000, storage_path=path_l2_dict)\n    else:\n        dict_l1 = pickle.load(open(path_l1_dict, \"rb\"))\n        dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n\n    dict_l1_length = len(dict_l1)\n    dict_l2_length = len(dict_l2)\n\n    idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n    idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n\n    max_length_l1 = extract_max_length(idx_sentences_l1)\n    max_length_l2 = extract_max_length(idx_sentences_l2)\n\n    data_set = prepare_sentences(idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2)\n    return (filt_clean_sen_l1, filt_clean_sen_l2), \\\n        data_set, \\\n        (max_length_l1, max_length_l2), \\\n        (dict_l1_length, dict_l2_length)\n```", "```py\ndef cleanup_checkpoints(model_dir, model_checkpoints):\n    for f in glob.glob(model_checkpoints + \"*\"):\n    os.remove(f)\n    try:\n        os.mkdir(model_dir)\n    except FileExistsError:\n        pass\n```", "```py\ndef get_seq2seq_model(session, forward_only, dict_lengths, max_sentence_lengths, model_dir):\n    model = Seq2SeqModel(\n            source_vocab_size=dict_lengths[0],\n            target_vocab_size=dict_lengths[1],\n            buckets=[max_sentence_lengths],\n            size=256,\n            num_layers=2,\n            max_gradient_norm=5.0,\n            batch_size=64,\n            learning_rate=0.5,\n            learning_rate_decay_factor=0.99,\n            forward_only=forward_only,\n            dtype=tf.float16)\n    ckpt = tf.train.get_checkpoint_state(model_dir)\n    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n        print(\"Reading model parameters from {}\".format(ckpt.model_checkpoint_path))\n        model.saver.restore(session, ckpt.model_checkpoint_path)\n    else:\n        print(\"Created model with fresh parameters.\")\n        session.run(tf.global_variables_initializer())\n    return model\n```", "```py\ndef train():\n    with tf.Session() as sess:\n        model = get_seq2seq_model(sess, False, dict_lengths, max_sentence_lengths, model_dir)\n        # This is the training loop.\n        step_time, loss = 0.0, 0.0\n        current_step = 0\n        bucket = 0\n        steps_per_checkpoint = 100\n        max_steps = 20000\n        while current_step < max_steps:\n            start_time = time.time()\n            encoder_inputs, decoder_inputs, target_weights = model.get_batch([data_set], bucket)\n            _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket, False)\n            step_time += (time.time() - start_time) / steps_per_checkpoint\n            loss += step_loss / steps_per_checkpoint\n            current_step += 1\n            if current_step % steps_per_checkpoint == 0:\n                perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n                print (\"global step {} learning rate {} step-time {} perplexity {}\".format(\n                model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity))\n                sess.run(model.learning_rate_decay_op)\n                model.saver.save(sess, model_checkpoints, global_step=model.global_step)\n                step_time, loss = 0.0, 0.0\n                encoder_inputs, decoder_inputs, target_weights = model.get_batch([data_set], bucket)\n                _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket, True)\n                eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\"inf\")\n                print(\" eval: perplexity {}\".format(eval_ppx))\n                sys.stdout.flush()    \n```", "```py\nif __name__ == \"__main__\":\n    _, data_set, max_sentence_lengths, dict_lengths = build_dataset(False)\n    cleanup_checkpoints(model_dir, model_checkpoints)\n    train()\n```", "```py\n$> python train_translator.py\n```", "```py\nRetrieving corpora: alignment-de-en.txt\n[sentences_to_indexes] Did not find 1097 words\n[sentences_to_indexes] Did not find 0 words\nCreated model with fresh parameters.\nglobal step 100 learning rate 0.5 step-time 4.3573073434829713 perplexity 526.6638556683066\neval: perplexity 159.2240770935855\n[...]\nglobal step 10500 learning rate 0.180419921875 step-time 4.35106209993362414 perplexity 2.0458043055629487\neval: perplexity 1.8646006006241982\n[...]\n```", "```py\nimport pickle\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport data_utils\nfrom train_translator import (get_seq2seq_model, path_l1_dict, path_l2_dict,\nbuild_dataset)\nmodel_dir = \"/tmp/translate\"\n```", "```py\ndef decode():\n    with tf.Session() as sess:\n        model = get_seq2seq_model(sess, True, dict_lengths, max_sentence_lengths, model_dir)\n        model.batch_size = 1\n        bucket = 0\n        for idx in range(len(data_set))[:5]:\n            print(\"-------------------\")\n            print(\"Source sentence: \", sentences[0][idx])\n            print(\"Source tokens: \", data_set[idx][0])\n            print(\"Ideal tokens out: \", data_set[idx][1])\n            print(\"Ideal sentence out: \", sentences[1][idx])\n            encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n                            {bucket: [(data_set[idx][0], [])]}, bucket)\n            _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n            target_weights, bucket, True)\n            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n            if data_utils.EOS_ID in outputs:\n                outputs = outputs[1:outputs.index(data_utils.EOS_ID)]\n            print(\"Model output: \", \" \".join([tf.compat.as_str(inv_dict_l2[output]) for output in outputs]))\n            sys.stdout.flush()\n```", "```py\nif __name__ == \"__main__\":\n    dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n    inv_dict_l2 = {v: k for k, v in dict_l2.items()}\n    build_dataset(True)\n    sentences, data_set, max_sentence_lengths, dict_lengths = build_dataset(False)\n    try:\n        print(\"Reading from\", model_dir)\n        print(\"Dictionary lengths\", dict_lengths)\n        print(\"Bucket size\", max_sentence_lengths)\n    except NameError:\n        print(\"One or more variables not in scope. Translation not possible\")\n        exit(-1)\n    decode()\n```", "```py\nReading model parameters from /tmp/translate/translate.ckpt-10500\n-------------------\nSource sentence: ['wiederaufnahme', 'der', 'sitzungsperiode']\nSource tokens: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1616, 7, 618]\nIdeal tokens out: [1, 1779, 8, 5, 549, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nIdeal sentence out: ['resumption', 'of', 'the', 'session']\nModel output: resumption of the session\n-------------------\nSource sentence: ['ich', 'bitte', 'sie', ',', 'sich', 'zu', 'einer', 'schweigeminute', 'zu', 'erheben', '.']\nSource tokens: [0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 266, 22, 5, 29, 14, 78, 3931, 14, 2414, 4]\nIdeal tokens out: [1, 651, 932, 6, 159, 6, 19, 11, 1440, 35, 51, 2639, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0]\nIdeal sentence out: ['please', 'rise', ',', 'then', ',', 'for', 'this', 'minute', \"'\", 's', 'silence', '.']\nModel output: i ask you to move , on an approach an approach .\n-------------------\nSource sentence: ['(', 'das', 'parlament', 'erhebt', 'sich', 'zu', 'einer', 'schweigeminute', '.', ')']\nSource tokens: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 52, 11, 58, 3267, 29, 14, 78, 3931, 4, 51]\nIdeal tokens out: [1, 54, 5, 267, 3541, 14, 2095, 12, 1440, 35, 51, 2639, 53, 2, 0, 0, 0, 0, 0, 0, 0, 0]\nIdeal sentence out: ['(', 'the', 'house', 'rose', 'and', 'observed', 'a', 'minute', \"'\", 's', 'silence', ')']\nModel output: ( the house ( observed and observed a speaker )\n-------------------\nSource sentence: ['frau', 'präsidentin', ',', 'zur', 'geschäftsordnung', '.']\nSource tokens: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 79, 151, 5, 49, 488, 4]\nIdeal tokens out: [1, 212, 44, 6, 22, 12, 91, 8, 218, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nIdeal sentence out: ['madam', 'president', ',', 'on', 'a', 'point', 'of', 'order', '.']\nModel output: madam president , on a point of order .\n-------------------\nSource sentence: ['wenn', 'das', 'haus', 'damit', 'einverstanden', 'ist', ',', 'werde', 'ich', 'dem', 'vorschlag', 'von', 'herrn', 'evans', 'folgen', '.']\nSource tokens: [0, 0, 0, 0, 85, 11, 603, 113, 831, 9, 5, 243, 13, 39, 141, 18, 116, 1939, 417, 4]\nIdeal tokens out: [1, 87, 5, 267, 2096, 6, 16, 213, 47, 29, 27, 1941, 25, 1441, 4, 2, 0, 0, 0, 0, 0, 0]\nIdeal sentence out: ['if', 'the', 'house', 'agrees', ',', 'i', 'shall', 'do', 'as', 'mr', 'evans', 'has', 'suggested', '.']\nModel output: if the house gave this proposal , i would like to hear mr byrne .\n```"]