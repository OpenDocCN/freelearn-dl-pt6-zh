- en: Image Classification using Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we haven't developed any **machine learning** (**ML**) projects for
    image processing tasks. Linear ML models and other regular **deep neural network** (**DNN**)
    models, such as **Multilayer Perceptrons** (**MLPs**) or **Deep Belief Networks**
    (**DBNs**), cannot learn or model non-linear features from images.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a **convolutional neural network** (**CNN**) is a type of
    feedforward neural network in which the connectivity pattern between its neurons
    is inspired by the animal visual cortex. In the last few years, CNNs have demonstrated
    superhuman performance in complex visual tasks such as image search services,
    self-driving cars, automatic video classification, voice recognition, and **natural
    language processing **(**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will see how to develop an end-to-end project for handling
    multi-label (that is, each entity can belong to multiple classes) image classification
    problems using CNN based on the Scala and **Deeplearning4j** (**DL4j**) framework
    with real **Yelp** image datasets. We will also discuss some theoretical aspects
    of CNNs and how to tune hyperparameters for better classification results before
    getting started.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, we will learn the following topics throughout this end-to-end
    project:'
  prefs: []
  type: TYPE_NORMAL
- en: The drawbacks of regular DNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CNN architectures: convolution operations and pooling layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image classification using CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning CNN hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image classification and drawbacks of DNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start developing the end-to-end project for image classification using
    CNN, we need some background studies, such as the drawbacks of regular DNNs, suitability
    of CNNs over DNNs for image classification, CNN constructions, CNN's different
    operations, and so on. Although regular DNNs work fine for small images (for example,
    MNIST, CIFAR-10), it breaks down for larger images because of the huge number
    of parameters it requires. For example, a 100 x 100 image has 10,000 pixels, and
    if the first layer has just 1,000 neurons (which already severely restricts the
    amount of information transmitted to the next layer), this means a total of 10
    million connections. And that's just for the first layer.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs solve this problem using partially connected layers. Because consecutive
    layers are only partially connected and because it heavily reuses its weights,
    a CNN has far fewer parameters than a fully connected DNN, which makes it much
    faster to train, reduces the risk of overfitting, and requires much less training
    data. Moreover, when a CNN has learned a kernel that can detect a particular feature,
    it can detect that feature anywhere on the image. In contrast, when a DNN learns
    a feature in one location, it can detect it only in that particular location.
    Since images typically have very repetitive features, CNNs are able to generalize
    much better than DNNs for image processing tasks, such as classification, using
    fewer training examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Importantly, DNN has no prior knowledge of how pixels are organized; it does
    not know that nearby pixels are close. A CNN''s architecture embeds this prior
    knowledge. Lower layers typically identify features in small areas of the images,
    while higher layers combine the lower-level features into larger features. This
    works well with most natural images, giving CNNs a decisive head start compared
    to DNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/045f23a6-f77a-48ab-8620-85fcdc44fe31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Regular DNN versus CNN'
  prefs: []
  type: TYPE_NORMAL
- en: For example, in *Figure 1*, on the left, you can see a regular three-layer neural
    network. On the right, a ConvNet arranges its neurons in three dimensions (width,
    height, and depth), as visualized in one of the layers. Every layer of a ConvNet
    transforms the 3D input volume to a 3D output volume of neuron activations. The
    red input layer holds the image, so its width and height would be the dimensions
    of the image, and the depth would be three (red, green, and blue channels).
  prefs: []
  type: TYPE_NORMAL
- en: So, all the multilayer neural networks we looked at had layers composed of a
    long line of neurons, and we had to flatten input images or data to 1D before
    feeding them to the neural network. However, what happens once you try to feed
    them a 2D image directly? The answer is that, in CNN, each layer is represented
    in 2D, which makes it easier to match neurons with their corresponding inputs.
    We will see examples of it in upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Another important fact is all the neurons in a feature map share the same parameters,
    so it dramatically reduces the number of parameters in the model, but, more importantly,
    it means that once the CNN has learned to recognize a pattern in one location,
    it can recognize it in any other location. In contrast, once a regular DNN has
    learned to recognize a pattern in one location, it can recognize it only in that
    particular location.
  prefs: []
  type: TYPE_NORMAL
- en: CNN architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In multilayer networks, such as MLP or DBN, the outputs of all neurons of the
    input layer are connected to each neuron in the hidden layer, so the output will
    again act as the input to the fully-connected layer. In CNN networks, the connection
    scheme that defines the convolutional layer is significantly different. The convolutional
    layer is the main type of layer in CNN, where each neuron is connected to a certain
    region of the input area called the **receptive field**.
  prefs: []
  type: TYPE_NORMAL
- en: In a typical CNN architecture, a few convolutional layers are connected in a
    cascade style, where each layer is followed by a **rectified linear unit** (**ReLU**)
    layer, then a pooling layer, then a few more convolutional layers (+ReLU), then
    another pooling layer, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output from each convolution layer is a set of objects called **feature
    maps** that are generated by a single kernel filter. The feature maps can then
    be used to define a new input to the next layer. Each neuron in a CNN network
    produces an output followed by an activation threshold, which is proportional
    to the input and not bound:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86c20f01-65a5-4544-a6ee-7fc6355e20e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A conceptual architecture of CNN'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 2*, the pooling layers are usually placed after the
    convolutional layers. The convolutional region is then divided by a pooling layer
    into sub-regions. Then, a single representative value is selected using either
    a max-pooling or average pooling technique to reduce the computational time of
    subsequent layers.
  prefs: []
  type: TYPE_NORMAL
- en: This way, the robustness of the feature with respect to its spatial position
    gets increased too. To be more specific, when the image properties, as feature
    maps, pass through the image, they get smaller and smaller as they progress through
    the network, but they also typically get deeper and deeper, since more feature
    maps will be added. At the top of the stack, a regular feedforward neural network
    is added, just like an MLP, which might compose of a few fully connected layers
    (+ReLUs), and the final layer outputs the prediction, for example, a softmax layer
    that outputs estimated class probabilities for a multiclass classification.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A convolution is a mathematical operation that slides one function over another
    and measures the integral of their pointwise multiplication. It has deep connections
    with the Fourier transform and the Laplace transform and is heavily used in signal
    processing. Convolutional layers actually use cross-correlations, which are very
    similar to convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the most important building block of a CNN is the convolutional layer:
    neurons in the first convolutional layer are not connected to every single pixel
    in the input image (as they were in previous chapters), but only to pixels in
    their receptive fields—see *Figure 3*. In turn, each neuron in the second convolutional
    layer is connected only to neurons located within a small rectangle in the first
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2d6b3bc-542d-42ce-9287-6c21eff983ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: CNN layers with rectangular local receptive fields'
  prefs: []
  type: TYPE_NORMAL
- en: This architecture allows the network to concentrate on low-level features in
    the first hidden layer, and then assemble them into higher-level features in the
    next hidden layer, and so on. This hierarchical structure is common in real-world
    images, which is one of the reasons why CNNs work so well for image recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layer and padding operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you understand how convolutional layers work, the pooling layers are quite
    easy to grasp. A pooling layer typically works on every input channel independently,
    so the output depth is the same as the input depth. You may alternatively pool
    over the depth dimension, as we will see next, in which case the image''s spatial
    dimensions (height and width) remain unchanged, but the number of channels is
    reduced. Let''s see a formal definition of pooling layers from a well-known TensorFlow
    website:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The pooling ops sweep a rectangular window over the input tensor, computing
    a reduction operation for each window (average, max, or max with argmax). Each
    pooling op uses rectangular windows of size called ksize separated by offset strides.
    For example, if strides are all ones, every window is used, if strides are all
    twos, every other window is used in each dimension, and so on."'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, just like in convolutional layers, each neuron in a pooling layer
    is connected to the outputs of a limited number of neurons in the previous layer,
    located within a small rectangular receptive field. You must define its size,
    the stride, and the padding type, just like before. However, a pooling neuron
    has no weights; all it does is aggregate the inputs using an aggregation function
    such as the max or mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, the goal of using pooling is to subsample the input image in order to
    reduce the computational load, the memory usage, and the number of parameters.
    This helps to avoid overfitting in the training stage. Reducing the input image
    size also makes the neural network tolerate a little bit of image shift. In the
    following example, we use a 2 x 2 pooling kernel and a stride of 2 with no padding.
    Only the max input value in each kernel makes it to the next layer since the other
    inputs are dropped:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c2fcbec-0188-4c45-8a7c-54f19f36d0d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: An example using max pooling, that is, subsampling'
  prefs: []
  type: TYPE_NORMAL
- en: Usually, *(stride_length)* x + filter_size <= input_layer_size* is recommended
    for most CNN-based network development.
  prefs: []
  type: TYPE_NORMAL
- en: Subsampling operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As stated earlier, a neuron located in a given layer is connected to the outputs
    of the neurons in the previous layer. Now, in order for a layer to have the same
    height and width as the previous layer, it is common to add zeros around the inputs,
    as shown in the diagram. This is called **SAME** or **zero padding**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The term SAME means that the output feature map has the same spatial dimensions
    as the input feature map. Zero padding is introduced to make the shapes match
    as needed, equally on every side of the input map. On the other hand, VALID means
    no padding and only drops the right-most columns (or bottom-most rows):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfdd4a04-457d-444c-9299-ae5649690df6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: SAME versus VALID padding with CNN'
  prefs: []
  type: TYPE_NORMAL
- en: Now we have the minimum theoretical knowledge about CNNs and their architectures,
    it's time to do some hands-on work and create convolutional, pooling, and subsampling
    operations using Deeplearning4j (aka. DL4j), which is one of the first commercial-grade distributed open
    source deep-learning libraries written for Java and Scala. It also provides integrated
    support for Hadoop and Spark. DL4j is designed to be used in business environments
    on distributed GPUs and CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional and subsampling operations in DL4j
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before getting started, setting up our programming environment is a prerequisite.
    So let's do that first.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring DL4j, ND4s, and ND4j
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following libraries can be integrated with DL4j. They will make your JVM
    experience easier, whether you''re developing your ML application in Java or Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DL4j**: Neural net platform'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ND4J**: NumPy for the JVM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataVec**: Tool for ML ETL operations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**JavaCPP**: The bridge between Java and native C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Arbiter**: Evaluation tool for ML algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RL4J**: Deep reinforcement learning for the JVM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ND4j is just like NumPy for JVM. It comes with some basic operations of linear
    algebra, such as matrix creation, addition, and multiplication. ND4S, on the other
    hand, is a scientific computing library for linear algebra and matrix manipulation.
    Basically, it supports n-dimensional arrays for JVM-based languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using Maven on Eclipse (or any other editor—that is, IntelliJ IDEA),
    use the following dependencies in the `pom.xml` file (inside the `<dependencies>`
    tag) for dependency resolution for DL4j, ND4s, and ND4j:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I used old versions since I was facing some compatibility issues, and it is
    still under active development. But feel free to adopt the latest upgrades. I
    believe readers can do it with minimal efforts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, if a native system BLAS is not configured on your machine, ND4j''s
    performance will be reduced. You will experience a warning once you execute simple
    code written in Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'However, installing and configuring BLAS such as OpenBLAS or IntelMKL is not
    that difficult; you can invest some time and do it. Refer to the following URL
    for details: [http://nd4j.org/getstarted.html#open](http://nd4j.org/getstarted.html#open).
    It is also to be noted that the following are prerequisites when working with
    DL4j:'
  prefs: []
  type: TYPE_NORMAL
- en: Java (developer version) 1.8+ (only 64-bit versions supported)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Maven for automated build and dependency manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IntelliJ IDEA or Eclipse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well done! Our programming environment is ready for a simple deep learning application
    development. Now it's time to get your hands dirty with some sample code. Let's
    see how to construct and train a simple CNN, using the CIFAR-10 dataset. CIFAR-10
    is one of the most popular benchmark datasets and has thousands of labelled images.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional and subsampling operations in DL4j
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this subsection, we will see an example of how to construct a CNN for MNIST
    data classification. The network will have two convolutional layers, two subsampling
    layers, one dense layer, and the output layer as the fully connected layer. The
    first layer is a convolutional layer followed by a subsampling layer, which is
    again followed by another convolutional layer. Then, a subsampling layer is followed
    by a dense layer, which is followed by an output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how these layers would look like using DL4j. The first convolution
    layer with ReLU as activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following activation functions are currently supported in DL4j:'
  prefs: []
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaky ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hard Tanh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softmax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ELU** (**Exponential Linear Units**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softsign
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softplus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second layer (that is, the first subsampling layer) is a subsampling layer
    with pooling type `MAX`, with kernel size 2 x 2 and stride size of 2 x 2 but no
    activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The third layer (2nd convolution layer) is a convolutional layer with ReLU
    as activation function, 1*1 stride:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The fourth layer (that is, the second subsampling layer) is a subsampling layer
    with pooling type `MAX`, with kernel size 2 x 2, and stride size of 2 x 2 but
    no activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The fifth layer is a dense layer with ReLU as an activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The sixth (that is, the final and fully connected layer) has Softmax as the
    activation function with the number of classes to be predicted (that is, 10):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the layers are constructed, the next task is to construct and build the
    CNN by chaining all the layers. Using DL4j, it goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we set up all the convolutional layers and initialize the network
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Conventionally, to train a CNN, all images need to be the same shape and size.
    So I placed the dimension as 28 x 28 in the preceding lines for simplicity. Now,
    you may be thinking, how do we train such a network? Well, now we will see this
    but, before that, we need to prepare the MNIST dataset, using the `MnistDataSetIterator
    ()` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s start training the CNN, using the train set and iterate for each
    epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have trained the CNN, the next task is to evaluate the model on the
    test set, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we compute some performance matrices, such as `Accuracy`, `Precision`,
    `Recall`, and `F1 measure`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For your ease, here I provided the full source code for this simple image classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Large-scale image classification using CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show a step-by-step example of developing a real-life
    ML project for image classification. However, we need to know the problem description
    first, to learn what sort of image classification needs to be done. Moreover,
    learning about the dataset is a mandate before getting started.
  prefs: []
  type: TYPE_NORMAL
- en: Problem description
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nowadays, food selfies and photo-centric social storytelling are becoming social
    trends. Food lovers willingly upload an enormous amount of selfies taken with
    food and a picture of the restaurant to social media and respective websites.
    And, of course, they also provide a written review that can significantly boost
    the popularity of the restaurant:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/454a5472-e1c8-4bfe-823d-5161d6510316.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Mining some insights about business from Yelp dataset'
  prefs: []
  type: TYPE_NORMAL
- en: For example, millions of unique visitors visit Yelp and have written more than
    135 million reviews. There are lots of photos and lots of users who are uploading
    photos. Business owners can post photos and message their customers. This way,
    Yelp makes money by **selling ads** to those local businesses. An interesting
    fact is that these photos provide rich local business information across categories.
    Thus, training a computer to understand the context of these photos is not a trivial
    one and also not an easy task (see *Figure 6* to get an insight).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the idea of this project is a challenging one: how can we turn those pictures
    into words? Let''s give it a try. More specifically, you are given photos that
    belong to a business. Now we need to build a model so that it can tags restaurants
    with multiple labels of the user-submitted photos automatically—that is, to predict
    the business attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: Description of the image dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For such a challenge we need to have a real dataset. Don't worry, there are
    several platforms where such datasets are publicly available or can be downloaded
    with some terms and conditions. One such platform is **Kaggle**, which provides
    a platform for data analytics and ML practitioners to try ML challenges and win
    prizes. The Yelp dataset and the description can be found at: [https://www.kaggle.com/c/yelp-restaurant-photo-classification](https://www.kaggle.com/c/yelp-restaurant-photo-classification).
  prefs: []
  type: TYPE_NORMAL
- en: 'The labels of the restaurants are manually selected by Yelp users when they
    submit a review. There are nine different labels annotated by the Yelp community
    associated in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0: good_for_lunch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1: good_for_dinner`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2: takes_reservations`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`3: outdoor_seating`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`4: restaurant_is_expensive`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`5: has_alcohol`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`6: has_table_service`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`7: ambience_is_classy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`8: good_for_kids`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So we need to predict these labels as accurately as possible. One thing to
    be noted is that since Yelp is a community-driven website, there are duplicated
    images in the dataset for several reasons. For example, users can accidentally
    upload the same photo to the same business more than once, or chain businesses
    can upload the same photo to different branches. There are six files in the dataset,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_photos.tgz`: Photos to be used as the training set (234,545 images)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_photos.tgz`: Photos to be used as the test set (500 images)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_photo_to_biz_ids.csv`: Provides the mapping between the photo ID to
    business ID (234,545 rows)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_photo_to_biz_ids.csv`: Provides the mapping between the photo ID to business
    ID ( 500 rows)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train.csv`: This is the main training dataset including business IDs, and
    their corresponding labels (1996 rows)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_submission.csv`: A sample submission—reference correct format for your
    predictions including `business_id` and the corresponding predicted labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workflow of the overall project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this project, we will see how to read images from `.jpg` format into a matrix
    representation in Scala. Then, we will further process and prepare those images
    feedable by the CNNs. We will see several image manipulations, such as squaring
    all the images and resizing every image to the same dimensions, before we apply
    a grayscale filter to the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1501815a-0ebd-4b08-a671-19a7990e0bc4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A conceptualize view of a CNN for image classification'
  prefs: []
  type: TYPE_NORMAL
- en: Then we train nine CNNs on training data for each class. Once the training is
    completed, we save the trained model, CNN configurations and parameters, so that
    they can be restored later on, and then we apply a simple aggregate function to
    assign classes to each restaurant, where each one has multiple images associated
    with it, each with its own vector of probabilities for each of the nine classes.
    Then we score test data and finally, evaluate the model using test images.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see the structure of each CNN. Each network will have two convolutional
    layers, two subsampling layers, one dense layer, and the output layer as the fully
    connected layer. The first layer is a convolutional layer followed by a subsampling
    layer, which is again followed by another convolutional layer, then a subsampling
    layer, then a dense layer, which is followed by an output layer. We will see each
    layer's structure later on.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing CNNs for image classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Scala object containing the `main()` method has the following workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: We read all the business labels from the `train.csv` file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We read and create a map from image ID to business ID of form `imageID` → `busID`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We get a list of images from the `photoDir` directory to load and process and,
    finally, get the image IDs of 10,000 images (feel free to set the range)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then read and process images into a `photoID` → vector map
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We chain the output of *step 3* and *step 4* to align the business feature,
    image IDs, and label IDs to get the feature extracted for the CNN
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We construct nine CNNs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We train all the CNNs and specify the model savings locations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then repeat *step 2* to *step 6* to extract the features from the test set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we evaluate the model and save the prediction in a CSV file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now let''s see how the preceding steps would look in a high-level diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffa6d1ab-e72d-4d69-b4b2-a358679a7ed5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: DL4j image processing pipeline for image classification'
  prefs: []
  type: TYPE_NORMAL
- en: 'Programmatically, the preceding steps can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Too much of a mouthful? Don't worry, we will now see each step in detail. If
    you look at the preceding steps carefully, you'll see *steps 1* to *step 5* are
    basically image processing and feature constructions.
  prefs: []
  type: TYPE_NORMAL
- en: Image processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When I tried to develop this application, I found that the photos are of different
    size and shape: some images are tall, some of them are wide, some of them are
    outside, some images are inside, and most of them are pictures of food. However,
    some are other, random things too. Another important aspect is, while training
    images varied in portrait/landscape and the number of pixels, most were roughly
    square, and many of them were exactly 500 x 375:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40a41e7c-baad-4e63-b049-12851134ce52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Resized figure (left the original and tall one, right the squared
    one)'
  prefs: []
  type: TYPE_NORMAL
- en: As we have already seen, CNN cannot work with images with a heterogeneous size
    and shape. There are many robust and efficient image processing techniques to
    extract only the **region of interest** (**ROI**). But, honestly, I am not an
    image processing expert, so I decided to keep this resizing step simpler.
  prefs: []
  type: TYPE_NORMAL
- en: CNN has a serious limitation as it cannot address the orientational and relative
    spatial relationships. Therefore, these components are not very important to a
    CNN. In short, CNN is not that suitable for images having heterogeneous shape
    and orientation. For why, people are now talking about the Capsule Networks. See
    more at the original paper at [https://arxiv.org/pdf/1710.09829v1.pdf](https://arxiv.org/pdf/1710.09829v1.pdf)
    and [https://openreview.net/pdf?id=HJWLfGWRb](https://openreview.net/pdf?id=HJWLfGWRb).
  prefs: []
  type: TYPE_NORMAL
- en: Naively, I made all the images square, but still, I tried to preserve the quality.
    The ROIs are centered in most cases, so capturing only the center-most square
    of each image is not that trivial. Nevertheless, we also need to convert each
    image to a grayscale image. Let's make irregularly shaped images square. Take
    a look at the following image, where the original one is on the left and the right
    is the square one (see *Figure 9)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have generated a square one, how did we achieve this? Well, I checked
    first if the height and the width are the same, if so, no resizing takes place.
    In the other two cases, I cropped the center region. The following method does
    the trick (but feel free to execute the `SquaringImage.scala` script to see the
    output):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Well done! Now that all of our training images are square, the next import
    preprocessing task is to resize them all. I decided to make all the images 128
    x 128 in size. Let''s see how the previous (the original) one looks after resizing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a529084c-1479-4008-ac6b-37c1de1bebb7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Image resizing (256 x 256, 128 x 128, 64 x 64 and 32 x 32 respectively)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following method did the trick (but feel free to execute the `ImageResize.scala`
    script to see a demo):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'By the way, for the image resizing and squaring, I used some built-in packages
    for image reading and some third-party packages for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the preceding packages, add the following dependencies in a Maven-friendly
    `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Although DL4j-based CNNs can handle color images, it's better to simplify the
    computation with grayscale images. Although color images are more exciting and
    effective, this way we can make the overall representation simpler and space efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s give an example of our previous step. We resized each image to a 256
    x 256 pixel image represented by 16,384 features, rather than 16,384 x 3 for a
    color image having three RGB channels (execute `GrayscaleConverter.scala` to see
    a demo). Let''s see how the converted image would look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c109324-5f47-4bf4-9aec-c9007d184095.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Left - original image, right the grayscale one RGB averaging'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding conversion is done using two methods called `pixels2Gray()` and
    `makeGray()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'So what happens under the hood? We chain the preceding three steps: make all
    the images square, then convert all of them to 25 x 256, and finally convert the
    resized image into a grayscale one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'So, in summary, we now have all the images in gray after squaring and resizing.
    The following image gives some sense of the conversion step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f8c2de5-bfa6-4582-871b-690811daae45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Resized figure (left the original and tall one, right the squared
    one)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following chaining also comes with some additional effort. Now we put these
    three steps together in the code, and we can finally prepare all of our images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Extracting image metadata
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up too this point, we have loaded and pre-processed raw images, but we have
    no idea about the image metadata that we need to make our CNNs learn. Thus, it's
    time to load the CSV files that contain metadata about each image.
  prefs: []
  type: TYPE_NORMAL
- en: 'I wrote a method to read such metadata in CSV format, called `readMetadata()`,
    which is used later on by two other methods called `readBusinessLabels` and `readBusinessToImageLabels`.
    These three methods are defined in the `CSVImageMetadataReader.scala` script.
    Here''s the signature of the `readMetadata()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `readBusinessLabels()` method maps from business ID to labels of the form
    `businessID` → Set (labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `readBusinessToImageLabels ()` method maps from image ID to business ID
    of the form `imageID` → `businessID`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Image feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have seen how to preprocess images so that features from those images
    can be extracted and fed into CNNs. Additionally, we have seen how to extract
    and map metadata and link it with the original images. Now it's time to extract
    features from those preprocessed images.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to keep in mind the provenance of the metadata of each image.
    As you can guess, we need three map operations for feature extractions. Essentially,
    we have three maps. For details see the `imageFeatureExtractor.scala` script:'
  prefs: []
  type: TYPE_NORMAL
- en: Business mapping with the form `imageID` → `businessID`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data map of the form `imageID` → image data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Label map of the form `businessID` → labels
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We first define a regular expression pattern to extract the `.jpg` name from
    the CSV `ImageMetadataReader` class, which is used to match against training labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we extract all the image IDs associated with their respective business
    ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to load and process all the images that are already preprocessed
    to extract the image IDs by mapping with the IDs extracted from the business IDs,
    as shown earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, we have been able to extract all the image IDs that are somehow associated
    with at least one business. The next move would be to read and process these images
    into an `imageID` → vector map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Well done! We are just one step behind to extract required to train our CNNs.
    The final step in feature extraction is to extract the pixel data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bee7abb-8627-4ec1-84b4-66d95a1b5bb4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Image data representation'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we need to keep track of four pieces of the object for each image—that
    is, `imageID`, `businessID`, labels, and pixel data. Thus, as shown in the preceding
    figure, the primary data structure is constructed with four data types (four tuples)—`imgID`,
    `businessID`, pixel data vector, and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, we should have a class containing all pieces of these objects. Don''t
    worry, everything we need is defined in the `featureAndDataAligner.scala` script.
    Once we have instantiated an instance of `featureAndDataAligner` using the following
    line of code in the `Main.scala` script (under the `main` method), the `businessMap`,
    `dataMap`, and `labMap` are provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the option type for `labMap` is used since we don''t have this information
    when we score on test data—that is, `None` is used for that invocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Excellent! Up to this point, we have managed to extract the features to train
    our CNNs. However, the feature in current form is still not suitable to feed into
    the CNNs, because we only have the feature vectors without labels. Thus, we need
    an intermediate conversion.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the ND4j dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As I said, we need an intermediate conversion and pre-process to get the training
    set to contain feature vectors as well as labels. The conversion is pretty straightforward:
    we need the feature vector and the business labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we have the `makeND4jDataSets` class (see `makeND4jDataSets.scala`
    for details). The class creates a ND4j dataset object from the data structure
    from the `alignLables` function in the form of `List[(imgID, bizID, labels, pixelVector)]`.
    First, we prepare the dataset using the `makeDataSet()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we need to convert the preceding data structure further, into `INDArray`,
    which can then be consumed by the CNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Training the CNNs and saving the trained models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have seen how to prepare the training set; now we have a challenge
    ahead. We have to train 234,545 images. Although the testing phase would be less
    exhaustive with only 500 images, it''s better to train each CNN using batch-mode
    using DL4j''s `MultipleEpochsIterator`. Here''s a list of important hyperparameters
    and their details:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Layers**: As we have already observed with our simple 5 layers MNIST, we
    received outstanding classification accuracy, which is very promising. Here I
    will try to construct a similar network having.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The number of samples**: If you''re training all the images, it will take
    too long. If you train using the CPU, not the GPU, it will take days. When I tried
    with 50,000 images, it took one whole day for a machine with an i7 processor and
    32 GB of RAM. Now you can imagine how long it would take for the whole dataset.
    In addition, it will require at least 256 GB of RAM even if you do the training
    in the batch model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of epochs**: This is the number of iterations through all the training
    records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of output feature maps (that is, nOut)**: This is the number of feature
    maps. Take a closer look at other examples in the DL4j GitHub repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning Rate**: From the TensorFlow-like framework, I got some insights.
    In my opinion, it would be great to set a learning rate of 0.01 and 0.001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of batch**: This is the number of records in each batch—32, 64, 128,
    and so on. I used 128.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, with the preceding hyperparameters, we can start training our CNNs. The
    following code does the trick. At first, we prepare the training set, then we
    define the required hyperparameters, then we normalize the dataset so the ND4j
    data frame is encoded and any labels that are considered true are 1s and the rest
    are zeros. Then we shuffle both the rows and labels of the encoded dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to create epochs for the dataset iterator using the `ListDataSetIterator`
    and `MultipleEpochsIterator` respectively. Once the datasets are converted into
    batch-model, we are then ready to train the constructed CNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we also save a `.json` file containing all the network
    configurations and a `.bin` file for holding all the weights and parameters of
    all the CNNs. This is done by two methods; namely, `saveNN()` and `loadNN()` that
    are defined in the `NeuralNetwok.scala` script. First, let''s see the signature
    of the `saveNN()` method that goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The idea is visionary as well as important since, as I said, you wouldn''t
    train your whole network for the second time to evaluate a new test set—that is,
    suppose you want to test just a single image. We also have another method named
    `loadNN()` that reads back the `.json` and `.bin` file we created earlier to a
    `MultiLayerNetwork` and is used to score new test data. The method goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The scoring approach that we''re going to use is pretty simple. It assigns
    business-level labels by averaging the image-level predictions. I know I did it
    naively, but you can try a better approach. What I have done is assign a business
    with label `0` if the average of the probabilities across all of its images that
    it belongs to class `0` is greater than 0.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we collect the model predictions from the `scoreModel()` method and merge
    with `alignedData`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can restore the trained and saved models, restore them back, and
    generate the submission file for Kaggle. The thing is that we need to aggregate
    image predictions to business scores for each model.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping up by executing the main() method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s wrap up the overall discussion by watching the performance of our model.
    The following code is an overall glimpse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: So, what's your impression? It's true that we haven't received outstanding classification
    accuracy. But we can still try with tuned hyperparameters. The next section provides
    some insight.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning and optimizing CNN hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following hyperparameters are very important and must be tuned to achieve
    optimized results.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dropout**: Used for random omission of feature detectors to prevent overfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sparsity**: Used to force activations of sparse/rare inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adagrad**: Used for feature-specific learning-rate optimization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: L1 and L2 regularization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weight transforms**: Useful for deep autoencoders'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability distribution manipulation**: Used for initial weight generation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient normalization and clipping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another important question is: when do you want to add a max pooling layer
    rather than a convolutional layer with the same stride? A max pooling layer has
    no parameters at all, whereas a convolutional layer has quite a few. Sometimes,
    adding a local response normalization layer that makes the neurons that most strongly
    activate inhibit neurons at the same location but in neighboring feature maps,
    encourages different feature maps to specialize and pushes them apart, forcing
    them to explore a wider range of features. It is typically used in the lower layers
    to have a larger pool of low-level features that the upper layers can build upon.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the main advantages observed during the training of large neural networks
    is overfitting, that is, generating very good approximations for the training
    data but emitting noise for the zones between single points. In the case of overfitting,
    the model is specifically adjusted to the training dataset, so it will not be
    used for generalization. Therefore, although it performs well on the training
    set, its performance on the test dataset and subsequent tests is poor because
    it lacks the generalization property:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7c1d031-2428-4f36-b4d1-a703de85c86e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Dropout versus without dropout'
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of this method is that it avoids all neurons in a layer to
    synchronously optimize their weights. This adaptation made in random groups avoids
    all the neurons converging on the same goals, thus de-correlating the adapted
    weights. A second property discovered in the dropout application is that the activation
    of the hidden units becomes sparse, which is also a desirable characteristic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since in CNN, one of the objective functions is to minimize the evaluated cost,
    we must define an optimizer. The following optimizers are supported by DL4j:'
  prefs: []
  type: TYPE_NORMAL
- en: SGD (learning rate only)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nesterovs momentum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adagrad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RMSProp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaDelta
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In most of the cases, we can adopt the implemented RMSProp, which is an advanced
    form of gradient descent, if the performance is not satisfactory. RMSProp performs
    better because it divides the learning rate by an exponentially decaying average
    of squared gradients. The suggested setting value of the decay parameter is 0.9,
    while a good default value for the learning rate is 0.001.
  prefs: []
  type: TYPE_NORMAL
- en: More technically, by using the most common optimizer, such as **Stochastic Gradient
    Descent** (**SGD**), the learning rates must scale with 1/T to get convergence,
    where T is the number of iterations. RMSProp tries to overcome this limitation
    automatically by adjusting the step size so that the step is on the same scale
    as the gradients. So, if you're training a neural network, but computing the gradients
    is mandatory, using RMSProp would be the faster way of learning in a mini-batch
    setting. Researchers also recommend using a Momentum optimizer while training
    a deep CNN or DNN.
  prefs: []
  type: TYPE_NORMAL
- en: From the layering architecture's perspective, CNN is different compared to DNN;
    it has a different requirement as well as tuning criteria. Another problem with
    CNNs is that the convolutional layers require a huge amount of RAM, especially
    during training, because the reverse pass of backpropagation requires all the
    intermediate values computed during the forward pass. During inference (that is,
    when making a prediction for a new instance), the RAM occupied by one layer can
    be released as soon as the next layer has been computed, so you only need as much
    RAM as required by two consecutive layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, during training, everything computed during the forward pass needs
    to be preserved for the reverse pass, so the amount of RAM needed is (at least)
    the total amount of RAM required by all layers. If your GPU runs out of memory
    while training a CNN, here are five things you could try to solve the problem
    (other than purchasing a GPU with more RAM):'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the mini-batch size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce dimensionality using a larger stride in one or more layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove one or more layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use 16-bit floats instead of 32-bit floats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribute the CNN across multiple devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen how to use and build real-life applications using
    CNNs, which are a type of feedforward artificial neural network in which the connectivity
    pattern between neurons is inspired by the organization of the animal visual cortex.
    Our image classifier application using CNN can classify real-life images with
    an acceptable level of accuracy, although we did not achieve higher accuracy.
    However, readers are encouraged to tune hyperparameters in the code and also try
    the same approach with another dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, and importantly since the internal data representation of a convolutional
    neural network does not take into account important spatial hierarchies between
    simple and complex objects, CNN has some serious drawbacks and limitation for
    certain instances. Therefore, I would suggest you take a look at the recent activities
    around capsule networks on GitHub at [https://github.com/topics/capsule-network](https://github.com/topics/capsule-network).
    Hopefully, you can get something useful out from there
  prefs: []
  type: TYPE_NORMAL
- en: This is, more-or-less, the end of our little journey in developing ML projects
    using Scala and different open source frameworks. Throughout the chapters, I tried
    to provide you with several examples of how to use these wonderful technologies
    efficiently for developing ML projects. During the writing of this book, I had
    to keep many constraints in my mind, for example, the page count, API availability,
    and my expertise. But I tried to make the book more-or-less simple, and I also
    tried to avoid details on the theory, as you can read about that in many books,
    blogs, and websites on Apache Spark, DL4j, and H2O itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will also keep the code of this book updated on my GitHub repo at: [https://github.com/PacktPublishing/Scala-Machine-Learning-Projects](https://github.com/PacktPublishing/Scala-Machine-Learning-Projects).
    Feel free to open a new issue or any pull request for improving this book and
    stay tuned.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I did not write this book to earn money but a major portion of the
    royalties will be spent for the child education in the rural areas of my home
    district in Bangladesh. I would like to say thanks and express my sincere gratitude
    for buying and enjoying this book!
  prefs: []
  type: TYPE_NORMAL
