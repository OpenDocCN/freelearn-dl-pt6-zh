<html><head></head><body>
		<div id="_idContainer246" class="Content">
			<h1 id="_idParaDest-164"><em class="italics"><a id="_idTextAnchor212"/>Appendix</em></h1>
		</div>
		<div>
			<div id="_idContainer247" class="Content">
			</div>
		</div>
		<div id="_idContainer248" class="Content">
			<h2>About</h2>
			<p>This section is included to assist the students to perform the activities in the book. It includes detailed steps that are to be performed by the students to achieve the objectives of the activities.</p>
		</div>
		<div id="_idContainer273" class="Content">
			<h2 id="_idParaDest-165"><a id="_idTextAnchor213"/>Chapter 1: Fundamentals of Robotics</h2>
			<h3 id="_idParaDest-166"><a id="_idTextAnchor214"/>Activity 1: Robot Positioning Using Odometry with Python</h3>
			<p><strong class="bold">Solution</strong></p>
			<p class="snippet">from math import pi</p>
			<p class="snippet">def wheel_distance(diameter, encoder, encoder_time, wheel, movement_time):</p>
			<p class="snippet">    time = movement_time / encoder_time</p>
			<p class="snippet">    wheel_encoder = wheel * time</p>
			<p class="snippet">    wheel_distance = (wheel_encoder * diameter * pi) / encoder</p>
			<p class="snippet">    </p>
			<p class="snippet">    return wheel_distance</p>
			<p class="snippet">from math import cos,sin</p>
			<p class="snippet">def final_position(initial_pos,wheel_axis,angle):</p>
			<p class="snippet">    final_x=initial_pos[0]+(wheel_axis*cos(angle))</p>
			<p class="snippet">    final_y=initial_pos[1]+(wheel_axis*sin(angle))</p>
			<p class="snippet">    final_angle=initial_pos[2]+angle</p>
			<p class="snippet">    </p>
			<p class="snippet">    return(final_x,final_y,final_angle)</p>
			<p class="snippet">def position(diameter,base,encoder,encoder_time,left,right,initial_pos,movement_time):</p>
			<p class="snippet">#First step: Wheels completed distance</p>
			<p class="snippet">    left_wheel=wheel_distance(diameter,encoder,encoder_time,left,movement_time)</p>
			<p class="snippet">    right_wheel=wheel_distance(diameter,encoder,encoder_time,right,movement_time)</p>
			<p class="snippet">#Second step: Wheel's central axis completed distance</p>
			<p class="snippet">    wheel_axis=(left_wheel+right_wheel)/2</p>
			<p class="snippet">#Third step: Robot's rotation angle</p>
			<p class="snippet">    angle=(right_wheel-left_wheel)/base</p>
			<p class="snippet">#Final step: Final position calculus</p>
			<p class="snippet">    final_pos=final_position(initial_pos,wheel_axis,angle)</p>
			<p class="snippet">    </p>
			<p class="snippet">    returnfinal_pos</p>
			<p class="snippet">position(10,80,76,5,600,900,(0,0,0),5)</p>
			<h4>Note:</h4>
			<p class="callout">For further observations, you can change the wheels' diameter to 15 cm and check the difference in the output. Similarly, you can change other input values and check the difference in the output.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor215"/>Chapter 2: Introduction to Computer Vision</h2>
			<h3 id="_idParaDest-168"><a id="_idTextAnchor216"/>Activity 2: Classify 10 Types of Clothes from the Fashion-MNIST Data</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li>Open up your Google Colab interface.</li>
				<li>Create a folder for the book, download the <strong class="inline">Dataset</strong> folder from GitHub, and upload it into the folder.</li>
				<li>Import the drive and mount it as follows:<p class="snippet">from google.colab import drive</p><p class="snippet">drive.mount('/content/drive')</p><p>Once you have mounted your drive for the first time, you will have to enter the authorization code mentioned by clicking on the URL given by Google and pressing the <strong class="bold">Enter</strong> key on your keyboard:</p><div id="_idContainer249" class="IMG---Figure"><img src="image/C13550_05_08.jpg" alt="Figure 2.39: Image displaying the Google Colab authorization step"/></div><h6>Figure 2.38: Image displaying the Google Colab authorization step</h6></li>
				<li>Now that you have mounted the drive, you need to set the path of the directory:<p class="snippet">cd /content/drive/My Drive/C13550/Lesson02/Activity02/</p></li>
				<li>Load the dataset and show five samples:<p class="snippet">from keras.datasets import fashion_mnist</p><p class="snippet">(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()</p><p>The output is as follows:</p><div id="_idContainer250" class="IMG---Figure"><img src="image/C1355_02_40.jpg" alt="Figure 2.40: Loading datasets with five samples"/></div><h6>Figure 2.39: Loading datasets with five samples</h6><p class="snippet">import random</p><p class="snippet">from sklearn import metrics</p><p class="snippet">from sklearn.utils import shuffle</p><p class="snippet">random.seed(42)</p><p class="snippet">from matplotlib import pyplot as plt</p><p class="snippet">for idx in range(5):</p><p class="snippet">    rnd_index = random.randint(0, 59999)</p><p class="snippet">    plt.subplot(1,5,idx+1),plt.imshow(x_train[idx],'gray')</p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><div id="_idContainer251" class="IMG---Figure"><img src="image/C1355_02_41.jpg" alt=""/></div><h6>Figure 2.40: Samples of images from the Fashion-MNIST dataset</h6></li>
				<li>Preprocess the data:<p class="snippet">import numpy as np</p><p class="snippet">from keras import utils as np_utils</p><p class="snippet">x_train = (x_train.astype(np.float32))/255.0</p><p class="snippet">x_test = (x_test.astype(np.float32))/255.0</p><p class="snippet">x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)</p><p class="snippet">x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)</p><p class="snippet">y_train = np_utils.to_categorical(y_train, 10)</p><p class="snippet">y_test = np_utils.to_categorical(y_test, 10)</p><p class="snippet">input_shape = x_train.shape[1:]</p></li>
				<li>Build the architecture of the neural network using <strong class="inline">Dense</strong> layers:<p class="snippet">from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau</p><p class="snippet">from keras.layers import Input, Dense, Dropout, Flatten</p><p class="snippet">from keras.preprocessing.image import ImageDataGenerator</p><p class="snippet">from keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization</p><p class="snippet">from keras.models import Sequential, Model</p><p class="snippet">from keras.optimizers import Adam, Adadelta</p><p class="snippet">def DenseNN(inputh_shape):</p><p class="snippet">    model = Sequential()</p><p class="snippet">    model.add(Dense(128, input_shape=input_shape))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    model.add(Dense(128))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    model.add(Dense(64))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    model.add(Flatten())</p><p class="snippet">    model.add(Dense(64))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    model.add(Dense(10, activation="softmax"))</p><p class="snippet">    return model</p><p class="snippet">model = DenseNN(input_shape)</p><h4>Note:</h4><p class="callout">The entire code file for this activity can be found on GitHub in the Lesson02 | Activity02 folder.</p></li>
				<li>Compile and train the model:<p class="snippet">optimizer = Adadelta()</p><p class="snippet">model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])</p><p class="snippet">ckpt = ModelCheckpoint('model.h5', save_best_only=True,monitor='val_loss', mode='min', save_weights_only=False)</p><p class="snippet">model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test), callbacks=[ckpt])</p><p>The accuracy obtained is <strong class="bold">88.72%</strong>. This problem is harder to solve, so that's why we have achieved less accuracy than in the last exercise.</p></li>
				<li>Make the predictions:<p class="snippet">import cv2</p><p class="snippet">images = ['ankle-boot.jpg', 'bag.jpg', 'trousers.jpg', 't-shirt.jpg']</p><p class="snippet">for number in range(len(images)):</p><p class="snippet">    imgLoaded = cv2.imread('Dataset/testing/%s'%(images[number]),0)</p><p class="snippet">    img = cv2.resize(imgLoaded, (28, 28))</p><p class="snippet">    img = np.invert(img)</p><p class="snippet">cv2.imwrite('test.jpg',img)</p><p class="snippet">    img = (img.astype(np.float32))/255.0</p><p class="snippet">    img = img.reshape(1, 28, 28, 1)</p><p class="snippet">    plt.subplot(1,5,number+1),plt.imshow(imgLoaded,'gray')</p><p class="snippet">    plt.title(np.argmax(model.predict(img)[0]))</p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><p>Output will look like this:</p></li>
			</ol>
			<div>
				<div id="_idContainer252" class="IMG---Figure">
					<img src="image/C1355_02_42.jpg" alt="Figure 2.42: Prediction for clothes using Neural Networks"/>
				</div>
			</div>
			<h6>Figure 2.41: Prediction for clothes using Neural Networks</h6>
			<p>It has classified the bag and the t-shirt correctly, but it has failed to classify the boots and the trousers. These samples are very different from the ones that it was trained for.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor217"/>Chapter 3: Fundamentals of Natural Language Processing</h2>
			<h3 id="_idParaDest-170"><a id="_idTextAnchor218"/>Activity 3: Process a Corpus</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Import the <strong class="inline">sklearn</strong> <strong class="inline">TfidfVectorizer</strong> and <strong class="inline">TruncatedSVD</strong> methods:<p class="snippet">from sklearn.feature_extraction.text import TfidfVectorizer</p><p class="snippet">from sklearn.decomposition import TruncatedSVD</p></li>
				<li>Load the corpus:<p class="snippet">docs = []</p><p class="snippet">ndocs = ["doc1", "doc2", "doc3"]</p><p class="snippet">for n in ndocs:</p><p class="snippet">    aux = open("dataset/"+ n +".txt", "r", encoding="utf8")</p><p class="snippet">    docs.append(aux.read())</p></li>
				<li>With <strong class="inline">spaCy</strong>, let's add some new stop words, tokenize the corpus, and remove the stop words. The new corpus without these words will be stored in a new variable:<p class="snippet">import spacy</p><p class="snippet">import en_core_web_sm</p><p class="snippet">from spacy.lang.en.stop_words import STOP_WORDS</p><p class="snippet">nlp = en_core_web_sm.load()</p><p class="snippet">nlp.vocab["\n\n"].is_stop = True</p><p class="snippet">nlp.vocab["\n"].is_stop = True</p><p class="snippet">nlp.vocab["the"].is_stop = True</p><p class="snippet">nlp.vocab["The"].is_stop = True</p><p class="snippet">newD = []</p><p class="snippet">for d, i in zip(docs, range(len(docs))):</p><p class="snippet">    doc = nlp(d)</p><p class="snippet">    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]</p><p class="snippet">    newD.append(' '.join(tokens))</p></li>
				<li>Create the TF-IDF matrix. I'm going to add some parameters to improve the results:<p class="snippet">vectorizer = TfidfVectorizer(use_idf=True, </p><p class="snippet">                            ngram_range=(1,2), </p><p class="snippet">                            smooth_idf=True,</p><p class="snippet">                            max_df=0.5)</p><p class="snippet">X = vectorizer.fit_transform(newD)</p></li>
				<li>Perform the LSA algorithm:<p class="snippet">lsa = TruncatedSVD(n_components=100,algorithm='randomized',n_iter=10,random_state=0)</p><p class="snippet">lsa.fit_transform(X)</p></li>
				<li>With pandas, we are shown a sorted <strong class="inline">DataFrame</strong> with the weights of the terms of each concept and the name of each feature:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">dic1 = {"Terms": terms, "Components": lsa.components_[0]}</p><p class="snippet">dic2 = {"Terms": terms, "Components": lsa.components_[1]}</p><p class="snippet">dic3 = {"Terms": terms, "Components": lsa.components_[2]}</p><p class="snippet">f1 = pd.DataFrame(dic1)</p><p class="snippet">f2 = pd.DataFrame(dic2)</p><p class="snippet">f3 = pd.DataFrame(dic3)</p><p class="snippet">f1.sort_values(by=['Components'], ascending=False)</p><p class="snippet">f2.sort_values(by=['Components'], ascending=False)</p><p class="snippet">f3.sort_values(by=['Components'], ascending=False)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer253" class="IMG---Figure">
					<img src="image/C1355_03_25.jpg" alt="Figure 3.25: Output example of the most relevant words in a concept (f1)"/>
				</div>
			</div>
			<h6>Figure 3.26: Output example of the most relevant words in a concept (f1)</h6>
			<h4>Note:</h4>
			<p class="callout">Do not worry if the keywords are not the same as yours, if the keywords represent a concept, it is a valid result. </p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor219"/>Chapter 4: Neural Networks with NLP</h2>
			<h3 id="_idParaDest-172"><a id="_idTextAnchor220"/>Activity 4: Predict the Next Character in a Sequence</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Import the libraries we need to solve the activity:<p class="snippet">import tensorflow as tf</p><p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.layers import LSTM, Dense, Activation, LeakyReLU</p><p class="snippet">import numpy as np</p></li>
				<li>Define the sequence of characters and multiply it by 100:<p class="snippet">char_seq = 'qwertyuiopasdfghjklñzxcvbnm' * 100</p><p class="snippet">char_seq = list(char_seq)</p></li>
				<li>Create a <strong class="inline">char2id</strong> dictionary to relate every character with an integer:<p class="snippet">char2id = dict([(char, idx) for idx, char in enumerate(set(char_seq))])</p></li>
				<li>Divide the sentence of characters into time series. The maximum length of time series will be five, so we will have vectors of five characters. Also, we are going to create the upcoming vector. The y_labels variable is the size of our vocabulary. We will use this variable later:<p class="snippet">maxlen = 5</p><p class="snippet">sequences = []</p><p class="snippet">next_char = []</p><p class="snippet"> </p><p class="snippet">for i in range(0,len(char_seq)-maxlen):</p><p class="snippet">    sequences.append(char_seq[i:i+maxlen])</p><p class="snippet">    next_char.append(char_seq[i+maxlen])</p><p class="snippet">    </p><p class="snippet">y_labels = len(char2id)</p><p class="snippet">print("5 first sequences: {}".format(sequences[:5]))</p><p class="snippet">print("5 first next characters: {}".format(next_char[:5]))</p><p class="snippet">print("Total sequences: {}".format(len(sequences)))</p><p class="snippet">print("Total output labels: {}".format(y_labels))</p></li>
				<li>So far, we have the sequences variable, which is an array of arrays, with the time series of characters. char is an array with the upcoming character. Now, we need to encode these vectors, so let's define a method to encode an array of characters using the information of char2id:<p class="snippet">def one_hot_encoder(seq, ids):</p><p class="snippet">    encoded_seq = np.zeros([len(seq),len(ids)])</p><p class="snippet">    for i,s in enumerate(seq):</p><p class="snippet">        encoded_seq[i][ids[s]] = 1</p><p class="snippet">    return encoded_seq</p></li>
				<li>Encode the variables into one-hot vectors. The shape of this is x = (2695,5,27) and y = (2695,27):<p class="snippet">x = np.array([one_hot_encoder(item, char2id) for item in sequences])</p><p class="snippet">y = np.array(one_hot_encoder(next_char, char2id))</p><p class="snippet">x = x.astype(np.int32)</p><p class="snippet">y = y.astype(np.int32)</p><p class="snippet"> </p><p class="snippet">print("Shape of x: {}".format(x.shape))</p><p class="snippet">print("Shape of y: {}".format(y.shape))</p><div id="_idContainer254" class="IMG---Figure"><img src="image/C1355_04_35.jpg" alt="Figure 4.35: Variables encoded into OneHotVectors"/></div><h6>Figure 4.35: Variables encoded into OneHotVectors</h6></li>
				<li>Split the data into train and test sets. To do this, we are going to use the <strong class="inline">train_test_split</strong> method of sklearn:<p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet"> </p><p class="snippet">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)</p><p class="snippet">print('x_train shape: {}'.format(x_train.shape)) </p><p class="snippet">print('y_train shape: {}'.format(y_train.shape))  </p><p class="snippet">print('x_test shape: {}'.format(x_test.shape)) </p><p class="snippet">print('y_test shape: {}'.format(y_test.shape))</p><div id="_idContainer255" class="IMG---Figure"><img src="image/C1355_04_36.jpg" alt="Figure 4.36: Splitting the data into train and test sets"/></div><h6>  </h6><h6>Figure 4.36: Splitting the data into train and test sets</h6></li>
				<li>With the data ready to be inserted in the neural network, create a Sequential model with two layers:<p>First layer: LSTM with eight neurons (the activation is tanh). input_shape is the maximum length of the sequences and the size of the vocabulary. So, because of the shape of our data, we do not need to reshape anything.</p><p>Second layer: Dense with 27 neurons. This is how we successfully complete the activity. Using a LeakyRelu activation will give you a good score. But why? Our output has many zeroes, so the network could fail and just return a vector of zeroes. Using LeakyRelu prevents this problem:</p><p class="snippet">model = Sequential()</p><p class="snippet">model.add(LSTM(8,input_shape=(maxlen,y_labels)))</p><p class="snippet">model.add(Dense(y_labels))</p><p class="snippet">model.add(LeakyReLU(alpha=.01)) </p><p class="snippet"> </p><p class="snippet">model.compile(loss='mse', optimizer='rmsprop')</p></li>
				<li>Train the model. The batch_size we use is 32, and we have 25 epochs:<p class="snippet">history = model.fit(x_train, y_train, batch_size=32, epochs=25, verbose=1)</p><div id="_idContainer256" class="IMG---Figure"><img src="image/C1355_04_37.jpg" alt="Figure 4.37: Training with a batch_size of 32 and 25 epochs"/></div><h6>Figure 4.37: Training with a batch_size of 32 and 25 epochs</h6></li>
				<li>Compute the error of your model. <p class="snippet">print('MSE: {:.5f}'.format(model.evaluate(x_test, y_test)))</p><div id="_idContainer257" class="IMG---Figure"><img src="image/C1355_04_38.jpg" alt="Figure 4.38: Error shown in the model"/></div><h6>Figure 4.38: Error shown in the model</h6></li>
				<li>Predict the test data and see the average percentage of hits. With this model, you will obtain an average of more than 90%:<p class="snippet">prediction = model.predict(x_test)</p><p class="snippet"> </p><p class="snippet">errors = 0</p><p class="snippet">for pr, res in zip(prediction, y_test):</p><p class="snippet">    if not np.array_equal(np.around(pr),res):</p><p class="snippet">        errors+=1</p><p class="snippet"> </p><p class="snippet">print("Errors: {}".format(errors))</p><p class="snippet">print("Hits: {}".format(len(prediction) - errors))</p><p class="snippet">print("Hit average: {}".format((len(prediction) - errors)/len(prediction)))</p><div id="_idContainer258" class="IMG---Figure"><img src="image/C1355_04_39.jpg" alt="Figure 4.39: Predicting the test data"/></div><h6>Figure 4.39: Predicting the test data</h6></li>
				<li>To end this activity, we need to create a function that accepts a sequence of characters and returns the next predicted value. To decode the prediction of the model, we first code a decode method. This method just search in the prediction the higher value and take the key character in the char2id dictionary.<p class="snippet">def decode(vec):</p><p class="snippet">    val = np.argmax(vec)</p><p class="snippet">    return list(char2id.keys())[list(char2id.values()).index(val)]</p></li>
				<li>Create a method to predict the next character in a given sentence:<p class="snippet">def pred_seq(seq):</p><p class="snippet">    seq = list(seq)</p><p class="snippet">    x = one_hot_encoder(seq,char2id)</p><p class="snippet">    x = np.expand_dims(x, axis=0)</p><p class="snippet">    prediction = model.predict(x, verbose=0)</p><p class="snippet">    return decode(list(prediction[0]))</p></li>
				<li>Finally, introduce the sequence 'tyuio' to predict the upcoming character. It will return 'p':<p class="snippet">pred_seq('tyuio')</p></li>
			</ol>
			<div>
				<div id="_idContainer259" class="IMG---Figure">
					<img src="image/C1355_04_40.jpg" alt="Figure 4.40: Final output with the predicted sequence"/>
				</div>
			</div>
			<h6>Figure 4.40: Final output with the predicted sequence</h6>
			<p>Congratulations! You have finished the activity. You can predict a value outputting a temporal sequence. This is also very important in finances, that is, when predicting future prices or stock movements.</p>
			<p>You can change the data and predict what you want. If you add a linguistic corpus, you will generate text from your own RNN language model. So, our future conversational agent could generate poems or news text. </p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor221"/>Chapter 5: Convolutional Neural Networks for Computer Vision</h2>
			<h3 id="_idParaDest-174"><a id="_idTextAnchor222"/>Activity 5: Making Use of Data Augmentation to Classify correctly Images of Flowers</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Open your Google Colab interface.<h4>Note:</h4><p class="callout">You will need to mount your drive using the Dataset folder, and accordingly set the path to continue ahead.</p><p class="snippet">import numpyasnp</p><p class="snippet">classes=['daisy','dandelion','rose','sunflower','tulip']</p><p class="snippet">X=np.load("Dataset/flowers/%s_x.npy"%(classes[0]))</p><p class="snippet">y=np.load("Dataset/flowers/%s_y.npy"%(classes[0]))</p><p class="snippet">print(X.shape)</p><p class="snippet">forflowerinclasses[1:]:</p><p class="snippet">    X_aux=np.load("Dataset/flowers/%s_x.npy"%(flower))</p><p class="snippet">    y_aux=np.load("Dataset/flowers/%s_y.npy"%(flower))</p><p class="snippet">    print(X_aux.shape)</p><p class="snippet">    X=np.concatenate((X,X_aux),axis=0)</p><p class="snippet">    y=np.concatenate((y,y_aux),axis=0)</p><p class="snippet">    </p><p class="snippet">print(X.shape)</p><p class="snippet">print(y.shape)</p></li>
				<li>To output some samples from the dataset:<p class="snippet">import random </p><p class="snippet">random.seed(42) </p><p class="snippet">from matplotlib import pyplot as plt</p><p class="snippet">import cv2</p><p class="snippet">for idx in range(5): </p><p class="snippet">    rnd_index = random.randint(0, 4000) </p><p class="snippet">    plt.subplot(1,5,idx+1),plt.imshow(cv2.cvtColor(X[rnd_index],cv2.COLOR_BGR2RGB)) </p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">    plt.savefig("flowers_samples.jpg", bbox_inches='tight')</p><p class="snippet">plt.show() </p><p>The output is as follows:</p><div id="_idContainer260" class="IMG---Figure"><img src="image/C1355_05_19.jpg" alt="Figure 5.19: Samples from the dataset"/></div><h6>Figure 5.23: Samples from the dataset</h6></li>
				<li>Now, we will normalize and perform one-hot encoding:<p class="snippet">from keras import utils as np_utils</p><p class="snippet">X = (X.astype(np.float32))/255.0 </p><p class="snippet">y = np_utils.to_categorical(y, len(classes))</p><p class="snippet">print(X.shape)</p><p class="snippet">print(y.shape)</p></li>
				<li>Splitting the training and testing set:<p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet">x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</p><p class="snippet">input_shape = x_train.shape[1:]</p><p class="snippet">print(x_train.shape)</p><p class="snippet">print(y_train.shape)</p><p class="snippet">print(x_test.shape)</p><p class="snippet">print(y_test.shape)</p><p class="snippet">print(input_shape)</p></li>
				<li>Import libraries and build the CNN:<p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.callbacks import ModelCheckpoint</p><p class="snippet">from keras.layers import Input, Dense, Dropout, Flatten</p><p class="snippet">from keras.layers import Conv2D, Activation, BatchNormalization</p><p class="snippet">def CNN(input_shape):</p><p class="snippet">    model = Sequential()</p><p class="snippet">    </p><p class="snippet">    model.add(Conv2D(32, kernel_size=(5, 5), padding='same',  strides=(2,2), input_shape=input_shape))</p><p class="snippet">    model.add(Activation('relu')) </p><p class="snippet">    model.add(BatchNormalization()) </p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    model.add(Conv2D(64, kernel_size=(3, 3), padding='same', strides=(2,2))) </p><p class="snippet">    model.add(Activation('relu')) </p><p class="snippet">    model.add(BatchNormalization()) </p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    model.add(Conv2D(128, kernel_size=(3, 3), padding='same', strides=(2,2))) </p><p class="snippet">    model.add(Activation('relu')) </p><p class="snippet">    model.add(BatchNormalization()) </p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    </p><p class="snippet">    model.add(Conv2D(256, kernel_size=(3, 3), padding='same', strides=(2,2))) </p><p class="snippet">    model.add(Activation('relu')) </p><p class="snippet">    model.add(BatchNormalization()) </p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    </p><p class="snippet">    model.add(Flatten())</p><p class="snippet">    model.add(Dense(512))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Dropout(0.5))</p><p class="snippet">    model.add(Dense(5, activation = "softmax"))</p><p class="snippet">    return model</p></li>
				<li>Declare ImageDataGenerator:<p class="snippet">from keras.preprocessing.image import ImageDataGenerator</p><p class="snippet">datagen = ImageDataGenerator(</p><p class="snippet">        rotation_range=10,</p><p class="snippet">        zoom_range = 0.2,</p><p class="snippet">        width_shift_range=0.2,</p><p class="snippet">        height_shift_range=0.2,</p><p class="snippet">        shear_range=0.1,</p><p class="snippet">        horizontal_flip=True</p><p class="snippet">        )</p></li>
				<li>We will now train the model:<p class="snippet">datagen.fit(x_train)</p><p class="snippet">model = CNN(input_shape)</p><p class="snippet">model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])</p><p class="snippet">ckpt = ModelCheckpoint('Models/model_flowers.h5', save_best_only=True,monitor='val_loss', mode='min', save_weights_only=False) </p><p class="snippet">//{…}##the detailed code can be found on Github##</p><p class="snippet">model.fit_generator(datagen.flow(x_train, y_train,</p><p class="snippet">                            batch_size=32),</p><p class="snippet">                    epochs=200,</p><p class="snippet">                    validation_data=(x_test, y_test),</p><p class="snippet">                    callbacks=[ckpt],</p><p class="snippet">                    steps_per_epoch=len(x_train) // 32,</p><p class="snippet">                    workers=4)</p></li>
				<li>After which, we will evaluate the model:<p class="snippet">from sklearn import metrics</p><p class="snippet">model.load_weights('Models/model_flowers.h5')</p><p class="snippet">y_pred = model.predict(x_test, batch_size=32, verbose=0)</p><p class="snippet">y_pred = np.argmax(y_pred, axis=1)</p><p class="snippet">y_test_aux = y_test.copy()</p><p class="snippet">y_test_pred = list()</p><p class="snippet">for i in y_test_aux:</p><p class="snippet">    y_test_pred.append(np.argmax(i))</p><p class="snippet">//{…}</p><p class="snippet">##the detailed code can be found on Github##</p><p class="snippet">print (y_pred)</p><p class="snippet"># Evaluate the prediction</p><p class="snippet">accuracy = metrics.accuracy_score(y_test_pred, y_pred)</p><p class="snippet">print('Acc: %.4f' % accuracy)</p></li>
				<li>The accuracy achieved is <strong class="bold">91.68%</strong>.</li>
				<li>Try the model with unseen data:<p class="snippet">classes = ['daisy','dandelion','rose','sunflower','tulip']</p><p class="snippet">images = ['sunflower.jpg','daisy.jpg','rose.jpg','dandelion.jpg','tulip .jpg']</p><p class="snippet">model.load_weights('Models/model_flowers.h5')</p><p class="snippet">for number in range(len(images)):</p><p class="snippet">    imgLoaded = cv2.imread('Dataset/testing/%s'%(images[number])) </p><p class="snippet">    img = cv2.resize(imgLoaded, (150, 150)) </p><p class="snippet">    img = (img.astype(np.float32))/255.0 </p><p class="snippet">    img = img.reshape(1, 150, 150, 3)</p><p class="snippet">    plt.subplot(1,5,number+1),plt.imshow(cv2.cvtColor(imgLoaded,cv2.COLOR_BGR2RGB)) </p><p class="snippet">    plt.title(np.argmax(model.predict(img)[0])) </p><p class="snippet">    plt.xticks([]),plt.yticks([]) </p><p class="snippet">plt.show()</p><p>Output will look like this:</p></li>
			</ol>
			<div>
				<div id="_idContainer261" class="IMG---Figure">
					<img src="image/C1355_05_20.jpg" alt="Figure 5.20: Prediction of roses result from Activity 1"/>
				</div>
			</div>
			<h6>Figure 5.24: Prediction of roses result from Activity05</h6>
			<h4>Note:</h4>
			<p class="callout">The detailed code for this activity can be found on GitHub - <a href="https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Activity05/Activity05.ipynb">https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Activity05/Activity05.ipynb</a></p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor223"/>Chapter 6: Robot Operating System (ROS)</h2>
			<h3 id="_idParaDest-176"><a id="_idTextAnchor224"/>Activity 6: Simulators and Sensor</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">We start by creating the packages and files:<p class="snippet">cd ~/catkin_ws/src</p><p class="snippet">catkin_create_pkg activity1 rospy sensor_msgs</p><p class="snippet">cd  activity1</p><p class="snippet">mkdir scripts</p><p class="snippet">cd scripts</p><p class="snippet">touch observer.py</p><p class="snippet">touch movement.py</p><p class="snippet">chmod +x observer.py</p><p class="snippet">chmod +x movement.py</p></li>
				<li>This is the implementation of the image obtainer node:<h4>Note:</h4><p class="callout">Add the aforementioned code to the <strong class="inline">observer.py</strong> file.</p><p class="snippet">#!/usr/bin/env python</p><p class="snippet">import rospy</p><p class="snippet">from sensor_msgs.msg import Image</p><p class="snippet">import cv2</p><p class="snippet">from cv_bridge import CvBridge</p><p class="snippet">class Observer:</p><p class="snippet">    bridge = CvBridge()</p><p class="snippet">    counter = 0</p><p class="snippet">    def callback(self, data):</p><p class="snippet">        if self.counter == 20:</p><p class="snippet">            cv_image = self.bridge.imgmsg_to_cv2(data, "bgr8")</p><p class="snippet">            cv2.imshow('Image',cv_image)</p><p class="snippet">            cv2.waitKey(1000)</p><p class="snippet">            cv2.destroyAllWindows()</p><p class="snippet">            self.counter = 0</p><p class="snippet">        else:</p><p class="snippet">            self.counter += 1</p><p class="snippet">    def observe(self):</p><p class="snippet">        rospy.Subscriber('/camera/rgb/image_raw', Image, self.callback)</p><p class="snippet">        rospy.init_node('observer', anonymous=True)</p><p class="snippet">        rospy.spin()</p><p class="snippet">if __name__ == '__main__':</p><p class="snippet">    obs = Observer()</p><p class="snippet">    obs.observe()</p><p>As you can see, this node is very similar to the one in <em class="italics">Exercise 21</em>, <em class="italics">Publishers and Subscribers</em>. The only differences are: </p></li>
				<li>A counter is used for showing only one image of twenty received.<p>We enter <strong class="inline">1000 (ms)</strong> as the <strong class="inline">Key()</strong> parameter so that each image is shown for a second.</p><p>This is the implementation of the movement node:</p><p class="snippet">#!/usr/bin/env python</p><p class="snippet">import rospy</p><p class="snippet">from geometry_msgs.msg import Twist</p><p class="snippet">def move():</p><p class="snippet">    pub = rospy.Publisher('/mobile_base/commands/velocity', Twist, queue_size=1)</p><p class="snippet">    rospy.init_node('movement', anonymous=True)</p><p class="snippet">    move = Twist()</p><p class="snippet">    move.angular.z = 0.5</p><p class="snippet">    rate = rospy.Rate(10)</p><p class="snippet">    while not rospy.is_shutdown():</p><p class="snippet">        pub.publish(move)</p><p class="snippet">        rate.sleep()</p><p class="snippet">if __name__ == '__main__':</p><p class="snippet">    try:</p><p class="snippet">        move()</p><p class="snippet">    except rospy.ROSInterruptException:</p><p class="snippet">        pass</p></li>
				<li>To execute the file, we will execute the code mentioned here.<h4>Note:</h4><p class="callout">Add this code to observer the <strong class="inline">.py</strong> file.</p><p class="snippet">cd ~/catkin_ws</p><p class="snippet">source devel/setup.bash</p><p class="snippet">roscore</p><p class="snippet">roslaunch turtlebot_gazebo turtlebot_world.launch</p><p class="snippet">rosrun activity1 observer.py</p><p class="snippet">rosrun activity1 movement.py</p></li>
				<li>Run both nodes and check the system functioning. You should see the robot turning on itself while images of what it sees are shown. This is a sequence of the execution:<p>The output will look like this:</p></li>
			</ol>
			<div>
				<div id="_idContainer262" class="IMG---Figure">
					<img src="image/C1355_06_09.jpg" alt="Figure 6.9: The first sequence of the execution of activity nodes"/>
				</div>
			</div>
			<h6>Figure 6.10: The first sequence of the execution of activity nodes</h6>
			<div>
				<div id="_idContainer263" class="IMG---Figure">
					<img src="image/C1355_06_10.jpg" alt="Figure 6.10: The second sequence of the execution of activity nodes"/>
				</div>
			</div>
			<h6>Figure 6.11: The second sequence of the execution of activity nodes</h6>
			<div>
				<div id="_idContainer264" class="IMG---Figure">
					<img src="image/C1355_06_11.jpg" alt="Figure 6.11: The third sequence of the execution of activity nodes"/>
				</div>
			</div>
			<h6>Figure 6.12: The third sequence of the execution of activity nodes</h6>
			<h4>Note:</h4>
			<p class="callout">The output will look similar but not exactly look as the one mentioned in figures 6.10, 6.11, and 6.12.</p>
			<p>Congratulations! You have completed the activity and at the end, you will have an output which is like figures 6.8, 6.9, and 6.10. By completing this activity successfully, you have been able to implement and work with nodes that let you subscribe to a camera which will show images in the virtual environment. You also learned to rotate a robot on itself that lets you view these images.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor225"/>Chapter 7: Build a Text-Based Dialogue System (Chatbot)</h2>
			<h3 id="_idParaDest-178"><a id="_idTextAnchor226"/>Activity 7: Create a Conversational Agent to Control a Robot</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Open up your Google Colab interface.</li>
				<li>Create a folder for the book and download the <strong class="inline">utils</strong>, <strong class="inline">responses</strong>, and <strong class="inline">training</strong> folder from Github and upload it in the folder.</li>
				<li>Import drive and mount it as follows:<p class="snippet">from google.colab import drive</p><p class="snippet">drive.mount('/content/drive')</p><h4>Note</h4><p class="callout">Every time you use a new collaborator, mount the drive to the desired folder.</p></li>
				<li>Once you have mounted your drive for the first time, you will need to enter the authorization code mentioned by clicking on the URL mentioned by Google and press the Enter key on your keyboard:<div id="_idContainer265" class="IMG---Figure"><img src="image/C13550_05_08.jpg" alt="Figure 7.24: Image displaying the Google Colab authorization step"/></div><h6>Figure 7.28: Image displaying the Google Colab authorization step</h6></li>
				<li>Now that you have mounted the drive, you need to set the path of the directory.<p class="snippet">cd /content/drive/My Drive/C13550/Lesson07/Activity01</p><h4>Note:</h4><p class="callout">The path mentioned in step 5 may change as per your folder setup on Google Drive. The path will always begin with cd /content/drive/My Drive/</p></li>
				<li>Import the chatbot_intro file:<p class="snippet">from chatbot_intro import *</p></li>
				<li>Define the GloVe model:<p class="snippet">filename = '../utils/glove.6B.50d.txt.word2vec'</p><p class="snippet">model = KeyedVectors.load_word2vec_format(filename, binary=False)</p></li>
				<li>List the responses and training sentences files:<p class="snippet">intent_route = 'training/'</p><p class="snippet">response_route = 'responses/'</p><p class="snippet">intents = listdir(intent_route)</p><p class="snippet">responses = listdir(response_route)</p><p class="snippet">print("Documents: ", intents)</p><div id="_idContainer266" class="IMG---Figure"><img src="image/C1355_07_25.jpg" alt="Figure 7.25: A list of intent documents"/></div><h6>Figure 7.29: A list of intent documents</h6></li>
				<li>Create document vectors:<p class="snippet">doc_vectors = np.empty((0,50), dtype='f')</p><p class="snippet">for i in intents:</p><p class="snippet">    with open(intent_route + i) as f:</p><p class="snippet">        sentences = f.readlines()</p><p class="snippet">    sentences = [x.strip() for x in sentences]</p><p class="snippet">    sentences = pre_processing(sentences)</p><p class="snippet">doc_vectors= np.append(doc_vectors,doc_vector(sentences,model),axis=0)</p><p class="snippet">print("Shape of doc_vectors:",doc_vectors.shape)</p><p class="snippet">print(" Vector representation of backward.txt:\n",doc_vectors)</p><div id="_idContainer267" class="IMG---Figure"><img src="image/C1355_07_26.jpg" alt="7.26: Shape of doc_vectors"/></div><h6>7.30: Shape of doc_vectors</h6></li>
				<li>Predict the intent:<p class="snippet">user_sentence = "Look to the right"</p><p class="snippet">user_sentence = pre_processing([user_sentence])</p><p class="snippet">user_vector = doc_vector(user_sentence,model).reshape(100,)</p><p class="snippet">intent = intents[select_intent(user_vector, doc_vectors)]</p><p class="snippet">intent</p></li>
			</ol>
			<div>
				<div id="_idContainer268" class="IMG---Figure">
					<img src="image/C1355_07_27.jpg" alt="7.27: Predicted intent"/>
				</div>
			</div>
			<h6>7.31: Predicted intent</h6>
			<p>Congratulations! You finished the activity. You can add more intents if you want to and train the GloVe model to achieve better results. By creating a function with all the code, you programmed and developing a movement node in ROS, you can order your robot to make movements and turn around.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor227"/>Chapter 8: Object Recognition to Guide a Robot Using CNNs</h2>
			<h3 id="_idParaDest-180"><a id="_idTextAnchor228"/>Activity 8: Multiple Object Detection and Recognition in Video</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Mount the drive:<p class="snippet">from google.colab import drive</p><p class="snippet">drive.mount('/content/drive')</p><p class="snippet">cd /content/drive/My Drive/C13550/Lesson08/</p></li>
				<li>Install the libraries:<p class="snippet">pip3 install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl</p></li>
				<li>Import the libraries:<p class="snippet">from imageai.Detection import VideoObjectDetection</p><p class="snippet">from matplotlib import pyplot as plt</p></li>
				<li>Declare the model:<p class="snippet">video_detector = VideoObjectDetection()</p><p class="snippet">video_detector.setModelTypeAsYOLOv3()</p><p class="snippet">video_detector.setModelPath("Models/yolo.h5")</p><p class="snippet">video_detector.loadModel()</p></li>
				<li>Declare the callback method:<p class="snippet">color_index = {'bus': 'red', 'handbag': 'steelblue', 'giraffe': 'orange', 'spoon': 'gray', 'cup': 'yellow', 'chair': 'green', 'elephant': 'pink', 'truck': 'indigo', 'motorcycle': 'azure', 'refrigerator': 'gold', 'keyboard': 'violet', 'cow': 'magenta', 'mouse': 'crimson', 'sports ball': 'raspberry', 'horse': 'maroon', 'cat': 'orchid', 'boat': 'slateblue', 'hot dog': 'navy', 'apple': 'cobalt', 'parking meter': 'aliceblue', 'sandwich': 'skyblue', 'skis': 'deepskyblue', 'microwave': 'peacock', 'knife': 'cadetblue', 'baseball bat': 'cyan', 'oven': 'lightcyan', 'carrot': 'coldgrey', 'scissors': 'seagreen', 'sheep': 'deepgreen', 'toothbrush': 'cobaltgreen', 'fire hydrant': 'limegreen', 'remote': 'forestgreen', 'bicycle': 'olivedrab', 'toilet': 'ivory', 'tv': 'khaki', 'skateboard': 'palegoldenrod', 'train': 'cornsilk', 'zebra': 'wheat', 'tie': 'burlywood', 'orange': 'melon', 'bird': 'bisque', 'dining table': 'chocolate', 'hair drier': 'sandybrown', 'cell phone': 'sienna', 'sink': 'coral', 'bench': 'salmon', 'bottle': 'brown', 'car': 'silver', 'bowl': 'maroon', 'tennis racket': 'palevilotered', 'airplane': 'lavenderblush', 'pizza': 'hotpink', 'umbrella': 'deeppink', 'bear': 'plum', 'fork': 'purple', 'laptop': 'indigo', 'vase': 'mediumpurple', 'baseball glove': 'slateblue', 'traffic light': 'mediumblue', 'bed': 'navy', 'broccoli': 'royalblue', 'backpack': 'slategray', 'snowboard': 'skyblue', 'kite': 'cadetblue', 'teddy bear': 'peacock', 'clock': 'lightcyan', 'wine glass': 'teal', 'frisbee': 'aquamarine', 'donut': 'mincream', 'suitcase': 'seagreen', 'dog': 'springgreen', 'banana': 'emeraldgreen', 'person': 'honeydew', 'surfboard': 'palegreen', 'cake': 'sapgreen', 'book': 'lawngreen', 'potted plant': 'greenyellow', 'toaster': 'ivory', 'stop sign': 'beige', 'couch': 'khaki'}</p><p class="snippet">def forFrame(frame_number, output_array, output_count, returned_frame):</p><p class="snippet">    plt.clf()</p><p class="snippet">    this_colors = []</p><p class="snippet">    labels = []</p><p class="snippet">    sizes = []</p><p class="snippet">    counter = 0</p><p class="snippet">    for eachItem in output_count:</p><p class="snippet">        counter += 1</p><p class="snippet">        labels.append(eachItem + " = " + str(output_count[eachItem]))</p><p class="snippet">        sizes.append(output_count[eachItem])</p><p class="snippet">        this_colors.append(color_index[eachItem])</p><p class="snippet">    plt.subplot(1, 2, 1)</p><p class="snippet">    plt.title("Frame : " + str(frame_number))</p><p class="snippet">    plt.axis("off")</p><p class="snippet">    plt.imshow(returned_frame, interpolation="none")</p><p class="snippet">    plt.subplot(1, 2, 2)</p><p class="snippet">    plt.title("Analysis: " + str(frame_number))</p><p class="snippet">    plt.pie(sizes, labels=labels, colors=this_colors, shadow=True, startangle=140, autopct="%1.1f%%")</p><p class="snippet">    plt.pause(0.01)</p></li>
				<li>Run Matplotlib and the video detection process:<p class="snippet">plt.show()</p><p class="snippet">video_detector.detectObjectsFromVideo(input_file_path="Dataset/videos/street.mp4", output_file_path="output-video" ,  frames_per_second=20, per_frame_function=forFrame,  minimum_percentage_probability=30, return_detected_frame=True, log_progress=True)</p><p>The output will be as shown in the following frames:</p><div id="_idContainer269" class="IMG---Figure"><img src="image/C1355_08_07.jpg" alt="Figure 8.7: ImageAI video object detection output"/></div></li>
			</ol>
			<h6> Figure 8.7: ImageAI video object detection output</h6>
			<p>As you can see, the model detects objects more or less properly. Now you can see the output video in your chapter 8 root directory with all the object detections in it.</p>
			<h4>Note:</h4>
			<p class="callout">There is an additional video added in the <strong class="inline">Dataset/videos</strong> folder – <strong class="inline">park.mp4</strong>. You can use the steps just mentioned and recognize objects in this video as well.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor229"/>Chapter 9: Computer Vision for Robotics</h2>
			<h3 id="_idParaDest-182"><a id="_idTextAnchor230"/>Activity 9: A Robotic Security Guard</h3>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Create a new package in your catkin workspace to contain the integration node. Do it with this command to include the correct dependencies:<p class="snippet">cd ~/catkin_ws/</p><p class="snippet">source devel/setup.bash</p><p class="snippet">roscore</p><p class="snippet">cd src</p><p class="snippet">catkin_create_pkg activity1 rospy cv_bridge geometry_msgs image_transport sensor_msgs std_msgs</p></li>
				<li>Switch to the package folder and create a new <strong class="inline">scripts</strong> directory. Then, create the Python file and make it executable:<p class="snippet">cd activity1</p><p class="snippet">mkdir scripts</p><p class="snippet">cd scripts</p><p class="snippet">touch activity.py</p><p class="snippet">touch activity_sub.py</p><p class="snippet">chmod +x activity.py</p><p class="snippet">chmod +x activity_sub.py</p></li>
				<li>This is the implementation of the first node:<p>Libraries importation:</p><p class="snippet">#!/usr/bin/env python</p><p class="snippet">import rospy</p><p class="snippet">import cv2</p><p class="snippet">import sys</p><p class="snippet">import os</p><p class="snippet">from cv_bridge import CvBridge, CvBridgeError</p><p class="snippet">from sensor_msgs.msg import Image</p><p class="snippet">from std_msgs.msg import String</p><p class="snippet">sys.path.append(os.path.join(os.getcwd(), '/home/alvaro/Escritorio/tfg/darknet/python/'))</p><p class="snippet">import darknet as dn</p><h4>Note</h4><p class="callout">The above mentioned path may change as per the directories placed in your computer.</p><p>Class definition:</p><p class="snippet">class Activity():</p><p class="snippet">    def __init__(self):</p><p>Node, subscriber, and network initialization:</p><p class="snippet">        rospy.init_node('Activity', anonymous=True)</p><p class="snippet">        self.bridge = CvBridge()</p><p class="snippet">        self.image_sub = rospy.Subscriber("camera/rgb/image_raw", Image, self.imageCallback)</p><p class="snippet">        self.pub = rospy.Publisher('yolo_topic', String, queue_size=10)</p><p class="snippet">        self.imageToProcess = None</p><p class="snippet">        cfgPath =  "/home/alvaro/Escritorio/tfg/darknet/cfg/yolov3.cfg"</p><p class="snippet">        weightsPath = "/home/alvaro/Escritorio/tfg/darknet/yolov3.weights"</p><p class="snippet">        dataPath = "/home/alvaro/Escritorio/tfg/darknet/cfg/coco2.data"</p><p class="snippet">        self.net = dn.load_net(cfgPath, weightsPath, 0)</p><p class="snippet">        self.meta = dn.load_meta(dataPath)</p><p class="snippet">        self.fileName = 'predict.jpg'</p><p class="snippet">        self.rate = rospy.Rate(10)</p><h4>Note</h4><p class="callout">The above mentioned path may change as per the directories placed in your computer.</p><p>Function image callback. It obtains images from the robot camera:</p><p class="snippet">    def imageCallback(self, data):</p><p class="snippet">        self.imageToProcess = self.bridge.imgmsg_to_cv2(data, "bgr8")</p><p>Main function of the node:</p><p class="snippet">    def run(self): </p><p class="snippet">        print("The robot is recognizing objects")</p><p class="snippet">        while not rospy.core.is_shutdown():</p><p class="snippet">            if(self.imageToProcess is not None):</p><p class="snippet">                cv2.imwrite(self.fileName, self.imageToProcess)</p><p>Method for making predictions on images:</p><p class="snippet">                r = dn.detect(self.net, self.meta, self.fileName)</p><p class="snippet">                objects = ""</p><p class="snippet">                for obj in r:</p><p class="snippet">                    objects += obj[0] + " "</p><p>Publish the predictions:</p><p class="snippet">                self.pub.publish(objects)</p><p class="snippet">                self.rate.sleep()</p><p>Program entry:</p><p class="snippet">if __name__ == '__main__':</p><p class="snippet">    dn.set_gpu(0)</p><p class="snippet">    node = Activity()</p><p class="snippet">    try:</p><p class="snippet">        node.run()</p><p class="snippet">    except rospy.ROSInterruptException:</p><p class="snippet">        pass</p></li>
				<li>This is the implementation of the second node:<p>Libraries importation:</p><p class="snippet">#!/usr/bin/env python</p><p class="snippet">import rospy</p><p class="snippet">from std_msgs.msg import String</p><p>Class definition:</p><p class="snippet">class ActivitySub():</p><p class="snippet">    yolo_data = ""</p><p class="snippet">    </p><p class="snippet">    def __init__(self):</p><p>Node initialization and subscriber definition:</p><p class="snippet">        rospy.init_node('ThiefDetector', anonymous=True)</p><p class="snippet">        rospy.Subscriber("yolo_topic", String, self.callback)</p><p class="snippet">    </p><p>The callback function for obtaining published data:</p><p class="snippet">    def callback(self, data):</p><p class="snippet">        self.yolo_data = data</p><p class="snippet">    def run(self):</p><p class="snippet">        while True:</p><p>Start the alarm if a person is detected in the data:</p><p class="snippet">            if "person" in str(self.yolo_data):</p><p class="snippet">                print("ALERT: THIEF DETECTED")</p><p class="snippet">                break</p><p>Program entry:</p><p class="snippet">if __name__ == '__main__':</p><p class="snippet">    node = ActivitySub()</p><p class="snippet">    try:</p><p class="snippet">        node.run()</p><p class="snippet">    except rospy.ROSInterruptException:</p><p class="snippet">        pass</p></li>
				<li>Now, you need to set the destination to the scripts folder:<p class="snippet">cd ../../</p><p class="snippet">cd ..</p><p class="snippet">cd src/activity1/scripts/</p></li>
				<li>Execute the movement.py file:<p class="snippet">touch movement.py</p><p class="snippet">chmod +x movement.py</p><p class="snippet">cd ~/catkin_ws</p><p class="snippet">source devel/setup.bash</p><p class="snippet">roslaunch turtlebot_gazebo turtlebot_world.launch</p></li>
				<li>Open a new terminal and execute the command to get the output:<p class="snippet">cd ~/catkin_ws</p><p class="snippet">source devel/setup.bash</p><p class="snippet">rosrun activity1 activity.py</p><p class="snippet">cd ~/catkin_ws</p><p class="snippet">source devel/setup.bash</p><p class="snippet">rosrun activity1 activity_sub.py</p><p class="snippet">cd ~/catkin_ws</p><p class="snippet">source devel/setup.bash</p><p class="snippet">rosrun activity1 movement.py</p></li>
				<li>Run both nodes at the <a id="_idTextAnchor231"/>same time. This is an execution example:<p>Gazebo situation:</p></li>
			</ol>
			<div>
				<div id="_idContainer270" class="IMG---Figure">
					<img src="image/C1355_09_16.jpg" alt="Figure 9.16: Example situation for the activity"/>
				</div>
			</div>
			<h6>Figure 9.16: Example situation for the activity</h6>
			<p>First node output:</p>
			<div>
				<div id="_idContainer271" class="IMG---Figure">
					<img src="image/C1355_09_17.jpg" alt="Figure 9.17: First activity node output"/>
				</div>
			</div>
			<h6>Figure 9.17: First activity node output</h6>
			<p>Second node output:</p>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="image/C1355_09_18.jpg" alt="Figure 9.18: Second activity node output"/>
				</div>
			</div>
			<h6>Figure 9.18: Second activity node output</h6>
		</div>
	</body></html>