<html><head></head><body><div id="book-columns"><div id="book-inner"><div class="chapter" title="Chapter 3. Transfer Image Style Across Various Domains"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/><span class="koboSpan" id="kobo.1.1">Chapter 3. Transfer Image Style Across Various Domains </span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Generative Adversarial Network is the most rapidly emerging branch of deep learning that is suitable for a wide range of creative applications (such as image editing or painting, style transfer, object transfiguration, photo enhancement, and many more).</span></p><p><span class="koboSpan" id="kobo.3.1">In this chapter, you will first learn the technique of generating or editing images based on certain conditions or characteristics. </span><span class="koboSpan" id="kobo.3.2">Then, you will stabilize GAN training to overcome the mode-collapse problem and apply</span><a id="id115" class="indexterm"/><span class="koboSpan" id="kobo.4.1"> a convergence measure metric with the </span><span class="strong"><strong><span class="koboSpan" id="kobo.5.1">Boundary Equilibrium</span></strong></span><span class="koboSpan" id="kobo.6.1"> approach. </span><span class="koboSpan" id="kobo.6.2">Finally, you will perform image to image translation across various domains (such as changing apple to orange or horse to zebra) using </span><a id="id116" class="indexterm"/>
<span class="strong"><strong><span class="koboSpan" id="kobo.7.1">Cycle Consistent Generative Network</span></strong></span><span class="koboSpan" id="kobo.8.1">.</span></p><p><span class="koboSpan" id="kobo.9.1">We will cover the following topics in this chapter:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.10.1">What is CGAN? </span><span class="koboSpan" id="kobo.10.2">Its concept and architecture</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.11.1">Generating fashion wardrobe from </span><code class="literal"><span class="koboSpan" id="kobo.12.1">Fashion-MNIST</span></code><span class="koboSpan" id="kobo.13.1"> data using CGAN</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.14.1">Stabilizing GAN training using Boundary Equilibrium GAN with Wasserstein distance</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.15.1">Image style transfer across different domains using CycleGAN</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.16.1">Generating oranges from apples using Tensorflow</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.17.1">Changing horse images into zebras automatically</span></li></ul></div><div class="section" title="Bridging the gap between supervised and unsupervised learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec16"/><span class="koboSpan" id="kobo.18.1">Bridging the gap between supervised and unsupervised learning</span></h1></div></div></div><p><span class="koboSpan" id="kobo.19.1">Humans learn by </span><a id="id117" class="indexterm"/><span class="koboSpan" id="kobo.20.1">observing and experiencing the physical world and our brains are very good at prediction without doing explicit computations to arrive at the correct answer. </span><span class="koboSpan" id="kobo.20.2">Supervised learning is all about predicting a label associated with the data and the goal is to generalize to new unseen data. </span><span class="koboSpan" id="kobo.20.3">In unsupervised learning, the data comes in with no labels, and the goal is often not to generalize any kind of prediction to new data.</span></p><p><span class="koboSpan" id="kobo.21.1">In the real world, labeled</span><a id="id118" class="indexterm"/><span class="koboSpan" id="kobo.22.1"> data is often scarce and expensive. </span><span class="koboSpan" id="kobo.22.2">The Generative Adversarial Network takes up a supervised learning approach to do unsupervised learning, by generating fake/synthetic looking data, and tries to determine if the generated sample is fake or real. </span><span class="koboSpan" id="kobo.22.3">This part (a discriminator doing classification) is a supervised component. </span><span class="koboSpan" id="kobo.22.4">But the actual goal of GAN is to understand what the data looks like (that is, its distribution or density estimation) and be able to generate new examples of what it has learned.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Introduction to Conditional GAN"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec17"/><span class="koboSpan" id="kobo.1.1">Introduction to Conditional GAN</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">A </span><span class="strong"><strong><span class="koboSpan" id="kobo.3.1">Generative Adversarial Network</span></strong></span><span class="koboSpan" id="kobo.4.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.5.1">GAN</span></strong></span><span class="koboSpan" id="kobo.6.1">) simultaneously trains two networks—a generator that learns to</span><a id="id119" class="indexterm"/><span class="koboSpan" id="kobo.7.1"> generate fake samples from an unknown distribution or noise and a discriminator that learns to distinguish fake from real samples.</span></p><p><span class="koboSpan" id="kobo.8.1">In the </span><a id="id120" class="indexterm"/>
<span class="strong"><strong><span class="koboSpan" id="kobo.9.1">Conditional GAN</span></strong></span><span class="koboSpan" id="kobo.10.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.11.1">CGAN</span></strong></span><span class="koboSpan" id="kobo.12.1">), the generator learns to generate a fake sample with a specific condition or characteristics (such as a label associated with an image or more detailed tag) rather than a generic sample from unknown noise distribution. </span><span class="koboSpan" id="kobo.12.2">Now, to add such a condition to both generator and discriminator, we will simply feed some vector </span><span class="emphasis"><em><span class="koboSpan" id="kobo.13.1">y</span></em></span><span class="koboSpan" id="kobo.14.1">, into both networks. </span><span class="koboSpan" id="kobo.14.2">Hence, both the discriminator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.15.1">D(X,y)</span></em></span><span class="koboSpan" id="kobo.16.1"> and generator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.17.1">G(z,y)</span></em></span><span class="koboSpan" id="kobo.18.1"> are jointly conditioned to two variables, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.19.1">z</span></em></span><span class="koboSpan" id="kobo.20.1"> or </span><span class="emphasis"><em><span class="koboSpan" id="kobo.21.1">X</span></em></span><span class="koboSpan" id="kobo.22.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.23.1">y</span></em></span><span class="koboSpan" id="kobo.24.1">.</span></p><p><span class="koboSpan" id="kobo.25.1">Now, the objective function of CGAN is:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.26.1"><img src="graphics/B08086_03_01.jpg" alt="Introduction to Conditional GAN"/></span></div><p><span class="koboSpan" id="kobo.27.1">The difference between GAN loss and CGAN loss lies in the additional parameter </span><span class="emphasis"><em><span class="koboSpan" id="kobo.28.1">y</span></em></span><span class="koboSpan" id="kobo.29.1"> in both a discriminator and generator function. </span><span class="koboSpan" id="kobo.29.2">The architecture of CGAN shown in the following figure now has an additional input layer (in the form of condition vector </span><span class="strong"><strong><span class="koboSpan" id="kobo.30.1">C</span></strong></span><span class="koboSpan" id="kobo.31.1">) that gets fed into both the </span><a id="id121" class="indexterm"/><span class="koboSpan" id="kobo.32.1">discriminator network and generator network.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.33.1"><img src="graphics/B08086_03_02.jpg" alt="Introduction to Conditional GAN"/></span></div><div class="section" title="Generating a fashion wardrobe with CGAN"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec39"/><span class="koboSpan" id="kobo.34.1">Generating a fashion wardrobe with CGAN</span></h2></div></div></div><p><span class="koboSpan" id="kobo.35.1">In this example, we </span><a id="id122" class="indexterm"/><span class="koboSpan" id="kobo.36.1">will implement conditional GAN to generate a fashion wardrobe using a </span><code class="literal"><span class="koboSpan" id="kobo.37.1">Fashion-MNIST</span></code><span class="koboSpan" id="kobo.38.1"> dataset (</span><a class="ulink" href="https://github.com/zalandoresearch/fashion-mnist"><span class="koboSpan" id="kobo.39.1">https://github.com/zalandoresearch/fashion-mnist</span></a><span class="koboSpan" id="kobo.40.1">). </span><span class="koboSpan" id="kobo.40.2">The </span><code class="literal"><span class="koboSpan" id="kobo.41.1">Fashion-MNIST</span></code><span class="koboSpan" id="kobo.42.1"> dataset</span><a id="id123" class="indexterm"/><span class="koboSpan" id="kobo.43.1"> is similar</span><a id="id124" class="indexterm"/><span class="koboSpan" id="kobo.44.1"> to the original </span><code class="literal"><span class="koboSpan" id="kobo.45.1">MNIST</span></code><span class="koboSpan" id="kobo.46.1"> dataset with a new set of gray-scale images and labels.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.47.1"><img src="graphics/B08086_03_03.jpg" alt="Generating a fashion wardrobe with CGAN"/></span></div><p><span class="koboSpan" id="kobo.48.1">Let's jump into the code to understand the working of CGAN with simple neural network architecture for</span><a id="id125" class="indexterm"/><span class="koboSpan" id="kobo.49.1"> both generator </span><a id="id126" class="indexterm"/><span class="koboSpan" id="kobo.50.1">and discriminator.</span></p><p><span class="koboSpan" id="kobo.51.1">First, we will define a new input variable to hold our condition:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.52.1">Y = tf.placeholder(tf.float32, shape=(None, num_labels))</span></pre></div><p><span class="koboSpan" id="kobo.53.1">Next, we incorporate the</span><a id="id127" class="indexterm"/><span class="koboSpan" id="kobo.54.1"> new variable </span><code class="literal"><span class="koboSpan" id="kobo.55.1">y</span></code><span class="koboSpan" id="kobo.56.1"> into the discriminator </span><code class="literal"><span class="koboSpan" id="kobo.57.1">D(X)</span></code><span class="koboSpan" id="kobo.58.1"> and generator </span><code class="literal"><span class="koboSpan" id="kobo.59.1">G(z)</span></code><span class="koboSpan" id="kobo.60.1">. </span><span class="koboSpan" id="kobo.60.2">Now, the discriminator</span><code class="literal"><span class="koboSpan" id="kobo.61.1">(x,y)</span></code><span class="koboSpan" id="kobo.62.1"> and generator</span><code class="literal"><span class="koboSpan" id="kobo.63.1">(z,y)</span></code><span class="koboSpan" id="kobo.64.1"> are different than the original GAN:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.65.1">Dhidden = 256  # hidden units of Discriminator's network
Ghidden = 512  # hidden units of Generator's network
K = 8          # maxout units of Discriminator

# Discriminator Network

def discriminator(x, y):
    u = tf.reshape(tf.matmul(x, DW1x) + tf.matmul(y, DW1y) + Db1, [-1, K, Dhidden])
    Dh1 = tf.nn.dropout(tf.reduce_max(u, reduction_indices=[1]), keep_prob)
    return tf.nn.sigmoid(tf.matmul(Dh1, DW2) + Db2)

# Generator Network

def generator(z,y):
    Gh1 = tf.nn.relu(tf.matmul(Z, GW1z) + tf.matmul(Y, GW1y) + Gb1)
    G = tf.nn.sigmoid(tf.matmul(Gh1, GW2) + Gb2)
    return G</span></pre></div><p><span class="koboSpan" id="kobo.66.1">Next, we will use our new networks and define a </span><code class="literal"><span class="koboSpan" id="kobo.67.1">loss</span></code><span class="koboSpan" id="kobo.68.1"> function:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.69.1">G_sample = generator(Z, Y)
DG = discriminator(G_sample, Y)

Dloss = -tf.reduce_mean(tf.log(discriminator(X, Y)) + tf.log(1 - DG))
Gloss = tf.reduce_mean(tf.log(1 - DG) - tf.log(DG + 1e-9)) </span></pre></div><p><span class="koboSpan" id="kobo.70.1">During training, we feed the value of </span><code class="literal"><span class="koboSpan" id="kobo.71.1">y</span></code><span class="koboSpan" id="kobo.72.1"> into both a generator network and discriminator network:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.73.1">X_mb, y_mb = mnist.train.next_batch(mini_batch_size)

Z_sample = sample_Z(mini_batch_size, noise_dim)

_, D_loss_curr = sess.run([Doptimizer, Dloss], feed_dict={X: X_mb, Z: Z_sample, Y:y_mb, keep_prob:0.5})

_, G_loss_curr = sess.run([Goptimizer, Gloss], feed_dict={Z: Z_sample, Y:y_mb, keep_prob:1.0})</span></pre></div><p><span class="koboSpan" id="kobo.74.1">Finally, we </span><a id="id128" class="indexterm"/><span class="koboSpan" id="kobo.75.1">generate new data samples based on certain conditions. </span><span class="koboSpan" id="kobo.75.2">For this example, we use the image label as our condition and set the label to be </span><code class="literal"><span class="koboSpan" id="kobo.76.1">7</span></code><span class="koboSpan" id="kobo.77.1">, that is, generating the image of </span><code class="literal"><span class="koboSpan" id="kobo.78.1">Sneaker</span></code><span class="koboSpan" id="kobo.79.1">. </span><span class="koboSpan" id="kobo.79.2">The</span><a id="id129" class="indexterm"/><span class="koboSpan" id="kobo.80.1"> conditional variable </span><code class="literal"><span class="koboSpan" id="kobo.81.1">y_sample</span></code><span class="koboSpan" id="kobo.82.1"> is a collection of one-hot encoded vectors with value </span><code class="literal"><span class="koboSpan" id="kobo.83.1">1</span></code><span class="koboSpan" id="kobo.84.1"> in the seventh index:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.85.1">nsamples=6

      Z_sample = sample_Z(nsamples, noise_dim)
        y_sample = np.zeros(shape=[nsamples, num_labels])
        y_sample[:, 7] = 1 # generating image based on label

        samples = sess.run(G_sample, feed_dict={Z: Z_sample, Y:y_sample})</span></pre></div><p><span class="koboSpan" id="kobo.86.1">Now let us execute the following steps to generate wardrobe images based on class label condition. </span><span class="koboSpan" id="kobo.86.2">First download the </span><code class="literal"><span class="koboSpan" id="kobo.87.1">Fashion-MNIST</span></code><span class="koboSpan" id="kobo.88.1"> dataset and save it under the </span><code class="literal"><span class="koboSpan" id="kobo.89.1">data</span></code><span class="koboSpan" id="kobo.90.1">/</span><code class="literal"><span class="koboSpan" id="kobo.91.1">fashion</span></code><span class="koboSpan" id="kobo.92.1"> directory by running the </span><code class="literal"><span class="koboSpan" id="kobo.93.1">download.py</span></code><span class="koboSpan" id="kobo.94.1"> script:</span></p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.95.1">python download.py</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.96.1"><img src="graphics/B08086_03_04.jpg" alt="Generating a fashion wardrobe with CGAN"/></span></div><p><span class="koboSpan" id="kobo.97.1">Next train the </span><a id="id130" class="indexterm"/><span class="koboSpan" id="kobo.98.1">CGAN model using</span><a id="id131" class="indexterm"/><span class="koboSpan" id="kobo.99.1"> the following command, which will generate sample images after every 1000 iterations under the </span><code class="literal"><span class="koboSpan" id="kobo.100.1">output</span></code><span class="koboSpan" id="kobo.101.1"> directory:</span></p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.102.1">python simple-cgan.py</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.103.1"><img src="graphics/B08086_03_05.jpg" alt="Generating a fashion wardrobe with CGAN"/></span></div><p><span class="koboSpan" id="kobo.104.1">The following is the output of running CGAN using a condition label set to </span><span class="strong"><strong><span class="koboSpan" id="kobo.105.1">4 (Coat)</span></strong></span><span class="koboSpan" id="kobo.106.1"> after </span><span class="strong"><strong><span class="koboSpan" id="kobo.107.1">80k</span></strong></span><span class="koboSpan" id="kobo.108.1"> iteration and </span><span class="strong"><strong><span class="koboSpan" id="kobo.109.1">7 (Sneaker)</span></strong></span><span class="koboSpan" id="kobo.110.1"> after </span><span class="strong"><strong><span class="koboSpan" id="kobo.111.1">60k</span></strong></span><span class="koboSpan" id="kobo.112.1"> iteration:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.113.1"><img src="graphics/B08086_03_06.jpg" alt="Generating a fashion wardrobe with CGAN"/></span></div></div><div class="section" title="Stabilizing training with Boundary Equilibrium GAN"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec40"/><span class="koboSpan" id="kobo.114.1">Stabilizing training with Boundary Equilibrium GAN</span></h2></div></div></div><p><span class="koboSpan" id="kobo.115.1">The popularity of GAN is rising rapidly among machine learning researchers. </span><span class="koboSpan" id="kobo.115.2">GAN researches can</span><a id="id132" class="indexterm"/><span class="koboSpan" id="kobo.116.1"> be categorized into two types: one that applies GAN into challenging problems and one that attempts to stabilize the training. </span><span class="koboSpan" id="kobo.116.2">Stabilizing GAN training is very crucial as the original GAN architecture suffers and has several shortcomings:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.117.1">Mode collapse</span></strong></span><span class="koboSpan" id="kobo.118.1">: Where generators collapse into very narrow distribution and the samples</span><a id="id133" class="indexterm"/><span class="koboSpan" id="kobo.119.1"> generated are not diverse. </span><span class="koboSpan" id="kobo.119.2">This problem of course violates the spirit of GAN.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.120.1">Evaluation of convergence metric</span></strong></span><span class="koboSpan" id="kobo.121.1">: There</span><a id="id134" class="indexterm"/><span class="koboSpan" id="kobo.122.1"> is no well-defined metric that tells us about the convergence between discriminator loss and generator loss.</span></li></ul></div><p><span class="koboSpan" id="kobo.123.1">The improved </span><span class="strong"><strong><span class="koboSpan" id="kobo.124.1">Wasserstein GAN</span></strong></span><span class="koboSpan" id="kobo.125.1"> (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.126.1">arXiv: 1704.00028,2017</span></em></span><span class="koboSpan" id="kobo.127.1">) is a newly proposed GAN algorithm that promises</span><a id="id135" class="indexterm"/><span class="koboSpan" id="kobo.128.1"> to solve the preceding problems by minimizing the Wasserstein distance (or Earth-Mover distance) by providing simple gradients to the networks (+1 if the output is considered real and -1 if the output is considered fake).</span></p><p><span class="koboSpan" id="kobo.129.1">The main idea </span><a id="id136" class="indexterm"/><span class="koboSpan" id="kobo.130.1">behind the </span><span class="strong"><strong><span class="koboSpan" id="kobo.131.1">BEGAN</span></strong></span><span class="koboSpan" id="kobo.132.1"> (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.133.1">arXiv: 1703.10717,2017</span></em></span><span class="koboSpan" id="kobo.134.1">) is to have a new </span><code class="literal"><span class="koboSpan" id="kobo.135.1">loss</span></code><span class="koboSpan" id="kobo.136.1"> function by using </span><span class="strong"><strong><span class="koboSpan" id="kobo.137.1">auto-encoder</span></strong></span><span class="koboSpan" id="kobo.138.1"> as a discriminator, where the real loss is derived from the Wasserstein</span><a id="id137" class="indexterm"/><span class="koboSpan" id="kobo.139.1"> distance (to cater the problem of mode collapse) between</span><a id="id138" class="indexterm"/><span class="koboSpan" id="kobo.140.1"> the reconstruction losses of real and generated images:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.141.1"><img src="graphics/B08086_03_07.jpg" alt="Stabilizing training with Boundary Equilibrium GAN"/></span></div><p><span class="koboSpan" id="kobo.142.1">A hyper-parameter gamma is added through the use of a weighting parameter </span><span class="emphasis"><em><span class="koboSpan" id="kobo.143.1">k</span></em></span><span class="koboSpan" id="kobo.144.1"> to give users the power to control the desired diversity:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.145.1"><img src="graphics/B08086_03_08.jpg" alt="Stabilizing training with Boundary Equilibrium GAN"/></span></div><p><span class="koboSpan" id="kobo.146.1">Unlike most GANs where discriminator and the generator are trained alternatively, BEGAN allows simultaneous training of both the networks in an adversarial way at each time step:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.147.1"><img src="graphics/B08086_03_09.jpg" alt="Stabilizing training with Boundary Equilibrium GAN"/></span></div><p><span class="koboSpan" id="kobo.148.1">Finally, it allows</span><a id="id139" class="indexterm"/><span class="koboSpan" id="kobo.149.1"> an approximate measure of convergence </span><span class="emphasis"><em><span class="koboSpan" id="kobo.150.1">M</span></em></span><span class="koboSpan" id="kobo.151.1"> to understand the performance of the whole network:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.152.1"><img src="graphics/B08086_03_10.jpg" alt="Stabilizing training with Boundary Equilibrium GAN"/></span></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="The training procedure of BEGAN"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec18"/><span class="koboSpan" id="kobo.1.1">The training procedure of BEGAN</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Steps involved in training BEGAN  are described as follows:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.3.1">The discriminator (the autoencoder) updates its weights to minimize the reconstruction</span><a id="id140" class="indexterm"/><span class="koboSpan" id="kobo.4.1"> loss of real images and in that way, starts to reconstruct real images better.</span></li><li class="listitem"><span class="koboSpan" id="kobo.5.1">Simultaneously, the discriminator starts to maximize the reconstruction loss of generated images.</span></li><li class="listitem"><span class="koboSpan" id="kobo.6.1">The generator works in an adversarial way to minimize the reconstruction loss of generated images.</span></li></ol></div><div class="section" title="Architecture of BEGAN"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec41"/><span class="koboSpan" id="kobo.7.1">Architecture of BEGAN</span></h2></div></div></div><p><span class="koboSpan" id="kobo.8.1">As shown in the following figure, the discriminator is a convolutional network with both deep encoder and</span><a id="id141" class="indexterm"/><span class="koboSpan" id="kobo.9.1"> decoder. </span><span class="koboSpan" id="kobo.9.2">The decoder has multiple layers of 3x3 convolution followed by an </span><span class="strong"><strong><span class="koboSpan" id="kobo.10.1">Exponential Linear Unit</span></strong></span><span class="koboSpan" id="kobo.11.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.12.1">ELU</span></strong></span><span class="koboSpan" id="kobo.13.1">). </span><span class="koboSpan" id="kobo.13.2">Downsampling is</span><a id="id142" class="indexterm"/><span class="koboSpan" id="kobo.14.1"> done with stride 2 convolutions. </span><span class="koboSpan" id="kobo.14.2">The embedding state of the autoencoder is mapped to fully connected layers. </span><span class="koboSpan" id="kobo.14.3">Both the generator and the decoder are deep deconvolution with identical architectures, but with different weights, and the upsampling is done using nearest-neighbors:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.15.1"><img src="graphics/B08086_03_11.jpg" alt="Architecture of BEGAN"/></span><div class="caption"><p><span class="koboSpan" id="kobo.16.1">Figure-1: The architecture of the BEGAN.</span></p><p><span class="koboSpan" id="kobo.17.1">Source: </span><span class="emphasis"><em><span class="koboSpan" id="kobo.18.1">arXiv: 1703.10717,2017,2017</span></em></span>
</p></div></div><p><span class="koboSpan" id="kobo.19.1">In the preceding figures, both the generator and the decoder of the discriminator is shown on the left-hand side. </span><span class="koboSpan" id="kobo.19.2">The encoder network of the discriminator is shown on the right-hand side.</span></p></div><div class="section" title="Implementation of BEGAN using Tensorflow"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec42"/><span class="koboSpan" id="kobo.20.1">Implementation of BEGAN using Tensorflow</span></h2></div></div></div><p><span class="koboSpan" id="kobo.21.1">Let us now</span><a id="id143" class="indexterm"/><span class="koboSpan" id="kobo.22.1"> dive deep</span><a id="id144" class="indexterm"/><span class="koboSpan" id="kobo.23.1"> into the code and implement the preceding concept along with the architecture to generate realistic attractive images.</span></p><p><span class="koboSpan" id="kobo.24.1">The generator network has multiple layers of 3x3 convolution with an </span><code class="literal"><span class="koboSpan" id="kobo.25.1">elu activation</span></code><span class="koboSpan" id="kobo.26.1"> function, followed by nearest neighbor upscaling, except at the final layer. </span><span class="koboSpan" id="kobo.26.2">The number of convolution layers is calculated from the height of the image:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.27.1">self.repeat_num = int(np.log2(height)) – 2.

</span><span class="koboSpan" id="kobo.27.2">def GeneratorCNN(z, hidden_num, output_num, repeat_num, data_format, reuse):
    with tf.variable_scope("G", reuse=reuse) as vs:
        num_output = int(np.prod([8, 8, hidden_num]))
        x = slim.fully_connected(z, num_output, activation_fn=None)
        x = reshape(x, 8, 8, hidden_num, data_format)
        
        for idx in range(repeat_num):
            x = slim.conv2d(x, hidden_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)
            x = slim.conv2d(x, hidden_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)
            if idx &lt; repeat_num - 1:
                x = upscale(x, 2, data_format)

        out = slim.conv2d(x, 3, 3, 1, activation_fn=None, data_format=data_format)

    variables = tf.contrib.framework.get_variables(vs)
    return out, variables</span></pre></div><p><span class="koboSpan" id="kobo.28.1">The encoder</span><a id="id145" class="indexterm"/><span class="koboSpan" id="kobo.29.1"> of the discriminator</span><a id="id146" class="indexterm"/><span class="koboSpan" id="kobo.30.1"> network has multiple layers of convolution with the </span><code class="literal"><span class="koboSpan" id="kobo.31.1">elu activation</span></code><span class="koboSpan" id="kobo.32.1"> function, followed by down-sampling using maxpooling except at the final convolution layer:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.33.1">def DiscriminatorCNN(x, input_channel, z_num, repeat_num, hidden_num, data_format):
    with tf.variable_scope("D") as vs:
        # Encoder
        x = slim.conv2d(x, hidden_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)

        prev_channel_num = hidden_num
        for idx in range(repeat_num):
            channel_num = hidden_num * (idx + 1)
            x = slim.conv2d(x, channel_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)
            x = slim.conv2d(x, channel_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)
            if idx &lt; repeat_num - 1:
                x = slim.conv2d(x, channel_num, 3, 2, activation_fn=tf.nn.elu, data_format=data_format)
                #x = tf.contrib.layers.max_pool2d(x, [2, 2], [2, 2], padding='VALID')

        x = tf.reshape(x, [-1, np.prod([8, 8, channel_num])])
        z = x = slim.fully_connected(x, z_num, activation_fn=None)</span></pre></div><p><span class="koboSpan" id="kobo.34.1">The decoder of the</span><a id="id147" class="indexterm"/><span class="koboSpan" id="kobo.35.1"> discriminator network is similar to the generator network, having multiple layers of</span><a id="id148" class="indexterm"/><span class="koboSpan" id="kobo.36.1"> convolution with an </span><code class="literal"><span class="koboSpan" id="kobo.37.1">elu activation</span></code><span class="koboSpan" id="kobo.38.1"> function followed by upsampling using nearest neighbor except at the final convolution layer:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.39.1">        num_output = int(np.prod([8, 8, hidden_num]))
        x = slim.fully_connected(x, num_output, activation_fn=None)
        x = reshape(x, 8, 8, hidden_num, data_format)
        
        for idx in range(repeat_num):
            x = slim.conv2d(x, hidden_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)
            x = slim.conv2d(x, hidden_num, 3, 1, activation_fn=tf.nn.elu, data_format=data_format)
            if idx &lt; repeat_num - 1:
                x = upscale(x, 2, data_format)

        out = slim.conv2d(x, input_channel, 3, 1, activation_fn=None, data_format=data_format)

    variables = tf.contrib.framework.get_variables(vs)</span></pre></div><p><span class="koboSpan" id="kobo.40.1">Now the generator loss and discriminator loss for both real, fake images discussed previously are optimized </span><a id="id149" class="indexterm"/><span class="koboSpan" id="kobo.41.1">using </span><span class="strong"><strong><span class="koboSpan" id="kobo.42.1">Adam Optimizer</span></strong></span><span class="koboSpan" id="kobo.43.1"> by executing the following code block:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.44.1">d_out, self.D_z, self.D_var = DiscriminatorCNN(
                tf.concat([G, x], 0), self.channel, self.z_num, self.repeat_num,
                self.conv_hidden_num, self.data_format)
        AE_G, AE_x = tf.split(d_out, 2)

        self.G = denorm_img(G, self.data_format)
        self.AE_G, self.AE_x = denorm_img(AE_G, self.data_format), denorm_img(AE_x, self.data_format)

if self.optimizer == 'adam':
            optimizer = tf.train.AdamOptimizer
        else:
            raise Exception("[!] Caution! </span><span class="koboSpan" id="kobo.44.2">Paper didn't use {} opimizer other than Adam".format(config.optimizer))

        g_optimizer, d_optimizer = optimizer(self.g_lr), optimizer(self.d_lr)

        self.d_loss_real = tf.reduce_mean(tf.abs(AE_x - x))
        self.d_loss_fake = tf.reduce_mean(tf.abs(AE_G - G))

        self.d_loss = self.d_loss_real - self.k_t * self.d_loss_fake
        self.g_loss = tf.reduce_mean(tf.abs(AE_G - G))</span></pre></div><p><span class="koboSpan" id="kobo.45.1">Now it's time to </span><a id="id150" class="indexterm"/><span class="koboSpan" id="kobo.46.1">execute the code for generating impressive celebrity images:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.47.1">First clone the following repository and then change the directory:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.48.1">git clone https://github.com/carpedm20/BEGAN-tensorflow.git</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.49.1">cd BEGAN-tensorflow</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.50.1">Next, run</span><a id="id151" class="indexterm"/><span class="koboSpan" id="kobo.51.1"> the following scripts to download the </span><code class="literal"><span class="koboSpan" id="kobo.52.1">CelebA</span></code><span class="koboSpan" id="kobo.53.1"> dataset under the </span><code class="literal"><span class="koboSpan" id="kobo.54.1">data</span></code><span class="koboSpan" id="kobo.55.1"> directory, and split it into training, validation, and test set:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.56.1">python download.py</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.57.1">Make sure p7zip is installed on your machine.</span></li><li class="listitem"><span class="koboSpan" id="kobo.58.1">Now start the training process as follows, which will save the generated samples under the </span><code class="literal"><span class="koboSpan" id="kobo.59.1">logs</span></code><span class="koboSpan" id="kobo.60.1"> directory:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.61.1">python main.py --dataset=CelebA --use_gpu=True</span></strong></span>
</pre></div></li></ol></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/><span class="koboSpan" id="kobo.62.1">Note</span></h3><p><span class="koboSpan" id="kobo.63.1">If you face the error </span><span class="strong"><strong><span class="koboSpan" id="kobo.64.1">Conv2DCustomBackpropInputOp only supports NHWC</span></strong></span><span class="koboSpan" id="kobo.65.1">, then refer to the following issue:</span></p><p>
<a class="ulink" href="https://github.com/carpedm20/BEGAN-tensorflow/ issues/29"><span class="koboSpan" id="kobo.66.1">https://github.com/carpedm20/BEGAN-tensorflow/
issues/29</span></a>
</p></div></div><p><span class="koboSpan" id="kobo.67.1">After</span><a id="id152" class="indexterm"/><span class="koboSpan" id="kobo.68.1"> executing the preceding command, while the training is going on you will notice information such as </span><code class="literal"><span class="koboSpan" id="kobo.69.1">Model</span></code><span class="koboSpan" id="kobo.70.1"> directory, logging directory, and various losses as follows:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.71.1"><img src="graphics/B08086_03_12.jpg" alt="Implementation of BEGAN using Tensorflow"/></span></div><p><span class="koboSpan" id="kobo.72.1">The output faces generated by BEGAN are visually realistic and attractive as shown in the following screenshot:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.73.1"><img src="graphics/B08086_03_13.jpg" alt="Implementation of BEGAN using Tensorflow"/></span><div class="caption"><p><span class="koboSpan" id="kobo.74.1">Figure-2: Generator output images (64x64) with gamma=0.5 after 350k steps</span></p></div></div><p><span class="koboSpan" id="kobo.75.1">The following sample output images (128 x 128) are generated after 250k steps:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.76.1"><img src="graphics/B08086_03_14.jpg" alt="Implementation of BEGAN using Tensorflow"/></span><div class="caption"><p><span class="koboSpan" id="kobo.77.1">Figure-3: Generator output images (128x128) with gamma=0.5 after 250k steps</span></p></div></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Image to image style transfer with CycleGAN"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec19"/><span class="koboSpan" id="kobo.1.1">Image to image style transfer with CycleGAN</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">The </span><span class="strong"><strong><span class="koboSpan" id="kobo.3.1">Cycle Consistent Generative Network</span></strong></span><span class="koboSpan" id="kobo.4.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.5.1">CycleGAN</span></strong></span><span class="koboSpan" id="kobo.6.1">), originally proposed in the paper </span><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">Unpaired image-to-image translation using CycleGAN</span></em></span><span class="koboSpan" id="kobo.8.1">—</span><span class="emphasis"><em><span class="koboSpan" id="kobo.9.1">arXiv: 1703.10593, 2017</span></em></span><span class="koboSpan" id="kobo.10.1">, aims at finding mapping </span><a id="id153" class="indexterm"/><span class="koboSpan" id="kobo.11.1">between the source domain and a target domain for a given image without any pairing information (such as greyscale to color, image to semantic labels, edge-map to photograph, horse to zebra, and so on).</span></p><p><span class="koboSpan" id="kobo.12.1">The key idea behind CycleGAN is to have two translator's F and G, where F will translate an image from domain </span><span class="emphasis"><em><span class="koboSpan" id="kobo.13.1">A</span></em></span><span class="koboSpan" id="kobo.14.1"> to domain </span><span class="emphasis"><em><span class="koboSpan" id="kobo.15.1">B</span></em></span><span class="koboSpan" id="kobo.16.1">, and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.17.1">G</span></em></span><span class="koboSpan" id="kobo.18.1"> will translate an image from domain </span><span class="emphasis"><em><span class="koboSpan" id="kobo.19.1">B</span></em></span><span class="koboSpan" id="kobo.20.1"> to domain </span><span class="emphasis"><em><span class="koboSpan" id="kobo.21.1">A</span></em></span><span class="koboSpan" id="kobo.22.1">. </span><span class="koboSpan" id="kobo.22.2">So, for an image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.23.1">x</span></em></span><span class="koboSpan" id="kobo.24.1"> in domain </span><span class="emphasis"><em><span class="koboSpan" id="kobo.25.1">A</span></em></span><span class="koboSpan" id="kobo.26.1">, we should expect the function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.27.1">G(F(x))</span></em></span><span class="koboSpan" id="kobo.28.1"> to be equivalent to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.29.1">x</span></em></span><span class="koboSpan" id="kobo.30.1"> and similarly for an image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.31.1">y</span></em></span><span class="koboSpan" id="kobo.32.1"> in domain </span><span class="emphasis"><em><span class="koboSpan" id="kobo.33.1">B</span></em></span><span class="koboSpan" id="kobo.34.1">, we should expect the function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.35.1">F(G(y))</span></em></span><span class="koboSpan" id="kobo.36.1"> to be equivalent to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.37.1">y</span></em></span><span class="koboSpan" id="kobo.38.1">.</span></p><div class="section" title="Model formulation of CycleGAN"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec43"/><span class="koboSpan" id="kobo.39.1">Model formulation of CycleGAN</span></h2></div></div></div><p><span class="koboSpan" id="kobo.40.1">The main goal of the </span><a id="id154" class="indexterm"/><span class="koboSpan" id="kobo.41.1">CycleGAN model is to learn mapping between the two domains </span><span class="emphasis"><em><span class="koboSpan" id="kobo.42.1">X</span></em></span><span class="koboSpan" id="kobo.43.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.44.1">Y</span></em></span><span class="koboSpan" id="kobo.45.1"> using the training samples </span><span class="emphasis"><em><span class="koboSpan" id="kobo.46.1">{xi}Ni=1 </span></em></span>
<span class="emphasis"><em><span class="koboSpan" id="kobo.47.1">∈</span></em></span>
<span class="emphasis"><em><span class="koboSpan" id="kobo.48.1">X</span></em></span><span class="koboSpan" id="kobo.49.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.50.1">{yj}Mj=1 </span></em></span>
<span class="emphasis"><em><span class="koboSpan" id="kobo.51.1">∈</span></em></span>
<span class="emphasis"><em><span class="koboSpan" id="kobo.52.1">Y</span></em></span><span class="koboSpan" id="kobo.53.1">. </span><span class="koboSpan" id="kobo.53.2">It also has two adversarial discriminators </span><span class="emphasis"><em><span class="koboSpan" id="kobo.54.1">D</span></em></span>
<sub><span class="koboSpan" id="kobo.55.1">X</span></sub><span class="koboSpan" id="kobo.56.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.57.1">D</span></em></span>
<sub><span class="koboSpan" id="kobo.58.1">Y</span></sub><span class="koboSpan" id="kobo.59.1">: where </span><span class="emphasis"><em><span class="koboSpan" id="kobo.60.1">D</span></em></span>
<sub><span class="koboSpan" id="kobo.61.1">X</span></sub><span class="koboSpan" id="kobo.62.1"> tries to distinguish between original images </span><span class="emphasis"><em><span class="koboSpan" id="kobo.63.1">{x}</span></em></span><span class="koboSpan" id="kobo.64.1"> and translated images </span><span class="emphasis"><em><span class="koboSpan" id="kobo.65.1">{F(y)}</span></em></span><span class="koboSpan" id="kobo.66.1">, and similarly, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.67.1">D</span></em></span>
<sub><span class="koboSpan" id="kobo.68.1">Y</span></sub><span class="koboSpan" id="kobo.69.1"> tries to distinguish between </span><span class="emphasis"><em><span class="koboSpan" id="kobo.70.1">{y}</span></em></span><span class="koboSpan" id="kobo.71.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.72.1">{G(x)}</span></em></span><span class="koboSpan" id="kobo.73.1">.</span></p><p><span class="koboSpan" id="kobo.74.1">CycleGAN model has two </span><code class="literal"><span class="koboSpan" id="kobo.75.1">loss</span></code><span class="koboSpan" id="kobo.76.1"> functions:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.77.1">Adversarial loss</span></strong></span><span class="koboSpan" id="kobo.78.1">: It matches</span><a id="id155" class="indexterm"/><span class="koboSpan" id="kobo.79.1"> the generated image's distribution to the target domain distribution:</span><div class="mediaobject"><span class="koboSpan" id="kobo.80.1"><img src="graphics/B08086_03_15.jpg" alt="Model formulation of CycleGAN"/></span></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.81.1">Cycle consistency loss</span></strong></span><span class="koboSpan" id="kobo.82.1">: It prevents</span><a id="id156" class="indexterm"/><span class="koboSpan" id="kobo.83.1"> the learned mappings </span><span class="emphasis"><em><span class="koboSpan" id="kobo.84.1">G</span></em></span><span class="koboSpan" id="kobo.85.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.86.1">F</span></em></span><span class="koboSpan" id="kobo.87.1"> from contradicting each other:</span><div class="mediaobject"><span class="koboSpan" id="kobo.88.1"><img src="graphics/B08086_03_16.jpg" alt="Model formulation of CycleGAN"/></span></div></li></ul></div><p><span class="koboSpan" id="kobo.89.1">The full CycleGAN objective function is given by:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.90.1"><img src="graphics/B08086_03_17.jpg" alt="Model formulation of CycleGAN"/></span></div></div><div class="section" title="Transforming apples into oranges using Tensorflow"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec44"/><span class="koboSpan" id="kobo.91.1">Transforming apples into oranges using Tensorflow</span></h2></div></div></div><p><span class="koboSpan" id="kobo.92.1">In this example, we </span><a id="id157" class="indexterm"/><span class="koboSpan" id="kobo.93.1">will transfer the style from an image in domain </span><span class="emphasis"><em><span class="koboSpan" id="kobo.94.1">A</span></em></span><span class="koboSpan" id="kobo.95.1"> to an image in another domain </span><span class="emphasis"><em><span class="koboSpan" id="kobo.96.1">B</span></em></span><span class="koboSpan" id="kobo.97.1">: more specifically, we will apply CycleGAN to transform apples into oranges or vice-versa by executing the following steps:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.98.1">First clone the following </span><code class="literal"><span class="koboSpan" id="kobo.99.1">git</span></code><span class="koboSpan" id="kobo.100.1"> repository and change the directory to CycleGAN-tensorflow:</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.101.1">
git clone https://github.com/xhujoy/CycleGAN-tensorflow
cd CycleGAN-tensorflow</span></pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.102.1">Now download the </span><code class="literal"><span class="koboSpan" id="kobo.103.1">apple2orange</span></code><span class="koboSpan" id="kobo.104.1"> dataset ZIP file using the </span><code class="literal"><span class="koboSpan" id="kobo.105.1">download_dataset.sh</span></code><span class="koboSpan" id="kobo.106.1"> script, extract</span><a id="id158" class="indexterm"/><span class="koboSpan" id="kobo.107.1"> and save it under the </span><code class="literal"><span class="koboSpan" id="kobo.108.1">datasets</span></code><span class="koboSpan" id="kobo.109.1"> directory:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.110.1">bash ./download_dataset.sh apple2orange</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.111.1">Next train the CycleGAN model using the downloaded </span><code class="literal"><span class="koboSpan" id="kobo.112.1">apple2orange</span></code><span class="koboSpan" id="kobo.113.1"> dataset. </span><span class="koboSpan" id="kobo.113.2">During the training phase, the model will be saved in the </span><code class="literal"><span class="koboSpan" id="kobo.114.1">checkpoint</span></code><span class="koboSpan" id="kobo.115.1"> directory and logging is enabled in the </span><code class="literal"><span class="koboSpan" id="kobo.116.1">logs</span></code><span class="koboSpan" id="kobo.117.1"> directory for visualization with TensorBoard:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.118.1">python main.py --dataset_dir=apple2orange</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.119.1"><img src="graphics/B08086_03_18.jpg" alt="Transforming apples into oranges using Tensorflow"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.120.1">Run the following command to visualize various losses (discriminator loss and generator loss) during the training phase in your browser, by navigating to </span><code class="literal"><span class="koboSpan" id="kobo.121.1">http://localhost:6006/</span></code><span class="koboSpan" id="kobo.122.1">:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.123.1">tensorboard --logdir=./logs</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.124.1"><img src="graphics/B08086_03_19.jpg" alt="Transforming apples into oranges using Tensorflow"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.125.1">Finally, we will load the trained model from the </span><code class="literal"><span class="koboSpan" id="kobo.126.1">checkpoint</span></code><span class="koboSpan" id="kobo.127.1"> directory to transfer a</span><a id="id159" class="indexterm"/><span class="koboSpan" id="kobo.128.1"> style across images, hence generating oranges from apple or vice-versa (based on the value passed (</span><code class="literal"><span class="koboSpan" id="kobo.129.1">AtoB</span></code><span class="koboSpan" id="kobo.130.1"> or </span><code class="literal"><span class="koboSpan" id="kobo.131.1">BtoA</span></code><span class="koboSpan" id="kobo.132.1">) to the </span><code class="literal"><span class="koboSpan" id="kobo.133.1">which_direction</span></code><span class="koboSpan" id="kobo.134.1"> parameter that indicates a style transfer from domain 1 to domain 2):</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.135.1">python main.py --dataset_dir=apple2orange --phase=test --which_direction=AtoB</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.136.1">The following are the sample output images generated in the </span><code class="literal"><span class="koboSpan" id="kobo.137.1">test</span></code><span class="koboSpan" id="kobo.138.1"> phase:</span><div class="mediaobject"><span class="koboSpan" id="kobo.139.1"><img src="graphics/B08086_03_20.jpg" alt="Transforming apples into oranges using Tensorflow"/></span><div class="caption"><p><span class="koboSpan" id="kobo.140.1">Figure- 4: The left-hand side shows transforming apples to oranges by passing AtoB in the direction parameter, whereas the right-hand side shows the output generated by passing BtoA in the direction parameter.</span></p></div></div></li></ol></div></div><div class="section" title="Transfiguration of a horse into a zebra with CycleGAN"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec45"/><span class="koboSpan" id="kobo.141.1">Transfiguration of a horse into a zebra with CycleGAN</span></h2></div></div></div><p><span class="koboSpan" id="kobo.142.1">Just like the previous example, in this section </span><a id="id160" class="indexterm"/><span class="koboSpan" id="kobo.143.1">we will use CycleGAN to transform a horse into a zebra or vice-versa by executing the following steps:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.144.1">First clone the following </span><code class="literal"><span class="koboSpan" id="kobo.145.1">git</span></code><span class="koboSpan" id="kobo.146.1"> repository and change the directory to </span><code class="literal"><span class="koboSpan" id="kobo.147.1">CycleGAN-tensorflow</span></code><span class="koboSpan" id="kobo.148.1"> (you can skip this step if you have already executed the previous example):</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.149.1">git clone https://github.com/xhujoy/CycleGAN-tensorflow
cd CycleGAN-tensorflow</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.150.1">Now download the </span><code class="literal"><span class="koboSpan" id="kobo.151.1">horse2zebra</span></code><span class="koboSpan" id="kobo.152.1"> ZIP file from Berkley, extract it, and save it under the </span><code class="literal"><span class="koboSpan" id="kobo.153.1">datasets</span></code><span class="koboSpan" id="kobo.154.1"> directory using the </span><code class="literal"><span class="koboSpan" id="kobo.155.1">download_dataset.sh</span></code><span class="koboSpan" id="kobo.156.1"> script:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.157.1">bash ./download_dataset.sh horse2zebra</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.158.1">Next, we will train our CycleGAN model using the </span><code class="literal"><span class="koboSpan" id="kobo.159.1">horse2zebra</span></code><span class="koboSpan" id="kobo.160.1"> dataset and use TensorBoard for visualizing the losses while training is going on:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.161.1">python main.py --dataset_dir=horse2zebra</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.162.1"><img src="graphics/B08086_03_21.jpg" alt="Transfiguration of a horse into a zebra with CycleGAN"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.163.1">Run the following command and navigate to </span><code class="literal"><span class="koboSpan" id="kobo.164.1">http://localhost:6006/</span></code><span class="koboSpan" id="kobo.165.1"> for the visualizing of various generator or discriminator losses:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.166.1">tensorboard --logdir=./logs</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.167.1"><img src="graphics/B08086_03_22.jpg" alt="Transfiguration of a horse into a zebra with CycleGAN"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.168.1">Finally, we will use the</span><a id="id161" class="indexterm"/><span class="koboSpan" id="kobo.169.1"> trained model from the </span><code class="literal"><span class="koboSpan" id="kobo.170.1">checkpoint</span></code><span class="koboSpan" id="kobo.171.1"> directory to transform a horse into a zebra or vice-versa, depending on whether the value </span><code class="literal"><span class="koboSpan" id="kobo.172.1">AtoB</span></code><span class="koboSpan" id="kobo.173.1"> or </span><code class="literal"><span class="koboSpan" id="kobo.174.1">BtoA</span></code><span class="koboSpan" id="kobo.175.1"> is passed to the </span><code class="literal"><span class="koboSpan" id="kobo.176.1">which_direction</span></code><span class="koboSpan" id="kobo.177.1"> parameter:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.178.1">python main.py --dataset_dir=horse2zebra --phase=test --which_direction=AtoB</span></strong></span>
</pre></div></li></ol></div><p><span class="koboSpan" id="kobo.179.1">The following sample</span><a id="id162" class="indexterm"/><span class="koboSpan" id="kobo.180.1"> output images are generated in the </span><code class="literal"><span class="koboSpan" id="kobo.181.1">test</span></code><span class="koboSpan" id="kobo.182.1"> phase:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.183.1"><img src="graphics/B08086_03_23.jpg" alt="Transfiguration of a horse into a zebra with CycleGAN"/></span><div class="caption"><p><span class="koboSpan" id="kobo.184.1">Figure-5: The left-hand side shows transforming horse to zebra, whereas the right-hand side shows translating zebra into horse.</span></p></div></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/><span class="koboSpan" id="kobo.1.1">Summary</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">So far you have learned the approach of creating images based on certain characteristics or conditions, by passing that condition vector into both generator and discriminator. </span><span class="koboSpan" id="kobo.2.2">Also, you have understood how to overcome model collapse problems by stabilizing your network training using BEGAN. </span><span class="koboSpan" id="kobo.2.3">Finally, you have implemented image to image style transfer by generating an orange from an apple and a zebra from a horse, or vice-versa, using CycleGAN. </span><span class="koboSpan" id="kobo.2.4">In the next chapter, we will solve complex real-life problems such as text to image synthesis and cross domain discovery by stacking or coupling two or more GAN models together.</span></p></div></div></div></body></html>