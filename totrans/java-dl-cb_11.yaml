- en: Applying Transfer Learning to Network Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will talk about transfer learning methods, which are essential
    to reuse a model that was previously developed. We will see how we can apply transfer
    learning to the model created in [Chapter 3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml),
    *Building Deep Neural Networks for Binary Classification*, as well as a pre-trained
    model from the DL4J Model Zoo API. We can use the DL4J transfer learning API to
    modify the network architecture, hold specific layer parameters while training,
    and fine-tune model configurations. Transfer learning enables improved performance
    and can develop skillful models. We pass learned parameters learned from another
    model to the current training session. If you have already set up the DL4J workspace
    for previous chapters, then you don't have to add any new dependencies in `pom.xml`;
    otherwise, you need to add the basic Deeplearning4j Maven dependency in `pom.xml`, as
    specified in [Chapter 3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml), *Building
    Deep Neural Networks for Binary Classification*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Modifying an existing customer retention model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning the learning configurations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing frozen layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing and loading Keras models and layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter's source code can be located here: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/11_Applying_Transfer_Learning_to_network_models/sourceCode/cookbookapp/src/main/java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/11_Applying_Transfer_Learning_to_network_models/sourceCode/cookbookapp/src/main/java).
  prefs: []
  type: TYPE_NORMAL
- en: After cloning the GitHub repository, navigate to the `Java-Deep-Learning-Cookbook/11_Applying_Transfer_Learning_to_network_models/sourceCode`
    directory, then import the `cookbookapp` project as a Maven project by importing `pom.xml`*.*
  prefs: []
  type: TYPE_NORMAL
- en: You need to have the pre-trained model from [Chapter 3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml), *Building
    Deep Neural Networks for Binary Classification*, to run the transfer learning
    example. The model file should be saved in your local system once the [Chapter
    3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml), *Building Deep Neural Networks
    for Binary Classification* source code is executed. You need to load the model
    here while executing the source code in this chapter. Also, for the `SaveFeaturizedDataExample` example,
    you need to update the train/test directories where the application will be saving
    featurized datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying an existing customer retention model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We created a customer churn model in [Chapter 3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml),
    *Building Deep Neural Networks for Binary Classification*, that is capable of
    predicting whether a customer will leave an organization based on specified data.
    We might want to train the existing model on newly available data. Transfer learning occurs
    when an existing model is exposed to fresh training on a similar model. We used
    the `ModelSerializer` class to save the model after training the neural network.
    We used a feed-forward network architecture to build a customer retention model.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will import an existing customer retention model and further
    optimize it using the DL4J transfer learning API.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Call the `load()` method to import the model from the saved location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the required `pom` dependency to use the `deeplearning4j-zoo` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the fine-tuning configuration for `MultiLayerNetwork` using the `TransferLearning`
    API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the fine-tuning configuration for `ComputationGraph` using the `TransferLearning` API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the training session using `TransferLearningHelper`. `TransferLearningHelper`
    can be created in two ways:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass in the model object that was created using the transfer learning builder
    (step 2) with the frozen layers mentioned:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Create it directly from the imported model by specifying the frozen layers
    explicitly:'
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Featurize the train/test data using the `featurize()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create train/test iterators by using `ExistingMiniBatchDataSetIterator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the training instance on top of the featurized data by calling `fitFeaturized()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model by calling `evaluate()` for unfrozen layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In step 1, the value of `saveUpdater` is going to be `true` if we plan to train
    the model at a later point. We have also discussed pre-trained models provided
    by DL4J''s model zoo API. Once we add the dependency for `deeplearning4j-zoo`,
    as mentioned in step 1, we can load pre-trained models such as VGG16, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: DL4J has support for many more pre-trained models under its transfer learning
    API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-tuning a configuration is the process of taking a model that was trained
    to perform a task and training it to perform another similar task. Fine-tuning
    configurations is specific to transfer learning. In steps 3 and 4, we added a
    fine-tuning configuration specific to the type of neural network. The following
    are possible changes that can be made using the DL4J transfer learning API:'
  prefs: []
  type: TYPE_NORMAL
- en: Update the weight initialization scheme, gradient update strategy, and the optimization
    algorithm (fine-tuning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify specific layers without altering other layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attach new layers to the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these modifications can be applied using the transfer learning API. The
    DL4J transfer learning API comes with a builder class to support these modifications. We
    will add a fine-tuning configuration by calling the `fineTuneConfiguration()` builder
    method.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, in step 4 we use `GraphBuilder` for transfer learning with
    computation graphs. Refer to our GitHub repository for concrete examples. Note
    that the transfer learning API returns an instance of the model from the imported
    model after applying all the modifications that were specified. The regular `Builder` class
    will build an instance of `MultiLayerNetwork` while `GraphBuilder` will build
    an instance of `ComputationGraph`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We may also be interested in making changes only in certain layers rather than
    making global changes across layers. The main motive is to apply further optimization
    to certain layers that are identified for further optimization. That also begs
    another question: How do we know the model details of a stored model? In order
    to specify layers that are to be kept unchanged, the transfer learning API requires
    layer attributes such as the layer name/layer number.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get these using the `getLayerWiseConfigurations()` method, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we execute the preceding, you should see the network configuration mentioned
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/439ce467-8a8e-4ad1-9005-46b639006f7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Gist URL for complete network configuration JSON is at [https://gist.github.com/rahul-raj/ee71f64706fa47b6518020071711070b](https://gist.github.com/rahul-raj/ee71f64706fa47b6518020071711070b).
  prefs: []
  type: TYPE_NORMAL
- en: Neural network configurations such as the learning rate, the weights used in
    neurons, optimization algorithms used, layer-specific configurations, and so on
    can be verified from the displayed JSON content.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some possible configurations from the DL4J transfer learning
    API to support model modifications. We need layer details (name/ID) in order to
    invoke these methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`setFeatureExtractor()`: To freeze the changes on specific layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`addLayer()`: To add one or more layers to the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nInReplace()/nOutReplace()`: Modifies the architecture of the specified layer
    by changing the `nIn` or `nOut` of the specified layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`removeLayersFromOutput()`: Removes the last `n` layers from the model (from
    the point where an output layer must be added back)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the last layer in the imported transfer learning model is a *dense*
    layer. because the DL4J transfer learning API doesn't enforce training configuration
    on imported model. So, we need to add an output layer to the model using the `addLayer()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '`setInputPreProcessor()`: Adds the specified preprocessor to the specified
    layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In step 5, we saw another way to apply transfer learning in DL4J, by using `TransferLearningHelper`.
    We discussed two ways in which it can be implemented. When you create `TransferLearningHelper` from
    the transfer learning builder, you need to specify `FineTuneConfiguration` as
    well. Values configured in `FineTuneConfiguration` will override for all non-frozen
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: There's a reason why `TransferLearningHelper` stands out from the regular way of
    handling transfer learning. Transfer learning models usually have frozen layers
    with constant values across training sessions. The purpose of frozen layers depends
    on the observation being made in the existing model performance. We have also
    mentioned the `setFeatureExtractor()` method, which is used to freeze specific
    layers. Layers can be skipped using this method. However, the model instance still
    holds the entire frozen and unfrozen part. So, we still use the entire model (including
    both the frozen and unfrozen parts) for computations during training.
  prefs: []
  type: TYPE_NORMAL
- en: Using `TransferLearningHelper`, we can reduce the overall training time by creating
    a model instance of just the unfrozen part. The frozen dataset (with all the frozen
    parameters) is saved to disk and we use the model instance that refers to the
    unfrozen part for the training. If all we have to train is just one epoch, then
    `setFeatureExtractor()` and the transfer learning helper API will have almost
    the same performance. Let's say we have 100 layers with 99 frozen layers and we
    are doing *N* epochs of training. If we use `setFeatureExtractor()`, then we will
    end up doing a forward pass for those 99 layers *N* times, which essentially takes
    additional time and memory.
  prefs: []
  type: TYPE_NORMAL
- en: In order to save training time, we create the model instance after saving the
    activation results of the frozen layers using the transfer learning helper API.
    This process is also known as featurization. The motive is to skip computations
    for frozen layers and train on unfrozen layers.
  prefs: []
  type: TYPE_NORMAL
- en: As a prerequisite, frozen layers need to be defined using the transfer learning
    builder or explicitly mentioned in the transfer learning helper.
  prefs: []
  type: TYPE_NORMAL
- en: '`TransferLearningHelper` was created in step 3, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding case, we explicitly specified freezing all of the layers up
    to `layer2` in the layer structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 6, we discussed saving the dataset after featurization. After featurization,
    we save the data to disk. We will need to fetch this featurized data to train
    on top of it. Training/evaluation will be easier if we separate it and then save
    it to disk. The dataset can be saved to disk using the `save()` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`saveTodisk()` is the customary way to save a dataset for training or testing.
    The implementation is straightforward as it''s all about creating two different
    directories (train/test) and deciding on the range of files that can be used for
    train/test. We''ll leave that implementation to you. You can refer to our example
    in the GitHub repository (`SaveFeaturizedDataExample.java`): [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/11_Applying%20Transfer%20Learning%20to%20network%20models/sourceCode/cookbookapp/src/main/java/SaveFeaturizedDataExample.java.](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/11_Applying_Transfer_Learning_to_network_models/sourceCode/cookbookapp/src/main/java/SaveFeaturizedDataExample.java)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In steps 7/8, we discussed training our neural network on top of featurized
    data. Our customer retention model follows `MultiLayerNetwork` architecture. This
    training instance will alter the network configuration for the unfrozen layers.
    Hence, we need to evaluate the unfrozen layers. In step 5, we evaluated just the
    model on the featurized test data as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If your network has the `ComputationGraph` structure, then you can use the `unfrozenGraph()`
    method instead of `unfrozenMLN()` to achieve the same result.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some important pre-trained models offered by the DL4J Model Zoo API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VGG16**: VGG-16 referred to in this paper: [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is a very deep convolutional neural network targeting large-scale image
    recognition tasks. We can use transfer learning to train the model further. All
    we have to do is import VGG16 from the model zoo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note that the underlying architecture of the VGG16 model in the DL4J Model Zoo
    API is `ComputationGraph`.
  prefs: []
  type: TYPE_NORMAL
- en: '**TinyYOLO**: TinyYOLO is referred to in this paper: [https://arxiv.org/pdf/1612.08242.pdf](https://arxiv.org/pdf/1612.08242.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is a real-time object detection model for fast and accurate image classification.
    We can apply transfer learning to this model as well after importing from it the
    model zoo, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that the underlying architecture of the TinyYOLO model in the DL4J model
    zoo API is `ComputationGraph`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Darknet19**: Darknet19 is referred to in this paper: [https://arxiv.org/pdf/1612.08242.pdf](https://arxiv.org/pdf/1612.08242.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is also known as YOLOV2, a faster object detection model for real-time
    object detection. We can apply transfer learning to this model after importing
    it from the model zoo, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Fine-tuning the learning configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While performing transfer learning, we might want to update the strategy for
    how weights are initialized, which gradients are updated, which activation functions
    are to be used, and so on. For that purpose, we fine-tune the configuration. In
    this recipe, we will fine-tune the configuration for transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use `FineTuneConfiguration()` to manage modifications in the model configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Call `fineTuneConfiguration()` to fine-tune the model configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw a sample fine-tuning implementation in step 1\. Fine-tuning configurations
    are intended for default/global changes that are applicable across layers. So,
    if we want to remove specific layers from being considered for fine-tuning configuration,
    then we need to make those layers frozen. Unless we do that, all the current values
    for the specified modification type (gradients, activation, and so on) will be
    overridden in the new model.
  prefs: []
  type: TYPE_NORMAL
- en: All the fine-tuning configurations mentioned above will be applied to all unfrozen
    layers, including output layers. So, you might get errors due to the addition
    of the `activation()` and `dropOut()` methods. Dropouts are relevant to hidden
    layers and we may have a different value range for output activation as well.
    A quick fix would be to remove these unless really needed. Otherwise, remove output
    layers from the model using the transfer learning helper API, apply fine-tuning,
    and then add the output layer back with a specific activation.
  prefs: []
  type: TYPE_NORMAL
- en: In step 2, if our original `MultiLayerNetwork` model has convolutional layers,
    then it is possible to make modifications in the convolution mode as well. As
    you might have guessed, this is applicable if you perform transfer learning for
    the image classification model from [Chapter 4](4a688ef9-2dd8-47de-abaf-456fa88bcfc2.xhtml),
    *Building Convolutional Neural Networks*. Also, if your convolutional neural network
    is supposed to run in CUDA-enabled GPU mode, then you can also mention the cuDNN
    algo mode with your transfer learning API. We can specify an algorithmic approach
    (`PREFER_FASTEST`, `NO_WORKSPACE`, or `USER_SPECIFIED`) for cuDNN. It will impact
    the performance and memory usage of cuDNN. Use the `cudnnAlgoMode()` method with
    the `PREFER_FASTEST` mode to achieve performance improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing frozen layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We might want to keep the training instance limited to certain layers, which
    means some layers can be kept frozen for the training instance, so we can focus
    on optimizing other layers while frozen layers are kept unchanged. We saw two
    ways of implementing frozen layers earlier: using the regular transfer learning
    builder and using the transfer learning helper. In this recipe, we will implement
    frozen layers for transfer layers.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define frozen layers by calling `setFeatureExtractor()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Call `fit()` to start the training instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, we used `MultiLayerNetwork` for demonstration purposes. For `MultiLayerNetwork`,
    `featurizeExtractionLayer` refers to the layer number (integer). For `ComputationGraph`, `featurizeExtractionLayer` refers
    to the layer name (`String`). By shifting frozen layer management to the transfer
    learning builder, it can be grouped along with all the other transfer learning
    functions, such as fine-tuning. This gives better modularization. However, the
    transfer learning helper has its own advantages, as we discussed in the previous
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Importing and loading Keras models and layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There can be times when you want to import a model that is not available in
    the DL4J Model Zoo API. You might have created your own model in Keras/TensorFlow,
    or you might be using a pre-trained model from Keras/TensorFlow. Either way, we
    can still load models from Keras/TensorFlow using the DL4J model import API.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe assumes that you already have the Keras model (pre-trained/not
    pre-trained) set up and ready to be imported to DL4J. We will skip the details
    about how to save Keras models to disk as it is beyond the scope of this book.
    Usually, Keras models are stored in `.h5` format, but that isn''t a restriction
    as the model-import API can import from other formats as well. As a prerequisite,
    we need to add the following Maven dependency in `pom.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use `KerasModelImport` to load an external `MultiLayerNetwork` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `KerasModelImport` to load an external `ComputationGraph` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `KerasModelBuilder` to import an external model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In step 1, we used `KerasModelImport` to load the external Keras model from
    disk. If the model was saved separately by calling `model.to_json()` and `model.save_weights()` 
    (in Keras), then we need to use the following variant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`importKerasSequentialModelAndWeights()`: Imports and creates `MultiLayerNetwork`
    from the Keras model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`importKerasModelAndWeights()`: Imports and creates `ComputationGraph` from
    the Keras model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider the following implementation for the `importKerasModelAndWeights()`
    method to perform step 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The third attribute, `enforceTrainConfig`, is a Boolean type, which indicates
    whether to enforce a training configuration or not. Again, if the model was saved separately
    using the `model.to_json()` and `model.save_weights()` Keras calls, then we need
    to use the following variant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In step 3, we discussed how to load `ComputationGraph` from the external model
    using `KerasModelBuilder`. One of the builder methods is `inputShape()`. It assigns
    input shape to the imported Keras model. DL4J requires the input shape to be specified.
    However, you don't have to deal with these if you go for the first two methods,
    discussed earlier, for the Keras model import. Those methods (`importKerasModelAndWeights()` and `importKerasSequentialModelAndWeights()`)
    internally make use of `KerasModelBuilder` to import models.
  prefs: []
  type: TYPE_NORMAL
