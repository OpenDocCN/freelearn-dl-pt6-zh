["```py\ninput_data = np.array([[1,2],[3,4]])\noutput_data = np.array([[3,4],[5,6]])\n```", "```py\n# define model\ninputs1 = Input(shape=(2,1))\nlstm1 = LSTM(1, activation = 'tanh', return_sequences=False,recurrent_initializer='Zeros',recurrent_activation='sigmoid')(inputs1)\nout= Dense(2, activation='linear')(lstm1)\nmodel = Model(inputs=inputs1, outputs=out)\nmodel.summary()\n```", "```py\nmodel.compile(optimizer='adam',loss='mean_squared_error')\nmodel.fit(input_data.reshape(2,2,1), output_data,epochs=1000)\nprint(model.predict(input_data[0].reshape(1,2,1)))\n# [[2.079641 1.8290598]]\n```", "```py\ninput_t0 = 1\ncell_state0 = 0\nforget0 = input_t0*model.get_weights()[0][0][1] + model.get_weights()[2][1]\nforget1 = 1/(1+np.exp(-(forget0)))\ncell_state1 = forget1 * cell_state0\ninput_t0_1 = input_t0*model.get_weights()[0][0][0] + model.get_weights()[2][0]\ninput_t0_2 = 1/(1+np.exp(-(input_t0_1)))\ninput_t0_cell1 = input_t0*model.get_weights()[0][0][2] + model.get_weights()[2][2]\ninput_t0_cell2 = np.tanh(input_t0_cell1)\ninput_t0_cell3 = input_t0_cell2*input_t0_2\ninput_t0_cell4 = input_t0_cell3 + cell_state1\noutput_t0_1 = input_t0*model.get_weights()[0][0][3] + model.get_weights()[2][3]\noutput_t0_2 = 1/(1+np.exp(-output_t0_1))\nhidden_layer_1 = np.tanh(input_t0_cell4)*output_t0_2\ninput_t1 = 2\ncell_state1 = input_t0_cell4\nforget21 = hidden_layer_1*model.get_weights()[1][0][1] + model.get_weights()[2][1] + input_t1*model.get_weights()[0][0][1]\nforget_22 = 1/(1+np.exp(-(forget21)))\ncell_state2 = cell_state1 * forget_22\ninput_t1_1 = input_t1*model.get_weights()[0][0][0] + model.get_weights()[2][0] + hidden_layer_1*model.get_weights()[1][0][0]\ninput_t1_2 = 1/(1+np.exp(-(input_t1_1)))\ninput_t1_cell1 = input_t1*model.get_weights()[0][0][2] + model.get_weights()[2][2]+ hidden_layer_1*model.get_weights()[1][0][2]\ninput_t1_cell2 = np.tanh(input_t1_cell1)\ninput_t1_cell3 = input_t1_cell2*input_t1_2\ninput_t1_cell4 = input_t1_cell3 + cell_state2\noutput_t1_1 = input_t1*model.get_weights()[0][0][3] + model.get_weights()[2][3]+ hidden_layer_1*model.get_weights()[1][0][3]\noutput_t1_2 = 1/(1+np.exp(-output_t1_1))\nhidden_layer_2 = np.tanh(input_t1_cell4)*output_t1_2\nfinal_output = hidden_layer_2 * model.get_weights()[3][0] + model.get_weights()[4]\n```", "```py\n[[2.079 1.829]]\n```", "```py\n# define model\ninputs1 = Input(shape=(2,1))\nlstm1 = LSTM(1, activation = 'tanh', return_sequences=False,recurrent_initializer='Zeros',recurrent_activation='sigmoid')(inputs1)\nout= Dense(1, activation='linear')(lstm1)\nmodel = Model(inputs=inputs1, outputs=out)\nmodel.summary()\n```", "```py\nmodel.compile(optimizer='adam',loss='mean_squared_error')\nmodel.fit(input_data.reshape(2,2,1), output_data.reshape(2,2,1),epochs=1000)\n```", "```py\nprint(model.predict(input_data[0].reshape(1,2,1)))\n```", "```py\n[[[1.7584195] [2.2500749]]]\n```", "```py\ninput_t0 = 1\ncell_state0 = 0\nforget0 = input_t0*model.get_weights()[0][0][1] + model.get_weights()[2][1]\nforget1 = 1/(1+np.exp(-(forget0)))\ncell_state1 = forget1 * cell_state0\ninput_t0_1 = input_t0*model.get_weights()[0][0][0] + model.get_weights()[2][0]\ninput_t0_2 = 1/(1+np.exp(-(input_t0_1)))\ninput_t0_cell1 = input_t0*model.get_weights()[0][0][2] + model.get_weights()[2][2]\ninput_t0_cell2 = np.tanh(input_t0_cell1)\ninput_t0_cell3 = input_t0_cell2*input_t0_2\ninput_t0_cell4 = input_t0_cell3 + cell_state1\noutput_t0_1 = input_t0*model.get_weights()[0][0][3] + model.get_weights()[2][3]\noutput_t0_2 = 1/(1+np.exp(-output_t0_1))\nhidden_layer_1 = np.tanh(input_t0_cell4)*output_t0_2\nfinal_output_1 = hidden_layer_1 * model.get_weights()[3][0] + model.get_weights()[4]\nfinal_output_1\n*# 1.7584*\n```", "```py\ninput_t1 = 2\ncell_state1 = input_t0_cell4\nforget21 = hidden_layer_1*model.get_weights()[1][0][1] + model.get_weights()[2][1] + input_t1*model.get_weights()[0][0][1]\nforget_22 = 1/(1+np.exp(-(forget21)))\ncell_state2 = cell_state1 * forget_22\ninput_t1_1 = input_t1*model.get_weights()[0][0][0] + model.get_weights()[2][0] + hidden_layer_1*model.get_weights()[1][0][0]\ninput_t1_2 = 1/(1+np.exp(-(input_t1_1)))\ninput_t1_cell1 = input_t1*model.get_weights()[0][0][2] + model.get_weights()[2][2]+ hidden_layer_1*model.get_weights()[1][0][2]\ninput_t1_cell2 = np.tanh(input_t1_cell1)\ninput_t1_cell3 = input_t1_cell2*input_t1_2\ninput_t1_cell4 = input_t1_cell3 + cell_state2\noutput_t1_1 = input_t1*model.get_weights()[0][0][3] + model.get_weights()[2][3]+ hidden_layer_1*model.get_weights()[1][0][3]\noutput_t1_2 = 1/(1+np.exp(-output_t1_1))\nhidden_layer_2 = np.tanh(input_t1_cell4)*output_t1_2\nfinal_output_2 = hidden_layer_2 * model.get_weights()[3][0] + model.get_weights()[4]\nfinal_output_2\n*# 2.250*\n```", "```py\n\ninputs1 = Input(shape=(2,1))\nlstm1,state_h,state_c = LSTM(1, activation = 'tanh', return_sequences=True, return_state = True, recurrent_initializer='Zeros',recurrent_activation='sigmoid')(inputs1)\nmodel = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])\n\n```", "```py\nprint(model.predict(input_data[0].reshape(1,2,1)))\n```", "```py\n[array([[[-0.256911 ], [-0.6683883]]], dtype=float32), array([[-0.6683883]], dtype=float32), array([[-0.96862674]], dtype=float32)]\n```", "```py\ninput_t0 = 1\ncell_state0 = 0\nforget0 = input_t0*model.get_weights()[0][0][1] + model.get_weights()[2][1]\nforget1 = 1/(1+np.exp(-(forget0)))\ncell_state1 = forget1 * cell_state0\ninput_t0_1 = input_t0*model.get_weights()[0][0][0] + model.get_weights()[2][0]\ninput_t0_2 = 1/(1+np.exp(-(input_t0_1)))\ninput_t0_cell1 = input_t0*model.get_weights()[0][0][2] + model.get_weights()[2][2]\ninput_t0_cell2 = np.tanh(input_t0_cell1)\ninput_t0_cell3 = input_t0_cell2*input_t0_2\ninput_t0_cell4 = input_t0_cell3 + cell_state1\noutput_t0_1 = input_t0*model.get_weights()[0][0][3] + model.get_weights()[2][3]\noutput_t0_2 = 1/(1+np.exp(-output_t0_1))\nhidden_layer_1 = np.tanh(input_t0_cell4)*output_t0_2\nprint(hidden_layer_1)\n```", "```py\ninput_t1 = 2\ncell_state1 = input_t0_cell4\nforget21 = hidden_layer_1*model.get_weights()[1][0][1] + model.get_weights()[2][1] + input_t1*model.get_weights()[0][0][1]\nforget_22 = 1/(1+np.exp(-(forget21)))\ncell_state2 = cell_state1 * forget_22\ninput_t1_1 = input_t1*model.get_weights()[0][0][0] + model.get_weights()[2][0] + hidden_layer_1*model.get_weights()[1][0][0]\ninput_t1_2 = 1/(1+np.exp(-(input_t1_1)))\ninput_t1_cell1 = input_t1*model.get_weights()[0][0][2] + model.get_weights()[2][2]+ hidden_layer_1*model.get_weights()[1][0][2]\ninput_t1_cell2 = np.tanh(input_t1_cell1)\ninput_t1_cell3 = input_t1_cell2*input_t1_2\ninput_t1_cell4 = input_t1_cell3 + cell_state2\noutput_t1_1 = input_t1*model.get_weights()[0][0][3] + model.get_weights()[2][3]+ hidden_layer_1*model.get_weights()[1][0][3]\noutput_t1_2 = 1/(1+np.exp(-output_t1_1))\nhidden_layer_2 = np.tanh(input_t1_cell4)*output_t1_2\nprint(hidden_layer_2, input_t1_cell4)\n```", "```py\ninputs1 = Input(shape=(2,1))\nlstm1,state_fh,state_fc,state_bh,state_bc = Bidirectional(LSTM(1, activation = 'tanh', return_sequences=True, return_state = True, recurrent_initializer='Zeros',recurrent_activation='sigmoid'))(inputs1)\nmodel = Model(inputs=inputs1, outputs=[lstm1, state_fh,state_fc,state_bh,state_bc])\nmodel.summary()\n```", "```py\n!wget https://www.dropbox.com/s/qpw1wnmho8v0gi4/atis.zip\n!unzip atis.zip\n```", "```py\nimport numpy as np \nimport pandas as pd\nimport pickle\nDATA_DIR=\"/content\"\ndef load_ds(fname='atis.train.pkl'):\n     with open(fname, 'rb') as stream:\n     ds,dicts = pickle.load(stream)\n     print('Done loading: ', fname)\n     print(' samples: {:4d}'.format(len(ds['query'])))\n     print(' vocab_size: {:4d}'.format(len(dicts['token_ids'])))\n     print(' slot count: {:4d}'.format(len(dicts['slot_ids'])))\n     print(' intent count: {:4d}'.format(len(dicts['intent_ids'])))\n     return ds,dicts\n```", "```py\nimport os\ntrain_ds, dicts = load_ds(os.path.join(DATA_DIR,'atis.train.pkl'))\ntest_ds, dicts = load_ds(os.path.join(DATA_DIR,'atis.test.pkl'))\n```", "```py\nt2i, s2i, in2i = map(dicts.get, ['token_ids', 'slot_ids','intent_ids'])\ni2t, i2s, i2in = map(lambda d: {d[k]:k for k in d.keys()}, [t2i,s2i,in2i])\nquery, slots, intent = map(train_ds.get, ['query', 'slot_labels', 'intent_labels'])\n```", "```py\nfor j in range(len(query[i])):\n        print('{:>33} {:>40}'.format(i2t[query[i][j]],\n                                     i2s[slots[i][j]]))\n```", "```py\ni2t2 = []\ni2s2 = []\nc_intent=[]\nfor i in range(4978):\n     a_query = []\n     b_slot = []\n     c_intent.append(i2in[intent[i][0]])\n     for j in range(len(query[i])):\n         a_query.append(i2t[query[i][j]])\n         b_slot.append(i2s[slots[i][j]])\n     i2t2.append(a_query)\n     i2s2.append(b_slot)\ni2t2 = np.array(i2t2)\ni2s2 = np.array(i2s2)\ni2in2 = np.array(c_intent)\n```", "```py\nfinal_sentences = []\nfinal_targets = []\nfinal_docs = []\nfor i in range(len(i2t2)):\n  tokens = ''\n  entities = ''\n  intent = ''\n  for j in range(len(i2t2[i])):\n    tokens= tokens + i2t2[i][j] + ' '\n    entities = entities + i2s2[i][j] +' '\n  intent = i2in2[i]\n  final_sentences.append(tokens)\n  final_targets.append(entities)\n  final_docs.append(intent)\n```", "```py\nfrom collections import Counter\ncounts = Counter()\nfor i,sentence in enumerate(final_sentences):\n     counts.update(sentence.split())\nsentence_words = sorted(counts, key=counts.get, reverse=True)\nchars = sentence_words\nnb_chars = len(chars)\nsentence_word_to_int = {word: i for i, word in enumerate(sentence_words, 1)}\nsentence_int_to_word = {i: word for i, word in enumerate(sentence_words, 1)}\nmapped_reviews = []\nfor review in final_sentences:\n     mapped_reviews.append([sentence_word_to_int[word] for word in review.split()])\n```", "```py\nfrom collections import Counter\ncounts = Counter()\nfor i,sentence in enumerate(final_targets):\n    counts.update(sentence.split())\ntarget_words = sorted(counts, key=counts.get, reverse=True)\nchars = target_words\nnb_chars = len(target_words)\n```", "```py\ntarget_word_to_int = {word: i for i, word in enumerate(target_words, 1)}\ntarget_int_to_word = {i: word for i, word in enumerate(target_words, 1)}\nmapped_targets = []\nfor review in final_targets:\n    mapped_targets.append([target_word_to_int[word] for word in review.split()])\n```", "```py\nfrom keras.preprocessing.sequence import pad_sequences\ny = pad_sequences(maxlen=124, sequences=mapped_targets, padding=\"post\", value=0)\nfrom keras.utils import to_categorical\ny2 = [to_categorical(i, num_classes=124) for i in y]\ny3 = np.array(y2)\n```", "```py\nlength_sent = []\nfor i in range(len(mapped_reviews)):\n     a = mapped_reviews[i]\n     b = len(a)\n     length_sent.append(b)\nnp.max(length_sent)\n```", "```py\nfrom keras.preprocessing.sequence import pad_sequences\nX = pad_sequences(maxlen=50, sequences=mapped_reviews, padding=\"post\", value=0)\nY = pad_sequences(maxlen=50, sequences=mapped_targets, padding=\"post\", value=0)\n```", "```py\nfrom keras.utils import to_categorical\ny2 = [to_categorical(i, num_classes=124) for i in Y]\ny2 = np.array(y2)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y2, test_size=0.30,random_state=10)\n```", "```py\ninput = Input(shape=(50,))\nmodel = Embedding(input_dim=891, output_dim=32, input_length=50)(input)\nmodel = Dropout(0.1)(model)\nmodel = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\nout = (Dense(124, activation=\"softmax\"))(model)\n```", "```py\nmodel = Model(input, out)\nmodel.summary()\n```", "```py\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.fit(X_train,y_train, batch_size=32, epochs=5, validation_data = (X_test,y_test), verbose=1)\n```", "```py\nfrom collections import Counter\ncounts = Counter()\nfor i,sentence in enumerate(final_docs):\n     counts.update(sentence.split())\nintent_words = sorted(counts, key=counts.get, reverse=True)\nchars = intent_words\nnb_chars = len(intent_words)\nintent_word_to_int = {word: i for i, word in enumerate(intent_words, 1)}\nintent_int_to_word = {i: word for i, word in enumerate(intent_words, 1)}\nmapped_docs = []\nfor review in final_docs:\n     mapped_docs.append([intent_word_to_int[word] for word in review.split()])\n```", "```py\nfrom keras.utils import to_categorical\ndoc2 = [to_categorical(i[0], num_classes=23) for i in mapped_docs]\ndoc3 = np.array(doc2)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,doc3, test_size=0.30,random_state=10)\n```", "```py\ninput = Input(shape=(50,))\nmodel2 = Embedding(input_dim=891, output_dim=32, input_length=50)(input)\nmodel2 = Dropout(0.1)(model2)\nmodel2 = Bidirectional(LSTM(units=100))(model2)\nout = (Dense(23, activation=\"softmax\"))(model2)\nmodel2 = Model(input, out)\n```", "```py\nmodel2.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n```", "```py\nmodel2.fit(X_train,y_train, batch_size=32, epochs=5, validation_data = (X_test,y_test), verbose=1)\n```", "```py\ndef preprocessing(text):\n     text2 = text.split()\n     a=[]\n     for i in range(len(text2)):\n         a.append(sentence_word_to_int[text2[i]])\n     return a\n```", "```py\ntext = \"BOS i would fly from boston to dallas EOS\"\n```", "```py\nindexed_text = preprocessing(text)\npadded_text=np.zeros(50)\npadded_text[:len(indexed_text)]=indexed_text\npadded_text=padded_text.reshape(1,50)\n```", "```py\npred_index_intent = np.argmax(model2.predict(c),axis=1)\nentity_int_to_word[pred_index_intent[0]]\n```", "```py\npred_entities = np.argmax(model.predict(padded_text),axis=2)\n\nfor i in range(len(pred_entities[0])):\n      if pred_entities[0][i]>1:\n            print('word: ',text.split()[i], 'entity: ',target_int_to_word[pred_entities[0][i]])\n\n```", "```py\nimport pandas as pd\nimport numpy as np\nimport string\nfrom string import digits\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, Dense\nimport numpy as np\n```", "```py\n$ wget https://www.dropbox.com/s/2vag8w6yov9c1qz/english%20to%20french.txt\n```", "```py\nlines= pd.read_table('english to french.txt', names=['eng', 'fr'])\n```", "```py\nlines = lines[0:50000]\n```", "```py\nlines.eng=lines.eng.apply(lambda x: x.lower())\nlines.fr=lines.fr.apply(lambda x: x.lower())\nexclude = set(string.punctuation)\nlines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\nlines.fr=lines.fr.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n```", "```py\nlines.fr = lines.fr.apply(lambda x : 'start '+ x + ' end')\n```", "```py\n# fit a tokenizer\nfrom keras.preprocessing.text import Tokenizer\nimport json\nfrom collections import OrderedDict\ndef create_tokenizer(lines):\n     tokenizer = Tokenizer()\n     tokenizer.fit_on_texts(lines)\n     return tokenizer\n```", "```py\neng_tokenizer = create_tokenizer(lines.eng)\noutput_dict = json.loads(json.dumps(eng_tokenizer.word_counts))\ndf =pd.DataFrame([output_dict.keys(), output_dict.values()]).T\ndf.columns = ['word','count']\ndf = df.sort_values(by='count',ascending = False)\ndf['cum_count']=df['count'].cumsum()\ndf['cum_perc'] = df['cum_count']/df['cum_count'].max()\nfinal_eng_words = df[df['cum_perc']<0.8]['word'].values\n```", "```py\nfr_tokenizer = create_tokenizer(lines.fr)\noutput_dict = json.loads(json.dumps(fr_tokenizer.word_counts))\ndf =pd.DataFrame([output_dict.keys(), output_dict.values()]).T\ndf.columns = ['word','count']\ndf = df.sort_values(by='count',ascending = False)\ndf['cum_count']=df['count'].cumsum()\ndf['cum_perc'] = df['cum_count']/df['cum_count'].max()\nfinal_fr_words = df[df['cum_perc']<0.8]['word'].values\n```", "```py\ndef filter_eng_words(x):\n     t = []\n     x = x.split()\n     for i in range(len(x)):\n         if x[i] in final_eng_words:\n             t.append(x[i])\n         else:\n             t.append('unk')\n     x3 = ''\n     for i in range(len(t)):\n         x3 = x3+t[i]+' '\n     return x3\n```", "```py\ndef filter_fr_words(x):\n     t = []\n     x = x.split()\n     for i in range(len(x)):\n         if x[i] in final_fr_words:\n             t.append(x[i])\n         else:\n             t.append('unk')\n     x3 = ''\n     for i in range(len(t)):\n         x3 = x3+t[i]+' '\n     return x3\n```", "```py\nfilter_eng_words('he is extremely good')\n```", "```py\nlines['fr']=lines['fr'].apply(filter_fr_words)\nlines['eng']=lines['eng'].apply(filter_eng_words)\n```", "```py\nall_eng_words=set()\nfor eng in lines.eng:\n     for word in eng.split():\n         if word not in all_eng_words:\n             all_eng_words.add(word)\n\nall_french_words=set()\nfor fr in lines.fr:\n     for word in fr.split():\n         if word not in all_french_words:\n             all_french_words.add(word)\n```", "```py\ninput_words = sorted(list(all_eng_words))\ntarget_words = sorted(list(all_french_words))\nnum_encoder_tokens = len(all_eng_words)\nnum_decoder_tokens = len(all_french_words)\n```", "```py\ninput_token_index = dict( [(word, i+1) for i, word in enumerate(input_words)])\ntarget_token_index = dict( [(word, i+1) for i, word in enumerate(target_words)])\n```", "```py\nlength_list=[]\nfor l in lines.fr:\n     length_list.append(len(l.split(' ')))\nfr_max_length = np.max(length_list)\n```", "```py\nlength_list=[]\nfor l in lines.eng:\n     length_list.append(len(l.split(' ')))\neng_max_length = np.max(length_list)\n```", "```py\nencoder_input_data = np.zeros((len(lines.eng), fr_max_length),dtype='float32')\ndecoder_input_data = np.zeros((len(lines.fr), fr_max_length),dtype='float32')\ndecoder_target_data = np.zeros((len(lines.fr), fr_max_length, num_decoder_tokens+1),dtype='float32')\n```", "```py\nfor i, (input_text, target_text) in enumerate(zip(lines.eng, lines.fr)):\n     for t, word in enumerate(input_text.split()):\n         encoder_input_data[i, t] = input_token_index[word]\n     for t, word in enumerate(target_text.split()):\n # decoder_target_data is ahead of decoder_input_data by one timestep\n         decoder_input_data[i, t] = target_token_index[word]\n         if t>0: \n # decoder_target_data will be ahead by one timestep\n # and will not include the start character.\n             decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n         if t== len(target_text.split())-1:\n             decoder_target_data[i, t:, 89] = 1\n```", "```py\nfor i in range(decoder_input_data.shape[0]):\n     for j in range(decoder_input_data.shape[1]):\n         if(decoder_input_data[i][j]==0):\n             decoder_input_data[i][j] = 89\n```", "```py\nprint(decoder_input_data.shape,encoder_input_data.shape,decoder_target_data.shape)\n```", "```py\n(50000, 17) (50000, 17) (50000, 17, 359)\n```", "```py\nmodel = Sequential()\nmodel.add(Embedding(len(input_words)+1, 128, input_length=fr_max_length, mask_zero=True))\nmodel.add((Bidirectional(LSTM(256, return_sequences = True))))\nmodel.add((LSTM(256, return_sequences=True)))\nmodel.add((Dense(len(target_token_index)+1, activation='softmax')))\n```", "```py\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n```", "```py\nmodel.fit(encoder_input_data, decoder_target_data,\n batch_size=32, epochs=5, validation_split=0.05)\n```", "```py\ncount = 0\ncorrect_count = 0\npred = model2.predict(encoder_input_data[47500:])\nfor i in range(2500):\n  t = np.argmax(pred[i], axis=1)\n  act = np.argmax(decoder_target_data[47500],axis=1)\n  correct_count += np.sum((act==t) & (act!=89))\n  count += np.sum(act!=89)\ncorrect_count/count\n# 0.19\n```", "```py\nencoder_input_data = np.zeros(\n    (len(lines.eng), eng_max_length),\n    dtype='float32')\ndecoder_input_data = np.zeros(\n    (len(lines.fr), fr_max_length),\n    dtype='float32')\ndecoder_target_data = np.zeros(\n    (len(lines.fr), fr_max_length, num_decoder_tokens+1),\n    dtype='float32')\n\nfor i, (input_text, target_text) in enumerate(zip(lines.eng, lines.fr)):\n    for t, word in enumerate(input_text.split()):\n        encoder_input_data[i, t] = input_token_index[word]\n    for t, word in enumerate(target_text.split()):\n        # decoder_target_data is ahead of decoder_input_data by one timestep\n        decoder_input_data[i, t] = target_token_index[word]\n        if t>0: \n            # decoder_target_data will be ahead by one timestep\n            # and will not include the start character.\n          decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n          if t== len(target_text.split())-1:\n            decoder_target_data[i, t:, 89] = 1\n\nfor i in range(decoder_input_data.shape[0]):\n  for j in range(decoder_input_data.shape[1]):\n    if(decoder_input_data[i][j]==0):\n      decoder_input_data[i][j] = 89 \n```", "```py\nmodel2 = Sequential()\nmodel2.add(Embedding(len(input_words)+1, 128, input_length=eng_max_length, mask_zero=True))\nmodel2.add((Bidirectional(LSTM(256))))\nmodel2.add(RepeatVector(fr_max_length))\nmodel2.add((LSTM(256, return_sequences=True)))\nmodel2.add((Dense(len(target_token_index)+1, activation='softmax')))\n```", "```py\nmodel2.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\nmodel2.fit(encoder_input_data, decoder_target_data,\n batch_size=128,epochs=5,validation_split=0.05)\n```", "```py\ncount = 0\ncorrect_count = 0\npred = model2.predict(encoder_input_data[47500:])\nfor i in range(2500):\n  t = np.argmax(pred[i], axis=1)\n  act = np.argmax(decoder_target_data[47500],axis=1)\n  correct_count += np.sum((act==t) & (act!=89))\n  count += np.sum(act!=89)\ncorrect_count/count\n```", "```py\n# We shall convert each word into a 128 sized vector\nembedding_size = 128\n```", "```py\nencoder_inputs = Input(shape=(None,))\nen_x= Embedding(num_encoder_tokens+1, embedding_size)(encoder_inputs)\nencoder = LSTM(256, return_state=True)\nencoder_outputs, state_h, state_c = encoder(en_x)\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]\n```", "```py\ndecoder_inputs = Input(shape=(None,))\ndex= Embedding(num_decoder_tokens+1, embedding_size)\nfinal_dex= dex(decoder_inputs)\ndecoder_lstm = LSTM(256, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(final_dex, initial_state=encoder_states)\ndecoder_outputs = Dense(2000,activation='tanh')(decoder_outputs)\ndecoder_dense = Dense(num_decoder_tokens+1, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n```", "```py\nmodel3 = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n```", "```py\nhistory3 = model3.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n batch_size=32,epochs=5,validation_split=0.05)\n```", "```py\nact = np.argmax(decoder_target_data, axis=2)\n```", "```py\ncount = 0\ncorrect_count = 0\npred = model3.predict([encoder_input_data[47500:],decoder_input_data[47500:]])\nfor i in range(2500):\n     t = np.argmax(pred[i], axis=1)\n     correct_count += np.sum((act[47500+i]==t) & (act[47500+i]!=0))\n     count += np.sum(decoder_input_data[47500+i]!=0)\ncorrect_count/count\n```", "```py\ndecoder_input_data_pred = np.zeros((len(lines.fr),fr_max_length),dtype='float32')\nfinal_pred = []\nfor i in range(2500):\nword = 284\n     for j in range(17):\n         decoder_input_data_pred[(47500+i), j] = word\n         pred =         model3.predict([encoder_input_data[(47500+i)].reshape(1,8),decoder_input_data_pred[47500+i].reshape(1,17)])\n         t = np.argmax(pred[0][j])\n         word = t\n         if word==89:\n             break\n     final_pred.append(list(decoder_input_data_pred[47500+i]))\n```", "```py\nfinal_pred2 = np.array(final_pred)\n```", "```py\ncount = 0\ncorrect_count = 0\nfor i in range(2500):\n     correct_count += np.sum((decoder_input_data[47500+i]==final_pred2[i]) & (decoder_input_data[47500+i]!=89))\n     count += np.sum(decoder_input_data[47500+i]!=89)\ncorrect_count/count\n```", "```py\nencoder_inputs = Input(shape=(eng_max_length,))\nen_x= Embedding(num_encoder_tokens+1, embedding_size)(encoder_inputs)\nen_x = Dropout(0.1)(en_x)\nencoder = LSTM(256, return_sequences=True, unroll=True)(en_x)\nencoder_last = encoder[:,-1,:]\n```", "```py\ndecoder_inputs = Input(shape=(fr_max_length,))\ndex= Embedding(num_decoder_tokens+1, embedding_size)\ndecoder= dex(decoder_inputs)\ndecoder = Dropout(0.1)(decoder)\ndecoder = LSTM(256, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n```", "```py\nt = Dense(5000, activation='tanh')(decoder)\nt2 = Dense(5000, activation='tanh')(encoder)\nattention = dot([t, t2], axes=[2, 2])\n```", "```py\nattention = Dense(eng_max_length, activation='tanh')(attention)\nattention = Activation('softmax')(attention)\n```", "```py\ncontext = dot([attention, encoder], axes = [2,1])\n```", "```py\ndecoder_combined_context = concatenate([context, decoder])\n```", "```py\noutput_dict_size = num_decoder_tokens+1\ndecoder_combined_context=Dense(2000, activation='tanh')(decoder_combined_context)\noutput=(Dense(output_dict_size, activation=\"softmax\"))(decoder_combined_context)\n```", "```py\nmodel4 = Model(inputs=[encoder_inputs, decoder_inputs], outputs=[output])\nmodel4.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])\n```", "```py\nmodel4.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n batch_size=32,epochs=5,validation_split=0.05)\n```", "```py\ndecoder_input_data_pred=np.zeros((len(lines.fr), fr_max_length), dtype='float32')\n```", "```py\nfinal_pred_att = []\nfor i in range(2500):\n     word = 284\n     for j in range(17):\n         decoder_input_data_pred[(47500+i), j] = word\n         pred =         model4.predict([encoder_input_data[(47500+i)].reshape(1,8),decoder_input_data_pred[47500+i].reshape(1,17)])\n         t = np.argmax(pred[0][j])\n         word = t\n         if word==89:\n             break\n     final_pred_att.append(list(decoder_input_data_pred[47500+i]))\n```", "```py\nfinal_pred2_att = np.array(final_pred_att)\ncount = 0\ncorrect_count = 0\nfor i in range(2500):\n     correct_count += np.sum((decoder_input_data[47500+i]==final_pred2_att[i]) & (decoder_input_data[47500+i]!=89))\n     count += np.sum(decoder_input_data[47500+i]!=89)\ncorrect_count/count\n```", "```py\nk = -1500\nt = model4.predict([encoder_input_data[k].reshape(1,encoder_input_data.shape[1]),decoder_input_data[k].reshape(1,decoder_input_data.shape[1])]).reshape(decoder_input_data.shape[1], num_decoder_tokens+1)\n```", "```py\nt2 = np.argmax(t,axis=1)\nfor i in range(len(t2)):\n     if int(t2[i])!=0:\n         print(list(target_token_index.keys())[int(t2[i]-1)])\n```", "```py\nje unk manger pas manger end end\n```", "```py\nt2 = decoder_input_data[k]\nfor i in range(len(t2)):\n     if int(t2[i])!=89:\n         print(list(target_token_index.keys())[int(t2[i]-1)])\n```", "```py\n je unk ne pas manger ça end\n```"]