- en: Deep Learning Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I am going to introduce the core concepts of **Deep Learning**
    (**DL**), the relationship it has with **Machine Learning** (**ML**) and **Artificial
    Intelligence** (**AI**), the different types of multilayered neural networks,
    and a list of real-world practical applications. I will try to skip mathematical
    equations as much as possible and keep the description very high level, with no
    reference to code examples. The goal of this chapter is to make readers aware
    of what DL really is and what you can do with it, while the following chapters
    will go much more into the details of this, with lots of practical code examples
    in Scala and Python (where this programming language can be used).
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: DL concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep neural networks** (**DNNs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical applications of DL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing DL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DL is a subset of ML that can solve particularly hard and large-scale problems
    in areas such as **Natural Language Processing** (**NLP**) and image classification.
    The expression DL is sometimes used in an interchangeable way with ML and AI,
    but both ML and DL are subsets of AI. AI is the broader concept that is implemented
    through ML. DL is a way of implementing ML, and involves neural network-based
    algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/127ce50b-0d72-4411-84c9-51b9f69d9eca.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1
  prefs: []
  type: TYPE_NORMAL
- en: AI is considered the ability of a machine (it could be any computer-controlled
    device or robot) to perform tasks that are typically associated with humans. It
    was introduced in the 1950s, with the goal of reducing human interaction, thereby
    making the machine do all the work. This concept is mainly applied to the development
    of systems that typically require human intellectual processes and/or the ability
    to learn from past experiences.
  prefs: []
  type: TYPE_NORMAL
- en: 'ML is an approach that''s used to implement AI. It is a field of computer science
    that gives computer systems the ability to learn from data without being explicitly
    programmed. Basically, it uses algorithms to find patterns in data and then uses
    a model that recognizes those patterns to make predictions on new data. The following
    diagram shows the typical process that''s used to train and build a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/889161c2-bb5f-492b-8a5b-3acdb5deb12d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2
  prefs: []
  type: TYPE_NORMAL
- en: 'ML can be classified into three types:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning algorithms, which use labeled data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms, which find patterns, starting from unlabeled
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semi-supervised learning, which uses a mix of the two (labeled and unlabeled
    data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing, supervised learning is the most common type of ML algorithm.
    Supervised learning can be divided into two groups – regression and classification
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows a simple regression problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c36b426-c6d8-4403-b913-662050ffd368.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are two inputs (or features), **Size** and **Price**,
    which are used to generate a curve-fitting line and make subsequent predictions
    of the property price.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows an example of supervised classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/acc798aa-87f2-4f2c-98e4-145c0bcb6373.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is labeled with benign (circles) and malignant (crosses) tumors
    for breast cancer patients. A supervised classification algorithm attempts, by
    fitting a line through the data, to part the tumors into two different classifications.
    Future data would then be classified as benign or malignant based on that straight-line
    classification. The case in the preceding graph has only two discrete outputs,
    but there are cases where there could be more than two classifications as well
  prefs: []
  type: TYPE_NORMAL
- en: 'While in supervised learning, labeled datasets help the algorithm determine
    what the correct answer is, in unsupervised learning, an algorithm is provided
    with an unlabeled dataset and depends on the algorithm itself to uncover structures
    and patterns in the data. In the following graphs (the graph on the right can
    be found at [https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/Images/supervised_unsupervised.png)[Images/supervised_unsupervised.png](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/Images/supervised_unsupervised.png)),
    no information is provided about the meaning of each data point. We ask the algorithm
    to find a structure in the data in a way that is independent of supervision. An
    unsupervised learning algorithm could find that there are two distinct clusters
    and then perform straight-line classification between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d705080d-f433-4b30-af21-182ca64e5d72.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5
  prefs: []
  type: TYPE_NORMAL
- en: '**DL** is the name for multilayered neural networks, which are networks that
    are composed of several hidden layers of nodes between the input and output. DL
    is a refinement of **Artificial Neural Networks** (**ANNs**), which emulate how
    the human brain learns (even if not closely) and how it solves problems. ANNs
    consist of an interconnected group of neurons, similar to the way neurons work
    in the human brain. The following diagram represents the general model of ANNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f6589c0-3379-4137-aac8-dcb67a14a15b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6
  prefs: []
  type: TYPE_NORMAL
- en: 'A neuron is the atomic unit of an ANN. It receives a given number of input
    (*x[i]*) before executing computation on it and finally sends the output to other
    neurons in the same network. The weights (*w[j]*), or *parameters*, represent
    the strength of the input connection – they can assume positive or negative values.
    The net input can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y[in] = x[1] X w[1] + x[2] X w[2] + x[3] X w[3] + … + x[n] X w[n]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output can be calculated by applying the activation function over the net
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = f(y[in])*'
  prefs: []
  type: TYPE_NORMAL
- en: The activation function allows an ANN to model complex non-linear patterns that
    simpler models may not represent correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram represents a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77e12f41-77b6-40ad-b0a9-6279e6220ca3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7
  prefs: []
  type: TYPE_NORMAL
- en: The first layer is the input layer – this is where features are put into the
    network. The last one is the output layer. Any layer in between that is not an
    input or output layer is a hidden layer. The term DL is used because of the multiple
    levels of hidden layers in neural networks that are used to resolve complex non-linear
    problems. At each layer level, any single node receives input data and a weight,
    and will then output a confidence score to the nodes of the next layer. This process
    happens until the output layer is reached. The error of the score is calculated
    on that layer. The errors are then sent back and the weights of the network are
    adjusted to improve the model (this is called **backpropagation** and happens
    inside a process called **gradient descent**, which we will discuss in [Chapter
    6](f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml), *Recurrent Neural Networks*).
    There are many variations of neural networks – more about them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, a final observation. You''re probably wondering why most
    of the concepts behind AI, ML, and DL have been around for decades, but have only
    been hyped up in the past 4 or 5 years. There are several factors that accelerated
    their implementation and made it possible to move them from theory to real-world
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cheaper computation**: In the last few decades, hardware has been a constraining
    factor for AI/ML/DL. Recent advances in both hardware (coupled with improved tools
    and software frameworks) and new computational models (including those around
    GPUs) have accelerated AI/ML/DL adoption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Greater data availability**: AI/ML/DL needs a huge amount of data to learn.
    The digital transformation of society is providing tons of raw material to move
    forward quickly. Big data now comes from diverse sources such as IoT sensors,
    social and mobile computing, smart cars, healthcare devices, and many others that
    are or will be used to train models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cheaper storage**: The increased amount of available data means that more
    space is needed for storage. Advances in hardware, cost reduction, and improved
    performance have made the implementation of new storage systems possible, all
    without the typical limitations of relational databases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More advanced algorithms**: Less expensive computation and storage enable
    the development and training of more advanced algorithms that also have impressive
    accuracy when solving specific problems such as image classification and fraud
    detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More, and bigger, investments**: Last but not least, investment in AI is
    no longer confined to universities or research institutes, but comes from many
    other entities, such as tech giants, governments, start-ups, and large enterprises
    across almost every business area.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNNs overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As stated in the previous section, a DNN is an ANN with multiple hidden layers
    between the input and output layers. Typically, they are feedforward networks
    in which data flows from the input layer to the output layer without looping back,
    but there are different flavors of DNNs – among them, those with the most practical
    applications are **Convolutional Neural Networks** (**CNNs**) and **Recurrent
    Neural Networks** (**RNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most common use case scenarios of CNNs are all to do with image processing,
    but are not restricted to other types of input, whether it be audio or video.
    A typical use case is image classification – the network is fed with images so
    that it can classify the data. For example, it outputs a lion if you give it a
    lion picture, a tiger when you give it a tiger picture, and so on. The reason
    why this kind of network is used for image classification is because it uses relatively
    little preprocessing compared to other algorithms in the same space – the network
    learns the filters that, in traditional algorithms, were hand-engineered.
  prefs: []
  type: TYPE_NORMAL
- en: Being a multilayered neural network, A CNN consists of an input and an output
    layer, as well as multiple hidden layers. The hidden layers can be convolutional,
    pooling, fully connected, and normalization layers. Convolutional layers apply
    a convolution operation ([https://en.wikipedia.org/wiki/Convolution](https://en.wikipedia.org/wiki/Convolution))
    to an input, before passing the result to the next layer. This operation emulates
    how the response of an individual physical neuron to a visual stimulus is generated.
    Each convolutional neuron processes only the data for its receptive field (which
    is the particular region of the sensory space of an individual sensory neuron
    in which a change in the environment will modify the firing of that neuron). Pooling
    layers are responsible for combining the outputs of clusters of neurons in a layer
    into a single neuron in the next layer. There are different implementations of
    poolings—max pooling, which uses the maximum value from each cluster from the
    prior layer; average pooling, which uses the average value from any cluster of
    neurons on the prior layer; and so on. Fully connected layers, instead, as you
    will clearly realize from their name, connect every neuron in a layer to every
    other neuron in another layer.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs don't parse all the training data at once, but they usually start with
    a sort of input scanner. For example, consider an image of 200 x 200 pixels as
    input. In this case, the model doesn't have a layer with 40,000 nodes, but a scanning
    input layer of 20 x 20, which is fed using the first 20 x 20 pixels of the original
    image (usually, starting in the upper-left corner). Once we have passed that input
    (and possibly used it for training), we feed it using the next 20 x 20 pixels
    (this will be explained better and in a more detailed manner in [Chapter 5](fbec1d8a-a92f-4899-af0f-11f3d545e0eb.xhtml),
    *Convolutional Neural Networks*; the process is similar to the movement of a scanner,
    one pixel to the right). Please note that the image isn't dissected into 20 x
    20 blocks, but the scanner moves over it. This input data is then fed through
    one or more convolutional layers. Each node of those layers only has to work with
    its close neighboring cells—not all of the nodes are connected to each other.
    The deeper a network becomes, the more its convolutional layers shrink, typically
    following a divisible factor of the input (if we started with a layer of 20, then,
    most probably, the next one would be a layer of 10 and the following a layer of
    5). Powers of two are commonly used as divisible factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram (by Aphex34—own work, CC BY-SA 4.0, [https://commons.wikimedia.org/w/index.php?curid=45679374](https://commons.wikimedia.org/w/index.php?curid=45679374))
    shows the typical architecture of a CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dccc596d-f722-451b-9a59-b5dac59e4f8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8
  prefs: []
  type: TYPE_NORMAL
- en: RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs are primarily popular for many NLP tasks (even if they are currently being
    used in different scenarios, which we will talk about in [Chapter 6](f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml),
    *Recurrent Neural Networks*). What's different about RNNs? Their peculiarity is
    that the connections between units form a directed graph along a sequence. This
    means that an RNN can exhibit a dynamic temporal behavior for a given time sequence.
    Therefore, they can use their internal state (memory) to process sequences of
    inputs, while in a traditional neural network, we assume that all inputs and outputs
    are independent of each other. This makes RNNs suitable for cases such as those,
    for example, when we want to predict the next word in a sentence – it is definitely
    better to know which words came before it. Now, you can understand why they are
    called recurrent – the same task is performed for every element of a sequence,
    with the output being dependent on the previous computations.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs have loops in them, allowing information to persist, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c18d6c33-bc28-4b76-8804-5d5ccff8990a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, a chunk of the neural network, **H**, receives some
    input, **x** and outputs a value, **o**. A loop allows information to be passed
    from one step of the network to the next. By unfolding the RNN in this diagram
    into a full network (as shown in the following diagram), it can be thought of
    as multiple copies of the same network, each passing information to a successor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/582aac54-452c-4688-b416-f4fc1b84bce4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10
  prefs: []
  type: TYPE_NORMAL
- en: Here, **x[t]** is the input at time step **t**, **H[t]** is the hidden state
    at time step **t** (and represents the memory of the network), and **o[t]** is
    the output at step **t**. The hidden states capture information about what happened
    in all the previous time steps. The output at a given step is calculated based
    only on the memory at time **t**. An RNN shares the same parameters across every
    step—that's because the same task is performed at each step; it just has different
    inputs—drastically reduces the total number of parameters it needs to learn. Outputs
    aren't necessary at each step, since this depends on the task at hand. Similarly,
    inputs aren't always needed at each time step.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs were first developed in the 1980s and only lately have they come in many
    new variants. Here''s a list of some of those architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully recurrent**: Every element has a weighted one-way connection to every
    other element in the architecture and has a single feedback connection to itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recursive**: The same set of weights is applied recursively over a structure,
    which resembles a graph structure. During this process, the structure is traversed
    in topological sorting ([https://en.wikipedia.org/wiki/Topological_sorting](https://en.wikipedia.org/wiki/Topological_sorting)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hopfield**: All of the connections are symmetrical. This is not suitable
    in scenarios where sequences of patterns need to be processed, as it requires
    stationary inputs only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elman network**: This is a three-layer network, arranged horizontally, plus
    a set of so-called **context units**. The middle hidden layer is connected to
    all of them, with a fixed weight of 1\. What happens at each time step is that
    the input is fed forward and then a learning rule is applied. Because the back-connections
    are fixed, a copy of the previous values of the hidden units is saved in the context
    units. This is the way the network can maintain a state. For this reason, this
    kind of RNN allows you to perform tasks that are beyond the power of a standard
    multilayered neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long short-term memory (LSTM)**: This is a DL that prevents back-propagated
    errors from vanishing or exploding gradients (this will be covered in more detail
    in [Chapter 6](f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml), *Recurrent Neural
    Networks*). Errors can flow backward through (in theory) an unlimited number of
    virtual layers unfolded in space. This means that an LSTM can learn tasks that
    require memories of events that could have happened several time steps earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bi-directional**: By concatenating the outputs of two RNNs, it can predict
    each element of a finite sequence. The first RNN processes the sequence from left
    to right, while the second one does so in the opposite direction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent multilayer perceptron network**: This consists of cascaded subnetworks,
    each containing multiple layers of nodes. Each subnetwork, except for the last
    layer (the only one that can have feedback connections), is feed-forward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 5](fbec1d8a-a92f-4899-af0f-11f3d545e0eb.xhtml), *Convolutional Neural
    Networks*, and [Chapter 6](f7a89101-15be-49e3-8bf5-8c74c655f6d7.xhtml), *Recurrent
    Neural Networks*, will go into more detail about CNNs and RNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: Practical applications of DL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The DL concepts and models that were illustrated in the previous two sections
    aren''t just pure theory – practical applications have been implemented from them.
    DL excels at identifying patterns in unstructured data; most use cases are related
    to media such as images, sound, video, and text. Nowadays, DL is applied in a
    number of use case scenarios across different business domains, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computer vision**: A number of applications in the automotive industry, facial
    recognition, motion detection, and real-time threat detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NLP**: Sentiment analysis in social media, fraud detection in finance and
    insurance, augmented search, and log analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical diagnosis**: Anomaly detection, pathology identification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search engines**: Image searching'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IoT**: Smart homes, predictive analysis using sensor data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manufacturing**: Predictive maintenance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Marketing**: Recommendation engines, automated target identification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Audio analysis**: Speech recognition, voice searching, and machine translation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many others that are yet to come.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the basics of DL were introduced. This overview was kept very
    high-level to help readers who are new to this topic and prepare them to tackle
    the more detailed and hands-on topics that are covered in the following chapters.
  prefs: []
  type: TYPE_NORMAL
