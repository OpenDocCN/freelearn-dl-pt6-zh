<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Neural Networks</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we will get an overview on neural networks. <span class="calibre5">We will see what a simple shallow neural network is and get some familiarity with how they work. We will do this by trying to identify the genre of a song using a shallow neural network. We will also recall our previous work on the spam detector to use the neural network. Further on, we will take a look at larger neural networks, known as <strong class="calibre4">deep learning</strong>, and apply what is known as a convolutional neural network to identify handwritten mathematical symbols. Finally we will revisit the bird species identifier covered previously and use deep learning to produce a much more accurate identifier.</span></p>
<p class="calibre2">The topics that we will be covering in this chapter are as follows:</p>
<ul class="calibre10">
<li class="calibre11">Understanding neural networks</li>
<li class="calibre11">Identifying the genre of a song using neural networks</li>
<li class="calibre11">Recalling our work on the spam detector to use neural networks</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Understanding neural networks</h1>
                
            
            <article>
                
<p class="calibre2">Neural networks, which were originally called artificial neural networks, are inspired by actual neurons found in animal's brains and other parts of the nervous system. Neurons are connected to each other and they receive and send impulses throughout the animal's body, or in the case of computing, the network.</p>
<p class="calibre2">The following diagram shows the components of a single neuron:</p>
<div class="cdpaligncenter"><img class="alignnone112" src="../images/00125.jpeg"/></div>
<div class="cdpaligncenter2">Components of a single neuron</div>
<p class="calibre2">The following graph shows how a neuron fires:</p>
<div class="cdpaligncenter"><img class="alignnone113" src="../images/00126.jpeg"/></div>
<div class="cdpaligncenter2">How a neuron fires</div>
<p class="calibre2">It is all or nothing, meaning, when the neuron gets enough input from its neighbors, it quickly fires and sends a signal down its axon to each forward-connected neuron.</p>
<p class="calibre2">Here, we can see actual neurons in a brain:</p>
<div class="cdpaligncenter"><img class="alignnone114" src="../images/00127.jpeg"/></div>
<div class="cdpaligncenter2">Actual neurons in a brain</div>
<p class="calibre2">A human brain has about 100 billion neurons all together, and has about 100 trillion connections. It is worth noting that the neural networks we create in software have at least 1 million times less complexity.</p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Feed-forward neural networks</h1>
                
            
            <article>
                
<p class="calibre2">Most of the neural networks that we design are feed forward and fully connected. This means that every neuron connects to every neuron in the next layer. The first layer receives inputs and the last layer gives outputs. The structure of the network, meaning the neuron counts and their connections, is decided ahead of time and cannot change, at least not during training. Also, every input must have the same number of values. This means that images, for example, may need to be resized to match the number of input neurons. The number of neurons in each layer is that layer's shape:</p>
<div class="cdpaligncenter"><img class="alignnone115" src="../images/00128.jpeg"/></div>
<div class="cdpaligncenter2">Feed-forward neural design</div>
<p class="calibre2">Each individual neuron adds up the values it receives from the prior layer. Each connection from one neuron to the next has a weight. When adding the inputs, the inputs are multiplied by the respective weights. Each neuron also has an extra input called a <strong class="calibre4">bias</strong>, which is not connected to any other neurons. Once the weighted inputs have been added, an activation function is applied to the sum.</p>
<p class="calibre2">There are several common activation functions, for example, the hyperbolic tangent, whose shape is shown here:</p>
<div class="cdpaligncenter"><img class="alignnone116" src="../images/00129.jpeg"/></div>
<div class="cdpaligncenter2"><span>Hyperbolic tangent</span></div>
<p class="calibre2">The output of each neuron is whatever comes out of the activation function.</p>
<p class="calibre2">The connection waits in a network start random and are adjusted during training. The purpose of training is to examine hundreds, or thousands, or even more example cases and adjust the network's weights until the network is sufficiently accurate.</p>
<p class="calibre2">After training, we have a network structure that we have already defined, and all the weights that were learned during training. As such, the following is true:</p>
<p class="cdpaligncenter1"><em class="calibre16">A trained neural network = Structure + Learned weights</em></p>
<p class="cdpalignleft1">This is shown here:</p>
<div class="cdpaligncenter3"><img class="alignnone117" src="../images/00130.jpeg"/></div>
<div class="cdpaligncenter2">Network structure after training with weights</div>
<p class="calibre2">Now the network is ready to use on new data outside the training set.</p>
<p class="calibre2">Training proceeds in batches, which means that several training cases are sent through the network and the outputs, called <strong class="calibre4">predictions</strong>, are collected. Then, the loss is computed for each batch, which is the measure of the overall error:</p>
<div class="cdpaligncenter"><img class="alignnone118" src="../images/00131.jpeg"/></div>
<div class="cdpaligncenter2">Training procedure: evaluate a batch, adjust weights, and repeat</div>
<p class="calibre2">Each weight in the network is then adjusted depending on whether and how much that weight contributed to the overall loss. With very gradual adjustments, it should be the case that when examples in this batch are visited again, predictions will be more accurate.</p>
<p class="calibre2">The network is often trained over several epochs. By an epoch, we mean all the training data having been processed once. So, 10 epochs means looking at the same training data 10 times. We often segregate 20% or so of the training data as a validation set. This is data that we don't use during training and instead only use to evaluate the model after each epoch.</p>
<p class="calibre2">Ideally, we want the network to become more accurate, which means we want to decrease loss, and this should be true for both the training set and the validation set.</p>
<p class="calibre2"/>
<p class="calibre2">The following set of graph shows this ideal kind of behavior:</p>
<div class="cdpaligncenter"><img class="alignnone119" src="../images/00132.jpeg"/></div>
<div class="cdpaligncenter2">Ideal behavior</div>
<p class="calibre2">Note the signs of overfitting, meaning training loss goes down but validation loss goes up.</p>
<p class="calibre2">If the network is not designed correctly, for example, if it has too many layers, the network may overfit, meaning it performs very well in the training set but poorly on the validation set. This is an issue because ultimately we want to use the neural network on new data from the real world, which will probably be a little different than the training set, hence we use a validation set to see how well the network performs on data it didn't see for training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Identifying the genre of a song with neural networks</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we're going to build a neural network that can identify the genre of a song. We will use the GTZAN Genre Collection (<a href="http://marsyasweb.appspot.com/download/data_sets/." class="calibre9">http://marsyasweb.appspot.com/download/data_sets/.GTZAN Genre Collection</a>). It has 1,000 different songs from over 10 different genres. There are 100 songs per genre and each song is about 30 seconds long.</p>
<p class="calibre2">We will use the  Python library, <kbd class="calibre12">librosa</kbd> to extract features from the songs. We will use <strong class="calibre4">Mel-frequency cepstral coefficients</strong> (<strong class="calibre4">MFCC</strong>). MFCC values mimic human hearing and they are commonly used in speech recognition applications as well as music genre detection. These MFCC values will be fed directly into the neural network.</p>
<p class="calibre2">To help us understand the MFCC, let's use two examples. Download Kick Loop 5 by Stereo Surgeon. You can do this by visiting <a href="https://freesound.org/people/Stereo%20Surgeon/sounds/266093/" class="calibre9">https://freesound.org/people/Stereo%20Surgeon/sounds/266093/</a>, and download Whistling by cmagar by visiting <a href="https://freesound.org/people/grrlrighter/sounds/98195/" class="calibre9">https://freesound.org/people/grrlrighter/sounds/98195/</a>. One of them is a low-bass beat and the other is a higher pitched whistling. They are clearly different and we are going to see how they look different with MFCC values.</p>
<p class="calibre2">Let's go to the code. First, we have to import the <kbd class="calibre12">librosa</kbd> library. We will also import <kbd class="calibre12">glob</kbd> because we are going to list the files in the different genre directories. Also, import <kbd class="calibre12">numpy</kbd> as usual. We will import <kbd class="calibre12">matplotlib</kbd> to draw the MFCC graphs. Then, will import the Sequential model from Keras. This is a typical feed-forward neural network. Finally, we will import the dense neural network layer, which is just a layer that has a bunch of neurons in it:</p>
<div class="cdpaligncenter"><img class="alignnone120" src="../images/00133.jpeg"/></div>
<p class="calibre2">Unlike a convolution, for example, it's going to have 2D representations. We are going to use import activation, which allows us to give each neuron layer an activation function, and we will also import <kbd class="calibre12">to_categorical</kbd>, which allows us to turn the class names into things such as rock, disco, and so forth, which is what's called one-hot encoding.</p>
<p class="calibre2">We have officially developed a helper function to display the MFCC values:</p>
<div class="cdpaligncenter"><img class="alignnone121" src="../images/00134.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2">First, we will load the song and then extract the MFCC values from it. Then, we'll use the <kbd class="calibre12">specshow</kbd>, which is a spectrogram show from the <kbd class="calibre12">librosa</kbd> library.</p>
<p class="calibre2">Here's the kick drum:</p>
<div class="cdpaligncenter"><img class="alignnone122" src="../images/00135.jpeg"/></div>
<p class="calibre2">We can see that at low frequency, the bass is very obvious and the rest of the time it's kind of like a wash. Not many other frequencies are represented.</p>
<p class="calibre2">However, if we look at the whistling, it's pretty clear that there's higher frequencies being represented:</p>
<div class="cdpaligncenter"><img class="alignnone123" src="../images/00136.jpeg"/></div>
<p class="calibre2">The darker the color, or closer to red, the more power is in that frequency range at that time. So, you can even see the kind of change in frequency with the whistles.</p>
<p class="calibre2">Now, here is the frequency for disco songs:</p>
<div class="cdpaligncenter"><img class="alignnone124" src="../images/00137.jpeg"/></div>
<p class="calibre2">This is the frequency output:</p>
<div class="cdpaligncenter"><img class="alignnone125" src="../images/00138.jpeg"/></div>
<p class="calibre2">You can sort of see the beats in the preceding outputs, but they're only 30 seconds long, so it is a little bit hard to see the individual beats.</p>
<p class="calibre2"/>
<p class="calibre2">Compare this with classical where there are not so much beats as a continuous kind of bassline such as one that would come from a cello, for example:</p>
<div class="cdpaligncenter"><img class="alignnone126" src="../images/00139.jpeg"/></div>
<p class="calibre2">Here is the frequency for hip-hop songs:</p>
<div class="cdpaligncenter"><img class="alignnone127" src="../images/00140.jpeg"/></div>
<div class="cdpaligncenter"><img class="alignnone128" src="../images/00141.jpeg"/></div>
<p class="calibre2">It looks kind of similar to disco, but if it were required that we could tell the difference with our own eyes, we wouldn't really need a neural network because it'd probably be a relatively simple problem. So, the fact that we can't really tell the difference between these is not our problem, it's the neural network's problem.</p>
<p class="calibre2">We have another auxiliary function here that again just loads the MFCC values, but this time we are preparing it for the neural network:</p>
<div class="cdpaligncenter"><img class="alignnone129" src="../images/00142.jpeg"/></div>
<p class="calibre2">We have loaded the MFCC values for the song, but because these values are between maybe negative 250 to positive 150, they are no good for a neural network. We don't want to feed in these large and small values. We want to feed in values near negative 1 and positive 1 or from 0 to 1. Therefore, we are going to figure out what the max is, the absolute value for each song, and then divide all the values by that max. Also, the songs are a slightly different length, so we want to pick just 25,000 MFCC values. We have to be super certain that what we feed into the neural network is always the same size, because there are only so many input neurons and we can't change that once we've built the network.</p>
<p class="calibre2">Next, we have a function called <kbd class="calibre12">generate _features_and_labels</kbd>, which will go through all the different genres and go through all the songs in the dataset and produce those MFCC values and the class names:</p>
<div class="cdpaligncenter"><img class="alignnone130" src="../images/00143.jpeg"/></div>
<p class="calibre2">As shown in the preceding screenshot, we will prepare a list for all the features and all the labels. Go through each of the 10 genres. For each genre, we will look at the files in that folder. The <kbd class="calibre12">'generes/'+genre+'/*.au'</kbd> folder shows how the dataset is organized. When we are processing that folder, there will be 100 songs each for each file, we will extract the features and put those features in the <kbd class="calibre12">all_features.append(features)</kbd> list. The name of the genre for that song needs to be put  in a list also. So, at the end, all features will have 1,000 entries and all labels will have 1,000 entries. In the case of all features, each of those 1,000 entries will have 25,000 entries. That will be a 1,000 x 25,000 matrix.</p>
<p class="calibre2">For all labels at the moment, there is a 1,000 entry-long list, and inside are words such as <kbd class="calibre12">blues</kbd>, <kbd class="calibre12">classical</kbd>, <kbd class="calibre12">country</kbd>, <kbd class="calibre12">disco</kbd>, <kbd class="calibre12">hiphop</kbd>, <kbd class="calibre12">jazz</kbd>, <kbd class="calibre12">metal</kbd>, <kbd class="calibre12">pop</kbd>, <kbd class="calibre12">reggae</kbd>, and <kbd class="calibre12">rock</kbd>. Now, this is going to be a problem because a neural network is not going to predict a word or even letters. We need to give it a one-hot encoding, which means that each word here is going to be represented as ten binary numbers. In the case of the blues, it is going to be one and then nine zeros. In the case of classical, it is going to be zero followed by one, followed by nine zeros, and so forth. First, we have to figure out all the unique names by using the <kbd class="calibre12">np.unique(all_labels, return_inverse=True)</kbd> command to get them back as integers. Then, we have to use <kbd class="calibre12">to_categorical</kbd>, which turns those integers into one-hot encoding. So, what comes back is 1000 x 10 dimensions. 1,000 because there are 1,000 songs, and each of those has ten binary numbers to represent the one-hot encoding. Then, return all the features stacked together by the command return <kbd class="calibre12">np.stack(all_features), onehot_labels</kbd> into a single matrix, as well as the one-hot matrix. So, we will call that upper function and save the features and labels:</p>
<div class="cdpaligncenter"><img class="alignnone131" src="../images/00144.jpeg"/></div>
<p class="calibre2">Just to be sure, we will print the shape of the features and the labels as shown in the following screenshot. So, it is 1,000 by 25,000 for the features and 1,000 by 10 for the labels. Now, we will split the dataset into a train and test split. Let's decide the 80% mark defined as <kbd class="calibre12">training_split= 0.8</kbd> to perform a split:</p>
<div class="cdpaligncenter"><img class="alignnone132" src="../images/00145.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2">Before that, we will shuffle, and before we shuffle, we need to put the labels with the features so that they don't shuffle in different orders. We will call <kbd class="calibre12">np.random.shuffle(alldata)</kbd> and do the shuffle, split it using <kbd class="calibre12">splitidx= int(len(alldata)*training_split)</kbd>, and then we will have train and testsets, as shown in the snapshot earlier. Looking at the shape of the train and the testsets, the train is 800, so 80% of the 1,000 for the rows: we have 25,010 features. Those aren't really all features, though. It is actually the 25,000 features plus the 10 for the one-hot encoding because, remember, we stacked those together before we shuffled. Therefore, we're going to have to strip that back off. We can do that with <kbd class="calibre12">train_input = train[:,:-10]</kbd>. For both the train input and the test input, we take everything but the last 10 columns, and for the labels, we take the 10 columns to the end, and then we can see what the shapes of the train input and train labels are. So now, we have the proper 800 by 25,000 and 800 by 10.</p>
<p class="calibre2">Next, we'll build the neural network:</p>
<div class="cdpaligncenter"><img class="alignnone133" src="../images/00146.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2">We are going to have a sequential neural network. The first layer will be a dense layers of 100 neurons. Now, just on the first layer, it matters that you give the input dimensions or the input shape, and that's going to be 25,000 in our case. This says how many input values are coming per example. Those 25,000 are going to connect to the 100 in the first layer. The first layer will do its weighted sum of its inputs, its weights, and bias term, and then we are going to run the <kbd class="calibre12">relu</kbd> activation function. <kbd class="calibre12">relu</kbd>, if you recall, states that anything less than 0 will turn out to be a 0. Anything higher than 0 will just be the value itself. These 100 will then connect to 10 more and that will be the output layer. It will be 10 because we have done someone-hot encoding and we have 10 binary numbers in that encoding.</p>
<p class="calibre2">The activation used in the code, <kbd class="calibre12">softmax</kbd>, says to take the output of the 10 and normalize them so that they add up to 1. That way, they end up being probabilities and whichever one of the 10 is the highest scoring, the highest probability, we take that to be the prediction and that will directly correspond to whichever position that highest number is in. For example, if it is in position 4, that would be disco (look in the code).</p>
<p class="calibre2">Next, we will compile the model, choose an optimizer such as Adam, and define the <kbd class="calibre12">loss</kbd> function. Any time you have multiple outputs like we have here (we have 10), you probably want to do categorical cross-entropy and metrics accuracy to see the accuracy as it's training and during evaluation, in addition to the loss, which is always shown: however, accuracy makes more sense to us. Next, we can print <kbd class="calibre12">model.summary</kbd>, which tells us details about the layers.</p>
<p class="calibre2">It will look something like the following:</p>
<div class="cdpaligncenter"><img class="alignnone134" src="../images/00147.jpeg"/></div>
<p class="calibre2">The output shape of the first 100 neuron layer is definitely 100 values because there are 100 neurons, and the output of the dense second layer is 10 because there are 10 neurons. So, why are there 2.5 million parameters, or weights, in the first layer? That's because we have 25,000 inputs. Well, we have 25,000 inputs and each one of those inputs is going to each one of the 100 dense neurons. So that's 2.5 million, and then plus 100, because each of those neurons in the 100 has its own bias term, its own bias weight, and that needs to be learned as well.</p>
<p class="calibre2">Overall, we have about 2.5 million parameters or weights. Next, we run the fit. It takes the training input and training labels, and takes the number of epochs that we want. We want 10, so that's 10 repeats over the trained input; it takes a batch size which says how many, in our case, songs to go through before updating the weights; and a <kbd class="calibre12">validation_split</kbd> of 0.2 says <em class="calibre16">take 20% of that trained input, split it out, don't actually train on that, and use that to evaluate how well it's doing after every epoch</em>. It never actually trains on the validation split, but the validation split lets us look at the progress as it goes.</p>
<p class="calibre2">Finally, because we did separate the training and test ahead of time, we're going to do an evaluation on the test, the test data, and print the loss and accuracy of that. Here it is with the training results:</p>
<div class="cdpaligncenter"><img class="alignnone135" src="../images/00148.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2">It was printing this as it went. It always prints the loss and the accuracy. This is on the training set itself, not the validation set, so this should get pretty close to 1.0. You actually probably don't want it to go close to 1.0 because that could represent overfitting, but if you let it go long enough, it often does reach 1.0 accuracy on the training set because it's memorizing the training set. What we really care about is the validation accuracy because that's letting us use the test set. It's data that it's just never looked at before, at least not for training, and indeed it's relatively close to the validation accuracy, which is our final accuracy. This final accuracy is on the test data that we separated ahead of time. Now we're getting an accuracy of around 53%. That seems relatively low until we realize that there are 10 different genres. Random guessing would give us 10% accuracy, so it's a lot better than random guessing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Revising the spam detector to use neural networks</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we're going to update the spam detector from before to use neural networks. Recall that the dataset used was from YouTube. There was an approximate of 2,000 comments with around half being spam and the other half not. These comments were of five different videos.</p>
<p class="calibre2">In the last version, we used a bag of words and a random forest. We carried out a parameter search to find the parameters best suited for the bag of words, which was the CountVectorizer that had 1,000 different words in it. These 1000 words were the top used words. We used unigrams instead of bigrams or trigrams. It would be good to drop the common and the stop words from the English language. The best way is to use TF-IDF. It was also found that using a 100 different trees would be best for the random forest. Now, we are going to use a bag of words but we're going to use a shallow neural network instead of the random forest. Also remember that we got 95 or 96 percent accuracy for the previous version.</p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Let's look at the code:</p>
<div class="cdpaligncenter"><img class="alignnone136" src="../images/00149.jpeg"/></div>
<p class="calibre2">We start with importing. We'll use pandas once more to load the dataset. This time, we're going to use the Keras Tokenizer. There's no particular reason to use Tokenizer, except to show an alternative technique. We will import NumPy and then proceed to import the sequential model for the neural networks, which is the typical feed-forward network. We then have dense layers that are the typical neuron layers. We're also going to add the dropout feature, which helps prevent over-fitting, and we're going to decide on the activation for each layer. We are going to use the <kbd class="calibre12">to_categorical</kbd> method from the <kbd class="calibre12">np_utils</kbd> library from Keras to produce one-hot encoding, and we're going to introduce <kbd class="calibre12">StratifiedKFold</kbd> to perform our cross-validation.</p>
<p class="calibre2">First, we load the datasets:</p>
<div class="cdpaligncenter"><img class="alignnone137" src="../images/00150.jpeg"/></div>
<p class="calibre2">There are five different CSV files. We will stack them on top of each other so that we have one big dataset. We then shuffle it by running a sample which picks random rows. We're going to say that we want to keep 100% of the data so that it effectively shuffles all of the data.</p>
<p class="calibre2">Now, the <kbd class="calibre12">StratifiedKFold</kbd> technique takes a number of splits, say five, and produces the indexes of the original dataset for those splits:</p>
<div class="cdpaligncenter"><img class="alignnone138" src="../images/00151.jpeg"/></div>
<p class="calibre2">We're going to get an 80%/20% split for training and testing. This 20% testing will differ with each split. It's an iterator, hence, we can use a <kbd class="calibre12">for</kbd> loop to look at all the different splits. We will print the testing positions to see that they don't overlap for each split:</p>
<div class="cdpaligncenter"><img class="alignnone139" src="../images/00152.jpeg"/></div>
<p class="calibre2">Here's the first split:</p>
<div class="cdpaligncenter"><img class="alignnone140" src="../images/00153.jpeg"/></div>
<p class="calibre2">Here's the second split:</p>
<div class="cdpaligncenter"><img class="alignnone141" src="../images/00154.jpeg"/></div>
<p class="calibre2">Here's the third:</p>
<div class="cdpaligncenter"><img class="alignnone142" src="../images/00155.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2">Here's the fourth:</p>
<div class="cdpaligncenter"><img class="alignnone143" src="../images/00156.jpeg"/></div>
<p class="calibre2">And finally, the fifth:</p>
<div class="cdpaligncenter"><img class="alignnone144" src="../images/00157.gif"/></div>
<p class="calibre2">It is now obvious that they don't overlap.</p>
<p class="calibre2">We then define a function that receives these indexes for the different splits and does the bag of words, builds a neural net, trains it, and evaluates it. We then return the score for that split. We begin by taking the positions for the train and test sets and extract the comments:</p>
<div class="cdpaligncenter"><img class="alignnone145" src="../images/00158.jpeg"/></div>
<p class="calibre2">We then proceed to build our Tokenizer. At this point, we can mention the number of words we want it to support in the Tokenizer. A general research led us to the conclusion that using 2,000 words was better than a 1000 words. For the random forest, using a 1,000 words is better and is supported by doing the GridSearch for all the different parameters. There's no particular reason to believe that because the bag of words works best with a 1,000 words in comparison to the random forest, that it is what is necessarily best for the neural network as well. So, we're going to use 2,000 words in this case. This is just a constructor. Nothing has really happened with the bag of words yet. The next thing we need to do is learn what the words are and that's going to happen by using the <kbd class="calibre12">fit_on_texts</kbd> method.</p>
<p class="calibre2"/>
<p class="calibre2">Now, <kbd class="calibre12">fit_on_texts</kbd> should only be used on the training set. We only want to learn the words in the training set. This helps us simulate the real world where you've only trained your model on a certain set of data and then the real world presents possibly something new that you've never seen before. To do this, we have a training testing split. We only want to learn the words on the training set. If there are words in the testing set that we've never seen before in the training set, they'll be ignored. This is good because that's how it's going to work in the real world.</p>
<p class="calibre2">We'll learn the words on the training set but then transform both the training and the testing comments into the bag of words model. The <kbd class="calibre12">texts_to _matrix </kbd>is used for the same. It produces a matrix which can be fed directly into the neural network. We give it the <kbd class="calibre12">train_content</kbd>, which are the comments, and the <kbd class="calibre12">test_content</kbd>. Then, we can then decide if we want <kbd class="calibre12">tfidf</kbd> scores, binary scores, or frequency counts. We're going to go with <kbd class="calibre12">tfidf</kbd> in this case. <kbd class="calibre12">tfidf</kbd> is a number between 0 and any random integer, possibly a large number, and in most cases it's not a good idea to give a neuron in a neural network very large numbers or very small numbers, meaning negative numbers. Here, we want to kind of scale these numbers between maybe 0 and 1, and -1 and 1. To scale between 0 and 1, we can divide by the max. So, we have to look at all the training examples, all the training numbers for TF-IDF, and divide each number by the maximum among those. We have to do the same for the test. Now, the train inputs and test inputs are <kbd class="calibre12">tfidf</kbd> scores that have been rescaled to 0 to 1.</p>
<p class="calibre2">We also shift it between -1 and 1 by subtracting the average from each score. Now, for the outputs, even though we could use binary, we're going to use categorical in this case for no particular reason, except just to show it. We're going to take all of the desired outputs, the classes, which is spam, not spam, and turn them into 1, 0 and 0, 1 encodings.</p>
<p class="calibre2">Now, we can build our network. We're going to build the network all over again for each train/test split so it starts randomly. We're going to build a sequential network, which is a typical feed-forward network. We're going to have a first layer of 512 neurons. They're going to receive 2,000 different inputs. There's 2,000 because that's the size of the bag of words.</p>
<p class="calibre2">We then use a <span class="calibre5">ReLU</span> activation. We could also use Tanh. ReLU is common in neural networks today. It's pretty fast as well as accurate. There's a 512 layer and then a 2 layer. The 2 is very specific because that's the output. We have one-hot encoding, so it's 1, 0, 0, 1, so that's two neurons. It has to match the number of outputs we have. Each of the two has links to 512 neurons from before. That's a lot of edges connecting the first layer to the second layer.</p>
<p class="calibre2"/>
<p class="calibre2">To prevent overfitting, we add a dropout. A 50% dropout means that every time it goes to update the weights, it just refuses to update half of them, a random half. We then find the weighted sum of their inputs.</p>
<p class="calibre2">We take that sum and run the softmax. Softmax takes these different outputs and turns them into probabilities so that one of them is highest and they're all between 0 and 1. Then, we compile the model to compute the loss as <kbd class="calibre12">categorical_ crossentropy</kbd>. This is usually something one uses when they use one-hot encoding. Let's use the Adamax optimizer. There are different optimizers that are available in Keras, and you can look at the Keras documentation at <a href="https://keras.io/" class="calibre9">https://keras.io/</a>.</p>
<p class="calibre2">Accuracy is an essential measure to work on while we train the network, and we also want to compute accuracy at the very end to see how well it's done.</p>
<p class="calibre2">We then run fit on the training set. <kbd class="calibre12">d_train_inputs</kbd> is the train inputs, and <kbd class="calibre12">d_train_inputs</kbd> is the matrix bag of words model, train outputs, and the one -hot encoding. We are going to say that we want 10 epochs, which means it'll go through the entire training set ten times, and a batch size of 16, which means it will go through 16 rows and compute the average loss and then update the weight.</p>
<p class="calibre2">After it's been fit, which indirectly means it's been trained, we evaluate the test. It's not until this point that it actually looks at the test. The scores that come out are going to be the loss and whatever other metrics we have, which in this case is accuracy. Therefore, we'll just show the accuracy times 100 to get a percent and we'll return the scores.</p>
<p class="calibre2">Now, let's build that split again, which is the k-fold split with five different folds:</p>
<div class="cdpaligncenter"><img class="alignnone146" src="../images/00159.jpeg"/></div>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">We collect the scores. For each split, we're going to run our <kbd class="calibre12">train_and_test</kbd> function and save the scores. Here, it is running on each split. If you scroll, you will see that you get the epochs going. We can see that the accuracy on the training input increases per epoch. Now, if this gets really high, you might start worrying about over-fitting, but after the 10 epochs, use the testing set which it's never seen before. This helps us obtain the accuracy number for the testing set. Then, we'll do it all again for the next split and we'll get a different accuracy. We'll do this a few more times until we have five different numbers, one for each split.</p>
<p class="calibre2">The average is found as follows: :</p>
<div class="cdpaligncenter"><img class="alignnone147" src="../images/00160.jpeg"/></div>
<p class="calibre2">Here, we get 95%, which is very close to what we got by using random forest. We didn't use this neural network example to show that we can get 100%. We used this method to demonstrate an alternative way to detect spam instead of the random forest method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we covered a brief introduction to neural networks, proceeded with feed-forward neural networks, and looked at a program to identify the genre of a song with neural networks. Finally, we revised our spam detector from earlier to make it work with neural networks.</p>
<p class="calibre2">In the next chapter, we'll look at deep learning and learn about convolutional neural networks.</p>


            </article>

            
        </section>
    </body></html>