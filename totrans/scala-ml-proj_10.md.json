["```py\n$ sudo apt-get update \n$ sudo apt-get install -y build-essential git \n```", "```py\n$ sudo apt-get install -y libopenblas-dev \n$ sudo apt-get install -y libatlas-base-dev \n```", "```py\n $ sudo apt-get install -y libopencv-dev \n```", "```py\n$ git clone --recursive https://github.com/apache/incubator-mxnet.git mxnet --branch 0.12.0 \n$ cd mxnet \n$ make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas \n$ make scalapkg \n$ make scalainsta \n```", "```py\n<dependency>\n    <groupId>org.sameersingh.scalaplot</groupId>\n    <artifactId>scalaplot</artifactId>\n    <version>0.0.4</version>\n</dependency>\n<dependency>\n    <groupId>args4j</groupId>\n    <artifactId>args4j</artifactId>\n    <version>2.0.29</version>\n</dependency>\n```", "```py\npackage com.packt.ScalaML.HAR \n\nimport ml.dmlc.mxnet.Context \nimport LSTMNetworkConstructor.LSTMModel \nimport scala.collection.mutable.ArrayBuffer \nimport ml.dmlc.mxnet.optimizer.Adam \nimport ml.dmlc.mxnet.NDArray \nimport ml.dmlc.mxnet.optimizer.RMSProp \nimport org.sameersingh.scalaplot.MemXYSeries \nimport org.sameersingh.scalaplot.XYData \nimport org.sameersingh.scalaplot.XYChart \nimport org.sameersingh.scalaplot.Style._ \nimport org.sameersingh.scalaplot.gnuplot.GnuplotPlotter \nimport org.sameersingh.scalaplot.jfreegraph.JFGraphPlotter  \n```", "```py\n// Retrieves the name of this Context object \nval ctx = Context.cpu() \n```", "```py\nval datasetPath = \"UCI_HAR_Dataset/\" \nval trainDataPath = s\"$datasetPath/train/Inertial Signals\" \nval trainLabelPath = s\"$datasetPath/train/y_train.txt\" \nval testDataPath = s\"$datasetPath/test/Inertial Signals\" \nval testLabelPath = s\"$datasetPath/test/y_test.txt\" \n```", "```py\nval trainData = Utils.loadData(trainDataPath, \"train\") \nval trainLabels = Utils.loadLabels(trainLabelPath) \nval testData = Utils.loadData(testDataPath, \"test\") \nval testLabels = Utils.loadLabels(testLabelPath) \n```", "```py\ndef loadData(dataPath: String, name: String): Array[Array[Array[Float]]] = { \n    val dataSignalsPaths = INPUT_SIGNAL_TYPES.map( signal => s\"$dataPath/${signal}${name}.txt\" ) \n    val signals = dataSignalsPaths.map { path =>  \n      Source.fromFile(path).mkString.split(\"n\").map { line =>  \n        line.replaceAll(\"  \", \" \").trim().split(\" \").map(_.toFloat) } \n    } \n\n    val inputDim = signals.length \n    val numSamples = signals(0).length \n    val timeStep = signals(0)(0).length   \n\n    (0 until numSamples).map { n =>  \n      (0 until timeStep).map { t => \n        (0 until inputDim).map( i => signals(i)(n)(t) ).toArray \n      }\n    .toArray \n    }\n    .toArray \n  } \n```", "```py\nprivate val INPUT_SIGNAL_TYPES = Array( \n    \"body_acc_x_\", \n    \"body_acc_y_\", \n    \"body_acc_z_\", \n    \"body_gyro_x_\", \n    \"body_gyro_y_\", \n    \"body_gyro_z_\", \n    \"total_acc_x_\", \n    \"total_acc_y_\", \n    \"total_acc_z_\") \n```", "```py\ndef loadLabels(labelPath: String): Array[Float] = {          \n       Source.fromFile(labelPath).mkString.split(\"n\").map(_.toFloat - 1)\n            } \n```", "```py\n// Output classes: used to learn how to classify \nprivate val LABELS = Array( \n    \"WALKING\",  \n    \"WALKING_UPSTAIRS\",  \n    \"WALKING_DOWNSTAIRS\",  \n    \"SITTING\",  \n    \"STANDING\",  \n    \"LAYING\") \n```", "```py\nval trainingDataCount = trainData.length // No. of training series  \nval testDataCount = testData.length // No. of testing series \nval nSteps = trainData(0).length // No. of timesteps per series \nval nInput = trainData(0)(0).length // No. of input parameters per timestep \n\nprintln(\"Number of training series: \"+ trainingDataCount) \nprintln(\"Number of test series: \"+ testDataCount) \nprintln(\"Number of timestep per series: \"+ nSteps) \nprintln(\"Number of input parameters per timestep: \"+ nInput) \n>>>\n```", "```py\nNumber of training series: 7352\nNumber of test series: 2947\nNumber of timestep per series: 128\nNumber of input parameters per timestep: 9\n```", "```py\nval nHidden = 128 // Number of features in a hidden layer  \nval nClasses = 6 // Total classes to be predicted  \n\nval learningRate = 0.001f \nval trainingIters = trainingDataCount * 100  // iterate 100 times on trainset: total 7352000 iterations \nval batchSize = 1500 \nval displayIter = 15000  // To show test set accuracy during training \nval numLstmLayer = 3 \n```", "```py\nval model = LSTMNetworkConstructor.setupModel(nSteps, nInput, nHidden, nClasses, batchSize, ctx = ctx) \n```", "```py\ncase class LSTMModel(exec: Executor, symbol: Symbol, data: NDArray, label: NDArray, argsDict: Map[String,                     NDArray], gradDict: Map[String, NDArray]) \n```", "```py\ndef setupModel(seqLen: Int, nInput: Int, numHidden: Int, numLabel: Int, batchSize: Int, numLstmLayer: Int = 1, dropout: Float = 0f, ctx: Context = Context.cpu()): LSTMModel = { \n//get the symbolic model \n    val sym = LSTMNetworkConstructor.getSymbol(seqLen, numHidden, numLabel, numLstmLayer = numLstmLayer) \n    val argNames = sym.listArguments() \n    val auxNames = sym.listAuxiliaryStates() \n// defining the initial argument and binding them to the model \n    val initC = for (l <- 0 until numLstmLayer) yield (s\"l${l}_init_c\", (batchSize, numHidden)) \n    val initH = for (l <- 0 until numLstmLayer) yield (s\"l${l}_init_h\", (batchSize, numHidden)) \n    val initStates = (initC ++ initH).map(x => x._1 -> Shape(x._2._1, x._2._2)).toMap \n    val dataShapes = Map(\"data\" -> Shape(batchSize, seqLen, nInput)) ++ initStates \n    val (argShapes, outShapes, auxShapes) = sym.inferShape(dataShapes) \n\n    val initializer = new Uniform(0.1f) \n    val argsDict = argNames.zip(argShapes).map { case (name, shape) => \n       val nda = NDArray.zeros(shape, ctx) \n       if (!dataShapes.contains(name) && name != \"softmax_label\") { \n         initializer(name, nda) \n       } \n       name -> nda \n    }.toMap \n\n    val argsGradDict = argNames.zip(argShapes) \n         .filter(x => x._1 != \"softmax_label\" && x._1 != \"data\") \n         .map( x => x._1 -> NDArray.zeros(x._2, ctx) ).toMap \n\n    val auxDict = auxNames.zip(auxShapes.map(NDArray.zeros(_, ctx))).toMap \n    val exec = sym.bind(ctx, argsDict, argsGradDict, \"write\", auxDict, null, null) \n    val data = argsDict(\"data\") \n    val label = argsDict(\"softmax_label\")  \n    LSTMModel(exec, sym, data, label, argsDict, argsGradDict)\n} \n```", "```py\n private def getSymbol(seqLen: Int, numHidden: Int, numLabel: Int, numLstmLayer: Int = 1, \n                        dropout: Float = 0f): Symbol = {  \n                //symbolic training and label variables \n                var inputX = Symbol.Variable(\"data\") \n                val inputY = Symbol.Variable(\"softmax_label\") \n\n                //the initial parameters and cells \n                var paramCells = Array[LSTMParam]() \n                var lastStates = Array[LSTMState]() \n                //numLstmLayer is 1  \n                for (i <- 0 until numLstmLayer) { \n                    paramCells = paramCells :+ LSTMParam(i2hWeight =\n                    Symbol.Variable(s\"l${i}_i2h_weight\"), \n                    i2hBias = Symbol.Variable(s\"l${i}_i2h_bias\"),                                                                                     \n                    h2hWeight = Symbol.Variable(s\"l${i}_h2h_weight\"),                                                                                                                                   \n                    h2hBias = Symbol.Variable(s\"l${i}_h2h_bias\")) \n                    lastStates = lastStates :+ LSTMState(c =\n                    Symbol.Variable(s\"l${i}_init_c\"),                                                                      \n                    h = Symbol.Variable(s\"l${i}_init_h\")) \n            } \n            assert(lastStates.length == numLstmLayer) \n            val lstmInputs = Symbol.SliceChannel()(inputX)(Map(\"axis\" \n            > 1, \"num_outputs\" -> seqLen,       \n            \"squeeze_axis\" -> 1)) \n\n            var hiddenAll = Array[Symbol]() \n            var dpRatio = 0f \n            var hidden: Symbol = null \n\n//for each one of the 128 inputs, create a LSTM Cell \n            for (seqIdx <- 0 until seqLen) { \n                  hidden = lstmInputs.get(seqIdx) \n// stack LSTM, where numLstmLayer is 1 so the loop will be executed only one time \n                  for (i <- 0 until numLstmLayer) { \n                        if (i == 0) dpRatio = 0f else dpRatio = dropout \n//for each one of the 128 inputs, create a LSTM Cell \n                        val nextState = lstmCell(numHidden, inData = hidden, \n                          prevState = lastStates(i), \n                          param = paramCells(i), \n                          seqIdx = seqIdx, layerIdx = i, dropout =\n                        dpRatio) \n                    hidden = nextState.h // has no effect \n                    lastStates(i) = nextState // has no effect \n              } \n// adding dropout before softmax has no effect- dropout is 0 due to numLstmLayer == 1 \n              if (dropout > 0f) hidden = Symbol.Dropout()()(Map(\"data\" -> hidden, \"p\" -> dropout)) \n// store the lstm cells output layers \n                  hiddenAll = hiddenAll :+ hidden\n    } \n```", "```py\nval finalOut = hiddenAll.reduce(_+_) \n```", "```py\n val fc = Symbol.FullyConnected()()(Map(\"data\" -> finalOut, \"num_hidden\" -> numLabel)) \n //softmax activation against the label \n Symbol.SoftmaxOutput()()(Map(\"data\" -> fc, \"label\" -> inputY)) \n```", "```py\nval i2h = Symbol.FullyConnected(s\"t${seqIdx}_l${layerIdx}_i2h\")()(Map(\"data\" -> inDataa, \"weight\" ->                 param.i2hWeight, \"bias\" -> param.i2hBias, \"num_hidden\" -> numHidden * 4)) \n```", "```py\nval h2h = Symbol.FullyConnected(s\"t${seqIdx}_l${layerIdx}_h2h\")()(Map(\"data\" -> prevState.h,\"weight\" ->             param.h2hWeight,\"bias\" -> param.h2hBias,\"num_hidden\" -> numHidden * 4)) \n```", "```py\nval gates = i2h + h2h \n```", "```py\nval sliceGates = Symbol.SliceChannel(s\"t${seqIdx}_l${layerIdx}_slice\")(gates)(Map(\"num_outputs\" -> 4)) \n```", "```py\nval sliceGates = Symbol.SliceChannel(s\"t${seqIdx}_l${layerIdx}_slice\")(gates)(Map(\"num_outputs\" -> 4)) \n```", "```py\nval forgetGate = Symbol.Activation()()(Map(\"data\" -> sliceGates.get(2), \"act_type\" -> \"sigmoid\")) \n```", "```py\nval ingate = Symbol.Activation()()(Map(\"data\" -> sliceGates.get(0), \"act_type\" -> \"sigmoid\"))   \nval inTransform = Symbol.Activation()()(Map(\"data\" -> sliceGates.get(1), \"act_type\" -> \"tanh\")) \n```", "```py\nval nextC = (forgetGate * prevState.c) + (ingate * inTransform) \n```", "```py\nval nextH = outGate * Symbol.Activation()()(Map(\"data\" -> nextC, \"act_type\" -> \"tanh\")) \n```", "```py\n  // LSTM Cell symbol \n  private def lstmCell( numHidden: Int, inData: Symbol, prevState: LSTMState, param: LSTMParam, \n                        seqIdx: Int, layerIdx: Int, dropout: Float = 0f): LSTMState = { \n        val inDataa = { \n              if (dropout > 0f) Symbol.Dropout()()(Map(\"data\" -> inData, \"p\" -> dropout)) \n              else inData \n                } \n        // add an hidden layer of size numHidden * 4 (numHidden set //to 28) that takes as input) \n        val i2h = Symbol.FullyConnected(s\"t${seqIdx}_l${layerIdx}_i2h\")()(Map(\"data\" -> inDataa,\"weight\"                             -> param.i2hWeight,\"bias\" -> param.i2hBias,\"num_hidden\" -> numHidden * 4)) \n        // add an hidden layer of size numHidden * 4 (numHidden set to 28) that takes output of the cell  \n        val h2h = Symbol.FullyConnected(s\"t${seqIdx}_l${layerIdx}_h2h\")()(Map(\"data\" ->                                    prevState.h,\"weight\" -> param.h2hWeight,\"bias\" -> param.h2hBias,\"num_hidden\" -> numHidden * 4)) \n\n        //concatenate them                                        \n        val gates = i2h + h2h  \n\n        //make 4 copies of gates \n        val sliceGates=Symbol.SliceChannel(s\"t${seqIdx}_l${layerIdx}_slice\")(gates)(Map(\"num_outputs\" \n       -> 4)) \n        // compute the gates \n        val ingate = Symbol.Activation()()(Map(\"data\" -> sliceGates.get(0), \"act_type\" -> \"sigmoid\")) \n        val inTransform = Symbol.Activation()()(Map(\"data\" -> sliceGates.get(1), \"act_type\" -> \"tanh\")) \n        val forgetGate = Symbol.Activation()()(Map(\"data\" -> sliceGates.get(2), \"act_type\" -> \"sigmoid\")) \n        val outGate = Symbol.Activation()()(Map(\"data\" -> sliceGates.get(3), \"act_type\" -> \"sigmoid\")) \n        // get the new cell state and the output \n        val nextC = (forgetGate * prevState.c) + (ingate * inTransform) \n        val nextH = outGate * Symbol.Activation()()(Map(\"data\" -> nextC, \"act_type\" -> \"tanh\")) \n        LSTMState(c = nextC, h = nextH) \n  } \n```", "```py\nval opt = new RMSProp(learningRate = learningRate) \n```", "```py\nval paramBlocks = model.symbol.listArguments() \n      .filter(x => x != \"data\" && x != \"softmax_label\") \n      .zipWithIndex.map { case (name, idx) => \n        val state = opt.createState(idx, model.argsDict(name)) \n        (idx, model.argsDict(name), model.gradDict(name), state, name) \n      }\n    .toArray \n```", "```py\nval testLosses = ArrayBuffer[Float]() \nval testAccuracies = ArrayBuffer[Float]() \nval trainLosses = ArrayBuffer[Float]() \nval trainAccuracies = ArrayBuffer[Float]()     \n```", "```py\nvar step = 1 \nwhile (step * batchSize <= trainingIters) { \n    val (batchTrainData, batchTrainLabel) = { \n        val idx = ((step - 1) * batchSize) % trainingDataCount \n        if (idx + batchSize <= trainingDataCount) { \n          val datas = trainData.drop(idx).take(batchSize) \n          val labels = trainLabels.drop(idx).take(batchSize) \n          (datas, labels) \n        } else { \n          val right = (idx + batchSize) - trainingDataCount \n          val left = trainingDataCount - idx \n          val datas = trainData.drop(idx).take(left) ++ trainData.take(right) \n          val labels = trainLabels.drop(idx).take(left) ++ trainLabels.take(right) \n          (datas, labels) \n    }  \n} \n```", "```py\nmodel.data.set(batchTrainData.flatten.flatten) \nmodel.label.set(batchTrainLabel) \n```", "```py\nmodel.exec.forward(isTrain = true) \nmodel.exec.backward() \n```", "```py\nparamBlocks.foreach { \n case (idx, weight, grad, state, name) => opt.update(idx, weight, grad, state) \n    } \n```", "```py\nval (acc, loss) = getAccAndLoss(model.exec.outputs(0), batchTrainLabel) \n      trainLosses += loss / batchSize \n      trainAccuracies += acc / batchSize \n```", "```py\ndef getAccAndLoss(pred: NDArray, label: Array[Float], dropNum: Int = 0): (Float, Float) = { \n    val shape = pred.shape \n    val maxIdx = NDArray.argmax_channel(pred).toArray \n    val acc = { \n      val sum = maxIdx.drop(dropNum).zip(label.drop(dropNum)).foldLeft(0f){ case (acc, elem) =>  \n        if (elem._1 == elem._2) acc + 1 else acc \n      } \n      sum \n    } \n    val loss = pred.toArray.grouped(shape(1)).drop(dropNum).zipWithIndex.map { case (array, idx) => \n        array(maxIdx(idx).toInt)   \n      }.map(-Math.log(_)).sum.toFloat    \n (acc, loss)  \n} \n```", "```py\nif ( (step * batchSize % displayIter == 0) || (step == 1) || (step * batchSize > trainingIters) ) { \n        println(s\"Iter ${step * batchSize}, Batch Loss = ${\"%.6f\".format(loss / batchSize)}, \n        Accuracy = ${acc / batchSize}\") \n    }\nIter 1500, Batch Loss = 1.189168, Accuracy = 0.14266667\n Iter 15000, Batch Loss = 0.479527, Accuracy = 0.53866667\n Iter 30000, Batch Loss = 0.293270, Accuracy = 0.83933336\n Iter 45000, Batch Loss = 0.192152, Accuracy = 0.78933334\n Iter 60000, Batch Loss = 0.118560, Accuracy = 0.9173333\n Iter 75000, Batch Loss = 0.081408, Accuracy = 0.9486667\n Iter 90000, Batch Loss = 0.109803, Accuracy = 0.9266667\n Iter 105000, Batch Loss = 0.095064, Accuracy = 0.924\n Iter 120000, Batch Loss = 0.087000, Accuracy = 0.9533333\n Iter 135000, Batch Loss = 0.085708, Accuracy = 0.966\n Iter 150000, Batch Loss = 0.068692, Accuracy = 0.9573333\n Iter 165000, Batch Loss = 0.070618, Accuracy = 0.906\n Iter 180000, Batch Loss = 0.089659, Accuracy = 0.908\n Iter 195000, Batch Loss = 0.088301, Accuracy = 0.87333333\n Iter 210000, Batch Loss = 0.067824, Accuracy = 0.9026667\n Iter 225000, Batch Loss = 0.060650, Accuracy = 0.9033333\n Iter 240000, Batch Loss = 0.045368, Accuracy = 0.93733335\n Iter 255000, Batch Loss = 0.049854, Accuracy = 0.96\n Iter 270000, Batch Loss = 0.062839, Accuracy = 0.968\n Iter 285000, Batch Loss = 0.052522, Accuracy = 0.986\n Iter 300000, Batch Loss = 0.060304, Accuracy = 0.98733336\n Iter 315000, Batch Loss = 0.049382, Accuracy = 0.9993333\n Iter 330000, Batch Loss = 0.052441, Accuracy = 0.9766667\n Iter 345000, Batch Loss = 0.050224, Accuracy = 0.9546667\n Iter 360000, Batch Loss = 0.057141, Accuracy = 0.9306667\n Iter 375000, Batch Loss = 0.047664, Accuracy = 0.938\n Iter 390000, Batch Loss = 0.047909, Accuracy = 0.93333334\n Iter 405000, Batch Loss = 0.043014, Accuracy = 0.9533333\n Iter 420000, Batch Loss = 0.054124, Accuracy = 0.952\n Iter 435000, Batch Loss = 0.044272, Accuracy = 0.95133334\n Iter 450000, Batch Loss = 0.058916, Accuracy = 0.96066666\n Iter 465000, Batch Loss = 0.072512, Accuracy = 0.9486667\n Iter 480000, Batch Loss = 0.080431, Accuracy = 0.94733334\n Iter 495000, Batch Loss = 0.072193, Accuracy = 0.9726667\n Iter 510000, Batch Loss = 0.068242, Accuracy = 0.972\n Iter 525000, Batch Loss = 0.057797, Accuracy = 0.964\n Iter 540000, Batch Loss = 0.063531, Accuracy = 0.918\n Iter 555000, Batch Loss = 0.068177, Accuracy = 0.9126667\n Iter 570000, Batch Loss = 0.053257, Accuracy = 0.9206667\n Iter 585000, Batch Loss = 0.058263, Accuracy = 0.9113333\n Iter 600000, Batch Loss = 0.054180, Accuracy = 0.90466666\n Iter 615000, Batch Loss = 0.051008, Accuracy = 0.944\n Iter 630000, Batch Loss = 0.051554, Accuracy = 0.966\n Iter 645000, Batch Loss = 0.059238, Accuracy = 0.9686667\n Iter 660000, Batch Loss = 0.051297, Accuracy = 0.9713333\n Iter 675000, Batch Loss = 0.052069, Accuracy = 0.984\n Iter 690000, Batch Loss = 0.040501, Accuracy = 0.998\n Iter 705000, Batch Loss = 0.053661, Accuracy = 0.96066666\n ter 720000, Batch Loss = 0.037088, Accuracy = 0.958\n Iter 735000, Batch Loss = 0.039404, Accuracy = 0.9533333\n```", "```py\n val (testLoss, testAcc) = test(testDataCount, batchSize, testData, testLabels, model)         \n  println(s\"TEST SET DISPLAY STEP:  Batch Loss = ${\"%.6f\".format(testLoss)}, Accuracy = $testAcc\") \n        testAccuracies += testAcc \n        testLosses += testLoss \n      } \n      step += 1 \n    }     \n  val (finalLoss, accuracy) = test(testDataCount, batchSize, testData, testLabels, model) \n  println(s\"FINAL RESULT: Batch Loss= $finalLoss, Accuracy= $accuracy\") \n```", "```py\nTEST SET DISPLAY STEP: Batch Loss = 0.065859, Accuracy = 0.9138107\n TEST SET DISPLAY STEP: Batch Loss = 0.077047, Accuracy = 0.912114\n TEST SET DISPLAY STEP: Batch Loss = 0.069186, Accuracy = 0.90566677\n TEST SET DISPLAY STEP: Batch Loss = 0.059815, Accuracy = 0.93043774\n TEST SET DISPLAY STEP: Batch Loss = 0.064162, Accuracy = 0.9192399\n TEST SET DISPLAY STEP: Batch Loss = 0.063574, Accuracy = 0.9307771\n TEST SET DISPLAY STEP: Batch Loss = 0.060209, Accuracy = 0.9229725\n TEST SET DISPLAY STEP: Batch Loss = 0.062598, Accuracy = 0.9290804\n TEST SET DISPLAY STEP: Batch Loss = 0.062686, Accuracy = 0.9311164\n TEST SET DISPLAY STEP: Batch Loss = 0.059543, Accuracy = 0.9250085\n TEST SET DISPLAY STEP: Batch Loss = 0.059646, Accuracy = 0.9263658\n TEST SET DISPLAY STEP: Batch Loss = 0.062546, Accuracy = 0.92941976\n TEST SET DISPLAY STEP: Batch Loss = 0.061765, Accuracy = 0.9263658\n TEST SET DISPLAY STEP: Batch Loss = 0.063814, Accuracy = 0.9307771\n TEST SET DISPLAY STEP: Batch Loss = 0.062560, Accuracy = 0.9324737\n TEST SET DISPLAY STEP: Batch Loss = 0.061307, Accuracy = 0.93518835\n TEST SET DISPLAY STEP: Batch Loss = 0.061102, Accuracy = 0.93281305\n TEST SET DISPLAY STEP: Batch Loss = 0.054946, Accuracy = 0.9375636\n TEST SET DISPLAY STEP: Batch Loss = 0.054461, Accuracy = 0.9365456\n TEST SET DISPLAY STEP: Batch Loss = 0.050856, Accuracy = 0.9290804\n TEST SET DISPLAY STEP: Batch Loss = 0.050600, Accuracy = 0.9334917\n TEST SET DISPLAY STEP: Batch Loss = 0.057579, Accuracy = 0.9277231\n TEST SET DISPLAY STEP: Batch Loss = 0.062409, Accuracy = 0.9324737\n TEST SET DISPLAY STEP: Batch Loss = 0.050926, Accuracy = 0.9409569\n TEST SET DISPLAY STEP: Batch Loss = 0.054567, Accuracy = 0.94027823\n FINAL RESULT: Batch Loss= 0.0545671,\n Accuracy= 0.94027823\n```", "```py\ndef test(testDataCount: Int, batchSize: Int, testDatas: Array[Array[Array[Float]]], \n      testLabels: Array[Float], model: LSTMModel): (Float, Float) = { \n    var testLoss, testAcc = 0f \n    for (begin <- 0 until testDataCount by batchSize) { \n      val (testData, testLabel, dropNum) = { \n        if (begin + batchSize <= testDataCount) { \n          val datas = testDatas.drop(begin).take(batchSize) \n          val labels = testLabels.drop(begin).take(batchSize) \n          (datas, labels, 0) \n        } else { \n          val right = (begin + batchSize) - testDataCount \n          val left = testDataCount - begin \n          val datas = testDatas.drop(begin).take(left) ++ testDatas.take(right) \n          val labels = testLabels.drop(begin).take(left) ++ testLabels.take(right) \n          (datas, labels, right) \n        } \n      } \n      //feed the test data to the deepNN \n      model.data.set(testData.flatten.flatten) \n      model.label.set(testLabel) \n\n      model.exec.forward(isTrain = false) \n      val (acc, loss) = getAccAndLoss(model.exec.outputs(0), testLabel) \n      testLoss += loss \n      testAcc += acc \n    } \n    (testLoss / testDataCount, testAcc / testDataCount) \n  } \n```", "```py\nmodel.exec.dispose() \n```", "```py\n    // visualize \n    val xTrain = (0 until trainLosses.length * batchSize by batchSize).toArray.map(_.toDouble) \n    val yTrainL = trainLosses.toArray.map(_.toDouble) \n    val yTrainA = trainAccuracies.toArray.map(_.toDouble) \n\n    val xTest = (0 until testLosses.length * displayIter by displayIter).toArray.map(_.toDouble) \n    val yTestL = testLosses.toArray.map(_.toDouble) \n    val yTestA = testAccuracies.toArray.map(_.toDouble) \n    var series = new MemXYSeries(xTrain, yTrainL, \"Train losses\") \n    val data = new XYData(series)       \n    series = new MemXYSeries(xTrain, yTrainA, \"Train accuracies\") \n    data += series \n    series = new MemXYSeries(xTest, yTestL, \"Test losses\") \n    data += series     \n    series = new MemXYSeries(xTest, yTestA, \"Test accuracies\") \n    data += series \n    val chart = new XYChart(\"Training session's progress over iterations!\", data) \n    chart.showLegend = true \n    val plotter = new JFGraphPlotter(chart)\n    plotter.gui() \n>>>\n```", "```py\n// Hyper parameters for the LSTM training\nval learningRate = 0.001f\nval trainingIters = trainingDataCount * 1000 // Loop 1000 times on the dataset\nval batchSize = 1500 // I would set it 5000 and see the performance\nval displayIter = 15000 // To show test set accuracy during training\nval numLstmLayer = 3 // 5, 7, 9 etc.\n```"]