<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Dropout and Convolutional Neural Networks"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Dropout and Convolutional Neural Networks</h1></div></div></div><p>In this chapter, we continue to look through the algorithms of deep learning. The pre-training that was taken into both DBN and SDA is indeed an innovative method, but deep learning also has other innovative methods. Among these methods, we'll go into the details of the particularly eminent algorithms, which are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The dropout learning algorithm</li><li class="listitem" style="list-style-type: disc">Convolutional neural networks</li></ul></div><p>Both algorithms are necessary to understand and master deep learning, so make sure you keep up.</p><div class="section" title="Deep learning algorithms without pre-training"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec22"/>Deep learning algorithms without pre-training</h1></div></div></div><p>In the previous <a id="id260" class="indexterm"/>chapter, you learned that layer-wise training with pre-training was a breakthrough for DBN and SDA. The reason why these algorithms need pre-training is because an issue occurs where an output error gradually vanishes and doesn't work well in neural networks with simple piled-up layers (we call this the vanishing gradient problem). The deep learning algorithm needs pre-training whether you want to improve the existing method or reinvent it—you might think of it like that.</p><p>However, actually, the deep learning algorithms in this chapter don't have a phase of pre-training, albeit in the deep learning algorithm without pre-training, we can get a result with higher precision and accuracy. Why is such a thing possible? Here is a brief reason. Let's think about why the vanishing gradient problem occurs—remember the equation of backpropagation? A delta in a layer is distributed to all the units of a previous layer by literally propagating networks backward. This means that in the network where all units are tied densely, the value of an error backpropagated to each unit becomes small. As you can see from the equations of backpropagation, the gradients of the weight are obtained by the multiplication of the weights and deltas among the units. Hence, the more terms we have, the more dense the networks are and the more possibilities we have for underflow. This causes the vanishing gradient problem.</p><p>Therefore, we can say that if the preceding problems can be avoided without pre-training, a machine can learn properly with deep neural networks. To achieve this, we need to arrange how to <a id="id261" class="indexterm"/>connect the networks. The deep learning algorithm in this chapter is a method that puts this contrivance into practice using various approaches.</p></div></div>
<div class="section" title="Dropout"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec23"/>Dropout</h1></div></div></div><p>If there's a problem <a id="id262" class="indexterm"/>with the network being tied densely, just force it to be sparse. Then the vanishing gradient problem won't occur and learning can be done properly. The algorithm based on such an idea is the <span class="strong"><strong>dropout</strong></span> algorithm. Dropout for deep neural networks was introduced in <span class="emphasis"><em>Improving neural networks by preventing co adaptation of feature detectors</em></span> (Hinton, et. al. 2012, <a class="ulink" href="http://arxiv.org/pdf/1207.0580.pdf">http://arxiv.org/pdf/1207.0580.pdf</a>) and refined in <span class="emphasis"><em>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</em></span> (Srivastava, et. al. 2014, <a class="ulink" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a>). In dropout, some of the units are, literally, forcibly dropped while training. What does this mean? Let's look at the following figures—firstly, neural networks:</p><div class="mediaobject"><img src="graphics/B04779_04_01.jpg" alt="Dropout"/></div><p>There is nothing <a id="id263" class="indexterm"/>special about this figure. It is a standard neural network with one input layer, two hidden layers, and one output layer. Secondly, the graphical model can be represented as follows by applying dropout to this network:</p><div class="mediaobject"><img src="graphics/B04779_04_02.jpg" alt="Dropout"/></div><p>Units that are dropped from the network are depicted with cross signs. As you can see in the preceding figure, dropped units are interpreted as non-existent in the network. This means we need to <a id="id264" class="indexterm"/>change the structure of the original neural network while the dropout learning algorithm is being applied. Thankfully, applying dropout to the network is not difficult from a computational standpoint. You can simply build a general deep neural network first. Then the dropout learning algorithm can be applied just by adding a dropout mask—a simple binary mask—to all the units in each layer. Units with the value of 0 in the binary mask are the ones that are dropped from the network.</p><p>This may remind you of DA (or SDA) discussed in the previous chapter because DA and dropout look similar at  first glance. Corrupting input data in DA also adds binary masks to the data when implemented. However, there are two remarkably different points between them. First, while it is true that both methods have the process of adding masks to neurons, DA applies the mask only to units in the input layer, whereas dropout applies it to units in the hidden layer. Some of the dropout algorithms apply masks to both the input layer and the hidden layer, but this is still different from DA. Second, in DA, once the corrupt input data is <a id="id265" class="indexterm"/>generated, the data will be used throughout the whole training epochs, but in dropout, the data with different masks will be used in each training epoch. This indicates that a neural network of a different shape is trained in each iteration. Dropout masks will be generated in each layer in each iteration according to the probability of dropout.</p><p>You might have a question—can we train the model even if the shape of the network is different in every step? The answer is yes. You can think of it this way—the network is well trained with dropout because it puts more weights on the existing neurons to reflect the characteristics of the input data. However, dropout has a single demerit, that is, it requires more training epochs than other algorithms to train and optimize the model, which means it takes more time until it is optimized. Another technique is introduced here to reduce this problem. Although the dropout algorithm itself was invented earlier, it was not enough for deep neural networks to gain the ability to generalize and get high precision rates just by using this method. With one more technique that makes the network even more sparse, we achieve deep neural networks to get higher accuracy. This technique is the improvement of the activation function, which we can say is a simple yet elegant solution.</p><p>All of the methods of neural networks explained so far utilize the sigmoid function or hyperbolic tangent as an activation function. You might get great results with these functions. However, as you can see from the shape of them, these curves saturate and kill the gradients when the input values or error values at a certain layer are relatively large or small.</p><p>One of <a id="id266" class="indexterm"/>the activation functions introduced to solve this problem <a id="id267" class="indexterm"/>is the <span class="strong"><strong>rectifier</strong></span>. A unit-applied <a id="id268" class="indexterm"/>rectifier is called a <span class="strong"><strong>Rectified Linear Unit</strong></span> (<span class="strong"><strong>ReLU</strong></span>). We can call the activation function itself ReLU. This function is described in the following equation:</p><div class="mediaobject"><img src="graphics/B04779_04_15.jpg" alt="Dropout"/></div><p>The function can be represented by the following figure:</p><div class="mediaobject"><img src="graphics/B04779_04_03.jpg" alt="Dropout"/></div><p>The broken line in the <a id="id269" class="indexterm"/>figure is the function called a <span class="strong"><strong>softplus function</strong></span>, the <a id="id270" class="indexterm"/>derivative of it is logistic function, which can be described as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_16.jpg" alt="Dropout"/></div><p>This is just for your information: we have the following relations that a smooth approximation to the rectifier. As you can see from the figure above, since the rectifier is far simpler than the sigmoid function and hyperbolic tangent, you can easily guess that the time cost will reduce when it is applied to the deep learning algorithm. In addition, because the derivative of the rectifier—which is necessary when calculating backpropagation errors—is also simple, we can, additionally, shorten the time cost. The equation of the derivative can be represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_17.jpg" alt="Dropout"/></div><p>Since both the rectifier and <a id="id271" class="indexterm"/>the derivative of it are very sparse, we can easily imagine that the neural networks will be also sparse through training. You may have also noticed that we no longer have to worry about gradient saturations because we don't have the causal curves that the sigmoid function and hyperbolic tangent contain anymore.</p><p>With the technique of dropout and the rectifier, a simple deep neural network can learn a problem without pre-training. In terms of the equations used to implement the dropout algorithm, they are not difficult because they are just  simple methods of adding dropout masks to multi-layer perceptrons. Let's look at them in order:</p><div class="mediaobject"><img src="graphics/B04779_04_18.jpg" alt="Dropout"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_04_19.jpg" alt="Dropout"/></span> denotes the activation function, which is, in this case, the rectifier. You see, the previous equation is for units in the hidden layer without dropout. What the dropout does is just apply the mask to them. It can be represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_20.jpg" alt="Dropout"/></div><div class="mediaobject"><img src="graphics/B04779_04_21.jpg" alt="Dropout"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_04_22.jpg" alt="Dropout"/></span> denotes the probability of dropout, which is generally set to 0.5. That's all for forward activation. As you <a id="id272" class="indexterm"/>can see from the equations, the term of the binary mask is the only difference from the ones of general neural networks. In addition, during backpropagation, we also have to add masks to the delta. Suppose we have the following equation:</p><div class="mediaobject"><img src="graphics/B04779_04_23.jpg" alt="Dropout"/></div><p>With this, we can define the delta as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_24.jpg" alt="Dropout"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_04_25.jpg" alt="Dropout"/></span> denotes the evaluation function (these equations are the same as we mentioned in <a class="link" href="ch02.html" title="Chapter 2. Algorithms for Machine Learning – Preparing for Deep Learning">Chapter 2</a>, <span class="emphasis"><em>Algorithms for Machine Learning – Preparing for Deep Learning</em></span>). We get the following equation:</p><div class="mediaobject"><img src="graphics/B04779_04_26.jpg" alt="Dropout"/></div><p>Here, the delta can be <a id="id273" class="indexterm"/>described as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_27.jpg" alt="Dropout"/></div><p>Now we have all the equations necessary for implementation, let's dive into the implementation. The package structure is as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_04.jpg" alt="Dropout"/></div><p>First, what we need to <a id="id274" class="indexterm"/>have is the rectifier. Like other activation functions, we implement it in <code class="literal">ActivationFunction.java</code> as <code class="literal">ReLU</code>:</p><div class="informalexample"><pre class="programlisting">public static double ReLU(double x) {
   if(x &gt; 0) {
       return x;
   } else {
       return 0.;
   }
}</pre></div><p>Also, we define <code class="literal">dReLU</code> as the derivative of the rectifier:</p><div class="informalexample"><pre class="programlisting">public static double dReLU(double y) {
   if(y &gt; 0) {
       return 1.;
   } else {
       return 0.;
   }
}</pre></div><p>Accordingly, we updated <a id="id275" class="indexterm"/>the constructor of <code class="literal">HiddenLayer.java</code> to support <code class="literal">ReLU</code>:</p><div class="informalexample"><pre class="programlisting">if (activation == "sigmoid" || activation == null) {

   this.activation = (double x) -&gt; sigmoid(x);
   this.dactivation = (double x) -&gt; dsigmoid(x);

} else if (activation == "tanh") {

   this.activation = (double x) -&gt; tanh(x);
   this.dactivation = (double x) -&gt; dtanh(x);

} else if (activation == "ReLU") {

   this.activation = (double x) -&gt; ReLU(x);
   this.dactivation = (double x) -&gt; dReLU(x);

} else {
   throw new IllegalArgumentException("activation function not supported");
}</pre></div><p>Now let's have a look at <code class="literal">Dropout.java</code>. In the source code, we'll build the neural networks of two hidden layers, and the probability of dropout is set to 0.5:</p><div class="informalexample"><pre class="programlisting">int[] hiddenLayerSizes = {100, 80};
double pDropout = 0.5;</pre></div><p>The constructor of <code class="literal">Dropout.java</code> can be written as follows (since the network is just a simple deep neural network, the code is also simple):</p><div class="informalexample"><pre class="programlisting">public Dropout(int nIn, int[] hiddenLayerSizes, int nOut, Random rng, String activation) {

   if (rng == null) rng = new Random(1234);

   if (activation == null) activation = "ReLU";

   this.nIn = nIn;
   this.hiddenLayerSizes = hiddenLayerSizes;
   this.nOut = nOut;
   this.nLayers = hiddenLayerSizes.length;
   this.hiddenLayers = new HiddenLayer[nLayers];
   this.rng = rng;

   // construct multi-layer
   for (int i = 0; i &lt; nLayers; i++) {
       int nIn_;
       if (i == 0) nIn_ = nIn;
       else nIn_ = hiddenLayerSizes[i - 1];

       // construct hidden layer
       hiddenLayers[i] = new HiddenLayer(nIn_, hiddenLayerSizes[i], null, null, rng, activation);
   }

   // construct logistic layer
   logisticLayer = new LogisticRegression(hiddenLayerSizes[nLayers - 1], nOut);
}</pre></div><p>As explained, now we have the <code class="literal">HiddenLayer</code> class with <code class="literal">ReLU</code> support, we can use <code class="literal">ReLU</code> as the activation function.</p><p>Once a model is built, what we <a id="id276" class="indexterm"/>do next is train the model with dropout. The method for training is simply called <code class="literal">train</code>. Since we need some layer inputs when calculating the backpropagation errors, we define the variable called <code class="literal">layerInputs</code> first to cache their respective input values:</p><div class="informalexample"><pre class="programlisting">List&lt;double[][]&gt; layerInputs = new ArrayList&lt;&gt;(nLayers+1);
layerInputs.add(X);</pre></div><p>Here, <code class="literal">X</code> is the original training data. We also need to cache the dropout masks for each layer for backpropagation, so let's define it as <code class="literal">dropoutMasks</code>:</p><div class="informalexample"><pre class="programlisting">List&lt;int[][]&gt; dropoutMasks = new ArrayList&lt;&gt;(nLayers);</pre></div><p>Training begins in a forward <a id="id277" class="indexterm"/>activation fashion. Look how we apply the dropout masks to the value; we merely multiply the activated values and binary masks:</p><div class="informalexample"><pre class="programlisting">// forward hidden layers
for (int layer = 0; layer &lt; nLayers; layer++) {

   double[] x_;  // layer input
   double[][] Z_ = new double[minibatchSize][hiddenLayerSizes[layer]];
   int[][] mask_ = new int[minibatchSize][hiddenLayerSizes[layer]];

   for (int n = 0; n &lt; minibatchSize; n++) {

       if (layer == 0) {
           x_ = X[n];
       } else {
           x_ = Z[n];
       }


       Z_[n] = hiddenLayers[layer].forward(x_);
       mask_[n] = dropout(Z_[n], pDrouput);  // apply dropout mask to units
   }

   Z = Z_;
   layerInputs.add(Z.clone());

   dropoutMasks.add(mask_);
}</pre></div><p>The dropout method is defined in <code class="literal">Dropout.java</code> as well. As explained in the equation, this method returns the values following the Bernoulli distribution:</p><div class="informalexample"><pre class="programlisting">public int[] dropout(double[] z, double p) {

   int size = z.length;
   int[] mask = new int[size];

   for (int i = 0; i &lt; size; i++) {
       mask[i] = binomial(1, 1 - p, rng);
       z[i] *= mask[i]; // apply mask
   }

   return mask;
}</pre></div><p>After forward propagation through the hidden layers, training data is forward propagated in the output layer of the logistic regression. Then, in the same way as the other neural networks algorithm, the deltas <a id="id278" class="indexterm"/>of each layer are going back through the network. Here, we apply the cached masks to the delta so that its values are backpropagated in the same network:</p><div class="informalexample"><pre class="programlisting">// forward &amp; backward output layer
D = logisticLayer.train(Z, T, minibatchSize, learningRate);

// backward hidden layers
for (int layer = nLayers - 1; layer &gt;= 0; layer--) {

   double[][] Wprev_;

   if (layer == nLayers - 1) {
       Wprev_ = logisticLayer.W;
   } else {
       Wprev_ = hiddenLayers[layer+1].W;
   }

   // apply mask to delta as well
   for (int n = 0; n &lt; minibatchSize; n++) {
       int[] mask_ = dropoutMasks.get(layer)[n];

       for (int j = 0; j &lt; D[n].length; j++) {
           D[n][j] *= mask_[j];
       }
   }

   D = hiddenLayers[layer].backward(layerInputs.get(layer), layerInputs.get(layer+1), D, Wprev_, minibatchSize, learningRate);
}</pre></div><p>After the training comes the test phase. But before we apply the test data to the tuned model, we need to configure the weights of the network. Dropout masks can't be simply applied to the test data because when masked, the shape of each network will be differentiated, and this may return different results because a certain unit may have a significant effect on certain <a id="id279" class="indexterm"/>features. Instead, what we do is smooth the weights of the network, which means we simulate the network where whole units are equally masked. This can be done using the following equation:</p><div class="mediaobject"><img src="graphics/B04779_04_28.jpg" alt="Dropout"/></div><p>As you can see from the equation, all the weights are multiplied by the probability of non-dropout. We define the method for this as <code class="literal">pretest</code>:</p><div class="informalexample"><pre class="programlisting">public void pretest(double pDropout) {

   for (int layer = 0; layer &lt; nLayers; layer++) {

       int nIn_, nOut_;

       if (layer == 0) {
           nIn_ = nIn;
       } else {
           nIn_ = hiddenLayerSizes[layer];
       }

       if (layer == nLayers - 1) {
           nOut_ = nOut;
       } else {
           nOut_ = hiddenLayerSizes[layer+1];
       }

       for (int j = 0; j &lt; nOut_; j++) {
           for (int i = 0; i &lt; nIn_; i++) {
               hiddenLayers[layer].W[j][i] *= 1 - pDropout;
           }
       }
   }
}</pre></div><p>We have to call this method once before the test. Since the network is a general multi-layered neural network, what we need to do for the prediction is just perform forward activation through the network:</p><div class="informalexample"><pre class="programlisting">public Integer[] predict(double[] x) {

   double[] z = new double[0];

   for (int layer = 0; layer &lt; nLayers; layer++) {

       double[] x_;

       if (layer == 0) {
           x_ = x;
       } else {
           x_ = z.clone();
       }

       z = hiddenLayers[layer].forward(x_);
   }

   return logisticLayer.predict(z);
}</pre></div><p>Compared to DBN and <a id="id280" class="indexterm"/>SDA, the dropout MLP is far simpler and easier to implement. It suggests the possibility that with a mixture of two or more techniques, we can get higher precision.</p></div>
<div class="section" title="Convolutional neural networks"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec24"/>Convolutional neural networks</h1></div></div></div><p>All the machine <a id="id281" class="indexterm"/>learning/deep learning algorithms you have learned about imply that the type of input data is one-dimensional. When you look at a real-world application, however, data is not necessarily one-dimensional. A typical case is an image. Though we can still convert two-dimensional (or higher-dimensional) data into a one-dimensional array from the standpoint of implementation, it would be better to build a model that can handle two-dimensional data as it is. Otherwise, some information embedded in the data, such as positional relationships, might be lost when flattened to one dimension.</p><p>To solve this problem, an algorithm called <span class="strong"><strong>Convolutional Neural Networks</strong></span> (<span class="strong"><strong>CNN</strong></span>) was proposed. In CNN, features are extracted from two-dimensional input data through convolutional layers and pooling layers (this will be explained later), and then these features are put into general multi-layer perceptrons. This preprocessing for MLP is inspired by human <a id="id282" class="indexterm"/>visual areas and can be described as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Segment the input data into several domains. This process is equivalent to a human's receptive fields.</li><li class="listitem" style="list-style-type: disc">Extract the features from the respective domains, such as edges and position aberrations.</li></ul></div><p>With these features, MLP can classify data accordingly.</p><p>The graphical model of CNN is not similar to that of other neural networks. Here is a briefly outlined example of CNN:</p><div class="mediaobject"><img src="graphics/B04779_04_05.jpg" alt="Convolutional neural networks"/></div><p>You may not fully understand what CNN is just from the figure. Moreover, you might feel that CNN is relatively complicated and difficult to understand. But you don't have to worry about that. It is a fact that CNN has a complicated graphical model and has unfamiliar terminologies such as convolution and pooling, which you don't hear about in other deep learning algorithms. However, when you look at the model step by step, there's nothing too difficult to understand. CNN consists of several types of layers specifically adjusted for image recognition. Let's look at each layer one by one in the next subsection. In the preceding figure, there are two convolution and pooling (<span class="strong"><strong>Subsampling</strong></span>) layers and fully connected multi-layer perceptrons in the network. We'll see what the <a id="id283" class="indexterm"/>convolutional layers do first.</p><div class="section" title="Convolution"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec25"/>Convolution</h2></div></div></div><p>Convolutional <a id="id284" class="indexterm"/>layers literally <a id="id285" class="indexterm"/>perform convolution, which means applying <a id="id286" class="indexterm"/>several filters to the image to extract features. These filters are called <span class="strong"><strong>kernels</strong></span>, and <a id="id287" class="indexterm"/>convolved images are called <span class="strong"><strong>feature maps</strong></span>. <a id="id288" class="indexterm"/>Let's see the following image (decomposed to color values) and kernel:</p><div class="mediaobject"><img src="graphics/B04779_04_06.jpg" alt="Convolution"/></div><p>With these, what is done with convolution is illustrated as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_07.jpg" alt="Convolution"/></div><p>The kernel slides <a id="id289" class="indexterm"/>across the image and returns the summation of its values within the kernel as a multiplication filter. You might have noticed that you can extract many kinds of features by changing kernel values. Suppose you have kernels with values as described here:</p><div class="mediaobject"><img src="graphics/B04779_04_08.jpg" alt="Convolution"/></div><p>You see that the kernel on the left extracts the edges of the image because it accentuates the color differences, and the one on the right blurs the image because it degrades the original values. The great thing about CNN is that in convolutional layers, you don't have to set these kernel values manually. Once initialized, CNN itself will learn the proper values through the learning algorithm (which means parameters trained in CNN are the weights of kernels) and can classify images very precisely in the end.</p><p>Now, let's think about why neural networks with convolutional layers (kernels) can predict <a id="id290" class="indexterm"/>with higher precision rates. The key here is the <span class="strong"><strong>local receptive field</strong></span>. In most layers in neural networks except CNN, all neurons are fully connected. This even causes slightly different data, for example, one-pixel parallel data would be regarded as completely different data in the network because this data is propagated to different neurons in hidden layers, whereas humans can easily understand they are the same. With fully connected layers, it is true that neural networks can recognize more complicated patterns, but at the same time <a id="id291" class="indexterm"/>they lack the ability to generalize and lack flexibility. In contrast, you can see that connections among neurons in convolutional layers are limited to their kernel size, making the model more robust to translated images. Thus, neural networks with their receptive fields limited locally <a id="id292" class="indexterm"/>are able to acquire <span class="strong"><strong>translation invariance</strong></span> when kernels are optimized.</p><p>Each kernel has its own <a id="id293" class="indexterm"/>values and extracts respective features from the image. Please bear in mind that the number of feature maps and the number of kernels are always the same, which means if we have 20 kernels, we have also twenty feature maps, that is, convolved images. This can be confusing, so let's explore another example. Given a gray-scaled <span class="strong"><strong>image</strong></span> and twenty <span class="strong"><strong>kernels</strong></span>, how many <span class="strong"><strong>feature maps</strong></span> are there? The answer is twenty. These twenty images will be propagated to the next layer. This is illustrated as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_09.jpg" alt="Convolution"/></div><p>So, how about this: suppose we have a 3-channeled image (for example, an RGB image) and the number of kernels is twenty, how many feature maps will there be? The answer is, again, twenty. But this time, the process of convolution is different from the one with gray-scaled, that is 1-channeled, images. When the image has multiple channels, kernels will <a id="id294" class="indexterm"/>be adapted separately for each channel. Therefore, in this case, we will have a total of 60 convolved images first, composed of twenty mapped images for each of the 3 channels. Then, all the convolved images originally from the same image will be combined into one feature map. As a result, we will have twenty feature maps. In other words, images are decomposed into different channeled data, applied kernels, and then combined into mixed-channeled images again. You can easily imagine from the flow in the preceding diagram that when we apply a kernel to a multi-channeled image to make decomposed images, the same kernel should be applied. This flow can be seen in the following figure:</p><div class="mediaobject"><img src="graphics/B04779_04_10.jpg" alt="Convolution"/></div><p>Computationally, the <a id="id295" class="indexterm"/>number of kernels is represented with the dimension of the weights' tensor. You'll see how to implement this later.</p></div><div class="section" title="Pooling"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec26"/>Pooling</h2></div></div></div><p>What pooling <a id="id296" class="indexterm"/>layers do is rather simple compared to convolutional layers. They actually do not train or learn by themselves but just <a id="id297" class="indexterm"/>downsample images propagated from convolutional layers. Why should we bother to do downsampling? You might think it may lose some significant information from the data. But here, again, as with convolutional layers, this process is necessary to make the network keep its translation invariance.</p><p>There are several ways of downsampling, but among them, max-pooling is the most famous. It can be represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_11.jpg" alt="Pooling"/></div><p>In a max-pooling <a id="id298" class="indexterm"/>layer, the input image is <a id="id299" class="indexterm"/>segmented into a set of non-overlapping sub-data and the maximum value is output from each data. This process not only keeps its translation invariance but also reduces the computation for the upper layers. With convolution and pooling, CNN can acquire robust features from the input.</p></div><div class="section" title="Equations and implementations"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec27"/>Equations and implementations</h2></div></div></div><p>Now we know <a id="id300" class="indexterm"/>what convolution and <a id="id301" class="indexterm"/>max-pooling are, let's describe the whole model with equations. We'll use the figure of convolution below in equations:</p><div class="mediaobject"><img src="graphics/B04779_04_12.jpg" alt="Equations and implementations"/></div><p>As shown in the <a id="id302" class="indexterm"/>figure, if we have an image <a id="id303" class="indexterm"/>with a size of <span class="inlinemediaobject"><img src="graphics/B04779_04_29.jpg" alt="Equations and implementations"/></span> and kernels with a size of <span class="inlinemediaobject"><img src="graphics/B04779_04_30.jpg" alt="Equations and implementations"/></span>, the convolution can be represented as:</p><div class="mediaobject"><img src="graphics/B04779_04_31.jpg" alt="Equations and implementations"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_04_32.jpg" alt="Equations and implementations"/></span> is the weight of the kernel, that is, the model parameter. Just bear in mind we've described each summation from 0, not from 1, so you get a better understanding. The equation, however, is not enough when we think about multi-convolutional layers because it does not have the information from the channel. Fortunately, it's not difficult because we can implement it just by adding one parameter to the kernel. The extended equation can be shown as:</p><div class="mediaobject"><img src="graphics/B04779_04_33.jpg" alt="Equations and implementations"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_04_34.jpg" alt="Equations and implementations"/></span> denotes <a id="id304" class="indexterm"/>the channel of the image. If <a id="id305" class="indexterm"/>the number of kernels is <span class="inlinemediaobject"><img src="graphics/B04779_04_35.jpg" alt="Equations and implementations"/></span> and the number of channels is <span class="inlinemediaobject"><img src="graphics/B04779_04_36.jpg" alt="Equations and implementations"/></span>, we have <span class="inlinemediaobject"><img src="graphics/B04779_04_37.jpg" alt="Equations and implementations"/></span>. Then, you can see from the equation that the size of the convolved image is <span class="inlinemediaobject"><img src="graphics/B04779_04_38.jpg" alt="Equations and implementations"/></span>.</p><p>After the convolution, all the convolved values will be activated by the activation function. We'll implement CNN with the rectifier—the most popular function these days—but you may use the sigmoid function, the hyperbolic tangent, or any other activation functions available instead. With the activation, we have:</p><div class="mediaobject"><img src="graphics/B04779_04_39.jpg" alt="Equations and implementations"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_04_40.jpg" alt="Equations and implementations"/></span> denotes the bias, the other model parameter. You can see that <span class="inlinemediaobject"><img src="graphics/B04779_04_40.jpg" alt="Equations and implementations"/></span> doesn't have subscripts of <span class="inlinemediaobject"><img src="graphics/B04779_04_41.jpg" alt="Equations and implementations"/></span> and <span class="inlinemediaobject"><img src="graphics/B04779_04_42.jpg" alt="Equations and implementations"/></span>, that is, we have <span class="inlinemediaobject"><img src="graphics/B04779_04_43.jpg" alt="Equations and implementations"/></span>, a one-dimensional array. Thus, we have <a id="id306" class="indexterm"/>forward-propagated the values of <a id="id307" class="indexterm"/>the convolutional layer.</p><p>Next comes the max-pooling layer. The propagation can simply be written as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_44.jpg" alt="Equations and implementations"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_04_45.jpg" alt="Equations and implementations"/></span> and <span class="inlinemediaobject"><img src="graphics/B04779_04_46.jpg" alt="Equations and implementations"/></span> are the size of pooling filter and <span class="inlinemediaobject"><img src="graphics/B04779_04_47.jpg" alt="Equations and implementations"/></span>. Usually, <span class="inlinemediaobject"><img src="graphics/B04779_04_45.jpg" alt="Equations and implementations"/></span> and <span class="inlinemediaobject"><img src="graphics/B04779_04_46.jpg" alt="Equations and implementations"/></span> are set to the same value of 2 ~ 4.</p><p>These two layers, the convolutional layer and the max-pooling layer, tend to be arrayed in this order, but you don't necessarily have to follow it. You can put two convolutional layers before max-pooling, for example. Also, while we put the activation right after the convolution, sometimes it is set after the max-pooling instead of the convolution. For simplicity, however, we'll implement CNN with the order and sequence of convolution–activation–max-pooling.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip06"/>Tip</h3><p>One important note here is that although the kernel weights will be learned from the data, the architecture, the size of kernel, and the size of pooling are all parameters.</p></div></div><p>The simple MLP follows after convolutional layers and max-pooling layers to classify the data. Here, since <a id="id308" class="indexterm"/>MLP can only accept one-dimensional data, we need to flatten the downsampled data as preprocessing to adapt it to the input layer of MLP. The extraction of features was completed before MLP, so <a id="id309" class="indexterm"/>formatting the data into one dimension won't be a problem. Thus, CNN can classify the image data once the model is optimized. To do this, as with other neural networks, the backpropagation algorithm is applied to CNN to train the model. We won't mention the equation related to MLP here.</p><p>The error from the input layer of MLP is backpropagated to the max-pooling layer, and this time it is unflattened to two dimensions to be adapted properly to the model. Since the max-pooling layer doesn't have model parameters, it simply backpropagates the error to the previous layer. The equation can be described as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_48.jpg" alt="Equations and implementations"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_04_49.jpg" alt="Equations and implementations"/></span> denotes the evaluation function. This error is then backpropagated to the convolutional layer, and with it we can calculate the gradients of the weight and the bias. Since the activation with the bias comes before the convolution when backpropagating, let's see the gradient of the bias first, as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_50.jpg" alt="Equations and implementations"/></div><p>To proceed with this equation, we define the following:</p><div class="mediaobject"><img src="graphics/B04779_04_51.jpg" alt="Equations and implementations"/></div><p>We also <a id="id310" class="indexterm"/>define:</p><div class="mediaobject"><img src="graphics/B04779_04_52.jpg" alt="Equations and implementations"/></div><p>With <a id="id311" class="indexterm"/>these, we get:</p><div class="mediaobject"><img src="graphics/B04779_04_53.jpg" alt="Equations and implementations"/></div><p>We can calculate the gradient of the weight (kernel) in the same way:</p><div class="mediaobject"><img src="graphics/B04779_04_54.jpg" alt="Equations and implementations"/></div><p>Thus, we can update the model parameters. If we have just one convolutional and max-pooling layer, the <a id="id312" class="indexterm"/>equations just given <a id="id313" class="indexterm"/>are all that we need. When we think of multi-convolutional layers, however, we also need to calculate the error of the convolutional layers. This can be represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_55.jpg" alt="Equations and implementations"/></div><p>Here, we get:</p><div class="mediaobject"><img src="graphics/B04779_04_56.jpg" alt="Equations and implementations"/></div><p>So, the error <a id="id314" class="indexterm"/>can be written as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_57.jpg" alt="Equations and implementations"/></div><p>We have to <a id="id315" class="indexterm"/>be careful when calculating this because there's a possibility of <span class="inlinemediaobject"><img src="graphics/B04779_04_58.jpg" alt="Equations and implementations"/></span> or <span class="inlinemediaobject"><img src="graphics/B04779_04_59.jpg" alt="Equations and implementations"/></span>, where there's no element in between the feature maps. To solve this, we need to add zero paddings to the top-left edges of them. Then, the equation is simply a convolution with the kernel flipped along both axes. Though the equations in CNN might look complicated, they are just a pile of summations of each parameter.</p><p>With all the previous equations, we can now implement CNN, so let's see how we do it. The package structure is as follows:</p><div class="mediaobject"><img src="graphics/B04779_04_13.jpg" alt="Equations and implementations"/></div><p>
<code class="literal">ConvolutionNeuralNetworks.java</code> is used to build the model outline of CNN, and the exact <a id="id316" class="indexterm"/>algorithms for training in the convolutional layers and max-pooling layers, forward propagations, and backpropagations are written in <code class="literal">ConvolutionPoolingLayer.java</code>. In the demo, we have the original image size of <code class="literal">12 </code>
<span class="inlinemediaobject"><img src="graphics/B04779_04_60.jpg" alt="Equations and implementations"/></span>
<code class="literal"> 12</code> with one channel:</p><div class="informalexample"><pre class="programlisting">final int[] imageSize = {12, 12};
final int channel = 1;</pre></div><p>The image will be propagated through two <code class="literal">ConvPoolingLayer</code> (convolutional layers and max-pooling layers). The number of kernels in the first layer is set to <code class="literal">10</code> with the size of <code class="literal">3 </code>
<span class="inlinemediaobject"><img src="graphics/B04779_04_60.jpg" alt="Equations and implementations"/></span>
<code class="literal"> 3</code> and <code class="literal">20</code> <a id="id317" class="indexterm"/>with the size of <code class="literal">2 </code>
<span class="inlinemediaobject"><img src="graphics/B04779_04_60.jpg" alt="Equations and implementations"/></span>
<code class="literal"> 2</code> in the second layer. The size of the pooling filters are both set to <code class="literal">2 </code>
<span class="inlinemediaobject"><img src="graphics/B04779_04_60.jpg" alt="Equations and implementations"/></span>
<code class="literal"> 2</code>:</p><div class="informalexample"><pre class="programlisting">int[] nKernels = {10, 20};
int[][] kernelSizes = { {3, 3}, {2, 2} };
int[][] poolSizes = { {2, 2}, {2, 2} };</pre></div><p>After the second max-pooling layer, there are <code class="literal">20</code> feature maps with the size of <code class="literal">2 </code>
<span class="inlinemediaobject"><img src="graphics/B04779_04_60.jpg" alt="Equations and implementations"/></span>
<code class="literal"> 2</code>. These maps are then flattened to <code class="literal">80</code> units and will be forwarded to the hidden layer with <code class="literal">20</code> neurons:</p><div class="informalexample"><pre class="programlisting">int nHidden = 20;</pre></div><p>We then create simple demo data of three patterns with a little noise. We'll leave out the code to create demo data here. If we illustrate the data, here is an example of it:</p><div class="mediaobject"><img src="graphics/B04779_04_14.jpg" alt="Equations and implementations"/></div><p>Now let's build the model. The constructor is similar to other deep learning models and rather simple. We <a id="id318" class="indexterm"/>construct multi <a id="id319" class="indexterm"/>
<code class="literal">ConvolutionPoolingLayers</code> first. The size for each layer is calculated in the method:</p><div class="informalexample"><pre class="programlisting">// construct convolution + pooling layers
for (int i = 0; i &lt; nKernels.length; i++) {
   int[] size_;
   int channel_;

   if (i == 0) {
       size_ = new int[]{imageSize[0], imageSize[1]};
       channel_ = channel;
   } else {
       size_ = new int[]{pooledSizes[i-1][0], pooledSizes[i-1][1]};
       channel_ = nKernels[i-1];
   }

   convolvedSizes[i] = new int[]{size_[0] - kernelSizes[i][0] + 1, size_[1] - kernelSizes[i][1] + 1};
   pooledSizes[i] = new int[]{convolvedSizes[i][0] / poolSizes[i][0], convolvedSizes[i][1] / poolSizes[i][0]};

   convpoolLayers[i] = new ConvolutionPoolingLayer(size_, channel_, nKernels[i], kernelSizes[i], poolSizes[i], convolvedSizes[i], pooledSizes[i], rng, activation);
}</pre></div><p>When you look at the constructor of the <code class="literal">ConvolutionPoolingLayer</code> class, you can see how the kernel and the bias are defined:</p><div class="informalexample"><pre class="programlisting">if (W == null) {

   W = new double[nKernel][channel][kernelSize[0]][kernelSize[1]];

   double in_ = channel * kernelSize[0] * kernelSize[1];
   double out_ = nKernel * kernelSize[0] * kernelSize[1] / (poolSize[0] * poolSize[1]);
   double w_ = Math.sqrt(6. / (in_ + out_));

   for (int k = 0; k &lt; nKernel; k++) {
       for (int c = 0; c &lt; channel; c++) {
           for (int s = 0; s &lt; kernelSize[0]; s++) {
               for (int t = 0; t &lt; kernelSize[1]; t++) {
                   W[k][c][s][t] = uniform(-w_, w_, rng);
               }
           }
       }
   }
}

if (b == null) b = new double[nKernel];</pre></div><p>Next comes the <a id="id320" class="indexterm"/>construction of MLP. Don't forget to flatten the downsampled data when passing through them:</p><div class="informalexample"><pre class="programlisting">// build MLP
flattenedSize = nKernels[nKernels.length-1] * pooledSizes[pooledSizes.length-1][0] * pooledSizes[pooledSizes.length-1][1];

// construct hidden layer
hiddenLayer = new HiddenLayer(flattenedSize, nHidden, null, null, rng, activation);

// construct output layer
logisticLayer = new LogisticRegression(nHidden, nOut);</pre></div><p>Once the model is built, we need to train it. In the <code class="literal">train</code> method, we cache all the forward-propagated data so that we can utilize it when backpropagating:</p><div class="informalexample"><pre class="programlisting">// cache pre-activated, activated, and downsampled inputs of each convolution + pooling layer for backpropagation
List&lt;double[][][][]&gt; preActivated_X = new ArrayList&lt;&gt;(nKernels.length);
List&lt;double[][][][]&gt; activated_X = new ArrayList&lt;&gt;(nKernels.length);
List&lt;double[][][][]&gt; downsampled_X = new ArrayList&lt;&gt;(nKernels.length+1);  // +1 for input X
downsampled_X.add(X);

for (int i = 0; i &lt; nKernels.length; i++) {
   preActivated_X.add(new double[minibatchSize][nKernels[i]][convolvedSizes[i][0]][convolvedSizes[i][1]]);
   activated_X.add(new double[minibatchSize][nKernels[i]][convolvedSizes[i][0]][convolvedSizes[i][1]]);
   downsampled_X.add(new double[minibatchSize][nKernels[i]][convolvedSizes[i][0]][convolvedSizes[i][1]]);
}</pre></div><p>
<code class="literal">preActivated_X</code> <a id="id321" class="indexterm"/>is defined for convolved <a id="id322" class="indexterm"/>feature maps, <code class="literal">activated_X</code> for activated features, and <code class="literal">downsampled_X</code> for downsampled features. We put and cache the original data into <code class="literal">downsampled_X</code>. The actual training begins with forward propagation through convolution and max-pooling:</p><div class="informalexample"><pre class="programlisting">// forward convolution + pooling layers
double[][][] z_ = X[n].clone();
for (int i = 0; i &lt; nKernels.length; i++) {
   z_ = convpoolLayers[i].forward(z_, preActivated_X.get(i)[n], activated_X.get(i)[n]);
   downsampled_X.get(i+1)[n] = z_.clone();
}</pre></div><p>The <code class="literal">forward</code> method of <code class="literal">ConvolutionPoolingLayer</code> is simple and consists of <code class="literal">convolve</code> and <code class="literal">downsample</code>. The <code class="literal">convolve</code> function does the convolution, and <code class="literal">downsample</code> does the max-pooling:</p><div class="informalexample"><pre class="programlisting">public double[][][] forward(double[][][] x, double[][][] preActivated_X, double[][][] activated_X) {

   double[][][] z = this.convolve(x, preActivated_X, activated_X);
   return  this.downsample(z);</pre></div><p>The values of <code class="literal">preActivated_X</code> and <code class="literal">activated_X</code> are set inside the convolve method. You can see that the method simply follows the equations explained previously:</p><div class="informalexample"><pre class="programlisting">public double[][][] convolve(double[][][] x, double[][][] preActivated_X, double[][][] activated_X) {

   double[][][] y = new double[nKernel][convolvedSize[0]][convolvedSize[1]];

   for (int k = 0; k &lt; nKernel; k++) {
       for (int i = 0; i &lt; convolvedSize[0]; i++) {
           for(int j = 0; j &lt; convolvedSize[1]; j++) {

               double convolved_ = 0.;

               for (int c = 0; c &lt; channel; c++) {
                   for (int s = 0; s &lt; kernelSize[0]; s++) {
                       for (int t = 0; t &lt; kernelSize[1]; t++) {
                           convolved_ += W[k][c][s][t] * x[c][i+s][j+t];
                       }
                   }
               }

               // cache pre-activated inputs
               preActivated_X[k][i][j] = convolved_ + b[k];
               activated_X[k][i][j] = this.activation.apply(preActivated_X[k][i][j]);
               y[k][i][j] = activated_X[k][i][j];
           }
       }
   }

   return y;
}</pre></div><p>The <code class="literal">downsample</code> <a id="id323" class="indexterm"/>method follows the <a id="id324" class="indexterm"/>equations as well:</p><div class="informalexample"><pre class="programlisting">public double[][][] downsample(double[][][] x) {

   double[][][] y = new double[nKernel][pooledSize[0]][pooledSize[1]];

   for (int k = 0; k &lt; nKernel; k++) {
       for (int i = 0; i &lt; pooledSize[0]; i++) {
           for (int j = 0; j &lt; pooledSize[1]; j++) {

               double max_ = 0.;

               for (int s = 0; s &lt; poolSize[0]; s++) {
                   for (int t = 0; t &lt; poolSize[1]; t++) {

                       if (s == 0 &amp;&amp; t == 0) {
                           max_ = x[k][poolSize[0]*i][poolSize[1]*j];
                           continue;
                       }
                       if (max_ &lt; x[k][poolSize[0]*i+s][poolSize[1]*j+t]) {
                           max_ = x[k][poolSize[0]*i+s][poolSize[1]*j+t];
                       }
                   }
               }

               y[k][i][j] = max_;
           }
       }
   }

   return y;
}</pre></div><p>You might think we've made some mistake here because there are so many <code class="literal">for</code> loops in these methods, but <a id="id325" class="indexterm"/>actually there's nothing <a id="id326" class="indexterm"/>wrong. As you can see from the equations of CNN, the algorithm requires many loops because it has many parameters. The code here works well, but practically, you could define and move the part of the innermost loops to other methods. Here, to get a better understanding, we've implemented CNN with many nested loops so that we can compare the code with equations. You can see now that CNN requires a lot of time to get results.</p><p>After we downsample the data, we need to flatten it:</p><div class="informalexample"><pre class="programlisting">// flatten output to make it input for fully connected MLP
double[] x_ = this.flatten(z_);
flattened_X[n] = x_.clone();</pre></div><p>The data is then forwarded to the hidden layer:</p><div class="informalexample"><pre class="programlisting">// forward hidden layer
Z[n] = hiddenLayer.forward(x_);</pre></div><p>Multi-class <a id="id327" class="indexterm"/>logistic regression is used in the <a id="id328" class="indexterm"/>output layer and the delta is then backpropagated to the hidden layer:</p><div class="informalexample"><pre class="programlisting">// forward &amp; backward output layer
dY = logisticLayer.train(Z, T, minibatchSize, learningRate);

// backward hidden layer
dZ = hiddenLayer.backward(flattened_X, Z, dY, logisticLayer.W, minibatchSize, learningRate);

// backpropagate delta to input layer
for (int n = 0; n &lt; minibatchSize; n++) {
   for (int i = 0; i &lt; flattenedSize; i++) {
       for (int j = 0; j &lt; nHidden; j++) {
           dX_flatten[n][i] += hiddenLayer.W[j][i] * dZ[n][j];
       }
   }

   dX[n] = unflatten(dX_flatten[n]);  // unflatten delta
}

// backward convolution + pooling layers
dC = dX.clone();
for (int i = nKernels.length-1; i &gt;= 0; i--) {
   dC = convpoolLayers[i].backward(downsampled_X.get(i), preActivated_X.get(i), activated_X.get(i), downsampled_X.get(i+1), dC, minibatchSize, learningRate);
}</pre></div><p>The <code class="literal">backward</code> method of <code class="literal">ConvolutionPoolingLayer</code> is the same as <code class="literal">forward</code>, also simple. Backpropagation of max-pooling is written in <code class="literal">upsample</code> and that of convolution is in <code class="literal">deconvolve</code>:</p><div class="informalexample"><pre class="programlisting">public double[][][][] backward(double[][][][] X, double[][][][] preActivated_X, double[][][][] activated_X, double[][][][] downsampled_X, double[][][][] dY, int minibatchSize, double learningRate) {

   double[][][][] dZ = this.upsample(activated_X, downsampled_X, dY, minibatchSize);
   return this.deconvolve(X, preActivated_X, dZ, minibatchSize, learningRate);

}</pre></div><p>What <a id="id329" class="indexterm"/>
<code class="literal">upsample</code> does is just transfer the delta to the convolutional layer:</p><div class="informalexample"><pre class="programlisting">public double[][][][] upsample(double[][][][] X, double[][][][] Y, double[][][][] dY, int minibatchSize) {

   double[][][][] dX = new double[minibatchSize][nKernel][convolvedSize[0]][convolvedSize[1]];

   for (int n = 0; n &lt; minibatchSize; n++) {

       for (int k = 0; k &lt; nKernel; k++) {
           for (int i = 0; i &lt; pooledSize[0]; i++) {
               for (int j = 0; j &lt; pooledSize[1]; j++) {

                   for (int s = 0; s &lt; poolSize[0]; s++) {
                       for (int t = 0; t &lt; poolSize[1]; t++) {

                           double d_ = 0.;

                           if (Y[n][k][i][j] == X[n][k][poolSize[0]*i+s][poolSize[1]*j+t]) {
                               d_ = dY[n][k][i][j];
                           }

                           dX[n][k][poolSize[0]*i+s][poolSize[1]*j+t] = d_;
                       }
                   }
               }
           }
       }
   }

   return dX;
}</pre></div><p>In <code class="literal">deconvolve</code>, we need to update the model parameter. Since we train the model with mini-batches, we calculate the summation of the gradients first:</p><div class="informalexample"><pre class="programlisting">// calc gradients of W, b
for (int n = 0; n &lt; minibatchSize; n++) {
   for (int k = 0; k &lt; nKernel; k++) {

       for (int i = 0; i &lt; convolvedSize[0]; i++) {
           for (int j = 0; j &lt; convolvedSize[1]; j++) {

               double d_ = dY[n][k][i][j] * this.dactivation.apply(Y[n][k][i][j]);

               grad_b[k] += d_;

               for (int c = 0; c &lt; channel; c++) {
                   for (int s = 0; s &lt; kernelSize[0]; s++) {
                       for (int t = 0; t &lt; kernelSize[1]; t++) {
                           grad_W[k][c][s][t] += d_ * X[n][c][i+s][j+t];
                       }
                   }
               }
           }
       }
   }
}</pre></div><p>Then, update <a id="id330" class="indexterm"/>the weight and the bias <a id="id331" class="indexterm"/>using these gradients:</p><div class="informalexample"><pre class="programlisting">// update gradients
for (int k = 0; k &lt; nKernel; k++) {
   b[k] -= learningRate * grad_b[k] / minibatchSize;

   for (int c = 0; c &lt; channel; c++) {
       for (int s = 0; s &lt; kernelSize[0]; s++) {
           for(int t = 0; t &lt; kernelSize[1]; t++) {
               W[k][c][s][t] -= learningRate * grad_W[k][c][s][t] / minibatchSize;
           }
       }
   }
}</pre></div><p>Unlike other algorithms, we have to calculate the parameters and delta discretely in CNN:</p><div class="informalexample"><pre class="programlisting">// calc delta
for (int n = 0; n &lt; minibatchSize; n++) {
   for (int c = 0; c &lt; channel; c++) {
       for (int i = 0; i &lt; imageSize[0]; i++) {
           for (int j = 0; j &lt; imageSize[1]; j++) {

               for (int k = 0; k &lt; nKernel; k++) {
                   for (int s = 0; s &lt; kernelSize[0]; s++) {
                       for (int t = 0; t &lt; kernelSize[1]; t++) {

                           double d_ = 0.;

                           if (i - (kernelSize[0] - 1) - s &gt;= 0 &amp;&amp; j - (kernelSize[1] - 1) - t &gt;= 0) {
                               d_ = dY[n][k][i-(kernelSize[0]-1)-s][j-(kernelSize[1]-1)-t] * this.dactivation.apply(Y[n][k][i- (kernelSize[0]-1)-s][j-(kernelSize[1]-1)-t]) * W[k][c][s][t];
                           }

                           dX[n][c][i][j] += d_;
                       }
                   }
               }
           }
       }
   }
}</pre></div><p>Now we train <a id="id332" class="indexterm"/>the model, so let's go on to <a id="id333" class="indexterm"/>the test part. The method for testing or prediction simply does the forward propagation, just like the other algorithms:</p><div class="informalexample"><pre class="programlisting">public Integer[] predict(double[][][] x) {

   List&lt;double[][][]&gt; preActivated = new ArrayList&lt;&gt;(nKernels.length);
   List&lt;double[][][]&gt; activated = new ArrayList&lt;&gt;(nKernels.length);

   for (int i = 0; i &lt; nKernels.length; i++) {
       preActivated.add(new double[nKernels[i]][convolvedSizes[i][0]][convolvedSizes[i][1]]);
       activated.add(new double[nKernels[i]][convolvedSizes[i][0]][convolvedSizes[i][1]]);
   }

   // forward convolution + pooling layers
   double[][][] z = x.clone();
   for (int i = 0; i &lt; nKernels.length; i++) {
       z = convpoolLayers[i].forward(z, preActivated.get(i), activated.get(i));
   }


   // forward MLP
   return logisticLayer.predict(hiddenLayer.forward(this.flatten(z)));
}</pre></div><p>Congratulations! That's all for CNN. Now you can run the code and see how it works. Here, we have CNN with two-dimensional data as input, but CNN can also have three-dimensional data if we expand the model. We can expect its application in medical fields, for example, finding malignant tumors from 3D-scanned data of human brains.</p><p>The process of <a id="id334" class="indexterm"/>convolution and pooling was originally invented by LeCun et al. in 1998 (<a class="ulink" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</a>), yet as you can see from the codes, it requires much calculation. We can assume that this method might not have been <a id="id335" class="indexterm"/>suitable for practical applications with computers at the time, not to mention making it deep. The reason CNN has gained more attention recently is probably because the power and capacity of computers has greatly developed. But still, we can't deny the problem. Therefore, it seems practical to use GPU, not CPU, when we have CNN with certain amounts of data. Since the implementation to optimize the algorithm to GPU is complicated, we won't write the codes here. Instead, in <a class="link" href="ch05.html" title="Chapter 5. Exploring Java Deep Learning Libraries – DL4J, ND4J, and More">Chapter 5</a>, <span class="emphasis"><em>Exploring Java Deep Learning Libraries – DL4J, ND4J, and More</em></span> and <a class="link" href="ch07.html" title="Chapter 7. Other Important Deep Learning Libraries">Chapter 7</a>, <span class="emphasis"><em>Other Important Deep Learning Libraries</em></span>, you'll see the library of deep learning that is capable of utilizing GPU.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec25"/>Summary</h1></div></div></div><p>In this chapter, you learned about two deep learning algorithms that don't require pre-training: deep neural networks with dropout and CNN. The key to high precision rates is how we make the network sparse, and dropout is one technique to achieve this. Another technique is the rectifier, the activation function that can solve the problem of saturation that occurred in the sigmoid function and the hyperbolic tangent. CNN is the most popular algorithm for image recognition and has two features: convolution and max-pooling. Both of these attribute the model to acquire translation invariance. If you are interested in how dropout, rectifier, and other activation functions contribute to the performance of neural networks, the following could be good references: <span class="emphasis"><em>Deep Sparse Rectifier Neural Networks</em></span> (Glorot, et. al. 2011, <a class="ulink" href="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf">http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf</a>), <span class="emphasis"><em>ImageNet Classification with Deep Convolutional Neural Networks</em></span> (Krizhevsky et. al. 2012, <a class="ulink" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>), and <span class="emphasis"><em>Maxout Networks</em></span> (Goodfellow et al. 2013, <a class="ulink" href="http://arxiv.org/pdf/1302.4389.pdf">http://arxiv.org/pdf/1302.4389.pdf</a>).</p><p>While you now know the popular and useful deep learning algorithms, there are still many of them that have not been mentioned in this book. This field of study is getting more and more active, and more and more new algorithms are appearing. But don't worry, as all the algorithms are based on the same root: neural networks. Now you know the way of thinking required to grasp or implement the model, you can fully understand whatever models you encounter.</p><p>We've implemented deep learning algorithms from scratch so you fully understand them. In the next chapter, you'll see how we can implement them with deep learning libraries to facilitate our research or applications.</p></div></body></html>