<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0;Deep Neural Networks"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Deep Neural Networks</h1></div></div></div><p>In this chapter, we'll be examining deep neural networks. These networks have shown excellent performance in terms of the accuracy of their classification on more challenging and advanced datasets like ImageNet, CIFAR10, and CIFAR100. For conciseness, we'll only be focusing on two networks, <span class="strong"><strong>ResNet</strong></span> [2][4] and <span class="strong"><strong>DenseNet</strong></span> [5]. While we will go into much more detail, it's important to take a minute to introduce these networks:</p><p>ResNet introduced the <a id="id83" class="indexterm"/>concept of residual learning which enabled it to build very deep networks by addressing the vanishing gradient problem in deep convolutional networks.</p><p>DenseNet improved the<a id="id84" class="indexterm"/> ResNet technique further by allowing every convolution to have direct access to inputs, and lower layer feature maps. It's also managed to keep the number of parameters low in deep networks by utilizing both the <span class="strong"><strong>Bottleneck</strong></span> and <span class="strong"><strong>Transition</strong></span> layers.</p><p>But why these two models, and not others? Well, since their introduction, there have been countless models such as <span class="strong"><strong>ResNeXt</strong></span> [6] and <span class="strong"><strong>FractalNet</strong></span> [7] which have been inspired by the technique used by these two <a id="id85" class="indexterm"/>networks. Likewise, with an understanding<a id="id86" class="indexterm"/> of both ResNet and DenseNet, we'll be able to use their design guidelines to build our own models. By using transfer learning, this will also allow us to take advantage of pretrained ResNet and DenseNet models for our own purposes. These reasons alone, along with their compatibility with Keras, make the two models ideal for exploring and complimenting the advanced deep learning scope of this book.</p><p>While this chapter's focus is on deep neural networks; we'll begin this chapter by discussing an<a id="id87" class="indexterm"/> important feature of Keras called the <span class="strong"><strong>Functional API</strong></span>. This API acts as an alternative method for building networks in Keras and enables us to build more complex networks that cannot be accomplished by the sequential model. The reason why we're focusing so much on this API is that it will become a very useful tool for building deep networks such as the two we're focusing on in this chapter. It's recommended that you've completed, <a class="link" href="ch01.html" title="Chapter 1. Introducing Advanced Deep Learning with Keras">Chapter 1</a>, <span class="emphasis"><em>Introducing Advanced Deep Learning with Keras</em></span>, before moving onto this chapter as we'll refer to introductory level code and concepts explored in that chapter as we take them to an advanced level in this chapter.</p><p>The goals of this chapter is to introduce:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The Functional API in Keras, as well as exploring examples of networks running it</li><li class="listitem" style="list-style-type: disc">Deep Residual Networks (ResNet versions 1 and 2) implementation in Keras</li><li class="listitem" style="list-style-type: disc">The implementation of Densely Connected Convolutional Networks (DenseNet) into Keras</li><li class="listitem" style="list-style-type: disc">Explore two popular deep learning models, <span class="strong"><strong>ResNet,</strong></span> and <span class="strong"><strong>DenseNet</strong></span></li></ul></div><div class="section" title="Functional API"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec11"/>Functional API</h1></div></div></div><p>In the sequential model that we first introduced in <a class="link" href="ch01.html" title="Chapter 1. Introducing Advanced Deep Learning with Keras">Chapter 1</a>, <span class="emphasis"><em>Introducing Advanced Deep Learning with Keras</em></span>, a layer is stacked on top of another layer. Generally, the model will be accessed through its input and output layers. We also learned that there is no simple mechanism if we find ourselves wanting to add an auxiliary input at the middle of the network, or even to extract an auxiliary output before the last layer.</p><p>That model also had its downside, for example, it doesn't support graph-like models or models that behave like Python functions. In addition, it's also difficult to share layers between the two models. Such limitations are addressed by the functional API and are the reason why it's a vital tool for anyone wanting to work with deep learning models.</p><p>The Functional API is guided by the following two concepts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A layer is an instance that<a id="id88" class="indexterm"/> accepts a tensor as an argument. The output of a layer is another tensor. To build a model, the layer instances are objects that are chained to one another through both input and output tensors. This will have similar end-result as would stacking multiple layers in the sequential model have. However, using layer instances makes it easier for models to have either auxiliary or multiple inputs and outputs since the input/output of each layer will be readily accessible.</li><li class="listitem" style="list-style-type: disc">A model is a function between one or <a id="id89" class="indexterm"/>more input tensors and output tensors. In between the model input and output, tensors are the layer instances that are chained to one another by layer input and output tensors. A model is, therefore, a function of one or more input layers and one or more output layers. The model instance formalizes the computational graph on how the data flows from input(s) to output(s).</li></ul></div><p>After you've completed building the functional API model, the training and evaluation are then performed by the same functions used in the sequential model. To illustrate, in a functional API, a 2D convolutional layer, <code class="literal">Conv2D</code>, with 32 filters and with <code class="literal">x</code> as the layer input tensor and <code class="literal">y</code> as the layer output tensor can be written as:</p><div class="informalexample"><pre class="programlisting">y = Conv2D(32)(x)</pre></div><p>We're also able to stack multiple layers to build our models. For example, we can rewrite the CNN on MNIST code, the same code we created in the last chapter, as shown in following listing:</p><p>You'll find Listing 2.1.1, <code class="literal">cnn-functional-2.1.1.py</code>, as follows. This shows us how we can convert the <code class="literal">cnn-mnist-1.4.1.py</code> code using the functional API:</p><div class="informalexample"><pre class="programlisting">import numpy as np
from keras.layers import Dense, Dropout, Input
from keras.layers import Conv2D, MaxPooling2D, Flatten
from keras.models import Model
from keras.datasets import mnist
from keras.utils import to_categorical


# compute the number of labels
num_labels = len(np.unique(y_train))

# convert to one-hot vector
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# reshape and normalize input images
image_size = x_train.shape[1]
x_train = np.reshape(x_train,[-1, image_size, image_size, 1])
x_test = np.reshape(x_test,[-1, image_size, image_size, 1])
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# network parameters
# image is processed as is (square grayscale)
input_shape = (image_size, image_size, 1)
batch_size = 128
kernel_size = 3
filters = 64 
dropout = 0.3 

# use functional API to build cnn layers
inputs = Input(shape=input_shape)
y = Conv2D(filters=filters,
           kernel_size=kernel_size,
           activation='relu')(inputs)
y = MaxPooling2D()(y)
y = Conv2D(filters=filters,
           kernel_size=kernel_size,
           activation='relu')(y)
y = MaxPooling2D()(y)
y = Conv2D(filters=filters,
           kernel_size=kernel_size,
           activation='relu')(y)
# image to vector before connecting to dense layer
y = Flatten()(y)
# dropout regularization
y = Dropout(dropout)(y)
outputs = Dense(num_labels, activation='softmax')(y)

# build the model by supplying inputs/outputs
model = Model(inputs=inputs, outputs=outputs)
# network model in text
model.summary()

# classifier loss, Adam optimizer, classifier accuracy
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# train the model with input images and labels
model.fit(x_train,
          y_train,
          validation_data=(x_test, y_test),
          epochs=20,
          batch_size=batch_size)

# model accuracy on test dataset
score = model.evaluate(x_test, y_test, batch_size=batch_size)
print("\nTest accuracy: %.1f%%" % (100.0 * score[1]))</pre></div><p>By default, <code class="literal">MaxPooling2D</code> uses <code class="literal">pool_size=2</code>, so the argument has been removed.</p><p>In the preceding listing every layer is a function of<a id="id90" class="indexterm"/> a tensor. They each generate a tensor as an output which becomes the input to the next layer. To create this model, we can call <code class="literal">Model()</code> and supply both the <code class="literal">inputs</code> and <code class="literal">outputs</code> tensors, or alternatively the lists of tensors. Everything else remains the same.</p><p>The same listing can also be trained and evaluated using the <code class="literal">fit()</code> and <code class="literal">evaluate()</code> functions, similar to the sequential model. The <code class="literal">sequential</code> class is, in fact, a subclass of the <code class="literal">Model</code> class. We need to remember that we inserted the <code class="literal">validation_data</code> argument in<a id="id91" class="indexterm"/> the <code class="literal">fit()</code> function to see the progress of validation accuracy during training. The accuracy ranges from 99.3% to 99.4% in 20 epochs.</p><div class="section" title="Creating a two-input and one-output model"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec17"/>Creating a two-input and one-output model</h2></div></div></div><p>We're now<a id="id92" class="indexterm"/> going to do something really exciting, creating an advanced model with two inputs and one output. Before we start, it's important to know that this is something that is not straightforward in the sequential model.</p><p>Let's suppose a new model for the MNIST digit classification is invented, and it's called the <span class="strong"><strong>Y-Network,</strong></span> as shown in <span class="emphasis"><em>Figure 2.1.1</em></span>. The Y-Network uses the same input twice, both on<a id="id93" class="indexterm"/> the left and right CNN branches. The network combines the results using <code class="literal">concatenate</code> layer. The merge operation <code class="literal">concatenate</code> is similar to stacking two tensors of the same shape along the concatenation axis to form one tensor. For example, concatenating two tensors of shape (3, 3, 16) along the last axis will result in a tensor of shape (3, 3, 32).</p><p>Everything else after the <code class="literal">concatenate</code> layer will remain the same as the previous CNN model. That is <code class="literal">Flatten-Dropout-Dense</code>:</p><div class="mediaobject"><img src="graphics/B08956_02_01.jpg" alt="Creating a two-input and one-output model"/><div class="caption"><p>Figure 2.1.1: The Y-Network accepts the same input twice but processes the input in two branches of convolutional networks. The outputs of the branches are combined using the concatenate layer. The last layer prediction is going to be similar to the previous CNN example.</p></div></div><p>To improve<a id="id94" class="indexterm"/> the performance of the model in <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>2.1.1</em></span>, we can propose several changes. Firstly, the branches of the Y-Network are doubling the number of filters to compensate for the halving of the feature maps size after <code class="literal">MaxPooling2D()</code>. For example, if the output of the first convolution is (28, 28, 32), after max pooling the new shape is (14, 14, 32). The next convolution will have a filter size of 64 and output dimensions of (14, 14, 64).</p><p>Second, although both branches <a id="id95" class="indexterm"/>have the same kernel size of 3, the right branch use a dilation rate of 2. <span class="emphasis"><em>Figure 2.1.2</em></span> shows the effect of different dilation rates on a kernel with size 3. The idea is that by increasing the coverage of the kernel using<a id="id96" class="indexterm"/> dilation rate, the CNN will enable the right branch to learn different feature maps. We'll use the option <code class="literal">padding='same'</code> to ensure that we will not have negative tensor dimensions when the dilated CNN is used. By using <code class="literal">padding='same'</code>, we'll keep the dimensions of the input the same as the output feature maps. This is<a id="id97" class="indexterm"/> accomplished by padding the input with zeros to make sure that the output has the <span class="emphasis"><em>same</em></span> size:</p><div class="mediaobject"><img src="graphics/B08956_02_02.jpg" alt="Creating a two-input and one-output model"/><div class="caption"><p>Figure 2.1.2: By increasing the dilate rate from 1, the effective kernel coverage also increases</p></div></div><p>Following listing shows the implementation of Y-Network. The two branches are created by the two for loops. Both branches expect the same input shape. The two <code class="literal">for</code> loops will create two 3-layer stacks of <code class="literal">Conv2D-Dropout-MaxPooling2D</code>. While we used the <code class="literal">concatenate</code> layer to combine the outputs of the left and right branches, we could also utilize the other merge functions of Keras, such as <code class="literal">add</code>, <code class="literal">dot</code>, <code class="literal">multiply</code>. The choice of the merge function is not purely arbitrary but must be based on a sound model design decision.</p><p>In the Y-Network, <code class="literal">concatenate</code> will not discard any portion of the feature maps. Instead, we'll let the <code class="literal">Dense</code> layer figure out what to do with the concatenated feature maps. Listing 2.1.2, <code class="literal">cnn-y-network-2.1.2.py</code> shows the Y-Network implementation using the Functional API:</p><div class="informalexample"><pre class="programlisting">import numpy as np

from keras.layers import Dense, Dropout, Input
from keras.layers import Conv2D, MaxPooling2D, Flatten
from keras.models import Model
from keras.layers.merge import concatenate
from keras.datasets import mnist
from keras.utils import to_categorical
from keras.utils import plot_model

# load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

   # compute the number of labels
   num_labels = len(np.unique(y_train))

   # convert to one-hot vector
   y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# reshape and normalize input images
image_size = x_train.shape[1]
x_train = np.reshape(x_train,[-1, image_size, image_size, 1])
x_test = np.reshape(x_test,[-1, image_size, image_size, 1])
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# network parameters
input_shape = (image_size, image_size, 1)
batch_size = 32
kernel_size = 3
dropout = 0.4
n_filters = 32

# left branch of Y network
left_inputs = Input(shape=input_shape)
x = left_inputs
filters = n_filters
# 3 layers of Conv2D-Dropout-MaxPooling2D
# number of filters doubles after each layer (32-64-128)
for i in range(3):
    x = Conv2D(filters=filters,
               kernel_size=kernel_size,
               padding='same',
               activation='relu')(x)
    x = Dropout(dropout)(x)
    x = MaxPooling2D()(x)
    filters *= 2

# right branch of Y network
right_inputs = Input(shape=input_shape)
y = right_inputs
filters = n_filters
# 3 layers of Conv2D-Dropout-MaxPooling2D
# number of filters doubles after each layer (32-64-128)
for i in range(3):
    y = Conv2D(filters=filters,
               kernel_size=kernel_size,
               padding='same',
               activation='relu',
               dilation_rate=2)(y)
    y = Dropout(dropout)(y)
    y = MaxPooling2D()(y)
    filters *= 2

# merge left and right branches outputs
y = concatenate([x, y])
# feature maps to vector before connecting to Dense layer
y = Flatten()(y)
y = Dropout(dropout)(y)
outputs = Dense(num_labels, activation='softmax')(y)

# build the model in functional API
model = Model([left_inputs, right_inputs], outputs)
# verify the model using graph
plot_model(model, to_file='cnn-y-network.png', show_shapes=True)
# verify the model using layer text description
model.summary()

# classifier loss, Adam optimizer, classifier accuracy
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# train the model with input images and labels
model.fit([x_train, x_train],
          y_train,
          validation_data=([x_test, x_test], y_test),
          epochs=20,
          batch_size=batch_size)

# model accuracy on test dataset
score = model.evaluate([x_test, x_test], y_test, batch_size=batch_size)
print("\nTest accuracy: %.1f%%" % (100.0 * score[1]))</pre></div><p>Taking a<a id="id98" class="indexterm"/> step back, we can note that the Y-Network is expecting two inputs for training and validation. The inputs are identical, so <code class="literal">[x_train, x_train]</code> is supplied.</p><p>Over the course of the 20 epochs, the accuracy of the Y-Network ranges from 99.4% to 99.5%. This is a slight improvement over the 3-stack CNN which achieved a range between 99.3% and 99.4% accuracy range. However, this was at the cost of both higher complexity and more than double the number of parameters. The following figure, <span class="emphasis"><em>Figure 2.1.3,</em></span> shows the architecture of the Y-Network as understood by Keras and generated by the <code class="literal">plot_model()</code> function:</p><div class="mediaobject"><img src="graphics/B08956_02_03.jpg" alt="Creating a two-input and one-output model"/><div class="caption"><p>Figure 2.1.3: The CNN Y-Network as implemented in Listing 2.1.2</p></div></div><p>This concludes<a id="id99" class="indexterm"/> our look at the Functional API. We should take this time to remember that the focus of this chapter is building deep neural networks, specifically ResNet and DenseNet. Therefore, we're only covering the Functional API materials needed to build them, as to cover the entire API would be beyond the scope of this book.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note02"/>Note</h3><p>The reader is referred to visit <a class="ulink" href="https://keras.io/">https://keras.io/</a> for <a id="id100" class="indexterm"/>additional information on functional API.</p></div></div></div></div></div>
<div class="section" title="Deep residual networks (ResNet)"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec12"/>Deep residual networks (ResNet)</h1></div></div></div><p>One key advantage of deep networks is that they have a great ability to learn different levels of<a id="id101" class="indexterm"/> representations from both inputs and feature maps. In both classification, segmentation, detection and a number of other computer vision problems, learning different levels of features generally leads to better performance.</p><p>However, you'll find that it's not easy to train deep networks as a result of the gradient vanishes (or explodes) with depth in the shallow layers during backpropagation. <span class="emphasis"><em>Figure 2.2.1</em></span> illustrates the problem of vanishing gradient. The network parameters are updated by backpropagation from the output layer to all previous layers. Since backpropagation is based on the chain rule, there is a tendency for gradients to diminish as they reach the shallow layers. This is due to the multiplication of small numbers, especially for the small absolute value of errors and parameters.</p><p>The number of multiplication operations will be proportional to the depth of the network. It's also worth noting that if the gradient degrades, the parameters will not be updated appropriately.</p><p>Hence, the network will fail to improve its performance:</p><div class="mediaobject"><img src="graphics/B08956_02_04.jpg" alt="Deep residual networks (ResNet)"/><div class="caption"><p>Figure 2.2.1: A common problem in deep networks is that the gradient vanishes as it reaches the shallow layers during backpropagation.</p></div></div><div class="mediaobject"><img src="graphics/B08956_02_05.jpg" alt="Deep residual networks (ResNet)"/><div class="caption"><p>Figure 2.2.2: A comparison between a block in a typical CNN and a block in ResNet. To prevent degradation in gradients during backpropagation, a shortcut connection is introduced.</p></div></div><p>To alleviate the degradation of the gradient in deep networks, ResNet introduced the concept of a deep residual learning framework. Let's analyze a block, a small segment of our deep network.</p><p>The preceding figure shows a comparison between a typical CNN block and a ResNet residual block. The idea of ResNet is that in order to prevent the gradient from degrading, we'll let the information flow through the shortcut connections to reach the shallow layers.</p><p>Next, we're<a id="id102" class="indexterm"/> going to look at more details within the discussion of the differences between the two blocks. <span class="emphasis"><em>Figure 2.2.3</em></span> shows more details of the CNN block of another<a id="id103" class="indexterm"/> commonly used deep network, VGG[3], and ResNet. We'll represent the<a id="id104" class="indexterm"/> layer feature maps as <span class="strong"><strong>x</strong></span>. The feature maps at layer <span class="emphasis"><em>l</em></span> are </p><div class="mediaobject"><img src="graphics/B08956_02_001.jpg" alt="Deep residual networks (ResNet)"/></div><p>. The operations in the CNN layer are <span class="strong"><strong>Conv2D-Batch Normalization</strong></span> (<span class="strong"><strong>BN</strong></span>)-<span class="strong"><strong>ReLU</strong></span>.</p><p>Let's suppose we represent this set of operations in the form of <span class="emphasis"><em>H</em></span>() = Conv2D-Batch Normalization(BN)-ReLU, that will then mean that:</p><div class="mediaobject"><img src="graphics/B08956_02_002.jpg" alt="Deep residual networks (ResNet)"/></div><p>          (Equation 2.2.1)</p><div class="mediaobject"><img src="graphics/B08956_02_003.jpg" alt="Deep residual networks (ResNet)"/></div><p>             (Equation 2.2.2)</p><p>In other words, the feature maps at layer <span class="emphasis"><em>l</em></span> - 2 are transformed to </p><div class="mediaobject"><img src="graphics/B08956_02_004.jpg" alt="Deep residual networks (ResNet)"/></div><p> by <span class="emphasis"><em>H</em></span>() = Conv2D-Batch Normalization(BN)-ReLU. The same set of operations is applied to transform </p><div class="mediaobject"><img src="graphics/B08956_02_005.jpg" alt="Deep residual networks (ResNet)"/></div><p> to </p><div class="mediaobject"><img src="graphics/B08956_02_006.jpg" alt="Deep residual networks (ResNet)"/></div><p>. To put<a id="id105" class="indexterm"/> this another way, if we have an 18-layer VGG, then there are 18 <span class="emphasis"><em>H</em></span>() operations before the input image is transformed to the 18<sup>th</sup> layer feature maps.</p><p>Generally speaking, we can observe that the layer <span class="emphasis"><em>l</em></span> output feature maps are directly affected by the previous feature maps only. Meanwhile, for ResNet:</p><div class="mediaobject"><img src="graphics/B08956_02_007.jpg" alt="Deep residual networks (ResNet)"/></div><p>                               (Equation 2.2.3)</p><div class="mediaobject"><img src="graphics/B08956_02_008.jpg" alt="Deep residual networks (ResNet)"/></div><p>          (Equation 2.2.4)</p><div class="mediaobject"><img src="graphics/B08956_02_06.jpg" alt="Deep residual networks (ResNet)"/><div class="caption"><p>Figure 2.2.3: A detailed layer operations for a plain CNN block and a Residual block</p></div></div><div class="mediaobject"><img src="graphics/B08956_02_009.jpg" alt="Deep residual networks (ResNet)"/></div><p> is made of <code class="literal">Conv2D-BN,</code> which is also known as the residual mapping. The <span class="strong"><strong>+</strong></span> sign is tensor element-wise addition between the shortcut connection and the output of </p><div class="mediaobject"><img src="graphics/B08956_02_010.jpg" alt="Deep residual networks (ResNet)"/></div><p>. The shortcut connection doesn't add extra parameters nor extra computational complexity.</p><p>The add operation<a id="id106" class="indexterm"/> can be implemented in Keras by the <code class="literal">add()</code> merge function. However, both the </p><div class="mediaobject"><img src="graphics/B08956_02_011.jpg" alt="Deep residual networks (ResNet)"/></div><p> equation and <span class="strong"><strong>x</strong></span> should have the same dimensions. If the dimensions are different, for example, when changing the feature maps size, we should perform a linear projection on <span class="strong"><strong>x</strong></span> as to match the size of </p><div class="mediaobject"><img src="graphics/B08956_02_012.jpg" alt="Deep residual networks (ResNet)"/></div><p>. In the original paper, the linear projection for the case, when the feature maps size is halved, is done by a <code class="literal">Conv2D</code> with a 1 × 1 kernel and <code class="literal">strides=2</code>.</p><p>Back in <a class="link" href="ch01.html" title="Chapter 1. Introducing Advanced Deep Learning with Keras">Chapter 1</a>, Introducing <span class="emphasis"><em>Advanced Deep Learning with Keras</em></span>, we discussed that <code class="literal">stride &gt; 1</code> is equivalent to skipping pixels during convolution. For example, if <code class="literal">strides=2</code>, we could skip every other pixel when we slide the kernel during the convolution process.</p><p>The preceding <span class="emphasis"><em>Equations</em></span> <span class="emphasis"><em>2.2.3</em></span> and <span class="emphasis"><em>2.2.4</em></span>, both model ResNet residual block operations. They imply that if the deeper layers can be trained to have fewer errors, then there is no reason why the shallower layers should have higher errors.</p><p>Knowing the<a id="id107" class="indexterm"/> basic building blocks of ResNet, we're able to design a deep residual network for image classification. This time, however, we're going to tackle a more challenging and advanced dataset.</p><p>In our examples, we're going to consider CIFAR10, which was one of the datasets the original paper was validated. In this example, Keras provides an API to conveniently access the CIFAR10 dataset, as shown:</p><div class="informalexample"><pre class="programlisting">from keras.datasets import cifar10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()</pre></div><p>Like MNIST, the CIFAR10 dataset has 10 categories. The dataset is a collection of small (32 × 32) RGB real-world images of an airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and a truck corresponding to each of the 10 categories. <span class="emphasis"><em>Figure 2.2.4</em></span> shows sample images from CIFAR10.</p><p>In the dataset, there are 50,000 labeled train images and 10,000 labeled test images for validation:</p><div class="mediaobject"><img src="graphics/B08956_02_07.jpg" alt="Deep residual networks (ResNet)"/><div class="caption"><p>Figure 2.2.4: Sample images from the CIFAR10 dataset. The full dataset has 50,000 labeled train images and 10,000 labeled test images for validation.</p></div></div><p>For the CIFAR10 data, ResNet can be built using different network architectures as shown in <span class="emphasis"><em>Table 2.2.1</em></span>. The values of both <span class="emphasis"><em>n</em></span> and the corresponding architectures of ResNet were validated in <span class="emphasis"><em>Table 2.2.2</em></span>. <span class="emphasis"><em>Table 2.2.1</em></span> means we have three sets of residual blocks. Each set has <span class="emphasis"><em>2n</em></span> layers corresponding to <span class="emphasis"><em>n</em></span> residual blocks. The extra layer in 32 × 32 is the first layer for the input image.</p><p>The <a id="id108" class="indexterm"/>kernel size is 3, except for the transition between two feature maps with different sizes that implements a linear mapping. For example, a <code class="literal">Conv2D</code> with a kernel size of 1 and <code class="literal">strides=2</code>. For the sake of consistency with DenseNet, we'll use the term Transition layer when we join two residual blocks of different sizes.</p><p>ResNet uses <code class="literal">kernel_initializer='he_normal'</code> in order to aid the convergence when backpropagation<a id="id109" class="indexterm"/> is taking place [1]. The last layer is made of <code class="literal">AveragePooling2D-Flatten-Dense</code>. It's worth noting at this point that ResNet does not use dropout. It also appears that the add merge operation and the 1 × 1 convolution have a self-regularizing effect. <span class="emphasis"><em>Figure 2.2.4</em></span> shows the ResNet model architecture for the CIFAR10 dataset as described in <span class="emphasis"><em>Table 2.2.1</em></span>.</p><p>The following listing shows the partial ResNet implementation within Keras. The code has been contributed to the Keras GitHub repository. From <span class="emphasis"><em>Table 2.2.2</em></span> we can also see that by modifying the value of <code class="literal">n</code>, we're able to increase the depth of the networks. For example, for <code class="literal">n = 18</code>, we already have ResNet110, a deep network with 110 layers. To build ResNet20, we use <code class="literal">n = 3</code>:</p><div class="informalexample"><pre class="programlisting">n = 3

# model version
# orig paper: version = 1 (ResNet v1), 
# Improved ResNet: version = 2 (ResNet v2)
version = 1

# computed depth from supplied model parameter n
if version == 1:
    depth = n * 6 + 2
elif version == 2:
    depth = n * 9 + 2
…
if version == 2:
    model = resnet_v2(input_shape=input_shape, depth=depth)
else:
    model = resnet_v1(input_shape=input_shape, depth=depth)</pre></div><p>The <code class="literal">resnet_v1()</code> method is a model builder for ResNet. It uses a utility function, <code class="literal">resnet_layer()</code> to help build the stack of <code class="literal">Conv2D-BN-ReLU</code>.</p><p>It's referred to as version 1, as we will see in the next section, an improved ResNet was proposed, and that has been called ResNet version 2, or v2. Over ResNet, ResNet v2 has an improved residual block design resulting in better performance.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Layers</p>
</th><th style="text-align: left" valign="bottom">
<p>Output Size</p>
</th><th style="text-align: left" valign="bottom">
<p>Filter Size</p>
</th><th style="text-align: left" valign="bottom">
<p>Operations</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Convolution</p>
</td><td style="text-align: left" valign="top">
<p>32 × 32</p>
</td><td style="text-align: left" valign="top">
<p>16</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_013.jpg" alt="Deep residual networks (ResNet)"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Residual Block</p>
<p>(1)</p>
</td><td style="text-align: left" valign="top">
<p>32 × 32</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_014.jpg" alt="Deep residual networks (ResNet)"/></div>
</td></tr><tr><td rowspan="2" style="text-align: left" valign="top">
<p>Transition Layer</p>
<p>(1)</p>
</td><td style="text-align: left" valign="top">
<p>32 × 32</p>
</td><td style="text-align: left" valign="top"> </td><td rowspan="2" style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_015.jpg" alt="Deep residual networks (ResNet)"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>16 × 16</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>Residual Block</p>
<p>(2)</p>
</td><td style="text-align: left" valign="top">
<p>16 × 16</p>
</td><td style="text-align: left" valign="top">
<p>32</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_016.jpg" alt="Deep residual networks (ResNet)"/></div>
</td></tr><tr><td rowspan="2" style="text-align: left" valign="top">
<p>Transition Layer</p>
<p>(2)</p>
</td><td style="text-align: left" valign="top">
<p>16 × 16</p>
</td><td style="text-align: left" valign="top"> </td><td rowspan="2" style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_017.jpg" alt="Deep residual networks (ResNet)"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8 × 8</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>Residual Block</p>
<p>(3)</p>
</td><td style="text-align: left" valign="top">
<p>8 × 8</p>
</td><td style="text-align: left" valign="top">
<p>64</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_018.jpg" alt="Deep residual networks (ResNet)"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Average Pooling</p>
</td><td style="text-align: left" valign="top">
<p>1 × 1</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_019.jpg" alt="Deep residual networks (ResNet)"/></div>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p>Table 2.2.1: ResNet network architecture configuration</p></blockquote></div><div class="mediaobject"><img src="graphics/B08956_02_08.jpg" alt="Deep residual networks (ResNet)"/><div class="caption"><p>Figure 2.2.4: The model architecture of ResNet for the CIFAR10 dataset classification</p></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p># Layers</p>
</th><th style="text-align: left" valign="bottom">
<p>n</p>
</th><th style="text-align: left" valign="bottom">
<p>% Accuracy on CIFAR10 (Original paper)</p>
</th><th style="text-align: left" valign="bottom">
<p>% Accuracy on CIFAR10 (This book)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>ResNet20</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>91.25</p>
</td><td style="text-align: left" valign="top">
<p>92.16</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ResNet32</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>92.49</p>
</td><td style="text-align: left" valign="top">
<p>92.46</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ResNet44</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>92.83</p>
</td><td style="text-align: left" valign="top">
<p>92.50</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ResNet56</p>
</td><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>93.03</p>
</td><td style="text-align: left" valign="top">
<p>92.71</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ResNet110</p>
</td><td style="text-align: left" valign="top">
<p>18</p>
</td><td style="text-align: left" valign="top">
<p>93.57</p>
</td><td style="text-align: left" valign="top">
<p>92.65</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p>Table 2.2.2: ResNet architectures validated with CIFAR10</p></blockquote></div><p>The following<a id="id110" class="indexterm"/> listing shows the partial code of <code class="literal">resnet-cifar10-2.2.1.py</code>, which is the Keras model implementation of ResNet v1:</p><div class="informalexample"><pre class="programlisting">def resnet_v1(input_shape, depth, num_classes=10):
    if (depth - 2) % 6 != 0:
        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')
    # Start model definition.
    num_filters = 16
    num_res_blocks = int((depth - 2) / 6)

    inputs = Input(shape=input_shape)
    x = resnet_layer(inputs=inputs)
    # Instantiate the stack of residual units
    for stack in range(3):
        for res_block in range(num_res_blocks):
            strides = 1
            if stack &gt; 0 and res_block == 0:
                strides = 2  # downsample
            y = resnet_layer(inputs=x,
                             num_filters=num_filters,
                             strides=strides)
            y = resnet_layer(inputs=y,
                             num_filters=num_filters,
                             activation=None)
            if stack &gt; 0 and res_block == 0
                # linear projection residual shortcut connection 
                # to match changed dims
                x = resnet_layer(inputs=x,
                                 num_filters=num_filters,
                                 kernel_size=1,
                                 strides=strides,
                                 activation=None,
                                 batch_normalization=False)
            x = add([x, y])
            x = Activation('relu')(x)
        num_filters *= 2

    # Add classifier on top.
    # v1 does not use BN after last shortcut connection-ReLU
    x = AveragePooling2D(pool_size=8)(x)
    y = Flatten()(x)
    outputs = Dense(num_classes,
                    activation='softmax',
                    kernel_initializer='he_normal')(y)

    # Instantiate model.
    model = Model(inputs=inputs, outputs=outputs)
    return model</pre></div><p>There are some minor differences from the original implementation of ResNet. In particular, we don't use SGD, and instead, we'll use Adam. This is because ResNet is easier to converge with Adam. We'll also use a learning rate (<code class="literal">lr</code>) scheduler, <code class="literal">lr_schedule()</code>, in order to schedule the decrease in <code class="literal">lr</code> at 80, 120, 160, and 180 epochs from the default 1e-3. The <code class="literal">lr_schedule()</code> function will be called after every epoch during training as part of the <code class="literal">callbacks</code> variable.</p><p>The other callback saves the <a id="id111" class="indexterm"/>checkpoint every time there is progress made in the validation accuracy. When training deep networks, it is a good practice to save the model or weight checkpoint. This is because it takes a substantial amount of time to train deep networks. When you want to use your network, all you need to do is simply reload the checkpoint, and the trained model is restored. This can be accomplished by calling Keras <code class="literal">load_model()</code>. The <code class="literal">lr_reducer()</code> function is included. In case the metric has plateaued before the schedule reduction, this <a id="id112" class="indexterm"/>callback will reduce the learning rate by the factor if the validation loss has not improved after <code class="literal">patience=5</code> epochs.</p><p>The <code class="literal">callbacks</code> variable is supplied when the <code class="literal">model.fit()</code> method is called. Similar to the original paper, the Keras implementation uses data augmentation, <code class="literal">ImageDataGenerator()</code>, in order to provide additional training data as part of the regularization schemes. As the number of training data increases, generalization will improve.</p><p>For example, a simple data augmentation is flipping the photo of the dog, as shown in following figure (<code class="literal">horizontal_flip=True</code>). If it is an image of a dog, then the flipped image is still an image of a dog. You can also perform other transformation, such as scaling, rotation, whitening, and so on, and the label will still remain the same:</p><div class="mediaobject"><img src="graphics/B08956_02_09.jpg" alt="Deep residual networks (ResNet)"/><div class="caption"><p>Figure 2.2.5: A simple data augmentation is flipping the original image</p></div></div><p>It's often difficult to exactly duplicate the implementation of the original paper, especially in the optimizer used and data augmentation, as there are slight differences in the performance of the Keras ResNet implementation in this book and the model in the original paper.</p></div>
<div class="section" title="ResNet v2"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec13"/>ResNet v2</h1></div></div></div><p>After the release of the <a id="id113" class="indexterm"/>second paper on ResNet [4], the original model presented in the previous section has been known as ResNet v1. The improved ResNet is commonly called ResNet v2. The improvement is mainly found in the arrangement of layers in the residual block as shown in following figure.</p><p> The prominent changes in ResNet v2 are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The use of a stack of 1 × 1 - 3 × 3 - 1 × 1 <code class="literal">BN-ReLU-Conv2D</code></li><li class="listitem" style="list-style-type: disc">Batch normalization and ReLU activation come before 2D convolution</li></ul></div><div class="mediaobject"><img src="graphics/B08956_02_10.jpg" alt="ResNet v2"/><div class="caption"><p>Figure 2.3.1: A comparison of residual blocks between ResNet v1 and ResNet v2</p></div></div><p>ResNet v2 is also<a id="id114" class="indexterm"/> implemented in the same code as <code class="literal">resnet-cifar10-2.2.1.py</code>:</p><div class="informalexample"><pre class="programlisting">def resnet_v2(input_shape, depth, num_classes=10):
    if (depth - 2) % 9 != 0:
        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')
    # Start model definition.
    num_filters_in = 16
    num_res_blocks = int((depth - 2) / 9)

    inputs = Input(shape=input_shape)
    # v2 performs Conv2D with BN-ReLU on input 
    # before splitting into 2 paths
    x = resnet_layer(inputs=inputs,
                     num_filters=num_filters_in,
                     conv_first=True)

    # Instantiate the stack of residual units
    for stage in range(3):
        for res_block in range(num_res_blocks):
            activation = 'relu'
            batch_normalization = True
            strides = 1
            if stage == 0:
                num_filters_out = num_filters_in * 4
                if res_block == 0:  # first layer and first stage
                    activation = None
                    batch_normalization = False
            else:
                num_filters_out = num_filters_in * 2
                if res_block == 0:  # 1st layer but not 1st stage
                    strides = 2    # downsample

            # bottleneck residual unit
            y = resnet_layer(inputs=x,
                             num_filters=num_filters_in,
                             kernel_size=1,
                             strides=strides,
                             activation=activation,
                             batch_normalization=batch_normalization,
                             conv_first=False)
            y = resnet_layer(inputs=y,
                             num_filters=num_filters_in,
                             conv_first=False)
            y = resnet_layer(inputs=y,
                             num_filters=num_filters_out,
                             kernel_size=1,
                             conv_first=False)
            if res_block == 0:
                # linear projection residual shortcut connection 
                # to match changed dims
                x = resnet_layer(inputs=x,
                                 num_filters=num_filters_out,
                                 kernel_size=1,
                                 strides=strides,
                                 activation=None,
                                 batch_normalization=False)
            x = add([x, y])

        num_filters_in = num_filters_out

    # add classifier on top.
    # v2 has BN-ReLU before Pooling
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = AveragePooling2D(pool_size=8)(x)
    y = Flatten()(x)
    outputs = Dense(num_classes,
                    activation='softmax',
                    kernel_initializer='he_normal')(y)

    # instantiate model.
    model = Model(inputs=inputs, outputs=outputs)
    return model</pre></div><p>ResNet v2's model<a id="id115" class="indexterm"/> builder is shown in the following code. For example, to build ResNet110 v2, we'll use <code class="literal">n = 12</code>:</p><div class="informalexample"><pre class="programlisting">n = 12

# model version
# orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)
version = 2

# computed depth from supplied model parameter n
if version == 1:
    depth = n * 6 + 2
elif version == 2:
    depth = n * 9 + 2
…
if version == 2:
    model = resnet_v2(input_shape=input_shape, depth=depth)
else:
    model = resnet_v1(input_shape=input_shape, depth=depth)</pre></div><p>The accuracy of ResNet v2 is shown in following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p># Layers</p>
</th><th style="text-align: left" valign="bottom">
<p>n</p>
</th><th style="text-align: left" valign="bottom">
<p>% Accuracy on CIFAR10 (Original paper)</p>
</th><th style="text-align: left" valign="bottom">
<p>% Accuracy on CIFAR10 (This book)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>ResNet56</p>
</td><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>NA</p>
</td><td style="text-align: left" valign="top">
<p>93.01</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ResNet110</p>
</td><td style="text-align: left" valign="top">
<p>18</p>
</td><td style="text-align: left" valign="top">
<p>93.63</p>
</td><td style="text-align: left" valign="top">
<p>93.15</p>
</td></tr></tbody></table></div><p>In the Keras applications package, ResNet50 has <a id="id116" class="indexterm"/>been implemented as well with the corresponding checkpoint for reuse. This is an alternative implementation but tied to the 50-layer ResNet v1.</p></div>
<div class="section" title="Densely connected convolutional networks (DenseNet)"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec14"/>Densely connected convolutional networks (DenseNet)</h1></div></div></div><div class="mediaobject"><img src="graphics/B08956_02_11.jpg" alt="Densely connected convolutional networks (DenseNet)"/><div class="caption"><p>Figure 2.4.1: A 4-layer Dense block in DenseNet. The input to each layer is made of all the previous feature maps.</p></div></div><p>DenseNet attacks the <a id="id117" class="indexterm"/>problem of vanishing gradient using a different approach. Instead of using shortcut connections, all the previous feature maps will become the input of the next layer. The preceding figure, shows an example of a dense interconnection in one Dense block.</p><p>For simplicity, in this figure, we'll only show four layers. Notice that the input to layer <span class="emphasis"><em>l</em></span> is the concatenation of all previous feature maps. If we designate the <code class="literal">BN-ReLU-Conv2D</code> as the operation <span class="emphasis"><em>H</em></span>(x), then the output of layer <span class="emphasis"><em>l</em></span> is:</p><div class="mediaobject"><img src="graphics/B08956_02_020.jpg" alt="Densely connected convolutional networks (DenseNet)"/></div><p>          (Equation 2.4.1)</p><p>
<code class="literal">Conv2D</code> uses a kernel of size 3. The number of feature maps generated per layer is called the growth rate, <span class="emphasis"><em>k</em></span>. Normally, <span class="emphasis"><em>k</em></span> = 12, but <span class="emphasis"><em>k</em></span> = 24 is also used in the paper, <span class="emphasis"><em>Densely Connected Convolutional Networks</em></span>, Huang, and others, 2017 [5]. Therefore, if the number of feature maps </p><div class="mediaobject"><img src="graphics/B08956_02_021.jpg" alt="Densely connected convolutional networks (DenseNet)"/></div><p> is </p><div class="mediaobject"><img src="graphics/B08956_02_022.jpg" alt="Densely connected convolutional networks (DenseNet)"/></div><p>, then the total number of feature maps at the end of the 4-layer Dense block in <span class="emphasis"><em>Figure 2.4.1</em></span> will be </p><div class="mediaobject"><img src="graphics/B08956_02_023.jpg" alt="Densely connected convolutional networks (DenseNet)"/></div><p>.</p><p>DenseNet also recommends that the Dense block is preceded by <code class="literal">BN-ReLU-Conv2D</code>, along with the number of feature maps twice the growth rate, </p><div class="mediaobject"><img src="graphics/B08956_02_024.jpg" alt="Densely connected convolutional networks (DenseNet)"/></div><p>. Therefore, at the end of the Dense block, the total number of feature maps will be 72. We'll also use the same kernel size, which is 3. At the output layer, DenseNet <a id="id118" class="indexterm"/>suggests that we perform an average pooling before the <code class="literal">Dense()</code> and <code class="literal">softmax</code> classifier. If the data augmentation is not used, a dropout layer must follow the Dense block <code class="literal">Conv2D</code>:</p><div class="mediaobject"><img src="graphics/B08956_02_12.jpg" alt="Densely connected convolutional networks (DenseNet)"/><div class="caption"><p>Figure 2.4.2: A layer in a Dense block of DenseNet, with and without the bottleneck layer BN-ReLU-Conv2D(1). We'll include the kernel size as an argument of Conv2D for clarity.</p></div></div><p>As the network gets deeper, two new problems will occur. Firstly, since every layer contributes <span class="emphasis"><em>k</em></span> feature maps, the number of inputs at layer <span class="emphasis"><em>l</em></span> is </p><div class="mediaobject"><img src="graphics/B08956_02_025.jpg" alt="Densely connected convolutional networks (DenseNet)"/></div><p>. Therefore, the feature maps can grow rapidly within deep layers, resulting in the computation becoming slow. For example, for a 101-layer network this will be 1200 + 24 = 1224 for <span class="emphasis"><em>k</em></span> = 12.</p><p>Secondly, similar to ResNet, as the network gets deeper the feature maps size will be reduced to increase the coverage of the kernel. If DenseNet uses concatenation in the merge operation, it must reconcile the differences in size.</p><p>To prevent the number of feature maps from increasing to the point of being computationally inefficient, DenseNet introduced the Bottleneck layer as shown in <span class="emphasis"><em>Figure 2.4.2</em></span>. The idea is that after every concatenation; a 1 × 1 convolution with a filter size equal to 4<span class="emphasis"><em>k</em></span> is now applied. This dimensionality reduction technique prevents the number of feature maps to be processed by <code class="literal">Conv2D(3)</code> from<a id="id119" class="indexterm"/> rapidly increasing.</p><p>The Bottleneck layer then modifies the DenseNet layer as <code class="literal">BN-ReLU-Conv2D(1)-BN-ReLU-Conv2D(3)</code>, instead of just <code class="literal">BN-ReLU-Conv2D(3)</code>. We've included the kernel size as an argument of <code class="literal">Conv2D</code> for clarity. With the Bottleneck layer, every <code class="literal">Conv2D(3)</code> is processing just the 4<span class="emphasis"><em>k</em></span> feature maps instead of </p><div class="mediaobject"><img src="graphics/B08956_02_026.jpg" alt="Densely connected convolutional networks (DenseNet)"/></div><p> for layer <span class="emphasis"><em>l</em></span>. For example, for the 101-layer network, the input to the last <code class="literal">Conv2D(3)</code> is still 48 feature maps for <span class="emphasis"><em>k</em></span> = 12 instead of 1224 as computed previously:</p><div class="mediaobject"><img src="graphics/B08956_02_13.jpg" alt="Densely connected convolutional networks (DenseNet)"/><div class="caption"><p>Figure 2.4.3: The transition layer in between two Dense blocks</p></div></div><p>To solve the problem in feature maps size mismatch, DenseNet divides a deep network into multiple dense blocks that are joined together by transition layers as shown in the preceding figure. Within each dense block, the feature map size (that is, width and height) will remain constant.</p><p>The role of the transition layer is to <span class="emphasis"><em>transition</em></span> from one feature map size to a smaller feature map size<a id="id120" class="indexterm"/> between two dense blocks. The reduction in size is usually half. This is accomplished by the average pooling layer. For example, an <code class="literal">AveragePooling2D</code> with default <code class="literal">pool_size=2</code> reduces the size from (64, 64, 256) to (32, 32, 256). The input to the transition layer is the output of the last concatenation layer in the previous dense block.</p><p>However, before the feature maps are passed to average pooling, their number will be reduced by a certain compression factor, </p><div class="mediaobject"><img src="graphics/B08956_02_027.jpg" alt="Densely connected convolutional networks (DenseNet)"/></div><p>, using <code class="literal">Conv2D(1)</code>. DenseNet uses </p><div class="mediaobject"><img src="graphics/B08956_02_028.jpg" alt="Densely connected convolutional networks (DenseNet)"/></div><p> in their experiment. For example, if the output of the last concatenation of the previous dense block is (64, 64, 512), then after <code class="literal">Conv2D(1)</code> the new dimensions of the feature maps will be (64, 64, 256). When compression and dimensionality reduction are put together, the transition layer is made of <code class="literal">BN-Conv2D(1)-AveragePooling2D</code> layers. In practice, batch normalization precedes the convolutional layer.</p><div class="section" title="Building a 100-layer DenseNet-BC for CIFAR10"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec18"/>Building a 100-layer DenseNet-BC for CIFAR10</h2></div></div></div><p>We're now going to build a <span class="strong"><strong>DenseNet-BC</strong></span> (<span class="strong"><strong>Bottleneck-Compression</strong></span>) with 100 layers for the CIFAR10 dataset, using the design principles that we discussed<a id="id121" class="indexterm"/> above.</p><p>Following table, shows the <a id="id122" class="indexterm"/>model configuration, while <span class="emphasis"><em>Figure 2.4.3</em></span> shows the model architecture. <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>2.4.1</em></span> shows us the partial Keras implementation of DenseNet-BC with 100 layers. We need to take note that we use <code class="literal">RMSprop</code> since it converges better than SGD or Adam when using DenseNet.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Layers</p>
</th><th style="text-align: left" valign="bottom">
<p>Output Size</p>
</th><th style="text-align: left" valign="bottom">
<p>DenseNet-100 BC</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Convolution</p>
</td><td style="text-align: left" valign="top">
<p>32 x 32</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_029.jpg" alt="Building a 100-layer DenseNet-BC for CIFAR10"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Dense Block</p>
<p>(1)</p>
</td><td style="text-align: left" valign="top">
<p>32 x 32</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_030.jpg" alt="Building a 100-layer DenseNet-BC for CIFAR10"/></div>
</td></tr><tr><td rowspan="2" style="text-align: left" valign="top">
<p>Transition Layer</p>
<p>(1)</p>
</td><td style="text-align: left" valign="top">
<p>32 x 32</p>
</td><td rowspan="2" style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_031.jpg" alt="Building a 100-layer DenseNet-BC for CIFAR10"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>16 x 16</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Dense Block</p>
<p>(2)</p>
</td><td style="text-align: left" valign="top">
<p>16 x 16</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_032.jpg" alt="Building a 100-layer DenseNet-BC for CIFAR10"/></div>
</td></tr><tr><td rowspan="2" style="text-align: left" valign="top">
<p>Transition Layer</p>
<p>(2)</p>
</td><td style="text-align: left" valign="top">
<p>16 x 16</p>
</td><td rowspan="2" style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_033.jpg" alt="Building a 100-layer DenseNet-BC for CIFAR10"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8 x 8</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Dense Block</p>
<p>(3)</p>
</td><td style="text-align: left" valign="top">
<p>8 x 8</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_034.jpg" alt="Building a 100-layer DenseNet-BC for CIFAR10"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Average Pooling</p>
</td><td style="text-align: left" valign="top">
<p>1 x 1</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_02_035.jpg" alt="Building a 100-layer DenseNet-BC for CIFAR10"/></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Classification Layer</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<code class="literal">Flatten-Dense(10)-softmax </code>
</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p>Table 2.4.1: DenseNet-BC with 100 layers for CIFAR10 classification</p></blockquote></div><div class="mediaobject"><img src="graphics/B08956_02_14.jpg" alt="Building a 100-layer DenseNet-BC for CIFAR10"/><div class="caption"><p>Figure 2.4.3: Model architecture of DenseNet-BC with 100 layers for CIFAR10 classification</p></div></div><p>Listing 2.4.1, <code class="literal">densenet-cifar10-2.4.1.py</code>: Partial Keras<a id="id123" class="indexterm"/> implementation of DenseNet-BC with 100 layers as shown in <span class="emphasis"><em>Table 2.4.1</em></span>:</p><div class="informalexample"><pre class="programlisting"># start model definition
# densenet CNNs (composite function) are made of BN-ReLU-Conv2D
inputs = Input(shape=input_shape)
x = BatchNormalization()(inputs)
x = Activation('relu')(x)
x = Conv2D(num_filters_bef_dense_block,
           kernel_size=3,
           padding='same',
           kernel_initializer='he_normal')(x)
x = concatenate([inputs, x])

# stack of dense blocks bridged by transition layers
for i in range(num_dense_blocks):
    # a dense block is a stack of bottleneck layers
    for j in range(num_bottleneck_layers):
        y = BatchNormalization()(x)
        y = Activation('relu')(y)
        y = Conv2D(4 * growth_rate,
                   kernel_size=1,
                   padding='same',
                   kernel_initializer='he_normal')(y)
        if not data_augmentation:
            y = Dropout(0.2)(y)
        y = BatchNormalization()(y)
        y = Activation('relu')(y)
        y = Conv2D(growth_rate,
                   kernel_size=3,
                   padding='same',
                   kernel_initializer='he_normal')(y)
        if not data_augmentation:
            y = Dropout(0.2)(y)
        x = concatenate([x, y])

    # no transition layer after the last dense block
    if i == num_dense_blocks - 1:
        continue

    # transition layer compresses num of feature maps and 
    # reduces the size by 2
    num_filters_bef_dense_block += num_bottleneck_layers * growth_rate
    num_filters_bef_dense_block = int(num_filters_bef_dense_block * compression_factor)
    y = BatchNormalization()(x)
    y = Conv2D(num_filters_bef_dense_block,
               kernel_size=1,
               padding='same',
               kernel_initializer='he_normal')(y)
    if not data_augmentation:
        y = Dropout(0.2)(y)
    x = AveragePooling2D()(y)


# add classifier on top
# after average pooling, size of feature map is 1 x 1
x = AveragePooling2D(pool_size=8)(x)
y = Flatten()(x)
outputs = Dense(num_classes,
                kernel_initializer='he_normal',
                activation='softmax')(y)

# instantiate and compile model
# orig paper uses SGD but RMSprop works better for DenseNet
model = Model(inputs=inputs, outputs=outputs)
model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(1e-3),
              metrics=['accuracy'])
model.summary()</pre></div><p>Training the Keras implementation in <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>2.4.1</em></span> for 200 epochs achieves a 93.74% accuracy vs. the 95.49% as reported in the paper. Data augmentation is used. We used the same callback functions in ResNet v1/v2 for DenseNet.</p><p>For the deeper layers, the <code class="literal">growth_rate</code> and <code class="literal">depth</code> variables must be <a id="id124" class="indexterm"/>changed using the table on the Python code. However, it will take a substantial amount of time to train the network at a depth of 250, or 190 as done in the paper. To give us an idea of training time, each epoch runs for about an hour on a 1060Ti GPU. Though there is also an implementation of DenseNet in the Keras applications module, it was trained on ImageNet.</p></div></div>
<div class="section" title="Conclusion"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec15"/>Conclusion</h1></div></div></div><p>In this chapter, we've presented Functional API as an advanced method for building complex deep neural network models using Keras. We also demonstrated<a id="id125" class="indexterm"/> how the Functional API could be used to build the multi-input-single-output Y-Network. This network, when compared to a single branch CNN network, archives better accuracy. For the rest of the book, we'll find the Functional API indispensable in building more complex and advanced models. For example, in the next chapter, the Functional API will enable us to build a modular encoder, decoder, and autoencoder.</p><p>We also spent a significant time exploring two important deep networks, ResNet and DenseNet. Both of these networks have been used not only in classification but also in other areas, such as segmentation, detection, tracking, generation, and visual/semantic understanding. We need to remember that it's more important that we understand the model design decisions in ResNet and DenseNet more closely than just following the original implementation. In that manner, we'll be able to use the key concepts of ResNet and DenseNet for our purposes.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec16"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Kaiming He and others. <span class="emphasis"><em>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</em></span>. Proceedings of the IEEE international conference on computer vision, 2015 (<a class="ulink" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf?spm=5176.100239.blogcont55892.28.pm8zm1&amp;file=He_Delving_Deep_into_ICCV_2015_paper.pdf">https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf?spm=5176.100239.blogcont55892.28.pm8zm1&amp;file=He_Delving_Deep_into_ICCV_2015_paper.pdf</a>).</li><li class="listitem">Kaiming He and others. <span class="emphasis"><em>Deep Residual Learning for Image Recognition</em></span>. Proceedings of the IEEE conference on computer vision and pattern recognition, 2016a(<a class="ulink" href="http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.">http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf</a>).</li><li class="listitem">Karen Simonyan and Andrew Zisserman. <span class="emphasis"><em>Very Deep Convolutional Networks for Large-Scale Image Recognition</em></span>. ICLR, 2015(<a class="ulink" href="https://arxiv.org/pdf/1409.15">https://arxiv.org/pdf/1409.1556/</a>).</li><li class="listitem">Kaiming He and others. <span class="emphasis"><em>Identity Mappings in Deep Residual Networks</em></span>. European Conference on Computer Vision. Springer International Publishing, 2016b(<a class="ulink" href="https://arxiv.org/pdf/1603.05027.">https://arxiv.org/pdf/1603.05027.pdf</a>).</li><li class="listitem">Gao Huang and others. <span class="emphasis"><em>Densely Connected Convolutional Networks</em></span>. Proceedings of the IEEE conference on computer vision and pattern recognition, 2017(<a class="ulink" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf</a>).</li><li class="listitem">Saining Xie and others. <span class="emphasis"><em>Aggregated Residual Transformations for Deep Neural Networks</em></span>. Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017(<a class="ulink" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf</a>).</li><li class="listitem">Gustav Larsson, Michael Maire and Gregory Shakhnarovich. <span class="emphasis"><em>Fractalnet: Ultra-Deep Neural Networks Without Residuals</em></span>. arXiv preprint arXiv:1605.07648, 2016 (<a class="ulink" href="https://arxiv.org/pdf/1605.07648.pdf">https://arxiv.org/pdf/1605.07648.pdf</a>).</li></ol></div></div></body></html>