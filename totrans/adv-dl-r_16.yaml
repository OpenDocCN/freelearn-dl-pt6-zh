- en: Text Classification Using Convolutional Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**) have been found to be useful in
    capturing high-level local features from data. On the other hand, **recurrent
    neural networks** (**RNNs**), such as **long short-term memory** (**LSTM**), have
    been found to be useful in capturing long-term dependencies in data involving
    sequences such as text. When we use CNNs and RNNs in the same model architecture,
    it gives rise to what''s called **convolutional recurrent neural networks** (**CRNNs**).'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter illustrates how to apply convolutional recurrent neural networks
    to text classification problems by combining the advantages of RNNs and CNNs networks.
    The steps that are involved in this process include text data preparation, defining
    a convolutional recurrent network model, training the model, and model assessment.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with the reuter_50_50 dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data for model building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing the model architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compiling and fitting the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the model and predicting classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with the reuter_50_50 dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, when dealing with text data, we made use of data that
    had already been converted into a sequence of integers for developing deep network
    models. In this chapter, we will use text data that needs to be converted into
    a sequence of integers. We will start by reading the data that we will use to
    illustrate how to develop a text classification deep network model. We will also
    explore the dataset that we'll use so that we have a better understanding of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will make use of the `keras`, `deepviz`, and `readtext`
    libraries, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For illustrating the steps involved in developing a convolutional recurrent
    network model, we will make use of the `reuter_50_50` text dataset, which is available
    from the UCI Machine Learning Repository: [https://archive.ics.uci.edu/ml/datasets/Reuter_50_50#](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50#).
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset contains text files in two folders, with one folder for the training
    data and another for the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: The folder containing the training data has 2,500 text files with 50 articles
    each from 50 authors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, the folder containing the test data also has 2,500 text files with
    50 articles each from the same 50 authors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading the training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can access the `reuter_50_50` dataset by going to `Data` folder from the
    link that we provided for the UCI Machine Learning Repository. From here, we can
    download the `C50.zip` folder. When unzipped, it contains a `C50` folder containing
    `C50train` and `C50test` folders. First, we will read the text files from the
    `C50train` folder using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With the help of the preceding code, we can read data on 2,500 articles from
    the `C50train` folder into `trainx` and also save information about the author's
    names into `trainy`. We start by setting the working directory to the `C50train`
    folder using the `setwd` function. The `C50train` folder contains 50 folders named
    after 50 authors, and each folder contains 50 articles written by the corresponding
    author. We assign a value of 1 to k and initiate `tr`, `trainx`, and `trainy`
    as a list. Then, we create a loop so that the author's name is stored in `trainy`,
    which contains the author's names for each article, and so that `trainx` contains
    the corresponding articles written by the authors. Note that, after reading data
    on these 2,500 text files, `trainx` also contains information about file names.
    Using the last line of code, we retain data on only 2,500 texts and remove information
    about the file names that we will not need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the content of text file 901 from the train data using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code and output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The test file 901 in `trainx` contains certain news items about drug trials
    by the Chiroscience Group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The author of this short article is Jonathan Birt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having read the text files and author names for the training data, we can repeat
    this process for the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will read the text files from the `C50test` folder located within the
    `C50` folder. We will use the following code to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the only change in this code is that we are creating `testx`
    and `testy` based on the test data located within the `C50test` folder. We read
    2,500 articles from the `C50test` folder into `testx` and save information about
    the author's names into `testy`. Once again, we use the last line of code to retain
    data on only 2,500 texts from the test data and remove information on file names,
    which isn't required for our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've created the training and test data, we will carry out data preprocessing
    so that we can develop an author classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data for model building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will prepare some data so that we can develop an author
    classification model. We will start by using tokens to convert text data that
    is available in the form of articles into a sequence of integers. We will also
    make changes to identify each author by unique integers. Subsequently, we will
    use padding and truncation to arrive at the same length for the sequence of integers
    that represent the articles by 50 authors. We will end this section by partitioning
    the training data into train and validation datasets and then carrying out one-hot
    encoding on the response variables.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization and converting text into a sequence of integers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start by carrying out tokenization and then converting the articles,
    which are in text form, into a sequence of integers. To do this, we can use the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code and output, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: For tokenization, we specify `num_words` as 500, indicating that we will use
    the 500 most frequent words from the text in the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that using `fit_text_tokenizer` automatically converts text into lowercase
    and removes any punctuation that can be observed in the articles containing text
    data. Converting text into lowercase helps us avoid duplicates of words, where
    one may contain lowercase alphabetical characters and another may have uppercase
    alphabetical characters. Punctuation is removed since it doesn't add value when
    developing the author classification model with text as input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use `texts_to_sequences` to convert the most frequent words in the text into
    a sequence of integers. The reason for doing this is to convert the unstructured
    data so that it has a structured format, which is required by deep learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of text file 7 shows a total of 314 integers that are between 1 and
    497.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at the output for text file 901 (the same example in the training data
    that we reviewed earlier), we can see that it consists of 48 integers between
    1 and 470\. The original text consists of over 80 words and those words that do
    not belong to the 500 most frequent words are not represented in this sequence
    of integers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first five integers, that is, 74, 356, 7, 9, and 199, correspond to the
    words `group`, `plc`, `said`, `on`, and `monday`, respectively. Other words at
    the beginning of the text that haven't been converted into integers do not belong
    to the top 500 most frequent words in the articles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s look at the number of integers per article in the training and
    test data. We can do this with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding summary, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of integers per article in the training data ranges from 31 to 918,
    with a median of about 326 words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, the integers per article range from 39 to 1001 for the test data,
    with a median of about 331.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the number of most frequent words is increased from 500 to a higher value,
    the median number of words is also expected to increase. This may lead to suitable
    changes needing to be made in the model architecture and parameter values. As
    an example, an increase in the number of words per article may call for more neurons
    in the deep network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A histogram of the number of integers per text file for the training data is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c45c3a5-2685-4d49-acbd-0be7edf681d6.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding histogram of integers per text file for the training data shows
    the overall pattern, with a mean and median of about 326\. The tail of this histogram
    is slightly longer toward the higher value, giving it a moderately right-skewed
    or positively-skewed pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've converted the text data into a sequence of integers, we will
    change the labels for the train and text data into integers as well.
  prefs: []
  type: TYPE_NORMAL
- en: Changing labels into integers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When developing deep learning networks for classification problems, we always
    use responses or labels in the form of integers. Author names for the train and
    test text data are stored in `trainy` and `testy`, respectively. Both `trainy`
    and `testy` are lists of 2,500 items that contain the names of 50 authors. To
    change the labels into integers, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, to convert labels containing author names into integers, we need
    to unlist them and then use integers from 0 to 49 to represent the 50 authors.
    We can also use `trainy_org` and `testy_org` to save these original integer labels
    for later use.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will carry out padding and truncation to make the data on a sequence
    of integers have an equal length for each article.
  prefs: []
  type: TYPE_NORMAL
- en: Padding and truncation of sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When developing the author classification model, the number of integers for
    each training and test text data need to be of equal length. We can achieve this
    by padding and truncating the sequence of integers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are specifying the maximum length of all the sequences, that is, `maxlen`,
    to be 300\. This will truncate any sequences that are longer than 300 integers
    in an article and add zeroes to sequences that are shorter than 300 integers in
    an article. Note that for padding and truncation, a default setting of "pre" has
    been used and is not specifically indicated in the code.
  prefs: []
  type: TYPE_NORMAL
- en: This means that for truncation and padding, the integers at the beginning of
    the sequence of integers are impacted. For padding and/or truncation toward the
    end of the sequence of integers, we can make use of `padding = "post"` and/or
    `truncation = "post"` within the code. We can also see that the dimensions of
    `trainx` show a 2,500 x 300 matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the output from text files 7 and 901 in the train data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Text file 7, which had 314 integers, has been reduced to 300 integers. Note
    that this step removed 14 integers at the beginning of the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text file 901, which had 48 integers, now has 300 integers, which has been achieved
    by adding zeros at the beginning of the sequence to artificially make the total
    number of integers 300.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will partition the training data into train and validation data, which
    will be required for training and assessing the network at the time of fitting
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Data partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of training the model, we use `validation_split`, which uses a
    specified percentage of training data to assess validation errors. The training
    data in this example contains data of the first 50 articles from the first author,
    followed by 50 articles from the second author, and so on. If we specify `validation_split`
    as 0.2, the model will be trained based on the first 80% (or 2,000) articles from
    the first 40 authors, and the last 20% (or 500) articles written by the last 10
    authors will be used for assessing validation errors. This will cause no input
    from the last 10 authors to be used in the model training. To overcome this problem,
    we will randomly partition the training data into train and validation data using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, to partition the data into train and validation data, we have
    used an 80:20 split. We also used the `set.seed` function for repeatability purposes.
  prefs: []
  type: TYPE_NORMAL
- en: After partitioning the train data, we will carry out one-hot encoding on the
    labels, which helps us represent the correct author with a value of one, and all
    the other authors with a value of zero.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding the labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To carry out one-hot encoding on the labels, we will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have used the `to_categorical` function to one-hot encode the response
    variable. We used 50 to indicate the presence of 50 classes since the articles
    have been written by 50 authors that we plan to classify, using articles that
    have been written by them as input.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the data is ready for developing the convolutional recurrent network model
    for author classification based on the articles they have written.
  prefs: []
  type: TYPE_NORMAL
- en: Developing the model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will make use of convolutional and LSTM layers in the same
    network. The convolutional recurrent network architecture can be captured in the
    form of a simple flowchart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92ef946a-77e5-4756-a3a1-a2617ba2b4ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that the flowchart contains embedding, convolutional 1D, maximum
    pooling, LSTM, and dense layers. Note that the embedding layer is always the first
    layer in the network and is commonly used for applications involving text data.
    The main purpose of the embedding layer is to find a mapping of each unique word,
    which in our example is 500, and turn it into a vector that is smaller in size,
    which we will specify using `output_dim`. In the convolutional layer, we will
    use the `relu` activation function. Similarly, the activation functions that will
    be used for the LSTM and dense layers will be `tanh` and `softmax`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the following code to develop the model architecture. This also
    includes the output of the model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: We have specified `input_dim` as 500, which was used as the number of most frequent
    words during data preparation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For `output_dim`, we are using 32, which represents the size of the embedding
    vector. However, note that other numbers can also be explored and we will do so
    later in this chapter, at the time of performance optimization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For `input_length`, we have specified 300, which is the number of integers in
    each sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After the embedding layer, we have added a 1D convolutional layer with 32 filters.
    In the previous chapters, we used a 2D convolutional layer when working on image
    classification problems. In this example, we have data involving sequences and,
    in such situations, a 1D convolutional layer is more appropriate. For this layer,
    we have specified the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The length of the 1D convolutional window is specified as 5 using `kernel_size`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use `valid` for padding to indicate that no padding is required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have specified the activation function as `relu`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strides of the convolution have been specified at 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The convolutional layer is followed by a pooling layer. The following are some
    of the comments for pooling and the subsequent layer:'
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layer helps us extract features, while the pooling layer after
    the convolutional layer helps us carry out downsampling and detect important features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, we have specified a pooling size of 4, which means that the
    size of the output (74) is one-fourth of the input (296). This can also be seen
    in the model summary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next layer is the LSTM with 32 units.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last layer is a dense layer with 50 units for the 50 authors, along with
    the `softmax` activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `softmax` activation function makes all 50 outputs have a total value of
    one and thus allows them to be used as probabilities for each of the 50 authors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see from the summary of the model, the total number of parameters
    in this network is 31,122.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will compile the model, followed by training it.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling and fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will compile the model and then train the model using the
    `fit` function using the training and validation dataset. We will also plot the
    loss and accuracy values that were obtained while training the model.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For compiling the model, we will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we've specified the `adam` optimizer. We're using `categorical_crossentropy`
    as the loss function since the labels are based on 50 authors. For the metrics,
    we've specified the accuracy of the author's classification.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the model has been compiled, it's ready for training.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will train the model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, we're training the model using `trainx` as input and `trainy` as output.
    The model's training is carried out for 30 epochs with a batch size of 32\. For
    assessing the validation loss and validation accuracy for each epoch during the
    training process, we make use of `validx` and `validy`, which we created earlier
    by taking approximately a 20% random sample from the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss and accuracy values based on the train and validation data for each
    of the 30 epochs are stored in `model_one`. The following is a plot of this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2647d6db-d60f-434a-a1e5-ecec9ff2e0e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss values for the training and validation data reduce as we go from 1
    to 30 epochs. However, the loss values for the validation data reduce at a slower
    pace compared to those for the training data as the training proceeds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The accuracy values for the training and validation data show a similar pattern
    in the opposite direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the number of epochs during training is likely to improve the loss
    and accuracy values; however, divergence between the curves is also expected to
    increase, with this potentially leading to an overfitting situation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will evaluate `model_one` and make predictions using training and test
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model and predicting classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will evaluate the model based on our training and test data.
    We will obtain accuracy by correctly classifying each author using a confusion
    matrix for the training and test data to gain further insights. We will also use
    bar plots to visualize the accuracy of identifying each author.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation with training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will evaluate the model''s performance using training data. Then,
    we will use the model to predict the class representing each of the 50 authors.
    The code for evaluating the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that, by using the training data, we obtain a loss value of
    about 1.457 and an accuracy of about 0.535\. Next, we use the model to make a
    prediction about the classes for the articles in the training data. These predictions
    are then used to arrive at an accuracy reading for each of the 50 classes representing
    50 authors. The code that''s used to achieve this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, to conserve space, we haven''t printed the output of
    the confusion matrix since it will be a 50 x 50 matrix. However, we have used
    information in the confusion matrix to arrive at the model''s accuracy by correctly
    predicting each author based on the articles they have written. The output that
    we''ve obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d97e23e-30f6-4829-afd0-3995073166e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding bar plot provides further insight into the model''s performance
    with respect to each author:'
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of correctly classifying an author has the highest value of 90%
    for author 15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The accuracy of correctly classifying an author has the lowest value of 0% for
    author 43.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model struggles to correctly classify articles from certain authors, such
    as those labeled 3, 8, 18, 31, 43, 45, and 48.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having assessed the model using training data, we will repeat this process with
    the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation with test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the model to obtain the loss and accuracy values from the test
    data using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can see that the loss and accuracy values based
    on the test data are 2.461 and 0.251, respectively. Both of these results are
    inferior to the ones we obtained based on the training data, which is usually
    expected. Predicting the classes and calculating the accuracy of classification
    for each author, as shown in the following code, would help provide further insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The information in the confusion matrix is stored in `tab1`, which is used
    for arriving at the accuracy of correctly classifying articles from each author.
    The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1292f3a5-02ee-4259-bbc8-eaf82fbc1b23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An overall accuracy of about 25% for the test data already suggested significantly
    inferior performance based on the test data. This can also be seen in the preceding
    bar chart. Let''s take a look at some of the observations we can make from this:'
  prefs: []
  type: TYPE_NORMAL
- en: For the authors labeled 31, 43, 45, and 48, none of the 50 articles written
    by each author were correctly classified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More than 80% of the articles from the authors labeled 15 and 38 were correctly
    classified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From this initial example, we can see that our model classification performance
    needs further improvement. The differences in performance that we have observed
    between the training and test data also indicate the presence of an overfitting
    problem. Thus, we need to make changes to the model architecture to obtain a model
    that not only provides higher accuracy in classification performance but also
    shows consistent performance between the training and test data. We will explore
    this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore changes we can make to the model architecture
    and other settings to improve author classification performance. We will carry
    out two experiments, and, for both of these two experiments, we will increase
    the number of most frequent words from 500 to 1,500 and increase the length of
    the sequences of integers from 300 to 400\. For both experiments, we will also
    add a dropout layer after the pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with reduced batch size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code that we''ll be using for this experiment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: We will update the model architecture by specifying `input_dim` as 1,500 and
    `input_length` as 400.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will reduce the batch size that's used at the time of fitting the model from
    32 to 16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address the overfitting problem, we have added a dropout layer with a rate
    of 25%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have kept all other settings the same as those we had used for the previous
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The loss and accuracy values based on the training and validation data for
    each of the 30 epochs is stored in `model_two`. The results can be seen in the
    following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d33b921e-8107-4abb-b084-e851906e486d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding plot indicates that the loss and accuracy values for the validation
    data stay flat for the last few epochs. However, they do not deteriorate. Next,
    we will obtain the loss and accuracy values based on the training and test data
    using the `evaluate` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code and output, we can observe that the loss and accuracy
    values for the training data show better results compared to the previous model.
    However, for the test data, although the accuracy value is better, the loss value
    is slightly worse.
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy that was obtained by correctly classifying the articles in the
    testing data from each author can be seen in the following bar plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/698c0af5-4bf0-4171-aea2-62ad35dc4d4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding bar plot, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The bar plot visually shows improvements compared to the previous model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous model, for the test data, we had four authors with no articles
    classified correctly. However, now, we don't have any authors with no correct
    classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next experiment, we will look at more changes we can make in an effort
    to improve the author's classification performance even further.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with batch size, kernel size, and filters in CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code that will be used for this experiment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: We have reduced the kernel size from 5 to 4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have increased the number of filters for the convolutional layer from 32
    to 64.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have reduced the batch size from 16 to 8 while training the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have kept all other settings the same as what was used for the previous model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The loss and accuracy values based on the training and validation data for
    each of the 30 epochs are stored in `model_three`. A plot of this data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfc795b3-827b-4043-b66f-968f6e21af1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The plot for the loss and accuracy shows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy values for the validation data remain flat for the last few epochs,
    whereas it increases at a relatively slower pace in the last few epochs for the
    training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss values based on the validation data start to increase during the last
    few epochs and continue to decrease for the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we will obtain the loss and accuracy values based on the train and test
    data using the `evaluate` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code and output, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy values based on the training data show an improvement
    compared to the previous two models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the test data, although the loss value is higher compared to the first two
    models, an accuracy value of about 34% shows better accuracy in classifying author
    articles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following bar plot shows the accuracy of correctly classifying the authors
    of articles in the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb04c5b6-ee46-46b9-9145-30b64a20acfb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding bar plot, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of correctly classifying articles from each author shows better
    performance compared to the previous two models since we don't have any authors
    with zero accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When comparing the three models that we've used so far using test data, we can
    see that the first model has four authors classified with 50% or higher accuracy.
    However, for the second and third models, the number of authors classified with
    50% or higher accuracy increases to 8 and 9, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we carried out two experiments that showed that the author
    classification performance of the model can be improved further.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we illustrated the steps for developing a convolutional recurrent
    neural network for author classification based on articles that they have written.
    Convolutional recurrent neural networks combine the advantages of two networks
    into one network. On one hand, convolutional networks can capture high-level local
    features from the data, while, on the other hand, recurrent networks can capture
    long-term dependencies in the data involving sequences.
  prefs: []
  type: TYPE_NORMAL
- en: First, convolutional recurrent neural networks extract features using a one-dimensional
    convolutional layer. These extracted features are then passed to the LSTM recurrent
    layer to obtain hidden long-term dependencies, which are then passed to a fully
    connected dense layer. This dense layer obtains the probability of the correct
    classification of each author based on the data in the articles. Although we used
    a convolutional recurrent neural network for the author classification problem,
    this type of deep network can be applied to other types of data involving sequences,
    such as natural language processing, speech, and video-related problems.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will be the last chapter of this book and will go over tips,
    tricks, and the road ahead. Developing deep learning networks for different types
    of data is both art and science. Every application brings new challenges, as well
    as an opportunity for us to learn and improve our skills. In the next chapter,
    we will summarize some such experiences that can turn out to be very useful in
    certain applications and help save a significant amount of time in arriving at
    models that perform well.
  prefs: []
  type: TYPE_NORMAL
