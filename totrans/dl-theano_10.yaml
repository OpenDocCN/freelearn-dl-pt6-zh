- en: Chapter 10. Predicting Times Sequences with Advanced RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers advanced techniques for recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The techniques seen in [Chapter 2](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 2. Classifying Handwritten Digits with a Feedforward Network"), *Classifying
    Handwritten Digits with a Feedforward Network*, for feedforward networks, such
    as going deeper with more layers, or adding a dropout layer, have been more challenging
    for recurrent networks and require some new design principles.
  prefs: []
  type: TYPE_NORMAL
- en: Since adding new layers increases the vanishing/exploding gradient issue, a
    new technique based on identity connections as for [Chapter 7](part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 7. Classifying Images with Residual Networks"), *Classifying Images with
    Residual Networks* has proved to provide state-of-the-art results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered are:'
  prefs: []
  type: TYPE_NORMAL
- en: Variational RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacked RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Transition RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highway connections and their application to RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout for RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The application of dropout inside neural networks has long been a subject of
    research, since the naïve application of dropout to the recurrent connection introduced
    lots more instability and difficulties to training the RNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'A solution has been discovered, derived from the variational **Bayes Network**
    theory. The resulting idea is very simple and consists of preserving the same
    dropout mask for the whole sequence on which the RNN is training, as shown in
    the following picture, and generating a new dropout mask at each new sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout for RNN](img/00179.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Such a technique is called **variational RNN.** For the connections that have
    the same arrows in the preceding figure, we'll keep the noise mask constant for
    the all sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'For that purpose, we''ll introduce the symbolic variables `_is_training` and
    `_noise_x` to add a random (variational) noise (dropout) to input, output, and
    recurrent connection during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Deep approaches for RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The core principle of deep learning to improve the representative power of
    a network is to add more layers. For RNN, two approaches to increase the number
    of layers are possible:'
  prefs: []
  type: TYPE_NORMAL
- en: The first one is known as **stacking** or **stacked recurrent network**, where
    the output of the hidden layer of a first recurrent net is used as input to a
    second recurrent net, and so on, with as many recurrent networks on top of each
    other:![Deep approaches for RNN](img/00180.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a depth *d* and *T* time steps, the maximum number of connections between
    input and output is *d + T – 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: The second approach is the **deep transition network**, consisting of adding
    more layers to the recurrent connection:![Deep approaches for RNN](img/00181.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this case, the maximum number of connections between input and output is
    *d x T*, which has been proved to be a lot more powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Both approaches provide better results.
  prefs: []
  type: TYPE_NORMAL
- en: However, in the second approach, as the number of layers increases by a factor,
    the training becomes much more complicated and unstable since the signal fades
    or explodes a lot faster. We'll address this problem later by tackling the principle
    of recurrent highway connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, as usual, sequences of words, represented as an array of index values
    in the vocabulary, and of dimension (`batch_size, num_steps`), are embedded into
    an input tensor of dimension (`num_steps, batch_size, hidden_size`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The symbolic input variable `_lr` enables the decrease of the learning rate
    during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let's begin with the first approach, the stacked recurrent networks.
  prefs: []
  type: TYPE_NORMAL
- en: Stacked recurrent networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To stack recurrent networks, we connect the hidden layer of the following recurrent
    network, to the input of the preceding recurrent network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stacked recurrent networks](img/00182.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: When the number of layers is one, our implementation is a recurrent network
    as in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we implement dropout in our simple RNN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We do the same in our LSTM model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Running our stacked networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We get 15,203,150 parameters for the RNN, with 326 **words per seconds** (WPS)
    on a CPU and 4,806 WPS on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: For LSTM, the number of parameters is 35,882,600 with a speed of 1,445 WPS on
    a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The stacked RNN do not converge, as we might have imagined: the vanishing/exploding
    gradient issue is increased with depth.'
  prefs: []
  type: TYPE_NORMAL
- en: LSTM, designed to reduce such as an issue, do converge a lot better when stacked,
    than as a single layer.
  prefs: []
  type: TYPE_NORMAL
- en: Deep transition recurrent network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contrary to stacked recurrent network, a deep transition recurrent network consists
    of increasing the depth of the network along the time direction, by adding more
    layers or *micro-timesteps* inside the recurrent connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this, let us come back to the definition of a transition/recurrent
    connection in a recurrent network: it takes as input the previous state ![Deep
    transition recurrent network](img/00183.jpeg) and the input data ![Deep transition
    recurrent network](img/00184.jpeg) at time step *t*, to predict its new state
    ![Deep transition recurrent network](img/00185.jpeg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a deep transition recurrent network (figure 2), the recurrent transition
    is developed with more than one layer, up to a recurrency depth *L*: the initial
    state is set to the output of the last transition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep transition recurrent network](img/00186.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Furthermore, inside the transition, multiple states or steps are computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep transition recurrent network](img/00187.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The final state is the output of the transition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep transition recurrent network](img/00188.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Highway networks design principle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adding more layers in the transition connections increases the vanishing or
    exploding gradient issue during backpropagation in long term dependency.
  prefs: []
  type: TYPE_NORMAL
- en: In the [Chapter 4](part0051_split_000.html#1GKCM1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 4. Generating Text with a Recurrent Neural Net"), *Generating Text with
    a Recurrent Neural Net*, LSTM and GRU networks have been introduced as solutions
    to address this issue. Second order optimization techniques also help overcome
    this problem.
  prefs: []
  type: TYPE_NORMAL
- en: A more general principle, based on **identity connections**, to improve the
    training in deep networks [Chapter 7](part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 7. Classifying Images with Residual Networks"), *Classifying Images with
    Residual Networks*, can also be applied to deep transition networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the principle in theory:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given an input *x* to a hidden layer *H* with weigh ![Highway networks design
    principle](img/00189.jpeg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Highway networks design principle](img/00190.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A highway networks design consists of adding the original input information
    (with an identity layer) to the output of a layer or a group of layers, as a shortcut:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = x*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two mixing gates, the *transform gate* ![Highway networks design principle](img/00191.jpeg)
    and the *carry gate*, ![Highway networks design principle](img/00192.jpeg) learn
    to modulate the influence of the transformation in the hidden layer, and the amount
    of original information to allow to pass through:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Highway networks design principle](img/00193.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Usually, to reduce the total number of parameters in order to get faster-to-train
    networks, the carry gate is taken as the complementary to 1 for the transform
    gate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Highway networks design principle](img/00194.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Recurrent Highway Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, let''s apply the highway network design to deep transition recurrent networks,
    which leads to the definition of **Recurrent Highway Networks** (**RHN**), and
    predict the output ![Recurrent Highway Networks](img/00185.jpeg) given ![Recurrent
    Highway Networks](img/00183.jpeg) the input of the transition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent Highway Networks](img/00186.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The transition is built with multiple steps of highway connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent Highway Networks](img/00195.jpeg)![Recurrent Highway Networks](img/00188.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here the transform gate is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent Highway Networks](img/00196.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And, to reduce the number of weights, the carry gate is taken as the complementary
    to the transform gate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent Highway Networks](img/00194.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For faster computation on a GPU, it is better to compute the linear transformation
    on inputs over different time steps ![Recurrent Highway Networks](img/00197.jpeg)
    and ![Recurrent Highway Networks](img/00198.jpeg) in a single big matrix multiplication,
    all-steps input matrices ![Recurrent Highway Networks](img/00199.jpeg) and ![Recurrent
    Highway Networks](img/00200.jpeg) at once, since the GPU will use a better parallelization,
    and provide these inputs to the recurrency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'With a deep transition between each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The recurrent hidden state of the RHN is sticky (the last hidden state of one
    batch is carried over to the next batch, to be used as an initial hidden state).
    These states are kept in a shared variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The number of parameters of the stacked RHN is *84,172,000*, its speed *420*
    wps on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: This model is the new state-of-the-art model for recurrent neural network accuracy
    on texts.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following topics for more insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hi**ghway Networks* at: [https://arxiv.org/abs/1505.00387](https://arxiv.org/abs/1505.00387)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Depth-Gated LSTM* at: [https://arxiv.org/abs/1508.03790](https://arxiv.org/abs/1508.03790)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning Longer Memory in Recurrent N**eural Networks* at: [https://arxiv.org/abs/1412.7753](https://arxiv.org/abs/1412.7753)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Grid Long Short-Term Memory*, Nal Kalchbrenner, Ivo Danihelka, Alex Graves'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zilly, J, Srivastava, R, Koutnik, J, Schmidhuber, J., *Recurrent Highway Networks*,
    2016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gal, Y, *A Theoretically Grounded Application of Dropout in Recurrent Neural
    Networks*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zaremba, W, Sutskever, I, Vinyals, O, *Recurrent neural network regularization*,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Press, O, Wolf, L, *Using the Output Embedding to Improve Language Models*,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gated Feedback Recurrent Neural Networks: Junyoung Chung, Caglar Gulcehre,
    Kyunghyun Cho, Yoshua Bengio 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Clockwork RNN: Jan Koutník, Klaus Greff, Faustino Gomez, Jürgen Schmidhuber
    2014'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A classic dropout method to improve network robustness may be applied to recurrent
    network sequence-wise or batch-wise to avoid instability and destruction of the
    recurrent transition. For example, when applied on word inputs/outputs, it is
    equivalent to removing the same words from the sentence, replacing them with a
    blank value.
  prefs: []
  type: TYPE_NORMAL
- en: The principle of stacking layers in deep learning to improve accuracy applies
    to recurrent networks that can be stacked in the depth direction without burden.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the same principle in the transition of the recurrent nets increases
    the vanishing/exploding issue, but is offset by the invention of the highway networks
    with identity connections.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced techniques for recurrent neural nets give state-of-the-art results
    in sequence prediction.
  prefs: []
  type: TYPE_NORMAL
