<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building a Deep Learning Gaming Chatbot</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Chatbots</strong>, or conversational agents, are an exploding trend in AI and are seen as the next human interface with the computer. From Siri, Alexa, and Google Home, there has been an explosion of commercial growth in this area, and you most likely already have interfaced with a computer in this manner. Therefore, it only seems natural that we cover how to build conversational agents for games. For our purposes, however, we are going to look at the class of bots called <strong>neural conversational agents</strong>. Their name follows from the fact that they are developed with neural networks. Now, chatbots don't have to just chat; we will also look at other ways conversational bots can be used in gaming.</p>
<p>In this chapter, we learn how to build neural conversational agents and how to apply these techniques to games. The following is a summary of the main topics we will cover:</p>
<ul>
<li>Neural conversational agents</li>
<li>Sequence-to-sequence learning</li>
<li>DeepPavlov</li>
<li>Building the bot server</li>
<li>Running the bot in Unity</li>
<li>Exercises</li>
</ul>
<p>We will now start building more practical real-world working examples of the projects. While not all of your training is complete, it is time we started to build pieces you can use. This means we will begin working with Unity in this chapter and things may start to get complicated quickly. Just remember to take your time and, if you need to, go over the material a few times. Again, the exercises at the end of the chapter are an excellent resource for additional learning.</p>
<p>In the next section, we explore the basics of neural conversational agents.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural conversational agents</h1>
                </header>
            
            <article>
                
<p>The concept of communicating with a computer via natural language first became popular as far back as Star Trek (1966 to 1969). In the series, we can often see Kirk, Scotty, and the gang issuing commands to the computer. Since then, many attempts have been made to build chatbots that can converse naturally with a human. During this often unsuccessful journey over the years, several linguistic methods have been developed. These methods are often grouped together and referred to as <strong>natural language processing</strong>, or <strong>NLP</strong>. Now, NLP still is the foundation for most chatbots, including the deep learning variety we will get to shortly.</p>
<p>We often group conversational agents by purpose or task. Currently, we categorize chatbots into two main types:</p>
<ul>
<li><strong>Goal-oriented</strong>:<strong> </strong>These bots are the kind Kirk would use or the ones you likely communicate with on a daily basis, and a good example is Siri or Alexa.</li>
<li><strong>General conversationalist</strong>: These chatbots are designed to converse with people regarding a wide range of topics, and a good example would be <strong>Microsoft Tay</strong>. Unfortunately, the Tay bot was perhaps a little too impressionable and picked up bad language, much like a two-year-old does.</li>
</ul>
<p>Gaming is certainly no stranger to chatbots, and attempts have been made to use both forms with varying success. While you may think goal-oriented bots make perfect sense, in reality the vocal/text is too slow and tedious for most repetitive gaming tasks. Even simple vocal commands (grunts or groans) are just too slow, at least currently. Therefore, we will look at the often under utilized conversational chatbots and how they can be used in gaming.</p>
<p>The following is a summary of the gaming tasks these bots could undertake:</p>
<ul>
<li><strong>Non-player characters</strong> (<strong>NPCs</strong>): This is an obvious first choice. NPCs are often scripted and become repetitive. How about an NPC that can converse naturally about a topic, perhaps revealing information when the right combination of words or phrases are used? The possibilities are endless here, and some NLP is already used in gaming for this matter.</li>
<li><strong>Player character</strong>: How about a game where you could converse with yourself? Perhaps the character has amnesia and is trying to remember information or learn a backstory.</li>
<li><strong>Promotion</strong>/<strong>hints</strong>: Perhaps as a way to promote your game, you build a bot that can hint at how to complete some difficult tasks or just as a way to talk about your game.</li>
<li><strong>MMO virtual character</strong>: What if, while you were away from your favorite MMO game, your character stayed in the game, unable to do actions, but still able to converse as you? This is the example we will look at in this chapter, and we will get to the action part later, when we explore <strong>reinforcement learning</strong>.</li>
</ul>
<p>There are likely dozens more uses that will evolve over time, but for now the preceding list should give you some great ideas regarding how to use chatbots in gaming. In the next section, we get into the background of what makes a conversationalist bot.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">General conversational models</h1>
                </header>
            
            <article>
                
<p>Conversational chatbots can be broken down further into two main forms: <strong>generative</strong> and <strong>selective</strong>. The method we will look at is called generative. Generative models learn by being fed a sequence of words and dialog in context/reply pairs. Internally, these models use RNN (LSTM) layers to learn and predict those sequences back to the conversant. An example of how this system works is as follows:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/79642a34-900d-4645-8724-247d67950d08.png" style="width:58.92em;height:17.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><br/>
Example of the generative conversational model</div>
<p>Note that each block in the diagram represents one LSTM cell. Each cell then remembers the sequence that text was part of. What may not be clear from the preceding diagram is that both sides of the conversation text were fed into the model before training. Thus, this model is not unlike the GANs we covered in <a href="cb51d15b-9855-47e2-8e45-f74a115ebfa8.xhtml" target="_blank">Chapter 3</a><em>, GAN for Games</em>. In the next section, we will get into the details of setting up this type of model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sequence-to-sequence learning</h1>
                </header>
            
            <article>
                
<p>In the previous section, we saw a high-level overview of our network model. In this section, we want to look at a Keras implementation of a generative conversational model that uses sequence-to-sequence learning. Before we get into the theory of this form of generative model, let's get the sample running, since it can take a while. The sample we will explore is the Keras reference sample for sequence-to-sequence machine translation. It is currently configured to do English-to-French translation. </p>
<p>Open up the <kbd>Chapter_4_1.py</kbd> sample code listing and get it running using these steps:</p>
<ol>
<li class="mce-root">Open up a shell or Anaconda window. Then run the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>python3 Chapter_4_1.py</strong></pre>
<ol start="2">
<li>This will run the sample, and it may take several hours to run. The sample can also consume a substantial amount of memory and this may force memory paging on lower memory systems. Paging memory to disk will take additional time to train, especially if you are not running an SSD. If you find that you are unable to complete training on this sample, reduce the number of <kbd>epochs</kbd> and/or <kbd>num_samples</kbd> parameters as follows:</li>
</ol>
<pre style="padding-left: 60px">batch_size = 64 # Batch size for training.<br/><strong>epochs = 100</strong> # Number of epochs to train for.<br/>latent_dim = 256 # Latent dimensionality of the encoding space.<br/><strong>num_samples = 10000</strong> # Number of samples to train on.</pre>
<ol start="3">
<li>Decrease the <kbd>epochs</kbd> or <kbd>num_samples</kbd> parameters if you are unable to train on the original values.</li>
</ol>
<ol start="4">
<li>After the sample has completed training, it will run through a test set of data. As it does so, it will output the results and you can see how well it is translating from English to French.</li>
<li>Open the <kbd>fra-eng</kbd> folder located in the chapter source code.</li>
</ol>
<ol start="6">
<li class="mce-root">Open the <kbd>fra.txt</kbd> file and the top few lines are as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">Go. Va !<br/>Hi. Salut !<br/>Run! Cours !<br/>Run! Courez !<br/>Wow! Ça alors !<br/>Fire! Au feu !<br/>Help! À l'aide !<br/>Jump. Saute.<br/>Stop! Ça suffit !<br/>Stop! Stop !<br/>Stop! Arrête-toi !<br/>Wait! Attends !<br/>Wait! Attendez !<br/>Go on. Poursuis.<br/>Go on. Continuez.<br/>Go on. Poursuivez.<br/>Hello! Bonjour !<br/>Hello! Salut !</pre>
<ol start="7">
<li>Notice how the training text (English/French) is split on punctuation and spaces. Also, note how the sequences vary in length. The sequences we input do not have to match the length of the output, and vice versa.</li>
</ol>
<p>The sample we just looked at uses sequence-to-sequence character encoding to translate text from English to French. Typically, chat generation is done with word-to-word encoding, but this sample uses a finer-grained character-to-character model. This has an advantage in games because the language we attempt to generate may not always be human. Keep in mind that while we are only generating translated text in this sample, the text paired with an input could be any response you deem appropriate. In the next section, we will break down the code and understand in some detail how this sample works.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Breaking down the code</h1>
                </header>
            
            <article>
                
<p>As we progress through the book, we will begin to only focus on important sections of code, sections that help us understand a concept or how a method is implemented. This will make it more important for you to open up the code and at least pursue it on your own. In the next exercise, we take a look at the important sections of the sample code:</p>
<ol>
<li class="mce-root">Open <kbd>Chapter_4_1.py</kbd> and scroll down to the comment <kbd>Vectorize the data</kbd>, as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px"># Vectorize the data.<br/>input_texts = []<br/>target_texts = []<br/>input_characters = set()<br/>target_characters = set()<br/>with open(data_path, 'r', encoding='utf-8') as f:<br/>    lines = f.read().split('\n')<br/>for line in lines[: min(num_samples, len(lines) - 1)]:<br/>    input_text, target_text = line.split('\t')<br/>    # We use "tab" as the "start sequence" character<br/>    # for the targets, and "\n" as "end sequence" character.<br/>    target_text = '\t' + target_text + '\n'<br/>    input_texts.append(input_text)<br/>    target_texts.append(target_text)<br/>    for char in input_text:<br/>        if char not in input_characters:<br/>            input_characters.add(char)<br/>    for char in target_text:<br/>        if char not in target_characters:<br/>            target_characters.add(char)<br/><br/>input_characters = sorted(list(input_characters))<br/>target_characters = sorted(list(target_characters))<br/>num_encoder_tokens = len(input_characters)<br/>num_decoder_tokens = len(target_characters)<br/>max_encoder_seq_length = max([len(txt) for txt in input_texts])<br/>max_decoder_seq_length = max([len(txt) for txt in target_texts])<br/><br/>print('Number of samples:', len(input_texts))<br/>print('Number of unique input tokens:', num_encoder_tokens)<br/>print('Number of unique output tokens:', num_decoder_tokens)<br/>print('Max sequence length for inputs:', max_encoder_seq_length)<br/>print('Max sequence length for outputs:', max_decoder_seq_length)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<ol start="2">
<li>This section of code inputs the training data and encodes it into the character sequences it uses to vectorize. Note how the <kbd>num_encoder_tokens</kbd> and <kbd>num_decoder_tokens</kbd> parameters being set here are dependent on the number of characters in each set and not the number of samples. Finally, the maximum length of the encoding and decoding sequences are set on the maximum length of the encoded characters in both.</li>
<li class="CDPAlignLeft CDPAlign">Next, we want to take a look at the vectorization of the input data. Vectorization of the data reduces the number of characters for each response match and is also the memory-intensive part, except, when we align this data, we want to keep the responses or targets to be one step ahead of the original input. This subtle difference allows our sequence-learning LSTM layers to predict the next patterns in the sequence. A diagram of how this works follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign" style="text-align: center;color: black;font-size: 1em"><img src="assets/24d3cbdd-04c1-443a-899b-083e3443bd54.png" style="width:37.25em;height:21.00em;"/></div>
<div class="packt_figref" style="text-align: center;color: black">Sequence-to-sequence model</div>
<ol start="4">
<li>In the diagram, we can see how the start of the text <strong>HELLO</strong> is being translated one step behind the response phrase <strong>SALUT</strong> (<em>hello</em> in French). Pay attention to how this works in the preceding code.</li>
</ol>
<ol start="5">
<li class="mce-root">We then build the layers that will map to our network model with the code as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px"># Define an input sequence and process it.<br/>encoder_inputs = Input(shape=(None, num_encoder_tokens))<br/>encoder = LSTM(latent_dim, return_state=True)<br/>encoder_outputs, state_h, state_c = encoder(encoder_inputs)<br/># We discard `encoder_outputs` and only keep the states.<br/>encoder_states = [state_h, state_c]<br/><br/># Set up the decoder, using `encoder_states` as initial state.<br/>decoder_inputs = Input(shape=(None, num_decoder_tokens))<br/># We set up our decoder to return full output sequences,<br/># and to return internal states as well. We don't use the<br/># return states in the training model, but we will use them in inference.<br/>decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)<br/>decoder_outputs, _, _ = decoder_lstm(decoder_inputs,<br/>                                     initial_state=encoder_states)<br/>decoder_dense = Dense(num_decoder_tokens, activation='softmax')<br/>decoder_outputs = decoder_dense(decoder_outputs)<br/><br/># Define the model that will turn<br/># `encoder_input_data` &amp; `decoder_input_data` into `decoder_target_data`<br/>model = Model([encoder_inputs, decoder_inputs], decoder_outputs)<br/><br/># Run training<br/>model.compile(optimizer='rmsprop', loss='categorical_crossentropy')<br/>model.fit([encoder_input_data, decoder_input_data], decoder_target_data,<br/>          batch_size=batch_size,<br/>          epochs=epochs,<br/>          validation_split=0.2)<br/># Save model<br/>model.save('s2s.h5')</pre>
<ol start="6">
<li>Note how we are creating encoder and decoder inputs along with decoder outputs. This code builds and trains the <kbd>model</kbd> and then saves it for later use in inference. We use the term <em>inference</em> to mean that a model is inferring or generating an answer or response to some input. A diagram of this sequence-to-sequence model in layer architecture follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/12795ffb-7a9e-46ca-a40a-668fcc1467e3.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">Encoder/decoder inference model</div>
<ol start="7">
<li>This model is quite complex and there is a lot going on here. We have just covered the first part of the model. Next, we need to cover the building of the thought vector and generating the sampling models. The final code to do this follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">encoder_model = Model(encoder_inputs, encoder_states)<br/><br/>decoder_state_input_h = Input(shape=(latent_dim,))<br/>decoder_state_input_c = Input(shape=(latent_dim,))<br/>decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]<br/>decoder_outputs, state_h, state_c = decoder_lstm(<br/>    decoder_inputs, initial_state=decoder_states_inputs)<br/>decoder_states = [state_h, state_c]<br/>decoder_outputs = decoder_dense(decoder_outputs)<br/>decoder_model = Model(<br/>    [decoder_inputs] + decoder_states_inputs,<br/>    [decoder_outputs] + decoder_states)<br/><br/># Reverse-lookup token index to decode sequences back to<br/># something readable.<br/>reverse_input_char_index = dict(<br/>    (i, char) for char, i in input_token_index.items())<br/>reverse_target_char_index = dict(<br/>    (i, char) for char, i in target_token_index.items())</pre>
<p class="mce-root"/>
<p>Look over this code and see whether you can understand the structure. We are still missing a critical piece of the puzzle and we will cover that in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thought vectors</h1>
                </header>
            
            <article>
                
<p>At the middle of the encoding and decoding text process is the generation of a thought vector. The <strong>thought vector</strong>, popularized by the godfather himself, Dr. Geoffrey Hinton, represents a vector that shows the context of one element in relation to many other elements.</p>
<p>For instance, the word <em>hello </em>could have a high relational context to many words or phrases, such as <em>hi</em>, <em>how are you?</em>, <em>hey</em>, <em>goodbye</em>, and so on. Likewise, words such as<em> red, blue, fire,</em> and <em>old</em> would have a low context when associated with the word <em>hello</em>, <em>at</em> least in regular day-to-day speech. The word or character contexts are based on the pairings we have in the machine translation file. In this example, we are using the French translation pairings, but the pairings could be anything.</p>
<p>This process takes place as part of the first encoding model into the thought vector or, in this case, a vector of probabilities. The LSTM layer calculates the probability or context of how the words/characters are related. You will often come across the following equation, which describes this transformation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bdbd394e-5e98-4223-9edf-54a2cce63f6d.png" style="width:28.00em;height:4.33em;"/></p>
<p>Consider the following:</p>
<ul>
<li><img class="fm-editor-equation" src="assets/25c0b6a5-acb5-430e-99f6-5fd59f56313c.png" style="font-size: 1em;color: #333333;width:5.17em;height:1.00em;"/><span>= output sequence</span></li>
<li><img class="fm-editor-equation" src="assets/3d1c7405-9215-42e5-90fc-ae7c618d0549.png" style="font-size: 1em;color: #333333;width:6.67em;height:1.17em;"/><span> = input sequence</span></li>
<li><img class="fm-editor-equation" src="assets/c3d20d3e-814a-4d58-baa8-d853a83e2572.png" style="font-size: 1em;color: #333333;width:0.75em;height:1.00em;"/><span>= Vector representation</span></li>
</ul>
<p>The <img class="fm-editor-equation" src="assets/63dfa677-e45e-441c-a9ea-28c0a85f887b.png" style="width:0.92em;height:1.17em;"/> represents the multiplication form of sigma (<img class="fm-editor-equation" src="assets/b107c7b6-9f98-40a1-b8c9-ebe14b24cffd.png" style="width:0.92em;height:1.00em;"/>) and is used to pool the probabilities into the thought vector. This is a big simplification of the whole process, and the interested reader is encouraged to Google more about sequence-to-sequence learning on their own. For our purposes, the critical thing to remember is that each word/character has a probability or context that relates it to another. Generating this thought vector can be time consuming and memory-intensive, as you may have already noticed. Therefore, for our purposes, we will look at a more comprehensive set of natural language tools in order to create a neural conversational bot in the next section.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DeepPavlov</h1>
                </header>
            
            <article>
                
<p><strong>DeepPavlov</strong> is a comprehensive open source framework for building chatbots and other conversational agents for a variety of purposes and tasks. While this bot is designed for more goal-oriented bots, it will suit us well, as it is full-featured and includes several sequence-to-sequence model variations. Let's take a look at how to build a simple pattern (sequence-to-sequence) recognition model in the following steps:</p>
<ol>
<li class="mce-root">Up until now, we have kept our Python environment loose, but that has to change. We now want to isolate our development environment so that we can easily replicate it to other systems later. The best way to do this is working with Python virtual environments. Create a new environment and then activate it with the following commands at an Anaconda window:</li>
</ol>
<pre style="color: black;padding-left: 60px">#Anaconda virtual environment<br/><strong>conda create --name dlgames</strong><br/>#when prompted choose yes<br/><strong>activate dlgames</strong></pre>
<ol start="2">
<li>If you don't use Anaconda, the process is a bit more involved, as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">#Python virtual environment<br/><strong>pip install virtualenv</strong><br/><strong>virutalenv dlgames</strong><br/><br/>#on Mac<br/>source dlgames/bin/activate<br/><br/>#on Windows<br/>dlgames\Scripts\activate</pre>
<ol start="3">
<li>Then we need to install DeepPavlov with the following command at a shell or an Anaconda window:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>pip install deeppavlov</strong></pre>
<ol start="4">
<li> This framework will attempt to install several libraries and may disrupt any existing Python environments. This is the other reason we are now using virtual environments.</li>
</ol>
<ol start="5">
<li class="mce-root">For our purposes, we are just going to look at the basic <kbd>Hello World</kbd> sample that is very simple to follow now that we have covered the background. We first do our imports as per standard as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">from deeppavlov.skills.pattern_matching_skill import PatternMatchingSkill<br/>from deeppavlov.agents.default_agent.default_agent import DefaultAgent <br/>from deeppavlov.agents.processors.highest_confidence_selector import HighestConfidenceSelector</pre>
<ol start="6">
<li>Now, DeepPavlov is based on Keras, but as you can see, the types we are using here wrap the functionality of a sequence-to-sequence pattern-matching model. The <kbd>PatternMatchingSkill</kbd> represents the sequence-to-sequence model we want to give our chatbot agent. Next, we import the <kbd>DefaultAgent</kbd> type, which is just the basic agent. After that, we introduce a confidence selector called <kbd>HighestConfidenceSelector</kbd>. Remember that the thought vector we generate is a vector of probabilities. The <kbd>HighestConfidenceSelector</kbd> selector always chooses the highest value relation or context that matches the corresponding word. </li>
<li class="mce-root">Next, we generate three sets of patterns with corresponding responses, shown in the following code:</li>
</ol>
<pre style="color: black;padding-left: 60px">hello = PatternMatchingSkill(responses=['Hello world!'], patterns=["hi", "hello", "good day"])<br/>bye = PatternMatchingSkill(['Goodbye world!', 'See you around'], patterns=["bye", "ciao", "see you"])<br/>fallback = PatternMatchingSkill(["I don't understand, sorry", 'I can say "Hello world!"'])</pre>
<ol start="8">
<li>Each <kbd>PatternMatchingSkill</kbd> represents a set of pattern/response-contextual pairs. Note how there may be multiple responses and patterns for each. The other great thing about this framework is the ability to interchange and add skills. In this case, we are using just pattern matching, but there are plenty of other skills the reader is encouraged to explore.</li>
<li class="mce-root">Finally, we build the agent and run it by simply printing the results with the final bit of code:</li>
</ol>
<pre style="color: black;padding-left: 60px">HelloBot = DefaultAgent([hello, bye, fallback], skills_selector=HighestConfidenceSelector())<br/><br/>print(HelloBot(['Hello!', 'Boo...', 'Bye.']))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="10">
<li>This last section of code creates a <kbd>DefaultAgent</kbd> with the three skills (<kbd>hello</kbd>, <kbd>bye</kbd>, and <kbd>fallback</kbd>) using the <kbd>HighestConfidenceSelector</kbd>. Then it runs the agent by feeding a set of three inputs nested inside the <kbd>print</kbd> statement.</li>
<li>Run the code as you normally would and look at the output. Is it what you expected?</li>
</ol>
<p>The simplicity of DeepPavlov makes it an excellent tool to build up various conversational chatbots for your games or other purposes if you so choose. The framework itself is very broad-featured and provides multiple natural language processing tools for a variety of tasks, including goal-oriented chatbots. Whole books could and probably should be written about Pavlov; if you have an interest in this, look more for NLP and DeepPavlov.</p>
<p>With our new tool in hand, we now need a platform in which to serve up our bots with great conversational abilities. In the next section, we explore how to build a server for our bot.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the chatbot server</h1>
                </header>
            
            <article>
                
<p>Python is a great framework and it provides a number of great tools for game development. However, we are going to focus on using Unity for our purposes. Unity is an excellent and very user-friendly game engine that will make setting up complex examples in later chapters a breeze. Don't worry if you don't know C#, the language of Unity, since we will be manipulating the engine through Python in many cases. This means we want the ability to run our Python code outside Unity and we want to do it on a server.</p>
<p>If you are developing your game in Python, using a server then becomes optional, except that there are very compelling reasons to set up your AI bots as services or microservices. Microservices are self-contained succinct applications or services that only interface through some form of well-known communication protocol. <strong>AI Microservices</strong> or <strong>AI as a Service</strong> (<strong>AIaaS</strong>) are quickly outpacing other forms of SaaS, and it will only be a matter of time untill this same business model converts to gaming as well. In any case, for now, the benefit we gain from creating our chatbot as a microservice is <strong>decoupling</strong>. Decoupling will allow you to easily convert this bot to other platforms in the future.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Microservices also introduce a new communication pattern into the mix. Typically, when a client app connects to a server, the communication is direct and immediate. But what if your connection is broken or the communication needs to be filtered, duplicated, or stored for later analysis or reuse? Then using a direct communication protocol becomes burdened by adding these additional functions, when it doesn't need to be. Instead, microservices introduce the concept of a <strong>message hub</strong>. This is essentially a container or post office where all the message traffic passes through. This allows for incredible flexibility and offlines the need for our communication protocol to manage extra tasks. We will take a look at how to install a very easy-to-use message hub in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Message hubs (RabbitMQ)</h1>
                </header>
            
            <article>
                
<p>If you have never come across the concept of microservices or message hubs before, you may be somewhat daunted by what is coming next. Don't be. Message hubs and microservices are designed to make it easier to connect, route, and troubleshoot issues with multiple services that need to talk to one another. As such, these systems are designed to be easy to set up, and easier to use. Let's see how easy it is to set up an excellent message queue platform called RabbitMQ in the next exercise:</p>
<ol>
<li>Navigate your browser over to <a href="https://www.rabbitmq.com/#getstarted">https://www.rabbitmq.com/#getstarted</a>.</li>
<li>Download and install <span class="packt_screen">RabbitMQ</span> for your platform. There is typically a download button near the top of the page. You may be prompted to install <span class="packt_screen">Erlang</span>, as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fd935c3c-eba2-4cd0-bb68-5ba1c950a390.png" style="width:31.67em;height:12.83em;"/></div>
<div class="packt_figref" style="text-align: center;color: black">Erlang warning dialog</div>
<div class="packt_figref" style="text-align: center;color: black"/>
<ol start="3">
<li><span class="packt_screen">Erlang</span> is a concurrent functional programming language and perfect for writing messaging hubs. If you don't have it on your system, just download and install it, again for your platform; next, restart the <span class="packt_screen">RabbitMQ</span> installation.</li>
</ol>
<ol start="4">
<li>For the most part, follow the installation choosing the defaults, except for the installation path. Make sure to keep the installation path short and memorable, as we will want to find it later. An example of setting the path in the installer for Windows as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a76e19d5-3010-4f8f-bb1f-adb38f4e34f6.png" style="width:37.92em;height:29.50em;"/><br/>
<br/>
Example of setting the installation path on Windows</div>
<p class="mce-root"/>
<ol start="5">
<li><span class="packt_screen">RabbitMQ </span>will install itself as a service on your platform. Depending on your system, you may get a number of security prompts requesting firewall or admin access. Just allow all these exceptions, as the hub needs full access. When the installation completes, RabbitMQ should be running on your system. Be sure to check the documentation for your platform if you have any concerns on the configuration or setup. RabbitMQ is designed to use secure communication but keeps itself fairly open for development. Please avoid installing the hub in a production system, and expect to do some security configuration.</li>
</ol>
<ol start="6">
<li class="mce-root">Next, we want to activate the RabbitMQ management tool so that we can get a good overview of how the hub works. Open up a Command Prompt and navigate to the <kbd>RabbitMQ</kbd> installation server folder (the one marked server). Then navigate to the <kbd>sbin</kbd> folder. When you are there, run the following command to install the management plugin (Windows or macOS):</li>
</ol>
<pre class="sourcecode bash hljs" style="color: black;padding-left: 60px"><strong>rabbitmq-plugins <span class="hljs-built_in">enable</span> rabbitmq_management</strong></pre>
<ol start="7">
<li>An example of how this looks in a Windows Command Prompt follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/1293acf7-062f-45f3-8703-9d4fdb597da8.png"/><br/>
<br/>
Installing the RabbitMQ management plugin</div>
<p>That completes the installation of the hub on your system. In the next section, we will see how to inspect the hub with the management interface.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing RabbitMQ</h1>
                </header>
            
            <article>
                
<p>RabbitMQ is a full-featured message hub that is very powerful and flexible in what it can do. There is a lot to RabbitMQ and it may be intimidating to some users less familiar with networking. Fortunately, we only need to use a few pieces right now, and in the future we will explore more functionality.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For now, though, open up a browser and follow along these steps to explore the hub's management interface:</p>
<ol>
<li class="CDPAlignLeft CDPAlign">Navigate your browser to <kbd>http://localhost:15672/</kbd> and you should see a login dialog.</li>
<li>Enter the <span class="packt_screen">username</span> as <kbd>guest</kbd> and the <span class="packt_screen">password</span> as <kbd>guest</kbd>. These are the default credentials and should work unless you've configured it otherwise. </li>
<li class="mce-root">After you log in, you will see the RabbitMQ interface:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black"><img src="assets/143641f0-db67-4da2-bb79-7fd03f000024.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">RabbitMQ management interface</div>
<ol start="4">
<li>There is a lot going on here, so for now just click around and explore the various options. Avoid changing any settings, at least for now and until requested to do so. RabbitMQ is very powerful, but we all know that with great power comes great responsibility.</li>
</ol>
<p>Now, currently, your message queue is empty, so you won't see a lot of activity, but we will soon resolve that in the next section, where we learn how to send and receive messages to and from the queue.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sending and receiving to/from the MQ</h1>
                </header>
            
            <article>
                
<p>RabbitMQ uses a protocol called <strong>Advanced Message Queuing Protocol</strong> (<strong>AMQP</strong>)<strong> </strong>for communication, which is a standard for all messaging middleware. This means that we can effectively swap out RabbitMQ for a more robust system, such as Kafka, in the future. This also means that, for the most part, all of the concepts we cover here will likely apply to similar messaging systems.</p>
<p>The first thing we will do is put a message on the queue from a very simple Python client. Open up the source file <kbd>Chapter_4_3.py</kbd> and follow these steps:</p>
<ol>
<li class="mce-root">Open the source code file and take a look:</li>
</ol>
<pre class="sourcecode python hljs" style="color: black;padding-left: 60px"><span class="hljs-keyword">import</span> pika

connection = pika.BlockingConnection(pika.ConnectionParameters(host=<span class="hljs-string">'localhost'</span>))
channel = connection.channel()<br/>channel.queue_declare(<strong>queue=<span class="hljs-string">'hello'</span></strong>)
channel.basic_publish(exchange=<span class="hljs-string">''</span>,
                      <strong>routing_key=<span class="hljs-string">'hello'</span></strong>,
                      body=<span class="hljs-string">'Hello World!'</span>)
print(<span class="hljs-string">" [x] Sent 'Hello World!'"</span>)
connection.close()</pre>
<ol start="2">
<li>The code is taken from the RabbitMQ reference tutorial and shows how to connect. It first connects to the hub and opens a <kbd>queue</kbd> called <kbd>hello</kbd>. A queue is like a mailbox or stack of messages. A hub may have several different queues. Then the code publishes a message to the <kbd>hello</kbd> queue with the body of <kbd>Hello World!</kbd>.</li>
<li class="mce-root">Before we can run the sample, we first need to install <kbd>Pika</kbd>. Pika is an AMQP connection library and can be installed with the following command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>pip install pika</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Then run the code file as you normally would and watch the output. It's not very exciting, is it?</li>
<li>Go to the RabbitMQ management interface again at <kbd>http://localhost:15672/</kbd> and see that we now have a single message in the hub, as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d9bffaeb-5bcf-47ac-bedf-9795061ae826.png" style="font-size: 1em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">RabbitMQ interface showing the addition of a message</div>
<ol start="6">
<li>The message we just sent will stay on the hub until we collect it later. This single feature will allow us to run individual services and make sure they are communicating correctly without having to worry about other consumers or publishers.</li>
</ol>
<p>For the purposes of RabbitMQ, we just wrote a publisher. In some cases, you many want a service or app to just publish messages, while in others you may want them to consume them. In the next exercise, <kbd>Chapter_4_4_py</kbd>, we will write a hub consumer or client:</p>
<ol>
<li class="mce-root">Open the source file <kbd>Chapter_4_4.py</kbd> and look at the code:</li>
</ol>
<pre class="sourcecode python hljs" style="color: black;padding-left: 60px"><span class="hljs-keyword">import</span> pika

connection = pika.BlockingConnection(pika.ConnectionParameters(host=<span class="hljs-string">'localhost'</span>))
channel = connection.channel()

channel.queue_declare(queue=<span class="hljs-string">'hello'</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">callback</span><span class="hljs-params">(ch, method, properties, body)</span>:</span>
    print(<span class="hljs-string">" [x] Received %r"</span> % body)

channel.basic_consume(callback,
                      queue=<span class="hljs-string">'hello'</span>,
                      no_ack=<span class="hljs-keyword">True</span>)

print(<span class="hljs-string">' [*] Waiting for messages. To exit press CTRL+C'</span>)
channel.start_consuming()</pre>
<ol start="2">
<li>The preceding code is almost identical to the previous example, except that this time it only consumes from the queue using an internal <kbd>callback</kbd> function to receive the response. In this example, also note how the script blocks itself and waits for the message. In most cases, the client will register a callback with the queue in order to register an event. That event is triggered when a new message enters the particular queue.</li>
<li>Run the code as you normally would and watch the first <kbd>Hello World</kbd> message get pulled from the queue and output on the client window.</li>
<li>Keep the client running and run another instance of the <kbd>Chapter_4_3.py</kbd> (publish) script and note how the client quickly consumes it and outputs it to the window.</li>
</ol>
<p>This completes the simple send and receive communication to/from the message hub. As you can see, the code is fairly straightforward and the configuration works out of the box, for the most part. If you do experience any issues with this setup, be sure to consult the RabbitMQ tutorials, which are an additional excellent resource for extra help. In the next section, we look at how to build the working chatbot server example.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing the message queue chatbot</h1>
                </header>
            
            <article>
                
<p>The chatbot server we want to create is essentially a combination of the three previous examples. Open up <kbd>Chapter_4_5.py</kbd> and follow the next exercise:</p>
<ol>
<li class="mce-root">The complete server code as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">import pika<br/>from deeppavlov.skills.pattern_matching_skill import PatternMatchingSkill<br/>from deeppavlov.agents.default_agent.default_agent import DefaultAgent <br/>from deeppavlov.agents.processors.highest_confidence_selector import HighestConfidenceSelector<br/><br/>hello = PatternMatchingSkill(responses=['Hello world!'], patterns=["hi", "hello", "good day"])<br/>bye = PatternMatchingSkill(['Goodbye world!', 'See you around'], patterns=["bye", "chao", "see you"])<br/>fallback = PatternMatchingSkill(["I don't understand, sorry", 'I can say "Hello world!"'])<br/><br/>HelloBot = DefaultAgent([hello, bye, fallback], skills_selector=HighestConfidenceSelector())<br/><br/>connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))<br/>channelin = connection.channel()<br/>channelin.exchange_declare(exchange='chat', exchange_type='direct', durable=True)<br/>channelin.queue_bind(exchange='chat', queue='chatin')<br/><br/>channelout = connection.channel()<br/>channelout.exchange_declare(exchange='chat', durable=True)<br/><br/>def callback(ch, method, properties, body):<br/>    global HelloBot, channelout<br/>    response = HelloBot([str(body)])[0].encode()<br/>    print(body,response)<br/>    channelout.basic_publish(exchange='chat',<br/>                      routing_key='chatout',<br/>                      body=response)<br/>    print(" [x] Sent response %r" % response)<br/><br/>channelin.basic_consume(callback, <br/>                      queue='chatin',<br/>                      no_ack=True)<br/><br/>print(' [*] Waiting for messages. To exit press CTRL+C')<br/>channelin.start_consuming()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<ol start="2">
<li>We essentially have a complete working <kbd>Hello World</kbd> chatbot server in fewer than 25 lines of code. Of course, the functionality is still limited, but by now you can certainly understand how to add other pattern-matching skills to the bot.<br/>
The important thing to note here is that we are consuming from a queue called <kbd>chatin</kbd> and publishing to a queue called <kbd>chatout</kbd>. These queues are now wrapped in an exchange called <kbd>chat</kbd>. You can think of an exchange as a routing service. Exchanges provide for additional functionality around queues, and the great thing is that they are optional. For use, though, we want to use exchanges, because they provide us with better global control of our services. There are four types of exchanges used in RabbitMQ and they are summarized here:
<ul>
<li><strong>Direct</strong>: Messages are sent directly to the queue marked in the message transmission.</li>
<li><strong>Fanout</strong>: Duplicate the message to all queues wrapped by the exchange. This is great when you want to add logging or historical archiving.</li>
<li><strong>Topic</strong>: This allows you to send messages to queues identified by matching the message queue. For instance, you could send a message to the queue <kbd>chat</kbd> and any queue wrapped in the same exchange containing the word <em>chat</em> receives the message. The topic exchange allows you to group like messages.</li>
<li><strong>Headers</strong>: This works similar to the topic exchange but instead filters based on the headers in the message itself. This is a great exchange to use for dynamic routing of messages with the appropriate headers.</li>
</ul>
</li>
<li>Run the <kbd>Chapter_4_5.py</kbd> server example and keep it running.</li>
<li class="mce-root">Next, open the <kbd>Chapter_4_6.py</kbd> file and look at the code shown:</li>
</ol>
<pre style="color: black;padding-left: 60px">import pika<br/><br/>connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))<br/>channelin = connection.channel()<br/><br/>channelin.exchange_declare(exchange='chat')<br/><br/>chat = 'boo'<br/><br/>channelin.basic_publish(exchange='chat',<br/>                      routing_key='chatin',<br/>                      body=chat)<br/>print(" [x] Sent '{0}'".format(chat))<br/>connection.close()</pre>
<ol start="5">
<li>The preceding code is just a sample client we can use to test the chatbot server. Note how the message variable <kbd>chat</kbd> is set to <kbd>'boo'</kbd>. When you run the code, check the output window of the chatbot server; this is the <kbd>Chapter_4_5.py</kbd> file we ran earlier. You should see a response message logged in the window that is appropriate to the chat message we just sent.</li>
</ol>
<p>At this point, you could write a full chat client that could communicate with our chatbot in Python. However, we want to connect our bot up to Unity and see how we can use our bot as a microservice in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the chatbot in Unity</h1>
                </header>
            
            <article>
                
<p><strong>Unity</strong> is quickly becoming the standard game engine for learning to develop games, virtual reality, and augmented reality applications. Now it is quickly becoming the standard platform for developing AI and ML applications as well, partly due to the excellent reinforcement learning platform the team at Unity has built. This Unity ML platform is a key component in our desire to use the tool, since it currently is at the cutting edge of advanced AI for games.</p>
<div class="packt_infobox">The AI team at Unity, led by Dr. Danny Lange and their senior developer Dr. Arthur Juliani, have made numerous suggestions and contributions to ideas for content in this book, both directly and indirectly. This, of course, has had a huge impact on using Unity for major portions of this book.</div>
<p>Installing Unity is quite straightforward, but we want to make sure we get the installation just right the first time. Therefore, follow these steps to install a version of Unity on your system:</p>
<ol>
<li>Navigate your browser to <a href="https://store.unity.com/download">https://store.unity.com/download</a> and accept the terms, and then download the Unity Download Assistant. This is the tool that downloads and installs the pieces we need.</li>
</ol>
<ol start="2">
<li>Run the <span class="packt_screen">Download Assistant</span> and select the following minimum components to install, as shown in the dialog as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a18fa234-6ec2-42f3-98e2-702f88ccff5a.png" style="width:36.25em;height:28.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">Selecting the installation components for Unity</div>
<ol start="3">
<li>Just be sure to install the latest version of Unity and select the components that match your preferred OS, as shown in the preceding screenshot. You may, of course, select other components at your discretion, but those are the minimum you will need for this book.</li>
<li>Next, set the path to install Unity to a well-known folder. A good choice is to set the folder name equal to the version. This allows you to have multiple versions of Unity on the same system that you can easily find. The following screenshot shows how you may do this on Windows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/56789132-7359-4d00-90b2-3fd856a3199b.png" style="width:37.83em;height:29.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">Setting the installation path to Unity</div>
<ol start="5">
<li>Those are the only critical parts to the installation and you can continue installing the software using the defaults.</li>
<li>Launch the Unity editor after it installs and you will be prompted to log in. Unity requires you to have an account, regardless of whether you are using the free version. Go back to <a href="http://unity.com">unity.com</a> and just create an account. After you are done setting up the account, go back in and log in to the editor.</li>
<li>After you log in, create a empty project called <kbd>Chatbot</kbd> and let the editor open to a blank scene. </li>
</ol>
<p class="mce-root"/>
<p>Unity is a full-featured game engine and may be intimidating if this is your first visit. There are plenty of online tutorials and videos that can get you up to speed on the interface. We will do our best to demonstrate concepts simply, but if you get lost, just take your time and work through the exercise a few times.</p>
<p>With Unity installed, we now have to install the components or assets that will allow us to easily connect to the chatbot server we just created. In the next section, we install the AMQP asset for Unity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing AMQP for Unity</h1>
                </header>
            
            <article>
                
<p>RabbitMQ has an excellent resource for plenty of cross-platform libraries that allow you to connect to the hub with ease. The library for C# does work well outside Unity but is problematic to set up. Fortunately, the good folks at Cymantic Labs have built and open sourced a version for Unity on GitHub. Let's see how to install this code in the next exercise:</p>
<ol>
<li class="mce-root">Download and unpack the code using <kbd>git</kbd> or as a ZIP file from <a href="https://github.com/CymaticLabs/Unity3D.Amqp.git">https://github.com/CymaticLabs/Unity3D.Amqp</a>:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>git clone https://github.com/CymaticLabs/Unity3D.Amqp.git</strong></pre>
<ol start="2">
<li>Switch to Unity from the menu, and select <span class="packt_screen">File</span> | <span class="packt_screen">Open Project</span> and navigate to the <kbd>Unity3D.Amqp\unity\CymaticLabs.UnityAmqp</kbd> folder where you installed the code. This will open the asset in its own project. Wait for the project to load.</li>
<li>Open the <kbd>Assets/CymanticLabs/Amqp/Scenes</kbd> folder in the <span class="packt_screen">Project</span> window (typically at the bottom).</li>
<li>Double-click on the <strong><span class="packt_screen">AmqpDemo</span></strong> scene to open it in the editor.</li>
<li>Press the <span class="packt_screen">Play</span> button at the top of the editor to run the scene. After you run the scene, you should see the following:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/0f86dbe5-62dd-4302-8d5d-6fbce169709a.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Setting the Amqp connection and sending a message</span></div>
<ol start="6">
<li>Press the <span class="packt_screen">Connect</span> button to connect to the local RabbitMQ.</li>
<li>Next, under <span class="packt_screen">Subscriptions</span>, set the exchange to <span class="packt_screen">chat</span>, and the queue to <span class="packt_screen">chatout</span>, and click <span class="packt_screen">Subscribe</span>. This will subscribe to the queue so we can see any return message in the Unity console window.</li>
<li>Finally, under <span class="packt_screen">Publish</span>, set the exchange to <span class="packt_screen">chat</span>, and the queue to <span class="packt_screen">chatin</span>, and type a message such as <kbd>hello</kbd>. Click the <span class="packt_screen">Send</span> button and you should see a response from the bot in the console window.</li>
</ol>
<p>That sets up our working chatbot. Of course, this is just the start of what is possible and the reader is certainly encouraged to explore further, but keep in mind we will revisit this code later and make use of it in a later section of the book.</p>
<p>That completes this chapter, and now you can take advantage of it for further learning in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>Use the following exercises to expand your learning and get more confident with the material in this chapter:</p>
<ol>
<li>Go back to the first exercise and load another set of translations. Train the bot on those and see what responses are generated after training. There are plenty of other language files available for training.</li>
<li>Set up your own conversational training file using the English/French translation one as an example. Remember, the matching responses can be anything and not just translated text.</li>
<li>Add additional pattern-matching skills to the DeepPavlov bot. Either the simple test one and/or the chatbot server.</li>
<li>The DeepPavlov chatbot uses a highest-value selection criteria for selecting a response. DeepPavlov does have a random selector as well. Change the response selector on the chatbot to use random.</li>
<li>Change the exchange type in the example to use <span class="packt_screen">Fanout</span> and create a log queue to log messages.</li>
<li>Change the exchange type to <span class="packt_screen">Topic</span> and see how you can group messages. Warning: this will likely break the example; see whether you can fix it.</li>
<li>Write a RabbitMQ publisher in Python that publishes to one or more different types of queues.</li>
<li>Create an entire set of conversation skills using the pattern-matching skill. Then, see how well your bot converses with you.</li>
<li>Add additional skills of other types to the chatbot server. This may require some additional homework on your part.</li>
<li>Write or run two chatbots over RabbitMQ and watch them converse with each other.</li>
</ol>
<p>Work through at least two or three of these exercises.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at building chatbots or neural conversational agents using neural networks and deep learning. We first saw what makes a chatbot and the main forms in use today: goal-oriented and conversational bots. Then we looked at how to build a basic machine translation conversational chatbot that used sequence-to-sequence learning. </p>
<p>After getting a background in sequence learning, we looked at the open source tool DeepPavlov. DeepPavlov is a powerful chat platform built on top of Keras and designed for many forms of neural agent conversation and tasks. This made it ideal for us to use the chatbot server as a base. Then we installed RabbitMQ, a microservices message hub platform that will allow our bot and all manner of other services to talk together later on.</p>
<p>Finally, we installed Unity and then quickly installed the AMQP plugin asset and connected to our chatbot server.</p>
<p>This completes our introductory section to deep learning, and, in the next section, we begin to get more into game AI by diving into <strong>deep reinforcement learning</strong>.</p>


            </article>

            
        </section>
    </body></html>