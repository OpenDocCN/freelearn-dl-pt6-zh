["```py\n# Libraries\nlibrary(keras)\nlibrary(EBImage)\n```", "```py\n# Read data\nsetwd(\"~/Desktop/image20\")\ntemp = list.files(pattern = \"*.jpg\")\nmypic <- list()\nfor (i in 1:length(temp))  {mypic[[i]] <- readImage(temp[[i]])}\npar(mfrow = c(5,4))\nfor (i in 1:length(temp)) plot(mypic[[i]])\n```", "```py\n# MNIST data\nmnist <- dataset_fashion_mnist()\nstr(mnist)\n\nOUTPUT\nList of 2\n $ train:List of 2\n  ..$ x: int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ y: int [1:60000(1d)] 9 0 0 3 0 2 7 2 5 5 ...\n $ test :List of 2\n  ..$ x: int [1:10000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ y: int [1:10000(1d)] 9 2 1 1 6 1 4 6 5 7 ...\n```", "```py\n#train and test data\ntrainx <- mnist$train$x\ntrainy <- mnist$train$y\ntestx <- mnist$test$x\ntesty <- mnist$test$y\ntable(mnist$train$y, mnist$train$y)\n\n       0    1    2    3    4    5    6    7    8    9\n  0 6000    0    0    0    0    0    0    0    0    0\n  1    0 6000    0    0    0    0    0    0    0    0\n  2    0    0 6000    0    0    0    0    0    0    0\n  3    0    0    0 6000    0    0    0    0    0    0\n  4    0    0    0    0 6000    0    0    0    0    0\n  5    0    0    0    0    0 6000    0    0    0    0\n  6    0    0    0    0    0    0 6000    0    0    0\n  7    0    0    0    0    0    0    0 6000    0    0\n  8    0    0    0    0    0    0    0    0 6000    0\n  9    0    0    0    0    0    0    0    0    0 6000\n\ntable(mnist$test$y,mnist$test$y)      \n       0    1    2    3    4    5    6    7    8    9\n  0 1000    0    0    0    0    0    0    0    0    0\n  1    0 1000    0    0    0    0    0    0    0    0\n  2    0    0 1000    0    0    0    0    0    0    0\n  3    0    0    0 1000    0    0    0    0    0    0\n  4    0    0    0    0 1000    0    0    0    0    0\n  5    0    0    0    0    0 1000    0    0    0    0\n  6    0    0    0    0    0    0 1000    0    0    0\n  7    0    0    0    0    0    0    0 1000    0    0\n  8    0    0    0    0    0    0    0    0 1000    0\n  9    0    0    0    0    0    0    0    0    0 1000\n```", "```py\n# Display images\npar(mfrow = c(8,8), mar = rep(0, 4))\nfor (i in 1:84) plot(as.raster(trainx[i,,], max = 255))\npar(mfrow = c(1,1))\n```", "```py\n# Reshape and resize\ntrainx <- array_reshape(trainx, c(nrow(trainx), 784))\ntestx <- array_reshape(testx, c(nrow(testx), 784))\ntrainx <- trainx / 255\ntestx <- testx / 255\nstr(trainx)\n\nOUTPUT\n\nnum [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...\n\n```", "```py\n# One-hot encoding\ntrainy <- to_categorical(trainy, 10)\ntesty <- to_categorical(testy, 10)\nhead(trainy)\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    0    0    0    0    0    0    0    0    0     1\n[2,]    1    0    0    0    0    0    0    0    0     0\n[3,]    1    0    0    0    0    0    0    0    0     0\n[4,]    0    0    0    1    0    0    0    0    0     0\n[5,]    1    0    0    0    0    0    0    0    0     0\n[6,]    0    0    1    0    0    0    0    0    0     0\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential() \nmodel %>% \n         layer_conv_2d(filters = 32, \n                        kernel_size = c(3,3), \n                        activation = 'relu', \n                        input_shape = c(28,28,1)) %>%   \n         layer_conv_2d(filters = 64, \n                        kernel_size = c(3,3), \n                        activation = 'relu') %>%  \n         layer_max_pooling_2d(pool_size = c(2,2)) %>% \n         layer_dropout(rate = 0.25) %>%   \n         layer_flatten() %>% \n         layer_dense(units = 64, activation = 'relu') %>%  \n         layer_dropout(rate = 0.25) %>% \n         layer_dense(units = 10, activation = 'softmax')\n```", "```py\n# Model summary\nsummary(model)\n__________________________________________________________________\nLayer (type                   Output Shape             Param #        \n==================================================================\nconv2d_1 (Conv2D)          (None, 26, 26, 32)            320            \n__________________________________________________________________\nconv2d_2 (Conv2D)          (None, 24, 24, 64)            18496          \n__________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D) (None, 12, 12, 64)         0              \n__________________________________________________________________\ndropout_1 (Dropout)        (None, 12, 12, 64)             0              \n__________________________________________________________________\nflatten_1 (Flatten)        (None, 9216)                   0              \n__________________________________________________________________\ndense_1 (Dense)            (None, 64)                    589888         \n__________________________________________________________________\ndropout_2 (Dropout)        (None, 64)                     0              \n__________________________________________________________________\ndense_2 (Dense)            (None, 10)                     650            \n==================================================================\nTotal params: 609,354\nTrainable params: 609,354\nNon-trainable params: 0\n___________________________________________________________________\n```", "```py\n# Compile model\nmodel %>% compile(loss = 'categorical_crossentropy',\n                  optimizer = optimizer_adadelta(),\n                  metrics = 'accuracy')\n```", "```py\n# Fit model\nmodel_one <- model %>% fit(trainx, \n                         trainy, \n                         epochs = 15, \n                         batch_size = 128, \n                         validation_split = 0.2)\nplot(model_one)\n```", "```py\n# Model evaluation\nmodel %>% evaluate(trainx, trainy)\n\n$loss  0.1151372\n$acc  0.9603167\n```", "```py\n# Prediction and confusion matrix\npred <- model %>%   predict_classes(trainx)\ntable(Predicted=pred, Actual=mnist$train$y)\n\nOUTPUT\n\n Actual\nPredicted    0    1    2    3    4    5    6    7    8    9\n        0 5655    1   53   48    1    0  359    0    2    0\n        1    1 5969    2    8    1    0    3    0    0    0\n        2   50    0 5642   23  219    0  197    0    2    0\n        3   42   23   20 5745   50    0   50    0    3    0\n        4    7    1  156  106 5566    0  122    0    4    0\n        5    0    0    0    0    0 5971    0    6    1   12\n        6  230    3  121   68  159    0 5263    0   11    0\n        7    0    0    0    0    0   22    0 5958    3  112\n        8   15    3    6    2    4    4    6    0 5974    0\n        9    0    0    0    0    0    3    0   36    0 5876\n```", "```py\n# Prediction probabilities\nprob <- model %>%   predict_proba(trainx) \nprob <- round(prob, 3)\ncbind(prob, Predicted_class = pred, Actual = mnist$train$y)[1:5,]\n\nOUTPUT\n                                                  Predicted_class Actual\n[1,] 0.000 0.000 0.000 0.000 0 0 0.000 0.001 0 0.999         9      9\n[2,] 1.000 0.000 0.000 0.000 0 0 0.000 0.000 0 0.000         0      0\n[3,] 0.969 0.000 0.005 0.003 0 0 0.023 0.000 0 0.000         0      0\n[4,] 0.023 0.000 0.000 0.968 0 0 0.009 0.000 0 0.000         3      3\n[5,] 0.656 0.001 0.000 0.007 0 0 0.336 0.000 0 0.000         0      0\n```", "```py\n# Model evaluation\nmodel %>% evaluate(testx, testy)\n\n$loss  0.240465\n$acc   0.9226\n```", "```py\n# Prediction and confusion matrix\npred <- model %>% predict_classes(testx)\ntable(Predicted=pred, Actual=mnist$test$y)\n\nOUTPUT\n         Actual\nPredicted   0   1   2   3   4   5   6   7   8   9\n        0 878   0  14  15   0   0  91   0   0   0\n        1   1 977   0   2   1   0   1   0   2   0\n        2  22   1 899   9  55   0  65   0   2   0\n        3  12  14   6 921  14   0  20   0   3   0\n        4   2   5  34  26 885   0  57   0   0   0\n        5   1   0   0   0   0 988   0   8   1   6\n        6  74   1  43  23  43   0 755   0   2   0\n        7   0   0   0   0   0   6   0 969   3  26\n        8  10   2   4   4   2   0  11   0 987   1\n        9   0   0   0   0   0   6   0  23   0 967\n```", "```py\n# Prediction probabilities\nprob <- model %>% predict_proba(testx) \nprob <- round(prob, 3)\ncbind(prob, Predicted_class = pred, Actual = mnist$test$y)[1:5,]\n\nOUTPUT\n Predicted_class Actual \n[1,] 0.000 0 0.000 0 0.000 0 0.000 0 0 1     9         9 \n[2,] 0.000 0 1.000 0 0.000 0 0.000 0 0 0     2         2 \n[3,] 0.000 1 0.000 0 0.000 0 0.000 0 0 0     1         1 \n[4,] 0.000 1 0.000 0 0.000 0 0.000 0 0 0     1         1 \n[5,] 0.003 0 0.001 0 0.004 0 0.992 0 0 0     6         6\n```", "```py\nsetwd(\"~/Desktop/image20\")\ntemp = list.files(pattern = \"*.jpg\")\nmypic <- list()\nfor (i in 1:length(temp))  {mypic[[i]] <- readImage(temp[[i]])}\nfor (i in 1:length(temp))  {mypic[[i]] <- channel(mypic[[i]], \"gray\")}\nfor (i in 1:length(temp)) {mypic[[i]] <- 1-mypic[[i]]}\nfor (i in 1:length(temp)) {mypic[[i]] <- resize(mypic[[i]], 28, 28)}\npar(mfrow = c(5,4), mar = rep(0, 4))\nfor (i in 1:length(temp)) plot(mypic[[i]])\n```", "```py\n# Reshape and row-bind\nfor (i in 1:length(temp)) {mypic[[i]] <- array_reshape(mypic[[i]], c(1,28,28,1))}\nnew <- NULL\nfor (i in 1:length(temp)) {new <- rbind(new, mypic[[i]])}\nstr(new)\n\nOUTPUT\n\nnum [1:20, 1:784] 0.0458 0.0131 0 0 0 ...\n```", "```py\n# Reshape\nnewx <- array_reshape(new, c(nrow(new),28,28,1))\nnewy <- c(0,4,5,5,6,6,7,7,8,8,9,0,9,1,1,2,2,3,3,4)\n```", "```py\n# Confusion matrix for 20 images\npred <- model %>%   predict_classes(newx)\ntable(Predicted=pred, Actual=newy)\n\nOUTPUT\n Actual\nPredicted 0 1 2 3 4 5 6 7 8 9\n        0 1 0 0 0 0 0 0 0 0 0\n        1 0 1 0 0 0 0 0 0 0 0\n        2 0 0 1 0 0 0 0 0 0 0\n        3 1 1 0 2 0 0 0 0 0 2\n        4 0 0 1 0 1 0 0 0 0 0\n        5 0 0 0 0 0 0 0 1 0 0\n        6 0 0 0 0 0 0 2 0 0 0\n        8 0 0 0 0 1 2 0 1 2 0\n```", "```py\n# Images with prediction probabilities, predicted class, and actual class \nsetwd(\"~/Desktop/image20\")\ntemp = list.files(pattern = \"*.jpg\")\nmypic <- list()\nfor (i in 1:length(temp))  {mypic[[i]] <- readImage(temp[[i]])}\nfor (i in 1:length(temp))  {mypic[[i]] <- channel(mypic[[i]], \"gray\")}\nfor (i in 1:length(temp)) {mypic[[i]] <- 1-mypic[[i]]}\nfor (i in 1:length(temp)) {mypic[[i]] <- resize(mypic[[i]], 28, 28)}\npredictions <-  predict_classes(model, newx)\nprobabilities <- predict_proba(model, newx)\nprobs <- round(probabilities, 2)\npar(mfrow = c(5, 4), mar = rep(0, 4))\nfor(i in 1:length(temp)) {plot(mypic[[i]])\n         legend(\"topleft\", legend = max(probs[i,]),  \n                bty = \"n\",text.col = \"white\",cex = 2)\n         legend(\"top\", legend = predictions[i],  \n                bty = \"n\",text.col = \"yellow\", cex = 2) \n         legend(\"topright\", legend = newy[i],  \n                bty = \"\",text.col = \"darkgreen\", cex = 2) }\n```", "```py\n# Images with prediction probabilities, predicted class, and actual class setwd(\"~/Desktop/image20\")\ntemp = list.files(pattern = \"*.jpg\")\nmypic <- list()\nfor (i in 1:length(temp)) {mypic[[i]] <- readImage(temp[[i]])}\nfor (i in 1:length(temp)) {mypic[[i]] <- flop(mypic[[i]])}\nfor (i in 1:length(temp)) {mypic[[i]] <- channel(mypic[[i]], \"gray\")}\nfor (i in 1:length(temp)) {mypic[[i]] <- 1-mypic[[i]]}\nfor (i in 1:length(temp)) {mypic[[i]] <- resize(mypic[[i]], 28, 28)}\npredictions <- predict_classes(model, newx)\nprobabilities <- predict_proba(model, newx)\nprobs <- round(probabilities, 2)\npar(mfrow = c(5, 4), mar = rep(0, 4))\nfor(i in 1:length(temp)) {plot(mypic[[i]])\n legend(\"topleft\", legend = max(probs[i,]), \n bty = \"\",text.col = \"black\",cex = 1.2)\n legend(\"top\", legend = predictions[i], \n bty = \"\",text.col = \"darkred\", cex = 1.2) \n legend(\"topright\", legend = newy[i], \n bty = \"\",text.col = \"darkgreen\", cex = 1.2) }\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential() \nmodel %>% \n         layer_conv_2d(filters = 32, kernel_size = c(3,3), \n                        activation = 'relu', input_shape = c(28,28,1)) %>%   \n         layer_conv_2d(filters = 32, kernel_size = c(3,3), \n                        activation = 'relu') %>%  \n         layer_max_pooling_2d(pool_size = c(2,2)) %>% \n         layer_dropout(rate = 0.25) %>%   \n         layer_conv_2d(filters = 64, kernel_size = c(3,3), \n                        activation = 'relu') %>% \n         layer_conv_2d(filters = 64, kernel_size = c(3,3), \n                        activation = 'relu') %>%  \n         layer_max_pooling_2d(pool_size = c(2,2)) %>% \n         layer_dropout(rate = 0.25) %>%   \n         layer_flatten() %>% \n         layer_dense(units = 512, activation = 'relu') %>%  \n         layer_dropout(rate = 0.5) %>% \n         layer_dense(units = 10, activation = 'softmax')\n\n# Compile model\nmodel %>% compile(loss = 'categorical_crossentropy',\n                  optimizer = optimizer_adadelta(),\n                  metrics = 'accuracy')\n\n# Fit model\nmodel_two <- model %>% fit(trainx, \n                         trainy, \n                         epochs = 15, \n                         batch_size = 128, \n                         validation_split = 0.2)\nplot(model_two)\n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(trainx, trainy)\n\n$loss 0.1587473\n$acc 0.94285\n```", "```py\n# Confusion matrix for training data\npred <- model %>%   predict_classes(trainx)\ntable(Predicted=pred, Actual=mnist$train$y)\n\nOUTPUT\n Actual\nPredicted    0    1    2    3    4    5    6    7    8    9\n        0 5499    0   58   63    3    0  456    0    4    0\n        1    2 5936    1    5    3    0    4    0    1    0\n        2   83    0 5669   13  258    0  438    0    7    0\n        3   69   52   48 5798  197    0  103    0    6    0\n        4    3    3  136   49 5348    0  265    0    5    0\n        5    0    0    0    0    0 5879    0    3    0    4\n        6  309    6   73   67  181    0 4700    0    2    0\n        7    0    0    0    0    0   75    0 5943    1  169\n        8   35    3   15    5   10    3   34    0 5974    2\n        9    0    0    0    0    0   43    0   54    0 5825\n```", "```py\n# Loss and accuracy for the test data\nmodel %>% evaluate(testx, testy)\n\n$loss 0.2233179\n$acc 0.9211\n\n# Confusion matrix for test data\npred <- model %>% predict_classes(testx)\ntable(Predicted=pred, Actual=mnist$test$y)\n\nOUTPUT\n Actual\nPredicted   0   1   2   3   4   5   6   7   8   9\n        0 875   1  18   8   0   0 104   0   3   0\n        1   0 979   0   2   0   0   0   0   0   0\n        2  19   0 926   9  50   0  78   0   1   0\n        3  10  14   9 936  35   0  19   0   3   0\n        4   2   0  30  12 869   0  66   0   0   0\n        5   0   0   0   0   0 971   0   2   1   2\n        6  78   3  16  29  45   0 720   0   1   0\n        7   0   0   0   0   0  18   0 988   1  39\n        8  16   3   1   4   1   0  13   0 989   1\n        9   0   0   0   0   0  11   0  10   1 958\n```"]