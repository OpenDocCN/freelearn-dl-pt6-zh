["```py\n# Libraries used\nlibrary(keras)\nlibrary(EBImage)\nlibrary(tensorflow)\n```", "```py\n# Pretrained model\npretrained <- application_resnet50(weights = \"imagenet\")\nsummary(pretrained)\n```", "```py\n# Read image data\nsetwd(\"~/Desktop\")\nimg <- image_load(\"dog.jpg\", target_size = c(224,224))\nx <- image_to_array(img)\nstr(x)\nOUTPUT\nnum [1:224, 1:224, 1:3] 70 69 68 73 88 79 18 22 21 20 ...\n\n# Image plot\nplot(as.raster(x, max = 255))  \n\n# Summary and histogram\nsummary(x)\nOUTPUT\n Min. 1st Qu. Median Mean 3rd Qu. Max. \n 0.0 89.0 150.0 137.7 190.0 255.0 \nhist(x)\n```", "```py\n# Preprocessing of input data\nx <- array_reshape(x, c(1, dim(x))) \nx <- imagenet_preprocess_input(x)\nhist(x)\n```", "```py\n# Predictions for top 5 categories\npreds <- pretrained %>% predict(x)\nimagenet_decode_predictions(preds, top = 5)[[1]]\nOutput\n  class_name  class_description       score\n1  n02094258    Norwich_terrier 0.769952953\n2  n02094114    Norfolk_terrier 0.126662806\n3  n02096294 Australian_terrier 0.046003290\n4  n02096177              cairn 0.040896162\n5  n02093991      Irish_terrier 0.005021056\n```", "```py\n# CIFAR10 data\ndata <- dataset_cifar10()\nstr(data)\nOUTPUT\nList of 2\n $ train:List of 2\n  ..$ x: int [1:50000, 1:32, 1:32, 1:3] 59 154 255 28 170 159 164 28 134 125 ...\n  ..$ y: int [1:50000, 1] 6 9 9 4 1 1 2 7 8 3 ...\n $ test :List of 2\n  ..$ x: int [1:10000, 1:32, 1:32, 1:3] 158 235 158 155 65 179 160 83 23 217 ...\n  ..$ y: num [1:10000, 1] 3 8 8 0 6 6 1 6 3 1 ...\n```", "```py\n# Partitioning the data into train and test\ntrainx <- data$train$x       \ntestx <- data$test$x\ntrainy <- to_categorical(data$train$y, num_classes = 10)\ntesty <- to_categorical(data$test$y, num_classes = 10)\n\ntable(data$train$y)\nOUTPUT\n   0    1    2    3    4    5    6    7    8    9 \n5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 \n\ntable(data$test$y)\nOUTPUT\n   0    1    2    3    4    5    6    7    8    9 \n1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 \n\n```", "```py\n# Category Labels\ndata$train$y[1:64,]\n [1] 6 9 9 4 1 1 2 7 8 3 4 7 7 2 9 9 9 3 2 6 4 3 6 6 2 6 3 5 4 0 0 9 1\n[34] 3 4 0 3 7 3 3 5 2 2 7 1 1 1 2 2 0 9 5 7 9 2 2 5 2 4 3 1 1 8 2\n```", "```py\n# Plot of first 64 pictures\npar(mfrow = c(8,8), mar = rep(0, 4))\nfor (i in 1:64) plot(as.raster(trainx[i,,,], max = 255))\npar(mfrow = c(1,1))\n```", "```py\n# Pre-processing and prediction\nx <- resize(trainx[2,,,], w = 224, h = 224)\nx <- array_reshape(x, c(1, dim(x)))\nx <- imagenet_preprocess_input(x)\npreds <- pretrained %>% predict(x)\nimagenet_decode_predictions(preds, top = 5)[[1]]\nOUTPUT\n  class_name class_description        score\n1  n03796401        moving_van 9.988740e-01\n2  n04467665     trailer_truck 7.548324e-04\n3  n03895866     passenger_car 2.044246e-04\n4  n04612504              yawl 2.441246e-05\n5  n04483307          trimaran 1.862814e-05\n```", "```py\n# Selecting first 2000 images\ntrainx <- data$train$x[1:2000,,,] \ntestx <- data$test$x[1:2000,,,] \n\n# One-hot encoding\ntrainy <- to_categorical(data$train$y[1:2000,], num_classes = 10)\ntesty <- to_categorical(data$test$y[1:2000,] , num_classes = 10)\n\n# Resizing train images to 224x224\nx <- array(rep(0, 2000 * 224 * 224 * 3), dim = c(2000, 224, 224, 3))\nfor (i in 1:2000) { x[i,,,] <- resize(trainx[i,,,], 224, 224) }\n\n# Plot of before/after resized image\npar(mfrow = c(1,2), mar = rep(0, 4))  \nplot(as.raster(trainx[2,,,], max = 255))\nplot(as.raster(x[2,,,], max = 255))\npar(mfrow = c(1,1))\n\ntrainx <- imagenet_preprocess_input(x)\n\n# Resizing test images to 224x224\nx <- array(rep(0, 2000 * 224 * 224 * 3), dim = c(2000, 224, 224, 3))\nfor (i in 1:2000) { x[i,,,] <- resize(testx[i,,,], 224, 224) }\ntestx <- imagenet_preprocess_input(x)\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu', \n                input_shape = c(224,224,3)) %>%   \n  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu') %>%  \n  layer_max_pooling_2d(pool_size = c(2,2)) %>% \n  layer_dropout(rate = 0.25) %>%   \n  layer_flatten() %>% \n  layer_dense(units = 256, activation = 'relu') %>%  \n  layer_dropout(rate = 0.25) %>% \n  layer_dense(units = 10, activation = 'softmax') \nsummary(model)\n_________________________________________________________________________\nLayer (type)                    Output Shape                Param # \n=========================================================================\nconv2d_6 (Conv2D)               (None, 222, 222, 32)         896 \n_________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 220, 220, 32)         9248 \n_________________________________________________________________________\nmax_pooling2d_22 (MaxPooling2D) (None, 110, 110, 32)          0 \n_________________________________________________________________________\ndropout_6 (Dropout)             (None, 110, 110, 32)          0 \n_________________________________________________________________________\nflatten_18 (Flatten)            (None, 387200)                0 \n__________________________________________________________________________\ndense_35 (Dense)                (None, 256)                99123456 \n__________________________________________________________________________\ndropout_7 (Dropout)             (None, 256)                   0 \n__________________________________________________________________________\ndense_36 (Dense)                (None, 10)                   2570 \n==========================================================================\nTotal params: 99,136,170\nTrainable params: 99,136,170\nNon-trainable params: 0\n___________________________________________________________________________________________\n\n# Compile\nmodel %>% compile(loss = 'categorical_crossentropy',\n optimizer = 'rmsprop', \n metrics = 'accuracy')\n\n# Fit\nmodel_one <- model %>% fit(trainx, \n                         trainy, \n                         epochs = 10, \n                         batch_size = 10, \n                         validation_split = 0.2)\n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(trainx, trainy)\n$loss\n[1] 3.335224\n$acc\n[1] 0.8455\n\n# Confusion matrix\npred <- model %>%   predict_classes(trainx)\ntable(Predicted=pred, Actual=data$train$y[1:2000,])\n         Actual\nPredicted   0   1   2   3   4   5   6   7   8   9\n        0 182   2   8   2   9   4   1   2  10   5\n        1   1 176   3   5   6   5   2   3   4   7\n        2   1   0 167   4   3   4   3   2   0   1\n        3   0   0   0 157   2   1   1   2   1   0\n        4   2   1   5   6 167   4   2   1   0   0\n        5   2   0   4   4   3 149   3   4   4   3\n        6   1   1   3   6   5   2 173   5   0   0\n        7   3   2   4   2   4   3   9 166   0   1\n        8  10   1   7   1   6   4   2   2 173   5\n        9   0   8   2   8   9   7  11  12  11 181\n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(testx, testy)\n$loss\n[1] 16.4562\n$acc\n[1] 0.2325\n\n# Confusion matrix\npred <- model %>% predict_classes(testx)\ntable(Predicted = pred, Actual = data$test$y[1:2000,])\n         Actual\nPredicted  0  1  2  3  4  5  6  7  8  9\n        0 82 24 29 17 16 10 17 19 67 19\n        1 16 65 20 26 18 21 26 26 33 53\n        2 10  0 26 20 20 18 14  5  1  2\n        3  6  5  8 21 12 22  9 12  9  3\n        4  4  8 22 11 22 16 25  9  6  4\n        5  5  7 12 29 17 29  9 19  4  9\n        6  6  6 20 17 23 15 51 25  6 13\n        7  3 10 10 15 21 16 11 37  3  5\n        8 34 22 20 12 22  2  7  7 61 24\n        9 30 51 28 31 27 36 47 34 27 71\n```", "```py\n# RESNET50 network without the top layer\npretrained <- application_resnet50(weights = \"imagenet\",\n                                   include_top = FALSE,\n                                   input_shape = c(224, 224, 3))\n\nmodel <- keras_model_sequential() %>% \n         pretrained %>% \n         layer_flatten() %>% \n         layer_dense(units = 256, activation = \"relu\") %>% \n         layer_dense(units = 10, activation = \"softmax\")\nsummary(model)\n_______________________________________________________________________\nLayer (type)                   Output Shape              Param # \n=======================================================================\nresnet50 (Model)               (None, 7, 7, 2048)        23587712 \n________________________________________________________________________\nflatten_6 (Flatten)            (None, 100352)               0 \n________________________________________________________________________\ndense_12 (Dense)               (None, 256)               25690368 \n________________________________________________________________________\ndense_13 (Dense)               (None, 10)                  2570 \n========================================================================\nTotal params: 49,280,650\nTrainable params: 49,227,530\nNon-trainable params: 53,120\n_________________________________________________________________________\n```", "```py\n# Freeze weights of resnet50 network\nfreeze_weights(pretrained)\n\n# Compile\nmodel %>% compile(loss = 'categorical_crossentropy',\n optimizer = 'rmsprop', \n metrics = 'accuracy')\n\nsummary(model)\n______________________________________________________\nLayer (type) Output Shape Param # \n======================================================\nresnet50 (Model) (None, 7, 7, 2048) 23587712 \n______________________________________________________\nflatten_6 (Flatten) (None, 100352) 0 \n______________________________________________________\ndense_12 (Dense) (None, 256) 25690368 \n______________________________________________________\ndense_13 (Dense) (None, 10) 2570 \n======================================================\nTotal params: 49,280,650\nTrainable params: 25,692,938\nNon-trainable params: 23,587,712\n______________________________________________________\n```", "```py\n# Fit model\nmodel_two <- model %>% fit(trainx, \n                        trainy, \n                        epochs = 10, \n                        batch_size = 10, \n                        validation_split = 0.2)\n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(trainx, trainy)\n$loss\n[1] 1.954347\n$acc\n[1] 0.8785\n\n# Confusion matrix\npred <- model %>%   predict_classes(trainx)\ntable(Predicted=pred, Actual=data$train$y[1:2000,])\n         Actual\nPredicted   0   1   2   3   4   5   6   7   8   9\n        0 182   0   5   2   3   0   2   0  10   1\n        1   1 156   1   1   1   0   2   0   4   0\n        2   2   0 172   3   4   0   4   0   1   0\n        3   0   0   1 133   2  12   2   1   0   0\n        4   1   0   8   4 188   3   4   2   0   0\n        5   1   0   4  22   3 162   1   3   0   0\n        6   0   0   3   9   3   0 192   1   1   0\n        7   3   0   5  10  10   5   0 188   0   0\n        8   5   0   3   3   0   1   0   1 182   0\n        9   7  35   1   8   0   0   0   3   5 202\n\n# Accuracy for each category\n100*diag(tab)/colSums(tab)\n       0        1        2        3        4 \n90.09901 81.67539 84.72906 68.20513 87.85047 \n       5        6        7        8        9 \n88.52459 92.75362 94.47236 89.65517 99.50739 \n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(testx, testy)\n$loss\n[1] 4.437256\n$acc\n[1] 0.768\n\n# Confusion matrix\npred <- model %>% predict_classes(testx)\ntable(Predicted = pred, Actual = data$test$y[1:2000,])\n         Actual\nPredicted   0   1   2   3   4   5   6   7   8   9\n        0 158   1  12   0   5   1   6   2  15   0\n        1   3 142   0   2   0   2   3   1   9   2\n        2   2   0 139   8   6   3   6   0   0   0\n        3   0   0   3  86   5  13   6   1   0   0\n        4   4   0  14   6 138   5  10   4   1   0\n        5   0   0  15  47   6 148   2  12   0   0\n        6   0   0   4  12   9   3 178   0   0   0\n        7   2   0   4  23  27   9   3 169   0   0\n        8  13   1   1   5   1   0   0   0 179   2\n        9  14  54   3  10   1   1   2   4  13 199\n\n# Accuracy for each category\n100*diag(tab)/colSums(tab)\n       0        1        2        3        4 \n80.61224 71.71717 71.28205 43.21608 69.69697 \n       5        6        7        8        9 \n80.00000 82.40741 87.56477 82.48848 98.02956 \n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(testx, testy)\n$loss\n[1] 4.005393\n$acc\n[1] 0.7715\n\n# Confusion matrix\npred <- model %>% predict_classes(testx)\ntable(Predicted = pred, Actual = data$test$y[1:2000,])\n         Actual\nPredicted   0   1   2   3   4   5   6   7   8   9\n        0 136   0  20   4   2   0   1   5   2   4\n        1   3 177   1   1   0   0   0   0   2  26\n        2   7   0 124   2   3   1   3   0   1   0\n        3   2   0   4  80   7   6   7   2   1   0\n        4   3   1  18   9 151   4   8   9   0   0\n        5   2   0   3  58   3 152   4   5   0   3\n        6   3   2   8  22   8   8 190   0   6   2\n        7   1   0  14  18  22  14   2 172   0   0\n        8  36  11   3   5   2   0   1   0 205  12\n        9   3   7   0   0   0   0   0   0   0 156\n\n# Accuracy for each category\n100*diag(tab)/colSums(tab)\n       0        1        2        3        4 \n69.38776 89.39394 63.58974 40.20101 76.26263 \n       5        6        7        8        9 \n82.16216 87.96296 89.11917 94.47005 76.84729 \n```", "```py\n# Model with RESNET50\npretrained <- application_resnet50(weights = 'imagenet',\n                                   include_top = FALSE,\n                                   input_shape = c(224, 224, 3))\n# Flags for hyperparameter tuning\nFLAGS <- flags(flag_integer(\"dense_units\", 256),\n               flag_numeric(\"dropout\", 0.1),\n               flag_integer(\"batch_size\", 10))\n\n# Model architecture\nmodel <- keras_model_sequential() %>% \n         pretrained %>% \n         layer_flatten() %>% \n         layer_dense(units = FLAGS$dense_units, activation = 'relu') %>% \n         layer_dropout(rate = FLAGS$dropout) %>%\n         layer_dense(units = 10, activation = 'softmax')\nfreeze_weights(pretrained)\n\n# Compile\nmodel %>% compile(loss = \"categorical_crossentropy\",\n                  optimizer = 'adam',\n                  metrics = 'accuracy')\n\n# Fit model\nhistory <- model %>% fit(trainx,\n                         trainy,\n                         epochs = 5,\n                         batch_size = FLAGS$batch_size,\n                         validation_split = 0.2)\n```", "```py\n# Set working directory\nsetwd('~/Desktop')\n\n# Hyperparameter tuning\nlibrary(tfruns)\nruns <- tuning_run(\"TransferLearning.R\", \n                   flags = list(dense_units = c(256, 512),\n                                dropout = c(0.1,0.3),\n                                batch_size = c(10, 30)))\n```", "```py\n# Results\nruns[,c(6:10)]\nData frame: 8 x 5 \n\n  metric_val_loss metric_val_acc flag_dense_units flag_dropout flag_batch_size\n1          1.1935         0.7525              512          0.3              30\n2          0.9521         0.7725              256          0.3              30\n3          1.1260         0.8200              512          0.1              30\n4          1.3276         0.7950              256          0.1              30\n5          1.1435         0.7700              512          0.3              10\n6          1.3096         0.7275              256          0.3              10\n7          1.3458         0.7850              512          0.1              10\n8          1.0248         0.7950              256          0.1              10\n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(testx, testy)\n$loss\n[1] 1.095251\n$acc\n[1] 0.7975\n\n# Confusion matrix\npred <- model %>% predict_classes(testx)\n(tab <- table(Predicted = pred, Actual = data$test$y[1:2000,]))\n         Actual\nPredicted   0   1   2   3   4   5   6   7   8   9\n        0 167   5  20   4   5   4   4  15  10   8\n        1   1 176   0   3   0   0   1   1   2  15\n        2   3   0 139   9   2   4   8   1   1   0\n        3   0   0   3  92   6   6   5   0   0   0\n        4   4   0  20  16 177  12  17  23   0   1\n        5   0   0   7  50   1 149   1   9   0   0\n        6   1   0   2  11   2   5 177   1   0   1\n        7   0   0   0   5   3   4   1 143   0   0\n        8  16   3   3   5   2   0   1   0 203   6\n        9   4  14   1   4   0   1   1   0   1 172\n\n# Accuracy for each category\n100*diag(tab)/colSums(tab)\n       0        1        2        3        4        5        6        7 \n85.20408 88.88889 71.28205 46.23116 89.39394 80.54054 81.94444 74.09326 \n       8        9 \n93.54839 84.72906 \n```", "```py\n# Pretrained model\npretrained <- application_vgg16(weights = 'imagenet', \n                           include_top = FALSE,\n                           input_shape = c(224, 224, 3))\n\n# Model architecture\nmodel <- keras_model_sequential() %>% \n  pretrained %>% \n  layer_flatten() %>% \n  layer_dense(units = 256, activation = \"relu\") %>% \n  layer_dense(units = 10, activation = \"softmax\")\nsummary(model)\n\nfreeze_weights(pretrained)\nsummary(model)\n_________________________________________________________________________\nLayer (type)                    Output Shape            Param # \n=========================================================================\nvgg16 (Model)                  (None, 7, 7, 512)        14714688 \n__________________________________________________________________________\nflatten (Flatten)              (None, 25088)              0 \n__________________________________________________________________________\ndense (Dense)                  (None, 256)              6422784 \n__________________________________________________________________________\ndense_1 (Dense)                (None, 10)                2570 \n==========================================================================\nTotal params: 21,140,042\nTrainable params: 6,425,354\nNon-trainable params: 14,714,688\n___________________________________________________________________________\n\n# Compile model\nmodel %>% compile(loss = 'categorical_crossentropy',\n                  optimizer = 'adam',    \n                  metrics = 'accuracy')\n\n# Fit model\nmodel_four <- model %>% fit(trainx, \n                         trainy, \n                         epochs = 10, \n                         batch_size = 10, \n                         validation_split = 0.2)\n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(testx, testy)\n$loss\n [1] 1.673867\n$acc\n [1] 0.7565\n\n# Confusion matrix\npred <- model %>% predict_classes(testx)\n(tab <- table(Predicted = pred, Actual = data$test$y[1:2000,]))\n         Actual\nPredicted   0   1   2   3   4   5   6   7   8   9\n        0 137   2  12   0   6   0   0   1  11   6\n        1   9 172   1   0   0   0   0   1   9  21\n        2   7   0 123  11  11   3   3   5   3   0\n        3   3   0  11 130  10  35   7   7   0   0\n        4   7   0  13   5 118   7  10   5   1   0\n        5   1   0  11  27   3 125   2   7   0   0\n        6   2   5  20  18  21   8 192   3   4   1\n        7   6   0   4   6  25   7   2 163   2   1\n        8  18   6   0   2   4   0   0   1 182   3\n        9   6  13   0   0   0   0   0   0   5 171\n\n# Accuracy for each category\n100*diag(tab)/colSums(tab)\n       0        1        2        3        4 \n69.89796 86.86869 63.07692 65.32663 59.59596 \n       5        6        7        8        9 \n67.56757 88.88889 84.45596 83.87097 84.23645 \n```"]