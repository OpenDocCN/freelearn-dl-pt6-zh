<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Disentangled Representation GANs"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Disentangled Representation GANs</h1></div></div></div><p>As we've explored, GANs can generate meaningful outputs by learning the data distribution. However, there was no control over the attributes of the outputs generated. Some variations of GANs like <span class="strong"><strong>Conditional GAN</strong></span> (<span class="strong"><strong>CGAN</strong></span>) and <span class="strong"><strong>Auxiliary Classifier GAN</strong></span> (<span class="strong"><strong>ACGAN</strong></span>), as discussed in the previous chapter are able to train a generator that<a id="id234" class="indexterm"/> is conditioned to synthesize specific outputs. For example, both CGAN and<a id="id235" class="indexterm"/> ACGAN can induce the generator to produce a specific MNIST digit. This is achieved by using both a 100-dim noise code and the corresponding one-hot label as inputs. However, other than the one-hot label, we have no other ways to control the properties of generated outputs.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>For a review on CGAN and ACGAN, please see <a class="link" href="ch04.html" title="Chapter 4. Generative Adversarial Networks (GANs)">Chapter 4</a>, <span class="emphasis"><em>Generative Adversarial Networks (GANs),</em></span> and <a class="link" href="ch05.html" title="Chapter 5. Improved GANs">Chapter 5</a>, <span class="emphasis"><em>Improved GANs</em></span>.</p></div></div><p>In this chapter, we will be covering the variations of GANs that enable us to modify the generator outputs. In the context of the MNIST dataset, apart from which number to produce, we may find that we want to control the writing style. This could involve the tilt or the width of the desired digit. In other words, GANs can also learn disentangled latent codes or representations that we can use to vary the attributes of the generator outputs. A disentangled code or representation is a tensor that can change a specific feature or attribute of the output data while not affecting the other attributes.</p><p>In the first section of<a id="id236" class="indexterm"/> this chapter, we will be discussing <span class="strong"><strong>InfoGAN</strong></span>: <span class="emphasis"><em>Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</em></span> [1], an extension to GANs. InfoGAN learns the disentangled representations in an unsupervised manner by maximizing the mutual information between the input codes and the output observation. On the MNIST dataset, InfoGAN disentangles the writing styles from digits dataset.</p><p>In the following part of the chapter, we'll also be discussing the <span class="strong"><strong>Stacked Generative Adversarial Networks</strong></span> or <span class="strong"><strong>StackedGAN</strong></span> [2], another extension to GANs. StackedGAN uses a<a id="id237" class="indexterm"/> pretrained encoder or classifier in order to aid in disentangling the latent codes. StackedGAN can be viewed as a stack of models, with each being made of an encoder and a GAN. Each GAN is trained in an adversarial manner by using the input and output data of the corresponding encoder.</p><p>In summary, the goal of this chapter is to present:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The concepts of disentangled representations</li><li class="listitem" style="list-style-type: disc">The principles of both InfoGAN and StackedGAN</li><li class="listitem" style="list-style-type: disc">Implementation of both InfoGAN and StackedGAN using Keras</li></ul></div><div class="section" title="Disentangled representations"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec34"/>Disentangled representations</h1></div></div></div><p>The original GAN was able to generate meaningful outputs, but the downside was that it couldn't be controlled. For example, if we trained a GAN to learn the distribution of celebrity faces, the generator would produce new images of celebrity-looking people. However, there is no way to<a id="id238" class="indexterm"/> influence the generator on the specific attributes of the face that we want. For example, we're unable<a id="id239" class="indexterm"/> to ask the generator for a face of a female celebrity with long black hair, a fair complexion, brown eyes, and whose smiling. The fundamental reason for this is because the 100-dim noise code that we use entangles all of the salient attributes of the generator outputs. We can recall that in Keras, the 100-dim code was generated by random sampling of uniform noise distribution:</p><div class="informalexample"><pre class="programlisting"># generate 64 fake images from 64 x 100-dim uniform noise
noise = np.random.uniform(-1.0, 1.0, size=[64, 100])
fake_images = generator.predict(noise)</pre></div><p>If we are able to modify the original GAN, such that we were able to separate the code or representation into entangled and disentangled interpretable latent codes, we would be able to tell the generator what to synthesize.</p><p>Following figure shows us a GAN with an entangled code and its variation with a mixture of entangled and disentangled representations. In the context of the hypothetical celebrity face generation, with the disentangled codes, we are able to indicate the gender, hairstyle, facial expression, skin complexion and eye color of the face we wish to generate. The <span class="emphasis"><em>n–dim</em></span> entangled code is still needed to represent all the other facial attributes that we have not disentangled like the face shape, facial hair, eye-glasses, as just three examples. The concatenation of entangled and disentangled codes serves as the new input to the generator. The total dimension of the concatenated code may not be necessarily 100:</p><div class="mediaobject"><img src="graphics/B08956_06_01.jpg" alt="Disentangled representations"/><div class="caption"><p>Figure 6.1.1: The GAN with the entangled code and its variation with both entangled and disentangled codes. This example is shown in the context of celebrity face generation.</p></div></div><p>Looking at preceding figure, it appears that GANs with disentangled representations can also be optimized in the<a id="id240" class="indexterm"/> same way as a vanilla GAN can be. This is because the generator<a id="id241" class="indexterm"/> output can be represented as:</p><div class="mediaobject"><img src="graphics/B08956_06_001.jpg" alt="Disentangled representations"/></div><p>          (Equation 6.1.1)</p><p>The code </p><div class="mediaobject"><img src="graphics/B08956_06_002.jpg" alt="Disentangled representations"/></div><p> is made of two elements:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Incompressible entangled noise code similar to GANs <span class="emphasis"><em>z</em></span> or noise vector.</li><li class="listitem">Latent codes, <span class="emphasis"><em>c</em></span><span class="emphasis"><em>1</em></span>,<span class="emphasis"><em>c</em></span><span class="emphasis"><em>2</em></span>,…,<span class="emphasis"><em>c</em></span><span class="emphasis"><em>L</em></span>, which represent the interpretable disentangled codes of the data distribution. Collectively all latent codes are represented by <span class="emphasis"><em>c</em></span>.</li></ol></div><p>For simplicity, all the latent codes are assumed to be independent:</p><div class="mediaobject"><img src="graphics/B08956_06_003.jpg" alt="Disentangled representations"/></div><p>          (Equation 6.1.2)</p><p>The generator function </p><div class="mediaobject"><img src="graphics/B08956_06_004.jpg" alt="Disentangled representations"/></div><p> is provided with both the incompressible noise code and the latent codes. From the point of view of the generator, optimizing </p><div class="mediaobject"><img src="graphics/B08956_06_005.jpg" alt="Disentangled representations"/></div><p> is the same as optimizing <span class="emphasis"><em>z</em></span>. The generator network will simply ignore the constraint imposed by the disentangled codes when coming up with a solution. The generator learns the distribution </p><div class="mediaobject"><img src="graphics/B08956_06_006.jpg" alt="Disentangled representations"/></div><p>. This will practically defeat the objective of disentangled representations.</p></div></div>
<div class="section" title="InfoGAN"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec35"/>InfoGAN</h1></div></div></div><p>To enforce the disentanglement of codes, InfoGAN proposed a regularizer to the original loss function that maximizes<a id="id242" class="indexterm"/> the mutual information between the<a id="id243" class="indexterm"/> latent codes <span class="emphasis"><em>c</em></span> and </p><div class="mediaobject"><img src="graphics/B08956_06_007.jpg" alt="InfoGAN"/></div><p>:</p><div class="mediaobject"><img src="graphics/B08956_06_008.jpg" alt="InfoGAN"/></div><p>          (Equation 6.1.3)</p><p>The regularizer forces the generator to consider the latent codes when it formulates a function that synthesizes the fake images. In the field of information theory, the mutual information between latent codes <span class="emphasis"><em>c</em></span> and </p><div class="mediaobject"><img src="graphics/B08956_06_009.jpg" alt="InfoGAN"/></div><p> is defined as:</p><div class="mediaobject"><img src="graphics/B08956_06_010.jpg" alt="InfoGAN"/></div><p>          (Equation 6.1.4)</p><p>Where <span class="emphasis"><em>H</em></span>(<span class="emphasis"><em>c</em></span>) is the entropy of the latent code <span class="emphasis"><em>c</em></span> and </p><div class="mediaobject"><img src="graphics/B08956_06_011.jpg" alt="InfoGAN"/></div><p> is the conditional entropy of <span class="emphasis"><em>c,</em></span> after observing the output of the generator, </p><div class="mediaobject"><img src="graphics/B08956_06_012.jpg" alt="InfoGAN"/></div><p>. Entropy is a measure of uncertainty of a random variable or an event. For example, information like, <span class="emphasis"><em>the sun rises in the east</em></span>, has low entropy. Whereas, <span class="emphasis"><em>winning the jackpot in the lottery</em></span> has high entropy.</p><p>In <span class="emphasis"><em>Equation 6.1.4</em></span>, maximizing the mutual information means minimizing </p><div class="mediaobject"><img src="graphics/B08956_06_013.jpg" alt="InfoGAN"/></div><p> or decreasing the uncertainty in the latent code upon observing the generated output. This makes sense since, for example, in the MNIST dataset, the generator becomes more confident in synthesizing the digit 8 if the GAN sees that it observed the digit 8.</p><p>However, it is hard to estimate </p><div class="mediaobject"><img src="graphics/B08956_06_014.jpg" alt="InfoGAN"/></div><p> since it requires knowledge of the posterior </p><div class="mediaobject"><img src="graphics/B08956_06_015.jpg" alt="InfoGAN"/></div><p>, which is something that we don't have access to. The workaround is to estimate the lower bound of mutual information by estimating the posterior with an auxiliary distribution <span class="emphasis"><em>Q</em></span>(<span class="emphasis"><em>c|x</em></span>). InfoGAN estimates the lower bound of mutual information as:</p><div class="mediaobject"><img src="graphics/B08956_06_016.jpg" alt="InfoGAN"/></div><p>          (Equation 6.1.5)</p><p>In InfoGAN, <span class="emphasis"><em>H</em></span>(<span class="emphasis"><em>c</em></span>) is assumed to be a constant. Therefore, maximizing the mutual information is a matter of maximizing the expectation. The generator must be confident that it has generated an output with the specific attributes. We should note that the maximum value of this expectation is zero. Therefore, the maximum of the lower bound of the mutual information is <span class="emphasis"><em>H</em></span>(<span class="emphasis"><em>c</em></span>). In InfoGAN, <span class="emphasis"><em>Q</em></span>(<span class="emphasis"><em>c</em></span>|<span class="emphasis"><em>x</em></span>) for discrete latent codes can be represented by <span class="emphasis"><em>softmax</em></span> nonlinearity. The expectation<a id="id244" class="indexterm"/> is the negative <code class="literal">categorical_crossentropy</code> loss in Keras.</p><p>For continuous codes<a id="id245" class="indexterm"/> of a single dimension, the expectation is a double integral over <span class="emphasis"><em>c</em></span> and <span class="emphasis"><em>x</em></span>. This is due to the expectation that samples from both disentangled code distribution and generator distribution. One way of estimating the expectation is by assuming the samples as a good measure of continuous data. Therefore, the loss is estimated as <span class="emphasis"><em>c</em></span> log <span class="emphasis"><em>Q</em></span>(<span class="emphasis"><em>c</em></span>|<span class="emphasis"><em>x</em></span>).</p><p>To complete the network of InfoGAN, we should have an implementation of <span class="emphasis"><em>Q</em></span>(<span class="emphasis"><em>c</em></span>|<span class="emphasis"><em>x</em></span>). For simplicity, the network <span class="emphasis"><em>Q</em></span> is an auxiliary network attached to the second to last layer of the discriminator. Therefore, this has a minimal impact on the training of the original GAN. Following figure shows InfoGAN network diagram:</p><div class="mediaobject"><img src="graphics/B08956_06_02.jpg" alt="InfoGAN"/><div class="caption"><p>Figure 6.1.2: A network diagram showing the discriminator and generator training in InfoGAN</p></div></div><p>Following table shows the<a id="id246" class="indexterm"/> loss functions of InfoGAN as compared to the original GAN. The loss<a id="id247" class="indexterm"/> functions of InfoGAN differ from the original GAN by an additional term </p><div class="mediaobject"><img src="graphics/B08956_06_017.jpg" alt="InfoGAN"/></div><p> where </p><div class="mediaobject"><img src="graphics/B08956_06_018.jpg" alt="InfoGAN"/></div><p> is a small positive constant. Minimizing the loss function<a id="id248" class="indexterm"/> of InfoGAN translates to minimizing the loss<a id="id249" class="indexterm"/> of the original GAN and maximizing the mutual information </p><div class="mediaobject"><img src="graphics/B08956_06_019.jpg" alt="InfoGAN"/></div><p>.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Network</p>
</th><th style="text-align: left" valign="bottom">
<p>Loss Functions</p>
</th><th style="text-align: left" valign="bottom">
<p>Number</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>GAN</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_06_020.jpg" alt="InfoGAN"/></div>
<div class="mediaobject"><img src="graphics/B08956_06_021.jpg" alt="InfoGAN"/></div>
</td><td style="text-align: left" valign="top">
<p>4.1.1</p>
<p>4.1.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>InfoGAN</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_06_022.jpg" alt="InfoGAN"/></div>
<div class="mediaobject"><img src="graphics/B08956_06_023.jpg" alt="InfoGAN"/></div>
<p>For continuous codes, InfoGAN recommends a value of </p><div class="mediaobject"><img src="graphics/B08956_06_024.jpg" alt="InfoGAN"/></div><p>. In our example, we set </p><div class="mediaobject"><img src="graphics/B08956_06_025.jpg" alt="InfoGAN"/></div><p>. For discrete codes, InfoGAN recommends </p><div class="mediaobject"><img src="graphics/B08956_06_026.jpg" alt="InfoGAN"/><div class="caption"><p>Table 6.1.1: A comparison between the loss functions of GAN and InfoGAN</p></div></div><p>.</p>
</td><td style="text-align: left" valign="top">
<p>6.1.1</p>
<p>6.1.2</p>
</td></tr></tbody></table></div><p>If applied on the MNIST dataset, InfoGAN can learn the disentangled discrete and continuous codes in order to modify the generator output attributes. For example, like CGAN and ACGAN, the discrete code in the form of a 10-dim one-hot label will be used to specify the digit to generate. However, we can add two continuous codes, one for controlling the angle of writing style and another<a id="id250" class="indexterm"/> for adjusting the stroke width. Following figure shows<a id="id251" class="indexterm"/> the codes for the MNIST digit in InfoGAN. We retain the entangled code with a smaller dimensionality to represent all other attributes:</p><div class="mediaobject"><img src="graphics/B08956_06_03.jpg" alt="InfoGAN"/><div class="caption"><p>Figure 6.1.3: The codes for both GAN and InfoGAN in the context of MNIST dataset</p></div></div></div>
<div class="section" title="Implementation of InfoGAN in Keras"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec36"/>Implementation of InfoGAN in Keras</h1></div></div></div><p>To implement InfoGAN on MNIST dataset, there are some changes that need to be made in the base code of ACGAN. As highlighted in following listing, the generator concatenates both entangled (<span class="emphasis"><em>z</em></span> noise code) and disentangled codes (one-hot label and continuous codes) to serve as input. The builder functions for the generator and discriminator are also implemented in <code class="literal">gan.py</code> in the <code class="literal">lib</code> folder.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note06"/>Note</h3><p>The complete code is available on GitHub:</p><p>
<a class="ulink" href="https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras">https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras</a>
</p></div></div><p>Listing 6.1.1, <code class="literal">infogan-mnist-6.1.1.py</code> shows us how the InfoGAN generator concatenates both entangled and disentangled codes to serve as input:</p><div class="informalexample"><pre class="programlisting">def generator(inputs,
              image_size,
              activation='sigmoid',
              labels=None,
              codes=None):
    """Build a Generator Model

    Stack of BN-ReLU-Conv2DTranpose to generate fake images.
    Output activation is sigmoid instead of tanh in [1].
    Sigmoid converges easily.

    # Arguments
        inputs (Layer): Input layer of the generator (the z-vector)
        image_size (int): Target size of one side (assuming square image)
        activation (string): Name of output activation layer
        labels (tensor): Input labels
        codes (list): 2-dim disentangled codes for InfoGAN

    # Returns
        Model: Generator Model
    """
    image_resize = image_size // 4
    # network parameters
    kernel_size = 5
    layer_filters = [128, 64, 32, 1]

    if labels is not None:
        if codes is None:
            # ACGAN labels
            # concatenate z noise vector and one-hot labels
            inputs = [inputs, labels]
        else:
<span class="strong"><strong>            # infoGAN codes</strong></span>
<span class="strong"><strong>            # concatenate z noise vector, one-hot labels, </strong></span>
<span class="strong"><strong>            # and codes 1 &amp; 2</strong></span>
<span class="strong"><strong>            inputs = [inputs, labels] + codes</strong></span>
        x = concatenate(inputs, axis=1)
    elif codes is not None:
        # generator 0 of StackedGAN
        inputs = [inputs, codes]
        x = concatenate(inputs, axis=1)
    else:
        # default input is just 100-dim noise (z-code)
        x = inputs

    x = Dense(image_resize * image_resize * layer_filters[0])(x)
    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)

    for filters in layer_filters:
        # first two convolution layers use strides = 2
        # the last two use strides = 1
        if filters &gt; layer_filters[-2]:
            strides = 2
        else:
            strides = 1
        x = BatchNormalization()(x)
        x = Activation('relu')(x)
        x = Conv2DTranspose(filters=filters,
                            kernel_size=kernel_size,
                            strides=strides,
                            padding='same')(x)

    if activation is not None:
        x = Activation(activation)(x)

    # generator output is the synthesized image x
    return Model(inputs, x, name='generator')</pre></div><p>The preceding listing shows the discriminator and <span class="emphasis"><em>Q</em></span>-Network with the original default GAN output. The three auxiliary<a id="id252" class="indexterm"/> outputs corresponding to discrete code (for one-hot label) <code class="literal">softmax</code> prediction and the continuous codes probabilities given the input MNIST digit<a id="id253" class="indexterm"/> image are highlighted.</p><p>Listing 6.1.2, <code class="literal">infogan-mnist-6.1.1.py</code>. InfoGAN discriminator and <span class="emphasis"><em>Q</em></span>-Network:</p><div class="informalexample"><pre class="programlisting">def discriminator(inputs,
                  activation='sigmoid',
                  num_labels=None,
                  num_codes=None):
    """Build a Discriminator Model

    Stack of LeakyReLU-Conv2D to discriminate real from fake
    The network does not converge with BN so it is not used here
    unlike in [1]

    # Arguments
        inputs (Layer): Input layer of the discriminator (the image)
        activation (string): Name of output activation layer
        num_labels (int): Dimension of one-hot labels for ACGAN &amp; InfoGAN
        num_codes (int): num_codes-dim Q network as output 
                    if StackedGAN or 2 Q networks if InfoGAN
                    

    # Returns
        Model: Discriminator Model
    """
    kernel_size = 5
    layer_filters = [32, 64, 128, 256]

    x = inputs
    for filters in layer_filters:
        # first 3 convolution layers use strides = 2
        # last one uses strides = 1
        if filters == layer_filters[-1]:
            strides = 1
        else:
            strides = 2
        x = LeakyReLU(alpha=0.2)(x)
        x = Conv2D(filters=filters,
                   kernel_size=kernel_size,
                   strides=strides,
                   padding='same')(x)

    x = Flatten()(x)
    # default output is probability that the image is real
    outputs = Dense(1)(x)
    if activation is not None:
        print(activation)
        outputs = Activation(activation)(outputs)

    if num_labels:
<span class="strong"><strong>        # ACGAN and InfoGAN have 2nd output</strong></span>
<span class="strong"><strong>        # 2nd output is 10-dim one-hot vector of label</strong></span>
<span class="strong"><strong>        layer = Dense(layer_filters[-2])(x)</strong></span>
<span class="strong"><strong>        labels = Dense(num_labels)(layer)</strong></span>
<span class="strong"><strong>        labels = Activation('softmax', name='label')(labels)</strong></span>
<span class="strong"><strong>        if num_codes is None:</strong></span>
<span class="strong"><strong>            outputs = [outputs, labels]</strong></span>
<span class="strong"><strong>        else:</strong></span>
<span class="strong"><strong>            # InfoGAN have 3rd and 4th outputs</strong></span>
<span class="strong"><strong>            # 3rd output is 1-dim continous Q of 1st c given x</strong></span>
<span class="strong"><strong>            code1 = Dense(1)(layer)</strong></span>
<span class="strong"><strong>            code1 = Activation('sigmoid', name='code1')(code1)</strong></span>

<span class="strong"><strong>            # 4th output is 1-dim continuous Q of 2nd c given x</strong></span>
<span class="strong"><strong>            code2 = Dense(1)(layer)</strong></span>
<span class="strong"><strong>            code2 = Activation('sigmoid', name='code2')(code2)</strong></span>

<span class="strong"><strong>            outputs = [outputs, labels, code1, code2]</strong></span>
    elif num_codes is not None:
	   # StackedGAN Q0 output
        # z0_recon is reconstruction of z0 normal distribution
        z0_recon =  Dense(num_codes)(x)
        z0_recon = Activation('tanh', name='z0')(z0_recon)
        outputs = [outputs, z0_recon]

 return Model(inputs, outputs, name='discriminator')</pre></div><p>
<span class="emphasis"><em>Figure 6.1.4</em></span> shows the InfoGAN model in Keras. Building the discriminator and adversarial models also requires a<a id="id254" class="indexterm"/> number of changes. The changes are on the loss<a id="id255" class="indexterm"/> functions used. The original discriminator loss function <code class="literal">binary_crossentropy</code>, the <code class="literal">categorical_crossentropy</code> for discrete code, and the <code class="literal">mi_loss</code> function for each continuous code comprise the overall loss function. Each loss function is given a weight of 1.0, except for the <code class="literal">mi_loss</code> function which is given 0.5 corresponding to <span class="inlinemediaobject"><img src="graphics/B08956_06_027.jpg" alt="Implementation of InfoGAN in Keras"/></span> for the continuous code.</p><p>
<span class="emphasis"><em>Listing 6.1.3</em></span> highlights the changes made. However, we should note that by using the builder function, the discriminator is instantiated as:</p><div class="informalexample"><pre class="programlisting"># call discriminator builder with 4 outputs: source, label, 
# and 2 codes
discriminator = gan.discriminator(inputs, num_labels=num_labels, with_codes=True)</pre></div><p>The generator is created by:</p><div class="informalexample"><pre class="programlisting"># call generator with inputs, labels and codes as total inputs 
# to generator
generator = gan.generator(inputs, image_size, labels=labels, codes=[code1, code2])</pre></div><div class="mediaobject"><img src="graphics/B08956_06_04.jpg" alt="Implementation of InfoGAN in Keras"/><div class="caption"><p>Figure 6.1.4: The InfoGAN Keras model</p></div></div><p>Listing 6.1.3, <code class="literal">infogan-mnist-6.1.1.py</code> shows us the mutual Information<a id="id256" class="indexterm"/> loss function as<a id="id257" class="indexterm"/> used in building the InfoGAN discriminator and adversarial networks:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>def mi_loss(c, q_of_c_given_x):</strong></span>
<span class="strong"><strong>    """ Mutual information, Equation 5 in [2], assuming H(c) is constant"""</strong></span>
<span class="strong"><strong>    # mi_loss = -c * log(Q(c|x))</strong></span>
<span class="strong"><strong>    return K.mean(-K.sum(K.log(q_of_c_given_x + K.epsilon()) * c, axis=1))</strong></span>


def build_and_train_models(latent_size=100):
    # load MNIST dataset
    (x_train, y_train), (_, _) = mnist.load_data()

    # reshape data for CNN as (28, 28, 1) and normalize
    image_size = x_train.shape[1]
    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])
    x_train = x_train.astype('float32') / 255

    # train labels
    num_labels = len(np.unique(y_train))
    y_train = to_categorical(y_train)

    model_name = "infogan_mnist"
    # network parameters
    batch_size = 64
    train_steps = 40000
    lr = 2e-4
    decay = 6e-8
    input_shape = (image_size, image_size, 1)
<span class="strong"><strong>    label_shape = (num_labels, )</strong></span>
<span class="strong"><strong>    code_shape = (1, )</strong></span>

    # build discriminator model
    inputs = Input(shape=input_shape, name='discriminator_input')
<span class="strong"><strong>    # call discriminator builder with 4 outputs: </strong></span>
<span class="strong"><strong>    # source, label, and 2 codes</strong></span>
<span class="strong"><strong>    discriminator = gan.discriminator(inputs,</strong></span>
<span class="strong"><strong>                                      num_labels=num_labels,</strong></span>
<span class="strong"><strong>                                      num_codes=2)</strong></span>
<span class="strong"><strong>    # [1] uses Adam, but discriminator converges easily with RMSprop</strong></span>
<span class="strong"><strong>    optimizer = RMSprop(lr=lr, decay=decay)</strong></span>
<span class="strong"><strong>    # loss functions: 1) probability image is real (binary crossentropy)</strong></span>
<span class="strong"><strong>    # 2) categorical cross entropy image label,</strong></span>
<span class="strong"><strong>    # 3) and 4) mutual information loss</strong></span>
<span class="strong"><strong>    loss = ['binary_crossentropy', 'categorical_crossentropy', mi_loss, mi_loss]</strong></span>
<span class="strong"><strong>    # lamda or mi_loss weight is 0.5</strong></span>
<span class="strong"><strong>    loss_weights = [1.0, 1.0, 0.5, 0.5]</strong></span>
<span class="strong"><strong>    discriminator.compile(loss=loss,</strong></span>
<span class="strong"><strong>                          loss_weights=loss_weights,</strong></span>
<span class="strong"><strong>                          optimizer=optimizer,</strong></span>
<span class="strong"><strong>                          metrics=['accuracy'])</strong></span>
    discriminator.summary()
    # build generator model
    input_shape = (latent_size, )
<span class="strong"><strong>    inputs = Input(shape=input_shape, name='z_input')</strong></span>
<span class="strong"><strong>    labels = Input(shape=label_shape, name='labels')</strong></span>
<span class="strong"><strong>    code1 = Input(shape=code_shape, name="code1")</strong></span>
<span class="strong"><strong>    code2 = Input(shape=code_shape, name="code2")</strong></span>
<span class="strong"><strong>    # call generator with inputs, </strong></span>
<span class="strong"><strong>    # labels and codes as total inputs to generator</strong></span>
<span class="strong"><strong>    generator = gan.generator(inputs,</strong></span>
<span class="strong"><strong>                              image_size,</strong></span>
<span class="strong"><strong>                              labels=labels,</strong></span>
<span class="strong"><strong>                              codes=[code1, code2])</strong></span>
    generator.summary()

    # build adversarial model = generator + discriminator
    optimizer = RMSprop(lr=lr*0.5, decay=decay*0.5)
    discriminator.trainable = False
<span class="strong"><strong>    # total inputs = noise code, labels, and codes</strong></span>
<span class="strong"><strong>    inputs = [inputs, labels, code1, code2]</strong></span>
<span class="strong"><strong>    adversarial = Model(inputs,</strong></span>
<span class="strong"><strong>                        discriminator(generator(inputs)),</strong></span>
<span class="strong"><strong>                        name=model_name)</strong></span>
<span class="strong"><strong>    # same loss as discriminator</strong></span>
<span class="strong"><strong>    adversarial.compile(loss=loss,</strong></span>
<span class="strong"><strong>                        loss_weights=loss_weights,</strong></span>
<span class="strong"><strong>                        optimizer=optimizer,</strong></span>
<span class="strong"><strong>                        metrics=['accuracy'])</strong></span>
    adversarial.summary()

    # train discriminator and adversarial networks
    models = (generator, discriminator, adversarial)
    data = (x_train, y_train)
    params = (batch_size, latent_size, train_steps, num_labels, model_name)
    train(models, data, params)</pre></div><p>As far as the training is concerned, we can see that InfoGAN is similar to ACGAN except that we need to supply <span class="emphasis"><em>c</em></span> for the<a id="id258" class="indexterm"/> continuous code. <span class="emphasis"><em>c</em></span> is drawn from normal distribution<a id="id259" class="indexterm"/> with a standard deviation of 0.5 and mean of 0.0. We'll use randomly sampled labels for the fake data and dataset class labels for the real data to represent discrete latent code. Following listing highlights the changes made on the training function. Similar to all previous GANs, the discriminator and generator (through adversarial) are trained alternately. During adversarial training, the discriminator weights are frozen. Sample generator output images are saved every 500 interval steps by using the <code class="literal">gan.py plot_images()</code> function.</p><p>Listing 6.1.4, <code class="literal">infogan-mnist-6.1.1.py</code> shows us how the training function for InfoGAN is similar to ACGAN. The only difference is that we supply continuous codes sampled from a<a id="id260" class="indexterm"/> normal distribution:</p><div class="informalexample"><pre class="programlisting">def train(models, data, params):
    """Train the Discriminator and Adversarial networks

    Alternately train discriminator and adversarial networks by batch.
    Discriminator is trained first with real and fake images,
    corresponding one-hot labels and continuous codes.
    Adversarial is trained next with fake images pretending to be real,
    corresponding one-hot labels and continous codes.
    Generate sample images per save_interval.

    # Arguments
        models (Models): Generator, Discriminator, Adversarial models
        data (tuple): x_train, y_train data
        params (tuple): Network parameters
    """
    # the GAN models
    generator, discriminator, adversarial = models
    # images and their one-hot labels
    x_train, y_train = data
    # network parameters
    batch_size, latent_size, train_steps, num_labels, model_name = params
    # the generator image is saved every 500 steps
    save_interval = 500
    # noise vector to see how the generator output evolves 
    # during training
    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])
    # random class labels and codes
    noise_label = np.eye(num_labels)[np.arange(0, 16) % num_labels]
<span class="strong"><strong>    noise_code1 = np.random.normal(scale=0.5, size=[16, 1])</strong></span>
<span class="strong"><strong>    noise_code2 = np.random.normal(scale=0.5, size=[16, 1])</strong></span>
    # number of elements in train dataset
    train_size = x_train.shape[0]
    print(model_name,
          "Labels for generated images: ",
          np.argmax(noise_label, axis=1))

    for i in range(train_steps):
        # train the discriminator for 1 batch
        # 1 batch of real (label=1.0) and fake images (label=0.0)
        # randomly pick real images and corresponding labels from dataset 
        rand_indexes = np.random.randint(0, train_size, size=batch_size)
        real_images = x_train[rand_indexes]
        real_labels = y_train[rand_indexes]
<span class="strong"><strong>        # random codes for real images</strong></span>
<span class="strong"><strong>        real_code1 = np.random.normal(scale=0.5, size=[batch_size, 1])</strong></span>
<span class="strong"><strong>        real_code2 = np.random.normal(scale=0.5, size=[batch_size, 1])</strong></span>
        # generate fake images, labels and codes
        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])
        fake_labels = np.eye(num_labels)[np.random.choice(num_labels,
                                                          batch_size)]
<span class="strong"><strong>        fake_code1 = np.random.normal(scale=0.5, size=[batch_size, 1])</strong></span>
<span class="strong"><strong>        fake_code2 = np.random.normal(scale=0.5, size=[batch_size, 1])</strong></span>
<span class="strong"><strong>        inputs = [noise, fake_labels, fake_code1, fake_code2]</strong></span>
        fake_images = generator.predict(inputs)

        # real + fake images = 1 batch of train data
        x = np.concatenate((real_images, fake_images))
        labels = np.concatenate((real_labels, fake_labels))
<span class="strong"><strong>        codes1 = np.concatenate((real_code1, fake_code1))</strong></span>
<span class="strong"><strong>        codes2 = np.concatenate((real_code2, fake_code2))</strong></span>

        # label real and fake images
        # real images label is 1.0
        y = np.ones([2 * batch_size, 1])
        # fake images label is 0.0
        y[batch_size:, :] = 0

<span class="strong"><strong>        # train discriminator network, log the loss and label accuracy</strong></span>
<span class="strong"><strong>        outputs = [y, labels, codes1, codes2]</strong></span>
<span class="strong"><strong>        # metrics = ['loss', 'activation_1_loss', 'label_loss',</strong></span>
<span class="strong"><strong>        # 'code1_loss', 'code2_loss', 'activation_1_acc',</strong></span>
<span class="strong"><strong>        # 'label_acc', 'code1_acc', 'code2_acc']</strong></span>
<span class="strong"><strong>        # from discriminator.metrics_names</strong></span>
<span class="strong"><strong>        metrics = discriminator.train_on_batch(x, outputs)</strong></span>
<span class="strong"><strong>        fmt = "%d: [discriminator loss: %f, label_acc: %f]"</strong></span>
<span class="strong"><strong>        log = fmt % (i, metrics[0], metrics[6])</strong></span>

        # train the adversarial network for 1 batch
        # 1 batch of fake images with label=1.0 and
        # corresponding one-hot label or class + random codes
        # since the discriminator weights are frozen in 
        # adversarial network only the generator is trained
        # generate fake images, labels and codes
        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])
        fake_labels = np.eye(num_labels)[np.random.choice(num_labels,
                                                          batch_size)]
<span class="strong"><strong>        fake_code1 = np.random.normal(scale=0.5, size=[batch_size, 1])</strong></span>
<span class="strong"><strong>        fake_code2 = np.random.normal(scale=0.5, size=[batch_size, 1])</strong></span>
        # label fake images as real
        y = np.ones([batch_size, 1])

        # note that unlike in discriminator training, 
        # we do not save the fake images in a variable
        # the fake images go to the discriminator input of the 
        # adversarial for classification
        # log the loss and label accuracy
<span class="strong"><strong>        inputs = [noise, fake_labels, fake_code1, fake_code2]</strong></span>
<span class="strong"><strong>        outputs = [y, fake_labels, fake_code1, fake_code2]</strong></span>
<span class="strong"><strong>        metrics  = adversarial.train_on_batch(inputs, outputs)</strong></span>
<span class="strong"><strong>        fmt = "%s [adversarial loss: %f, label_acc: %f]"</strong></span>
<span class="strong"><strong>        log = fmt % (log, metrics[0], metrics[6])</strong></span>

        print(log)
        if (i + 1) % save_interval == 0:
            if (i + 1) == train_steps:
                show = True
            else:
                show = False

            # plot generator images on a periodic basis
            gan.plot_images(generator,
                            noise_input=noise_input,
                            noise_label=noise_label,
                            noise_codes=[noise_code1, noise_code2],
                            show=show,
                            step=(i + 1),
                            model_name=model_name)

    # save the model after training the generator
    # the trained generator can be reloaded for
    # future MNIST digit generation
    generator.save(model_name + ".h5")</pre></div></div>
<div class="section" title="Generator outputs of InfoGAN"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec37"/>Generator outputs of InfoGAN</h1></div></div></div><p>Similar to all previous GANs that have been presented to us, we've trained InfoGAN for 40,000 steps. After the<a id="id261" class="indexterm"/> training is completed, we're able to run the InfoGAN generator to generate new outputs using the model saved on the <code class="literal">infogan_mnist.h5</code> file. The following validations are conducted:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Generate digits 0 to 9 by varying the discrete labels from 0 to 9. Both continuous codes are set to zero. The results are shown in <span class="emphasis"><em>Figure 6.1.5</em></span>. We can see that the InfoGAN discrete code can control the digits produced by the generator:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 infogan-mnist-6.1.1.py --generator=infogan_mnist.h5 --digit=0 --code1=0 --code2=0</strong></span></pre></div><p>              to</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 infogan-mnist-6.1.1.py --generator=infogan_mnist.h5 --digit=9 --code1=0 --code2=0</strong></span>
</pre></div></li><li class="listitem">Examine the effect of the first continuous code to understand which attribute has been affected. We vary the first continuous code from -2.0 to 2.0 for digits 0 to 9. The second continuous code is set to 0.0. <span class="emphasis"><em>Figure 6.1.6</em></span> shows that the first continuous code controls the thickness of the digit:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 infogan-mnist-6.1.1.py --generator=infogan_mnist.h5 --digit=0 --code1=0 --code2=0 --p1</strong></span>
</pre></div></li><li class="listitem">Similar to the previous step, but instead focusing more on the second continuous code. <span class="emphasis"><em>Figure 6.1.7</em></span> shows that the second continuous code controls the rotation angle (tilt) of the writing style:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 infogan-mnist-6.1.1.py --generator=infogan_mnist.h5 --digit=0 --code1=0 --code2=0 --p2</strong></span>
</pre></div></li></ol></div><div class="mediaobject"><img src="graphics/B08956_06_06.jpg" alt="Generator outputs of InfoGAN"/><div class="caption"><p>Figure 6.1.5: The images generated by the InfoGAN as the discrete code is varied from 0 to 9. Both continuous codes are set to zero.</p></div></div><div class="mediaobject"><img src="graphics/B08956_06_07.jpg" alt="Generator outputs of InfoGAN"/><div class="caption"><p>Figure 6.1.6: The images generated by InfoGAN as the first continuous code is varied from -2.0 to 2.0 for digits 0 to 9. The second continuous code is set to zero. The first continuous code controls the thickness of the digit.</p></div></div><div class="mediaobject"><img src="graphics/B08956_06_08.jpg" alt="Generator outputs of InfoGAN"/><div class="caption"><p>Figure 6.1.7: The images generated by InfoGAN as the second continuous code is varied from -2.0 to 2.0 for digits 0 to 9. The first continuous code is set to zero. The second continuous code controls the rotation angle (tilt) of the writing style.</p></div></div><p>From these<a id="id262" class="indexterm"/> validation results, we can see that apart from the ability to generate MNIST looking digits, InfoGAN expanded the ability of conditional GANs such as CGAN and ACGAN. The network automatically learned two arbitrary codes that can control the specific attributes of the generator output. It would be interesting to see what additional attributes could be controlled if we increased the number of continuous codes beyond 2.</p></div>
<div class="section" title="StackedGAN"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec38"/>StackedGAN</h1></div></div></div><p>In the same spirit as InfoGAN, StackedGAN proposes a method for disentangling latent representations for<a id="id263" class="indexterm"/> conditioning generator outputs. However, StackedGAN uses a different approach to the problem. Instead of learning how to condition the noise to produce the desired output, StackedGAN breaks down a GAN into a stack of GANs. Each GAN is trained independently in the usual discriminator-adversarial manner with its own latent code.</p><p>
<span class="emphasis"><em>Figure 6.2.1</em></span> shows us how StackedGAN works in the context of the hypothetical celebrity face generation. Assuming that the <span class="emphasis"><em>Encoder</em></span> network is trained to classify celebrity faces.</p><p>The <span class="emphasis"><em>Encoder</em></span> network is made of a stack of simple encoders, <span class="emphasis"><em>Encoder</em></span>
<span class="emphasis"><em>i</em></span>
<span class="emphasis"><em> where i = 0 … n - 1</em></span> corresponding to <span class="emphasis"><em>n</em></span> features. Each encoder extracts certain facial features. For example, <span class="emphasis"><em>Encoder</em></span><sub>0</sub> may be the encoder for hairstyle features, <span class="emphasis"><em>Features</em></span>1. All the simple encoders contribute to making the overall <span class="emphasis"><em>Encoder</em></span> perform correct predictions.</p><p>The idea behind StackedGAN is that if we would like to build a GAN that generates fake celebrity faces, we should simply invert the <span class="emphasis"><em>Encoder</em></span>. StackedGAN are made of a stack of simpler GANs, GAN<sub>i</sub> where i = 0 … <span class="emphasis"><em>n</em></span> - 1 corresponding to <span class="emphasis"><em>n</em></span> features. Each GAN<sub>i</sub> learns to invert<a id="id264" class="indexterm"/> the process of its corresponding encoder, <span class="emphasis"><em>Encoder</em></span>
<sub>i</sub>. For example, <span class="emphasis"><em>GAN</em></span><sub>0</sub> generates fake celebrity faces from fake hairstyle features which is the inverse of the <span class="emphasis"><em>Encoder</em></span><sub>0</sub> process.</p><p>Each <span class="emphasis"><em>GAN</em></span>
<sub>i</sub> uses a latent code, <span class="emphasis"><em>z</em></span>
<sub>i</sub>, that conditions its generator output. For example, the latent code, <span class="emphasis"><em>z</em></span><sub>0</sub>, can alter the hairstyle from curly to wavy. The stack of GANs can also act as one to synthesize fake celebrity faces, completing the inverse process of the whole <span class="emphasis"><em>Encoder</em></span>. The latent code of each <span class="emphasis"><em>GAN</em></span><sub>i</sub>, <span class="emphasis"><em>z</em></span>
<sub>i</sub>, can be used to alter specific attributes of fake celebrity faces:</p><div class="mediaobject"><img src="graphics/B08956_06_09.jpg" alt="StackedGAN"/><div class="caption"><p>Figure 6.2.1: The basic idea of StackedGAN in the context of celebrity faces generation. Assuming that there is a hypothetical deep encoder network that can perform classification on celebrity faces, a StackedGAN simply inverts the process of the encoder.</p></div></div></div>
<div class="section" title="Implementation of StackedGAN in Keras"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec39"/>Implementation of StackedGAN in Keras</h1></div></div></div><p>The detailed network<a id="id265" class="indexterm"/> model of StackedGAN can be seen in the following figure. For conciseness, only two encoder-GANs per stack are shown. The figure may initially appear complex, but it is just a repetition of an encoder-GAN. Meaning that if we understood how to train one encoder-GAN, the rest uses the same concept. In the following section, we assume that the StackedGAN is designed for the MNIST digit generation:</p><div class="mediaobject"><img src="graphics/B08956_06_10.jpg" alt="Implementation of StackedGAN in Keras"/><div class="caption"><p>Figure 6.2.2: A StackedGAN is made of a stack of an encoder and GAN. The encoder is pre-trained to perform classification. <span class="emphasis"><em>Generator</em></span><sub>1</sub>, <span class="emphasis"><em>G</em></span><sub>1</sub>, learns to synthesize <span class="emphasis"><em>f</em></span><sub>1</sub><sub>f</sub> features conditioned on the fake label, <span class="emphasis"><em>y</em></span>
<sub>f</sub>, and latent code, <span class="emphasis"><em>z</em></span><sub>1</sub><sub>f</sub>. <span class="emphasis"><em>Generator</em></span><sub>0</sub>, <span class="emphasis"><em>G</em></span><sub>0</sub>, produces fake images using both the fake features, <span class="emphasis"><em>f</em></span><sub>1</sub><sub>f</sub> and latent code, <span class="emphasis"><em>z</em></span><sub>0</sub><sub>f</sub>.</p></div></div><p>StackedGAN starts<a id="id266" class="indexterm"/> with an <span class="emphasis"><em>Encoder</em></span>. It could be a trained classifier that predicts the correct labels. The intermediate features vector, <span class="emphasis"><em>f</em></span><sub>1</sub><sub>r</sub>, is made available for GAN training. For MNIST, we can use a CNN-based classifier similar to what we discussed in <a class="link" href="ch01.html" title="Chapter 1. Introducing Advanced Deep Learning with Keras">Chapter 1</a>, <span class="emphasis"><em>Introducing Advanced Deep Learning with Keras</em></span>. Following figure shows the <span class="emphasis"><em>Encoder</em></span> and its network model implementation in Keras:</p><div class="mediaobject"><img src="graphics/B08956_06_11.jpg" alt="Implementation of StackedGAN in Keras"/><div class="caption"><p>Figure 6.2.3: The encoder in StackedGAN is a simple CNN-based classifier</p></div></div><p>
<span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>6.2.1</em></span> shows the Keras code for preceding figure. It is similar to the CNN-based classifier in <a class="link" href="ch01.html" title="Chapter 1. Introducing Advanced Deep Learning with Keras">Chapter 1</a>, <span class="emphasis"><em>Introducing Advanced Deep Learning with Keras</em></span> except that we use a <code class="literal">Dense</code> layer to extract the 256-dim feature. There are two output models, <span class="emphasis"><em>Encoder</em></span><sub>0</sub> and <span class="emphasis"><em>Encoder</em></span><sub>1</sub>. Both will be used to train the StackedGAN.</p><p>The <span class="emphasis"><em>Encoder</em></span><sub>0</sub> output, <span class="emphasis"><em>f</em></span><sub>0</sub><sub>r</sub>, is the 256-dim feature vector that we want <span class="emphasis"><em>Generator</em></span><sub>1</sub> to learn to synthesize. It is available as an auxiliary output of <span class="emphasis"><em>Encoder</em></span><sub>0</sub>, <span class="emphasis"><em>E</em></span><sub>0</sub>. The overall <span class="emphasis"><em>Encoder</em></span> is trained to classify MNIST digits, <span class="emphasis"><em>x</em></span>
<sub>r</sub>. The correct labels, <span class="emphasis"><em>y</em></span>
<sub>r</sub>, are predicted by <span class="emphasis"><em>Encoder</em></span><sub>1</sub>, <span class="emphasis"><em>E</em></span><sub>1</sub>. In the process, the<a id="id267" class="indexterm"/> intermediate set of features, <span class="emphasis"><em>f</em></span><sub>1</sub><span class="emphasis"><em>r</em></span>, is learned and made available for <span class="emphasis"><em>Generator</em></span><sub>0</sub> training. Subscript <span class="emphasis"><em>r</em></span> is used to emphasize and distinguish real data from fake data when the GAN is trained against this encoder.</p><p>Listing 6.2.1, <code class="literal">stackedgan-mnist-6.2.1.py</code> shows encoder implementation in Keras:</p><div class="informalexample"><pre class="programlisting">def build_encoder(inputs, num_labels=10, feature1_dim=256):
    """ Build the Classifier (Encoder) Model sub networks

    Two sub networks: 
    1) Encoder0: Image to feature1 (intermediate latent feature)
    2) Encoder1: feature1 to labels

    # Arguments
        inputs (Layers): x - images, feature1 - feature1 layer output
        num_labels (int): number of class labels
        feature1_dim (int): feature1 dimensionality

    # Returns
        enc0, enc1 (Models): Description below 
    """
    kernel_size = 3
    filters = 64

    x, feature1 = inputs
    # Encoder0 or enc0
    y = Conv2D(filters=filters,
               kernel_size=kernel_size,
               padding='same',
               activation='relu')(x)
    y = MaxPooling2D()(y)
    y = Conv2D(filters=filters,
               kernel_size=kernel_size,
               padding='same',
               activation='relu')(y)
    y = MaxPooling2D()(y)
    y = Flatten()(y)
    feature1_output = Dense(feature1_dim, activation='relu')(y)
    # Encoder0 or enc0: image to feature1 
    enc0 = Model(inputs=x, outputs=feature1_output, name="encoder0")

    # Encoder1 or enc1
    y = Dense(num_labels)(feature1)
    labels = Activation('softmax')(y)
    # Encoder1 or enc1: feature1 to class labels
    enc1 = Model(inputs=feature1, outputs=labels, name="encoder1")

    # return both enc0 and enc1
    return enc0, enc1</pre></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Network</p>
</th><th style="text-align: left" valign="bottom">
<p>Loss Functions</p>
</th><th style="text-align: left" valign="bottom">
<p>Number</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>GAN</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_06_028.jpg" alt="Implementation of StackedGAN in Keras"/></div>
<div class="mediaobject"><img src="graphics/B08956_06_029.jpg" alt="Implementation of StackedGAN in Keras"/></div>
</td><td style="text-align: left" valign="top">
<p>4.1.1</p>
<p>4.1.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>StackedGAN</p>
</td><td style="text-align: left" valign="top">
<div class="mediaobject"><img src="graphics/B08956_06_030.jpg" alt="Implementation of StackedGAN in Keras"/></div>
<div class="mediaobject"><img src="graphics/B08956_06_031.jpg" alt="Implementation of StackedGAN in Keras"/></div>
<div class="mediaobject"><img src="graphics/B08956_06_032.jpg" alt="Implementation of StackedGAN in Keras"/></div>
<div class="mediaobject"><img src="graphics/B08956_06_033.jpg" alt="Implementation of StackedGAN in Keras"/></div>
<div class="mediaobject"><img src="graphics/B08956_06_034.jpg" alt="Implementation of StackedGAN in Keras"/></div>
<p>where <span class="inlinemediaobject"><img src="graphics/B08956_06_035.jpg" alt="Implementation of StackedGAN in Keras"/></span> are weights and</p>
<div class="mediaobject"><img src="graphics/B08956_06_036.jpg" alt="Implementation of StackedGAN in Keras"/></div>
</td><td style="text-align: left" valign="top">
<p>6.2.1</p>
<p>6.2.2</p>
<p>6.2.3</p>
<p>6.2.4</p>
<p>6.2.5</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p>Table 6.2.1: A comparison between the loss functions of GAN and StackedGAN. ~<span class="emphasis"><em>p</em></span>
<sub>data</sub> means sampling from the corresponding encoder data (input, feature or output).</p></blockquote></div><p>Given the <span class="emphasis"><em>Encoder</em></span> inputs (<span class="emphasis"><em>x</em></span><sub>r</sub>) intermediate features (<span class="emphasis"><em>f</em></span>1<span class="emphasis"><em>r</em></span>) and labels (<span class="emphasis"><em>y</em></span>
<span class="emphasis"><em>r</em></span>), each GAN is trained in<a id="id268" class="indexterm"/> the usual discriminator–adversarial manner. The loss functions are given by <span class="emphasis"><em>Equation </em></span><span class="emphasis"><em>6.2.1</em></span> to <span class="emphasis"><em>6.2.5</em></span> in <span class="emphasis"><em>Table 6.2.1</em></span>. Equations <span class="emphasis"><em>6.2.1</em></span> and <span class="emphasis"><em>6.2.2</em></span> are the usual loss functions of the generic GAN. StackedGAN has two additional loss functions, <span class="strong"><strong>Conditional</strong></span> and <span class="strong"><strong>Entropy</strong></span>.</p><p>The conditional loss<a id="id269" class="indexterm"/> function, </p><div class="mediaobject"><img src="graphics/B08956_06_037.jpg" alt="Implementation of StackedGAN in Keras"/></div><p> in <span class="emphasis"><em>Equation 6.2.3</em></span>, ensures that the<a id="id270" class="indexterm"/> generator does<a id="id271" class="indexterm"/> not ignore the input, <span class="emphasis"><em>f</em></span><sub>i+1</sub>, when synthesizing the output, <span class="emphasis"><em>f</em></span><sub>i</sub>, from input noise code <span class="emphasis"><em>z</em></span><sub>i</sub>. The encoder, <span class="emphasis"><em>Encoder</em></span><sub>i</sub>, must be able to<a id="id272" class="indexterm"/> recover the generator input by inverting the process of the generator, <span class="emphasis"><em>Generator</em></span><sub>i</sub>. The difference between the generator input and the recovered input using the encoder is measured by <span class="emphasis"><em>L2</em></span> or Euclidean distance <span class="strong"><strong>Mean Squared Error</strong></span> (<span class="strong"><strong>MSE</strong></span>). <span class="emphasis"><em>Figure 6.2.4</em></span> shows the network elements involved in the computation of <span class="inlinemediaobject"><img src="graphics/B08956_06_038.jpg" alt="Implementation of StackedGAN in Keras"/></span>:</p><div class="mediaobject"><img src="graphics/B08956_06_12.jpg" alt="Implementation of StackedGAN in Keras"/><div class="caption"><p>Figure 6.2.4: A simpler version of Figure 6.2.3 showing only the network elements involved in the computation of <span class="inlinemediaobject"><img src="graphics/B08956_06_039.jpg" alt="Implementation of StackedGAN in Keras"/></span>
</p></div></div><p>The conditional<a id="id273" class="indexterm"/> loss function, however, introduces a new problem for us. The generator ignores the input noise code, <span class="emphasis"><em>z</em></span>
<span class="emphasis"><em>i</em></span> and simply relies on <span class="emphasis"><em>f</em></span>
<sub>i+1</sub>. Entropy loss function, </p><div class="mediaobject"><img src="graphics/B08956_06_40.jpg" alt="Implementation of StackedGAN in Keras"/></div><p> in <span class="emphasis"><em>Equation</em></span> <span class="emphasis"><em>6.2.4</em></span>, ensures<a id="id274" class="indexterm"/> that the generator does not ignore the noise code, <span class="emphasis"><em>z</em></span>
<span class="emphasis"><em>i</em></span>. The <span class="emphasis"><em>Q</em></span>-Network recovers the noise code from the output of the generator. The difference between the recovered noise and the input noise is also measured by <span class="emphasis"><em>L2</em></span> or the MSE. Following figure shows the network elements involved in the computation of </p><div class="mediaobject"><img src="graphics/B08956_06_041.jpg" alt="Implementation of StackedGAN in Keras"/></div><p>:</p><div class="mediaobject"><img src="graphics/B08956_06_13.jpg" alt="Implementation of StackedGAN in Keras"/><div class="caption"><p>Figure 6.2.5: A simpler version of Figure 6.2.3 only showing us the network elements involved in the computation of </p><div class="mediaobject"><img src="graphics/B08956_06_042.jpg" alt="Implementation of StackedGAN in Keras"/></div><p>
</p></div></div><p>The last loss function is similar to<a id="id275" class="indexterm"/> the usual GAN loss. It's made of a discriminator loss </p><div class="mediaobject"><img src="graphics/B08956_06_043.jpg" alt="Implementation of StackedGAN in Keras"/></div><p> and a generator (through adversarial) loss </p><div class="mediaobject"><img src="graphics/B08956_06_044.jpg" alt="Implementation of StackedGAN in Keras"/></div><p>. Following figure shows us the elements involved in the GAN loss:</p><div class="mediaobject"><img src="graphics/B08956_06_14.jpg" alt="Implementation of StackedGAN in Keras"/><div class="caption"><p>Figure 6.2.6: A simpler version of Figure 6.2.3 showing only the network elements involved in the computation of </p><div class="mediaobject"><img src="graphics/B08956_06_045.jpg" alt="Implementation of StackedGAN in Keras"/></div><p> and </p><div class="mediaobject"><img src="graphics/B08956_06_46.jpg" alt="Implementation of StackedGAN in Keras"/></div><p>
</p></div></div><p>In <span class="emphasis"><em>Equation</em></span> <span class="emphasis"><em>6.2.5</em></span>, the weighted sum of the three generator loss functions is the final generator loss function. In the Keras code that we will present, all the weights are set to 1.0, except for the entropy loss<a id="id276" class="indexterm"/> which is set to 10.0. In <span class="emphasis"><em>Equation 6.2.1</em></span> to <span class="emphasis"><em>Equation 6.2.5</em></span>, <span class="emphasis"><em>i</em></span> refers to the encoder and GAN group id or level. In the original paper, the network is first trained independently and then jointly. During independent training, the encoder is trained first. During joint training, both real and fake data are used.</p><p>The implementation of the StackedGAN generator and discriminator in Keras requires few changes to provide auxiliary points to access the intermediate features. <span class="emphasis"><em>Figure 6.2.7</em></span> shows the generator Keras model. <span class="emphasis"><em>Listing 6.2.2</em></span> illustrates the function that builds two generators (<code class="literal">gen0</code> and <code class="literal">gen1</code>) corresponding to <span class="emphasis"><em>Generator</em></span>0 and <span class="emphasis"><em>Generator</em></span><sub>1</sub>. The <code class="literal">gen1</code> generator is<a id="id277" class="indexterm"/> made of three <code class="literal">Dense</code> layers with label and the noise code <span class="emphasis"><em>z</em></span>1<span class="emphasis"><em>f</em></span> as inputs. The third layer generates the fake <span class="emphasis"><em>f</em></span><sub>1</sub><span class="emphasis"><em>f</em></span> feature. The <code class="literal">gen0</code> generator is similar to other GAN generators that we've presented and can be instantiated using the generator builder in <code class="literal">gan.py</code>:</p><div class="informalexample"><pre class="programlisting"># gen0: feature1 + z0 to feature0 (image)
gen0 = gan.generator(feature1, image_size, codes=z0)</pre></div><p>The <code class="literal">gen0</code> input is <span class="emphasis"><em>f</em></span><sub>1</sub> features and the noise code <span class="emphasis"><em>z</em></span><sub>0</sub>. The output is the generated fake image, <span class="emphasis"><em>x</em></span><sub><span class="emphasis"><em>f</em></span></sub>:</p><div class="mediaobject"><img src="graphics/B08956_06_15.jpg" alt="Implementation of StackedGAN in Keras"/><div class="caption"><p>Figure 6.2.7: A StackedGAN Generator model in Keras</p></div></div><p>Listing 6.2.2, <code class="literal">stackedgan-mnist-6.2.1.py</code> shows us<a id="id278" class="indexterm"/> generator implementation in Keras:</p><div class="informalexample"><pre class="programlisting">def build_generator(latent_codes, image_size, feature1_dim=256):
    """Build Generator Model sub networks

    Two sub networks: 1) Class and noise to feature1 (intermediate feature)
    2) feature1 to image

    # Arguments
        latent_codes (Layers): discrete code (labels), noise and feature1 features
        image_size (int): Target size of one side (assuming square image)
        feature1_dim (int): feature1 dimensionality

    # Returns
        gen0, gen1 (Models): Description below
    """

    # Latent codes and network parameters
    labels, z0, z1, feature1 = latent_codes
    # image_resize = image_size // 4
    # kernel_size = 5
    # layer_filters = [128, 64, 32, 1]

    # gen1 inputs
    inputs = [labels, z1]      # 10 + 50 = 62-dim
    x = concatenate(inputs, axis=1)
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    fake_feature1 = Dense(feature1_dim, activation='relu')(x)
    # gen1: classes and noise (feature2 + z1) to feature1
    gen1 = Model(inputs, fake_feature1, name='gen1')

    # gen0: feature1 + z0 to feature0 (image)
    gen0 = gan.generator(feature1, image_size, codes=z0)

    return gen0, gen1</pre></div><p>
<span class="emphasis"><em>Figure 6.2.8</em></span> shows the discriminator Keras model. We provide the functions to build <span class="emphasis"><em>Discriminator</em></span><sub>0</sub> and <span class="emphasis"><em>Discriminator</em></span><sub>1</sub> (<code class="literal">dis0</code> and <code class="literal">dis1</code>).The <code class="literal">dis0</code> discriminator is similar to a GAN discriminator except for<a id="id279" class="indexterm"/> the feature vector input and the auxiliary network <span class="emphasis"><em>Q</em></span><sub>0</sub> that recovers <span class="emphasis"><em>z</em></span><sub>0</sub>. The builder function in <code class="literal">gan.py</code> is used to create <code class="literal">dis0</code>:</p><div class="informalexample"><pre class="programlisting">dis0 = gan.discriminator(inputs, num_codes=z_dim)</pre></div><p>The <code class="literal">dis1</code> discriminator is made of a three-layer MLP as shown in <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>6.2.3</em></span>. The last layer discriminates<a id="id280" class="indexterm"/> between the real and fake <span class="emphasis"><em>f</em></span><sub>1</sub>. <span class="emphasis"><em>Q</em></span><sub>1</sub> network shares the first two layers of <code class="literal">dis1</code>. Its third layer recovers <span class="emphasis"><em>z</em></span><sub>1</sub>:</p><div class="mediaobject"><img src="graphics/B08956_06_16.jpg" alt="Implementation of StackedGAN in Keras"/><div class="caption"><p>Figure 6.2.8: A StackedGAN Discriminator model in Keras</p></div></div><p>Listing 6.2.3, <code class="literal">stackedgan-mnist-6.2.1.py</code> shows the <span class="emphasis"><em>Discriminator</em></span><sub>1</sub> implementation in Keras:</p><div class="informalexample"><pre class="programlisting">def build_discriminator(inputs, z_dim=50):
    """Build Discriminator 1 Model

    Classifies feature1 (features) as real/fake image and recovers
    the input noise or latent code (by minimizing entropy loss)

    # Arguments
        inputs (Layer): feature1
        z_dim (int): noise dimensionality

    # Returns
        dis1 (Model): feature1 as real/fake and recovered latent code
    """

    # input is 256-dim feature1
    x = Dense(256, activation='relu')(inputs)
    x = Dense(256, activation='relu')(x)

    # first output is probability that feature1 is real
    f1_source = Dense(1)(x)
    f1_source = Activation('sigmoid', name='feature1_source')(f1_source)

    # z1 reonstruction (Q1 network)
    z1_recon = Dense(z_dim)(x)
    z1_recon = Activation('tanh', name='z1')(z1_recon)

    discriminator_outputs = [f1_source, z1_recon]
    dis1 = Model(inputs, discriminator_outputs, name='dis1')
    return dis1 </pre></div><p>With all builder<a id="id281" class="indexterm"/> functions available, StackedGAN is assembled in <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>6.2.4</em></span>. Before training StackedGAN, the encoder is pretrained. Note that we already incorporated the three generator loss functions (adversarial, conditional, and entropy) in the adversarial model training. The <span class="emphasis"><em>Q</em></span>-Network shares some common layers with the discriminator model. Therefore, its loss function is also incorporated in the discriminator model training.</p><p>Listing 6.2.4, <code class="literal">stackedgan-mnist-6.2.1.py</code>. Building StackedGAN in Keras:</p><div class="informalexample"><pre class="programlisting">def build_and_train_models():
    # load MNIST dataset
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    # reshape and normalize images
    image_size = x_train.shape[1]
    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])
    x_train = x_train.astype('float32') / 255

    x_test = np.reshape(x_test, [-1, image_size, image_size, 1])
    x_test = x_test.astype('float32') / 255

    # number of labels
    num_labels = len(np.unique(y_train))
    # to one-hot vector
    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    model_name = "stackedgan_mnist"
    # network parameters
    batch_size = 64
    train_steps = 40000
    lr = 2e-4
    decay = 6e-8
    input_shape = (image_size, image_size, 1)
    label_shape = (num_labels, )
    z_dim = 50
    z_shape = (z_dim, )
    feature1_dim = 256
    feature1_shape = (feature1_dim, )

    # build discriminator 0 and Q network 0 models
    inputs = Input(shape=input_shape, name='discriminator0_input')
    dis0 = gan.discriminator(inputs, num_codes=z_dim)
    # [1] uses Adam, but discriminator converges easily with RMSprop
    optimizer = RMSprop(lr=lr, decay=decay)
    # loss fuctions: 1) probability image is real (adversarial0 loss)
    # 2) MSE z0 recon loss (Q0 network loss or entropy0 loss)
    loss = ['binary_crossentropy', 'mse']
    loss_weights = [1.0, 10.0]
    dis0.compile(loss=loss,
                 loss_weights=loss_weights,
                 optimizer=optimizer,
                 metrics=['accuracy'])
    dis0.summary() # image discriminator, z0 estimator 

    # build discriminator 1 and Q network 1 models
    input_shape = (feature1_dim, )
    inputs = Input(shape=input_shape, name='discriminator1_input')
    dis1 = build_discriminator(inputs, z_dim=z_dim )
    # loss fuctions: 1) probability feature1 is real (adversarial1 loss)
    # 2) MSE z1 recon loss (Q1 network loss or entropy1 loss)
    loss = ['binary_crossentropy', 'mse']
    loss_weights = [1.0, 1.0]
    dis1.compile(loss=loss,
                 loss_weights=loss_weights,
                 optimizer=optimizer,
                 metrics=['accuracy'])
    dis1.summary() # feature1 discriminator, z1 estimator

    # build generator models
    feature1 = Input(shape=feature1_shape, name='feature1_input')
    labels = Input(shape=label_shape, name='labels')
    z1 = Input(shape=z_shape, name="z1_input")
    z0 = Input(shape=z_shape, name="z0_input")
    latent_codes = (labels, z0, z1, feature1)
    gen0, gen1 = build_generator(latent_codes, image_size)
    gen0.summary() # image generator 
    gen1.summary() # feature1 generator

    # build encoder models
    input_shape = (image_size, image_size, 1)
    inputs = Input(shape=input_shape, name='encoder_input')
    enc0, enc1 = build_encoder((inputs, feature1), num_labels)
    enc0.summary() # image to feature1 encoder
    enc1.summary() # feature1 to labels encoder (classifier)
    encoder = Model(inputs, enc1(enc0(inputs)))
    encoder.summary() # image to labels encoder (classifier)

    data = (x_train, y_train), (x_test, y_test)
    train_encoder(encoder, data, model_name=model_name)

    # build adversarial0 model =
    # generator0 + discriminator0 + encoder0
    optimizer = RMSprop(lr=lr*0.5, decay=decay*0.5)
    # encoder0 weights frozen
    enc0.trainable = False
    # discriminator0 weights frozen
    dis0.trainable = False
    gen0_inputs = [feature1, z0]
    gen0_outputs = gen0(gen0_inputs)
    adv0_outputs = dis0(gen0_outputs) + [enc0(gen0_outputs)]
    # feature1 + z0 to prob feature1 is 
    # real + z0 recon + feature0/image recon
    adv0 = Model(gen0_inputs, adv0_outputs, name="adv0")
    # loss functions: 1) prob feature1 is real (adversarial0 loss)
    # 2) Q network 0 loss (entropy0 loss)
    # 3) conditional0 loss
    loss = ['binary_crossentropy', 'mse', 'mse']
    loss_weights = [1.0, 10.0, 1.0]
    adv0.compile(loss=loss,
                 loss_weights=loss_weights,
                 optimizer=optimizer,
                 metrics=['accuracy'])
    adv0.summary()

    # build adversarial1 model = 
    # generator1 + discriminator1 + encoder1
    # encoder1 weights frozen
    enc1.trainable = False
    # discriminator1 weights frozen
    dis1.trainable = False
    gen1_inputs = [labels, z1]
    gen1_outputs = gen1(gen1_inputs)
    adv1_outputs = dis1(gen1_outputs) + [enc1(gen1_outputs)]
    # labels + z1 to prob labels are real + z1 recon + feature1 recon
    adv1 = Model(gen1_inputs, adv1_outputs, name="adv1")
    # loss functions: 1) prob labels are real (adversarial1 loss)
    # 2) Q network 1 loss (entropy1 loss)
    # 3) conditional1 loss (classifier error)
    loss_weights = [1.0, 1.0, 1.0]
    loss = ['binary_crossentropy', 'mse', 'categorical_crossentropy']
    adv1.compile(loss=loss,
                 loss_weights=loss_weights,
                 optimizer=optimizer,
                 metrics=['accuracy'])
    adv1.summary()

    # train discriminator and adversarial networks
    models = (enc0, enc1, gen0, gen1, dis0, dis1, adv0, adv1)
    params = (batch_size, train_steps, num_labels, z_dim, model_name)
    train(models, data, params)</pre></div><p>Finally, the training<a id="id282" class="indexterm"/> function bears a resemblance to a typical GAN training except that we only train one GAN at a time (that is, <span class="emphasis"><em>GAN</em></span><sub>1</sub> then <span class="emphasis"><em>GAN</em></span><sub>0</sub>). The code is shown in <span class="emphasis"><em>Listing</em></span> <span class="emphasis"><em>6.2.5</em></span>. It's worth noting that the training sequence is:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="emphasis"><em>Discriminator</em></span><sub>1</sub> and <span class="emphasis"><em>Q</em></span><sub>1</sub> networks by minimizing the discriminator and entropy losses</li><li class="listitem"><span class="emphasis"><em>Discriminator</em></span><sub>0</sub> and <span class="emphasis"><em>Q</em></span><sub>0</sub> networks by minimizing the discriminator and entropy losses</li><li class="listitem"><span class="emphasis"><em>Adversarial</em></span><sub>1</sub> network by minimizing the adversarial, entropy, and conditional losses</li><li class="listitem"><span class="emphasis"><em>Adversarial</em></span><sub>0</sub> network by minimizing the adversarial, entropy, and conditional losses</li></ol></div><p>Listing 6.2.5, <code class="literal">stackedgan-mnist-6.2.1.py</code> shows us training<a id="id283" class="indexterm"/> the StackedGAN in Keras:</p><div class="informalexample"><pre class="programlisting">def train(models, data, params):
    """Train the discriminator and adversarial Networks

    Alternately train discriminator and adversarial networks by batch.
    Discriminator is trained first with real and fake images,
    corresponding one-hot labels and latent codes.
    Adversarial is trained next with fake images pretending to be real,
    corresponding one-hot labels and latent codes.
    Generate sample images per save_interval.

    # Arguments
        models (Models): Encoder, Generator, Discriminator, Adversarial models
        data (tuple): x_train, y_train data
        params (tuple): Network parameters

    """
    # the StackedGAN and Encoder models
    enc0, enc1, gen0, gen1, dis0, dis1, adv0, adv1 = models
    # network parameters
    batch_size, train_steps, num_labels, z_dim, model_name = params
    # train dataset
    (x_train, y_train), (_, _) = data
    # the generator image is saved every 500 steps
    save_interval = 500

    # label and noise codes for generator testing
    z0 = np.random.normal(scale=0.5, size=[16, z_dim])
    z1 = np.random.normal(scale=0.5, size=[16, z_dim])
    noise_class = np.eye(num_labels)[np.arange(0, 16) % num_labels]
    noise_params = [noise_class, z0, z1]
    # number of elements in train dataset
    train_size = x_train.shape[0]
    print(model_name,
          "Labels for generated images: ",
          np.argmax(noise_class, axis=1))

    for i in range(train_steps):
        # train the discriminator1 for 1 batch
        # 1 batch of real (label=1.0) and fake feature1 (label=0.0)
        # randomly pick real images from dataset
        rand_indexes = np.random.randint(0, train_size, size=batch_size)
        real_images = x_train[rand_indexes]
        # real feature1 from encoder0 output
        real_feature1 = enc0.predict(real_images)
        # generate random 50-dim z1 latent code
        real_z1 = np.random.normal(scale=0.5, size=[batch_size, z_dim])
        # real labels from dataset
        real_labels = y_train[rand_indexes]

        # generate fake feature1 using generator1 from
        # real labels and 50-dim z1 latent code
        fake_z1 = np.random.normal(scale=0.5, size=[batch_size, z_dim])
        fake_feature1 = gen1.predict([real_labels, fake_z1])

        # real + fake data
        feature1 = np.concatenate((real_feature1, fake_feature1))
        z1 = np.concatenate((fake_z1, fake_z1))

        # label 1st half as real and 2nd half as fake
        y = np.ones([2 * batch_size, 1])
        y[batch_size:, :] = 0

        # train discriminator1 to classify feature1 
        # as real/fake and recover
        # latent code (z1). real = from encoder1, 
        # fake = from genenerator1 
        # joint training using discriminator part of advserial1 loss
        # and entropy1 loss
        metrics = dis1.train_on_batch(feature1, [y, z1])
        # log the overall loss only (fr dis1.metrics_names)
        log = "%d: [dis1_loss: %f]" % (i, metrics[0])


        # train the discriminator0 for 1 batch
        # 1 batch of real (label=1.0) and fake images (label=0.0)
        # generate random 50-dim z0 latent code
        fake_z0 = np.random.normal(scale=0.5, size=[batch_size, z_dim])
        # generate fake images from real feature1 and fake z0
        fake_images = gen0.predict([real_feature1, fake_z0])

        # real + fake data
        x = np.concatenate((real_images, fake_images))
        z0 = np.concatenate((fake_z0, fake_z0))

        # train discriminator0 to classify image as real/fake and recover
        # latent code (z0)
        # joint training using discriminator part of advserial0 loss
        # and entropy0 loss
        metrics = dis0.train_on_batch(x, [y, z0])
        # log the overall loss only (fr dis0.metrics_names)
        log = "%s [dis0_loss: %f]" % (log, metrics[0])

        # adversarial training 
        # generate fake z1, labels
        fake_z1 = np.random.normal(scale=0.5, size=[batch_size, z_dim])
        # input to generator1 is sampling fr real labels and
        # 50-dim z1 latent code
        gen1_inputs = [real_labels, fake_z1]

        # label fake feature1 as real
        y = np.ones([batch_size, 1])

        # train generator1 (thru adversarial) by 
        # fooling the discriminator
        # and approximating encoder1 feature1 generator
        # joint training: adversarial1, entropy1, conditional1
        metrics = adv1.train_on_batch(gen1_inputs, [y, fake_z1, real_labels])
        fmt = "%s [adv1_loss: %f, enc1_acc: %f]"
        # log the overall loss and classification accuracy
        log = fmt % (log, metrics[0], metrics[6])


        # input to generator0 is real feature1 and 
        # 50-dim z0 latent code
        fake_z0 = np.random.normal(scale=0.5, size=[batch_size, z_dim])
        gen0_inputs = [real_feature1, fake_z0]

        # train generator0 (thru adversarial) by 
        # fooling the discriminator
        # and approximating encoder1 image source generator
        # joint training: adversarial0, entropy0, conditional0
        metrics = adv0.train_on_batch(gen0_inputs, [y, fake_z0, real_feature1])
        # log the overall loss only
        log = "%s [adv0_loss: %f]" % (log, metrics[0])

        print(log)
        if (i + 1) % save_interval == 0:
            if (i + 1) == train_steps:
                show = True
            else:
                show = False
            generators = (gen0, gen1)
            plot_images(generators,
                        noise_params=noise_params,
                        show=show,
                        step=(i + 1),
                        model_name=model_name)

    # save the modelis after training generator0 &amp; 1
    # the trained generator can be reloaded for
    # future MNIST digit generation
    gen1.save(model_name + "-gen1.h5")
    gen0.save(model_name + "-gen0.h5")</pre></div></div>
<div class="section" title="Generator outputs of StackedGAN"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec40"/>Generator outputs of StackedGAN</h1></div></div></div><p>After training the StackedGAN for 10,000 steps, the <span class="emphasis"><em>Generator</em></span><sub>0</sub> and <span class="emphasis"><em>Generator</em></span><sub>1</sub> models are saved<a id="id284" class="indexterm"/> on files. Stacked together, <span class="emphasis"><em>Generator</em></span><sub>0</sub> and <span class="emphasis"><em>Generator</em></span><sub>1</sub> can synthesize fake images conditioned on label and noise codes, <span class="emphasis"><em>z</em></span><sub>0</sub> and <span class="emphasis"><em>z</em></span><sub>1</sub>.</p><p>The StackedGAN generator can be qualitatively validated by:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Varying the discrete labels from 0 to 9 with both noise codes, <span class="emphasis"><em>z</em></span><sub>0</sub> and <span class="emphasis"><em>z</em></span><sub>1</sub> sampled from<a id="id285" class="indexterm"/> a normal distribution with a mean of 0.5 and standard -deviation of 1.0. The results are shown in <span class="emphasis"><em>Figure 6.2.9</em></span>. We're able to see that the StackedGAN discrete code can control the digits produced by the generator:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 stackedgan-mnist-6.2.1.py </strong></span>
<span class="strong"><strong>--generator0=stackedgan_mnist-gen0.h5 </strong></span>
<span class="strong"><strong>--generator1=stackedgan_mnist-gen1.h5 --digit=0</strong></span>
<span class="strong"><strong>python3 stackedgan-mnist-6.2.1.py </strong></span>
<span class="strong"><strong>--generator0=stackedgan_mnist-gen0.h5 </strong></span>
<span class="strong"><strong>--generator1=stackedgan_mnist-gen1.h5 --digit=9</strong></span>
</pre></div><p>              to</p></li><li class="listitem">Varying the first noise code, <span class="emphasis"><em>z</em></span><sub>0</sub>, as a constant vector from -4.0 to 4.0 for digits 0 to 9 as shown as follows. The second noise code, <span class="emphasis"><em>z</em></span><sub>0</sub>, is set to zero vector. <span class="emphasis"><em>Figure 6.2.10</em></span> shows that the first noise code controls the thickness of the digit. For example, for digit 8:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 stackedgan-mnist-6.2.1.py </strong></span>
<span class="strong"><strong>--generator0=stackedgan_mnist-gen0.h5 </strong></span>
<span class="strong"><strong>--generator1=stackedgan_mnist-gen1.h5 --z0=0 --z1=0 –p0 </strong></span>
<span class="strong"><strong>--digit=8</strong></span>
</pre></div></li><li class="listitem">Varying the second noise code, <span class="emphasis"><em>z</em></span><sub>1</sub>, as a constant vector from -1.0 to 1.0 for digits 0 to 9 shown as follows. The first noise code, <span class="emphasis"><em>z</em></span><sub>0</sub>, is set to zero vector. <span class="emphasis"><em>Figure 6.2.11</em></span> shows that the second noise code controls the rotation (tilt) and to a certain extent the thickness of the digit. For example, for digit 8:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python3 stackedgan-mnist-6.2.1.py </strong></span>
<span class="strong"><strong>--generator0=stackedgan_mnist-gen0.h5 </strong></span>
<span class="strong"><strong>--generator1=stackedgan_mnist-gen1.h5 --z0=0 --z1=0 –p1 </strong></span>
<span class="strong"><strong>--digit=8</strong></span>
</pre></div></li></ol></div><div class="mediaobject"><img src="graphics/B08956_06_17.jpg" alt="Generator outputs of StackedGAN"/><div class="caption"><p>Figure 6.2.9: Images generated by StackedGAN as the discrete code is varied from 0 to 9. Both <span class="inlinemediaobject"><img src="graphics/B08956_06_047.jpg" alt="Generator outputs of StackedGAN"/></span> and <span class="inlinemediaobject"><img src="graphics/B08956_06_048.jpg" alt="Generator outputs of StackedGAN"/></span> have been sampled from a normal distribution with zero mean and 0.5 standard deviation.</p></div></div><div class="mediaobject"><img src="graphics/B08956_06_18.jpg" alt="Generator outputs of StackedGAN"/><div class="caption"><p>Figure 6.2.10: Images generated by using a StackedGAN as the first noise code, <span class="emphasis"><em>z</em></span><sub>0</sub>, varies from constant vector -4.0 to 4.0 for digits 0 to 9. <span class="emphasis"><em>z</em></span><sub>0</sub> appears to control the thickness of each digit.</p></div></div><div class="mediaobject"><img src="graphics/B08956_06_19.jpg" alt="Generator outputs of StackedGAN"/><div class="caption"><p>Figure 6.2.11: The images generated by StackedGAN as the second noise code, <span class="emphasis"><em>z</em></span><sub>1</sub>, varies from constant vector -1.0 to 1.0 for digits 0 to 9. <span class="emphasis"><em>z</em></span><sub>1</sub> appears to control the rotation (tilt) and the thickness of stroke of each digit.</p></div></div><p>
<span class="emphasis"><em>Figures 6.2.9</em></span> to <span class="emphasis"><em>6.2.11</em></span> demonstrate that the StackedGAN has provided additional control on the<a id="id286" class="indexterm"/> attributes of the generator outputs. The control and attributes are (label, which digit), (<span class="emphasis"><em>z</em></span>0, digit thickness), and (<span class="emphasis"><em>z</em></span>1, digit tilt). From this example, there are other possible experiments that we can control such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Increasing the number of elements of the stack from the current 2</li><li class="listitem" style="list-style-type: disc">Decreasing the dimension of codes <span class="emphasis"><em>z</em></span>0 and <span class="emphasis"><em>z</em></span>1, like in InfoGAN</li></ul></div><p>Following figure shows the differences between the latent codes of InfoGAN and StackedGAN. The basic idea of disentangling codes is to put a constraint on the loss functions such that only specific attributes are affected by a code. Structure-wise, InfoGAN are easier to implement when compared to StackedGAN. InfoGAN is also faster to train:</p><div class="mediaobject"><img src="graphics/B08956_06_20.jpg" alt="Generator outputs of StackedGAN"/><div class="caption"><p>Figure 6.2.12: Latent representations for different GANs</p></div></div></div>
<div class="section" title="Conclusion"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec41"/>Conclusion</h1></div></div></div><p>In this chapter, we've discussed how to disentangle the latent representations of GANs. Earlier on in the chapter, we discussed how InfoGAN maximizes<a id="id287" class="indexterm"/> the mutual information in order to force the generator to<a id="id288" class="indexterm"/> learn disentangled latent vectors. In the MNIST dataset example, InfoGAN uses three representations and a noise code as inputs. The noise represents the rest of the attributes in the form of an entangled representation. StackedGAN approaches the problem in a different way. It uses a stack of encoder-GANs to learn how to synthesize fake features and images. The encoder is first trained to provide a dataset of features. Then, the encoder-GANs are trained jointly to learn how to use the noise code to control attributes of the generator output.</p><p>In the next chapter, we will embark on a new type of GAN that is able to generate new data in another domain. For example, given an image of a horse, the GAN can perform an automatic transformation to an image of a zebra. The interesting feature of this type of GAN is that it can be trained without supervision.</p></div>
<div class="section" title="Reference"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec42"/>Reference</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Xi Chen and others. <span class="emphasis"><em>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</em></span>. Advances in Neural Information Processing Systems, 2016(<a class="ulink" href="http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf">http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf</a>).</li><li class="listitem">Xun Huang and others. <span class="emphasis"><em>Stacked Generative Adversarial Networks</em></span>. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Vol. 2, 2017(<a class="ulink" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf</a>).</li></ol></div></div></body></html>