<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Benchmarking and Neural Network Optimization</h1>
                </header>
            
            <article>
                
<p>Benchmarking is a standard against which we compare solutions to find out whether they are good or not. In the context of deep learning, we might set benchmarks for an existing model that is performing pretty well. We might test our model against factors such as accuracy, the amount of data handled, memory consumption, and JVM garbage collection tuning. In this chapter, we briefly talk about the benchmarking possibilities with your DL4J applications. We will start with general guidelines and then move on to more DL4J-specific benchmarking settings. At the end of the chapter, we will look at a hyperparameter tuning example that shows how to find the best neural network parameters in order to yield the best results. </p>
<p class="mce-root">In this chapter, we will cover the following recipes:</p>
<ul>
<li>DL4J/ND4J specific configuration</li>
<li>Setting up heap spaces and garbage collection</li>
<li>Using asynchronous ETL</li>
<li>Using arbiter to monitor neural network behavior</li>
<li>Performing hyperparameter tuning</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="mce-root">The code for this chapter is located at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java</a>.</p>
<p class="mce-root"/>
<p class="mce-root">After cloning our GitHub repository, navigate to the <kbd>Java-Deep-Learning-Cookbook/12_Benchmarking_and_Neural_Network_Optimization/sourceCode</kbd> directory. Then import the <kbd>cookbookapp</kbd> project as a Maven project by importing <kbd>pom.xml</kbd>.</p>
<p class="mce-root">The following are links to two examples:</p>
<ul>
<li class="mce-root">Hyperparameter tuning example: <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java/HyperParameterTuning.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java/HyperParameterTuning.java</a></li>
<li class="mce-root">Arbiter UI example: <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java/HyperParameterTuningArbiterUiExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java/HyperParameterTuningArbiterUiExample.java</a></li>
</ul>
<p>This chapter's examples are based on a customer churn dataset (<a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/resources" target="_blank">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/resources</a>). This dataset is included in the project directory. </p>
<p>Although we are explaining DL4J/ND4J-specific benchmarks in this chapter, it is recommended you follow general benchmarking guidelines. <span>The following</span> some important generic benchmarks that are common for any neural network:</p>
<ul>
<li><strong>Perform warm-up iterations before the actual benchmark task</strong>: Warm-up iterations refer to a set of iterations performed on benchmark tasks before commencing the actual ETL operation or network training. Warm up iterations are important because the execution of the first few iterations will be slow. This can add to the total duration of the benchmark tasks and we could end up with wrong/inconsistent conclusions. The slow execution of the first few iterations may be because of the compilation time taken by JVM, the lazy-loading approach of DL4J/ND4J libraries, or the learning phase of DL4J/ND4J libraries. This learning phase refers to the time taken to learn the memory requirements for execution.</li>
<li><strong>Perform benchmark tasks multiple times</strong>: To make sure that benchmark results are reliable, we need to run benchmark tasks multiple times. The host system may have multiple apps/processes running in parallel apart from the benchmark instance. So, the runtime performance will vary over time. In order to assess this situation, we need to run benchmark tasks multiple times.</li>
<li><strong>Understand where you set the benchmarks and why</strong>: We need to assess whether we are setting the right benchmarking. If we target operation a, then make sure that only operation a is being timed for benchmark. Also, we have to make sure that we are using the right libraries for the right situation. The latest versions of libraries are always preferred. It is also important to assess DL4J/ND4J configurations used in our code. The default configurations may suffice in regular scenarios, but manual configuration may be required for optimal performance. <span>The following</span> some of the default configuration options for reference: 
<ul>
<li>Memory configurations (heap space setup).</li>
<li>Garbage collection and workspace configuration (changing the frequency at which the garbage collector is called).</li>
<li>Add cuDNN support (utilizing a CUDA-powered GPU machine with better performance).</li>
<li>Enable DL4J cache mode (to bring in cache memory for the training instance). This will be a DL4J-specific change. </li>
</ul>
</li>
</ul>
<p>We discussed cuDNN in <a href="f88b350b-16e2-425b-8425-4631187c7803.xhtml" target="_blank">Chapter 1</a>, <em>Introduction to Deep Learning in Java</em>, while we talked about DL4J in GPU environments. These configuration options will be discussed further in upcoming recipes.</p>
<ul>
<li><strong>Run the benchmark on a range of sizes</strong>: It is important to run the benchmark on multiple different input sizes/shapes to get a complete picture of its performance. Mathematical computations such as matrix multiplications vary over different dimensions. </li>
<li><strong>Understand the hardware</strong>: The training instance with the smallest minibatch size will perform better on a CPU than on a GPU system. When we use a large minibatch size, the observation will be exactly the opposite. The training instance will now be able to utilize GPU resources. In the same way, a large layer size can better utilize GPU resources. Writing network configurations without understanding the underlying hardware will not allow us to exploit its full capabilities. </li>
<li><strong>Reproduce the benchmarks and understand their limits</strong>: In order to troubleshoot performance bottlenecks against a set benchmark, we always need to reproduce them. It is helpful to assess the circumstance under which poor performance occurs. On top of that, we also need to understand the limitations put on certain benchmarks. Certain benchmarks set on a specific layer won't tell you anything about the performance factor of other layers. </li>
<li><strong>Avoid common benchmark mistakes</strong>:
<ul>
<li>Consider using the latest version of DL4J/ND4J. To apply the latest performance improvements, try snapshot versions.</li>
<li>Pay attention to the types of native libraries used (such as cuDNN).</li>
<li>Run enough iterations and with a reasonable minibatch size to yield consistent results.</li>
<li>Do not compare results across hardware without accounting for the differences.</li>
</ul>
</li>
</ul>
<p>In order to benefit from the latest fixes for performance issues, you need to have latest version in your local. If you want to run the source on the latest fix and if the new version hasn't been released, then you can make use of snapshot versions. To find out more about working with snapshot versions, go to <a href="https://deeplearning4j.org/docs/latest/deeplearning4j-config-snapshots">https://deeplearning4j.org/docs/latest/deeplearning4j-config-snapshots</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL4J/ND4J-specific configuration</h1>
                </header>
            
            <article>
                
<p>Apart from general benchmarking guidelines, we need to follow additional benchmarking configurations that are DL4J/ND4J-specific. These are important benchmarking configurations that target the hardware and mathematical computations.    </p>
<p>Because ND4J is the JVM computation library for DL4J, benchmarks mostly target mathematical computations. Any benchmarks discussed with regard to ND4J can then also be applied to DL4J. Let's discuss DL4J/ND4J-specific benchmarks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Make sure you have downloaded cudNN from the following link: <a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a>. Install it before attempting to configure it with DL4J. Note that cuDNN doesn't come as a bundle with CUDA. So, adding the CUDA dependency alone will not be enough.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Detach the <kbd>INDArray</kbd> data to use it across workspaces:</li>
</ol>
<pre style="padding-left: 60px">INDArray array = Nd4j.rand(6, 6);<br/> INDArray mean = array.mean(1);<br/> INDArray result = mean.detach();</pre>
<ol start="2">
<li>Remove all workspaces that were created during training/evaluation in case they are running short of RAM:</li>
</ol>
<pre style="padding-left: 60px">Nd4j.getWorkspaceManager().destroyAllWorkspacesForCurrentThread();</pre>
<ol start="3">
<li>Leverage an array instance from another workspace in the current workspace by calling <kbd>leverageTo()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">LayerWorkspaceMgr.leverageTo(ArrayType.ACTIVATIONS, myArray);</pre>
<ol start="4">
<li>Track the time spent on every iteration during training using <kbd>PerformanceListener</kbd>:</li>
</ol>
<pre style="padding-left: 60px">model.setListeners(new PerformanceListener(frequency,reportScore)); </pre>
<ol start="5">
<li>Add the following Maven dependency for cuDNN support:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/>   &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>   &lt;artifactId&gt;deeplearning4j-cuda-x.x&lt;/artifactId&gt; //cuda version to be specified<br/>   &lt;version&gt;1.0.0-beta4&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>
<ol start="6">
<li>Configure DL4J/cuDNN to favor performance over memory:</li>
</ol>
<pre style="padding-left: 60px">MultiLayerNetwork config = new NeuralNetConfiguration.Builder()<br/> .cudnnAlgoMode(ConvolutionLayer.AlgoMode.PREFER_FASTEST) //prefer performance over memory<br/> .build();</pre>
<ol start="7">
<li>Configure <kbd>ParallelWrapper</kbd> to support multi-GPU training/inferences:</li>
</ol>
<pre style="padding-left: 60px">ParallelWrapper wrapper = new ParallelWrapper.Builder(model)<br/> .prefetchBuffer(deviceCount)<br/>.workers(Nd4j.getAffinityManager().getNumberOfDevices())<br/>.trainingMode(ParallelWrapper.TrainingMode.SHARED_GRADIENTS)<br/>.thresholdAlgorithm(new AdaptiveThresholdAlgorithm())<br/> .build();</pre>
<p class="mce-root"/>
<ol start="8">
<li>Configure <kbd>ParallelInference</kbd> as follows:</li>
</ol>
<pre style="padding-left: 60px">ParallelInference inference = new ParallelInference.Builder(model)<br/> .inferenceMode(InferenceMode.BATCHED)<br/>.batchLimit(maxBatchSize)<br/> .workers(workerCount)<br/> .build();<br/> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>A workspace is a memory management model that enables the reuse of memory for cyclic workloads without having to introduce a JVM garbage collector. <kbd>INDArray</kbd> memory content is invalidated once in every workspace loop. Workspaces can be integrated for training or inference. </p>
<p>In step 1, we start with workspace benchmarking. The <kbd>detach()</kbd> method will detach the specific INDArray from the workspace and will return a copy. So, how do we enable workspace modes for our training instance?  Well, if you're using the latest DL4J version (from 1.0.0-alpha onwards), then this feature is enabled by default. We target version 1.0.0-beta 3 in this book.</p>
<p>In step 2, we removed workspaces from the memory, as shown here:</p>
<pre>Nd4j.getWorkspaceManager().destroyAllWorkspacesForCurrentThread();</pre>
<p>This will destroy workspaces from the current running thread only. We can release memory from workspaces in this way by running this piece of code in the thread in question. </p>
<p>DL4J also lets you implement your own workspace manager for layers. For example, activation results from one layer during training can be placed in one workspace, and the results of the inference can be placed in another workspace. This is possible using DL4J's  <kbd>LayerWorkspaceMgr</kbd>, as mentioned in step 3. Make sure that the returned array (<kbd>myArray</kbd> in step 3) is defined as <kbd>ArrayType.ACTIVATIONS</kbd>:</p>
<pre>LayerWorkspaceMgr.create(ArrayType.ACTIVATIONS,myArray);</pre>
<p>It is fine to use different workspace modes for training/inference. But it is recommended you use <kbd>SEPARATE</kbd> mode for training and <kbd>SINGLE</kbd> mode for inference because inference only involves a forward pass and doesn't involve backpropagation. However, for training instances with high resource consumption/memory, it might be better to go for <kbd>SEPARATE</kbd> workspace mode because it consumes less memory. Note that <kbd>SEPARATE</kbd> is the default workspace mode in DL4J.</p>
<p class="mce-root"/>
<p>In step 4, two attributes are used while creating <kbd>PerformanceListener</kbd>: <kbd>reportScore</kbd> and <kbd>frequency</kbd>. <kbd>reportScore</kbd> is a Boolean variable and <kbd>frequency</kbd> is the iteration count by which time needs to be tracked. If <kbd>reportScore</kbd> is <kbd>true</kbd>, then it will report the score (just like in <kbd>ScoreIterationListener</kbd>) along with information on the time spent on each iteration. </p>
<p>In step 7, we used <kbd>ParallelWrapper</kbd> or <kbd>ParallelInference</kbd> for multi-GPU devices. Once we have created a neural network model, we can create a parallel wrapper using it. We specify the count of devices, a training mode, and the number of workers for the parallel wrapper.</p>
<p>We need to make sure that our training instance is cost-effective. It is not feasible to spend a lot adding multiple GPUs and then utilizing one GPU in training. Ideally, we want to utilize all GPU hardware to speed up the training/inference process and get better results. <kbd>ParallelWrapper</kbd> and <kbd>ParallelInference</kbd> serve this purpose.</p>
<p><span>The following</span> some configurations supported by <kbd>ParallelWrapper</kbd> and <kbd>ParallelInference</kbd>:</p>
<ul>
<li><kbd>prefetchBuffer(deviceCount)</kbd>: This parallel wrapper method specifies dataset prefetch options. We mention the number of devices here.</li>
<li><kbd>trainingMode(mode)</kbd>: This parallel wrapper method specifies the distributed training method. <kbd>SHARED_GRADIENTS</kbd> refers to the gradient sharing method for distributed training. </li>
<li><kbd>workers(Nd4j.getAffinityManager().getNumberOfDevices())</kbd>: This parallel wrapper method specifies the number of workers. We set the number of workers to the number of available systems.</li>
<li><kbd>inferenceMode(mode)</kbd>: This parallel inference method specifies the distributed inference method. <kbd>BATCHED</kbd> mode is an optimization. If a large number of requests come in, it will process them in batches. If there is a small number of requests, then they will be processed as usual without batching. As you might have guessed, this is the perfect option if you're in production. </li>
<li><kbd>batchLimit(batchSize)</kbd>: This parallel inference method specifies the batch size limit and is only applicable if you use <kbd>BATCHED</kbd> mode in <kbd>inferenceMode()</kbd>. </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The performance of ND4J operations can also vary upon input array ordering. ND4J enforces the ordering of arrays. Performance in mathematical operations (including general ND4J operations) depends on the input and result array orders. For example, performance in operations such as simple addition, such as <em>z = x + y</em>, will vary in line with the input array orders. It happens due to memory striding: it is easier to read the memory sequence if they're close/adjacent to each other than when they're spread far apart. ND4J is faster on computations with larger matrices. By default, ND4J arrays are C-ordered. IC ordering refers to row-major ordering and the memory allocation resembles that of an array in C:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1181 image-border" src="assets/fd7db9f6-a996-485b-87e1-3f535f59a786.png" style="width:35.50em;height:22.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"> (Image courtesy: Eclipse Deeplearning4j Development Team. Deeplearning4j: Open-source distributed deep learning for the JVM, Apache Software Foundation License 2.0. http://deeplearning4j.org)</div>
<p>ND4J supplies the <kbd>gemm()</kbd> method for advanced matrix multiplication between two INDArrays depending on whether we require multiplication after transposing it. This method returns the result in F-order, which means the memory allocation resembles that of an array in Fortran. F-ordering refers to column-major ordering. Let's say we have passed a C-ordered array to collect the results from the <kbd>gemm()</kbd> method; ND4J automatically detects it, creates an F-ordered array, and then passes the result to a C-ordered array.</p>
<p>To learn more about array ordering and how ND4J handles array ordering, go to <a href="https://deeplearning4j.org/docs/latest/nd4j-overview">https://deeplearning4j.org/docs/latest/nd4j-overview</a>.</p>
<p class="mce-root"/>
<p>It is also critical to assess the minibatch size used for training. We need to experiment with different minibatch sizes while performing multiple training sessions by acknowledging the hardware specs, data, and evaluation metrics. For a CUDA-enabled GPU environment, the minibatch size will have a big role to play with regard to benchmarks if you use a large enough value. When we talk about a large minibatch size, we are referring to a minibatch size that can be justified against the entire dataset. For very small minibatch sizes, we won't observe any noticeable performance difference with the CPU/GPU after the benchmarks. At the same time, we need to watch out for changes in model accuracy as well. An ideal minibatch size is when we utilize the hardware to its full ability without affecting model accuracy. In fact, we aim for better results with better performance (shorter training time). </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up heap spaces and garbage collection</h1>
                </header>
            
            <article>
                
<p>Memory heap spaces and garbage collection are frequently discussed yet are often the most frequently ignored benchmarks. With DL4J/ND4J, you can configure two types of memory limit: on-heap memory and off-heap memory. Whenever an INDArray is collected by the JVM garbage collector, the off-heap memory will be de-allocated, assuming that it is not being used anywhere else. In this recipe, we will set up heap spaces and garbage collection for benchmarking.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Add the required VM arguments to the Eclipse/IntelliJ IDE, as shown in the following example:</li>
</ol>
<pre style="padding-left: 60px">-Xms1G -Xmx6G -Dorg.bytedeco.javacpp.maxbytes=16G -Dorg.bytedeco.javacpp.maxphysicalbytes=20G</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">For example, in IntelliJ IDE, we can add VM arguments to the runtime configuration:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1182 image-border" src="assets/7aef487e-7668-429d-9bce-b73c51d3c941.png" style="width:89.58em;height:56.17em;"/></p>
<ol start="2">
<li>Run the following command after changing the memory limits to suit your hardware (for command-line executions):</li>
</ol>
<pre style="padding-left: 60px"><strong>java -Xms1G -Xmx6G -Dorg.bytedeco.javacpp.maxbytes=16G -Dorg.bytedeco.javacpp.maxphysicalbytes=20G YourClassName</strong><br/> </pre>
<ol start="3">
<li>Configure a server-style generational garbage collector for JVM:</li>
</ol>
<pre style="padding-left: 60px"><strong>java -XX:+UseG1GC</strong></pre>
<ol start="4">
<li>Reduce the frequency of garbage collector calls using ND4J:</li>
</ol>
<pre style="padding-left: 60px">Nd4j.getMemoryManager().setAutoGcWindow(3000);</pre>
<ol start="5">
<li>Disable garbage collector calls instead of step 4:</li>
</ol>
<pre style="padding-left: 60px">Nd4j.getMemoryManager().togglePeriodicGc(false);</pre>
<p class="mce-root"/>
<ol start="6">
<li>Allocate memory chunks in memory-mapped files instead of RAM:</li>
</ol>
<pre style="padding-left: 60px">WorkspaceConfiguration memoryMap = WorkspaceConfiguration.builder()<br/> .initialSize(2000000000)<br/> .policyLocation(LocationPolicy.MMAP)<br/> .build();<br/> try (MemoryWorkspace workspace = Nd4j.getWorkspaceManager().getAndActivateWorkspace(memoryMap, "M")) {<br/> INDArray example = Nd4j.create(10000);<br/> }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we performed on-heap/off-heap memory configurations. On-heap memory simply means the memory that is managed by the JVM heap (garbage collector). Off-heap memory refers to memory that is not managed directly, such as that used with INDArrays. Both off-heap and on-heap memory limits can be controlled using the following VM options in Java command-line arguments:</p>
<ul>
<li><kbd>-Xms</kbd>: This defines how much memory will be consumed by the JVM heap at application startup.</li>
<li><kbd>-Xmx</kbd>: This defines the maximum memory that can be consumed by the JVM heap at any point in runtime. This involves allotting memory only up to this point when it is required.</li>
<li><kbd>-Dorg.bytedeco.javacpp.maxbytes</kbd>: This specifies the off-heap memory limit. </li>
<li><kbd>-Dorg.bytedeco.javacpp.maxphysicalbytes</kbd>: This specifies the maximum number of bytes that can be allotted to the application at any given time. Usually, this takes a larger value than <kbd>-Xmx</kbd> and <kbd>maxbytes</kbd> combined. </li>
</ul>
<p>Suppose we want to configure 1 GB initially on-heap, 6 GB max on-heap, 16 GB off-heap, and 20 GB maximum for processes; the VM arguments will look as follows, and as shown in step 1:</p>
<pre><strong>-Xms1G -Xmx6G -Dorg.bytedeco.javacpp.maxbytes=16G -Dorg.bytedeco.javacpp.maxphysicalbytes=20G</strong></pre>
<p>Note that you will need to adjust this in line with the memory available in your hardware.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>It is also possible to set up these VM options as an environment variable. We can create an environment variable named <kbd>MAVEN_OPTS</kbd> and put VM options there. You can choose either step 1 or step 2, or set them up with an environment variable. Once this is done, you can skip to step 3. </p>
<p>In steps 3, 4, and 5, we discussed memory automatically using some tweaks in garbage collection. The garbage collector manages memory management and consumes on-heap memory. DL4J is tightly coupled with the garbage collector. If we talk about ETL, every <kbd>DataSetIterator</kbd> object takes 8 bytes of memory. The garbage collector can induce further latency in the system. To that end, we configure <strong>G1GC</strong> (short for <strong>Garbage First Garbage Collector</strong>) tuning in step 3.</p>
<p>If we pass 0 ms (milliseconds) as an attribute to the <kbd>setAutoGcWindow()</kbd> method, as in step 4, it will just disable this particular option. <kbd>getMemoryManager()</kbd> will return a backend-specific implementation of <kbd>MemoryManager</kbd> for lower-level memory management. </p>
<p>In step 6, we discussed configuring memory-mapped files to allocate more memory for INDArrays. We have created a 1 GB memory map file in step 4. Note that memory-mapped files can be created and supported only when using the <kbd>nd4j-native</kbd> library. Memory mapped files are slower than memory allocation in RAM. Step 4 can be applied if the minibatch size memory requirement is higher than the amount of RAM available.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>DL4J has a dependency with JavaCPP that acts as a bridge between Java and C++: <a href="https://github.com/bytedeco/javacpp">https://github.com/bytedeco/javacpp</a>.</p>
<p>JavaCPP works on the basis of the <kbd>-Xmx</kbd> value set on the heap space (off-heap memory) and the overall memory consumption will not exceed this value. DL4J seeks help from the garbage collector and JavaCPP to deallocate memory.</p>
<p>For training sessions with large amounts of data involved, it is important to have more RAM for the off-heap memory space than for on-heap memory (JVM). Why? Because our datasets and computations are involved with INDArrays and are stored in the off-heap memory space.</p>
<p class="mce-root"/>
<p>It is important to identify the memory limits of running applications. <span>The following</span> some instances where the memory limit needs to be properly configured:</p>
<ul>
<li>For GPU systems, <kbd>maxbytes</kbd> and <kbd>maxphysicalbytes</kbd> are the important memory limit settings. We are dealing with off-heap memory here. Allocating reasonable memory to these settings allows us to consume more GPU resources.</li>
<li>For <kbd>RunTimeException</kbd> that refer to memory allocation issues, one possible reason may be the unavailability of off-heap memory spaces. If we don't use the memory limit (off-heap space) settings discussed in the <em>Setting up heap space and garbage collection</em> recipe, the off-heap memory space can be reclaimed by the JVM garbage collector. This can then cause memory allocation issues. </li>
<li>If you have limited-memory environments, then it is not recommended to use large values for the <kbd>-Xmx</kbd> and <kbd>-Xms</kbd> options. For instance, if we use <kbd>-Xms6G</kbd> for an 8 GB RAM system, we leave only 2 GB for the off-heap memory space, the OS, and for other processes.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>If you're interested in knowing more about G1GC garbage collector tuning, you can read about it here: <a href="https://www.oracle.com/technetwork/articles/java/g1gc-1984535.html">https://www.oracle.com/technetwork/articles/java/g1gc-1984535.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using asynchronous ETL</h1>
                </header>
            
            <article>
                
<p>We use synchronous ETL for demonstration purposes. But for production, asynchronous ETL is preferable. In production, the existence of a single low-performance ETA component can cause a performance bottleneck. In DL4J, we load data to the disk using <kbd>DataSetIterator</kbd>. It can load the data from disk or, memory, or simply load data asynchronously. Asynchronous ETL uses an asynchronous loader in the background. Using multithreading, it loads data into the GPU/CPU and other threads take care of compute tasks. In the following recipe, we will perform asynchronous ETL operations in DL4J.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create asynchronous iterators with asynchronous prefetch:</li>
</ol>
<pre style="padding-left: 60px">DatasetIterator asyncIterator = new AsyncMultiDataSetIterator(iterator);</pre>
<ol start="2">
<li>Create asynchronous iterators with synchronous prefetch:</li>
</ol>
<pre style="padding-left: 60px">DataSetIterator shieldIterator = new AsyncShieldDataSetIterator(iterator);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we created an iterator using <kbd>AsyncMultiDataSetIterator</kbd>. We can use  <kbd>AsyncMultiDataSetIterator</kbd> or <kbd>AsyncDataSetIterator</kbd> to create asynchronous iterators. There are multiple ways in which you can configure an <kbd>AsyncMultiDataSetIterator</kbd>. There are multiple ways to create <kbd>AsyncMultiDataSetIterator</kbd> by passing further attributes such as <kbd>queSize</kbd> (the number of mini-batches that can be prefetched at once) and <kbd>useWorkSpace</kbd> (a Boolean type indicating whether workspace configuration should be used). While using <kbd>AsyncDataSetIterator</kbd>, we use the current dataset before calling <kbd>next()</kbd> to get the next dataset. Also note that we should not store datasets without the <kbd>detach()</kbd> call. If you do, then the memory used by INDArray data inside the dataset will eventually be overwritten within  <kbd>AsyncDataSetIterator</kbd>. For custom iterator implementations, make sure you don't initialize something huge using the <kbd>next()</kbd> call during training/evaluation. Instead, keep all such initialization inside the constructor to avoid undesired workspace memory consumption.</p>
<p>In step 2, we created an iterator using <kbd>AsyncShieldDataSetIterator</kbd>. To opt out of asynchronous prefetch, we can use <kbd>AsyncShieldMultiDataSetIterator</kbd> or <kbd>AsyncShieldDataSetIterator</kbd>. These wrappers will prevent asynchronous prefetch in data-intensive operations such as training, and can be used for debugging purposes.</p>
<p class="mce-root"/>
<p>If the training instance performs ETL every time it runs, we are basically recreating the data every time it runs. Eventually, the whole process (training and evaluation) will get slower. We can handle this better using a pre-saved dataset. We discussed pre-save using <kbd>ExistingMiniBatchDataSetIterator</kbd> in the previous chapter, when we pre-saved feature data and then later loaded it using <kbd>ExistingMiniBatchDataSetIterator</kbd>. We can convert it to an asynchronous iterator (as in step 1 or step 2) and kill two birds with one stone: pre-saved data with asynchronous loading. This is essentially a performance benchmark that further optimizes the ETL process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Let's say our minibatch has 100 samples and we specify <kbd>queSize</kbd> as <kbd>10</kbd>; 1,000 samples will be prefetched every time. The memory requirement of the workspace depends on the size of the dataset, which arises from the underlying iterator. The workspace will be adjusted for varying memory requirements (for example, time series with varying lengths). Note that asynchronous iterators are internally supported by <kbd>LinkedBlockingQueue</kbd>. This queue data structure orders elements in <strong>First In First Out</strong> (<strong>FIFO</strong>) mode. Linked queues generally have more throughput than array-based queues in concurrent environments. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using arbiter to monitor neural network behavior</h1>
                </header>
            
            <article>
                
<p>Hyperparameter optimization/tuning is the process of finding the optimal values for hyperparameters in the learning process. Hyperparameter optimization partially automates the process of finding optimal hyperparameters using certain search strategies. Arbiter is part of the DL4J deep learning library and is used for hyperparameter optimization. Arbiter can be used to find high-performing models by tuning the hyperparameters of the neural network. Arbiter has a UI that visualizes the results of the hyperparameter tuning process. </p>
<p>In this recipe, we will set up arbiter and visualize the training instance to take a look at neural network behavior. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Add the arbiter Maven dependency in <kbd>pom.xml</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/>   &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>   &lt;artifactId&gt;arbiter-deeplearning4j&lt;/artifactId&gt;<br/>   &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;<br/> &lt;dependency&gt;<br/>   &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>   &lt;artifactId&gt;arbiter-ui_2.11&lt;/artifactId&gt;<br/>   &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>
<ol start="2">
<li>Configure the search space using <kbd>ContinuousParameterSpace</kbd>:</li>
</ol>
<pre style="padding-left: 60px">ParameterSpace&lt;Double&gt; learningRateParam = new ContinuousParameterSpace(0.0001,0.01);</pre>
<ol start="3">
<li>Configure the search space using <kbd>IntegerParameterSpace</kbd>:</li>
</ol>
<pre style="padding-left: 60px">ParameterSpace&lt;Integer&gt; layerSizeParam = new IntegerParameterSpace(5,11);   </pre>
<ol start="4">
<li>Use <kbd>OptimizationConfiguration</kbd> to combine all components required to execute the hyperparameter tuning process:</li>
</ol>
<pre style="padding-left: 60px">OptimizationConfiguration optimizationConfiguration = new             OptimizationConfiguration.Builder()<br/> .candidateGenerator(candidateGenerator)<br/> .dataProvider(dataProvider)<br/> .modelSaver(modelSaver)<br/> .scoreFunction(scoreFunction)<br/> .terminationConditions(conditions)<br/> .build();</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 2, we created  <kbd>ContinuousParameterSpace</kbd> to configure the search space for hyperparameter optimization:</p>
<pre>ParameterSpace&lt;Double&gt; learningRateParam = new ContinuousParameterSpace(0.0001,0.01);</pre>
<p class="mce-root"/>
<p>In the preceding case, the hyperparameter tuning process will select continuous values in the range (0.0001, 0.01) for the learning rate. Note that arbiter doesn't really automate the hyperparameter tuning process. We still need to specify the range of values or a list of options by which the hyperparameter tuning process takes place. In other words, we need to specify a search space with all the valid values for the tuning process to pick the best combination that can produce the best results. We have also mentioned <kbd>IntegerParameterSpace</kbd>, where the search space is an ordered space of integers between a maximum/minimum value.</p>
<p>Since there are multiple training instances with different configurations, it takes a while  to finish the hyperparameter optimization-tuning process. At the end, the best configuration will be returned.</p>
<p>In step 2, once we have defined our search space using <kbd>ParameterSpace</kbd> or <kbd>OptimizationConfiguration</kbd>, we need to add it to <kbd>MultiLayerSpace</kbd> or <kbd>ComputationGraphSpace</kbd>. These are the arbiter counterparts of DL4J's <kbd>MultiLayerConfiguration</kbd> and <kbd>ComputationGraphConfiguration</kbd>.</p>
<p>Then we added <kbd>candidateGenerator</kbd> using the <kbd>candidateGenerator()</kbd> builder method. <kbd>candidateGenerator</kbd> chooses candidates (various combinations of hyperparameters) for hyperparameter tuning. It can use different approaches, such as random search and grid search, to pick the next configuration for hyperparameter tuning. </p>
<p><kbd>scoreFunction()</kbd> specifies the evaluation metrics used for evaluation during the hyperparameter tuning process.</p>
<p><kbd>terminationConditions()</kbd> is used to mention all termination conditions for the training instance. Hyperparameter tuning will then proceed with the next configuration in the sequence. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing hyperparameter tuning</h1>
                </header>
            
            <article>
                
<p>Once search spaces are defined using <kbd>ParameterSpace</kbd> or <kbd>OptimizationConfiguration</kbd>, with a possible range of values, the next step is to complete network configuration using  <kbd>MultiLayerSpace</kbd> or <kbd>ComputationGraphSpace</kbd>. After that, we start the training process. We perform multiple training sessions during the hyperparameter tuning process.</p>
<p>In this recipe, we will perform and visualize the hyperparameter tuning process. We will be using <kbd>MultiLayerSpace</kbd> for the demonstration.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Add a search space for the layer size using <kbd>IntegerParameterSpace</kbd>:</li>
</ol>
<pre style="padding-left: 60px">ParameterSpace&lt;Integer&gt; layerSizeParam = new IntegerParameterSpace(startLimit,endLimit);</pre>
<ol start="2">
<li>Add a search space for the learning rate using <kbd>ContinuousParameterSpace</kbd>:</li>
</ol>
<pre style="padding-left: 60px">ParameterSpace&lt;Double&gt; learningRateParam = new ContinuousParameterSpace(0.0001,0.01);</pre>
<ol start="3">
<li>Use <kbd>MultiLayerSpace</kbd> to build a configuration space by adding all the search spaces to the relevant network configuration:</li>
</ol>
<pre style="padding-left: 60px">MultiLayerSpace hyperParamaterSpace = new MultiLayerSpace.Builder()<br/> .updater(new AdamSpace(learningRateParam))<br/> .addLayer(new DenseLayerSpace.Builder()<br/>   .activation(Activation.RELU)<br/>   .nIn(11)<br/>   .nOut(layerSizeParam)<br/>   .build())<br/> .addLayer(new DenseLayerSpace.Builder()<br/>   .activation(Activation.RELU)<br/>   .nIn(layerSizeParam)<br/>   .nOut(layerSizeParam)<br/>   .build())<br/> .addLayer(new OutputLayerSpace.Builder()<br/>   .activation(Activation.SIGMOID)<br/>   .lossFunction(LossFunctions.LossFunction.XENT)<br/>   .nOut(1)<br/>   .build())<br/> .build();<br/><br/></pre>
<ol start="4">
<li>Create <kbd>candidateGenerator</kbd> from <kbd>MultiLayerSpace</kbd>:</li>
</ol>
<pre style="padding-left: 60px">Map&lt;String,Object&gt; dataParams = new HashMap&lt;&gt;();<br/> dataParams.put("batchSize",new Integer(10));<br/><br/>CandidateGenerator candidateGenerator = new RandomSearchGenerator(hyperParamaterSpace,dataParams);</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>Create a data source by implementing the <kbd>DataSource</kbd> interface:</li>
</ol>
<pre style="padding-left: 60px">public static class ExampleDataSource implements DataSource{<br/>  public ExampleDataSource(){<br/>     //implement methods from DataSource<br/>  }<br/> }</pre>
<p style="padding-left: 60px">We will need to implement four methods: <kbd>configure()</kbd>, <kbd>trainData()</kbd>, <kbd>testData()</kbd>, and <kbd>getDataType()</kbd>:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li>The following is an example implementation of <kbd>configure()</kbd>:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 120px">public void configure(Properties properties) {<br/>    this.minibatchSize = Integer.parseInt(properties.getProperty("minibatchSize", "16"));<br/> }</pre>
<ul>
<li style="list-style-type: none">
<ul>
<li>Here's an example implementation of <kbd>getDataType()</kbd>:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 120px">public Class&lt;?&gt; getDataType() {<br/> return DataSetIterator.class;<br/> }</pre>
<ul>
<li style="list-style-type: none">
<ul>
<li>Here's an example implementation of <kbd>trainData()</kbd>:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 120px">public Object trainData() {<br/> try{<br/> DataSetIterator iterator = new RecordReaderDataSetIterator(dataPreprocess(),minibatchSize,labelIndex,numClasses);<br/> return dataSplit(iterator).getTestIterator();<br/> }<br/> catch(Exception e){<br/> throw new RuntimeException();<br/> }<br/> }</pre>
<ul>
<li style="list-style-type: none">
<ul>
<li>Here's an example implementation of <kbd>testData()</kbd>:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 120px">public Object testData() {<br/> try{<br/> DataSetIterator iterator = new RecordReaderDataSetIterator(dataPreprocess(),minibatchSize,labelIndex,numClasses);<br/> return dataSplit(iterator).getTestIterator();<br/> }<br/> catch(Exception e){<br/> throw new RuntimeException();<br/> }<br/> }</pre>
<ol start="6">
<li>Create an array of termination conditions:</li>
</ol>
<pre style="padding-left: 60px">TerminationCondition[] conditions = {<br/>   new MaxTimeCondition(maxTimeOutInMinutes, TimeUnit.MINUTES),<br/>   new MaxCandidatesCondition(maxCandidateCount)<br/>};</pre>
<ol start="7">
<li>Calculate the score of all models that were created using different combinations of configurations:</li>
</ol>
<pre style="padding-left: 60px">ScoreFunction scoreFunction = new EvaluationScoreFunction(Evaluation.Metric.ACCURACY);</pre>
<ol start="8">
<li>Create <kbd>OptimizationConfiguration</kbd> and add termination conditions and the score function:</li>
</ol>
<pre style="padding-left: 60px">OptimizationConfiguration optimizationConfiguration = new OptimizationConfiguration.Builder()<br/> .candidateGenerator(candidateGenerator)<br/> .dataSource(ExampleDataSource.class,dataSourceProperties)<br/> .modelSaver(modelSaver)<br/> .scoreFunction(scoreFunction)<br/> .terminationConditions(conditions)<br/> .build();</pre>
<ol start="9">
<li>Create <kbd>LocalOptimizationRunner</kbd> to run the hyperparameter tuning process:</li>
</ol>
<pre style="padding-left: 60px">IOptimizationRunner runner = new LocalOptimizationRunner(optimizationConfiguration,new MultiLayerNetworkTaskCreator());</pre>
<ol start="10">
<li>Add listeners to <kbd>LocalOptimizationRunner</kbd> to ensure events are logged properly (skip to step 11 to add <kbd>ArbiterStatusListener</kbd>):</li>
</ol>
<pre style="padding-left: 60px">runner.addListeners(new LoggingStatusListener());</pre>
<ol start="11">
<li>Execute the hyperparameter tuning by calling the <kbd>execute()</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">runner.execute();</pre>
<p class="mce-root"/>
<ol start="12">
<li>Store the model configurations and replace <kbd>LoggingStatusListener</kbd> with <kbd>ArbiterStatusListener</kbd>:</li>
</ol>
<pre style="padding-left: 60px">StatsStorage storage = new FileStatsStorage(new File("HyperParamOptimizationStatsModel.dl4j"));<br/> runner.addListeners(new ArbiterStatusListener(storage));</pre>
<ol start="13">
<li>Attach the storage to <kbd>UIServer</kbd>:</li>
</ol>
<pre style="padding-left: 60px">UIServer.getInstance().attach(storage);</pre>
<ol start="14">
<li>Run the hyperparameter tuning session and go to the following URL to view the visualization:</li>
</ol>
<pre style="padding-left: 60px">http://localhost:9000/arbiter</pre>
<ol start="15">
<li>Evaluate the best score from the hyperparameter tuning session and display the results in the console:</li>
</ol>
<pre style="padding-left: 60px">double bestScore = runner.bestScore();<br/> int bestCandidateIndex = runner.bestScoreCandidateIndex();<br/> int numberOfConfigsEvaluated = runner.numCandidatesCompleted();</pre>
<p style="padding-left: 60px">You should see the output shown in the following snapshot. The model's best score, the index where the best model is located, and the number of configurations evaluated in the process are displayed:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1184 image-border" src="assets/8f118f92-bd5f-475e-a313-f071925878ed.png" style="width:73.42em;height:17.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 4, we set up a strategy by which the network configurations will be picked up from the search space. We use <kbd>CandidateGenerator</kbd> for this purpose. We created a parameter mapping to store all data mappings for use with the data source and passed it to <kbd>CandidateGenerator</kbd>.</p>
<p class="mce-root"/>
<p>In step 5, we implemented the <kbd>configure()</kbd> method along with three other methods from the <kbd>DataSource</kbd> interface. The <kbd>configure()</kbd> method accepts a <kbd>Properties</kbd> attribute, which has all parameters to be used with the data source. If we want to pass <kbd>miniBatchSize</kbd> as a property, then we can create a <kbd>Properties</kbd> instance as shown here:</p>
<pre>Properties dataSourceProperties = new Properties();<br/> dataSourceProperties.setProperty("minibatchSize", "64");</pre>
<p>Note that the minibatch size needs to be mentioned as a string: <kbd>"64"</kbd> and not <kbd>64</kbd>.</p>
<p>The custom <kbd>dataPreprocess()</kbd> method pre-processes data. <kbd>dataSplit()</kbd> creates <kbd>DataSetIteratorSplitter</kbd> to generate train/test iterators for training/evaluation.</p>
<p>In step 4, <kbd>RandomSearchGenerator</kbd> generates candidates for hyperparameter tuning at random. If we explicitly mention a probability distribution for the hyperparameters, then the random search will favor those hyperparameters according to their probability.  <kbd>GridSearchCandidateGenerator</kbd> generates candidates using a grid search. For discrete hyperparameters, the grid size is equal to the number of hyperparameter values. For integer hyperparameters, the grid size is the same as <kbd>min(discretizationCount,max-min+1)</kbd>.</p>
<p>In step 6, we defined termination conditions. Termination conditions control how far the training process should progress. Termination conditions could be <kbd>MaxTimeCondition</kbd>, <kbd>MaxCandidatesCondition</kbd>, or we can define our own termination conditions.</p>
<p>In step 7, we created a score function to mention how each and every model is evaluated during the hyperparameter optimization process.</p>
<p>In step 8, we created <kbd>OptimizationConfiguration</kbd> comprising these termination conditions. Apart from termination conditions, we also added the following configurations to <kbd>OptimizationConfiguration</kbd>: </p>
<ul>
<li>The location at which the model information has to be stored</li>
<li>The candidate generator that was created earlier</li>
<li>The data source that was created earlier</li>
<li>The type of evaluation metrics to be considered</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p><kbd>OptimizationConfiguration</kbd> ties all the components together to execute the hyperparameter optimization. Note that the <kbd>dataSource()</kbd> method expects two attributes: one is the class type of your data source class, the other is the data source properties that we want to pass on (<kbd>minibatchSize</kbd> in our example). The <kbd>modelSaver()</kbd> builder method requires you to mention the location of the model being trained. We can store model information (model score and other configurations) in the resources folder, and then we can create a <kbd>ModelSaver</kbd> instance as follows:</p>
<pre>ResultSaver modelSaver = new FileModelSaver("resources/");</pre>
<p>In order to visualize the results using arbiter, skip step 10, follow step 12, and then execute the visualization task runner.</p>
<p>After following the instructions in steps 13 and 14,  you should be able to see arbiter's UI visualization, as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1185 image-border" src="assets/a25f2b28-308c-483a-a17a-027792c1020b.png" style="width:102.00em;height:57.08em;"/></p>
<p>It is very intuitive and easy to figure out the best model score from the arbiter visualization. If you run multiple sessions of hyperparameter tuning, then you can select a particular session from the drop-down list at the top. Further important information displayed on the UI is pretty self-explanatory at this stage.</p>


            </article>

            
        </section>
    </body></html>