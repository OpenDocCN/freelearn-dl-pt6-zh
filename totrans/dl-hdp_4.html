<html><head></head><body><div id="book-columns"><div id="book-inner"><div class="chapter" title="Chapter 4. Recurrent Neural Network"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/><span class="koboSpan" id="kobo.1.1">Chapter 4. Recurrent Neural Network</span></h1></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"><span class="koboSpan" id="kobo.2.1"> </span></td><td valign="top"><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.3.1">I think the brain is essentially a computer and consciousness is like a computer program. </span><span class="koboSpan" id="kobo.3.2">It will cease to run when the computer is turned off. </span><span class="koboSpan" id="kobo.3.3">Theoretically, it could be re-created on a neural network, but that would be very difficult, as it would require all one's memories.</span></em></span>
</p></td><td valign="top"><span class="koboSpan" id="kobo.4.1"> </span></td></tr><tr><td valign="top"><span class="koboSpan" id="kobo.5.1"> </span></td><td colspan="2" align="right" valign="top" style="text-align: center"><span class="koboSpan" id="kobo.6.1">--</span><span class="attribution"><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">Stephen Hawking</span></em></span></span></td></tr></table></div><p><span class="koboSpan" id="kobo.8.1">To solve every problem, people do not initiate their thinking process from scratch. </span><span class="koboSpan" id="kobo.8.2">Our thoughts are non-volatile, and it is persistent just like the </span><span class="strong"><strong><span class="koboSpan" id="kobo.9.1">Read Only Memory</span></strong></span><span class="koboSpan" id="kobo.10.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.11.1">ROM</span></strong></span><span class="koboSpan" id="kobo.12.1">) of a computer. </span><span class="koboSpan" id="kobo.12.2">When we read an article, we understand the meaning of every word from our understanding of earlier words in the sentences.</span></p><p><span class="koboSpan" id="kobo.13.1">Let us take a real life example to explain this context a bit more. </span><span class="koboSpan" id="kobo.13.2">Let us assume we want to make a classification based on the events happening at every point in a video. </span><span class="koboSpan" id="kobo.13.3">As we do not have the information of the earlier events of the video, it would be a cumbersome task for the traditional deep neural networks to find some distinguishing reasons to classify those. </span><span class="koboSpan" id="kobo.13.4">Traditional deep neural networks cannot perform this operation, and hence, it has been one of the major limitations for them.</span></p><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.14.1">Recurrent neural networks</span></strong></span><span class="koboSpan" id="kobo.15.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.16.1">RNN</span></strong></span><span class="koboSpan" id="kobo.17.1">) [103] are a special type of neural network, which provides many enigmatic solutions for these difficult machine learning and deep learning problems. </span><span class="koboSpan" id="kobo.17.2">In the last chapter, we discussed convolutional neural networks, which is specialized in processing a set of values </span><span class="emphasis"><em><span class="koboSpan" id="kobo.18.1">X</span></em></span><span class="koboSpan" id="kobo.19.1"> (For example, an image). </span><span class="koboSpan" id="kobo.19.2">Similarly, RNNs are magical while processing a sequence of values, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.20.1">x (0)</span></em></span><span class="koboSpan" id="kobo.21.1">,</span><span class="emphasis"><em><span class="koboSpan" id="kobo.22.1"> x (1)</span></em></span><span class="koboSpan" id="kobo.23.1">,</span><span class="emphasis"><em><span class="koboSpan" id="kobo.24.1">x(2)</span></em></span><span class="koboSpan" id="kobo.25.1">,</span><span class="emphasis"><em><span class="koboSpan" id="kobo.26.1">...</span></em></span><span class="koboSpan" id="kobo.27.1">,</span><span class="emphasis"><em><span class="koboSpan" id="kobo.28.1"> x(τ-1)</span></em></span><span class="koboSpan" id="kobo.29.1">. </span><span class="koboSpan" id="kobo.29.2">To start with RNNs in this chapter, let us first place this network next to convolutional neural networks so that you can get an idea of its basic functionalities, and to basically know about this network.</span></p><p><span class="koboSpan" id="kobo.30.1">Convolutional neural networks can easily scale to images with large width, height, and depth. </span><span class="koboSpan" id="kobo.30.2">Moreover, some convolutional neural networks can also process images with variable sizes.</span></p><p><span class="koboSpan" id="kobo.31.1">In contrast, recurrent networks can readily scale to long sequences; also, most of those can also process variable length sequences. </span><span class="koboSpan" id="kobo.31.2">To process these arbitrary sequences of inputs, RNN uses their internal memory.</span></p><p><span class="koboSpan" id="kobo.32.1">RNNs generally operate on mini-batches of sequences, and contain vectors </span><span class="emphasis"><em><span class="koboSpan" id="kobo.33.1">x (t)</span></em></span><span class="koboSpan" id="kobo.34.1"> with the time-step index </span><span class="emphasis"><em><span class="koboSpan" id="kobo.35.1">t</span></em></span><span class="koboSpan" id="kobo.36.1"> ranging from </span><span class="emphasis"><em><span class="koboSpan" id="kobo.37.1">0</span></em></span><span class="koboSpan" id="kobo.38.1"> to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.39.1">(τ-1)</span></em></span><span class="koboSpan" id="kobo.40.1">. </span><span class="koboSpan" id="kobo.40.2">The sequence length </span><span class="emphasis"><em><span class="koboSpan" id="kobo.41.1">τ</span></em></span><span class="koboSpan" id="kobo.42.1"> can also vary for each member of the mini-batch. </span><span class="koboSpan" id="kobo.42.2">This time-step index should not always refer to the time intervals in the real world, but can also point to the position inside the sequence.</span></p><p><span class="koboSpan" id="kobo.43.1">A RNN, when unfolded in time, can be seen as a deep neural network with indefinite number of layers. </span><span class="koboSpan" id="kobo.43.2">However, compared to common deep neural networks, the basic functionalities and architecture of RNNs are somewhat different. </span><span class="koboSpan" id="kobo.43.3">For RNNs, the main function of the layers is to bring in memory, and not hierarchical processing. </span><span class="koboSpan" id="kobo.43.4">For other deep neural networks, the input is only provided in the first layer, and the output is produced at the final layer. </span><span class="koboSpan" id="kobo.43.5">However, in RNNs, the inputs are generally received at each time step, and the corresponding outputs are computed at those intervals. </span><span class="koboSpan" id="kobo.43.6">With every network iteration, fresh information is integrated into every layer, and the network can go along with this information for an indefinite number of network updates. </span><span class="koboSpan" id="kobo.43.7">However, during the training phase, the recurrent weights need to learn which information they should pass onwards, and what they must reject. </span><span class="koboSpan" id="kobo.43.8">This feature generates the primary motivation for a special form of RNN, called </span><span class="strong"><strong><span class="koboSpan" id="kobo.44.1">Long short-term memory</span></strong></span><span class="koboSpan" id="kobo.45.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.46.1">LSTM</span></strong></span><span class="koboSpan" id="kobo.47.1">).</span></p><p><span class="koboSpan" id="kobo.48.1">RNNs started its journey a few decades back [104], but recently, it has significantly become a popular choice for modeling sequences of variable length. </span><span class="koboSpan" id="kobo.48.2">As of now, RNN has been successfully implemented in various problems such as learning word embedding [105], language modelling [106] [107] [108], speech recognition [109], and online handwritten recognition [110].</span></p><p><span class="koboSpan" id="kobo.49.1">In this chapter, we will discuss everything you need to know about RNN and the associated core components. </span><span class="koboSpan" id="kobo.49.2">We will introduce Long short-term memory later in the chapter, which is a special type of RNN.</span></p><p><span class="koboSpan" id="kobo.50.1">The topic-wise organization of this chapter is as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.51.1">What makes recurrent networks distinctive from others?</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.52.1">Recurrent neural networks(RNNs)</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.53.1">Backpropagation through time (BPTT)</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.54.1">Long short-term memory (LSTM)</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.55.1">Bi-directional RNNs</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.56.1">Distributed deep RNNs</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.57.1">RNNs with Deeplearning4j</span></li></ul></div><div class="section" title="What makes recurrent networks distinctive from others?"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec24"/><span class="koboSpan" id="kobo.58.1">What makes recurrent networks distinctive from others?</span></h1></div></div></div><p><span class="koboSpan" id="kobo.59.1">You might be curious to know the specialty of RNNs. </span><span class="koboSpan" id="kobo.59.2">This section of the chapter will discuss these things, and from the next section onwards, we will talk about the building blocks of this type of network.</span></p><p><span class="koboSpan" id="kobo.60.1">From </span><a class="link" href="ch03.html" title="Chapter 3.  Convolutional Neural Network"><span class="koboSpan" id="kobo.61.1">
Chapter 3
</span></a><span class="koboSpan" id="kobo.62.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.63.1">Convolutional Neural Network</span></em></span><span class="koboSpan" id="kobo.64.1">, you have probably got a sense of the harsh limitation of convolutional networks and that their APIs are too constrained; the network can only take an input of a fixed-sized vector, and also generates a fixed-sized output. </span><span class="koboSpan" id="kobo.64.2">Moreover, these operations are performed through a predefined number of intermediate layers. </span><span class="koboSpan" id="kobo.64.3">The primary reason that makes RNNs distinctive from others is their ability to operate over long sequences of vectors, and produce different sequences of vectors as the output.</span></p><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"><span class="koboSpan" id="kobo.65.1"> </span></td><td valign="top"><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.66.1">"If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs"</span></em></span>
</p></td><td valign="top"><span class="koboSpan" id="kobo.67.1"> </span></td></tr><tr><td valign="top"><span class="koboSpan" id="kobo.68.1"> </span></td><td colspan="2" align="right" valign="top" style="text-align: center"><span class="koboSpan" id="kobo.69.1">--</span><span class="attribution"><span class="emphasis"><em><span class="koboSpan" id="kobo.70.1">Alex Lebrun</span></em></span></span></td></tr></table></div><p><span class="koboSpan" id="kobo.71.1">We show different types of input-output relationships of the neural networks in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.72.1">Figure 4.1</span></em></span><span class="koboSpan" id="kobo.73.1"> to portray the differences. </span><span class="koboSpan" id="kobo.73.2">We show five types of input-output relations as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.74.1">One to one</span></strong></span><span class="koboSpan" id="kobo.75.1">: This input-output relationship is for traditional neural network processing without the involvement of a RNN. </span><span class="koboSpan" id="kobo.75.2">Mostly used for image classification, where the mapping is from fixed-sized input to fixed-sized output.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.76.1">One to many</span></strong></span><span class="koboSpan" id="kobo.77.1">: In this kind of relationship, the input and output maintain a one-to-many relationship. </span><span class="koboSpan" id="kobo.77.2">The model generates a sequence of outputs with one fixed-sized input. </span><span class="koboSpan" id="kobo.77.3">Often observed where the model takes an image (image captioning), and produces a sentence of words.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.78.1">Many to one</span></strong></span><span class="koboSpan" id="kobo.79.1">: In this type of relationship, the model takes a sequence of inputs, and outputs one single observation. </span><span class="koboSpan" id="kobo.79.2">For example, in case of sentiment analysis, a sentence or reviews are provided to the model; it classifies the sentence as either a positive or negative sentiment.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.80.1">Many to many (Variable intermedia states)</span></strong></span><span class="koboSpan" id="kobo.81.1">: The model receives a sequence of inputs, and a corresponding sequence of outputs are generated. </span><span class="koboSpan" id="kobo.81.2">In this type, the RNN reads a sentence in English, and then translates and outputs a sentence in German. </span><span class="koboSpan" id="kobo.81.3">Used in case of Machine Translation.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.82.1">Many to many (Fixed number of intermedia state)</span></strong></span><span class="koboSpan" id="kobo.83.1">: The model receives a synced sequence of input, and generates a sequence of outputs. </span><span class="koboSpan" id="kobo.83.2">For example, in video classification, we might wish to classify every event of the movie.</span></li></ul></div><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.84.1"><img src="graphics/image_04_001.jpg" alt="What makes recurrent networks distinctive from others?"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.85.1">Figure 4.1: The rectangles in the figure represent each element of the sequence vector, the arrows signify the functions. </span><span class="koboSpan" id="kobo.85.2">Input vectors are shown in red, and output vectors are in blue. </span><span class="koboSpan" id="kobo.85.3">The green color represents the intermediate RNN's state. </span><span class="koboSpan" id="kobo.85.4">Image taken from [111].</span></p><p><span class="koboSpan" id="kobo.86.1">Operations that involve sequences are generally more powerful and appealing than networks with fixed-sized inputs and outputs. </span><span class="koboSpan" id="kobo.86.2">These models are used to build more intelligent systems. </span><span class="koboSpan" id="kobo.86.3">In the next sections, we will see how RNNs are built, and how the network unites the input vectors with their state vector with a defined function to generate a new state vector.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Recurrent neural networks(RNNs)"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec25"/><span class="koboSpan" id="kobo.1.1">Recurrent neural networks(RNNs)</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">In this section, we will discuss the architecture of the RNN. </span><span class="koboSpan" id="kobo.2.2">We will talk about how time is unfolded for the recurrence relation, and used to perform the computation in RNNs.</span></p><div class="section" title="Unfolding recurrent computations"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec37"/><span class="koboSpan" id="kobo.3.1">Unfolding recurrent computations</span></h2></div></div></div><p><span class="koboSpan" id="kobo.4.1">This section will explain how unfolding a recurrent relation results in sharing of parameters across a deep network structure, and converts it into a computational model.</span></p><p><span class="koboSpan" id="kobo.5.1">Let us consider a simple recurrent form of a dynamical system:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.6.1"><img src="graphics/image_04_002-1.jpg" alt="Unfolding recurrent computations"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.7.1">In the preceding equation, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.8.1">s </span><sup><span class="koboSpan" id="kobo.9.1">(t)</span></sup></em></span><span class="koboSpan" id="kobo.10.1"> represents the state of the system at time </span><span class="emphasis"><em><span class="koboSpan" id="kobo.11.1">t</span></em></span><span class="koboSpan" id="kobo.12.1">, and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.13.1">θ</span></em></span><span class="koboSpan" id="kobo.14.1"> is the same parameter shared across all the iterations.</span></p><p><span class="koboSpan" id="kobo.15.1">This equation is called a recurrent equation, as the computation of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.16.1">s </span><sup><span class="koboSpan" id="kobo.17.1">(t)</span></sup></em></span><span class="koboSpan" id="kobo.18.1"> requires the value returned by </span><span class="emphasis"><em><span class="koboSpan" id="kobo.19.1">s </span><sup><span class="koboSpan" id="kobo.20.1">(t-1)</span></sup></em></span><span class="koboSpan" id="kobo.21.1">, the value of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.22.1">s </span><sup><span class="koboSpan" id="kobo.23.1">(t-1)</span></sup></em></span><span class="koboSpan" id="kobo.24.1"> will require the value of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.25.1">s </span><sup><span class="koboSpan" id="kobo.26.1">(t-2)</span></sup></em></span><span class="koboSpan" id="kobo.27.1">, and so on.</span></p><p><span class="koboSpan" id="kobo.28.1">This is a simple representation of a dynamic system for understanding purpose. </span><span class="koboSpan" id="kobo.28.2">Let us take one more example, where the dynamic system is driven by an external signal </span><span class="emphasis"><em><span class="koboSpan" id="kobo.29.1">x </span><sup><span class="koboSpan" id="kobo.30.1">(t)</span></sup></em></span><span class="koboSpan" id="kobo.31.1">, and produces output </span><span class="emphasis"><em><span class="koboSpan" id="kobo.32.1">y </span><sup><span class="koboSpan" id="kobo.33.1">(t)</span></sup></em></span><span class="koboSpan" id="kobo.34.1">:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.35.1"><img src="graphics/image_04_003-1.jpg" alt="Unfolding recurrent computations"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.36.1">RNNs, ideally, follow the second type of equation, where the intermediate state retains the information about the whole past sequence. </span><span class="koboSpan" id="kobo.36.2">However, any equation that involves recurrence can be used to model the RNN.</span></p><p><span class="koboSpan" id="kobo.37.1">Therefore, similar to the feed-forward neural networks, the state of the hidden (intermediate) layers of RNNs can be defined using the variable </span><span class="emphasis"><em><span class="koboSpan" id="kobo.38.1">h</span></em></span><span class="koboSpan" id="kobo.39.1"> at time </span><span class="emphasis"><em><span class="koboSpan" id="kobo.40.1">t</span></em></span><span class="koboSpan" id="kobo.41.1">, as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.42.1"><img src="graphics/image_04_004-1.jpg" alt="Unfolding recurrent computations"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.43.1">We will explain the functionality of this preceding equation in a RNN in the next part of this section. </span><span class="koboSpan" id="kobo.43.2">As of now, to illustrate the functionality of this hidden layer, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.44.1">Figure 4.2</span></em></span><span class="koboSpan" id="kobo.45.1"> shows a simple recurrent network with no output. </span><span class="koboSpan" id="kobo.45.2">The left side of the figure shows a network whose current state influences the next state. </span><span class="koboSpan" id="kobo.45.3">The box in the middle of the loop represents the delay between two successive time steps.</span></p><p><span class="koboSpan" id="kobo.46.1">As shown in the preceding recurrent equation, we can unfold or unroll the hidden states in time. </span><span class="koboSpan" id="kobo.46.2">The right side of the image shows the unfolded structure of the recurrent network. </span><span class="koboSpan" id="kobo.46.3">There, the network can be converted to a feed-forward network by unfolding over time.</span></p><p><span class="koboSpan" id="kobo.47.1">In an unfolded network, each variable for each time step can be shown as a separate node of the network.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.48.1"><img src="graphics/image_04_005.jpg" alt="Unfolding recurrent computations"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.49.1">Figure 4.2: The left part of the figure shows the recurrent network where the information passes through multiple times through the hidden layer with each time step. </span><span class="koboSpan" id="kobo.49.2">On the right, we have the unfolded structure of the same network. </span><span class="koboSpan" id="kobo.49.3">Each node of this network is associated with one timestamp.</span></p><p><span class="koboSpan" id="kobo.50.1">So, from </span><span class="emphasis"><em><span class="koboSpan" id="kobo.51.1">Figure 4.2</span></em></span><span class="koboSpan" id="kobo.52.1">, the unfolding operation can be defined as an operation that performs the mapping of the circuit on the left-hand side to a computational model split into multiple states on the right-hand side.</span></p><div class="section" title="Advantages of a model unfolded in time"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec15"/><span class="koboSpan" id="kobo.53.1">Advantages of a model unfolded in time</span></h3></div></div></div><p><span class="koboSpan" id="kobo.54.1">Unfolding a network in time provides a few major advantages, which are listed as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.55.1">A model without a parameter would require many training examples for learning purposes. </span><span class="koboSpan" id="kobo.55.2">However, learning a shared single model helps to generalize the sequence lengths, even those that are not present in the training set. </span><span class="koboSpan" id="kobo.55.3">This allows the model to estimate the upcoming sequence data with fewer training examples.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.56.1">Irrespective of the length of the sequence, the input size of the model will always remain the same. </span><span class="koboSpan" id="kobo.56.2">The input size in an unfolded model is specified in terms of transition from the hidden state to the other. </span><span class="koboSpan" id="kobo.56.3">However, for other cases, it is specified in terms of undefined length of the history of states.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.57.1">Due to parameter sharing, the same transition function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.58.1">f</span></em></span><span class="koboSpan" id="kobo.59.1">, with the same parameter can be used at every time step.</span></li></ul></div></div></div><div class="section" title="Memory of RNNs"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec38"/><span class="koboSpan" id="kobo.60.1">Memory of RNNs</span></h2></div></div></div><p><span class="koboSpan" id="kobo.61.1">As of now, you might have got some idea that the primary difference between a feed forward neural network and recurrent network is the feedback loop. </span><span class="koboSpan" id="kobo.61.2">The feedback loop is ingested into its own intermediate outcome as the input to the next state. </span><span class="koboSpan" id="kobo.61.3">The same task is performed for every element of the input sequence. </span><span class="koboSpan" id="kobo.61.4">Hence, the output of each hidden state depends on the previous computations. </span><span class="koboSpan" id="kobo.61.5">In a practical situation, each hidden state is not only concerned about the current input sequence in action, but also about what they perceived one step back in time. </span><span class="koboSpan" id="kobo.61.6">So, ideally, every hidden state must have all the information of the previous step's outcome.</span></p><p><span class="koboSpan" id="kobo.62.1">Due to this requirement of persistent information, it is said that RNNs have their </span><span class="emphasis"><em><span class="koboSpan" id="kobo.63.1">own memory</span></em></span><span class="koboSpan" id="kobo.64.1">. </span><span class="koboSpan" id="kobo.64.2">The sequential information is preserved as memory in the recurrent network's hidden state. </span><span class="koboSpan" id="kobo.64.3">This helps to handle the upcoming time steps as the network cascades forward to update the processing with each new sequence.</span></p><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.65.1">Figure 4.3</span></em></span><span class="koboSpan" id="kobo.66.1"> shows the concept of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.67.1">simple recurrent neural networks</span></em></span><span class="koboSpan" id="kobo.68.1"> proposed by Elman back in 1990 [112]; it shows the illustration of persistent memory for a RNN.</span></p><p><span class="koboSpan" id="kobo.69.1">In the next figure, a part of the word sequence AYSXWQF at the bottom represents the input example currently under consideration. </span><span class="koboSpan" id="kobo.69.2">Each box of this input example represents a pool of units. </span><span class="koboSpan" id="kobo.69.3">The forward arrow shows the complete set of trainable mapping from each sending input unit to each output unit for the next time step. </span><span class="koboSpan" id="kobo.69.4">The context unit, which can be considered as the persistent memory unit, preserves the output of the previous steps. </span><span class="koboSpan" id="kobo.69.5">The backward arrow, directed from the hidden layer to the context unit shows a copy operation of the output, used for evaluating the outcome for the next time step.</span></p><p><span class="koboSpan" id="kobo.70.1">The decision where a RNN reaches at time step </span><span class="emphasis"><em><span class="koboSpan" id="kobo.71.1">t</span></em></span><span class="koboSpan" id="kobo.72.1">, depends mostly on its last decision of the time step at </span><span class="emphasis"><em><span class="koboSpan" id="kobo.73.1">(t-1)</span></em></span><span class="koboSpan" id="kobo.74.1">. </span><span class="koboSpan" id="kobo.74.2">Therefore, it can be inferred that unlike traditional neural networks, RNNs have two sources of input.</span></p><p><span class="koboSpan" id="kobo.75.1">One is the current input unit under consideration, which is </span><span class="emphasis"><em><span class="koboSpan" id="kobo.76.1">X</span></em></span><span class="koboSpan" id="kobo.77.1"> in the following figure, and the other one is the information received from the recent past, which is taken from the context units in the figure. </span><span class="koboSpan" id="kobo.77.2">The two sources, in combination, decide the output of the current time step. </span><span class="koboSpan" id="kobo.77.3">More about this will be discussed in the next section.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.78.1"><img src="graphics/image_04_006.jpg" alt="Memory of RNNs"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.79.1">Figure 4.3: A simple recurrent neural networks with the concept of memory of RNN is shown in this figure.</span></p></div><div class="section" title="Architecture"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec39"/><span class="koboSpan" id="kobo.80.1">Architecture</span></h2></div></div></div><p><span class="koboSpan" id="kobo.81.1">So, we have come to know that RNNs have their memory, which collects information about what has been computed so far. </span><span class="koboSpan" id="kobo.81.2">In this section, we will discuss the general architecture of RNNs and their functioning.</span></p><p><span class="koboSpan" id="kobo.82.1">A typical RNN, unfolded (or unrolled) at the time of the calculation involved in its forward computation is shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.83.1">Figure 4.4</span></em></span><span class="koboSpan" id="kobo.84.1">.</span></p><p><span class="koboSpan" id="kobo.85.1">Unrolling or unfolding a network means to write out the network for the complete sequences of input. </span><span class="koboSpan" id="kobo.85.2">Let us take an example before we start explaining the architecture. </span><span class="koboSpan" id="kobo.85.3">If we have a sequence of 10 words, the RNN would then be unfolded into a 10-layer deep neural network, one layer for each word, as depicted by the following diagram:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.86.1"><img src="graphics/image_04_007.jpg" alt="Architecture"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.87.1">Figure 4.4: The figure shows a RNN being unrolled or unfolded into a full network.</span></p><p><span class="koboSpan" id="kobo.88.1">The time period to reach from input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.89.1">x</span></em></span><span class="koboSpan" id="kobo.90.1"> to output </span><span class="emphasis"><em><span class="koboSpan" id="kobo.91.1">o</span></em></span><span class="koboSpan" id="kobo.92.1"> is split into several timestamps given by </span><span class="emphasis"><em><span class="koboSpan" id="kobo.93.1">(t-1)</span></em></span><span class="koboSpan" id="kobo.94.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.95.1">t</span></em></span><span class="koboSpan" id="kobo.96.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.97.1">(t+1)</span></em></span><span class="koboSpan" id="kobo.98.1">, and so on.</span></p><p><span class="koboSpan" id="kobo.99.1">The computational steps and formulae for an RNN are listed as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.100.1">In the preceding figure, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.101.1">x</span><sub><span class="koboSpan" id="kobo.102.1">t</span></sub></em></span><span class="koboSpan" id="kobo.103.1"> is the input at time step </span><span class="emphasis"><em><span class="koboSpan" id="kobo.104.1">t</span></em></span><span class="koboSpan" id="kobo.105.1">. </span><span class="koboSpan" id="kobo.105.2">The figure shows computations for three timestamps </span><span class="emphasis"><em><span class="koboSpan" id="kobo.106.1">(t-1)</span></em></span><span class="koboSpan" id="kobo.107.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.108.1">t</span></em></span><span class="koboSpan" id="kobo.109.1">, and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.110.1">(t+1)</span></em></span><span class="koboSpan" id="kobo.111.1">, where the inputs are </span><span class="emphasis"><em><span class="koboSpan" id="kobo.112.1">x</span><sub><span class="koboSpan" id="kobo.113.1">(t-1)</span></sub></em></span><span class="koboSpan" id="kobo.114.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.115.1">x</span><sub><span class="koboSpan" id="kobo.116.1">t</span></sub></em></span><span class="koboSpan" id="kobo.117.1">, and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.118.1">x</span><sub><span class="koboSpan" id="kobo.119.1">(t+1)</span></sub><span class="koboSpan" id="kobo.120.1">,</span></em></span><span class="koboSpan" id="kobo.121.1"> respectively. </span><span class="koboSpan" id="kobo.121.2">For example, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.122.1">x</span><sub><span class="koboSpan" id="kobo.123.1">1</span></sub></em></span><span class="koboSpan" id="kobo.124.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.125.1">x</span><sub><span class="koboSpan" id="kobo.126.1">2</span></sub></em></span><span class="koboSpan" id="kobo.127.1"> are the vectors that correspond to the second and third word of the sequence.</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.128.1">s</span><sub><span class="koboSpan" id="kobo.129.1">t</span></sub></em></span><span class="koboSpan" id="kobo.130.1"> represents the hidden state at time step </span><span class="emphasis"><em><span class="koboSpan" id="kobo.131.1">t</span></em></span><span class="koboSpan" id="kobo.132.1">. </span><span class="koboSpan" id="kobo.132.2">Conceptually, this state defines the memory of the neural network. </span><span class="koboSpan" id="kobo.132.3">Mathematically, the formulation for </span><span class="emphasis"><em><span class="koboSpan" id="kobo.133.1">s</span><sub><span class="koboSpan" id="kobo.134.1">t</span></sub></em></span><span class="koboSpan" id="kobo.135.1"> or the process of carrying memory can be written as follows:</span></li></ul></div><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.136.1"><img src="graphics/image_04_008-1.jpg" alt="Architecture"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.137.1">So, the hidden state is a function of the input at time step </span><span class="emphasis"><em><span class="koboSpan" id="kobo.138.1">x</span><sub><span class="koboSpan" id="kobo.139.1">t</span></sub></em></span><span class="koboSpan" id="kobo.140.1">, multiplied by the weight </span><span class="emphasis"><em><span class="koboSpan" id="kobo.141.1">U</span></em></span><span class="koboSpan" id="kobo.142.1">, and addition of the hidden state of the last time step </span><span class="emphasis"><em><span class="koboSpan" id="kobo.143.1">s</span><sub><span class="koboSpan" id="kobo.144.1">t-1</span></sub></em></span><span class="koboSpan" id="kobo.145.1">, which is multiplied by its own hidden-state-to-hidden-state matrix </span><span class="emphasis"><em><span class="koboSpan" id="kobo.146.1">W</span></em></span><span class="koboSpan" id="kobo.147.1">. </span><span class="koboSpan" id="kobo.147.2">This hidden-state-to-hidden-state is often termed as a transition matrix, and is similar to a Markov chain. </span><span class="koboSpan" id="kobo.147.3">The weight matrices behave as filters, which primarily decide the importance of both, the past hidden state and the current input. </span><span class="koboSpan" id="kobo.147.4">The error generated for the current state would be sent back via backpropagation to update these weights until the error is minimized to the desired value.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note17"/><span class="koboSpan" id="kobo.148.1">Note</span></h3><p><span class="koboSpan" id="kobo.149.1">To calculate the first hidden state, we would require determining the value </span><span class="emphasis"><em><span class="koboSpan" id="kobo.150.1">s-1</span></em></span><span class="koboSpan" id="kobo.151.1">, which is generally initialized to all zeroes.</span></p></div></div><p><span class="koboSpan" id="kobo.152.1">Unlike a traditional deep neural network, where different parameters are used for the computation at each layer, a RNN shares the same parameters (here, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.153.1">U</span></em></span><span class="koboSpan" id="kobo.154.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.155.1">V</span></em></span><span class="koboSpan" id="kobo.156.1">, and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.157.1">W</span></em></span><span class="koboSpan" id="kobo.158.1">) across all the time steps to calculate the value of the hidden layer. </span><span class="koboSpan" id="kobo.158.2">This makes the life of a neural network much easier, as we need to learn fewer number of parameters.</span></p><p><span class="koboSpan" id="kobo.159.1">This sum of the weight input and hidden state is squeezed by the function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.160.1">f</span></em></span><span class="koboSpan" id="kobo.161.1">, which usually is a nonlinearity such as a logistic sigmoid function, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.162.1">tan h</span></em></span><span class="koboSpan" id="kobo.163.1">, or ReLU:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.164.1">In the last figure, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.165.1">o</span><sub><span class="koboSpan" id="kobo.166.1">t</span></sub></em></span><span class="koboSpan" id="kobo.167.1"> is represented as the output at time step t. </span><span class="koboSpan" id="kobo.167.2">The output at step </span><span class="emphasis"><em><span class="koboSpan" id="kobo.168.1">o</span><sub><span class="koboSpan" id="kobo.169.1">t</span></sub></em></span><span class="koboSpan" id="kobo.170.1"> is solely computed based on the memory available for the network at time t. </span><span class="koboSpan" id="kobo.170.2">Theoretically, although RNNs can persist memory for arbitrarily long sequences, in practice, it's a bit complicated, and they are limited to looking back only a few time steps. </span><span class="koboSpan" id="kobo.170.3">Mathematically, this can be represented as follows:</span></li></ul></div><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.171.1"><img src="graphics/image_04_009-1.jpg" alt="Architecture"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.172.1">The next section shall discuss how to train a RNN through back propagation.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Backpropagation through time (BPTT)"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec26"/><span class="koboSpan" id="kobo.1.1">Backpropagation through time (BPTT)</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">You have already learnt that the primary requirement of RNNs is to distinctly classify the sequential inputs. </span><span class="koboSpan" id="kobo.2.2">The backpropagation of error and gradient descent primarily help to perform these tasks.</span></p><p><span class="koboSpan" id="kobo.3.1">In case of feed forward neural networks, backpropagation moves in the backward direction from the final error outputs, weights, and inputs of each hidden layer. </span><span class="koboSpan" id="kobo.3.2">Backpropagation assigns the weights responsible for generating the error, by calculating their partial derivatives: 
</span><span class="inlinemediaobject"><span class="koboSpan" id="kobo.4.1"><img src="graphics/B05883_04_16.jpg" alt="Backpropagation through time (BPTT)"/></span></span><span class="koboSpan" id="kobo.5.1">
where </span><span class="emphasis"><em><span class="koboSpan" id="kobo.6.1">E</span></em></span><span class="koboSpan" id="kobo.7.1"> denotes the error and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.8.1">w</span></em></span><span class="koboSpan" id="kobo.9.1"> is the respective weights. </span><span class="koboSpan" id="kobo.9.2">The derivatives are applied on the learning rate, and the gradient decreases to update the weights so as to minimize the error rate.</span></p><p><span class="koboSpan" id="kobo.10.1">However, a RNN, without using backpropagation directly, uses an extension of it, termed as </span><span class="strong"><strong><span class="koboSpan" id="kobo.11.1">backpropagation through time</span></strong></span><span class="koboSpan" id="kobo.12.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.13.1">BPTT</span></strong></span><span class="koboSpan" id="kobo.14.1">). </span><span class="koboSpan" id="kobo.14.2">In this section, we will discuss BPTT to explain how the training works for RNNs.</span></p><div class="section" title="Error computation"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec40"/><span class="koboSpan" id="kobo.15.1">Error computation</span></h2></div></div></div><p><span class="koboSpan" id="kobo.16.1">The </span><span class="strong"><strong><span class="koboSpan" id="kobo.17.1">backpropagation through time</span></strong></span><span class="koboSpan" id="kobo.18.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.19.1">BPTT</span></strong></span><span class="koboSpan" id="kobo.20.1">) learning algorithm is a natural extension of the traditional backpropagation method, which computes the gradient descent on a complete unrolled neural network.</span></p><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.21.1">Figure 4.5</span></em></span><span class="koboSpan" id="kobo.22.1"> shows the errors associated with each hidden state for an unrolled RNN. </span><span class="koboSpan" id="kobo.22.2">Mathematically, the errors associated with each state can be given as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.23.1"><img src="graphics/image_04_011-1.jpg" alt="Error computation"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.24.1">where </span><span class="emphasis"><em><span class="koboSpan" id="kobo.25.1">o</span><sub><span class="koboSpan" id="kobo.26.1">t</span></sub></em></span><span class="koboSpan" id="kobo.27.1"> represents the correct output, and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.28.1">ô</span><sub><span class="koboSpan" id="kobo.29.1">t</span></sub></em></span><span class="koboSpan" id="kobo.30.1"> represents the predicted word at time step </span><span class="emphasis"><em><span class="koboSpan" id="kobo.31.1">t</span></em></span><span class="koboSpan" id="kobo.32.1">. </span><span class="koboSpan" id="kobo.32.2">The total error (cost function) of the whole network is calculated as the summation of all the intermediate errors at each time step.</span></p><p><span class="koboSpan" id="kobo.33.1">If the RNN is unfolded into multiple time steps, starting from </span><span class="emphasis"><em><span class="koboSpan" id="kobo.34.1">t</span><sub><span class="koboSpan" id="kobo.35.1">0</span></sub></em></span><span class="koboSpan" id="kobo.36.1"> to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.37.1">t</span><sub><span class="koboSpan" id="kobo.38.1">n-1</span></sub></em></span><span class="koboSpan" id="kobo.39.1">, the total error can be written as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.40.1"><img src="graphics/image_04_012-1.jpg" alt="Error computation"/></span></div><p>
</p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.41.1"><img src="graphics/image_04_013.jpg" alt="Error computation"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.42.1">Figure 4.5: The figure shows errors associated with every time step for a RNN.</span></p><p><span class="koboSpan" id="kobo.43.1">In the backpropagation through time method, unlike the traditional method, the gradient descent weight is updated in each time step.</span></p><p><span class="koboSpan" id="kobo.44.1">Let </span><span class="emphasis"><em><span class="koboSpan" id="kobo.45.1">w</span><sub><span class="koboSpan" id="kobo.46.1">ij</span></sub></em></span><span class="koboSpan" id="kobo.47.1"> denote the connection of weight from neuron </span><span class="emphasis"><em><span class="koboSpan" id="kobo.48.1">i</span></em></span><span class="koboSpan" id="kobo.49.1"> to neuron </span><span class="emphasis"><em><span class="koboSpan" id="kobo.50.1">j</span></em></span><span class="koboSpan" id="kobo.51.1">. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.52.1">η</span></em></span><span class="koboSpan" id="kobo.53.1"> denotes the learning rate of the network. </span><span class="koboSpan" id="kobo.53.2">So, mathematically, the weight update with gradient descent at every time step is given by the following equation:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.54.1"><img src="graphics/B05883_04.jpg" alt="Error computation"/></span></div><p>
</p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Long short-term memory"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec27"/><span class="koboSpan" id="kobo.1.1">Long short-term memory</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">In this section, we will discuss a special unit called </span><span class="strong"><strong><span class="koboSpan" id="kobo.3.1">Long short-term memory</span></strong></span><span class="koboSpan" id="kobo.4.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.5.1">LSTM</span></strong></span><span class="koboSpan" id="kobo.6.1">), which is integrated into RNN. </span><span class="koboSpan" id="kobo.6.2">The main purpose of LSTM is to prevent a significant problem of RNN, called the vanishing gradient problem.</span></p><div class="section" title="Problem with deep backpropagation with time"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec41"/><span class="koboSpan" id="kobo.7.1">Problem with deep backpropagation with time</span></h2></div></div></div><p><span class="koboSpan" id="kobo.8.1">Unlike the traditional feed forward network, due to unrolling of a RNN with narrow time steps, the feed forward network generated this way could be aggressively deep. </span><span class="koboSpan" id="kobo.8.2">This sometimes makes it extremely difficult to train via backpropagation through the time procedure.</span></p><p><span class="koboSpan" id="kobo.9.1">In the first chapter, we discussed the vanishing gradient problem. </span><span class="koboSpan" id="kobo.9.2">An unfolded RNN suffers from the vanishing gradient problem of exploding while performing backpropagation through time.</span></p><p><span class="koboSpan" id="kobo.10.1">Every state of a RNN depends on its input and its previous output multiplied by the current hidden state vector. </span><span class="koboSpan" id="kobo.10.2">The same operations happen to the gradient in the reverse direction during backpropagation through time. </span><span class="koboSpan" id="kobo.10.3">The layers and numerous time steps of the unfolded RNN relate to each other through multiplication, hence the derivatives are susceptible to vanish with every pass.</span></p><p><span class="koboSpan" id="kobo.11.1">On the other hand, a small gradient tends to get smaller, while a large gradient gets even larger while passing through every time step. </span><span class="koboSpan" id="kobo.11.2">This creates the vanishing or exploding gradient problem respectively for a RNN.</span></p></div><div class="section" title="Long short-term memory"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec42"/><span class="koboSpan" id="kobo.12.1">Long short-term memory</span></h2></div></div></div><p><span class="koboSpan" id="kobo.13.1">In the mid-90s, an updated version of RNNs with a special unit, called </span><span class="strong"><strong><span class="koboSpan" id="kobo.14.1">Long short-term memory</span></strong></span><span class="koboSpan" id="kobo.15.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.16.1">LSTM</span></strong></span><span class="koboSpan" id="kobo.17.1">) units, was proposed by German researchers Sepp Hochreiter and Juergen Schmidhuber [116] to protect against the exploding or vanishing gradient problem.</span></p><p><span class="koboSpan" id="kobo.18.1">LSTM helps to maintain a constant error, which can be propagated though time and through each layer of the network. </span><span class="koboSpan" id="kobo.18.2">This preservation of constant error allows the unrolled recurrent networks to learn on an aggressively deep network, even unrolled by a thousand time steps. </span><span class="koboSpan" id="kobo.18.3">This eventually opens a channel to link the causes of effects remotely.</span></p><p><span class="koboSpan" id="kobo.19.1">The architecture of LSTM maintains a constant error flow through the internal state of special memory units. </span><span class="koboSpan" id="kobo.19.2">The following figure (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.20.1">Figure 4.6</span></em></span><span class="koboSpan" id="kobo.21.1">) shows a basic block diagram of a LSTM for easy understanding:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.22.1"><img src="graphics/image_04_015.jpg" alt="Long short-term memory"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.23.1">Figure 4.6: The figure shows a basic model of the Long-short term memorys</span></p><p><span class="koboSpan" id="kobo.24.1">As shown in the preceding figure, an LSTM unit is composed of a memory cell that primarily stores information for long periods of time. </span><span class="koboSpan" id="kobo.24.2">Three specialized gate neurons-- write gate, read gate, and forget gate-protect the access to this memory cell. </span><span class="koboSpan" id="kobo.24.3">Unlike the digital storage of computers, the gates are analog in nature, with a range of 0 to 1. </span><span class="koboSpan" id="kobo.24.4">Analog devices have an added advantage over digital ones, as they are differentiable, and hence, serve the purpose for the backpropagation method. </span><span class="koboSpan" id="kobo.24.5">The gate cell of LSTM, instead of forwarding the information as inputs to the next neurons, sets the associated weights connecting the rest of the neural network to the memory cell. </span><span class="koboSpan" id="kobo.24.6">The memory cell is, basically, a self-connected linear neuron. </span><span class="koboSpan" id="kobo.24.7">When the forget cell is reset (turned </span><span class="strong"><strong><span class="koboSpan" id="kobo.25.1">0</span></strong></span><span class="koboSpan" id="kobo.26.1">), the memory cell writes its content to itself and remembers the last content of the memory. </span><span class="koboSpan" id="kobo.26.2">For a memory write operation though, the forget gate and write get should be set (turned </span><span class="strong"><strong><span class="koboSpan" id="kobo.27.1">1</span></strong></span><span class="koboSpan" id="kobo.28.1">). </span><span class="koboSpan" id="kobo.28.2">Also, when the forget gate outputs something close to </span><span class="strong"><strong><span class="koboSpan" id="kobo.29.1">1</span></strong></span><span class="koboSpan" id="kobo.30.1">, the memory cell effectively forgets all the previous contents that it had stored. </span><span class="koboSpan" id="kobo.30.2">Now, when the write gate is set, it allows any information to write into its memory cell. Similarly, when the read gate outputs a </span><span class="strong"><strong><span class="koboSpan" id="kobo.31.1">1</span></strong></span><span class="koboSpan" id="kobo.32.1">, it will allow the rest of the network to read from its memory cell.</span></p><p><span class="koboSpan" id="kobo.33.1">As explained earlier, the problem with computing the gradient descent for traditional RNNs is that the error gradient vanishes rapidly while propagating through the time steps in the unfolded network. </span><span class="koboSpan" id="kobo.33.2">Adding an LSTM unit, the error values when backpropagated from the output are collected in the memory cell of the LSTM units. </span><span class="koboSpan" id="kobo.33.3">This phenomenon is also known as </span><span class="emphasis"><em><span class="koboSpan" id="kobo.34.1">error carousel</span></em></span><span class="koboSpan" id="kobo.35.1">. </span><span class="koboSpan" id="kobo.35.2">We will use the following example to describe how LSTM overcomes the vanishing gradient problem for RNNs:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.36.1"><img src="graphics/B05883_04_07-2.jpg" alt="Long short-term memory"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.37.1">Figure 4.7: The figure shows a Long-short term memory unfolded in time. </span><span class="koboSpan" id="kobo.37.2">It also depicts how the content of the memory cell is protected with the help of three gates.</span></p><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.38.1">Figure 4.7</span></em></span><span class="koboSpan" id="kobo.39.1"> shows a Long short-term memory unit unrolled through time. </span><span class="koboSpan" id="kobo.39.2">We will start with initializing the value of the forget gate to </span><span class="strong"><strong><span class="koboSpan" id="kobo.40.1">1</span></strong></span><span class="koboSpan" id="kobo.41.1"> and write gate to </span><span class="strong"><strong><span class="koboSpan" id="kobo.42.1">1</span></strong></span><span class="koboSpan" id="kobo.43.1">. </span><span class="koboSpan" id="kobo.43.2">As shown in the preceding figure, this will write an information </span><span class="strong"><strong><span class="koboSpan" id="kobo.44.1">K</span></strong></span><span class="koboSpan" id="kobo.45.1"> into the memory cell. </span><span class="koboSpan" id="kobo.45.2">After writing, this value is retained in the memory cell by setting the value of the forget gate to </span><span class="strong"><strong><span class="koboSpan" id="kobo.46.1">0</span></strong></span><span class="koboSpan" id="kobo.47.1">. </span><span class="koboSpan" id="kobo.47.2">We then set the value of the read gate as </span><span class="strong"><strong><span class="koboSpan" id="kobo.48.1">1</span></strong></span><span class="koboSpan" id="kobo.49.1">, which reads and outputs the value </span><span class="strong"><strong><span class="koboSpan" id="kobo.50.1">K</span></strong></span><span class="koboSpan" id="kobo.51.1"> from the memory cell. </span><span class="koboSpan" id="kobo.51.2">From the point of loading </span><span class="strong"><strong><span class="koboSpan" id="kobo.52.1">K</span></strong></span><span class="koboSpan" id="kobo.53.1"> into the memory cell to the point of reading the same from the memory cell backpropagation through time is followed.</span></p><p><span class="koboSpan" id="kobo.54.1">The error derivatives received from the read point backpropagate through the network with some nominal changes, until the write point. </span><span class="koboSpan" id="kobo.54.2">This happens because of the linear nature of the memory neuron. </span><span class="koboSpan" id="kobo.54.3">Thus, with this operation, we can maintain the error derivatives over hundreds of steps without going into the trap of the vanishing gradient problems.</span></p><p><span class="koboSpan" id="kobo.55.1">So there are many reasons for why Long short-term memory outperforms standard RNNs. </span><span class="koboSpan" id="kobo.55.2">LSTM was able to achieve the best known result in unsegmented connected handwriting recognition [117]; also, it is equally successfully applied to automated speech recognition. </span><span class="koboSpan" id="kobo.55.3">As of now, the major technological companies such as Apple, Microsoft, Google, Baidu, and so on have started to widely use LSTM networks as a primary component for their latest products [118].</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Bi-directional RNNs"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"/><span class="koboSpan" id="kobo.1.1">Bi-directional RNNs</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">This section of the chapter will discuss the major limitations of RNNs and how bi-directional RNN, a special type of RNN helps to overcome those shortfalls. </span><span class="koboSpan" id="kobo.2.2">Bi-directional neural networks, apart from taking inputs from the past, takes the information from the future context for its required prediction.</span></p><div class="section" title="Shortfalls of RNNs"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec43"/><span class="koboSpan" id="kobo.3.1">Shortfalls of RNNs</span></h2></div></div></div><p><span class="koboSpan" id="kobo.4.1">The computation power of standard or unidirectional RNNs has constraints, as the current state cannot reach its future input information. </span><span class="koboSpan" id="kobo.4.2">In many cases, the future input information coming up later becomes extremely useful for sequence prediction. </span><span class="koboSpan" id="kobo.4.3">For example, in speech recognition, due to linguistic dependencies, the appropriate interpretation of the voice as a phoneme might depend on the next few spoken words. </span><span class="koboSpan" id="kobo.4.4">The same situation might also arise in handwriting recognition.</span></p><p><span class="koboSpan" id="kobo.5.1">In some modified versions of RNN, this feature is partially attained by inserting some delay of a certain amount (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.6.1">N</span></em></span><span class="koboSpan" id="kobo.7.1">) of time steps in the output. </span><span class="koboSpan" id="kobo.7.2">This delay helps to capture the future information to predict the data. </span><span class="koboSpan" id="kobo.7.3">Although, theoretically, in order to capture most of the available future information, the value of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.8.1">N</span></em></span><span class="koboSpan" id="kobo.9.1"> can be set as very large, but in a practical scenario, the prediction power of the model actually reduces with a large value of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.10.1">N</span></em></span><span class="koboSpan" id="kobo.11.1">. </span><span class="koboSpan" id="kobo.11.2">The paper [113] has put some logical explanation for this inference. </span><span class="koboSpan" id="kobo.11.3">As the value of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.12.1">N</span></em></span><span class="koboSpan" id="kobo.13.1"> increases, most of the computational power of a RNN only focuses on remembering the input information for 
</span><span class="inlinemediaobject"><span class="koboSpan" id="kobo.14.1"><img src="graphics/image_04_017.jpg" alt="Shortfalls of RNNs"/></span></span><span class="koboSpan" id="kobo.15.1">

 (from </span><span class="emphasis"><em><span class="koboSpan" id="kobo.16.1">Figure 4.8</span></em></span><span class="koboSpan" id="kobo.17.1">) to predict the outcome, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.18.1">y</span><sub><span class="koboSpan" id="kobo.19.1">tc</span></sub></em></span><span class="koboSpan" id="kobo.20.1">. </span><span class="koboSpan" id="kobo.20.2">(</span><span class="emphasis"><em><span class="koboSpan" id="kobo.21.1">t</span><sub><span class="koboSpan" id="kobo.22.1">c</span></sub></em></span><span class="koboSpan" id="kobo.23.1"> in figure denotes the current time step in consideration). </span><span class="koboSpan" id="kobo.23.2">Therefore, the model will have less processing power to combine the prediction knowledge received from different input vectors. </span><span class="koboSpan" id="kobo.23.3">The following </span><span class="emphasis"><em><span class="koboSpan" id="kobo.24.1">Figure 4.8</span></em></span><span class="koboSpan" id="kobo.25.1"> shows an illustration of the amount of input information needed for different types of RNNs:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.26.1"><img src="graphics/image_04_018-1.jpg" alt="Shortfalls of RNNs"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.27.1">Figure 4.8: The figure shows visualizations of input information used by different types of RNNs. </span><span class="koboSpan" id="kobo.27.2">[113]</span></p></div><div class="section" title="Solutions to overcome"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec44"/><span class="koboSpan" id="kobo.28.1">Solutions to overcome</span></h2></div></div></div><p><span class="koboSpan" id="kobo.29.1">To subjugate the limitations of a unidirectional RNN explained in the last section, </span><span class="strong"><strong><span class="koboSpan" id="kobo.30.1">bidirectional recurrent network</span></strong></span><span class="koboSpan" id="kobo.31.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.32.1">BRNN</span></strong></span><span class="koboSpan" id="kobo.33.1">) was invented in 1997 [113].</span></p><p><span class="koboSpan" id="kobo.34.1">The basic idea behind bidirectional RNN is to split the hidden state of a regular RNN into two parts. </span><span class="koboSpan" id="kobo.34.2">One part is responsible for the forward states (positive time direction), and the other part for the backward states (negative time direction). </span><span class="koboSpan" id="kobo.34.3">Outputs generated from the forward states are not connected to the inputs of the backward states, and vice versa. </span><span class="koboSpan" id="kobo.34.4">A simple version of a bidirectional RNN, unrolled in three time steps is shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.35.1">Figure 4.9</span></em></span><span class="koboSpan" id="kobo.36.1">.</span></p><p><span class="koboSpan" id="kobo.37.1">With this structure, as both the time directions are taken care of, the currently evaluated time frame can easily use the input information from the past and the future. </span><span class="koboSpan" id="kobo.37.2">So, the objective function of the current output will eventually minimize, as we do not need to put further delays to include the future information. </span><span class="koboSpan" id="kobo.37.3">This was necessary for regular RNNs as stated in the last section.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.38.1"><img src="graphics/image_04_019.jpg" alt="Solutions to overcome"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.39.1">Figure 4.9: The figure shows the conventional structure of a bidirectional neural network unrolled in three time steps.</span></p><p><span class="koboSpan" id="kobo.40.1">So far, bidirectional RNNs have been found to be extremely useful in applications such as speech recognition [114], handwriting recognition, bioinformatics [115], and so on.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Distributed deep RNNs"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"/><span class="koboSpan" id="kobo.1.1">Distributed deep RNNs</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">As you now have an understanding of a RNN, its applications, features, and architecture, we can now move on to discuss how to use this network as distributed architecture. </span><span class="koboSpan" id="kobo.2.2">Distributing RNN is not an easy task, and hence, only a few researchers have worked on this in the past. </span><span class="koboSpan" id="kobo.2.3">Although the primary concept of data parallelism is similar for all the networks, distributing RNNs among multiple servers requires some brainstorming and a bit tedious work too.</span></p><p><span class="koboSpan" id="kobo.3.1">Recently, one work from Google [119] has tried to distribute recurrent networks in many servers in a speech recognition task. </span><span class="koboSpan" id="kobo.3.2">In this section, we will discuss this work on distributed RNNs with the help of Hadoop.</span></p><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.4.1">Asynchronous stochastic gradient descent</span></strong></span><span class="koboSpan" id="kobo.5.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.6.1">ASGD</span></strong></span><span class="koboSpan" id="kobo.7.1">) can be used for large-scale training of a RNN. </span><span class="koboSpan" id="kobo.7.2">ASGD has particularly shown success in sequence discriminative training of the deep neural networks.</span></p><p><span class="koboSpan" id="kobo.8.1">A two-layer deep Long short-term memory RNN is used to build the Long short-term memory network. </span><span class="koboSpan" id="kobo.8.2">Each Long short-term memory consists of 800 memory cells. </span><span class="koboSpan" id="kobo.8.3">The paper uses 13 million parameters for the LSTM network. </span><span class="koboSpan" id="kobo.8.4">For cell input and output units tan h (hyperbolic tangent activation) is used, and for the write, read, and forget gates, the logistic sigmoid function is used.</span></p><p><span class="koboSpan" id="kobo.9.1">For training purposes, the input speech training data can be split and randomly shuffled across multiple DataNodes of the Hadoop framework. </span><span class="koboSpan" id="kobo.9.2">The Long short-term memory is put across all these DataNodes, and distributed training is performed on those datasets in parallel. </span><span class="koboSpan" id="kobo.9.3">Asynchronous stochastic gradient descent is used for this distributed training. </span><span class="koboSpan" id="kobo.9.4">One parameter server, dedicated for maintaining the current state of all model parameters, is used.</span></p><p><span class="koboSpan" id="kobo.10.1">To implement this procedure on Hadoop, each DataNode has to perform asynchronous stochastic gradient descent operations on the partitioned data. </span><span class="koboSpan" id="kobo.10.2">Each worker, running on each block of the DataNodes works on the partitions, one utterance at a time. </span><span class="koboSpan" id="kobo.10.3">For each utterance of the speech, the model parameter </span><span class="emphasis"><em><span class="koboSpan" id="kobo.11.1">P</span></em></span><span class="koboSpan" id="kobo.12.1"> is fetched from the parameter server mentioned earlier. </span><span class="koboSpan" id="kobo.12.2">The workers compute the current state of every frame; decipher the speech utterance to calculate the final outer gradients. </span><span class="koboSpan" id="kobo.12.3">The updated parameter is then sent back to the parameter server. </span><span class="koboSpan" id="kobo.12.4">The workers then repeatedly request the parameter server to provide the latest parameters. </span><span class="koboSpan" id="kobo.12.5">Backpropagation through time is then performed to calculate the updated parameter gradient for the next set of frames, which is again sent back to the parameter server.</span></p></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="RNNs with Deeplearning4j"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/><span class="koboSpan" id="kobo.1.1">RNNs with Deeplearning4j</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Training a RNN is not a simple task, and it can be extremely computationally demanding sometimes. </span><span class="koboSpan" id="kobo.2.2">With long sequences of training data involving many time steps, the training, sometimes becomes extremely difficult. </span><span class="koboSpan" id="kobo.2.3">As of now, you have got a better theoretical understanding of how and why backpropagation through time is primarily used for training a RNN. </span><span class="koboSpan" id="kobo.2.4">In this section, we will consider a practical example of the use of a RNN and its implementation using Deeplearning4j.</span></p><p><span class="koboSpan" id="kobo.3.1">We now take an example to give an idea of how to do the sentiment analysis of a movie review dataset using RNN. </span><span class="koboSpan" id="kobo.3.2">The main problem statement of this network is to take some raw text of a movie review as input, and classify that movie review as either positive or negative based on the contents present. </span><span class="koboSpan" id="kobo.3.3">Each word of the raw review text is converted to vectors using the Word2Vec model, and then fed into a RNN. </span><span class="koboSpan" id="kobo.3.4">The example uses a large-scale dataset of raw movie reviews taken from </span><a class="ulink" href="http://ai.stanford.edu/~amaas/data/sentiment/"><span class="koboSpan" id="kobo.4.1">
http://ai.stanford.edu/~amaas/data/sentiment/
</span></a><span class="koboSpan" id="kobo.5.1">.</span></p><p><span class="koboSpan" id="kobo.6.1">The whole implementation of this model using DL4J can be split into the following few steps:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.7.1">Download and extract the raw movie reviews data.</span></li><li class="listitem"><span class="koboSpan" id="kobo.8.1">Configure the network configuration needed for training, and evaluate the performance.</span></li><li class="listitem"><span class="koboSpan" id="kobo.9.1">Load each review and convert the words to vectors using the Word2Vec model.</span></li><li class="listitem"><span class="koboSpan" id="kobo.10.1">Perform training for multiple predefined epochs. </span><span class="koboSpan" id="kobo.10.2">For each epoch, the performance is evaluated on the test set.</span></li><li class="listitem"><span class="koboSpan" id="kobo.11.1">To download and extract the movie reviews' data, we need to set up the download configuration first. </span><span class="koboSpan" id="kobo.11.2">The following code snippet sets all the things needed to do so:</span><pre class="programlisting"><span class="koboSpan" id="kobo.12.1">        public static final String DATA_URL = 
        "http://ai.stanford.edu/~amaas/data/sentiment/*"; 
</span></pre></li><li class="listitem"><span class="koboSpan" id="kobo.13.1">Location to save and extract the training and testing data in the local file path is set as follows:</span><pre class="programlisting"><span class="koboSpan" id="kobo.14.1">        public static final String DATA_PATH = FilenameUtils.concat
        (System.getProperty("java.io.tmpdir"),local_file_path); 
</span></pre></li><li class="listitem"><span class="koboSpan" id="kobo.15.1">Location of the local filesystem for the Google News vectors is given as follows:</span><pre class="programlisting"><span class="koboSpan" id="kobo.16.1">        public static final String WORD_VECTORS_PATH =    
        "/PATH_TO_YOUR_VECTORS/GoogleNews-vectors-negative300.bin"; 
</span></pre></li><li class="listitem"><span class="koboSpan" id="kobo.17.1">The following code helps to download the data from the web URL to the local file path:</span><pre class="programlisting"><span class="koboSpan" id="kobo.18.1">        if( !archiveFile.exists() )
        { 
         System.out.println("Starting data download (80MB)..."); 
         FileUtils.copyURLToFile(new URL(DATA_URL), archiveFile); 
         System.out.println("Data (.tar.gz file) downloaded to " +  
         archiveFile.getAbsolutePath()); 
        
         extractTarGz(archizePath, DATA_PATH); 
        }
        else 
        {       
         System.out.println("Data (.tar.gz file) already exists at " +  
         archiveFile.getAbsolutePath()); 
         if( !extractedFile.exists())
           { 
            extractTarGz(archizePath, DATA_PATH); 
           } 
         else 
           { 
            System.out.println("Data (extracted) already exists at " +   
            extractedFile.getAbsolutePath()); 
           } 
         } 
         } 
</span></pre></li><li class="listitem"><span class="koboSpan" id="kobo.19.1">Now, as we have downloaded the raw movie reviews' data, we can now move to set up our RNN to perform the training of this data. </span><span class="koboSpan" id="kobo.19.2">The downloaded data is split on a number of examples used in each mini batch to work on each worker of Hadoop for distributed training purposes. </span><span class="koboSpan" id="kobo.19.3">We need to declare a variable, </span><code class="literal"><span class="koboSpan" id="kobo.20.1">batchSize</span></code><span class="koboSpan" id="kobo.21.1">, for this purpose. </span><span class="koboSpan" id="kobo.21.2">Here, as a sample, we use each batch of 50 examples, which will be split across multiple blocks of Hadoop, where the workers will run in parallel:</span><pre class="programlisting"><span class="koboSpan" id="kobo.22.1">      int batchSize = 50;      
      int vectorSize = 300; 
      int nEpochs = 5;  
      int truncateReviewsToLength = 300; 
</span><span class="strong"><strong><span class="koboSpan" id="kobo.23.1">      MultiLayerConfiguration conf = new             
      NeuralNetConfiguration.Builder()</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.24.1">        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_
         DESCENT)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.25.1">        .iterations(1)
        .updater(Updater.RMSPROP) 
        .regularization(true).l2(1e-5) 
        .weightInit(WeightInit.XAVIER) 
        .gradientNormalization(GradientNormalization
        .ClipElementWiseAbsoluteValue).gradientNormalizationThreshold
        (1.0) 
        .learningRate(0.0018) 
        .list() 
        .layer(0, new GravesLSTM.Builder()
              .nIn(vectorSize)
              .nOut(200) 
              .activation("softsign")
              .build()) 
        .layer(1, new RnnOutputLayer.Builder()
              .activation("softmax") 
              .lossFunction(LossFunctions.LossFunction.MCXENT)
              .nIn(200)
              .nOut(2)
              .build()) 
        .pretrain(false)
        .backprop(true)
        .build();</span></strong></span><span class="koboSpan" id="kobo.26.1"> 
 
      MultiLayerNetwork net = new MultiLayerNetwork(conf); 
      </span><span class="strong"><strong><span class="koboSpan" id="kobo.27.1">net.init();</span></strong></span><span class="koboSpan" id="kobo.28.1"> 
      net.setListeners(new ScoreIterationListener(1)); 
</span></pre></li><li class="listitem"><span class="koboSpan" id="kobo.29.1">As we set the network configuration for a RNN, we can now move on to the training operation as follows:</span><pre class="programlisting"><span class="koboSpan" id="kobo.30.1">      DataSetIterator train = new AsyncDataSetIterator(new    
      SentimentExampleIterator(DATA_PATH,wordVectors,
      batchSize,truncateReviewsToLength,true),1);
      DataSetIterator test = new AsyncDataSetIterator(new          
      SentimentExampleIterator(DATA_PATH,wordVectors,100,
      truncateReviewsToLength,false),1); 
      for( int i=0; i&lt;nEpochs; i++ )
      { 
        </span><span class="strong"><strong><span class="koboSpan" id="kobo.31.1">net.fit(train); 
        train.reset();</span></strong></span><span class="koboSpan" id="kobo.32.1"> 
        System.out.println("Epoch " + i + " complete. </span><span class="koboSpan" id="kobo.32.2">Starting    
        evaluation:"); 
</span></pre><p><span class="koboSpan" id="kobo.33.1">The testing of the network is performed by creating an object of the </span><code class="literal"><span class="koboSpan" id="kobo.34.1">Evaluation</span></code><span class="koboSpan" id="kobo.35.1"> class as follows:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.36.1">        Evaluation evaluation = new Evaluation(); 
        while(test.hasNext())
        { 
          DataSet t = test.next(); 
          INDArray features = t.getFeatureMatrix(); 
          INDArray lables = t.getLabels(); 
          INDArray inMask = t.getFeaturesMaskArray(); 
          INDArray outMask = t.getLabelsMaskArray(); 
          INDArray predicted =  
          net.output(features,false,inMask,outMask); 
          evaluation.evalTimeSeries(lables,predicted,outMask); 
        } 
      test.reset(); 
 
      System.out.println(evaluation.stats()); 
      } 
</span></pre></li></ol></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/><span class="koboSpan" id="kobo.1.1">Summary</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">RNNs are special compared to other traditional deep neural networks because of their capability to work over long sequences of vectors, and to output different sequences of vectors. </span><span class="koboSpan" id="kobo.2.2">RNNs are unfolded over time to work like a feed-forward neural network. </span><span class="koboSpan" id="kobo.2.3">The training of RNNs is performed with backpropagation of time, which is an extension of the traditional backpropagation algorithm. </span><span class="koboSpan" id="kobo.2.4">A special unit of RNNs, called Long short-term memory, helps to overcome the limitations of the backpropagation of time algorithm.</span></p><p><span class="koboSpan" id="kobo.3.1">We also talked about the bidirectional RNN, which is an updated version of the unidirectional RNN. </span><span class="koboSpan" id="kobo.3.2">Unidirectional RNNs sometimes fail to predict correctly because of lack of future input information. </span><span class="koboSpan" id="kobo.3.3">Later, we discussed distribution of deep RNNs and their implementation with Deeplearning4j. </span><span class="koboSpan" id="kobo.3.4">Asynchronous stochastic gradient descent can be used for the training of the distributed RNN. </span><span class="koboSpan" id="kobo.3.5">In the next chapter, we will discuss another model of deep neural network, called the Restricted Boltzmann machine.</span></p></div></div></div></body></html>