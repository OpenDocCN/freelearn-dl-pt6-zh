["```py\n\ninputs = [1,2]\nweights = [1,1,1]\n\ndef perceptron_predict(inputs, weights):\n    activation = weights[0]\n    for i in range(len(inputs)-1):\n        activation += weights[i] * input\n    return 1.0 if activation >= 0.0 else 0.0\n\nprint(perceptron_predict(inputs,weights))\n```", "```py\ntrain = [[1,2],[2,3],[1,1],[2,2],[3,3],[4,2],[2,5],[5,5],[4,1],[4,4]]\nweights = [1,1,1]\n\ndef perceptron_predict(inputs, weights):\n    activation = weights[0]    \n    for i in range(len(inputs)-1):\n      activation += weights[i+1] * inputs[i]\n      return 1.0 if activation >= 0.0 else 0.0\n\nfor inputs in train:\n  print(perceptron_predict(inputs,weights))\n```", "```py\ndef perceptron_predict(inputs, weights):\n activation = weights[0]\n for i in range(len(inputs)-1):\n  activation += weights[i + 1] * inputs[i]\n return 1.0 if activation >= 0.0 else 0.0\n\ndef train_weights(train, learning_rate, epochs):\n weights = [0.0 for i in range(len(train[0]))]\n for epoch in range(epochs):\n  sum_error = 0.0\n  for inputs in train:\n   prediction = perceptron_predict(inputs, weights)\n   error = inputs[-1] - prediction\n   sum_error += error**2\n   weights[0] = weights[0] + learning_rate * error\n   for i in range(len(inputs)-1):\n    weights[i + 1] = weights[i + 1] + learning_rate * error * inputs[i]\n  print('>epoch=%d, learning_rate=%.3f, error=%.3f' % (epoch, learning_rate, sum_error))\n return weights\n\ntrain = [[1.5,2.5,0],[2.5,3.5,0],[1.0,11.0,1],[2.3,2.3,1],[3.6,3.6,1],[4.2,2.4,0],[2.4,5.4,0],[5.1,5.1,1],[4.3,1.3,0],[4.8,4.8,1]]\n\nlearning_rate = 0.1\nepochs = 10\nweights = train_weights(train, learning_rate, epochs)\nprint(weights)\n```", "```py\nweights = [0.0 for i in range(len(train[0]))]\n```", "```py\ntrain = [[1.5,2.5,0.0],[2.5,3.5,0.0],[1.0,11.0,1.0],[2.3,2.3,1.0],[3.6,3.6,1.0],[4.2,2.4,0.0],[2.4,5.4,0.0],[5.1,5.1,1.0],[4.3,1.3,0.0],[4.8,4.8,1.0]]\n```", "```py\nreturn 1.0 if activation >= 0.0 else 0.0\n```", "```py\nreturn 1.0 if activation * (activation>0) >= 0.0 else 0.0\n```", "```py\npip install tensorflow \nOR \nconda install tensorflow    //using Anaconda\n```", "```py\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n```", "```py\nimport tensorflow as tf\n# Parameters\nlearning_rate = 0.001\ntraining_epochs = 15\nbatch_size = 100\ndisplay_step = 1\n# Network Parameters\nn_hidden_1 = 256 # 1st layer number of neurons\nn_hidden_2 = 256 # 2nd layer number of neurons\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)\n```", "```py\n# tf Graph input\nX = tf.placeholder(\"float\", [None, n_input])\nY = tf.placeholder(\"float\", [None, n_classes])\n\n# Store layers weight & bias\nweights = {\n 'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n 'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n 'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n}\nbiases = {\n 'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n 'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n 'out': tf.Variable(tf.random_normal([n_classes]))\n}\n```", "```py\n# Create model\ndef multilayer_perceptron(x):\n # Hidden fully connected layer with 256 neurons\n layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n # Hidden fully connected layer with 256 neurons\n layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n # Output fully connected layer with a neuron for each class\n out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n return out_layer\n\n# Construct model\nlogits = multilayer_perceptron(X)\n```", "```py\n# Define loss and optimizer\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss_op)\n```", "```py\n# Initializing the variables\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n sess.run(init)\n # Training cycle\n for epoch in range(training_epochs):\n   avg_cost = 0.\n   total_batch = int(mnist.train.num_examples/batch_size)\n   # Loop over all batches\n   for i in range(total_batch):\n     batch_x, batch_y = mnist.train.next_batch(batch_size)\n     # Run optimization op (backprop) and cost op (to get loss value)\n     _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,Y: batch_y})\n     # Compute average loss\n     avg_cost += c / total_batch\n```", "```py\n# Display logs per epoch step\n if epoch % display_step == 0:\n print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n print(\"Optimization Finished!\")\n```", "```py\n# Test model\n pred = tf.nn.softmax(logits) # Apply softmax to logits\n correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n```", "```py\n# Calculate accuracy\n accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))\n```", "```py\ntf.placeholder(\"float\", [None, n_input])\n...\ntf.Variable(tf.random_normal([n_input, n_hidden_1]))\n```", "```py\ndef multilayer_perceptron(x):\n  layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n  layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n  out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n  return out_layer\n\nlogits = multilayer_perceptron(X)\n```", "```py\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss_op)\ninit = tf.global_variables_initializer()\n```", "```py\nwith tf.Session() as sess:\n  sess.run(init)\n```", "```py\n_, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,Y: batch_y})\n```", "```py\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\n```", "```py\nencoding_dim = 32\n```", "```py\ninput_img = Input(shape=(784,))\n```", "```py\nencoded = Dense(encoding_dim, activation='ReLU')(input_img)\nencoder = Model(input_img, encoded)\n```", "```py\ndecoded = Dense(784, activation='sigmoid')(encoded)\nautoencoder = Model(input_img, decoded)\nencoded_input = Input(shape=(encoding_dim,))\n\ndecoder_layer = autoencoder.layers[-1]\ndecoder = Model(encoded_input, decoder_layer(encoded_input))\n\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n```", "```py\n    from tensorflow.keras.datasets import mnist\n    import numpy as np\n    (x_train, _), (x_test, _) = mnist.load_data()\n    ```", "```py\n    x_train = x_train.astype('float32') / 255.\n    x_test = x_test.astype('float32') / 255.\n    x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n    x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n    print( x_train.shape)\n    print( x_test.shape)\n    ```", "```py\n    autoencoder.fit(x_train, x_train, epochs=50, batch_size=256,\n     shuffle=True, validation_data=(x_test, x_test))\n\n    encoded_imgs = encoder.predict(x_test)\n    decoded_imgs = decoder.predict(encoded_imgs)\n    ```", "```py\n    import matplotlib.pyplot as plt\n    n = 10 # how many digits we will display\n    plt.figure(figsize=(20, 4))\n    for i in range(n):\n     # display original\n     ax = plt.subplot(2, n, i + 1)\n     plt.imshow(x_test[i].reshape(28, 28))\n     plt.gray()\n     ax.get_xaxis().set_visible(False)\n     ax.get_yaxis().set_visible(False)\n\n    # display reconstruction\n     ax = plt.subplot(2, n, i + 1 + n)\n     plt.imshow(decoded_imgs[i].reshape(28, 28))\n     plt.gray()\n     ax.get_xaxis().set_visible(False)\n     ax.get_yaxis().set_visible(False)\n    plt.show()\n    ```"]