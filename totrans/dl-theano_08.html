<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;8.&#xA0;Translating and Explaining with Encoding &#x2013; decoding Networks"><div class="book" id="2F4UM2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08" class="calibre1"/>Chapter 8. Translating and Explaining with Encoding – decoding Networks</h1></div></div></div><p class="calibre8">Encoding-decoding techniques occur when inputs and outputs belong to the same space. For example, image segmentation consists of transforming an input image into a new image, the segmentation mask; translation consists of transforming a character sequence into a new character sequence; and question-answering consists of replying to a sequence of words with a new sequence of words.</p><p class="calibre8">To address these challenges, encoding-decoding networks are networks composed of two symmetric parts: an encoding network and a decoding network. The encoder network encodes the input data into a vector, which will be used by the decoder network to produce an output, such as a <span class="strong"><em class="calibre12">translation</em></span>, an <span class="strong"><em class="calibre12">answer</em></span> to the input question, an <span class="strong"><em class="calibre12">explanation</em></span>, or an <span class="strong"><em class="calibre12">annotation</em></span> of an input sentence or an input image.</p><p class="calibre8">An encoder network is usually composed of the first layers of a network of the type of the ones presented in the previous chapters, without the last layers for dimensionality reduction and classification. Such a truncated network produces a multi-dimensional vector, named <span class="strong"><em class="calibre12">features</em></span>, that gives an <span class="strong"><em class="calibre12">internal state representation</em></span> to be used by the decoder to produce the output representation.</p><p class="calibre8">This chapter decomposes into the following key concepts:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Sequence-to-sequence networks</li><li class="listitem">Application to machine translation</li><li class="listitem">Application to chatbots</li><li class="listitem">Deconvolutions</li><li class="listitem">Application to image segmentation</li><li class="listitem">Application to image captioning</li><li class="listitem">Refinements in decoding techniques</li></ul></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Translating and Explaining with Encoding &#x2013; decoding Networks">
<div class="book" title="Sequence-to-sequence networks for natural language processing"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch08lvl1sec75" class="calibre1"/>Sequence-to-sequence networks for natural language processing</h1></div></div></div><p class="calibre8">Rule-based systems are being replaced by end-to-end neural networks because of their increase <a id="id323" class="calibre1"/>in performance.</p><p class="calibre8">An <a id="id324" class="calibre1"/>end-to-end neural network means the network directly infers all possible rules by example, without knowing the underlying rules, such as syntax and conjugation; the words (or the characters) are directly fed into the <a id="id325" class="calibre1"/>network as input. The same is true for the output format, which can be directly the word indexes themselves. The architecture of the network takes care of learning the rules with its coefficients.</p><p class="calibre8">The architecture of choice for such end-to-end encoding-decoding networks applied to <span class="strong"><strong class="calibre2">Natural Language Processing</strong></span> (<span class="strong"><strong class="calibre2">NLP</strong></span>), is the <span class="strong"><strong class="calibre2">sequence-to-sequence network</strong></span>, displayed in the following figure:</p><div class="mediaobject"><img src="../images/00116.jpeg" alt="Sequence-to-sequence networks for natural language processing" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Word indexes are converted into their continuous multi-dimensional values in the embedded space with a lookup table. This conversion, presented in <a class="calibre1" title="Chapter 3. Encoding Word into Vector" href="part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 3</a>, <span class="strong"><em class="calibre12">Encoding Word into Vector</em></span> is a crucial step to encode the discrete word indexes into a high dimensional space that a neural network can process.</p><p class="calibre8">Then, a first stack of LSTM is run on the input word embeddings, to encode the inputs and produce the thought vector. A second stack of LSTM is initiated with this vector as an initial internal state, and is expected to produce the next word for each word in the target sentence.</p><p class="calibre8">At <a id="id326" class="calibre1"/>the core, is our classical step function for the LSTM cell, with input, forget, output, and cell gates:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def </strong></span>LSTM( hidden_size):
  W <span class="strong"><strong class="calibre2">=</strong></span> shared_norm((hidden_size, 4<span class="strong"><strong class="calibre2">*</strong></span>hidden_size))
  U <span class="strong"><strong class="calibre2">=</strong></span> shared_norm((hidden_size, 4<span class="strong"><strong class="calibre2">*</strong></span>hidden_size))
  b <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros(4<span class="strong"><strong class="calibre2">*</strong></span>hidden_size)

  params <span class="strong"><strong class="calibre2">=</strong></span> [W, U, b]

  <span class="strong"><strong class="calibre2">def</strong></span> forward(m, X, h_, C_ ):
    XW = T.dot(X, W)
    h_U = T.dot(h_, U)
    bfr_actv = XW + h_U + b

    f = T.nnet.sigmoid( bfr_actv[:, 0:hidden_size] )
    i = T.nnet.sigmoid( bfr_actv[:, 1<span class="strong"><strong class="calibre2">*</strong></span>hidden_size:2*hidden_size] )
    o = T.nnet.sigmoid( bfr_actv[:, 2<span class="strong"><strong class="calibre2">*</strong></span>hidden_size:3*hidden_size] )
    Cp = T.tanh( bfr_actv[:, 3<span class="strong"><strong class="calibre2">*</strong></span>hidden_size:4*hidden_size] )

    C = i<span class="strong"><strong class="calibre2">*</strong></span>Cp + f<span class="strong"><strong class="calibre2">*</strong></span>C_
    h = o<span class="strong"><strong class="calibre2">*</strong></span>T.tanh( C )
    C = m[:, None]<span class="strong"><strong class="calibre2">*</strong></span>C + (1.0 - m)[:, None]<span class="strong"><strong class="calibre2">*</strong></span>C_
    h = m[:, None]<span class="strong"><strong class="calibre2">*</strong></span>h + (1.0 - m)[:, None]<span class="strong"><strong class="calibre2">*</strong></span>h_

    h, C = T.cast(h, theano.config.floatX), T.cast(h, theano.config.floatX)
    <span class="strong"><strong class="calibre2">return</strong></span> h, C

  <span class="strong"><strong class="calibre2">return</strong></span> forward<span class="strong"><strong class="calibre2">, </strong></span>params</pre></div><p class="calibre8">A simple closure is better than a class. There are not enough methods and parameters to go for a class. Writing classes impose to add lots of <code class="email">self</code>. Before all variables, an <code class="email">__init__</code> method.</p><p class="calibre8">To reduce computational cost, the full stack of layers is built into a one-step function and the recurrency is added to the top of the full stack step function that the output of the last layer produces for each timestep. Some other implementations have every layer independently recurrent, which is a lot less efficient (more than two times slower).</p><p class="calibre8">On top of the <span class="strong"><em class="calibre12">X</em></span> input, a mask variable, <code class="email">m</code>, stops the recurrency when set to zero: hidden and cell states are kept constant when there is no more data (mask value is zero). Since the inputs are processed in batches, sentences in each batch can have different lengths and, thanks to the mask, all sentences in a batch can be processed in parallel with the same number of steps, corresponding to the maximal sentence length. The recurrency stops at a different position for each row in the batch.</p><p class="calibre8">The <a id="id327" class="calibre1"/>reason for a closure of a class is because the model cannot be applied directly to some symbolic input variables as in previous examples: indeed, the model is applied to the sequences inside a recurrency loop (with the scan operator). For this reason, in many high level deep learning frameworks, each layer is designed as a module that exposes a forward/backward method, to be added in various architectures (parallel branches and recurrency), as in this example.</p><p class="calibre8">The full stack step function of the encoder/decoder to be placed inside their respective recurrency loop can be designed as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def stack</strong></span>( voca_size, hidden_size, num_layers, <span class="strong"><strong class="calibre2">embedding</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, <span class="strong"><strong class="calibre2">target_voca_size</strong></span>=0):
    params <span class="strong"><strong class="calibre2">=</strong></span> []

    <span class="strong"><strong class="calibre2">if</strong></span> embedding <span class="strong"><strong class="calibre2">==</strong></span> None:
        embedding = shared_norm( (voca_size, hidden_size) )
        params.append(embedding)

    layers = []
    <span class="strong"><strong class="calibre2">for</strong></span> i <span class="strong"><strong class="calibre2">in</strong></span> range(num_layers):
        f, p = LSTM(hidden_size)
        layers.append(f)
        params += p

    <span class="strong"><strong class="calibre2">def </strong></span>forward( mask, inputs, h_, C_, <span class="strong"><strong class="calibre2">until_symbol</strong></span> = <span class="strong"><strong class="calibre2">None</strong></span>):
        <span class="strong"><strong class="calibre2">if</strong></span> until_symbol <span class="strong"><strong class="calibre2">== </strong></span>None :
            output = embedding[inputs]
        <span class="strong"><strong class="calibre2">else</strong></span>:
            output = embedding[T.cast( inputs.argmax(axis=-1), "int32" )]

        hos = []
        Cos = []
      <span class="strong"><strong class="calibre2">for</strong></span> i <span class="strong"><strong class="calibre2">in</strong></span> range(num_layers):
            hs, Cs = layers[i](mask, output, h_[i], C_[i])
            hos.append(hs)
            Cos.append(Cs)
            output = hs

        <span class="strong"><strong class="calibre2">if</strong></span> target_voca_size <span class="strong"><strong class="calibre2">!=</strong></span> 0:
            output_embedding = shared_norm((hidden_size, target_voca_size))
            params.append(output_embedding)
            output = T.dot(output, output_embedding)

        outputs = (T.cast(output, theano.config.floatX),T.cast(hos, theano.config.floatX),T.cast(Cos, theano.config.floatX))

        <span class="strong"><strong class="calibre2">if</strong></span> until_symbol <span class="strong"><strong class="calibre2">!=</strong></span> None:
            <span class="strong"><strong class="calibre2">return </strong></span>outputs, theano.scan_module.until( T.eq(output.argmax(<span class="strong"><strong class="calibre2">axis</strong></span>=-1)[0], until_symbol) )

        <span class="strong"><strong class="calibre2">return</strong></span> outputs

    <span class="strong"><strong class="calibre2">return</strong></span> forward<span class="strong"><strong class="calibre2">, </strong></span>params</pre></div><p class="calibre8">The <a id="id328" class="calibre1"/>first part is the conversion of the input to the embedding space. The second part is the stack of LSTM layers. For the decoder (when <code class="email">target_voca_size != 0</code>), a linear layer is added to compute the output.</p><p class="calibre8">Now that we have our encoder/decoder step function, let's build the full encoder-decoder network.</p><p class="calibre8">First, the encoder-decoder network has to encode the input into the internal state representation:</p><div class="informalexample"><pre class="programlisting">encoderInputs, encoderMask = T.imatrices(2)
h0,C0 = T.tensor3s(2)

encoder, encoder_params = stack(valid_data.source_size, opt.hidden_size, opt.num_layers)

([encoder_outputs, hS, CS], encoder_updates) = theano.scan(
  <span class="strong"><strong class="calibre2">fn</strong></span> = encoder,
  <span class="strong"><strong class="calibre2">sequences</strong></span> = [encoderMask, encoderInputs],
  <span class="strong"><strong class="calibre2">outputs_info</strong></span> = [<span class="strong"><strong class="calibre2">None</strong></span>, h0, C0])</pre></div><p class="calibre8">To encode the input, the encoding stack step function is run recurrently on each word.</p><p class="calibre8">When <code class="email">outputs_info</code> is composed of three variables, the scan operator considers that the output of the scan operation is composed of three values.</p><p class="calibre8">These outputs come from the encoding stack step function and correspond to:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The output of the stack</li><li class="listitem">The hidden states of the stack, and</li><li class="listitem">The cell states for the stack, for each step/word of the input sentence</li></ul></div><p class="calibre8">In <code class="email">outputs_info</code>, <code class="email">None</code> indicates to consider that the encoder will produce three outputs, but only the last two will be fed back into the step function (<code class="email">h0 -&gt; h_</code> and <code class="email">C0 -&gt; C_</code>).</p><p class="calibre8">Given <a id="id329" class="calibre1"/>that sequences point to two sequences, the step function for the scan operation has to handle four arguments.</p><p class="calibre8">Then, once the input sentence has been encoded into a vector, the encoder-decoder network decodes it:</p><div class="informalexample"><pre class="programlisting">decoderInputs, decoderMask, decoderTarget = T.imatrices(<span class="strong"><strong class="calibre2">3</strong></span>)

decoder, decoder_params = stack(valid_data.target_size, opt.hidden_size, opt.num_layers, <span class="strong"><strong class="calibre2">target_voca_size</strong></span>=valid_data.target_size)

([decoder_outputs, h_vals, C_vals], decoder_updates) = theano.scan(
  <span class="strong"><strong class="calibre2">fn</strong></span> = decoder,
  <span class="strong"><strong class="calibre2">sequences</strong></span> = [decoderMask, decoderInputs],
  <span class="strong"><strong class="calibre2">outputs_info</strong></span> = [None, hS[<span class="strong"><strong class="calibre2">-1</strong></span>], CS[<span class="strong"><strong class="calibre2">-1</strong></span>]])

params = <span class="strong"><strong class="calibre2">encoder_params</strong></span> + <span class="strong"><strong class="calibre2">decoder_params</strong></span>
</pre></div><p class="calibre8">The last states <code class="email">hS[-1]</code>, <code class="email">CS[-1]]</code> of the encoder network are fed as initial hidden and cell states of the decoder network.</p><p class="calibre8">Computing the log likelihood on top of the output is the same as in the previous chapter on sequences.</p><p class="calibre8">For evaluation, the last predicted word has to be fed into the input of the decoder to predict the next word, which is a bit different from training, where input and output sequences are known:</p><div class="mediaobject"><img src="../images/00117.jpeg" alt="Sequence-to-sequence networks for natural language processing" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">In this case, <code class="email">None</code> in <code class="email">outputs_info</code> can be replaced with an initial value, <code class="email">prediction_start</code>, the <code class="email">start</code> token. Since it is not <code class="email">None</code> anymore, this initial value will <a id="id330" class="calibre1"/>be fed into the step function of the decoder, as long as it is with <code class="email">h0</code> and <code class="email">C0</code>. The scan operator considers that there are three previous values to feed into the decoder function (and not two as before) at each step. Since the <code class="email">decoderInputs</code> is removed from the input sequences, the number of arguments to the decoder stack step function remains four: the previous predicted output value is used in place of the fed input value. That way, the same decoder function can be used for both training and prediction:</p><div class="informalexample"><pre class="programlisting">prediction_mask = theano.shared(np.ones(( opt.max_sent_size, 1), <span class="strong"><strong class="calibre2">dtype=</strong></span>"int32"))

prediction_start = np.zeros(( 1, valid_data.target_size), dtype=theano.config.floatX)
prediction_start[0, valid_data.idx_start] = 1
prediction_start = theano.shared(prediction_start)

([decoder_outputs, h_vals, C_vals], decoder_updates) = theano.scan(
  <span class="strong"><strong class="calibre2">fn</strong></span> = decoder,
  <span class="strong"><strong class="calibre2">sequences</strong></span> = [prediction_mask],
  <span class="strong"><strong class="calibre2">outputs_info</strong></span> = [prediction_start, hS[-1], CS[-1]],
  <span class="strong"><strong class="calibre2">non_sequences</strong></span> = valid_data.idx_stop
  )</pre></div><p class="calibre8">The non-sequence parameter, <code class="email">valid_data.idx_stop</code>, indicates to the decoder step function <a id="id331" class="calibre1"/>that it is in prediction mode, meaning the input is not a word index, but its previous output (requires finding the max index).</p><p class="calibre8">Also in prediction mode, one sentence at a time is predicted (batch size is <code class="email">1</code>). The loop is stopped when the <code class="email">end</code> token is produced, thanks to the <code class="email">theano.scan_module.until</code> output in the decoder stack step function, and does not need to decode further words.</p></div></div>
<div class="book" title="Seq2seq for translation" id="2G3F81-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec76" class="calibre1"/>Seq2seq for translation</h1></div></div></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Sequence-to-sequence</strong></span> (<span class="strong"><strong class="calibre2">Seq2seq</strong></span>) networks <a id="id332" class="calibre1"/>have their first application in language translation.</p><p class="calibre8">A translation <a id="id333" class="calibre1"/>task has been <a id="id334" class="calibre1"/>designed for the conferences of the <span class="strong"><strong class="calibre2">Association for Computational Linguistics</strong></span> (<span class="strong"><strong class="calibre2">ACL</strong></span>), with a dataset, WMT16, composed of translations of news in different languages. The purpose of this dataset is to evaluate new translation systems or techniques. We'll use the German-English dataset.</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, preprocess the data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> 0-preprocess_translations.py --srcfile data/src-train.txt --targetfile data/targ-train.txt --srcvalfile data/src-val.txt --targetvalfile data/targ-val.txt --outputfile data/demo
First pass through data to get vocab...
Number of sentences in training: 10000
Number of sentences in valid: 2819
Source vocab size: Original = 24995, Pruned = 24999
Target vocab size: Original = 35816, Pruned = 35820
(2819, 2819)
Saved 2819 sentences (dropped 181 due to length/unk filter)
(10000, 10000)
Saved 10000 sentences (dropped 0 due to length/unk filter)
Max sent length (before dropping): 127</pre></div></li><li class="listitem" value="2">Train the <code class="email">Seq2seq</code> network:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> 1-train.py  --dataset translation</pre></div><p class="calibre24">At first glance, you notice the GPU time for one epoch is <span class="strong"><em class="calibre12">445.906425953</em></span>, hence ten times faster than on the CPU (<span class="strong"><em class="calibre12">4297.15962195</em></span>).</p></li><li class="listitem" value="3">Once <a id="id335" class="calibre1"/>trained, translate your sentences in English to German, loading the trained model :<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> 1-train.py  --dataset translation --model model_translation_e100_n2_h500</pre></div></li></ol><div class="calibre13"/></div></div>
<div class="book" title="Seq2seq for chatbots" id="2H1VQ1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec77" class="calibre1"/>Seq2seq for chatbots</h1></div></div></div><p class="calibre8">A second <a id="id336" class="calibre1"/>target application of sequence-to-sequence networks is question-answering, or chatbots.</p><p class="calibre8">For that purpose, download the Cornell Movie--Dialogs Corpus and preprocess it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">wget</strong></span> http://www.mpi-sws.org/~cristian/data/cornell_movie_dialogs_corpus.zip -P /sharedfiles/
<span class="strong"><strong class="calibre2">unzip</strong></span> /sharedfiles/cornell_movie_dialogs_corpus.zip  -d /sharedfiles/cornell_movie_dialogs_corpus

<span class="strong"><strong class="calibre2">python </strong></span>0-preprocess_movies.py</pre></div><p class="calibre8">This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts.</p><p class="calibre8">Since source and target sentences are in the same language, they use the same vocabulary, and the decoding network can use the same word embedding as the encoding network:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">if</strong></span> opt.dataset <span class="strong"><strong class="calibre2">==</strong></span> "chatbot":
    embeddings = encoder_params[0]</pre></div><p class="calibre8">The same commands are true for <code class="email">chatbot</code> dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> 1-train.py  --dataset chatbot # training
<span class="strong"><strong class="calibre2">python</strong></span> 1-train.py  --dataset chatbot --model model_chatbot_e100_n2_h500 # answer my question</pre></div></div>
<div class="book" title="Improving efficiency of sequence-to-sequence network" id="2I0GC1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec78" class="calibre1"/>Improving efficiency of sequence-to-sequence network</h1></div></div></div><p class="calibre8">A <a id="id337" class="calibre1"/>first interesting point to notice in the chatbot example is the reverse ordered input sequence: such a technique has been shown to improve results.</p><p class="calibre8">For translation, it is very common then to use a bidirectional LSTM to compute the internal state as seen in <a class="calibre1" title="Chapter 5. Analyzing Sentiment with a Bidirectional LSTM" href="part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 5</a>, <span class="strong"><em class="calibre12">Analyzing Sentiment with a Bidirectional LSTM</em></span>: two LSTMs, one running in the forward order, the other in the reverse order, run in parallel on the sequence, and their outputs are concatenated:</p><div class="mediaobject"><img src="../images/00118.jpeg" alt="Improving efficiency of sequence-to-sequence network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Such a mechanism captures better information given future and past.</p><p class="calibre8">Another technique is the <span class="strong"><em class="calibre12">attention mechanism</em></span> that will be the focus of the next chapter.</p><p class="calibre8">Lastly, <span class="strong"><em class="calibre12">refinement techniques </em></span>have been developed and tested with two-dimensional Grid LSTM, which are not very far from stacked LSTM (the only difference is a gating mechanism in the depth/stack direction):</p><div class="mediaobject"><img src="../images/00119.jpeg" alt="Improving efficiency of sequence-to-sequence network" class="calibre9"/><div class="caption"><p class="calibre29">Grid long short-term memory</p></div></div><p class="calibre10"> </p><p class="calibre8">The principle of refinement is to run the stack in both orders on the input sentence as well, sequentially. The idea behind this formulation is to have the encoder network revisit or re-encode the sentence, after having encoded it in the forward direction, and implicitly capture <a id="id338" class="calibre1"/>some time patterns. Also, note that the 2D-grid gives more possible interactions for this re-encoding, re-encoding the vector at each prediction step, using previously outputted words as an orientation for the next predicted word. All this improvement is linked to a bigger <a id="id339" class="calibre1"/>computational capacity, in <span class="strong"><strong class="calibre2">O(n m)</strong></span> for this re-encoder network (<span class="strong"><em class="calibre12">n</em></span> and <span class="strong"><em class="calibre12">m</em></span> represent the length of input and target sentences), while being of <span class="strong"><strong class="calibre2">O(n+m)</strong></span> for the encoder-decoder network.</p><p class="calibre8">All these techniques decrease perplexity. When the model is trained, consider also using the <span class="strong"><strong class="calibre2">beam search algorithm</strong></span> that will keep track of the top-N possible predictions with their probabilities, instead of one, at each time step, to avoid the possibility that one bad prediction ranking at first position could lead to further erroneous predictions.</p></div>
<div class="book" title="Deconvolutions for images"><div class="book" id="2IV0U2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec79" class="calibre1"/>Deconvolutions for images</h1></div></div></div><p class="calibre8">In the <a id="id340" class="calibre1"/>case of images, researchers have been looking for decoding operations acting as the inverse of the encoding convolutions.</p><p class="calibre8">The first application was the analysis and understanding of convolutional networks, as seen in <a class="calibre1" title="Chapter 2. Classifying Handwritten Digits with a Feedforward Network" href="part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 2</a>, <span class="strong"><em class="calibre12">Classifying Handwritten Digits with a Feedforward Network</em></span>, composed of convolutional layers, max-pooling layers and rectified linear units. To better understand the network, the idea is to visualize the parts of an image that are most discriminative for a given unit of a network: one single neuron in a high level feature map is left non-zero and, from that activation, the signal is retro-propagated back to the 2D input.</p><p class="calibre8">To reconstruct <a id="id341" class="calibre1"/>the signal through the max pooling layers, the idea is to keep track of the position of the maxima within each pooling region during the forward pass. Such architecture, named <span class="strong"><strong class="calibre2">DeConvNet</strong></span> can be shown as:</p><div class="mediaobject"><img src="../images/00120.jpeg" alt="Deconvolutions for images" class="calibre9"/><div class="caption"><p class="calibre29">Visualizing and understanding convolutional networks</p></div></div><p class="calibre10"> </p><p class="calibre8">The signal is retro-propagated to the position that had the maximal value during the forward pass.</p><p class="calibre8">To reconstruct the signal through the ReLU layers, three methods have been proposed:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre12">Back-propagation</em></span> retro-propagates only to the positions that have been positive</li><li class="listitem"><span class="strong"><em class="calibre12">Backward DeconvNet</em></span> retro-propagates only the positive gradients</li><li class="listitem"><span class="strong"><em class="calibre12">Guided back-propagation</em></span> retro-propagates only to a position that satisfies both previous conditions, positive input during forward pass and positive gradient</li></ul></div><p class="calibre8">The <a id="id342" class="calibre1"/>methods are illustrated in the following figure:</p><div class="mediaobject"><img src="../images/00121.jpeg" alt="Deconvolutions for images" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The retro-propagation from the first layers gives various sorts of filter:</p><div class="mediaobject"><img src="../images/00122.jpeg" alt="Deconvolutions for images" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">However, <a id="id343" class="calibre1"/>from higher layers in the network, the guided back-propagation gives much better results:</p><div class="mediaobject"><img src="../images/00123.jpeg" alt="Deconvolutions for images" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">It is also possible to condition the back-propagation on an input image, that will activate more than one neuron, from which the retro-propagation will be applied, to get a more precise input visualization:</p><div class="mediaobject"><img src="../images/00124.jpeg" alt="Deconvolutions for images" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The <a id="id344" class="calibre1"/>back-propagation can also be applied to the original input image rather than a blank one, a process that has been named <span class="strong"><strong class="calibre2">Inceptionism</strong></span> by Google <a id="id345" class="calibre1"/>research, when retro-propagation is used to augment the output probability:</p><div class="mediaobject"><img src="../images/00125.jpeg" alt="Deconvolutions for images" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">But the <a id="id346" class="calibre1"/>main purpose of deconvolution is for scene segmentation or image semantic analysis, where the deconvolution is replaced by a learned <a id="id347" class="calibre1"/>upsampling convolution, such as in the <span class="strong"><strong class="calibre2">SegNet network</strong></span>:</p><div class="mediaobject"><img src="../images/00126.jpeg" alt="Deconvolutions for images" class="calibre9"/><div class="caption"><p class="calibre29"> SegNet: A deep convolutional encoder-decoder architecture for image segmentation</p></div></div><p class="calibre10"> </p><p class="calibre8">At every step in the deconvolution process, lower input features are usually concatenated to the <a id="id348" class="calibre1"/>current features for upsampling.</p><p class="calibre8">The <span class="strong"><strong class="calibre2">DeepMask network</strong></span> takes a mixed approach, deconvolutioning only the patches containing <a id="id349" class="calibre1"/>the objects. For that purpose, it is trained on input patches of 224x224 containing the objects (+/- 16 pixels in translation) instead of the full image:</p><div class="mediaobject"><img src="../images/00127.jpeg" alt="Deconvolutions for images" class="calibre9"/><div class="caption"><p class="calibre29">Learning to segment object candidates</p></div></div><p class="calibre10"> </p><p class="calibre8">The convolutions of the encoder (VGG-16) network have a downsampling of factor 16, leading to a feature map of 14x14.</p><p class="calibre8">A joint learning trains two branches, one for segmentation, one for scoring if the object is present, centered, and at the right scale in the patch.</p><p class="calibre8">The branch of interest is the semantic branch that upsamples to a 56x56 segmentation map of the object in the patch from the 14x14 feature map. To upsample, is possible if:</p><div class="book"><ul class="itemizedlist"><li class="listitem">A fully connected layer, meaning that each position in the upsampled map depends on all features and has the global picture to predict the value</li><li class="listitem">A convolution (or locally connected layer), reducing the number of parameters, but also predicting each position score with a partial view</li><li class="listitem">A mixed approach, consisting of two linear layers with no non-linearity between them, in a way to perform a dimensionality reduction, as presented in the preceding figure</li></ul></div><p class="calibre8">The <a id="id350" class="calibre1"/>output mask is then upsampled back to the original patch dimensions 224x224 by a simple bilinear upsampling layer.</p><p class="calibre8">To deal with the full input image, fully connected layers can be transformed into convolutions with a kernel size equal to the fully connected layer input size and the same coefficients, so that the network becomes fully convolutional, with stride 16, when applied <a id="id351" class="calibre1"/>to the full image.</p><p class="calibre8">As sequence-to-sequence networks have been refined with a bidirectional reencoding mechanism, the <span class="strong"><strong class="calibre2">SharpMask</strong></span> approach improves the sharpness of the upsampling deconvolutional process using the input convolutional features at the equivalent scale:</p><div class="mediaobject"><img src="../images/00128.jpeg" alt="Deconvolutions for images" class="calibre9"/><div class="caption"><p class="calibre29">Learning to refine object segments</p></div></div><p class="calibre10"> </p><p class="calibre8">While the SegNet approach only learns to deconvolve from an up-sampled map produced by keeping track of the max pooling indices, the SharpMask approach directly reuses the input <a id="id352" class="calibre1"/>feature maps, a very usual technique for coarse-to-finegrained approaches.</p><p class="calibre8">Lastly, bear <a id="id353" class="calibre1"/>in mind that it is possible to improve the results one step further with the application of a <span class="strong"><strong class="calibre2">Conditional Random Fields</strong></span> (<span class="strong"><strong class="calibre2">CRF</strong></span>) post-processing step, either for one-dimensional inputs such as texts, or two-dimensional inputs such as segmentation images.</p></div>
<div class="book" title="Multimodal deep learning" id="2JTHG1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec80" class="calibre1"/>Multimodal deep learning</h1></div></div></div><p class="calibre8">To open <a id="id354" class="calibre1"/>the possible applications further, the encoding-decoding framework can be applied with different modalities, such as, for example, for image captioning.</p><p class="calibre8">Image captioning consists of describing the content of the image with words. The input is an image, naturally encoded into a thought vector with a deep convolutional network.</p><p class="calibre8">The text to describe the content of the image can be produced from this internal state vector with the same stack of LSTM networks as a decoder, as in Seq2seq networks:</p><p class="calibre8"> </p><div class="mediaobject"><img src="../images/00129.jpeg" alt="Multimodal deep learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">
</p></div>
<div class="book" title="Further reading" id="2KS221-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec81" class="calibre1"/>Further reading</h1></div></div></div><p class="calibre8">Please refer to the following topics for better insights:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre12">Sequence to Sequence Learning with Neural Networks</em></span>, Ilya Sutskever, Oriol Vinyals, Quoc V. Le, Dec 2014</li><li class="listitem"><span class="strong"><em class="calibre12">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</em></span>, Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, Sept 2014</li><li class="listitem"><span class="strong"><em class="calibre12">Neural Machine Translation by Jointly Learning to Align and Translate</em></span>, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, May 2016</li><li class="listitem"><span class="strong"><em class="calibre12">A Neural Conversational Model</em></span>, Oriol Vinyals, Quoc Le, July 2015</li><li class="listitem"><span class="strong"><em class="calibre12">Fast and Robust Neural Network Joint Models for Statistical Machine Translation</em></span>, Jacob Devlin, Rabih Zbib, Zhongqiang Huang,Thomas Lamar, Richard Schwartz, John Mkahoul, 2014</li><li class="listitem"><span class="strong"><em class="calibre12">SYSTRAN's Pure Neural Machine Translation Systems</em></span>, Josep Crego, Jungi Kim, Guillaume Klein, Anabel Rebollo, Kathy Yang, Jean Senellart, Egor Akhanov, Patrice Brunelle, Aurelien Coquard, Yongchao Deng, Satoshi Enoue, Chiyo Geiss, Joshua Johanson, Ardas Khalsa, Raoum Khiari, Byeongil Ko, Catherine Kobus, Jean Lorieux, Leidiana Martins, Dang-Chuan Nguyen, Alexandra Priori, Thomas Riccardi, Natalia Segal, Christophe Servan, Cyril Tiquet, Bo Wang, Jin Yang, Dakun Zhang, Jing Zhou, Peter Zoldan, 2016</li><li class="listitem"><span class="strong"><em class="calibre12">Blue: a method for automatic evaluatoin of machine translation,</em></span> Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu, 2002</li><li class="listitem">ACL 2016 translation task</li><li class="listitem"><span class="strong"><em class="calibre12">Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs</em></span>, Cristian Danescu-NiculescuMizil and Lillian Lee2011 at: <a class="calibre1" href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a></li><li class="listitem"><span class="strong"><em class="calibre12">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</em></span>, Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L., Yuille 2014</li><li class="listitem"><span class="strong"><em class="calibre12">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</em></span>, Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla, Oct 2016</li><li class="listitem"><span class="strong"><em class="calibre12">R-FCN: Object Detection via Region-based Fully Convolutional Networks</em></span>, Jifeng Dai, Yi Li, Kaiming He, Jian Sun2016</li><li class="listitem"><span class="strong"><em class="calibre12">Learning to segment object candidates</em></span>, Pedro O. Pinheiro, Ronan Collobert, Piotr Dollar, June 2015</li><li class="listitem"><span class="strong"><em class="calibre12">Learning to refine object segments</em></span>, Pedro O. Pinheiro, Tsung-Yi Lin, Ronan Collobert, Piotr Dollàr, Mar 2016</li><li class="listitem"><span class="strong"><em class="calibre12">Visualizing and Understanding Convolutional Networks</em></span>, Matthew D Zeiler, Rob Fergus, Nov 2013</li><li class="listitem"><span class="strong"><em class="calibre12">Show and tell: A Neural Image Caption Generator</em></span>, Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, 2014</li></ul></div></div>
<div class="book" title="Summary" id="2LQIK1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec82" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">As for love, head-to-toe positions provide exciting new possibilities: encoder and decoder networks use the same stack of layers but in their opposite directions.</p><p class="calibre8">Although it does not provide new modules to deep learning, such a technique of <span class="strong"><em class="calibre12">encoding-decoding</em></span> is quite important because it enables the training of the networks 'end-to-end', that is, directly feeding the inputs and corresponding outputs, without specifying any rules or patterns to the networks and without decomposing encoding training and decoding training into two separate steps.</p><p class="calibre8">While image classification was a one-to-one task, and sentiment analysis a many-to-one task, encoding-decoding techniques illustrate many-to-many tasks, such as translation or image segmentation.</p><p class="calibre8">In the next chapter, we'll introduce an <span class="strong"><em class="calibre12">attention mechanism</em></span> that provides the ability for encoder-decoder architecture to focus on some parts of the input in order to produce a more accurate output.</p></div></body></html>