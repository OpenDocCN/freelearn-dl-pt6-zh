["```py\n    # Check if GPU is detected\n    import tensorflow as tf\n    tf.test.gpu_device_name()\n    ```", "```py\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n    ```", "```py\n    cd \"/content/gdrive/My Drive/Lesson-9/\"\n    ```", "```py\n    !unzip data.csv.zip\n    ```", "```py\n    import os\n    import re\n    import pandas as pd\n    from keras.preprocessing.text import Tokenizer\n    from keras.preprocessing.sequence import pad_sequences\n    from keras.models import Sequential\n    from keras.layers import Dense, Embedding, LSTM\n    ```", "```py\n    def preprocess_data(data_file_path):\n        data = pd.read_csv(data_file_path, header=None) # read the csv\n        data.columns = ['rating', 'title', 'review'] # add column names\n        data['review'] = data['review'].apply(lambda x: x.lower()) # change all text to lower\n        data['review'] = data['review'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) # remove all numbers\n        return data\n    ```", "```py\n    df = preprocess_data('data.csv')\n    ```", "```py\n    # initialize tokenization\n    max_features = 2000\n    maxlength = 250\n    tokenizer = Tokenizer(num_words=max_features, split=' ')\n    # fit tokenizer\n    tokenizer.fit_on_texts(df['review'].values)\n    X = tokenizer.texts_to_sequences(df['review'].values)\n    # pad sequences\n    X = pad_sequences(X, maxlen=maxlength)\n    ```", "```py\n    # get target variable\n    y_train = pd.get_dummies(df.rating).values\n    ```", "```py\n    embed_dim = 128\n    hidden_units = 100\n    n_classes = 5\n    model = Sequential()\n    model.add(Embedding(max_features, embed_dim, input_length = X.shape[1]))\n    model.add(LSTM(hidden_units))\n    model.add(Dense(n_classes, activation='softmax'))\n    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n    print(model.summary())\n    ```", "```py\n    # fit the model\n    model.fit(X[:100000, :], y_train[:100000, :], batch_size = 128, epochs=15, validation_split=0.2)\n    ```", "```py\n    # save model and tokenizer\n    model.save('trained_model.h5')  # creates a HDF5 file 'trained_model.h5'\n    with open('trained_tokenizer.pkl', 'wb') as f: # creates a pickle file 'trained_tokenizer.pkl'\n        pickle.dump(tokenizer, f)\n    ```", "```py\n    from google.colab import files\n    files.download('trained_model.h5')\n    files.download('trained_tokenizer.pkl')\n    ```", "```py\n    import re\n    import pickle\n    import numpy as np\n    from flask import Flask, request, jsonify\n    from keras.models import load_model\n    from keras.preprocessing.sequence import pad_sequences\n    ```", "```py\n    def load_variables():\n        global model, tokenizer\n        model = load_model('trained_model.h5')\n        model._make_predict_function()  #https://github.com/keras-team/keras/issues/6462\n        with open('trained_tokenizer.pkl',  'rb') as f:\n            tokenizer = pickle.load(f)\n    ```", "```py\n    def do_preprocessing(reviews):\n        processed_reviews = []\n        for review in reviews:\n            review = review.lower()\n            processed_reviews.append(re.sub('[^a-zA-z0-9\\s]', '', review))\n        processed_reviews = tokenizer.texts_to_sequences(np.array(processed_reviews))\n        processed_reviews = pad_sequences(processed_reviews, maxlen=250)\n        return processed_reviews\n    ```", "```py\n    app = Flask(__name__)\n    ```", "```py\n    @app.route('/')\n    def home_routine():\n        return 'Hello World!'\n    ```", "```py\n    @app.route('/prediction', methods=['POST'])\n    def get_prediction():\n      # get incoming text\n      # run the model\n        if request.method == 'POST':\n            data = request.get_json()\n        data = do_preprocessing(data)\n        predicted_sentiment_prob = model.predict(data)\n        predicted_sentiment = np.argmax(predicted_sentiment_prob, axis=-1)\n        return str(predicted_sentiment)\n    ```", "```py\n    if __name__ == '__main__':\n      # load model\n      load_variables()\n      app.run(debug=True)\n    ```", "```py\n    python app.py\n    ```", "```py\n    curl -X POST \\\n    127.0.0.1:5000/prediction \\\n    -H 'Content-Type: application/json' \\\n    -d '[\"The book was very poor\", \"Very nice!\", \"The author could have done more\", \"Amazing product!\"]'\n    ```", "```py\n    [0 4 2 4]\n    ```", "```py\napp.run(host=0.0.0.0, port=80)\n```", "```py\ncurl -X POST \\\n0.0.0.0:80/prediction \\\n-H 'Content-Type: application/json' \\\n-d '[\"The book was very poor\", \"Very nice!\", \"The author could have done more\", \"Amazing product!\"]' \n```", "```py\n    Flask==1.0.2\n    numpy==1.14.1\n    keras==2.2.4\n    tensorflow==1.10.0\n    ```", "```py\n    FROM python:3.6-slim\n    COPY ./app.py /deploy/\n    COPY ./requirements.txt /deploy/\n    COPY ./trained_model.h5 /deploy/\n    COPY ./trained_tokenizer.pkl /deploy/\n    WORKDIR /deploy/\n    RUN pip install -r requirements.txt\n    EXPOSE 80\n    ENTRYPOINT [\"python\", \"app.py\"]\n    ```", "```py\n    docker build -f Dockerfile -t app-packt .\n    ```", "```py\n    docker run -p 80:80 app-packt\n    ```", "```py\n    chmod 400 key-file-name.pem\n    ```", "```py\n    ssh -i /path/my-key-pair.pem ec2-user@public-dns-name\n    ```", "```py\n    sudo amazon-linux-extras install docker\n    sudo yum install docker\n    sudo service docker start\n    sudo usermod -a -G docker ec2-user\n    ```", "```py\n    scp -i /path/my-key-pair.pem file-to-copy ec2-user@public-dns-name:/home/ec2-user\n    ```", "```py\n    curl -X POST \\\n    public-dns-name:80/predict \\\n    -H 'Content-Type: application/json' \\\n    -d '[\"The book was very poor\", \"Very nice!\", \"The author could have done more\", \"Amazing product!\"]'\n    ```"]