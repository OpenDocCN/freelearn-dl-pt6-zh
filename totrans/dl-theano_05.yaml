- en: Chapter 5. Analyzing Sentiment with a Bidirectional LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is a bit more practical to get a better sense of the commonly used
    recurrent neural networks and word embeddings presented in the two previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: It is also an opportunity to introduce the reader to a new application of deep
    learning, sentiment analysis, which is another field of **Natural Language Processing**
    (**NLP**). It is a many-to-one scheme, where a variable-length sequence of words
    has to be assigned to one class. An NLP problem where such a scheme can be used
    similarly is language detection (english, french, german, italian, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: While the previous chapter demonstrated how to build a recurrent neural network
    from scratch, this chapter shows how a high-level library built on top of Theano,
    Keras, can help implement and train the model with prebuilt modules. Thanks to
    this example, the reader should be able to decide when to use Keras in their projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following points are developed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A recap of recurrent neural networks and word embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectional recurrent networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated sentiment analysis is the problem of identifying opinions expressed
    in text. It normally involves the classification of text into categories such
    as *positive*, *negative*, and *neutral*. Opinions are central to almost all human
    activities and they are key influencers of our behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, neural networks and deep learning approaches have been used to build
    sentiment analysis systems. Such systems have the ability to automatically learn
    a set of features to overcome the drawbacks of handcrafted approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNN**) have been proved in the literature
    to be a very useful technique to represent sequential inputs, such as text. A
    special extension of recurrent neural networks called **Bi-directional Recurrent
    Neural Networks** (**BRNN**) can capture both the preceding and the following
    contextual information in a text.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll present an example to show how a bidirectional recurrent
    neural network using the **Long Short Term Memory** (**LSTM**) architecture can
    be used to deal with the problem of the sentiment analysis. We aim to implement
    a model in which, given an input of text (that is, a sequence of words), the model
    attempts to predict whether it is positive, negative, or neutral.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keras is a high-level neural network API, written in Python and capable of
    running on top of either TensorFlow or Theano. It was developed to make implementing
    deep learning models as fast and easy as possible for research and development.
    You can install Keras easily using conda, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When writing your Python code, importing Keras will tell you which backend
    is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you have installed Tensorflow, it might not use Theano. To specify which
    backend to use, write a Keras configuration file, `~/.keras/keras.json:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to specify the Theano backend directly with the environment
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the device used is the device we specified for Theano in the `~/.theanorc`
    file. It is also possible to modify these variables with Theano environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Programming with Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras provides a set of methods for data preprocessing and for building models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Layers and models are callable functions on tensors and return tensors. In
    Keras, there is no difference between a layer/module and a model: a model can
    be part of a bigger model and composed of multiple layers. Such a sub-model behaves
    as a module, with inputs/outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a network with two linear layers, a ReLU non-linearity in between,
    and a softmax output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `model` module contains methods to get input and output shape for either
    one or multiple inputs/outputs, and list the submodules of our module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In order to avoid specify inputs to every layer, Keras proposes a functional
    way of writing models with the `Sequential` module, to build a new module or model
    composed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following definition of the model builds exactly the same model as shown
    previously, with `input_dim` to specify the input dimension of the block that
    would be unknown otherwise and generate an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `model` is considered a module or layer that can be part of a bigger model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Each module/model/layer can be compiled then and trained with data :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Let us see Keras in practice.
  prefs: []
  type: TYPE_NORMAL
- en: SemEval 2013 dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us start by preparing the data. In this chapter, we will use the standard
    dataset used in the supervised task of Twitter sentiment classification (message-level)
    presented in the SemEval 2013 competition. It contains 3662 tweets as a training
    set, 575 tweets as a development set, and 1572 tweets as a testing set. Each sample
    in this dataset consists of the tweet ID, the polarity (positive, negative, or
    neutral) and the tweet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s download the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**A** refers to subtask A, which is message-level sentiment classification
    *our aim of study in this chapter*, where **B** refers to subtask B term level
    sentiment analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: The `input` directories do not contain the labels, just the tweets. `full` contains
    one more level of classification, *subjective* or *objective*. Our interest is
    in the `gold` or `cleansed` directories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the script to convert them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Preprocessing text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know, it is common to use URLs, user mentions, and hashtags frequently
    on Twitter. Thus, first we need to preprocess the tweets as follow.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that all the tokens are separated using the space. Each tweet is lowercased.
  prefs: []
  type: TYPE_NORMAL
- en: 'The URLs, user mentions, and hashtags are replaced by the `<url>`, `<user>`,
    and `<hashtag>` tokens respectively. This step is done using the `process` function,
    it takes a tweet as input, tokenizes it using the NLTK `TweetTokenizer`, preprocesses
    it, and returns the set of words (token) in the tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, if we have the tweet `RT @mhj: just an example! :D http://example.com
    #NLP`, the function process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: returns
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function is used to read the datasets and return a list of tuples,
    where each tuple represents one sample of (tweet, class), with the class an integer
    in {0, 1, or 2} defining the polarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build the vocabulary, which is a dictionary to map each word to
    a fixed index. The following function receives as input a set of data and returns
    the vocabulary and maximum length of the tweets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need a function to transfer each tweet or set of tweets into the indices
    based on the vocabulary if the words exist, or replacing **out-of-vocabulary**
    (**OOV**) words with the unknown token (index 0) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can save some memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Keras provides a helper method to pad the sequences to ensure they all have
    the same length, so that a batch of sequences can be represented by a tensor,
    and use optimized operations on tensors, either on a CPU or on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the method pads at the beginning, which helps get us better classification
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preprocessing text data](img/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, Keras provides a method to convert the classes into their one-hot encoding
    representation, by adding a dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preprocessing text data](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'With Keras `to_categorical` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Designing the architecture for the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main blocks of the model in this example will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the words of the input sentence are mapped to vectors of real numbers.
    This step is called vector representation of words or word embedding (for more
    details, see [Chapter 3](part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 3. Encoding Word into Vector"), *Encoding Word into Vector*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Afterwards, this sequence of vectors is represented by one fixed-length and
    real-valued vector using a bi-LSTM encoder. This vector summarizes the input sentence
    and contains semantic, syntactic, and/or sentimental information based on the
    word vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, this vector is passed through a softmax classifier to classify the
    sentence into positive, negative, or neutral.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector representations of words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Word embeddings are an approach to distributional semantics that represents
    words as vectors of real numbers. Such a representation has useful clustering
    properties, since the words that are semantically and syntactically related are
    represented by similar vectors (see [Chapter 3](part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 3. Encoding Word into Vector"), *Encoding Word into Vector*).
  prefs: []
  type: TYPE_NORMAL
- en: The main aim of this step is to map each word into a continuous, low-dimensional,
    and real-valued vector, which can later be used as an input to any model. All
    the word vectors are stacked into a matrix ![Vector representations of words](img/00087.jpeg);
    here, *N* is the vocabulary size and d the vector dimension. This matrix is called
    the embedding layer or the lookup table layer. The embedding matrix can be initialized
    using a pre-trained model such as **Word2vec** or **Glove**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, we can simply define the embedding layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter represents the vocabulary size, `output_dim` is the vector
    dimension, and `input_length` is the length of the input sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us add this layer as the input layer to the model and declare the model
    as a sequential model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Sentence representation using bi-LSTM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A recurrent neural network has the ability to represent sequences such as sentences.
    However, in practice, learning long-term dependencies with a vanilla RNN is difficult
    due to vanishing/exploding gradients. As presented in the previous chapter, **Long
    Short-Term Memory** (**LSTM**) networks were designed to have more persistent
    memory (that is, state), specialized in keeping and transmitting long-term information,
    making them very useful for capturing long-term dependencies between the elements
    of a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM units are the basic components of the model used in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras proposes a method, `TimeDistributed`, to clone any model in multiple
    time steps and make it recurrent. But for commonly used recurrent units such as
    LSTM, there already exists a module in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is identical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And for the subsequent layers, we do not need to specify the input size (this
    is the case since the LSTM layer comes after the embedding layer), thus we can
    define the `lstm` unit simply as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Last but not least, in this model, we''d like to use a bidirectional LSTM.
    It has proved to lead to better results, capturing the meaning of the current
    word given the previous words, as well as words appearing after:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sentence representation using bi-LSTM](img/00088.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To make this unit process the input bidirectionally, we can simply use Bidirectional,
    a bidirectional wrapper for RNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Outputting probabilities with the softmax classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, we can pass the vector obtained from `bi_lstm` to a softmax classifier
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let us print the summary of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Compiling and training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that the model is defined, it is ready to be compiled. To compile the model
    in Keras, we need to determine the optimizer, the loss function, and optionally
    the evaluation metrics. As we mentioned previously, the problem is to predict
    if the tweet is positive, negative, or neutral. This problem is known as a multi-category
    classification problem. Thus, the loss (or the objective) function that will be
    used in this example is the `categorical_crossentropy`. We will use the `rmsprop`
    optimizer and the accuracy evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, you can find state-of-the-art optimizers, objectives, and evaluation
    metrics implemented. Compiling the model in Keras is very easy using the compile
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We have defined the model and compiled it, and it is now ready to be trained.
    We can train or fit the model on the defined data by calling the fit function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training process runs for a certain number of iterations through the dataset,
    called epochs, which can be specified using the `epochs` parameter. We can also
    set the number of instances that are fed to the model at each step using the `batch_size`
    argument. In this case, we will use a small number of `epochs` = `30` and use
    a small batch size of `10`. We can also evaluate the model during training by
    explicitly feeding the development set using the `validation_data` parameter,
    or choosing a sub set from the training set using the `validation_split` parameter.
    In this case, we will use the development set that we defined previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have trained the model on the train test and now we can evaluate the performance
    of the network on the test set. This can be done using the `evaluation()` function.
    This function returns the loss value and the metrics values for the model in test
    mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Saving and loading the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To save the weights of the Keras model, simply call the `save` function, and
    the model is serialized into `.hdf5` format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'To load the model, use the `load_model` function provided by Keras as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'It is now ready for evaluation and does not need to be compiled. For example,
    on the same test set we must obtain the same results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Running the example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the model, we can execute the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '*SemEval Sentiment Analysis in Twitter* [https://www.cs.york.ac.uk/semeval-2013/task2.html](https://www.cs.york.ac.uk/semeval-2013/task2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Personality insights with IBM Watson demo* [https://personality-insights-livedemo.mybluemix.net/](https://personality-insights-livedemo.mybluemix.net/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tone analyzer* [https://tone-analyzer-demo.mybluemix.net/](https://tone-analyzer-demo.mybluemix.net/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Keras* [https://keras.io/](https://keras.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep Speech: Scaling up end-to-end speech recognition, Awni Hannun, Carl Case,
    Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev
    Satheesh, Shubho Sengupta, Adam Coates, Andrew Y. Ng, 2014'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech Recognition with Deep Recurrent Neural Networks, Alex Graves, Abdel-Rahman
    Mohamed, Geoffrey Hinton, 2013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin, Dario
    Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro,
    Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse
    Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick
    LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan
    Raiman, Sanjeev Satheesh,David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang,
    Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu, 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter acted as a review of the basic concepts introduced in the previous
    chapters, while introducing a new application, sentiment analysis, and a high-level
    library, Keras, to simplify the development of models with the Theano engine.
  prefs: []
  type: TYPE_NORMAL
- en: Among these basic concepts were recurrent networks, word embeddings, batch sequence
    padding, and class one-hot encoding. Bidirectional recurrency was presented to
    improve the results.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll see how to apply recurrency to images, with another
    library, Lasagne, which is more lightweight than Keras, and will let you mix the
    library modules with your own code for Theano more smoothly.
  prefs: []
  type: TYPE_NORMAL
