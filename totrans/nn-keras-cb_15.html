<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Audio Analysis</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapters, we learned about dealing with sequential text data. Audio can also be considered sequential data, with varying amplitudes over time. In this chapter, we will be learning about the following:</p>
<ul>
<li>Classifying a song by genre</li>
<li>Music generation using deep learning</li>
<li>Transcribing audio into text</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classifying a song by genre</h1>
                </header>
            
            <article>
                
<p>In this case study, we will be classifying a song into one of 10 possible genres. Imagine a scenario where we are tasked to automatically classify the genre of a song without manually listening to it. This way, we can potentially minimize operational overload as far as possible.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The strategy we'll adopt is as follows:</p>
<ol>
<li>Download a dataset of various audio recordings and the genre they fit into.</li>
<li>Visualize and contrast a spectrogram of the audio signal for various genres.</li>
<li>Perform CNN operations on top of a spectrogram:
<ul>
<li>Note that we will be performing a CNN 1D operation on a spectrogram, as the concept of translation does not apply in the case of audio recordings</li>
</ul>
</li>
</ol>
<ol start="4">
<li>Extract features from the CNN after multiple convolution and pooling operations.</li>
<li>Flatten the output and pass it through a dense layer that has 10 possible classes in an output layer.</li>
<li>Minimize categorical cross-entropy to classify the audio recording to one of the 10 possible classes.</li>
</ol>
<p>Once we classify the audio, we'll plot the embeddings of each audio input so that similar audio recordings are grouped together. This way, we will be in a position to identify the genre of a new song without listening to it, thus automatically classifying the audio input into a genre.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The strategy discussed above is coded as follows (the code file is available as <kbd>Genre_classification.ipynb</kbd> in GitHub):</p>
<ol>
<li>Download the dataset and import the relevant packages:</li>
</ol>
<pre style="padding-left: 60px">import sys, re, numpy as np, <span>pandas as pd, music21, IPython, pickle, librosa, librosa.dsiplay, os</span><br/>from glob import glob<br/>from tqdm import tqdm<br/>from keras.utils import np_utils</pre>
<ol start="2">
<li>Loop through the audio files to extract the  <kbd>mel spectrogram</kbd> input features of the input audio, and store the output genre for the audio input:</li>
</ol>
<pre style="padding-left: 60px">song_specs=[]<br/>genres = []<br/>for genre in os.listdir('...'): # Path to genres folder<br/>  song_folder = '...' # Path to songs folder<br/>  for song in os.listdir(song_folder):<br/>    if song.endswith('.au'):<br/>      signal, sr = librosa.load(os.path.join(song_folder, song), sr=16000)<br/>      melspec = librosa.feature.melspectrogram(signal, sr=sr).T[:1280,]<br/>      song_specs.append(melspec)<br/>      genres.append(genre)<br/>      print(song)<br/>  print('Done with:', genre)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">In the preceding code, we are loading the audio file and extracting its features. Additionally, we are extracting the <kbd>melspectrogram</kbd> features of the signal. Finally, we are storing the <kbd>mel</kbd> features as <span>the </span>input array and the genre as <span>the </span>output array.</p>
<ol start="3">
<li>Visualize the spectrogram:</li>
</ol>
<pre style="padding-left: 60px">plt.subplot(121)<br/>librosa.display.specshow(librosa.power_to_db(song_specs[302].T),<br/> y_axis='mel',<br/> x_axis='time',)<br/>plt.title('Classical audio spectrogram')<br/>plt.subplot(122)<br/>librosa.display.specshow(librosa.power_to_db(song_specs[402].T),<br/> y_axis='mel',<br/> x_axis='time',)<br/>plt.title('Rock audio spectrogram')</pre>
<p style="padding-left: 60px">The following is the output of the preceding code:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1397 image-border" src="Images/1d90d784-81f5-45d9-bbae-80908b53091b.png" style="width:60.42em;height:40.75em;" width="725" height="489"/></p>
<p style="padding-left: 60px">You can see that there is a distinct difference between the <span>classical audio </span><span>spectrogram</span><span> and the</span> <span>rock audio s</span><span>pectrogram</span><span>.</span></p>
<ol start="4">
<li>Define the input and output arrays:</li>
</ol>
<pre style="padding-left: 60px">song_specs = np.array(song_specs)<br/><br/>song_specs2 = []<br/>for i in range(len(song_specs)):<br/>     tmp = song_specs[i]<br/>     song_specs2.append(tmp[:900][:])<br/>song_specs2 = np.array(song_specs2)</pre>
<p style="padding-left: 60px">Convert the output classes into one-hot encoded versions:</p>
<pre style="padding-left: 60px">genre_one_hot = pd.get_dummies(genres)</pre>
<ol start="5">
<li>Create train-and-test datasets:</li>
</ol>
<pre style="padding-left: 60px">x_train, x_test, y_train, y_test = train_test_split(song_specs2, np.array(genre_one_hot),test_size=0.1,random_state = 42)</pre>
<ol start="6">
<li>Build and compile the method:</li>
</ol>
<pre style="padding-left: 60px">input_shape = (1280, 128)<br/>inputs = Input(input_shape)<br/>x = inputs<br/>levels = 64<br/>for level in range(7):<br/>     x = Conv1D(levels, 3, activation='relu')(x)<br/>     x = BatchNormalization()(x)<br/>     x = MaxPooling1D(pool_size=2, strides=2)(x)<br/>     levels *= 2<br/>     x = GlobalMaxPooling1D()(x)<br/>for fc in range(2):<br/>     x = Dense(256, activation='relu')(x)<br/>     x = Dropout(0.5)(x)<br/>labels = Dense(10, activation='softmax')(x)</pre>
<p style="padding-left: 60px">Note that the <kbd>Conv1D</kbd> method in the preceding code works in a manner very similar to that of <kbd>Conv2D</kbd>; however, it is a one-dimensional filter in <kbd><span>Conv1D</span></kbd> and a two-dimensional one in <kbd>Conv2D</kbd>:</p>
<pre style="padding-left: 60px">model = Model(inputs=[inputs], outputs=[labels])<br/>adam = keras.optimizers.Adam(lr=0.0001)<br/>model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])</pre>
<ol start="7">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 60px">history = model.fit(x_train, y_train,batch_size=128,epochs=100,verbose=1,validation_data=(x_test, y_test))</pre>
<p style="padding-left: 60px">From the preceding code, we can see that the model classifies with an accuracy of ~60% on the test dataset.</p>
<ol start="8">
<li>Extract the output from the pre-final layer of the model:</li>
</ol>
<pre style="padding-left: 60px">from keras.models import Model<br/>layer_name = 'dense_14'<br/>intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)<br/>intermediate_output = intermediate_layer_model.predict(song_specs2)</pre>
<p style="padding-left: 60px">The preceding code produces output at the pre-final layer.</p>
<ol start="9">
<li>Reduce the dimensions of the embeddings to 2, using <kbd>t-SNE</kbd> so that we can now plot our work on a chart:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.manifold import TSNE<br/>tsne_model = TSNE(n_components=2, verbose=1, random_state=0)<br/>tsne_img_label = tsne_model.fit_transform(intermediate_output)<br/>tsne_df = pd.DataFrame(tsne_img_label, columns=['x', 'y'])<br/>tsne_df['image_label'] = genres</pre>
<ol start="10">
<li>Plot the <kbd>t-SNE</kbd> output:</li>
</ol>
<pre style="padding-left: 60px">from ggplot import *<br/>chart = ggplot(tsne_df, aes(x='x', y='y', color='genres'))+ geom_point(size=70,alpha=0.5)<br/>chart</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The following is the chart for the preceding code:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1398 image-border" src="Images/f0e76348-4592-4fb2-b376-87ec1d393faf.png" style="width:57.50em;height:39.00em;" width="690" height="468"/></p>
<p>From the preceding diagram, we can see that audio recordings for similar genres are located together. This way, we are now in a position to classify a new song into one of the possible genres automatically, without manual inspection. However, if the probability of an audio belonging to a certain genre is not very high, it will potentially go to a manual review so that misclassifications are uncommon.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Generating music using deep learning</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we learned about generating text by going through a novel. In this section, we will learn about generating audio from a sequence of audio notes.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>A MIDI file typically contains information about the notes and chords of the audio file, whereas the note object contains information about the pitch, octave, and offset of the notes. The chord object contains a set of notes that are played at the same time.</p>
<p>The strategy that we'll adopt to build a music generator is as follows:</p>
<ul>
<li>Extract the notes present in audio file</li>
<li>Assign a unique ID for each note.</li>
<li>Take a sequence of 100 historical notes, and the 101<sup>st</sup> note shall be the output.</li>
<li>Fit an LSTM model. </li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The strategy discussed above is coded as follows (the code file is available as <kbd>Music_generation.ipynb</kbd> in GitHub) along with the recommended audio file:</p>
<ol>
<li>Import the relevant packages and dataset:</li>
</ol>
<pre style="padding-left: 60px">!pip install mido music21<br/>import mido, glob, os<br/>from mido import MidiFile, MidiTrack, Message<br/>import numpy as np<br/>from music21 import converter, instrument, note, chord<br/>from keras.utils import np_utils<br/>from keras.layers import Input, LSTM, Dropout, Dense, Activation<br/>from keras.models import Model<br/><br/>fname = '/content/nintendo.mid'</pre>
<ol start="2">
<li>Read the content of the file:</li>
</ol>
<pre style="padding-left: 60px">midi = converter.parse(fname)</pre>
<p style="padding-left: 60px">The preceding code reads a stream of scores.</p>
<ol start="3">
<li>Define a function that reads the stream of scores and extracts the notes from it (along with silence, if present in the audio file):</li>
</ol>
<pre style="padding-left: 60px">def parse_with_silence(midi=midi):<br/>     notes = []<br/>     notes_to_parse = None<br/>     parts = instrument.partitionByInstrument(midi)<br/>     if parts: # file has instrument parts<br/>         notes_to_parse = parts.parts[0].recurse()<br/>     else: # file has notes in a flat structure<br/>         notes_to_parse = midi.flat.notes<br/>     for ix, element in enumerate(notes_to_parse):<br/>         if isinstance(element, note.Note):<br/>             _note = str(element.pitch)<br/>             notes.append(_note)<br/>         elif isinstance(element, chord.Chord):<br/>             _note = '.'.join(str(n) for n in element.normalOrder)<br/>             notes.append(_note)<br/>         elif isinstance(element, note.Rest):<br/>             _note = '#'+str(element.seconds)<br/>             notes.append(_note)<br/>     return notes</pre>
<p style="padding-left: 60px">In the preceding code, we are obtaining the notes by looping through the elements and, depending on whether the element is a note, a chord, or a rest (which indicates silence), we extract the corresponding notes, append them, and return the appended list.</p>
<ol start="4">
<li>Extract the notes from the input audio file's stream:</li>
</ol>
<pre style="padding-left: 60px">notes = parse_with_silence()</pre>
<p style="padding-left: 60px">A sample note output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/b3f25f4f-d8de-4fc1-bcae-dee7545cfe57.png" width="584" height="24"/></p>
<p style="padding-left: 60px">Note that the values starting with a <kbd>#</kbd> indicate silence (the duration is the same as the number adjacent to <kbd>#</kbd>).</p>
<ol start="5">
<li>Create the input and output dataset by creating a dictionary of the note's ID and its corresponding name:</li>
</ol>
<pre style="padding-left: 60px"># get all unique values in notes<br/>pitchnames = sorted(set(item for item in notes))<br/># create a dictionary to map pitches to integers<br/>note_to_int = dict((note, number) for number, note in enumerate(pitchnames))<br/>network_input = []<br/>network_output = []</pre>
<ol start="2">
<li style="list-style-type: none">
<ol start="1">
<li>Create a sequence of input and output arrays:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 120px">sequence_length = 100<br/>for i in range(0, len(notes) - sequence_length, 1):<br/>     sequence_in = notes[i:i + sequence_length]<br/>     sequence_out = notes[i + sequence_length]<br/>     network_input.append([note_to_int[char] for char in sequence_in])<br/>     network_output.append(note_to_int[sequence_out])</pre>
<p style="padding-left: 60px">In the preceding step, we are taking a sequence of 100 notes as input and extracting the output at the 101st time step.</p>
<p style="padding-left: 60px">Additionally, we are also converting the note into its corresponding ID:</p>
<pre style="padding-left: 60px">n_patterns = len(network_input)<br/># reshape the input into a format compatible with LSTM layers<br/>network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))<br/># normalize input<br/>network_input = network_input / np.max(network_input)<br/>network_output = np_utils.to_categorical(network_output)<br/><br/>N = 9 * len(network_input)//10<br/>print(network_input.shape, network_output.shape)<br/># (36501, 100, 1) (36501, 50)</pre>
<p style="padding-left: 60px">In the preceding code, we are reshaping the input data so that it can then be fed into an LSTM layer (which requires the <kbd>batch_size</kbd> shape, time steps, and the number of features per time step).</p>
<p style="padding-left: 60px">Additionally, we are normalizing the input, and we are also converting the output into a one-hot encoded set of vectors.</p>
<ol start="6">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 60px">model.fit(network_input, network_output, epochs=100, batch_size=32, verbose = 1)</pre>
<p style="padding-left: 60px">The following is the output of the preceding code:</p>
<p style="padding-left: 60px" class="CDPAlignCenter CDPAlign"><img src="Images/3ee599ca-17c2-42a4-962e-f5fb91653bdd.png" style="width:33.58em;height:26.58em;" width="393" height="311"/></p>
<ol start="7">
<li>Generate predictions:</li>
</ol>
<pre style="padding-left: 60px">from tqdm import trange<br/>print('generating prediction stream...')<br/>start = np.random.randint(0, len(network_input)-1)<br/>int_to_note = dict((number, note) for number, note in enumerate(pitchnames))<br/>pattern = network_input[start].tolist()<br/>prediction_output = []</pre>
<p style="padding-left: 60px">Note that, in the preceding code, we have chosen a random audio location, from where we'll sample a sequence that will be used as a seed for prediction in future time steps.</p>
<ol start="8">
<li>Generate predictions by taking a sequence of 100 notes at a time, generating the next prediction, appending it to the input sequence, and generating the next prediction (by taking the latest sequence of the last 100 notes):</li>
</ol>
<pre style="padding-left: 60px">for note_index in trange(500):<br/>     prediction_input = np.reshape(pattern, (1, len(pattern), 1))<br/>     prediction = model.predict(prediction_input, verbose=0)<br/>     index = np.argmax(prediction)<br/>     result = int_to_note[index]<br/>     prediction_output.append(result)<br/>     pattern.append([index/49])<br/>     pattern = pattern[1:len(pattern)]</pre>
<p style="padding-left: 60px" class="mce-root">Note that we are dividing the index (which is the predicted output of <span>the </span>model) by 49, as we did in the same exercise while building the model (divided by <kbd>np.max(network_input)</kbd>).</p>
<div class="mce-root packt_tip">The preceding exercise is slightly different than the text generation exercise, where we performed embedding on top of input word IDs, as we are not performing embedding in this scenario. The model is still working without embedding in this scenario, potentially because there are fewer unique values in the input.</div>
<ol start="9">
<li>Create note values based on values generated by the model:</li>
</ol>
<pre style="padding-left: 60px">offset = 0<br/>output_notes = []<br/><br/># create note and chord objects based on the values generated by the model<br/>print('creating notes and chords')<br/>for pattern in prediction_output:<br/>    <br/>    # pattern is a chord<br/>    if (('.' in pattern) or pattern.isdigit()) and pattern[0]!='#':<br/>        notes_in_chord = pattern.split('.')<br/>        notes = []<br/>        for current_note in notes_in_chord:<br/>            new_note = note.Note(int(current_note))<br/>            new_note.storedInstrument = instrument.Piano()<br/>            notes.append(new_note)<br/>        new_chord = chord.Chord(notes)<br/>        new_chord.offset = offset<br/>        output_notes.append(new_chord)<br/>        <br/>    # pattern is a note<br/>    elif pattern[0]!='#':<br/>        new_note = note.Note(pattern)<br/>        new_note.offset = offset<br/>        new_note.storedInstrument = instrument.Piano()<br/>        output_notes.append(new_note)<br/>        <br/>    # pattern is silence<br/>    else:<br/>        new_note = note.Rest()<br/>        new_note.offset = offset<br/>        new_note.storedInstrument = instrument.Piano()<br/>        new_note.quarterLength = float(pattern[1:])<br/>        output_notes.append(new_note)<br/>    # increase offset each iteration so that notes do not stack<br/>    offset += 0.5</pre>
<p style="padding-left: 60px">Note that, in the preceding code, we are offsetting each note by 0.5 seconds, so that the notes do not stack on top of one another while producing the output.</p>
<ol start="10">
<li>Write the generated predictions into a music stream:</li>
</ol>
<pre style="padding-left: 60px">from music21 import stream<br/>midi_stream = stream.Stream(output_notes)<br/>midi_stream.write('midi', fp='OP.mid')</pre>
<p>Now, you should be able to listen to the music that's been generated by your model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Transcribing audio into text</h1>
                </header>
            
            <article>
                
<p>In <a href="1f989cf7-40b3-4ecd-9457-0dd648746922.xhtml" target="_blank">Chapter 14</a>, <em>End-to-End Learning</em>, we learned about transcribing handwritten text images into text. In this section, we will be leveraging a similar end-to-end model to transcribe voices into text.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The strategy that we'll adopt to transcribe voices is as follows:</p>
<ul>
<li>Download a dataset that contains the audio file and its corresponding transcriptions (<em>ground truths</em>)</li>
<li>Specify a sampling rate while reading the audio files:
<ul>
<li>If the sampling rate is 16,000, we'll be extracting 16,000 data points per second of audio.</li>
</ul>
</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<ul>
<li>Extract a Fast Fourier Transformation of the audio array:
<ul>
<li>An FFT ensures that we have only the most important features of a signal.</li>
<li>By default, the FFT gives us <em>n/2</em> number of data points, where <em>n</em> is the number of data points in the whole audio recording.</li>
</ul>
</li>
<li>Sample the FFT features of the audio where we extract 320 data points at a time; that is, we extract 20 milliseconds (320/16000 = 1/50 seconds) of audio data at a time</li>
<li>Additionally, we will sample 20 milliseconds of data at 10-millisecond intervals.</li>
<li>For this exercise, we'll be working on an audio recording where the audio duration is, at most, 10 seconds</li>
<li>We will store the 20 milliseconds of audio data into an array:
<ul>
<li>We have already seen that we sample 20 milliseconds of data for every 10 milliseconds.</li>
<li>Thus, for a one-second audio clip, we will have 100 x 320 data points, and for a 10- second audio clip, we'll have 1,000 x 320 = 320,000 data points.</li>
</ul>
</li>
<li>We will initialize an empty array of 160,000 data points and overwrite the values with the FFT values—as we have already learned that the FFT values are one half of the original number of data points</li>
<li>For each array of 1,000 x 320 data points, we'll store the corresponding transcriptions</li>
<li>We'll assign an index for each character and then convert the output into a list of indices</li>
<li>Additionally, we'll also be storing the input length (which is the predefined number of time steps) and the label lengths (which are the actual number of characters present in the output)</li>
<li>Furthermore, we will define the CTC loss function that is based on the actual output, the predicted output, the number of time steps (input length), and the label length (the number of characters in the output)</li>
<li>We'll define the model that is a combination of <kbd>conv1D</kbd> (as this is audio data) and GRU</li>
<li>Furthermore, we will ensure that we normalize data using batch normalization so that <span>the </span>gradients do not vanish</li>
<li>We'll run the model on batches of data, where we randomly sample batches of data and feed them to the model that tries to minimize the CTC loss</li>
<li>Finally, we will decode the model predictions on a new data point by using the <kbd>ctc_decode</kbd> method</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The strategy discussed above is coded as follows (the code file is available as <kbd>Voice transcription.ipynb</kbd> in GitHub):</p>
<ol>
<li>Download the dataset and import the relevant packages:</li>
</ol>
<pre style="padding-left: 60px"><strong>$wget http://www.openslr.org/resources/12/train-clean-100.tar.gz</strong><br/><strong>$tar xzvf train-clean-100.tar.gz</strong><br/><br/>import librosa<br/>import numpy as np<br/>import pandas as pd</pre>
<ol start="2">
<li>Read all the file names and their corresponding transcriptions and turn them into separate lists:</li>
</ol>
<pre style="padding-left: 60px">import os, numpy as np<br/>org_path = '/content/LibriSpeech/train-clean-100/'<br/>count = 0<br/>inp = []<br/>k=0<br/>audio_name = []<br/>audio_trans = []<br/>for dir1 in os.listdir(org_path):<br/>     dir2_path = org_path+dir1+'/'<br/>     for dir2 in os.listdir(dir2_path):<br/>     dir3_path = dir2_path+dir2+'/'<br/> <br/>     for audio in os.listdir(dir3_path):<br/>         if audio.endswith('.txt'):<br/>             k+=1<br/>             file_path = dir3_path + audio<br/>             with open(file_path) as f:<br/>                 line = f.readlines()<br/>                 for lines in line:<br/>                     audio_name.append(dir3_path+lines.split()[0]+'.flac')<br/>                     words2 = lines.split()[1:]<br/>                     words4=' '.join(words2)<br/>                     audio_trans.append(words4)</pre>
<p class="mce-root"/>
<ol start="3">
<li>Store the length of the transcription into a list so that we can understand the maximum transcription length:</li>
</ol>
<pre style="padding-left: 60px">import re<br/>len_audio_name=[]<br/>for i in range(len(audio_name)):<br/>     tmp = audio_trans[i]<br/>     len_audio_name.append(len(tmp))</pre>
<ol start="4">
<li>For this exercise, to be in a position to train a model on a single GPU, we'll perform this exercise on the first 2,000 audio files whose transcriptions are fewer than 100 characters in length:</li>
</ol>
<pre style="padding-left: 60px">final_audio_name = []<br/>final_audio_trans = []<br/>for i in range(len(audio_name)):<br/>     if(len_audio_name[i]&lt;100):<br/>         final_audio_name.append(audio_name[i])<br/>         final_audio_trans.append(audio_trans[i])</pre>
<p style="padding-left: 60px">In the preceding code, we are storing the audio name and the corresponding audio transcription for only those audio recordings that have a transcription length of fewer than 100 characters.</p>
<ol start="5">
<li>Store the inputs as a 2D array and the corresponding outputs of only those audio files that have a duration of fewer than 10 seconds:</li>
</ol>
<pre style="padding-left: 60px">inp = []<br/>inp2 = []<br/>op = []<br/>op2 = []<br/>for j in range(len(final_audio_name)):<br/>     t = librosa.core.load(final_audio_name[j],sr=16000, mono= True) <br/>     if(t[0].shape[0]&lt;160000):<br/>         t = np.fft.rfft(t[0])<br/>         t2 = np.zeros(160000)<br/>         t2[:len(t)] = t<br/>         inp = []<br/>         for i in range(t2.shape[0]//160):<br/>             inp.append(t2[(i*160):((i*160)+320)])<br/>             inp2.append(inp)<br/>             op2.append(final_audio_trans[j])</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="6">
<li>Create an index for each unique character in the data:</li>
</ol>
<pre style="padding-left: 60px">import itertools<br/>list2d = op2<br/>charList = list(set(list(itertools.chain(*list2d))))</pre>
<ol start="7">
<li>Create the input and the label lengths:</li>
</ol>
<pre style="padding-left: 60px">num_audio = len(op2)<br/>y2 = []<br/>input_lengths = np.ones((num_audio,1))*243<br/>label_lengths = np.zeros((num_audio,1))<br/>for i in range(num_audio):<br/>     val = list(map(lambda x: charList.index(x), op2[i]))<br/>     while len(val)&lt;243:<br/>         val.append(29)<br/>     y2.append(val)<br/>     label_lengths[i] = len(op2[i])<br/>     input_lengths[i] = 243</pre>
<p style="padding-left: 60px">Note that we are creating an input length that is 243, as the output of the model (which we are going to build in a later step) has 243 time steps.</p>
<ol start="8">
<li>Define the CTC loss function:</li>
</ol>
<pre style="padding-left: 60px">import keras.backend as K<br/>def ctc_loss(args):<br/>    y_pred, labels, input_length, label_length = args<br/>    return K.ctc_batch_cost(labels, y_pred, input_length, label_length</pre>
<ol start="9">
<li>Define the model:</li>
</ol>
<pre style="padding-left: 60px">input_data = Input(name='the_input', shape = (999,161), dtype='float32')<br/>inp = BatchNormalization(name="inp")(input_data)<br/>conv= Conv1D(filters=220, kernel_size = 11,strides = 2, padding='valid',activation='relu')(inp)<br/>conv = BatchNormalization(name="Normal0")(conv)<br/>conv1= Conv1D(filters=220, kernel_size = 11,strides = 2, padding='valid',activation='relu')(conv)<br/>conv1 = BatchNormalization(name="Normal1")(conv1)<br/>gru_3 = GRU(512, return_sequences = True, name = 'gru_3')(conv1)<br/>gru_4 = GRU(512, return_sequences = True, go_backwards = True, name = 'gru_4')(conv1)<br/>merged = concatenate([gru_3, gru_4])<br/>normalized = BatchNormalization(name="Normal")(merged)<br/>dense = TimeDistributed(Dense(30))(normalized)<br/>y_pred = TimeDistributed(Activation('softmax', name='softmax'))(dense)<br/>Model(inputs = input_data, outputs = y_pred).summary()</pre>
<ol start="10">
<li>Define the input and the output parameters of the CTC loss function:</li>
</ol>
<pre style="padding-left: 60px">from keras.optimizers import Adam<br/>Optimizer = Adam(lr = 0.001)<br/>labels = Input(name = 'the_labels', shape=[243], dtype='float32')<br/>input_length = Input(name='input_length', shape=[1],dtype='int64')<br/>label_length = Input(name='label_length',shape=[1],dtype='int64')<br/>output = Lambda(ctc_loss, output_shape=(1,),name='ctc')([y_pred, labels, input_length, label_length])</pre>
<ol start="11">
<li>Build and compile the model:</li>
</ol>
<pre style="padding-left: 60px">model = Model(inputs = [input_data, labels, input_length, label_length], outputs= output)<br/>model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = Optimizer, metrics = ['accuracy'])</pre>
<p style="padding-left: 60px">A summary of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/4cdf1be1-cff4-4eb0-8b4b-1f5ce3b93a33.png" style="width:41.83em;height:29.33em;" width="768" height="538"/></p>
<p class="mce-root"/>
<ol start="12">
<li>Fit the model on batches of data that are sampled from the input:</li>
</ol>
<pre style="padding-left: 60px">for i in range(2500):<br/>     samp=random.sample(range(x2.shape[0]-25),32)<br/>     batch_input=[inp2[i] for i in samp]<br/>     batch_input = np.array(batch_input)<br/>     batch_input = batch_input/np.max(batch_input)<br/>     batch_output = [y2[i] for i in samp]<br/>     batch_output = np.array(batch_output)<br/>     input_lengths2 = [input_lengths[i] for i in samp]<br/>     label_lengths2 = [label_lengths[i] for i in samp]<br/>     input_lengths2 = np.array(input_lengths2)<br/>     label_lengths2 = np.array(label_lengths2)<br/>     inputs = {'the_input': batch_input,<br/>             'the_labels': batch_output,<br/>             'input_length': input_lengths2,<br/>             'label_length': label_lengths2}<br/>     outputs = {'ctc': np.zeros([32])} <br/>     model.fit(inputs, outputs,batch_size = 32, epochs=1, verbose =1)</pre>
<p style="padding-left: 60px">In the preceding code, we are looping through and extracting batches of data 2,500 times, normalizing the input data, and fitting the model.</p>
<p style="padding-left: 60px">Also, we are performing a high number of epochs, as the CTC loss decreases slowly for this particular dataset and model combination.</p>
<ol start="13">
<li>Predict the test audio:</li>
</ol>
<pre style="padding-left: 60px">model2 = Model(inputs = input_data, outputs = y_pred)<br/><br/>k=-12<br/>pred= model2.predict(np.array(inp2[k]).reshape(1,999,161)/100)</pre>
<p style="padding-left: 60px">In the preceding code, we are specifying a model (<kbd>model2</kbd>) that takes the input test array and extracts the model prediction in each of the 243 time steps.</p>
<p style="padding-left: 60px">Additionally, we are extracting the prediction for the 12<sup>th</sup> element from the last of the input array (note that we have excluded the last 25 input data points from being considered while training the model). Furthermore, we have also pre-processed it in the same way that we did before, by passing the input data to the model-training process.</p>
<ol start="14">
<li>Decode the predictions on the new data point:</li>
</ol>
<pre style="padding-left: 60px">def decoder(pred):<br/>     pred_ints = (K.eval(K.ctc_decode(pred,[243])[0][0])).flatten().tolist()<br/>     out = ""<br/>     for i in range(len(pred_ints)):<br/>         if pred_ints[i]&lt;28:<br/>         out = out+charList[pred_ints[i]]<br/>     print(out)</pre>
<p style="padding-left: 60px">In the preceding code, we were decoding the prediction using the <kbd>ctc_decode</kbd> method. Alternatively, we could have decoded <span>the </span>prediction in the same way as we extracted the prediction in the handwritten image transcription. Finally, we print out the predictions.</p>
<p style="padding-left: 60px">We'll be in a position to decode the predictions by calling the previously defined function:</p>
<pre style="padding-left: 60px">decoder(pred)</pre>
<p style="padding-left: 60px">The output of one of the predictions is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/06e5ae31-fcc7-4a45-893e-13d568a5ed0e.png" style="width:40.42em;height:1.17em;" width="485" height="14"/></p>
<p style="padding-left: 60px">While the preceding output looks as if it's gibberish, it sounds phonetically similar to the actual audio, which is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/cd63b2fd-7610-4ec3-99d0-17b98214b6d8.png" style="width:47.58em;height:1.67em;" width="600" height="21"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Some ways in which we can further improve the accuracy of our transcriptions are as follows:</p>
<ul>
<li>Train on more data points</li>
<li>Incorporate a language model to perform fuzzy matching on the output so that we correct the predicted output</li>
</ul>


            </article>

            
        </section>
    </div>



  </body></html>