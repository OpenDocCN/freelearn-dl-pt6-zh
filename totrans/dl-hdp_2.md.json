["```py\n  0, -0.500568579838,  0.687106471955 \n  1,  0.190067977988, -0.341116711905 \n  0,  0.995019651532,  0.663292952846 \n  0, -1.03053733564,   0.342392729177 \n  1,  0.0376749555484,-0.836548188848 \n  0, -0.113745482508,  0.740204108847 \n  1,  0.56769119889,  -0.375810486522 \n\n```", "```py\nNd4j.ENFORCE_NUMERICAL_STABILITY = true; \nint batchSize = 50; \nint seed = 123; \ndouble learningRate = 0.005; \n\n```", "```py\nint nEpochs = 30;  \nint numInputs = 2; \nint numOutputs = 2; \nint numHiddenNodes = 20; \n\n```", "```py\nRecordReader rr = new CSVRecordReader(); \nrr.initialize(new FileSplit(new File(\"saturn_data_train.csv\"))); \nDataSetIterator trainIter = new RecordReaderDataSetIterator      \n                            (rr,batchSize,0,2); \n\n```", "```py\nRecordReader rrTest = new CSVRecordReader(); \nrrTest.initialize(new FileSplit(new File(\"saturn_data_eval.csv\"))); \nDataSetIterator trainIter = new RecordReaderDataSetIterator\n                            (rrTest,batchSize,0,2); \n\n```", "```py\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder() \n.seed(seed)\n.iterations(1)                          \n.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) \n.learningRate(learningRate) \n.updater(Updater.NESTEROVS).momentum(0.9)\n.list() \n.layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(numHiddenNodes) \n   .weightInit(WeightInit.XAVIER) \n   .activation(\"relu\")\n   .build()) \n .layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD) \n   .weightInit(WeightInit.XAVIER)                         \n   .activation(\"softmax\")\n   .nIn(numHiddenNodes).nOut(numOutputs).build())\n .pretrain(false)\n .backprop(true)\n .build(); \n\n```", "```py\nMultiLayerNetwork model = new MultiLayerNetwork(conf); \nmodel.init(); \n\n```", "```py\nmodel.setListeners(new ScoreIterationListener(5));      \nfor ( int n = 0; n < nEpochs; n++) \n{ \n\n```", "```py\n model.fit( trainIter ); \n}  \nSystem.out.println(\"Evaluating the model....\");  \nEvaluation eval = new Evaluation(numOutputs); \nwhile(testIter.hasNext())\n  { \n    DataSet t = testIter.next(); \n    INDArray features = t.getFeatureMatrix(); \n    INDArray lables = t.getLabels(); \n    INDArray predicted = model.output(features,false); \n    eval.eval(lables, predicted); \n  } \nSystem.out.println(eval.stats()); \n\n```", "```py\ndouble xMin = -15;\ndouble xMax = 15; \ndouble yMin = -15; \ndouble yMax = 15; \n\nint nPointsPerAxis = 100; \ndouble[][] evalPoints = new double[nPointsPerAxis*nPointsPerAxis][2]; \nint count = 0; \nfor( int i=0; i<nPointsPerAxis; i++ )\n{ \n for( int j=0; j<nPointsPerAxis; j++ )\n { \n   double x = i * (xMax-xMin)/(nPointsPerAxis-1) + xMin; \n   double y = j * (yMax-yMin)/(nPointsPerAxis-1) + yMin; \n\n   evalPoints[count][0] = x; \n   evalPoints[count][1] = y; \n\n   count++; \n } \n} \n\nINDArray allXYPoints = Nd4j.create(evalPoints); \n\nINDArray predictionsAtXYPoints = model.output(allXYPoints); \n\n```", "```py\n\nrr.initialize(new FileSplit(new File(\"saturn_data_train.csv\"))); \nrr.reset(); \nint nTrainPoints = 500; \ntrainIter = new RecordReaderDataSetIterator(rr,nTrainPoints,0,2); \nDataSet ds = trainIter.next(); \nPlotUtil.plotTrainingData(ds.getFeatures(), ds.getLabels(),allXYPoints, predictionsAtXYPoints, nPointsPerAxis); \n\n```", "```py\nrrTest.initialize(new FileSplit(new File(\"saturn_data_eval.csv\"))); \nrrTest.reset(); \nint nTestPoints = 100; \ntestIter = new RecordReaderDataSetIterator(rrTest,nTestPoints,0,2); \nds = testIter.next(); \nINDArray testPredicted = model.output(ds.getFeatures()); \nPlotUtil.plotTestData(ds.getFeatures(), ds.getLabels(), testPredicted, allXYPoints, predictionsAtXYPoints, nPointsPerAxis); \n\n```", "```py\no.d.o.l.ScoreIterationListener - Score at iteration 0 is    \n                                 0.6313823699951172 \no.d.o.l.ScoreIterationListener - Score at iteration 5 is \n                                 0.6154170989990234 \no.d.o.l.ScoreIterationListener - Score at iteration 10 is     \n                                 0.4763660430908203 \no.d.o.l.ScoreIterationListener - Score at iteration 15 is \n                                 0.52469970703125 \no.d.o.l.ScoreIterationListener - Score at iteration 20 is    \n                                 0.4296367645263672 \no.d.o.l.ScoreIterationListener - Score at iteration 25 is \n                                 0.4755714416503906 \no.d.o.l.ScoreIterationListener - Score at iteration 30 is \n                                 0.3985047912597656 \no.d.o.l.ScoreIterationListener - Score at iteration 35 is \n                                 0.4304619598388672 \no.d.o.l.ScoreIterationListener - Score at iteration 40 is   \n                                 0.3672477722167969 \no.d.o.l.ScoreIterationListener - Score at iteration 45 is \n                                 0.39150180816650393 \no.d.o.l.ScoreIterationListener - Score at iteration 50 is \n                                 0.3353725051879883 \no.d.o.l.ScoreIterationListener - Score at iteration 55 is \n                                 0.3596681213378906 \n\n```", "```py\nEvaluating the model.... \nExamples labeled as 0 classified by model as 0: 48 times \nExamples labeled as 1 classified by model as 1: 52 times \n\n```", "```py\n<!-- https://mvnrepository.com/artifact/org.Deeplearning4j/Deeplearning4j-hadoop --> \n<dependency> \n    <groupId>org.Deeplearning4j</groupId> \n    <artifactId>Deeplearning4j-hadoop</artifactId> \n    <version>0.0.3.2.7</version> \n</dependency> \n\n```", "```py\n<!-- https://mvnrepository.com/artifact/org.Deeplearning4j/dl4j-spark-nlp_2.11 --> \n<dependency> \n    <groupId>org.Deeplearning4j</groupId> \n    <artifactId>dl4j-spark-nlp_2.11</artifactId> \n    <version>0.5.0</version> \n</dependency> \n\n```", "```py\nSparkDl4jMultiLayer sparkNet = new SparkDl4jMultiLayer(sc,conf, \n                               new ParameterAveragingTrainingMaster\n                              .Builder(numExecutors(),dataSetObjSize \n                              .batchSizePerWorker(batchSizePerExecutor) \n                              .averagingFrequency(1) \n                              .repartionData(Repartition.Always) \n                              .build()); \nsparkNet.setCollectTrainingStats(true); \n\n```", "```py\nConfiguration config = new Configuration(); \nFileSystem hdfs = FileSystem.get(tempDir.toUri(), config); \nRemoteIterator<LocatedFileStatus> fileIter = hdfs.listFiles\n  (new org.apache.hadoop.fs.Path(tempDir.toString()),false); \n\nList<String> paths = new ArrayList<>(); \nwhile(fileIter.hasNext())\n  { \n   String path = fileIter.next().getPath().toString(); \n   paths.add(path); \n  } \n\n```", "```py\n2016-01-01 17:00:00.000,830a7u3,u323fy8902,1,USA,100.00,Legit \n2016-01-01 18:03:01.256,830a7u3,9732498oeu,3,FR,73.20,Legit \n2016-01-03 02:53:32.231,78ueoau32,w234e989,1,USA,1621.00,Fraud \n2016-01-03 09:30:16.832,t842uocd,9732498oeu,4,USA,43.19,Legit \n2016-01-04 23:01:52.920,t842uocd,cza8873bm,10,MX,159.65,Legit \n2016-01-05 02:28:10.648,t842uocd,fgcq9803,6,CAN,26.33,Fraud \n2016-01-05 10:15:36.483,rgc707ke3,tn342v7,2,USA,-0.90,Legit \n\n```", "```py\nSchema inputDataSchema = new Schema.Builder() \n     .addColumnString(\"DateTimeString\") \n     .addColumnsString(\"CustomerID\", \"MerchantID\")  \n     .addColumnInteger(\"NumItemsInTransaction\") \n     .addColumnCategorical(\"MerchantCountryCode\",  \n      Arrays.asList(\"USA\",\"CAN\",\"FR\",\"MX\")) \n     .addColumnDouble(\"TransactionAmountUSD\",0.0,null,false,false)  \n     .addColumnCategorical(\"FraudLabel\",Arrays.asList(\"Fraud\",\"Legit\"))\n     .build(); \n\nSystem.out.println(\"\\n\\nOther information obtainable from schema:\"); \nSystem.out.println(\"Number of columns: \" + \n                   inputDataSchema.numColumns()); \nSystem.out.println(\"Column names: \" +              \n                   inputDataSchema.getColumnNames()); \nSystem.out.println(\"Column types: \" +  \n                   inputDataSchema.getColumnTypes()); \n\n```", "```py\nTransformProcess tp = new TransformProcess.Builder(inputDataSchema) \n.removeColumns(\"CustomerID\",\"MerchantID\") \n.filter(new ConditionFilter(\n new CategoricalColumnCondition(\"MerchantCountryCode\", \n ConditionOp.NotInSet, new HashSet<>(Arrays.asList(\"USA\",\"MX\"))))) \n\n```", "```py\n.conditionalReplaceValueTransform( \n  \"TransactionAmountUSD\",       \n  new DoubleWritable(0.0),      \n  new DoubleColumnCondition(\"TransactionAmountUSD\",ConditionOp.LessThan\n  , 0.0))   \n\n```", "```py\n.stringToTimeTransform(\"DateTimeString\",\"YYYY-MM-DD HH:mm:ss.SSS\",            \n DateTimeZone.UTC) \n.renameColumn(\"DateTimeString\", \"DateTime\") \n.transform(new DeriveColumnsFromTimeTransform.Builder(\"DateTime\") \n   .addIntegerDerivedColumn(\"HourOfDay\", DateTimeFieldType.hourOfDay()) \n   .build())\n.removeColumns(\"DateTime\")\n.build(); \n\n```", "```py\nSchema outputSchema = tp.getFinalSchema(); \n\nSystem.out.println(\"\\nSchema after transforming data:\"); \nSystem.out.println(outputSchema); \n\n```", "```py\nSparkConf conf = new SparkConf(); \nconf.setMaster(\"local[*]\"); \nconf.setAppName(\"DataVec Example\"); \n\nJavaSparkContext sc = new JavaSparkContext(conf); \n\nString directory = new  ClassPathResource(\"exampledata.csv\").getFile()\n.getParent(); \n\n```", "```py\nJavaRDD<String> stringData = sc.textFile(directory); \n\n```", "```py\nRecordReader rr = new CSVRecordReader(); \nJavaRDD<List<Writable>> parsedInputData = stringData.map(new  StringToWritablesFunction(rr)); \n\n```", "```py\nSparkTransformExecutor exec = new SparkTransformExecutor(); \nJavaRDD<List<Writable>> processedData = exec.execute(parsedInputData, \ntp); \n\nJavaRDD<String> processedAsString = processedData.map(new \nWritablesToStringFunction(\",\"));  \n\n```", "```py\nprocessedAsString.saveAsTextFile(\"hdfs://your/hdfs/save/path/here\") \n\nList<String> processedCollected = processedAsString.collect(); \nList<String> inputDataCollected = stringData.collect(); \n\nSystem.out.println(\"\\n ---- Original Data ----\"); \nfor(String s : inputDataCollected) System.out.println(s); \n\nSystem.out.println(\"\\n ---- Processed Data ----\"); \nfor(String s : processedCollected) System.out.println(s); \n\n```", "```py\n14:20:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.0 KB, free 1390.9 MB) \n16/08/27 14:20:12 INFO MemoryStore: ensureFreeSpace(10065) called with curMem=106480, maxMem=1458611159 \n16/08/27 14:20:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.8 KB, free 1390.9 MB) \n16/08/27 14:20:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:46336 (size: 9.8 KB, free: 1391.0 MB) \n16/08/27 14:20:12 INFO SparkContext: Created broadcast 0 from textFile at BasicDataVecExample.java:144 \n16/08/27 14:20:13 INFO SparkTransformExecutor: Starting execution of stage 1 of 7 \n16/08/27 14:20:13 INFO SparkTransformExecutor: Starting execution of stage 2 of 7 \n16/08/27 14:20:13 INFO SparkTransformExecutor: Starting execution of stage 3 of 7 \n16/08/27 14:20:13 INFO SparkTransformExecutor: Starting execution of stage 4 of 7 \n16/08/27 14:20:13 INFO SparkTransformExecutor: Starting execution of stage 5 of 7 \n\n```", "```py\n---- Processed Data ---- \n17,1,USA,100.00,Legit \n2,1,USA,1621.00,Fraud \n9,4,USA,43.19,Legit \n23,10,MX,159.65,Legit \n10,2,USA,0.0,Legit \n\n```"]