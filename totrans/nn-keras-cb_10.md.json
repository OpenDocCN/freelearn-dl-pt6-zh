["```py\ndocs = [\"I enjoy playing TT\", \"I like playing TT\"]\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(min_df=0, token_pattern=r\"\\b\\w+\\b\")\nvectorizer.fit(docs)\n```", "```py\nvector = vectorizer.transform(docs)\n```", "```py\nprint(vectorizer.vocabulary_)\nprint(vector.shape)\nprint(vector.toarray())\n```", "```py\nx = []\ny = []\nfor i in range(len(docs)):\n     for j in range(len(docs[i].split())):\n         t_x = []\n         t_y = []\n         for k in range(4):\n             if(j==k):\n                 t_y.append(docs[i].split()[k])\n                 continue\n             else:\n                 t_x.append(docs[i].split()[k])\n         x.append(t_x)\n         y.append(t_y)\n\nx2 = []\ny2 = []\nfor i in range(len(x)):\n     x2.append(' '.join(x[i]))\n     y2.append(' '.join(y[i]))\n```", "```py\nvector_x = vectorizer.transform(x2)\nvector_x.toarray()\nvector_y = vectorizer.transform(y2)\nvector_y.toarray()\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(3, activation='linear', input_shape=(5,)))\nmodel.add(Dense(5,activation='sigmoid'))\n```", "```py\nmodel.compile(loss='binary_crossentropy',optimizer='adam')\n\nmodel.fit(vector_x, vector_y, epochs=1000, batch_size=4,verbose=1)\n```", "```py\nfrom keras.models import Model\nlayer_name = 'dense_5'\nintermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n```", "```py\nfor i in range(len(vectorizer.vocabulary_)):\n     word = list(vectorizer.vocabulary_.keys())[i]\n     word_vec = vectorizer.transform([list(vectorizer.vocabulary_.keys())[i]]).toarray()\n     print(word, intermediate_layer_model.predict(word_vec))\n```", "```py\n$pip install gensim\n```", "```py\nimport gensim\nimport pandas as pd\n```", "```py\ndata=pd.read_csv('https://www.dropbox.com/s/8yq0edd4q908xqw/airline_sentiment.csv')\ndata.head()                   \n```", "```py\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop = set(stopwords.words('english'))\ndef preprocess(text):\n    text=text.lower()\n    text=re.sub('[^0-9a-zA-Z]+',' ',text)\n    words = text.split()\n    words2 = [i for i in words if i not in stop]\n    words3=' '.join(words2)\n    return(words3)\ndata['text'] = data['text'].apply(preprocess)\n```", "```py\ndata['text'][0].split()\n```", "```py\nlist_words=[]\nfor i in range(len(data)):\n     list_words.append(data['text'][i].split())\n```", "```py\nlist_words[:3]\n```", "```py\nfrom gensim.models import Word2Vec\n```", "```py\nmodel = Word2Vec(size=100,window=5,min_count=30, sg=0, alpha = 0.025)\n```", "```py\nmodel.build_vocab(list_words)\n```", "```py\nmodel.wv.vocab.keys()\n```", "```py\nmodel.train(list_words, total_examples=model.corpus_count, epochs=100)\n```", "```py\nmodel = Word2Vec(list_words,size=100,window=5,min_count=30, iter = 100)\n```", "```py\nmodel['month']\n```", "```py\nmodel.similarity('month','year')\n0.48\n```", "```py\nmodel.most_similar('month')\n```", "```py\nmodel = Word2Vec(size=100,window=5,min_count=30, sg=0)\nmodel.build_vocab(list_words)\nmodel.train(list_words, total_examples=model.corpus_count, epochs=5)\nmodel.most_similar('month')\n```", "```py\n$wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n```", "```py\n$gunzip '/content/GoogleNews-vectors-negative300.bin.gz'\n```", "```py\nfrom gensim.models import KeyedVectors\nfilename = '/content/GoogleNews-vectors-negative300.bin'\nmodel = KeyedVectors.load_word2vec_format(filename, binary=True)\n```", "```py\nmodel.most_similar('month')\n```", "```py\nresult = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\nprint(result)\n```", "```py\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom nltk.tokenize import word_tokenize\n```", "```py\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop = set(stopwords.words('english'))\ndef preprocess(text):\n    text=text.lower()\n    text=re.sub('[^0-9a-zA-Z]+',' ',text)\n    words = text.split()\n    words2 = [i for i in words if i not in stop]\n    words3=' '.join(words2)\n    return(words3)\ndata['text'] = data['text'].apply(preprocess)\n```", "```py\nimport nltk\nnltk.download('punkt')\ntagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data['text'])]\n```", "```py\nmax_epochs = 100\nvec_size = 300\nalpha = 0.025\nmodel = Doc2Vec(size=vec_size,\n                alpha=alpha,\n                min_alpha=0.00025,\n                min_count=30,\n                dm =1)\n```", "```py\nmodel.build_vocab(tagged_data)\n```", "```py\nmodel.train(tagged_data,epochs=100,total_examples=model.corpus_count)\n```", "```py\nmodel['wife']\n```", "```py\nmodel.docvecs[0]\n```", "```py\nsimilar_doc = model.docvecs.most_similar('457')\nprint(similar_doc)\n```", "```py\ndata['text'][457]\n```", "```py\ndata['text'][827]\n```", "```py\nfrom gensim.models.fasttext import FastText\n```", "```py\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop = set(stopwords.words('english'))\ndef preprocess(text):\n    text=text.lower()\n    text=re.sub('[^0-9a-zA-Z]+',' ',text)\n    words = text.split()\n    words2 = [i for i in words if i not in stop]\n    words3=' '.join(words2)\n    return(words3)\ndata['text'] = data['text'].apply(preprocess)\n```", "```py\nlist_words=[]\nfor i in range(len(data)):\n     list_words.append(data['text'][i].split())\n```", "```py\nft_model = FastText(size=100)\nft_model.build_vocab(list_words)\n```", "```py\nft_model.train(list_words, total_examples=ft_model.corpus_count,epochs=100)\n```", "```py\nft_model.similarity('first','firstli')\n```", "```py\nresult = ft_model.most_similar(positive=['exprience', 'prmise'], negative=['experience'], topn=1)\nprint(result)\n```", "```py\nft_model.most_similar('exprience', topn=1)\n```", "```py\n$pip install glove_python\n```", "```py\nfrom glove import Corpus, Glove\n```", "```py\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nnltk.download('stopwords')\nstop = set(stopwords.words('english'))\ndata = pd.read_csv('https://www.dropbox.com/s/8yq0edd4q908xqw/airline_sentiment.csv?dl=1')\ndef preprocess(text):\n    text=text.lower()\n    text=re.sub('[^0-9a-zA-Z]+',' ',text)\n    words = text.split()\n    words2 = [i for i in words if i not in stop]\n    words3=' '.join(words2)\n    return(words3)\ndata['text'] = data['text'].apply(preprocess)\nlist_words=[]\nfor i in range(len(data)):\n      list_words.append(data['text'][i].split())\n```", "```py\ncorpus.fit(list_words, window=5)\n```", "```py\ncorpus.dictionary\n```", "```py\ncorpus.matrix.todense()\n```", "```py\nglove = Glove(no_components=100, learning_rate=0.025)\nglove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n```", "```py\nglove.word_biases.tolist()\nglove.word_vectors.tolist()\n```", "```py\nglove.word_vectors[glove.dictionary['united']]\n```", "```py\nglove.most_similar('united')\n```", "```py\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nnltk.download('stopwords')\nstop = set(stopwords.words('english'))\ndata=pd.read_csv('https://www.dropbox.com/s/8yq0edd4q908xqw/airline_sentiment.csv?dl=1')\n```", "```py\ndef preprocess(text):\n text=text.lower()\n text=re.sub('[^0-9a-zA-Z]+',' ',text)\n words = text.split()\n words2 = [i for i in words if i not in stop]\n words3=' '.join(words2)\n return(words3)\ndata['text'] = data['text'].apply(preprocess)\n```", "```py\nt=[]\nfor i in range(len(data)):\n t.append(data['text'][i].split())\n```", "```py\nfrom gensim.models import Word2Vec\nmodel = Word2Vec(size=100,window=5,min_count=30, sg=0)\n```", "```py\nmodel.build_vocab(t)\nmodel.train(t, total_examples=model.corpus_count, epochs=100)\n```", "```py\nimport numpy as np\nfeatures= []\nfor i in range(len(t)):\n      t2 = t[i]\n      z = np.zeros((1,100))\n      k=0\n      for j in range(len(t2)):\n            try:\n              z = z+model[t2[j]]\n              k= k+1\n            except KeyError:\n              continue\n      features.append(z/k)\n```", "```py\nfeatures = np.array(features)\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, data['airline_sentiment'], test_size=0.30,random_state=10)\nX_train = X_train.reshape(X_train.shape[0],100)\nX_test = X_test.reshape(X_test.shape[0],100)\n```", "```py\nfrom keras.layers import Dense, Activation\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.layers.embeddings import Embedding\nmodel = Sequential()\nmodel.add(Dense(1000,input_dim = 100,activation='relu'))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n```", "```py\nmodel.fit(X_train, y_train, batch_size=128, nb_epoch=5, validation_data=(X_test, y_test),verbose = 1)\n```", "```py\npred = model.predict(X_test)\npred2 = np.where(pred>0.5,1,0)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, pred2)\n```"]