- en: Chapter 3. Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, [Chapter 2](ch02.html "Chapter 2. Deep Neural Networks"),
    *Deep Neural Networks*, you were introduced to the concepts of deep neural networks.
    We're now going to move on to look at autoencoders, which are a neural network
    architecture that attempts to find a compressed representation of the given input
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the previous chapters, the input data may be in multiple forms including,
    speech, text, image, or video. An autoencoder will attempt to find a representation
    or code in order to perform useful transformations on the input data. As an example,
    in denoising autoencoders, a neural network will attempt to find a code that can
    be used to transform noisy data into clean ones. Noisy data could be in the form
    of an audio recording with static noise which is then converted into clear sound.
    Autoencoders will learn the code automatically from the data alone without human labeling.
    As such, autoencoders can be classified under **unsupervised** learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In later chapters of this book, we will look at **Generative Adversarial Networks**
    (**GANs**) and **Variational Autoencoders** (**VAEs**) which are also representative
    forms of unsupervised learning algorithms. This is in contrast to the supervised
    learning algorithms we discussed in the previous chapters where human annotations were
    required.
  prefs: []
  type: TYPE_NORMAL
- en: In its simplest form, an autoencoder will learn the representation or code by
    trying to copy the input to output. However, using an autoencoder is not as simple
    as copying the input to output. Otherwise, the neural network would not be able
    to uncover the hidden structure in the input distribution.
  prefs: []
  type: TYPE_NORMAL
- en: An autoencoder will encode the input distribution into a low-dimensional tensor,
    which usually takes the form of a vector. This will approximate the hidden structure
    that is commonly referred to as the latent representation, code, or vector. This
    process constitutes the encoding part. The latent vector will then be decoded
    by the decoder part to recover the original input.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of the latent vector being a low-dimensional compressed representation
    of the input distribution, it should be expected that the output recovered by
    the decoder can only approximate the input. The dissimilarity between the input
    and the output can be measured by a loss function.
  prefs: []
  type: TYPE_NORMAL
- en: But why would we use autoencoders? Simply put, autoencoders have practical applications
    both in their original form or as part of more complex neural networks. They're
    a key tool in understanding the advanced topics of deep learning as they give
    you a low-dimensional latent vector. Furthermore, it can be efficiently processed
    to perform structural operations on the input data. Common operations include
    denoising, colorization, feature-level arithmetic, detection, tracking, and segmentation,
    to name just a few.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the goal of this chapter is to present:'
  prefs: []
  type: TYPE_NORMAL
- en: The principles of autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement autoencoders into the Keras neural network library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main features of denoising and colorization autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principles of autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we're going to go over the principles of autoencoders. In this
    section, we're going to be looking at autoencoders with the MNIST dataset, which
    we were first introduced to in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we need to be made aware that an autoencoder has two operators, these
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder**: This transforms the input, *x*, into a low-dimensional latent
    vector, *z = f(x)*. Since the latent vector is of low dimension, the encoder is
    forced to learn only the most important features of the input data. For example,
    in the case of MNIST digits, the important features to learn may include writing
    style, tilt angle, roundness of stroke, thickness, and so on. Essentially, these
    are the most important information needed to represent digits zero to nine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder**: This tries to recover the input from the latent vector,![Principles
    of autoencoders](img/B08956_03_001.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: . Although the latent vector has a low dimension, it has a sufficient size to allow
    the decoder to recover the input data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The goal of the decoder is to make
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: as close as possible to *x*. Generally, both the encoder and decoder are non-linear
    functions. The dimension of *z* is a measure of the number of salient features
    it can represent. The dimension is usually much smaller than the input dimensions
    for efficiency and in order to constrain the latent code to learn only the most
    salient properties of the input distribution[1].
  prefs: []
  type: TYPE_NORMAL
- en: The autoencoder has the tendency to memorize the input when the dimension of
    the latent code is significantly bigger than *x*.
  prefs: []
  type: TYPE_NORMAL
- en: A suitable loss function,
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ', is a measure of how dissimilar the input, *x*, from the output which is the
    recovered input,'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '. As shown in the following equation, the **Mean Squared Error** (**MSE**)
    is an example of such a loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (Equation 3.1.1)
  prefs: []
  type: TYPE_NORMAL
- en: In this example, *m* is the output dimensions (For example, in MNIST *m = width
    × height × channels = 28 × 28 × 1 = 784*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: are the elements of *x* and
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: respectively. Since the loss function is a measure of dissimilarity between
    the input and output, we're able to use alternative reconstruction loss functions
    such as the binary cross entropy or **structural similarity index** (**SSIM**).
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1.1: Block diagram of an autoencoder'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1.2: An autoencoder with MNIST digit input and output. The latent
    vector is 16-dim.'
  prefs: []
  type: TYPE_NORMAL
- en: To put the autoencoder in context, *x* can be an MNIST digit which has a dimension
    of 28 × 28 × 1 = 784\. The encoder transforms the input into a low-dimensional
    *z* that can be a 16-dimension latent vector. The decoder will attempt to recover
    the input in the form of
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: from *z*. Visually, every MNIST digit *x* will appear similar to
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: . *Figure 3.1.2* demonstrates this autoencoding process to us. We can observe
    that the decoded digit 7, while not exactly the same remains close enough.
  prefs: []
  type: TYPE_NORMAL
- en: Since both encoder and decoder are non-linear functions, we can use neural networks
    to implement both. For example, in the MNIST dataset, the autoencoder can be implemented
    by MLP or CNN. The autoencoder can be trained by minimizing the loss function
    through backpropagation. Similar to other neural networks, the only requirement
    is that the loss function must be differentiable.
  prefs: []
  type: TYPE_NORMAL
- en: If we treat the input as a distribution, we can interpret the encoder as an
    encoder of distribution,
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and the decoder, as the decoder of distribution,
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '. The loss function of the autoencoder is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (Equation 3.1.2)
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function simply means that we would like to maximize the chances of
    recovering the input distribution given the latent vector distribution. If the
    decoder output distribution is assumed to be Gaussian, then the loss function
    boils down to MSE since:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (Equation 3.1.3)
  prefs: []
  type: TYPE_NORMAL
- en: In this example,
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: represents a Gaussian distribution with a mean of
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and variance of
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: . A constant variance is assumed. The decoder output
  prefs: []
  type: TYPE_NORMAL
- en: '![Principles of autoencoders](img/B08956_03_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: is assumed to be independent. While *m* is the output dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Building autoencoders using Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're now going to move onto something really exciting, building an autoencoder
    using Keras library. For simplicity, we'll be using the MNIST dataset for the
    first set of examples. The autoencoder will then generate a latent vector from
    the input data and recover the input using the decoder. The latent vector in this
    first example is 16-dim.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we're going to implement the autoencoder by building the encoder. *Listing* *3.2.1*
    shows the encoder that compresses the MNIST digit into a 16-dim latent vector.
    The encoder is a stack of two `Conv2D`. The final stage is a `Dense` layer with
    16 units to generate the latent vector. *Figure 3.2.1* shows the architecture
    model diagram generated by `plot_model()` which is the same as the text version
    produced by `encoder.summary()`. The shape of the output of the last `Conv2D`
    is saved to compute the dimensions of the decoder input layer for easy reconstruction
    of the MNIST image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Listing 3.2.1, shows `autoencoder-mnist-3.2.1.py`. This is an autoencoder
    implementation using Keras. The latent vector is 16-dim:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Building autoencoders using Keras](img/B08956_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2.1: The encoder model is a made up of Conv2D(32)-Conv2D(64)-Dense(16)
    in order to generate the low dimensional latent vector'
  prefs: []
  type: TYPE_NORMAL
- en: The decoder in *Listing* *3.2.1* decompresses the latent vector in order to
    recover the MNIST digit. The decoder input stage is a `Dense` layer that will
    accept the latent vector. The number of units is equal to the product of the saved
    `Conv2D` output dimensions from the encoder. This is done so we can easily resize
    the output of the `Dense` layer for `Conv2DTranspose` to finally recover the original
    MNIST image dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder is made of a stack of three `Conv2DTranspose`. In our case, we're
    going to use a **Transposed CNN** (sometimes called deconvolution), which is more
    commonly used in decoders. We can imagine transposed CNN (`Conv2DTranspose`) as
    the reversed process of CNN. In a simple example, if the CNN converts an image
    to feature maps, the transposed CNN will produce an image given feature maps.
    *Figure 3.2.2* shows the decoder model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building autoencoders using Keras](img/B08956_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2.2: The decoder model is made of a Dense(16)-Conv2DTranspose(64)
    -Conv2DTranspose(32)-Conv2DTranspose(1). The input is the latent vector decoded
    to recover the original input.'
  prefs: []
  type: TYPE_NORMAL
- en: By joining the encoder and decoder together, we're able to build the autoencoder.
    *Figure 3.2.3* illustrates the model diagram of the autoencoder. The tensor output
    of the encoder is also the input to a decoder which generates the output of the
    autoencoder. In this example, we'll be using the MSE loss function and Adam optimizer.
    During training, the input is the same as the output, `x_train`. We should note
    that in our example, there are only a few layers which are sufficient enough to
    drive the validation loss to 0.01 in one epoch. For more complex datasets, you
    may need a deeper encoder, decoder as well as more epochs of training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building autoencoders using Keras](img/B08956_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2.3: The autoencoder model is built by joining an encoder model and
    a decoder model together. There are 178k parameters for this autoencoder.'
  prefs: []
  type: TYPE_NORMAL
- en: After training the autoencoder for one epoch with a validation loss of 0.01,
    we're able to verify if it can encode and decode the MNIST data that it has not
    seen before. *Figure 3.2.4* shows us eight samples from the test data and the
    corresponding decoded images. Except for minor blurring in the images, we're able
    to easily recognize that the autoencoder is able to recover the input with good
    quality. The results will improve as we train for a larger number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building autoencoders using Keras](img/B08956_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2.4: Prediction of the autoencoder from the test data. The first 2
    rows are the original input test data. The last 2 rows are the predicted data.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we may be wondering how we can visualize the latent vector in
    space. A simple method for visualization is to force the autoencoder to learn
    the MNIST digits features using a 2-dim latent vector. From there, we're able
    to project this latent vector on a 2D space in order to see how the MNIST codes
    are distributed. By setting the `latent_dim = 2` in `autoencoder-mnist-3.2.1.py`
    code and by using the `plot_results()` to plot the MNIST digit as a function of
    the 2-dim latent vector, *Figure 3.2.5* and *Figure 3.2.6* shows the distribution
    of MNIST digits as a function of latent codes. These figures were generated after
    20 epochs of training. For convenience, the program is saved as `autoencoder-2dim-mnist-3.2.2.py`
    with the partial code shown in *Listing 3.2.2*.
  prefs: []
  type: TYPE_NORMAL
- en: Following is Listing 3.2.2, `autoencoder-2dim-mnist-3.2.2.py`, which shows the
    function for visualization of the MNIST digits distribution over 2-dim latent
    codes. The rest of the code is practically similar to *Listing 3.2.1* and no longer
    shown here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Building autoencoders using Keras](img/B08956_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2.5: A MNIST digit distribution as a function of latent code dimensions,
    *z*[0] and *z*[1]. Original color photo can be found on the book GitHub repository,
    https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter3-autoencoders/README.md.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building autoencoders using Keras](img/B08956_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2.6: Digits generated as the 2-dim latent vector space is navigated'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3.2.5*, we'll be able to see that the latent codes for a specific
    digit are clustering on a region in space. For example, digit 0 is on the lower
    left quadrant, while digit 1 is on the upper right quadrant. Such clustering is
    mirrored in *Figure 3.2.6*. In fact, the same figure shows the result of navigating
    or generating new digits from the latent space as shown in the *Figure 3.2.5*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, starting from the center and varying the value of a 2-dim latent
    vector towards the lower left quadrant, shows us that the digit changes from 2
    to 0\. This is expected since from *Figure 3.2.5*, we're able to see that the
    codes for the digit 2 clusters are near the center, and as discussed digit 0 codes
    cluster in the lower left quadrant. For *Figure 3.2.6*, we've only explored the
    regions between -4.0 and +4.0 for each latent dimension.
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in *Figure 3.2.5*, the latent code distribution is not continuous
    and ranges beyond
  prefs: []
  type: TYPE_NORMAL
- en: '![Building autoencoders using Keras](img/B08956_03_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: . Ideally, it should look like a circle where there are valid values everywhere.
    Because of this discontinuity, there are regions where if we decode the latent
    vector, hardly recognizable digits will be produced.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising autoencoder (DAE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''re now going to build an autoencoder with a practical application. Firstly,
    let''s paint a picture and imagine that the MNIST digits images were corrupted
    by noise, thus making it harder for humans to read. We''re able to build a **Denoising
    Autoencoder** (**DAE**) to remove the noise from these images. *Figure 3.3.1*
    shows us three sets of MNIST digits. The top rows of each set (for example, MNIST
    digits 7, 2, 1, 9, 0, 6, 3, 4, 9) are the original images. The middle rows show
    the inputs to DAE, which are the original images corrupted by noise. The last
    rows show the outputs of DAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3.1: Original MNIST digits (top rows), corrupted original images (middle
    rows) and denoised images (last rows)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3.2: The input to the denoising autoencoder is the corrupted image.
    The output is the clean or denoised image. The latent vector is assumed to be
    16-dim.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 3.3.2*, the denoising autoencoder has practically the same
    structure as the autoencoder for MNIST that we presented in the previous section.
    The input is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (Equation 3.3.1)
  prefs: []
  type: TYPE_NORMAL
- en: In this formula,
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: represents the original MNIST image corrupted by *noise*.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of the encoder is to discover how to produce the latent vector,
    *z*, that will enable the decoder to recover
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'by minimizing the dissimilarity loss function such as MSE, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (Equation 3.3.2)
  prefs: []
  type: TYPE_NORMAL
- en: In this example, *m* is the output dimensions (for example, in MNIST *m = width
    × height × channels = 28 × 28 × 1 = 784*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: are the elements of
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ', respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: To implement DAE, we're going to need to make a few changes on the autoencoder
    presented in the previous section. Firstly, the training input data should be
    corrupted MNIST digits. The training output data is the same original clean MNIST
    digits. This is like telling the autoencoder what the corrected images should
    be or asking it to figure out how to remove noise given a corrupted image. Lastly,
    we must validate the autoencoder on the corrupted MNIST test data.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST digit 7 shown on the left of *Figure 3.3.2* is an actual corrupted
    image input. The one on the right is the clean image output of a trained denoising
    autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 3.3.1* shows the denoising autoencoder which has been contributed
    to the Keras GitHub repository. Using the same MNIST dataset, we''re able to simulate
    corrupted images by adding random noise. The noise added is a Gaussian distribution
    with a mean,'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and standard deviation of
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: . Since adding random noise may push the pixel data into invalid values of less
    than 0 or greater than 1, the pixel values are clipped to [0.1, 1.0] range.
  prefs: []
  type: TYPE_NORMAL
- en: Everything else will remain practically the same as the autoencoder from the
    previous section. We'll use the same MSE loss function and Adam optimizer as the
    autoencoder. However, the number of epoch for training has increased to 10\. This
    is to allow sufficient parameter optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.3.1* shows actual validation data with both the corrupted and denoised
    test MNIST digits. We''re even able to see that humans will find it difficult
    to read the corrupted MNIST digits. *Figure 3.3.3* shows a certain level of robustness
    of DAE as the level of noise is increased from'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: to
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: . At
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ', DAE is still able to recover the original images. However, at'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ', a few digits such as 4 and 5 in the second and third sets can no longer be
    recovered correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising autoencoder (DAE)](img/B08956_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3.3: Performance of denoising autoencoder as the noise level is increased'
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen in Listing 3.3.1, `denoising-autoencoder-mnist-3.3.1.py` shows us a Denoising
    autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Automatic colorization autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're now going to work on another practical application of autoencoders. In
    this case, we're going to imagine that we have a grayscale photo and that we want
    to build a tool that will automatically add color to them. We would like to replicate
    the human abilities in identifying that the sea and sky are blue, the grass field
    and trees are green, while clouds are white, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 3.4.1*, if we are given a grayscale photo of a rice field
    on the foreground, a volcano in the background and sky on top, we're able to add
    the appropriate colors.
  prefs: []
  type: TYPE_NORMAL
- en: '![Automatic colorization autoencoder](img/B08956_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4.1: Adding color to a grayscale photo of the Mayon Volcano. A colorization
    network should replicate human abilities by adding color to a grayscale photo.
    Left photo is grayscale. The right photo is color. Original color photo can be
    found on the book GitHub repository, https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter3-autoencoders/README.md.'
  prefs: []
  type: TYPE_NORMAL
- en: A simple automatic colorization algorithm seems like a suitable problem for
    autoencoders. If we can train the autoencoder with a sufficient number of grayscale
    photos as input and the corresponding colored photos as output, it could possibly
    discover the hidden structure on properly applying colors. Roughly, it is the
    reverse process of denoising. The question is, can an autoencoder add color (good
    noise) to the original grayscale image?
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing* *3.4.1* shows the colorization autoencoder network. The colorization
    autoencoder network is a modified version of denoising autoencoder that we used for
    the MNIST dataset. Firstly, we need a dataset of grayscale to colored photos.
    The CIFAR10 database, which we have used before, has 50,000 training and 10,000
    testing 32 × 32 RGB photos that can be converted to grayscale. As shown in the
    following listing, we''re able to use the `rgb2gray()` function to apply weights
    on R, G, and B components to convert from color to grayscale.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 3.4.1, `colorization-autoencoder-cifar10-3.4.1.py`, shows us a colorization
    autoencoder using the CIFAR10 dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We've increased the capacity of the autoencoder by adding one more block of
    convolution and transposed convolution. We've also doubled the number of filters
    at each CNN block. The latent vector is now 256-dim in order to increase the number
    of salient properties it can represent as discussed in the autoencoder section.
    Finally, the output filter size has increased to three, or equal to the number
    of channels in RGB of the expected colored output.
  prefs: []
  type: TYPE_NORMAL
- en: The colorization autoencoder is now trained with the grayscale as inputs and
    original RGB images as outputs. The training will take more epochs and uses the
    learning rate reducer to scale down the learning rate when the validation loss
    is not improving. This can be easily done by telling the `callbacks` argument
    in the Keras `fit()` function to call the `lr_reducer()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3.4.2* demonstrates colorization of grayscale images from the test
    dataset of CIFAR10\. *Figure 3.4.3* compares the ground truth with the colorization
    autoencoder prediction. The autoencoder performs an acceptable colorization job.
    The sea or sky is predicted to be blue, animals have varying brown shades, the
    cloud is white, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: There are some noticeable wrong predictions like red vehicles have become blue
    or blue vehicles become red, and the occasional green field has been mistaken
    as blue skies, and dark or golden skies are converted to blue skies.
  prefs: []
  type: TYPE_NORMAL
- en: '![Automatic colorization autoencoder](img/B08956_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4.2: Automatic grayscale to color image conversion using the autoencoder.
    CIFAR10 test grayscale input images (left) and predicted color images (right).
    Original color photo can be found on the book GitHub repository, https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter3-autoencoders/README.md.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Automatic colorization autoencoder](img/B08956_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4.3: Side by side comparison of ground truth color images and predicted
    colorized images. Original color photos can be found on the book GitHub repository,
    https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter3-autoencoders/README.md.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've been introduced to autoencoders, which are neural networks that
    compress input data into low-dimensional codes in order to efficiently perform
    structural transformations such as denoising and colorization. We've laid the
    foundations to the more advanced topics of GANs and VAEs, that we will introduce
    in later chapters, while still exploring how autoencoders can utilize Keras. We've
    demonstrated how to implement an autoencoder from two building block models, both
    encoder and decoder. We've also learned how the extraction of a hidden structure
    of input distribution is one of the common tasks in AI.
  prefs: []
  type: TYPE_NORMAL
- en: Once the latent code has been uncovered, there are many structural operations
    that can be performed on the original input distribution. In order to gain a better
    understanding of the input distribution, the hidden structure in the form of the
    latent vector can be visualized using low-level embedding similar to what we did
    in this chapter or through more sophisticated dimensionality reduction techniques
    such t-SNE or PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from denoising and colorization, autoencoders are used in converting input
    distribution to low-dimensional latent codes that can be further processed for
    other tasks such as segmentation, detection, tracking, reconstruction, visual
    understanding, and so on. In [Chapter 8](ch08.html "Chapter 8. Variational Autoencoders
    (VAEs)"), *Variational Autoencoders (VAEs)*, we will discuss VAEs which are structurally
    the same as autoencoder but differ by having an interpretable latent code that
    can produce a continuous latent codes projection. In the next chapter, we will
    embark on one of the most important recent breakthroughs in AI, the introduction
    of GANs where we will learn of the core strengths of GANs and their ability to
    synthesize data or signals that look real.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ian Goodfellow and others. *Deep learning*. Vol. 1\. Cambridge: MIT press,
    2016 ([http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
