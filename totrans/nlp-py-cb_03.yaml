- en: Pre-Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization – learning to use the inbuilt tokenizers of NLTK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming – learning to use the inbuilt stemmers of NLTK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lemmatization – learning to use the WordnetLemmatizer of NLTK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stopwords – learning to use the stopwords corpus and seeing the difference it
    can make
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edit distance – writing your own algorithm to find edit distance between two
    strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing two short stories and extracting the common vocabulary between two
    of them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned to read, normalize, and organize raw data
    coming from heterogeneous forms and formats into uniformity. In this chapter,
    we will go a step forward and prepare the data for consumption in our NLP applications.
    Preprocessing is the most important step in any kind of data processing task,
    or else we fall prey to the age old computer science cliché of *garbage in, garbage
    out*. The aim of this chapter is to introduce some of the critical preprocessing
    steps such as tokenization, stemming, lemmatization, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be seeing six different recipes. We will build up the
    chapter by performing each preprocessing task in individual recipes—tokenization,
    stemming, lemmatization, stopwords treatment, and edit distance—in that order.
    In the last recipe, we will look at an example of how we can combine some of these
    preprocessing techniques to find common vocabulary between two free-form texts.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization – learning to use the inbuilt tokenizers of NLTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understand the meaning of tokenization, why we need it, and how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first see what a token is. When you receive a document or a long string
    that you want to process or work on, the first thing you'd want to do is break
    it into words and punctuation marks. This is what we call the process of tokenization.
    We will see what types of tokenizers are available with NLTK and implement them
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a file named `tokenizer.py` and add the following import lines to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Import the four different types of tokenizers that we are going to explore in
    this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with `LineTokernizer`. Add the following two lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As the name implies, this tokenizer is supposed to divide the input string
    into lines (not sentences, mind you!). Let''s see the output and what the tokenizer
    does:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it has returned a list of three strings, meaning the given input
    has been divided in to three lines on the basis of where the newlines are. `LineTokenizer`
    simply divides the given input string into new lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will see `SpaceTokenizer`. As the name implies, it is supposed to divide
    on split on space characters. Add the following lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sTokenizer` object is an object of `SpaceTokenize`. We have invoked the
    `tokenize()` method and we shall see the output now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the input `rawText` is split on the space character  `""`.  On
    to the next one! It''s the `word_tokenize()` method. Add the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'See the difference here. The other two we have seen so far are classes, whereas
    this is a method of the `nltk` module. This is the method that we will be using
    most of the time going forward as it does exactly what we''ve defined to be tokenization.
    It breaks up words and punctuation marks. Let''s see the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the difference between `SpaceTokenizer` and `word_tokenize()`
    is clearly visible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, on to the last one. There''s a special `TweetTokernizer` that we can use
    when dealing with special case strings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Tweets contain special words, special characters, hashtags, and smileys that
    we want to keep intact. Let''s see the output of these two lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As we see, the `Tokenizer` kept the hashtag word intact and didn't break it;
    the smileys are also kept intact and are not lost. This is one special little
    class that can be used when the application demands it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the output of the program in full. We have already seen it in detail,
    so I will not be going into it again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw three tokenizer classes and a method implemented to do the job in the
    NLTK module. It's not very difficult to understand how to do it, but it is worth
    knowing why we do it. The smallest unit to process in language processing task
    is a token. It is very much like a divide-and-conquer strategy, where we try to
    make sense of the smallest units at a granular level and add them up to understand
    the semantics of the sentence, paragraph, document, and the corpus (if any) by
    moving up the level of detail.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming – learning to use the inbuilt stemmers of NLTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's understand the concept of a stem and the process of stemming. We will
    learn why we need to do it and how to perform it using inbuilt NLTK stemming classes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So what is a stem supposed to be? A stem is the base form of a word without
    any suffixes. And a stemmer is what removes the suffixes and returns the stem
    of the word. Let's see what types of stemmers are available with NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a file named `stemmers.py` and add the following import lines to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Importing the four different types of tokenizers that we are going to explore
    in this recipe
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we apply any stems, we need to tokenize the input text. Let''s quickly
    get that done with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The token list contains all the `tokens` generated from the `raw` input string.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we shall `seePorterStemmer`. Let''s add the following three lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we initialize the stemmer object. Then we apply the stemmer to all `tokens`
    of the input text, and finally we `print` the output. Let''s see the output and
    we will know more:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the output, all the words have been rid of the trailing `'s'`,
    `'es'`, `'e'`, `'ed'`, `'al'`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next one is `LancasterStemmer`. This one is supposed to be even more error
    prone as it contains many more suffixes to be removed than `porter`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The same drill! Just that this time we have `LancasterStemmer` instead of `PrterStemmer`.
    Let''s see the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We shall discuss the difference in the output section, but we can make out that
    the suffixes that are dropped are bigger than Porter. `'us'`, `'e'`, `'th'`, `'eral'`,
    `"ered"`, and many more!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the output of the program in full. We will compare the output of both
    the stemmers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As we compare the output of both the stemmers, we see that `lancaster` is clearly
    the greedier one when dropping suffixes. It tries to remove as many characters
    from the end as possible, whereas `porter` is non-greedy and removes as little
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For some language processing tasks, we ignore the form available in the input
    text and work with the stems instead. For example, when you search on the Internet
    for *cameras*, the result includes documents containing the word *camera* as well
    as *cameras*, and vice versa. In hindsight though, both words are the same; the
    stem is *camera*.
  prefs: []
  type: TYPE_NORMAL
- en: Having said this, we can clearly see that this method is quite error prone,
    as the spellings are quite meddled with after a stemmer is done reducing the words.
    At times, it might be okay, but if you really want to understand the semantics,
    there is a lot of data loss here. For this reason, we shall next see what is called
    **lemmatization**.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization – learning to use the WordnetLemmatizer of NLTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understand what lemma and lemmatization are. Learn how lemmatization differs
    from Stemming, why we need it, and how to perform it using `nltk` library's `WordnetLemmatizer`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lemma is a lexicon headword or, more simply, the base form of a word. We have
    already seen what a stem is, but a lemma is a dictionary-matched base form unlike
    the stem obtained by removing/replacing the suffixes. Since it is a dictionary
    match, lemmatization is a slower process than Stemming.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a file named `lemmatizer.py` and add the following import lines to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We will need to tokenize the sentences first, and we shall use the `PorterStemmer`
    to compare the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we apply any stems, we need to tokenize the input text. Let''s quickly
    get that done with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The token list contains all the tokens generated from the `raw` input string.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will `applyPorterStemmer`, which we have already seen in the previous
    recipe. Let''s add the following three lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: First, we initialize the stemmer object. Then we apply the stemmer on all `tokens`
    of the input text, and finally we print the output. We shall check the output
    at the end of the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we apply the `lemmatizer`. Add the following three lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Run, and the output of these three lines will be like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As you see, it understands that for nouns it doesn't have to remove the trailing
    `'s'`. But for non-nouns, for example, legions and armies, it removes suffixes
    and also replaces them. However, what it’s essentially doing is a dictionary match.
    We shall discuss the difference in the output section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the output of the program in full. We will compare the output of both
    the stemmers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As we compare the output of the stemmer and the `lemmatizer`, we see that the
    stemmer makes a lot of mistakes and the `lemmatizer` makes very few mistakes.
    However, it doesn't do anything with the word `'murdered'`, and that is an error.
    Yet, as an end product, `lemmatizer` does a far better job of getting us the base
    form than the stemmer.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`WordNetLemmatizer` removes affixes only if it can find the resulting word
    in the dictionary. This makes the process of lemmatization slower than Stemming.
    Also, it understands and treats capitalized words as special words; it doesn’t
    do any processing for them and returns them as is. To work around this, you may
    want to convert your input string to lowercase and then run lemmatization on it.'
  prefs: []
  type: TYPE_NORMAL
- en: All said and done, lemmatization is still not perfect and will make mistakes.
    Check the input string and the result of this recipe; it couldn't convert `'murdered'`
    to `'murder'`. Similarly, it will handle the word `'women'` correctly but can't
    handle `'men'`.
  prefs: []
  type: TYPE_NORMAL
- en: Stopwords – learning to use the stopwords corpus and seeing the difference it
    can make
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using the Gutenberg corpus as an example in this recipe. The Gutenberg
    corpus is part of the NLTK data module. It contains a selection of 18 texts from
    some 25,000 electronic books from the project Gutenberg text archives. It is  `PlainTextCorpus`,
    meaning there are no categories involved with this corpus. It is best suited if
    you want to play around with the words/tokens without worrying about the affinity
    of the text to any particular topic. One of the objectives of this little recipe
    is also to introduce one of the most important preprocessing steps in text analytics—stopwords
    treatment.
  prefs: []
  type: TYPE_NORMAL
- en: In accordance with the objectives, we will use this corpus to elaborate the
    usage of Frequency Distribution of the NLTK module in Python within the context
    of stopwords. To give a small synopsis, a stopword is a word that, though it has
    significant syntactic value in sentence formation, carries very negligible or
    minimal semantic value. When you are not working with the syntax but with a bag-of-words
    kind of approach (for example, TF/IDF), it makes sense to get rid of stopwords
    except the ones that you are specifically interested in.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `nltk.corpus.stopwords` is also a corpus as part of the NLTK Data module
    that we will use in this recipe, along with `nltk.corpus.gutenberg`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a new file named `Gutenberg.py` and add the following three lines of
    code to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we are importing the required libraries and the Gutenberg corpus in the
    first two lines. The second line is used to check if the corpus was loaded successfully.
    Run the file on the Python interpreter and you should get an output that looks
    similar to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the names of all 18 Gutenberg texts are printed on the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following two lines of code, where we are doing a little preprocessing
    step on the list of all words from the corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The first line simply copies the list of all words in the corpus from the sample
    bible—`kjv.txt` in the `gb_words` variable. The second, and interesting, step
    is where we are iterating over the entire list of words from Gutenberg, discarding
    all the words/tokens whose length is two characters or less.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will access `nltk.corpus.stopwords` and do `stopwords` treatment on
    the filtered words list from the previous list. Add the following lines of code
    for the same:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The first line simply loads words from the stopwords corpus into the `stopwords`
    variable for the `english` language. The second line is where we are filtering
    out all `stopwords` from the filtered word list we had developed in the previous
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will simply apply `nltk.FreqDist` to the list of preprocessed `words`
    and the plain list of `words`. Add these lines to do the same:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Create the `FreqDist` object by passing as argument the words list that we formulated
    in steps 2 and 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we want to see some of the characteristics of the frequency distribution
    that we just made. Add the following four lines in the code and we will see what
    each does:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `most_common(10)` function will return the `10` most common words in the
    word bag being processed by frequency distribution. What it outputs is what we
    will discuss and elaborate now.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you run this program, you should get something similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you look carefully at the output, the most common 10 words in the unprocessed
    or plain list of words won't make much sense. Whereas from the preprocessed bag
    of words, the most common 10 words such as `god`, `lord`, and `man` give us a
    quick understanding that we are dealing with a text related to faith or religion.
  prefs: []
  type: TYPE_NORMAL
- en: The foremost objective of this recipe is to introduce you to the concept of
    stopwords treatment for text preprocessing techniques that you would most likely
    have to do before running any complex analysis on your data. The NLTK stopwords
    corpus contains stop-words for 11 languages. When you are trying to analyze the
    importance of keywords in any text analytics application, treating the stopwords
    properly will take you a long way. Frequency distribution will help you get the
    importance of words. Statistically speaking, this distribution would ideally look
    like a bell curve if you plot it on a two-dimensional plane of frequency and importance
    of words.
  prefs: []
  type: TYPE_NORMAL
- en: Edit distance – writing your own algorithm to find edit distance between two
    strings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Edit distance, also called as **Levenshtein distance** is a metric used to measure
    the similarity between two distances. Essentially, it’s a count of how many edit
    operations, deletions, insertions, or substitutions will transform a given String
    `A` to String `B`. We shall write our own algorithm to calculate the edit distance
    and then compare it against `nltk.metrics.distance.edit_distance()` for a sanity
    check.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may want to look up a little more on the Levenshtein distance part for mathematical
    equations. We will look at the algorithm implementation in python and why we do
    it, but it may not be feasible to cover the complete mathematics behind it. Here’s
    a link on Wikipedia: [https://en.wikipedia.org/wiki/Levenshtein_distance](https://en.wikipedia.org/wiki/Levenshtein_distance).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a file named `edit_distance_calculator.py` and add the following import
    lines to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We just imported the inbuilt `nltk` library's `edit_distance` function from
    the `nltk.metrics.distance` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define our method to accept two strings and calculate the edit distance
    between the two. `str1` and `str2` are two strings that the function accepts,
    and we will return an integer distance value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to get the length of the two input strings. We will be using
    the length to create an *m x n* table where `m` and `n` are the lengths of the
    two strings `s1` and `s2` respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will create `table` and initialize the first row and first column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This will initialize the two-dimensional array and the contents will look like
    the following table in memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/dc28e376-df91-45f1-b9a5-b400657cd191.png)'
  prefs: []
  type: TYPE_IMG
- en: Please note that this is inside a function and I'm using the example strings
    we are going to pass to the function to elaborate the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now comes the tricky part. We are going to fill up the matrix using the formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `cost` is calculated on whether the characters in contention are the same
    or they edition, specifically deletion or insertion. The formula in the next line
    for is calculating the value of the cell in the matrix, the first two take care
    of substitution and the third one is for substitution. We also add the cost of
    the previous step to it and take the minimum of the three.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end, we return the value of the last cell, that is, `table[m,n]`, as
    the final edit distance value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will call our function and the `nltk` library''s  `edit_distance()`
    function on two strings and check the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Our words are `hand` and `and`. Only a single delete operation on the first
    string or a single insertion operation on the second string will give us a match.
    Hence, the expected Levenshtein score is `1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s the output of the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the NLTK `edit_distance()` returns `1` and so does our algorithm.
    Fair to say that our algorithm is doing as expected, but I would urge you guys
    to test it further by running it through with some more examples.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I''ve already given you a brief on the algorithm; now let’s see how the matrix
    *table* gets populated with the algorithm. See the attached table here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/719ea28d-7009-4ffc-a111-b8f99a86de11.png)'
  prefs: []
  type: TYPE_IMG
- en: You've already seen how we initialized the matrix. Then we filled up the matrix
    using the formula in algorithm. The yellow trail you see is the significant numbers.
    After the first iteration, you can see that the distance is moving in the direction
    of 1 consistently and the final value that we return is denoted by the green background
    cell.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the applications of the edit distance algorithm are multifold. First and
    foremost, it is used in spell checkers and auto-suggestions in text editors, search
    engines, and many such text-based applications. Since the cost of comparisons
    is equivalent to the product of the length of the strings to be compared, it is
    sometimes impractical to apply it to compare large texts.
  prefs: []
  type: TYPE_NORMAL
- en: Processing two short stories and extracting the common vocabulary between two
    of them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe is supposed to give you an idea of how to handle a typical text
    analytics problem when you come across it. We will be using multiple preprocessing
    techniques in the process of getting to our outcome. The recipe will end with
    an important preprocessing task and not a real application of text analysis. We
    will be using a couple of short stories from [http://www.english-for-students.com/](http://www.english-for-students.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be removing all special characters, splitting words, doing case folds,
    and some set and list operations in this recipe. We won’t be using any special
    libraries, just Python programming tricks.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a file named `lemmatizer.py` and create a couple of long strings with
    short stories or any news articles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: There we have two short stories from the aforementioned website!
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will remove some of the special characters from the texts. We are
    removing all newlines (`''\n''`), commas, full stops, exclamations, question marks,
    and so on. At the end, we convert the entire string to lowercase with the `casefold()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will split the texts into words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `split` on the `""` character, we split and get the list of words from
    `story1` and `story2.` Let''s see the output after this step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, all the special characters are gone and a list of words is created.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's create a vocabulary out of this list of words. A vocabulary is a set
    of words. No repeats!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling the Python internal `set()` function on the list will deduplicate the
    list and convert it into a set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Here are the deduplicated sets, the vocabularies of both the stories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the final step. Produce the common vocabulary between these two stories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Python allows the set operation `&` (AND), which we are using to find the set
    of common entries between these two vocabulary sets. Let''s see the output of
    the final step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: And there it is, the end-goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So here, we saw how we can go from a couple of narratives to the common vocabulary
    between them. We didn’t use any fancy libraries, nor did we perform any complex
    operations. Yet we built a base from which we can take this bag-of-words forward
    and do many things with it.
  prefs: []
  type: TYPE_NORMAL
- en: From here on, we can think of many different applications, such as text similarity,
    search engine tagging, text summarization, and many more.
  prefs: []
  type: TYPE_NORMAL
