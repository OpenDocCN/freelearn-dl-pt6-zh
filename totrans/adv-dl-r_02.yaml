- en: Revisiting Deep Learning Architecture and Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is part of a broader machine learning and artificial intelligence
    field that uses artificial neural networks. One of the main advantages of deep
    learning methods is that they help to capture complex relationships and patterns
    contained in data. When the relationships and patterns are not very complex, traditional
    machine learning methods may work well. However, with the availability of technologies
    that help to generate and process more and more unstructured data, such as images,
    text, and videos, deep learning methods have become increasingly popular as they
    are almost a default choice to deal with such data. Computer vision and **natural
    language processing** (**NLP**) are two areas that are seeing interesting applications
    in a wide variety of fields, such as driverless cars, language translation, computer
    games, and even creating new artwork.
  prefs: []
  type: TYPE_NORMAL
- en: Within the deep learning toolkit, we now have an increasing array of neural
    network techniques that can be applied to a specific type of task. For example,
    when developing image classification models, a special type of deep network called
    a **convolutional neural network** (**CNN**) has proved to be effective in capturing
    unique patterns that exist in image-related data. Similarly, another popular deep
    learning network called **recurrent neural networks** (**RNNs**) and its variants
    have been found useful in dealing with data involving sequences of words or integers.
    Another popular and interesting deep learning network called a **generative adversarial
    network** (**GAN**) has the capability to generate new images, speech, music,
    or artwork.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will use these and other popular deep learning networks using
    R software. Each chapter presents a complete example that has been specifically
    developed to run on a regular laptop or computer. The main idea is to avoid getting
    bogged down by a huge amount of data needing advanced computing resources at the
    first stage of applying deep learning methods. You will be able to go over all
    the steps using the illustrated examples in this book. The examples used also
    include the best practices for each topic, and you will find them useful. You
    will also find a hands-on and applied approach helpful in quickly seeing the big
    picture when trying to replicate these deep learning methods when faced with a
    new problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter provides an overview of the deep learning methods with R that
    are covered in this book. We will go over the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning with R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process of developing a deep network model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular deep learning techniques with R and RStudio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning with R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start by looking at the popularity of deep learning networks and also
    take a look at a version of some of the important R packages used in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning trend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning techniques make use of neural network-based models and have seen
    increasing interest in the last few years.A Google trends website for the search
    term **deep learning** provides the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e00aa45a-06c8-4f2b-8d54-bb32d931d475.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding plot has 100 as the peak popularity of a search term, and other
    numbers are relative to this highest point. It can be observed that the interest
    in the term **deep learning** has gradually increased in popularity since around
    2014\. For the last two years, it has enjoyed peak popularity. One of the reasons
    for the popularity of deep learning networks is the availability of the free and
    open source libraries, TensorFlow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Versions of key R packages used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this book, we will use the Keras R package that uses TensorFlow as a backend
    for building deep learning networks. An output from a typical R session, used
    for the examples illustrated in this book, providing various version-related information,
    is provided in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As seen previously, for this book we have used the 3.6 version of R that was
    released in April 2019\. The nickname for this R version is Planting of a Tree.
    The version used for the Keras package is 2.2.4.1\. In addition, all the application
    examples illustrated in the book have been run on a Mac computer with 8 GB of
    RAM. The main reason for using this specification is that it will allow a reader
    to go through all the examples without needing advanced computing resources to
    get started with any deep learning network covered in the book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will go over the process of developing a deep network
    model that is broken down into five general steps.
  prefs: []
  type: TYPE_NORMAL
- en: Process of developing a deep network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Developing a deep learning network model can be broken down into five key steps
    shown in the following flowchart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5608bd8c-32f4-4d05-ad3d-42c2c0442a93.png)'
  prefs: []
  type: TYPE_IMG
- en: Each step mentioned in the preceding flowchart can have varying requirements
    based on the type of data used, the type of deep learning network being developed,
    and also the main objective of developing a model. We will go over each step to
    develop a general idea about what is involved.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data for a deep network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing deep learning neural network models requires the variables to have
    a certain format. Independent variables may come with a varying scale, with some
    variable values in decimals and some other variables in thousands. Using such
    varying scales of variables is not very efficient when training a network. Before
    developing deep learning networks, we make changes such that the variables have
    similar scales. The process used for achieving this is called **normalization**.
  prefs: []
  type: TYPE_NORMAL
- en: Two commonly used methods for normalization are z-score normalization and min-max
    normalization. In z-score normalization, we subtract the mean from each value
    and divide it by the standard deviation. This transformation results in values
    that lie between -3 and +3 with a mean of 0 and a standard deviation of 1\. For
    a min-max normalization, we subtract the minimum value from each data point, and
    then divide it by the range. This transformation converts data to having values
    between zero and one.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, see the following plots, where we have obtained 10,000 data
    points randomly from a normal distribution with a mean of 35 and a standard deviation
    of 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9274ac87-fd1c-4e1a-8578-e5f9b3fbbe66.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding plots, we can observe that after z-score normalization, the
    data points mostly lie between -3 and +3\. Similarly, after min-max normalization,
    the range of values changes to data points between 0 and 1\. However, the overall
    pattern seen in the original data is retained after both types of normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Another important step in preparing data when using a categorical response variable
    is to carry out one-hot encoding. One-hot encoding converts a categorical variable
    to a new binary format that has values containing either 0 or 1\. This is achieved
    very easily by using the `to_categorical()` function available in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically data processing steps for unstructured data, such as image or text,
    are more involved compared with a situation where we are dealing with structured
    data. In addition, the nature of data preparation steps can vary from one type
    of data to another. For example, the way we prepare image data for developing
    a deep learning classification model is likely to be very different from the way
    we prepare text data for developing a movie review sentiment classification model.
    However, one important thing to note is that before we can develop deep learning
    models from unstructured data, they need to be first converted into a structured
    format. An example of converting unstructured image data into a structured format
    is shown in the following screenshot, using a picture of the handwritten digit
    *five*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95f65454-c5c4-4767-99a4-b9634f5c62aa.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be observed from the preceding screenshot, when we read an image file
    containing a black and white handwritten digit *five* with 28 x 28 dimensions
    in R, it gets converted to numbers in rows and columns, giving it a structured
    format. The right-hand side of the screenshot shows data with 28 rows and 28 columns.
    The numbers in the body of the table are pixel values that range from 0 to 255,
    where a value of zero represents the black color and 255 represents the white
    color in the picture. When developing deep learning models, we make use of some
    forms of such structured data that are derived from image data.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data for developing the model is prepared in the required format, we
    can then develop the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a deep learning model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Developing the architecture of a model involves defining various items, such
    as the type and number of layers for the network, the type of activation function,
    the number of units or neurons to use in the network, and also providing the data-related
    input/output values. An example of specifying a simple sequential model architecture
    using Keras in R is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that a sequential model allows us to develop models layer by layer. As
    seen from the preceding code, two layers of densely connected networks have been
    added as a part of the sequential model. Two important decisions while choosing
    a model architecture involve the number and type of layers and the type of activation
    function for a layer. The number and type of layers to use is guided by the nature
    and complexity of the data. For a fully connected network (also known as a multilayer
    perceptron), we can use a dense layer with the help of the `layer_dense` function
    available in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, when working with image data, we are likely to use convolutional
    layers in the network, using the `layer_conv_2d` function. We will discuss more
    details about specific model architectures with examples in each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different types of activation functions that are used in deep learning
    networks. A rectified linear unit, or `relu`, is a popular activation function
    used in hidden layers, and it uses a very simple calculation. If the input is
    negative, it returns a value of zero and, for everything else, there is no change
    to the original value. As an example, let''s look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates 10,000 random numbers from a normal distribution
    with a mean of two and a standard deviation of 10, and stores the results in `x`.
    And then negative values are changed to zero and stored in y. A histogram of x
    and a scatter plot for x and y are given in the following graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19566188-4019-4952-bcdc-d7ac1db23736.png)'
  prefs: []
  type: TYPE_IMG
- en: It can be observed from the preceding histogram that x has values that are both
    positive and negative. The scatter plot, based on the original x values and the modified
    y value that is obtained after converting negative values to zero, visualizes
    the impact of the `relu` activation function. In the scatter plot, the data points
    to the left of x = 0 are flat and have a zero slope. The data points to the right
    of x = 0 have a perfect linear pattern with a slope of 1.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main advantages of using the `relu` activation function is its simple
    calculation. For developing deep learning network models, this becomes an important
    factor as it helps to reduce computational cost. For many deep learning networks,
    a rectified linear unit is used as the default activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another popular activation function used for developing deep networks is `softmax`,
    which is usually used in the outer layer of the network. Let''s look at the following
    code to understand it better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we have taken a random sample of 1,000 values from a
    uniform distribution that lies between 1 and 5\. To use the `softmax` function,
    we can divide the exponential of each input value x by the sum of the exponential
    values of x. The resulting histogram, based on the x values, and the scatter plot
    of x and y values are shown in the following graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6669b6e-51a5-4f48-be04-36981b9cecd1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can observe that the preceding histogram provides an approximate uniform
    pattern for the x values. The impact of the `softmax` function can be seen from
    the scatter plot where the output values now lie between 0 and 1\. This conversion
    is very useful for interpreting the results in terms of probabilities as the values
    now are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Lie between 0 and 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total of these probabilities is 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This aspect of the `softmax` activation function, where results can be interpreted
    in terms of probabilities, makes it a popular choice when developing deep learning
    classification models. It works well whether we use it for image classification
    or text classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these two activation functions, we also make use of others that may
    be more suitable for a specific deep leaning model.
  prefs: []
  type: TYPE_NORMAL
- en: Once a model architecture to be used is specified, the next step is to compile
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Compiling a model typically involves specifying the loss function, choosing
    an optimizer, and specifying the metrics to be used. These choices, however, depend
    on the type of problem that is being addressed. The following code is an example
    of R for compiling a deep learning binary classification model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding loss function specified is `binary_crossentropy`, which is used
    when the response variable has two classes. Binary cross-entropy can be calculated
    using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46d4e0aa-51e4-44b7-b6b3-fe2a37dcd8c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding formula, y represents the actual class and `yhat` represents
    the prediction probability. Let''s consider two examples using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As seen in `Example-1`, there are a total of six cases represented by y where
    the first three cases indicate the actual class to be 0, and the next three cases
    have the actual class as 1\. The prediction probabilities captured by `yhat` is
    the probability that a case belongs to category 1\. In `Example-1`, the `yhat`
    values correctly classify all six cases, and the average of all loss values is
    about 0.228\. In `Example-2`, the `yhat` values correctly classify only four cases,
    and the average of all loss values now increases to about 0.762\. The binary cross-entropy
    loss function in this way helps to assess the classification performance of a
    model. The lower the loss value is, the better the classification performance,
    and the higher the loss value is, the worse the classification performance of
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: There are various other loss functions that are used based on the type of problem
    for which the deep learning network is being developed. For classification models
    where the response variables have more than two classes, we make use of the `categorical_crossentropy`
    loss function. For regression problems with numeric response variables, the mean
    square error (`mse`) may be an appropriate loss function.
  prefs: []
  type: TYPE_NORMAL
- en: When specifying an optimizer to be used by the model, `adam` is a popular choice
    for deep learning networks, giving good results in a wide variety of situations.
    Other commonly used optimizers include `rmsprop` and `adagrad`. When a deep learning
    network is being trained, the parameters of the network are modified based on
    feedback obtained from the loss function. How this modification of parameters
    takes place is based on which optimizer is used. The choice of a suitable optimizer
    is therefore important in arriving at a suitable model.
  prefs: []
  type: TYPE_NORMAL
- en: When compiling the model, we also specify a suitable metric that will be used
    for monitoring the training process. For classification problems, `accuracy` is
    a one of the most commonly used metrics. For regression problems, the mean absolute
    error is a commonly specified metric.
  prefs: []
  type: TYPE_NORMAL
- en: Once we compile a model, we are ready to fit it.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fitting or training of the model is carried out with the help of data. An example
    of a code used for fitting a classification model is provided as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, fitting a model includes `training`, which is the data
    on independent variables, and `trainLabels`, which contain labels for the response
    variable. The number of epochs is specified to indicate the number of iterations
    of all samples in the training data that will be used during the training process.
    Batch size refers to the number of samples from the training data to be used,
    after which the model parameters will be updated. In addition, we also specify
    any validation split, where a 0.2 or 20% split means that the last 20% of samples
    from the training data will be kept separate from the training process to assess
    the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: When fitting a model, different layers in the network have a random initialization
    of weights. Due to this random initialization of network weights, if we fit a
    model again with the same data, same architecture, and same settings, we will
    get slightly different results. This will occur not only in a different session
    of R, but also in the same session when a model is trained again.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many situations where getting repeatable results is important. As
    an example, while publishing a deep learning-related article in a peer-reviewed
    international journal, you may need to generate more plots from the same model
    based on reviewer feedback. Another situation could be where a team working on
    the same project may like to share a model and also results with other members
    of the team. The easiest way to obtain the same results from the model is to save
    and then reload the model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We can save the model by specifying `filepath` and then reload when required.
    Saving a model allows us to obtain repeatable results when we use the model again.
    It also allows us to have a way to share the same model with others who can obtain
    exactly the same results, as well as helping in situations where each run takes
    a lot of time. Saving and reloading the model allows you to resume the training
    process when you train the model again.
  prefs: []
  type: TYPE_NORMAL
- en: Once a model is fit, its performance can be assessed using both training and
    testing data.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing the model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Assessing the performance of a deep learning classification model requires
    developing a confusion matrix that summarizes predictions for actual and predicted
    classes. Consider an example where a classification model is developed to classify
    graduate school applicants in one of two categories where class 0 refers to  applications
    that have not been accepted, and class 1 refers to accepted applications. An example
    of a confusion matrix for this situation explaining the key concepts is provided
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/884c53eb-52d9-44bd-acef-15d06693329f.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding confusion matrix, there are 208 applicants who were actually
    not accepted and the model also correctly predicts that they should not be accepted.
    This cell in the confusion matrix is also called the **true negative**. Similarly,
    there are 29 applicants who were actually accepted and the model also correctly
    predicts that they should be accepted. This cell in the confusion matrix is called
    the **true positive**. We also have cells with numbers indicating the incorrect
    classification of applicants by the model. There are 15 applicants who were actually
    not accepted, but the model incorrectly predicts that they should be accepted
    and this cell is called a **false negative**.
  prefs: []
  type: TYPE_NORMAL
- en: Another name for making an error when incorrectly classifying category 0 as
    belonging to category 1 is a type-1 error. Finally, there are 73 applicants that
    were actually accepted but the model incorrectly predicts them to belong to the
    not-accepted category, and this cell is called a **false positive**. Aanother
    name for such an incorrect classification is a type-2 error.
  prefs: []
  type: TYPE_NORMAL
- en: From the confusion matrix, we can calculate the accuracy of the classification
    performance by adding numbers to the diagonal and dividing the numbers by the
    total. So, the accuracy based on the preceding matrix is (208+29)/(208+29+73+15),
    or 72.92%. Apart from the accuracy, we can also find out the model performance
    in correctly classifying each category. We can calculate the accuracy of correctly
    classifying category 1, also called sensitivity, as 29/(29+73), or 28.4%. Similarly,
    we can calculate the accuracy of correctly classifying category 0, also called
    specificity, as 208/(208+15), or 93.3%.
  prefs: []
  type: TYPE_NORMAL
- en: Note, that the confusion matrix can be used when developing a classification
    model. However, other situations may call for other suitable ways of assessing
    the deep learning network.
  prefs: []
  type: TYPE_NORMAL
- en: We can now briefly go over the deep learning techniques covered in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning techniques with R and RStudio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The term **deep** in deep learning refers to a neural network model having
    several layers, and the learning takes place with the help of data. And based
    on the type of data used, deep learning may be categorized into two major categories,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b572444-8411-43e4-be7e-7afff4434d3f.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram, the type of data used for developing a deep
    neural network model can be of a structured or unstructured type. In [Chapter
    2](c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml), *Deep Neural Networks for Multi-Class
    Classification*, we illustrate the use of a deep learning network for classification
    problems using structured data where the response variable is of the categorical
    type. In [Chapter 3](07c9aa4a-1c93-490a-bfcd-7c4bcde639d5.xhtml), *Deep Neural
    Networks for Regression*, we illustrate the use of a deep learning network for
    regression problems using structured data where the response is a continuous type
    of variable. Chapters 4 to 12 illustrate the use of deep learning networks for
    mainly two types of unstructured data that involve images and text. In chapters
    4 to 8, we provide application examples of some popular deep learning networks
    using image data, which is regarded as an unstructured type of data. Finally,
    in chapters 9 to 12, we cover some popular deep learning networks that are useful
    with text data, which is another major category within unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's briefly go over the examples and techniques covered in chapters 2
    to 12.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many problems where the main objective is to develop a classification
    model that uses data to classify observations into two or more categories. For
    example, a patient may be classified as normal, suspect, or pathological based
    on the data on several variables. The deep learning network in this case will
    use data on several patients where the outcome is already available, and it will
    learn to classify a patient into one of the three categories.
  prefs: []
  type: TYPE_NORMAL
- en: Another example of a classification problem could be where students send applications
    to a graduate school. An application from a student may be accepted or rejected
    based on variables such as GPA, GRE, and ranking of the school during their undergraduate
    degree. Another interesting example could be where student-related data is used
    for developing a model that helps to classify first-year students into those that
    are likely to stay with the current school and those who are likely to transfer
    to another school. A similar model can be developed to classify customers who
    are likely to stay with a business or switch to a competitor.
  prefs: []
  type: TYPE_NORMAL
- en: One of the challenges involved while developing a classification model is that
    of class imbalance. For example, when dealing with medical data, the number of
    patients classified as normal may be much larger than the number of patients who
    are classified as pathological. Similarly, when applying to a graduate program
    at one of the top universities, it is very likely that the data contains a significantly
    higher number of cases where an applicant is not accepted. Deep network models
    are useful in addressing such concerns easily. The Keras library used in this
    book provides a user-friendly interface not only to address such issues easily, but
    also to help in obtaining suitable classification models with the help of fast
    experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml), *Deep Neural Networks
    for Multi-Class Classification*, we provide an illustration of a multi-class deep
    learning classification model using R.
  prefs: []
  type: TYPE_NORMAL
- en: Regression problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Structured data involving numeric response variables is classified as a regression
    problem. For example, the price of a house in a city may depend on variables such
    as the age of the house, the crime rate in the city, the number of rooms, and
    the property tax rate. Although statistical methods, such as multiple linear regression
    and elastic net regression, can also be useful for these situations, deep learning
    networks offer certain advantages. One of the main advantages of using neural
    networks in general is that they can capture non-linearity. Unlike statistical
    methods that require certain assumptions to be met before we can use them, neural
    network-based models are more flexible to use and do not require many assumptions
    to be fulfilled.
  prefs: []
  type: TYPE_NORMAL
- en: Many applications involving regression problems also call for identifying variables
    or features that have a significant impact on the response variable. However with
    deep learning networks, such feature engineering is inbuilt, and it doesn't call
    for any extra effort in extracting important features. One thing to note regarding
    deep learning networks is that the larger the dataset being used, the more effective
    the resulting prediction model will be. In [Chapter 3](07c9aa4a-1c93-490a-bfcd-7c4bcde639d5.xhtml),
    *Deep Neural Networks for Regression*, we provide an illustration of a deep learning
    regression model using R.
  prefs: []
  type: TYPE_NORMAL
- en: Image classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image data is classified as an unstructured type of data. One of the popular
    applications of deep learning networks involves developing image classification
    and recognition models. Image classification has various applications, such as
    face recognition on smartphones or on social media networks, classification of
    medical image data, classification of handwritten digits, and self-driving cars.
    Note that it is not possible to develop a classification model directly from unstructured
    data. The unstructured data needs to be first converted into a structured form
    before deep learning networks can be developed. For example, a black and white
    image may have dimensions of 21 x 21 and thus contain data on 441 (21 x 21) pixels.
    Once we convert an image into numbers representing all the pixels, it becomes
    feasible to develop image classification models. Although humans can classify
    a type of dress, a person, or certain object very easily, even when the images
    may have different sizes or orientation, training a computer to do so is a challenging
    task.
  prefs: []
  type: TYPE_NORMAL
- en: The Keras library provides several easy-to-use features for processing image
    data that helps in developing deep learning image classification networks. The
    effectiveness of having deep networks or neural networks with many layers especially
    comes to the fore when it comes to image recognition and classification problems.
    In [Chapter 4](356e6d56-329c-433e-8b3e-969453363ee9.xhtml), *Image Classification
    and Recognition*, we provide an illustration of applying a deep learning image
    classification model using R.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image classification tasks become challenging when the number of categories
    increases and images within a category show significant variability. Such situations
    also require a larger number of samples so that features inherent in each category
    can be captured more accurately by the classification model. For example, a fashion
    retailer may have a large variety of fashion items and may be interested in developing
    a classification model from the image data of such fashion items. A special type
    of deep network, called a **convolutional neural network** (**CNN**), has proven
    to be highly effective in situations that call for large scale image classification
    and recognition tasks. CNNs are the most popular networks for such applications
    and are regarded as the gold standard for large-scale image classification problems.
    These networks are capable of capturing various minute details in an image with
    the help of different types of layers in the network. In [Chapter 5](7285aaf1-8ca5-4f1d-95d8-057ce1fbf5f9.xhtml),
    *Image Classification Using Convolutional Neural Networks*, we provide an illustration
    of applying a CNN to image classification using R.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning methods that involve classification and prediction models using
    data that has a response or a dependent variable are part of supervised deep learning
    methods. When working with structured or unstructured data, there are situations
    where the response variable is either not available or not used. Applications
    of deep learning networks that do not use a response variable are classified as
    unsupervised deep learning methods. For example, an application of deep learning
    may involve image data from which we want to extract important features in order
    to achieve dimension reduction. Another example involves handwritten images that
    contain unwanted noise and a deep network is used for denoising the images. In
    such situations, autoencoder networks have been found to be very useful for performing
    unsupervised deep learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder neural networks make use of an encoder and decoder network. When
    the image data is passed through an encoder and the resulting dimension is lower
    than that of the original image, the network is forced to extract only the most
    important features from the input data. And then the decoder part of the network
    reconstructs the original data from whatever is available from the output of the
    encoder. In [Chapter 6](489413e8-85df-4912-b59a-bd119d93c967.xhtml), *Applying
    Autoencoder Neural Networks Using Keras*, we provide an illustration of applying
    an autoencoder neural network for dimension reduction, de-noising, and image correction
    when working with image data using R.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing deep learning classification models when the image data has several
    categories is a challenging task. It becomes even more challenging when the number
    of images available is limited. In such situations, it may be possible to take
    advantage of an existing model that has been developed with the help of a much
    larger dataset and reuse the patterns it has learned by customizing it for another
    classification task. This reuse of a pretrained deep network model for a new classification
    task is known as transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: The Keras library provides various pretrained models for image classification
    tasks that are trained using over a million images, and that capture reusable
    features that can be applied to similar but new data. Transferring what a pretrained
    model has learned from a large number of samples to a model that is being built
    with a much smaller sample size helps to save computational resources. In addition,
    use of the transfer learning approach can help to outperform a model that is built
    from scratch using a smaller dataset. In [Chapter 7](c316ef95-6026-4e25-9dd4-7e3a191721d0.xhtml), *Image
    Classification for Small Data Using Transfer Learning*, we cover transfer learning
    and illustrate the utilization of a pre trained deep learning image classification
    model using R.
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An article in The Verge (Reference: [https://www.theverge.com/2018/10/25/18023266/ai-art-portrait-christies-obvious-sold](https://www.theverge.com/2018/10/25/18023266/ai-art-portrait-christies-obvious-sold))
    reported that an artwork named *Portrait of Edmond Belamy* created using an artificial
    intelligence algorithm was sold for $432,500\. This artwork was estimated to sell
    for about $7,000 to $10,000\. The deep learning algorithm that was used to create
    this artwork is called a **generative adversarial network** (**GAN**). The unique
    attribute of generative adversarial networks is that two deep networks are made
    to compete against each other to generate something meaningful. The two networks
    that compete against each other and try to outsmart one another are called generator
    and discriminator networks.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a situation where we want to generate new handwritten images of the
    digit *five*. A generative adversarial network in this case would involve a generator
    network that creates fake images of the handwritten digit *five* from simply random
    noise and sends it to a discriminator network. The fake images are mixed with
    genuine images and the discriminator network, which is trained to differentiate
    between real and fake images of the handwritten digit *five*, will try its best
    to successfully differentiate between real and fake images. These two networks
    are made to compete against each other until the generator network starts making
    realistic-looking fake images that the discriminator network finds increasingly
    difficult to differentiate between. In addition to image data, application of
    generative adversarial networks can be extended to generate new text or even new
    music. We will illustrate an application of a generative adversarial network to
    generate new images in [Chapter 8](7031c1cb-e20d-4e86-8667-393d0cceddca.xhtml), *Creating
    New Images Using Generative Adversarial Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: Deep network for text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text data has certain unique characteristics that makes it a very different
    type of unstructured data compared to image data. As mentioned earlier, unstructured
    data requires extra processing steps to arrive at a structured format that can
    be used for developing a deep learning classification network. One of the applications
    of deep learning with text data involves developing a deep neural network sentiment
    classification model.
  prefs: []
  type: TYPE_NORMAL
- en: To develop a sentiment classification model, labels capturing sentiment related
    to the text data are needed. For example, we may use text data on movie reviews
    and a related sentiment label (positive review or negative review) to develop
    a model that can be used to automate the process. Another example could be the
    development of a sentiment classification model using text data on tweets. Such
    a model can be useful in comparing sentiments contained in thousands of tweets
    or and after an important event. Examples of such events where sentiment classification
    models can be useful include sentiments contained in tweets before and after the
    release of a new smartphone by a company, and sentiments contained in tweets before
    and after the performance of a presidential candidate in a live debate. A deep
    network for a sentiment classification model using text data is illustrated in
    [Chapter 9](491ea3a8-47e9-48b4-8553-7387528c8594.xhtml), *Deep Networks for Text
    Classification*.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A unique characteristic of text data is the fact that the placement of words
    in a text sequence has some meaning. **Recurrent neural networks** (**RNNs**)
    are well suited to work with data involving such sequences. Recurrent networks
    allow output from the previous step to be passed as input to the following step.
    This process of feeding prior information at a step allows recurrent networks
    to have memory, which is very useful for dealing with data involving sequences.
    The name **recurrent** in RNN also comes from the fact that the output at a step
    depends on information from the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs can be used to develop a sentiment classification model where the text
    data could be movie reviews, tweets, product reviews, and so on. Developing such
    a sentiment classification model will also need the labels that will be used for
    training the network. We go over steps for developing a recurrent neural network
    model for sentiment classification using R in [Chapter 10](acfbe36f-dae6-40ad-96b5-0b0e87ce0f8d.xhtml),
    *Text Classification Using Recurrent Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: Long short-term memory network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Long short-term memory** (**LSTM**) networks are a special type of recurrent
    neural network. LSTM networks are useful when data regarding the sequence of words
    or integers has long-term dependencies. For example, two words that are important
    for correctly classifying sentiment contained in a movie review may be separated
    by many words in a long sentence. A sentiment classification model using a regular
    RNN will have difficulty capturing such long-term dependency between words. A
    regular RNN is useful when dependency between words or integers in a sequence
    is immediate or when two important words are next to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from sentiment classification, the application of LSTM networks can also
    be useful for speech recognition, language translation, anomaly detection, time
    series forecasting, answering questions, and so on. An application of an LSTM
    network for movie review sentiment classification is illustrated in [Chapter 11](da73d1c6-4377-4a8f-9bee-01262444f136.xhtml),
    *Text Classification Using Long Short-Term Memory Network*.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional recurrent networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**) are useful for capturing high-level
    local features from the image or text data, and LSTM networks can capture long-term
    dependencies in the data involving sequences. When we use both CNNs and a recurrent
    network in the same model architecture, it is called a **convolutional recurrent
    neural network** (**CRNN**). As an example, if we consider data on articles and
    their authors, we may be interested in developing an author classification model
    where we can train a network to take text data containing an article as input
    and then help to make a prediction in terms of probability regarding the author.
    For this, we can first use a one-dimensional convolutional layer to extract important
    features from the data. These extracted features can then be passed to the LSTM
    recurrent layer to obtain the hidden long-term dependencies, which, in turn, are
    passed to a fully connected dense layer. This dense layer can then obtain the
    probability of correct authorship. CRNNs can also be applied to problems related
    to natural language processing, speech, and video. In [Chapter 12](be0c6dfc-045c-4698-b36d-74eca5e0a629.xhtml),
    *Text Classification Using Convolutional Recurrent Networks*, we illustrate the
    use of CRNNs for developing a model that can classify an author, based on articles
    written by them.'
  prefs: []
  type: TYPE_NORMAL
- en: Tips, tricks, and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we provide an illustration of applying several popular deep learning
    methods using R. When working on more complex problems requiring the application
    of deep learning networks, the use of certain supporting tools may sometimes be
    very helpful. TensorFlow provides such a tool; it is called **TensorBoard** and
    is useful for visualizing deep network training performance, especially in situations
    that call for experimentation. Similarly, there is a package called **Local Interpretable
    Model-Agnostic Explanations** (**LIME**) that can help with visualization and
    interpretation of specific predictions. We also get many outputs, such as summaries
    and plots, when developing a deep network model. There is a package called **tfruns**
    that can help to keep everything in one place for easy reference. There is a callback
    feature in the Keras package that helps with stopping a network training at a
    suitable time. We will discuss all these tips, tricks, and best practices in [Chapter
    13](af4eb94d-f4fb-41df-9df8-797e4771484d.xhtml), *Tips, Tricks, and the Road Ahead*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning methods that make use of artificial neural networks have been
    increasing in popularity in recent years. A number of areas of application involving
    deep learning methods include driverless cars, image classification, natural language
    processing, and new image generation. We started this first chapter by looking
    at the popularity of the deep learning term as reported from a Google trend website.
    We described a general five-step process for applying deep learning methods and
    developed some broad ideas about details within each step. We then briefly looked
    at deep learning techniques covered in each chapter and situations in which they
    are applied, along with some best practices.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we get started with an application example and illustrate
    steps for developing a deep network model for multi-class classification problems.
  prefs: []
  type: TYPE_NORMAL
