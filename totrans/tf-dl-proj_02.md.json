["```py\n{1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella', 31: 'handbag', 32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove', 41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle', 46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon', 51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut', 61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed', 67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse', 75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven', 80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book', 85: 'clock', 86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}\n```", "```py\nconda create -n TensorFlow_api python=3.5 numpy pillow\nactivate TensorFlow_api\n```", "```py\npip install TensorFlow-gpu\nconda install -c menpo opencv\nconda install -c conda-forge imageio\npip install tqdm, moviepy\n```", "```py\nmkdir api_project\ncd api_project\ngit init\ngit remote add -f origin https://github.com/tensorflow/models.git\n```", "```py\ngit config core.sparseCheckout true\necho \"research/object_detection/*\" >> .git/info/sparse-checkout\ngit pull origin master\n```", "```py\nprotoc-3.4.0-win32/bin/protoc.exe object_detection/protos/*.proto --python_out=.\n```", "```py\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport six.moves.urllib as urllib\nimport tarfile\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom time import gmtime, strftime\nimport json\nimport cv2\n```", "```py\ntry:\n    from moviepy.editor import VideoFileClip\nexcept:\n    # If FFmpeg (https://www.ffmpeg.org/) is not found \n    # on the computer, it will be downloaded from Internet \n    # (an Internet connect is needed)\n    import imageio\n    imageio.plugins.ffmpeg.download()\n    from moviepy.editor import VideoFileClip\n```", "```py\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as vis_util\n```", "```py\nclass DetectionObj(object):\n    \"\"\"\n    DetectionObj is a class suitable to leverage \n    Google Tensorflow detection API for image annotation from   \n    different sources: files, images acquired by own's webcam,\n    videos.\n    \"\"\"\n\n    def __init__(self, model='ssd_mobilenet_v1_coco_11_06_2017'):\n        \"\"\"\n        The instructions to be run when the class is instantiated\n        \"\"\"\n\n        # Path where the Python script is being run\n        self.CURRENT_PATH = os.getcwd()\n\n        # Path where to save the annotations (it can be modified)\n        self.TARGET_PATH = self.CURRENT_PATH\n\n        # Selection of pre-trained detection models\n        # from the Tensorflow Model Zoo\n        self.MODELS = [\"ssd_mobilenet_v1_coco_11_06_2017\",\n                       \"ssd_inception_v2_coco_11_06_2017\",\n                       \"rfcn_resnet101_coco_11_06_2017\",\n                       \"faster_rcnn_resnet101_coco_11_06_2017\",\n                       \"faster_rcnn_inception_resnet_v2_atrous_\\\n                        coco_11_06_2017\"]\n\n        # Setting a threshold for detecting an object by the models\n        self.THRESHOLD = 0.25 # Most used threshold in practice\n\n        # Checking if the desired pre-trained detection model is available\n        if model in self.MODELS:\n            self.MODEL_NAME = model\n        else:\n            # Otherwise revert to a default model\n            print(\"Model not available, reverted to default\", self.MODELS[0])\n            self.MODEL_NAME = self.MODELS[0]\n\n        # The file name of the Tensorflow frozen model\n        self.CKPT_FILE = os.path.join(self.CURRENT_PATH, 'object_detection',\n                                      self.MODEL_NAME,  \n                                      'frozen_inference_graph.pb')\n\n        # Attempting loading the detection model, \n        # if not available on disk, it will be \n        # downloaded from Internet\n        # (an Internet connection is required)\n        try:\n            self.DETECTION_GRAPH = self.load_frozen_model()\n        except:\n            print ('Couldn\\'t find', self.MODEL_NAME)\n            self.download_frozen_model()\n            self.DETECTION_GRAPH = self.load_frozen_model()\n\n        # Loading the labels of the classes recognized by the detection model\n        self.NUM_CLASSES = 90\n        path_to_labels = os.path.join(self.CURRENT_PATH,\n                                    'object_detection', 'data',\n                                       'mscoco_label_map.pbtxt')\n        label_mapping = \\ \n                    label_map_util.load_labelmap(path_to_labels)\n        extracted_categories = \\\n                 label_map_util.convert_label_map_to_categories(\n                 label_mapping, max_num_classes=self.NUM_CLASSES,                                              \n                                           use_display_name=True)\n        self.LABELS = {item['id']: item['name'] \\\n                       for item in extracted_categories}\n        self.CATEGORY_INDEX = label_map_util.create_category_index\\\n(extracted_categories)\n\n        # Starting the tensorflow session\n        self.TF_SESSION = tf.Session(graph=self.DETECTION_GRAPH)\n```", "```py\ndef load_frozen_model(self):\n    \"\"\"\n    Loading frozen detection model in ckpt \n    file from disk to memory\n    \"\"\"\n\n    detection_graph = tf.Graph()\n    with detection_graph.as_default():\n        od_graph_def = tf.GraphDef()\n        with tf.gfile.GFile(self.CKPT_FILE, 'rb') as fid:\n             serialized_graph = fid.read()\n             od_graph_def.ParseFromString(serialized_graph)\n             tf.import_graph_def(od_graph_def, name='')\n\n    return detection_graph\n```", "```py\ndef download_frozen_model(self):\n    \"\"\"\n    Downloading frozen detection model from Internet\n    when not available on disk\n    \"\"\"\n    def my_hook(t):\n        \"\"\"\n        Wrapping tqdm instance in order to monitor URLopener \n        \"\"\"\n        last_b = [0]\n\n        def inner(b=1, bsize=1, tsize=None):\n            if tsize is not None:\n                t.total = tsize\n            t.update((b - last_b[0]) * bsize)\n            last_b[0] = b\n        return inner\n\n    # Opening the url where to find the model\n    model_filename = self.MODEL_NAME + '.tar.gz'\n    download_url = \\\n        'http://download.tensorflow.org/models/object_detection/'\n    opener = urllib.request.URLopener()\n\n    # Downloading the model with tqdm estimations of completion\n    print('Downloading ...')\n    with tqdm() as t:\n        opener.retrieve(download_url + model_filename,\n                        model_filename, reporthook=my_hook(t))\n\n    # Extracting the model from the downloaded tar file\n    print ('Extracting ...')\n    tar_file = tarfile.open(model_filename)\n    for file in tar_file.getmembers():\n            file_name = os.path.basename(file.name)\n            if 'frozen_inference_graph.pb' in file_name:\n                tar_file.extract(file,\n                     os.path.join(self.CURRENT_PATH,  \n                                  'object_detection'))\n```", "```py\n    def load_image_from_disk(self, image_path):\n\n        return Image.open(image_path)\n\n    def load_image_into_numpy_array(self, image):\n\n        try:\n            (im_width, im_height) = image.size\n            return np.array(image.getdata()).reshape(\n                (im_height, im_width, 3)).astype(np.uint8)\n        except:\n            # If the previous procedure fails, we expect the\n            # image is already a Numpy ndarray\n            return image\n```", "```py\ndef detect(self, images, annotate_on_image=True):\n        \"\"\"\n        Processing a list of images, feeding it \n        into the detection model and getting from it scores,  \n        bounding boxes and predicted classes present \n        in the images\n        \"\"\"\n        if type(images) is not list:\n            images = [images]\n        results = list()\n        for image in images:\n            # the array based representation of the image will \n            # be used later in order to prepare the resulting\n            # image with boxes and labels on it.\n            image_np = self.load_image_into_numpy_array(image)\n\n            # Expand dimensions since the model expects images\n            # to have shape: [1, None, None, 3]\n            image_np_expanded = np.expand_dims(image_np, axis=0)\n            image_tensor = \\ \n                  self.DETECTION_GRAPH.get_tensor_by_name(  \n                                               'image_tensor:0')\n\n            # Each box represents a part of the image where a \n            # particular object was detected.\n            boxes = self.DETECTION_GRAPH.get_tensor_by_name(\n                                            'detection_boxes:0')\n\n            # Each score represent how level of confidence \n            # for each of the objects. Score could be shown \n            # on the result image, together with the class label.\n            scores = self.DETECTION_GRAPH.get_tensor_by_name(\n                                           'detection_scores:0')\n            classes = self.DETECTION_GRAPH.get_tensor_by_name(\n                                          'detection_classes:0')\n            num_detections = \\\n                     self.DETECTION_GRAPH.get_tensor_by_name(\n                                             'num_detections:0')\n\n         # Actual detection happens here\n         (boxes, scores, classes, num_detections) = \\\n                     self.TF_SESSION.run(\n                     [boxes, scores, classes, num_detections],\n                     feed_dict={image_tensor: image_np_expanded})\n\n        if annotate_on_image:\n            new_image = self.detection_on_image(\n                            image_np, boxes, scores, classes)\n            results.append((new_image, boxes, \n                            scores, classes, num_detections))\n        else:\n            results.append((image_np, boxes, \n                            scores, classes, num_detections))\n        return results\n```", "```py\n    def detection_on_image(self, image_np, boxes, scores,  \n                           classes):\n        \"\"\"\n        Put detection boxes on the images over \n        the detected classes\n        \"\"\"\n        vis_util.visualize_boxes_and_labels_on_image_array(\n            image_np,\n            np.squeeze(boxes),\n            np.squeeze(classes).astype(np.int32),\n            np.squeeze(scores),\n            self.CATEGORY_INDEX,\n            use_normalized_coordinates=True,\n            line_thickness=8)\n        return image_np\n```", "```py\n    def visualize_image(self, image_np, image_size=(400, 300), \n                        latency=3, bluish_correction=True):\n\n        height, width, depth = image_np.shape\n        reshaper = height / float(image_size[0])\n        width = int(width / reshaper)\n        height = int(height / reshaper)\n        id_img = 'preview_' + str(np.sum(image_np))\n        cv2.startWindowThread()\n        cv2.namedWindow(id_img, cv2.WINDOW_NORMAL)\n        cv2.resizeWindow(id_img, width, height)\n        if bluish_correction:\n            RGB_img = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n            cv2.imshow(id_img, RGB_img)\n        else:\n            cv2.imshow(id_img, image_np)\n        cv2.waitKey(latency*1000)\n```", "```py\n\"{\"scores\": [0.9092628359794617], \"classes\": [\"dog\"], \"boxes\": [[0.025611668825149536, 0.22220897674560547, 0.9930437803268433, 0.7734537720680237]]}\"\n```", "```py\n    def serialize_annotations(self, boxes, scores, classes,                                       filename='data.json'):\n        \"\"\"\n        Saving annotations to disk, to a JSON file\n        \"\"\"\n\n        threshold = self.THRESHOLD\n        valid = [position for position, score in enumerate(\n                                              scores[0]) if score > threshold]\n        if len(valid) > 0:\n            valid_scores = scores[0][valid].tolist()\n            valid_boxes  = boxes[0][valid].tolist()\n            valid_class = [self.LABELS[int(\n                                   a_class)] for a_class in classes[0][valid]]\n            with open(filename, 'w') as outfile:\n                json_data = {'classes': valid_class,\n                    'boxes':valid_boxes, 'scores': valid_scores})\n                json.dump(json_data, outfile)\n```", "```py\n    def get_time(self):\n        \"\"\"\n        Returning a string reporting the actual date and time\n        \"\"\"\n        return strftime(\"%Y-%m-%d_%Hh%Mm%Ss\", gmtime())\n```", "```py\n    def annotate_photogram(self, photogram):\n        \"\"\"\n        Annotating a video's photogram with bounding boxes\n        over detected classes\n        \"\"\"\n        new_photogram, boxes, scores, classes, num_detections =  \n                                                      self.detect(photogram)[0]\n        return new_photogram\n```", "```py\n    def capture_webcam(self):\n        \"\"\"\n        Capturing an image from the integrated webcam\n        \"\"\"\n\n        def get_image(device):\n            \"\"\"\n            Internal function to capture a single image  \n            from the camera and return it in PIL format\n            \"\"\"\n\n            retval, im = device.read()\n            return im\n\n        # Setting the integrated webcam\n        camera_port = 0\n\n        # Number of frames to discard as the camera \n        # adjusts to the surrounding lights\n        ramp_frames = 30\n\n        # Initializing the webcam by cv2.VideoCapture\n        camera = cv2.VideoCapture(camera_port)\n\n        # Ramping the camera - all these frames will be \n        # discarded as the camera adjust to the right light levels\n        print(\"Setting the webcam\")\n        for i in range(ramp_frames):\n            _ = get_image(camera)\n\n        # Taking the snapshot\n        print(\"Now taking a snapshot ... \", end='')\n        camera_capture = get_image(camera)\n        print('Done')\n\n        # releasing the camera and making it reusable\n        del (camera)\n        return camera_capture\n```", "```py\n    def file_pipeline(self, images, visualize=True):\n        \"\"\"\n        A pipeline for processing and annotating lists of\n        images to load from disk\n        \"\"\"\n        if type(images) is not list:\n            images = [images]\n        for filename in images:\n            single_image = self.load_image_from_disk(filename)\n            for new_image, boxes, scores, classes, num_detections in  \n                                                     self.detect(single_image):\n                self.serialize_annotations(boxes, scores, classes,\n                                           filename=filename + \".json\")\n                if visualize:\n                    self.visualize_image(new_image)\n```", "```py\n    def video_pipeline(self, video, audio=False):\n        \"\"\"\n        A pipeline to process a video on disk and annotating it\n        by bounding box. The output is a new annotated video.\n        \"\"\"\n        clip = VideoFileClip(video)\n        new_video = video.split('/')\n        new_video[-1] = \"annotated_\" + new_video[-1]\n        new_video = '/'.join(new_video)\n        print(\"Saving annotated video to\", new_video)\n        video_annotation = clip.fl_image(self.annotate_photogram)\n        video_annotation.write_videofile(new_video, audio=audio)\n```", "```py\n    def webcam_pipeline(self):\n        \"\"\"\n        A pipeline to process an image acquired by the internal webcam\n        and annotate it, saving a JSON file to disk\n        \"\"\"\n        webcam_image = self.capture_webcam()\n        filename = \"webcam_\" + self.get_time()\n        saving_path = os.path.join(self.CURRENT_PATH, filename + \".jpg\")\n        cv2.imwrite(saving_path, webcam_image)\n        new_image, boxes, scores, classes, num_detections =  \n                                            self.detect(webcam_image)[0]\n        json_obj = {'classes': classes, 'boxes':boxes, 'scores':scores}\n        self.serialize_annotations(boxes, scores, classes,                                   filename=filename+\".json\")\n        self.visualize_image(new_image, bluish_correction=False)\n```", "```py\nfrom TensorFlow_detection import DetectionObj\nif __name__ == \"__main__\":\n    detection = DetectionObj(model='ssd_mobilenet_v1_coco_11_06_2017')\n    images = [\"./sample_images/intersection.jpg\",\n              \"./sample_images/busy_street.jpg\", \"./sample_images/doge.jpg\"]\n    detection.file_pipeline(images)\n```", "```py\n\"scores\": [0.9099398255348206, 0.8124723434448242, 0.7853631973266602, 0.709653913974762, 0.5999227166175842, 0.5942907929420471, 0.5858771800994873, 0.5656214952468872, 0.49047672748565674, 0.4781857430934906, 0.4467884600162506, 0.4043623208999634, 0.40048354864120483, 0.38961756229400635, 0.35605812072753906, 0.3488095998764038, 0.3194449841976166, 0.3000411093235016, 0.294520765542984, 0.2912806570529938, 0.2889115810394287, 0.2781482934951782, 0.2767323851585388, 0.2747304439544678]\n```", "```py\n\"classes\": [\"car\", \"person\", \"person\", \"person\", \"person\", \"car\", \"car\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"car\", \"car\", \"person\", \"person\", \"car\", \"car\", \"person\", \"car\", \"car\", \"car\"]\n```", "```py\nfrom TensorFlow_detection import DetectionObj\nif __name__ == \"__main__\":\n    detection = DetectionObj(model='ssd_mobilenet_v1_coco_11_06_2017')\n    detection.video_pipeline(video=\"./sample_videos/ducks.mp4\", audio=False)\n```", "```py\nfrom TensorFlow_detection import DetectionObj\nif __name__ == \"__main__\":\n    detection = DetectionObj(model='ssd_mobilenet_v1_coco_11_06_2017')\n    detection.webcam_pipeline()\n```", "```py\nfrom tensorflow_detection import DetectionObj\nfrom threading import Thread\nimport cv2\n\ndef resize(image, new_width=None, new_height=None):\n    \"\"\"\n    Resize an image based on a new width or new height\n    keeping the original ratio\n    \"\"\"\n    height, width, depth = image.shape\n    if new_width:\n        new_height = int((new_width / float(width)) * height)\n    elif new_height:\n        new_width = int((new_height / float(height)) * width)\n    else:\n        return image\n    return cv2.resize(image, (new_width, new_height), \\\n                      interpolation=cv2.INTER_AREA)\n\nclass webcamStream:\n    def __init__(self):\n        # Initialize webcam\n        self.stream = cv2.VideoCapture(0)\n        # Starting TensorFlow API with SSD Mobilenet\n        self.detection = DetectionObj(model=\\\n                        'ssd_mobilenet_v1_coco_11_06_2017')\n        # Start capturing video so the Webca, will tune itself\n        _, self.frame = self.stream.read()\n        # Set the stop flag to False\n        self.stop = False\n        #\n        Thread(target=self.refresh, args=()).start()\n\n    def refresh(self):\n        # Looping until an explicit stop is sent \n        # from outside the function\n        while True:\n            if self.stop:\n                return\n            _, self.frame = self.stream.read()\n\n    def get(self):\n        # returning the annotated image\n        return self.detection.annotate_photogram(self.frame)\n\n    def halt(self):\n        # setting the halt flag\n        self.stop = True\n\nif __name__ == \"__main__\":\n    stream = webcamStream()\n    while True:\n        # Grabbing the frame from the threaded video stream \n        # and resize it to have a maximum width of 400 pixels\n        frame = resize(stream.get(), new_width=400)\n        cv2.imshow(\"webcam\", frame)\n        # If the space bar is hit, the program will stop\n        if cv2.waitKey(1) & 0xFF == ord(\" \"):\n            # First stopping the streaming thread\n            stream.halt()\n            # Then halting the while loop\n            break\n```"]