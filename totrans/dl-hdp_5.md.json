["```py\nNd4j.MAX_SLICES_TO_PRINT = -1; \nNd4j.MAX_ELEMENTS_PER_SLICE = -1; \nNd4j.ENFORCE_NUMERICAL_STABILITY = true; \nfinal int numRows = 4; \nfinal int numColumns = 1; \nint outputNum = 10; \nint numSamples = 150; \n\n```", "```py\nint batchSize = 150; \nint iterations = 100; \nint seed = 123; \nint listenerFreq = iterations/2; \n\n```", "```py\nlog.info(\"Load data....\");\nDataSetIterator iter = new IrisDataSetIterator(batchSize, numSamples); \nDataSet iris = iter.next();\n\n```", "```py\nNeuralNetConfiguration conf = new NeuralNetConfiguration.Builder()\n.regularization(true)       \n .miniBatch(true) \n.layer(new RBM.Builder().l2(1e-1).l1(1e-3) \n    .nIn(numRows * numColumns)       \n    .nOut(outputNum)\n```", "```py\n.activation(\"relu\") \n\n```", "```py\n.weightInit(WeightInit.RELU) \n .lossFunction(LossFunctions.LossFunction.RECONSTRUCTION\n     _CROSSENTROPY.k(3) \n\n```", "```py\n.hiddenUnit(HiddenUnit.RECTIFIED).visibleUnit(VisibleUnit.GAUSSIAN) \n .updater(Updater.ADAGRAD).gradientNormalization(Gradient\n     Normalization.ClipL2PerLayer) \n    .build())       \n  .seed(seed)    \n  .iterations(iterations) \n\n```", "```py\n.learningRate(1e-3)\n  .optimizationAlgo(OptimizationAlgorithm.LBFGS)       \n  .build();       \n\nLayer model = LayerFactories.getFactory(conf.getLayer()).create(conf);       \nmodel.setListeners(new ScoreIterationListener(listenerFreq));\n\nlog.info(\"Evaluate weights....\");       \nINDArray w = model.getParam(DefaultParamInitializer.WEIGHT_KEY);       \nlog.info(\"Weights: \" + w);\n```", "```py\niris.scale(); \n\n```", "```py\nlog.info(\"Train model....\");       \nfor(int i = 0; i < 20; i++)\n  {       \n   log.info(\"Epoch \"+i+\":\");model.fit(iris.getFeatureMatrix()); }\n```", "```py\nlog.info(\"Load data....\");\nDataSetIterator iter = new MnistDataSetIterator(batchSize,numSamples,\ntrue);\n\n```", "```py\nlog.info(\"Build model....\");\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n  .seed(seed)\n  .iterations(iterations)\n  .optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT)\n  .list()\n  .layer(0, new RBM.Builder().nIn(numRows * numColumns).nOut(1000)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())\n  .layer(1, new RBM.Builder().nIn(1000).nOut(500)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())\n  .layer(2, new RBM.Builder().nIn(500).nOut(250)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())\n  .layer(3, new RBM.Builder().nIn(250).nOut(100)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())\n  .layer(4, new RBM.Builder().nIn(100).nOut(30)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())  \n  .layer(5, new RBM.Builder().nIn(30).nOut(100)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())  \n  .layer(6, new RBM.Builder().nIn(100).nOut(250)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())\n  .layer(7, new RBM.Builder().nIn(250).nOut(500)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())\n  .layer(8, new  RBM.Builder().nIn(500).nOut(1000)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())\n  .layer(9, new OutputLayer.Builder(LossFunctions.LossFunction.\n   RMSE_XENT).nIn(1000).nOut(numRows*numColumns).build())\n  .pretrain(true)\n  .backprop(true)\n  .build();\n\nMultiLayerNetwork model = new MultiLayerNetwork(conf);\nmodel.init();\n\n```", "```py\nlog.info(\"Train model....\");\nwhile(iter.hasNext())\n  {\n   DataSet next = iter.next();\n   model.fit(new DataSet(next.getFeatureMatrix(),next.\n   getFeatureMatrix()));\n  }\n\n```", "```py\nLoad data.... \nBuild model.... \nTrain model.... \n\no.d.e.u.d.DeepAutoEncoderExample - Train model.... \no.d.n.m.MultiLayerNetwork - Training on layer 1 with 1000 examples \no.d.o.l.ScoreIterationListener - Score at iteration 0 is 394.462 \no.d.n.m.MultiLayerNetwork - Training on layer 2 with 1000 examples \no.d.o.l.ScoreIterationListener - Score at iteration 1 is 506.785 \no.d.n.m.MultiLayerNetwork - Training on layer 3 with 1000 examples \no.d.o.l.ScoreIterationListener - Score at iteration 2 is 255.582 \no.d.n.m.MultiLayerNetwork - Training on layer 4 with 1000 examples \no.d.o.l.ScoreIterationListener - Score at iteration 3 is 128.227 \n\n......................................... \n\no.d.n.m.MultiLayerNetwork - Finetune phase \no.d.o.l.ScoreIterationListener - Score at iteration 9 is 132.45428125 \n\n........................... \n\no.d.n.m.MultiLayerNetwork - Finetune phase \no.d.o.l.ScoreIterationListener - Score at iteration 31 is 135.949859375 \no.d.o.l.ScoreIterationListener - Score at iteration 32 is 135.9501875 \no.d.n.m.MultiLayerNetwork - Training on layer 1 with 1000 examples \no.d.o.l.ScoreIterationListener - Score at iteration 33 is 394.182 \no.d.n.m.MultiLayerNetwork - Training on layer 2 with 1000 examples \no.d.o.l.ScoreIterationListener - Score at iteration 34 is 508.769 \no.d.n.m.MultiLayerNetwork - Training on layer 3 with 1000 examples \n\n............................ \n\no.d.n.m.MultiLayerNetwork - Finetune phase \no.d.o.l.ScoreIterationListener - Score at iteration 658 is 142.4304375 \no.d.o.l.ScoreIterationListener - Score at iteration 659 is 142.4311875 \n\n```"]