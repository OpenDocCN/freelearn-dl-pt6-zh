- en: Chapter 3. Deep Belief Nets and Stacked Denoising Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From this chapter through to the next chapter, you are going to learn the algorithms
    of deep learning. We'll follow the fundamental math theories step by step to fully
    understand each algorithm. Once you acquire the fundamental concepts and theories
    of deep learning, you can easily apply them to practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the topics you will learn about are:'
  prefs: []
  type: TYPE_NORMAL
- en: Reasons why deep learning could be a breakthrough
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The differences between deep learning and past machine learning (neural networks)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theories and implementations of the typical algorithms of deep learning, **deep
    belief nets** (**DBN**), and **Stacked Denoising Autoencoders** (**SDA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks fall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned about the typical algorithm of neural networks
    and saw that nonlinear classification problems cannot be solved with perceptrons
    but can be solved by making multi-layer modeled neural networks. In other words,
    nonlinear problems can be learned and solved by inserting a hidden layer between
    the input and output layer. There is nothing else to it; but by increasing the
    number of neurons in a layer, the neural networks can express more patterns as
    a whole. If we ignore the time cost or an over-fitting problem, theoretically,
    neural networks can approximate any function.
  prefs: []
  type: TYPE_NORMAL
- en: So, can we think this way? If we increase the number of hidden layers—accumulate
    hidden layers over and over—can neural networks solve any complicated problem?
    It's quite natural to come up with this idea. And, as a matter of course, this
    idea has already been examined. However, as it turns out, this trial didn't work
    well. Just accumulating layers didn't make neural networks solve the world's problems.
    On the contrary, some cases have less accuracy when predicting than others with
    fewer layers.
  prefs: []
  type: TYPE_NORMAL
- en: Why do these cases happen? It's not wrong for neural networks with more layers
    to have more expression. So, where is the problem? Well, it is caused because
    of the feature that learning algorithms have in feed-forward networks. As we saw
    in the previous chapter, the backpropagation algorithm is used to propagate the
    learning error into the whole network efficiently with the multi-layer neural
    networks. In this algorithm, an error is reversed in each layer of the neural
    network and is conveyed to the input layer one by one in order. By backpropagating
    the error at the output layer to the input layer, the weight of the network is
    adjusted at each layer in order and the whole weight of a network is optimized.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the problem occurs. If the number of layers of a network is small,
    an error backpropagating from an output layer can contribute to adjusting the
    weights of each layer well. However, once the number of layers increases, an error
    gradually disappears every time it backpropagates layers, and doesn't adjust the
    weight of the network. At a layer near the input layer, an error is not fed back
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: The neural networks where the link among layers is dense have an inability to
    adjust weights. Hence, the weight of the whole of the networks cannot be optimized
    and, as a matter of course, the learning cannot go well. This serious problem
    is known as the **vanishing gradient problem** and has troubled researchers as
    a huge problem that the neural network had for a long time until deep learning
    showed up. The neural network algorithm reached a limit at an early stage.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks' revenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because of the vanishing gradient problem, neural networks lost their popularity
    in the field of machine learning. We can say that the number of cases used for
    data mining in the real world by neural networks was remarkably small compared
    to other typical algorithms such as logistic regression and SVM.
  prefs: []
  type: TYPE_NORMAL
- en: But then deep learning showed up and broke all the existing conventions. As
    you know, deep learning is the neural network accumulating layers. In other words,
    it is deep neural networks, and it generates astounding predictability in certain
    fields. Now, speaking of AI research, it's no exaggeration to say that it's the
    research into deep neural networks. Surely it's the counterattack by neural networks.
    If so, why didn't the vanishing gradient problem matter in deep learning? What's
    the difference between this and the past algorithm?
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll look at why deep learning can generate such predictability
    and its mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning's evolution – what was the breakthrough?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can say that there are two algorithms that triggered deep learning's popularity.
    The first one, as mentioned in [Chapter 1](ch01.html "Chapter 1. Deep Learning
    Overview"), *Deep Learning Overview*, is DBN pioneered by Professor Hinton ([https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)).
    The second one is SDA, proposed by Vincent et al. ([http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf](http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf)).
    SDA was introduced a little after the introduction of DBN. It also recorded high
    predictability even with deep layers by taking a similar approach to DBN, although
    the details of the algorithm are different.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what is the common approach that solved the vanishing gradient problem?
    Perhaps you are nervously preparing to solve difficult equations in order to understand
    DBN or SDA, but don''t worry. DBN is definitely an algorithm that is understandable.
    On the contrary, the mechanism itself is really simple. Deep learning was established
    by a very simple and elegant solution. The solution is: **layer-wise training**.
    That''s it. You might think it''s obvious if you see it, but this is the approach
    that made deep learning popular.'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, in theory if there are more units or layers of neural
    networks, it should have more expressions and increase the number of problems
    it is able to solve. It doesn't work well because an error cannot be fed back
    to each layer correctly and parameters, as a whole network, cannot be adjusted
    properly. This is where the innovation was brought in for learning at a respective
    layer. Because each layer adjusts the weights of the networks independently, the
    whole network (that is, the parameters of the model) can be optimized properly
    even though the numbers of layers are piled up.
  prefs: []
  type: TYPE_NORMAL
- en: Previous models didn't go well because they tried to backpropagate errors from
    an output layer to an input layer straight away and tried to optimize themselves
    by adjusting the weights of the network with backpropagated errors. So, the algorithm
    shifted to layer-wise training and then the model optimization went well. That's
    what the breakthrough was for deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: However, although we simply say **layer-wise training**, we need techniques
    for how to implement the learning. Also, as a matter of course, parameter adjustments
    for whole networks can't only be done with layer-wise training. We need the final
    adjustment. This phase of layer-wise training is called **pre-training** and the
    last adjustment phase is called **fine-tuning**. We can say that the bigger feature
    introduced in DBN and SDA is pre-training, but these two features are both part
    of the the necessary flow of deep learning. How do we do pre-training? What can
    be done in fine-tuning? Let's take a look at these questions one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning with pre-training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning is more like neural networks with accumulated hidden layers.
    The layer-wise training in pre-training undertakes learning at each layer. However,
    you might still have the following questions: if both layers are hidden (that
    is, neither of the layers are input nor output layers), then how is the training
    done? What can the input and output be?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before thinking of these questions, remind yourself of the following point
    again (reiterated persistently): deep learning is neural networks with piled up
    layers. This mean, model parameters are still the weights of the network (and
    bias) in deep learning. Since these weights (and bias) need to be adjusted among
    each layer, in the standard three layered neural network (that is, the input layer,
    the hidden layer, and the output layer), we need to optimize only the weights
    between the input layer and the hidden layer and between the hidden layer and
    the output layer. In deep learning, however, the weight between two hidden layers
    also needs to be adjusted.'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, let's think about the input of a layer. You can imagine this easily
    with a quick thought. The value propagated from the previous layer will become
    the input as it is. The value propagated from the previous layer is none other
    than the value forward propagated from the previous layers to the current layer
    by using the weight of the network, the same as in general feed-forward networks.
    It looks simple in writing, but you can see that it has an important meaning if
    you step into it further and try to understand what it means. The value from the
    previous layer becomes the input, which means that the features the previous layer(s)
    learned become the input of the current layer, and from there the current layer
    newly learns the feature of the given data. In other words, in deep learning,
    features are learned from the input data in stages (and semi-automatically). This
    implies a mechanism where the deeper a layer becomes, the higher the feature it
    learns. This is what normal multi-layer neural networks couldn't do and the reason
    why it is said "a machine can learn a concept."
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s think about the output. Please bear in mind that thinking about
    the output means thinking about how it learns. DBN and SDA have completely different
    approaches to learning, but both fill the following condition: to learn in order
    to equate output values and input values. You might think "What are you talking
    about?" but this is the technique that makes deep learning possible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The value comes and goes back to the input layer through the hidden layer,
    and the technique is to adjust the weight of the networks (that is, to equate
    the output value and the input value) to eliminate the error at that time. The
    graphical model can be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning with pre-training](img/B04779_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It looks different from standard neural networks at a glance, but there''s
    nothing special. If we intentionally draw the diagram of the input layer and the
    output layer separately, the mechanism is the same shape as the normal neural
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning with pre-training](img/B04779_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For a human, this action of *matching input and output* is not intuitive, but
    for a machine it is a valid action. If so, how it can learn features from input
    data by matching the output layer and input layer?
  prefs: []
  type: TYPE_NORMAL
- en: 'Need a little explanation? Let''s think about it this way: in the algorithm
    of machine learning, including neural networks, learning intends to minimize errors
    between the model''s prediction output and the dataset output. The mechanism is
    to remove an error by finding a pattern from the input data and making data with
    a common pattern the same output value (for example, 0 or 1). What would then
    happen if we turned the output value into the input value?'
  prefs: []
  type: TYPE_NORMAL
- en: When we look at problems that should be solved as a whole through deep learning,
    input data is, fundamentally, a dataset that can be divided into some patterns.
    This means that there are some common features in the input data. If so, in the
    process of learning where each output value becomes respective input data, the
    weight of networks should be adjusted to focus more on the part that reflects
    the common features. And, even within the data categorized in the same class,
    learning should be processed to reduce weight on the non-common feature part,
    that is, the noise part.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you should understand what the input and output is in a certain layer and
    how learning progresses. Once the pre-training is done at a certain layer, the
    network moves on to learning in the next layer. However, as you can see in the
    following images, please also keep in mind that a hidden layer becomes an input
    layer when the network moves to learning in the next layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning with pre-training](img/B04779_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The point here is that the layer after the pre-training can be treated as normal
    feed-forward neural networks where the weight of the networks is adjusted. Hence,
    if we think about the input value, we can simply calculate the value forward propagated
    from the input layer to the current layer through the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to now, we''ve looked through the flow of pre-training (that is, layer-wise
    training). In the hidden layers of deep neural networks, features of input data
    are extracted in stages through learning where the input matches the output. Now,
    some of you might be wondering: I understand that features can be learned in stages
    from input data by pre-training, but that alone doesn''t solve the classification
    problem. So, how can it solve the classification problem?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, during pre-training, the information pertaining to which data belongs
    to which class is not provided. This means the pre-training is unsupervised training
    and it just analyzes the hidden pattern using only input data. This is meaningless
    if it can''t be used to solve the problem however it extracts features. Therefore,
    the model needs to complete one more step to solve classification problems properly.
    That is fine-tuning. The main roles of fine-tuning are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: To add an output layer to deep neural networks that completed pre-training and
    to perform supervised training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To do final adjustments for the whole deep neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This can be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning with pre-training](img/B04779_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The supervised training in an output layer uses a machine learning algorithm,
    such as logistic regression or SVM. Generally, logistic regression is used more
    often considering the balance of the amount of calculation and the precision gained.
  prefs: []
  type: TYPE_NORMAL
- en: In fine-tuning, sometimes only the weights of an output layer will be adjusted,
    but normally the weights of whole neural networks, including the layer where the
    weights have been adjusted in pre-training, will also be adjusted. This means
    the standard learning algorithm, or in other words the backpropagation algorithm,
    is applied to the deep neural networks just as one multi-layer neural network.
    Thus, the model of neural networks with the problem of solving more complicated
    classification is completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even so, you might have the following questions: why does learning go well
    with the standard backpropagation algorithm even in multi-layer neural networks
    where layers are piled up? Doesn''t the vanishing gradient problem occur? These
    questions can be solved by pre-training. Let''s think about the following: in
    the first place, the problem is that the weights of each network are not correctly
    adjusted due to improperly fed back errors in multi-layer neural networks without
    pre-training; in other words, the multi-layer neural networks where the vanishing
    gradient problem occurs. On the other hand, once the pre-training is done, the
    learning starts from the point where the weight of the network is almost already
    adjusted. Therefore, a proper error can be propagated to a layer close to an input
    layer. Hence the name fine-tuning. Thus, through pre-training and fine-tuning,
    eventually deep neural networks become neural networks with increased expression
    by having deep layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the next section onwards, we will finally look through the theory and
    implementation of DBN and SDA, the algorithms of deep learning. But before that,
    let''s look back at the flow of deep learning once again. Below is the summarized
    diagram of the flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning with pre-training](img/B04779_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The parameters of the model are optimized layer by layer during pre-training
    and then adjusted as single deep neural networks during fine-tuning. Deep learning,
    the breakthrough of AI, is a very simple algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's look through the theory and implementation of deep learning algorithms.
    In this chapter, we will see DBN and SDA (and the related methods). These algorithms
    were both researched explosively, mainly between 2012 and 2013 when deep learning
    started to spread out rapidly and set the trend of deep learning on fire. Even
    though there are two methods, the basic flow is the same and consistent with pre-training
    and fine-tuning, as explained in the previous section. The difference between
    these two is which pre-training (that is, unsupervised training) algorithm is
    applied to them.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, if there could be difficult points in deep learning, it should be
    the theory and equation of the unsupervised training. However, you don't have
    to be afraid. All the theories and implementations will be explained one by one,
    so please read through the following sections carefully.
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The method used in the layer-wise training of DBN, pre-training, is called **Restricted
    Boltzmann Machines** (**RBM**). To begin with, let's take a look at the RBM that
    forms the basis of DBN. As RBM stands for Restricted Boltzmann Machines, of course
    there's a method called **Boltzmann Machines** (**BMs**). Or rather, BMs are a
    more standard form and RBM is the special case of them. Both are one of the neural
    networks and both were proposed by Professor Hinton.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of RBM and DBNs can be done without understanding the detailed
    theory of BMs, but in order to understand these concepts, we''ll briefly look
    at the idea BMs are based on. First of all, let''s look at the following figure,
    which shows a graphical model of BMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'BMs look intricate because they are fully connected, but they are actually
    just simple neural networks with two layers. By rearranging all the units in the
    networks to get a better understanding, BMs can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Please bear in mind that normally the input/output layer is called the **visible
    layer** in BMs and RBMs (a hidden layer is commonly used as it is), for it is
    the networks that presume the hidden condition (unobservable condition) from the
    observable condition. Also, the neurons of the visible layer are called **visible
    units** and the neurons of the hidden layer are called **hidden units**. Signs
    in the previous figure are described to match the given names.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the diagram, the structure of BMs is not that different from
    standard neural networks. However, its way of thinking has a big feature. The
    feature is to adopt the concept of *energy* in neural networks. Each unit has
    a stochastic state respectively and the whole of the networks' energy is determined
    depending on what state each unit takes. (The first model that adopted the concept
    of energy in networks is called the **Hopfield network**, and BMs are the developed
    version of it. Since details of the Hopfield network are not totally relevant
    to deep learning, it is not explained in this book.) The condition that memorizes
    the correct data is the steady state of networks and the least amount of energy
    these networks have. On the other hand, if data with noise is provided to the
    network, each unit has a different state, but not a steady state, hence its condition
    makes the transition to stabilize the whole network, in other words, to transform
    it into a steady state.
  prefs: []
  type: TYPE_NORMAL
- en: This means that the weights of the model are adjusted and the state of each
    unit is transferred to minimize the energy function the networks have. These operations
    can remove the noise and extract the feature from inputs as a whole. Although
    the energy of networks sounds enormous, it's not too difficult to imagine because
    minimizing the energy function has the same effect as minimizing the error function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of BMs was wonderful, but various problems occurred when BMs were
    actually applied to practical problems. The biggest problem was that BMs are fully
    connected networks and take an enormous amount of calculation time. Therefore,
    RBM was devised. RBM is the algorithm that can solve various problems in a realistic
    time frame by making BMs restricted. Just as in BM, RBM is a model based on the
    energy of a network. Let''s look at RBM in the diagram below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Restricted Boltzmann machines](img/B04779_03_14.jpg) is the number of
    units in the visible layer and ![Restricted Boltzmann machines](img/B04779_03_15.jpg)
    the number of units in the hidden layer. ![Restricted Boltzmann machines](img/B04779_03_16.jpg)
    denotes the value of a visible unit, ![Restricted Boltzmann machines](img/B04779_03_17.jpg)
    the value of a hidden unit, and ![Restricted Boltzmann machines](img/B04779_03_18.jpg)
    the weight between these two units. As you can see, the difference between BM
    and RBM is that RBM doesn't have connections between the same layer. Because of
    this restriction, the amount of calculation decreases and it can be applied to
    realistic problems.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look through the theory.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be careful that, as a prerequisite, the value that each visible unit and hidden
    unit in RBM can take is generally {0, 1}, that is, binary (this is the same as
    BMs).
  prefs: []
  type: TYPE_NORMAL
- en: If we expand the theory, it can also handle continuous values. However, this
    could make equations complex, where it's not the core of the theory and where
    it's implemented with binary in the original DBN proposed by Professor Hinton.
    Therefore, we'll also implement binary RBM in this book. RBM with binary inputs
    is sometimes called **Bernoulli RBM**.
  prefs: []
  type: TYPE_NORMAL
- en: 'RBM is the energy-based model, and the status of a visible layer or hidden
    layer is treated as a stochastic variable. We''ll look at the equations in order.
    First of all, each visible unit is propagated to the hidden units throughout a
    network. At this time, each hidden unit takes a binary value based on the probability
    distribution generated in accordance with its propagated inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Restricted Boltzmann machines](img/B04779_03_19.jpg) is the bias in
    a hidden layer and ![Restricted Boltzmann machines](img/B04779_03_20.jpg) denotes
    the sigmoid function.
  prefs: []
  type: TYPE_NORMAL
- en: This time, it was conversely propagated from a hidden layer to a visible layer
    through the same network. As in the previous case, each visible unit takes a binary
    value based on probability distribution generated in accordance with propagated
    values.
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Restricted Boltzmann machines](img/B04779_03_23.jpg) is the bias of
    the visible layer. This value of visible units is expected to match the original
    input values. This means if ![Restricted Boltzmann machines](img/B04779_03_24.jpg),
    the weight of the network as a model parameter, and ![Restricted Boltzmann machines](img/B04779_03_25.jpg),
    ![Restricted Boltzmann machines](img/B04779_03_26.jpg), the bias of a visible
    layer and a hidden layer, are shown as a vector parameter, ![Restricted Boltzmann
    machines](img/B04779_03_27.jpg), it leans ![Restricted Boltzmann machines](img/B04779_03_27.jpg)
    in order for the probability ![Restricted Boltzmann machines](img/B04779_03_28.jpg)
    that can be obtained above to get close to the distribution of ![Restricted Boltzmann
    machines](img/B04779_03_29.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'For this learning, the energy function, that is, the evaluation function, needs
    to be defined. The energy function is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, the joint probability density function showing the demeanor of a network
    can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_31.jpg)![Restricted Boltzmann
    machines](img/B04779_03_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding formulas, the equations for the training of parameters will
    be determined. We can get the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the **log likelihood** can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we''ll calculate each gradient against the model parameter. The derivative
    can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Some equations in the middle are complicated, but it turns out to be simple
    with the term of the probability distribution of the model and the original data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the gradient of each parameter is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_36.jpg)![Restricted Boltzmann
    machines](img/B04779_03_37.jpg)![Restricted Boltzmann machines](img/B04779_03_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now then, we could find the equation of the gradient, but a problem occurs when
    we try to apply this equation as it is. Think about the term of ![Restricted Boltzmann
    machines](img/B04779_03_39.jpg). This term implies that we have to calculate the
    probability distribution for all the {0, 1} patterns, which can be assumed as
    input data that includes patterns that don't actually exist in the data.
  prefs: []
  type: TYPE_NORMAL
- en: We can easily imagine how this term can cause a combinatorial explosion, meaning
    we can't solve it within a realistic time frame. To solve this problem, the method
    for approximating data using Gibbs sampling, called **Contrastive Divergence**
    (**CD**), was introduced. Let's look at this method now.
  prefs: []
  type: TYPE_NORMAL
- en: Here, ![Restricted Boltzmann machines](img/B04779_03_40.jpg) is an input vector.
    Also, ![Restricted Boltzmann machines](img/B04779_03_41.jpg) is an input (output)
    vector that can be obtained by sampling for k-times using this input vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_42.jpg)![Restricted Boltzmann
    machines](img/B04779_03_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, when approximating ![Restricted Boltzmann machines](img/B04779_03_44.jpg)
    after reiterating Gibbs sampling, the derivative of the log likelihood function
    can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the model parameter can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_46.jpg)![Restricted Boltzmann
    machines](img/B04779_03_47.jpg)![Restricted Boltzmann machines](img/B04779_03_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Restricted Boltzmann machines](img/B04779_03_49.jpg) is the number of
    iterations and ![Restricted Boltzmann machines](img/B04779_03_50.jpg) is the learning
    rate. As shown in the preceding formulas, generally, CD that performs sampling
    k-times is shown as CD-k. It's known that CD-1 is sufficient when applying the
    algorithm to realistic problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s go through the implementation of RMB. The package structure is
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Restricted Boltzmann machines](img/B04779_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's look through the `RestrictedBoltzmannMachines.java` file. Because the
    first part of the main method just defines the variables needed for a model and
    generates demo data, we won't look at it here.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in the part where we generate an instance of a model, you may notice there
    are many `null` values in arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When you look at the constructor, you might know that these `null` values are
    the RBM''s weight matrix, bias of hidden units, and bias of visible units. We
    define arguments as `null` here because they are for DBN''s implementation. In
    the constructor, these are initialized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is training. CD-1 is applied for each mini-batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look into the essential point of RBM, the `contrastiveDivergence`
    method. CD-1 can obtain a sufficient solution when we actually run this program
    (and so we have k = 1 in the demo), but this method is defined to deal with CD-k
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'It appears that two different types of method, `sampleHgivenV` and `gibbsHVH`,
    are used in CD-k, but when you look into `gibbsHVH`, you can see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: So, CD-k consists of only two methods for sampling, `sampleVgivenH` and `sampleHgivenV`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the name of the method indicates, `sampleHgivenV` is the method that sets
    the probability distribution and sampling data generated in a hidden layer based
    on the given value of visible units and vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `propup` and `propdown` tags that set values to respective means are the
    method that activates values of each unit by the `sigmoid` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `binomial` method that sets a value to a sample is defined in `RandomGenerator.java`.
    The method returns `0` or `1` based on the binomial distribution. With this method,
    a value of each unit becomes binary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once approximated values are obtained by sampling, what we need to do is just
    calculate the gradient of a `model` parameter and renew a parameter using a mini-batch.
    There''s nothing special here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we''re done with the `model` training. Next comes the test and evaluation
    in general cases, but note that the model cannot be evaluated with barometers
    such as accuracy because RBM is a generative model. Instead, let''s briefly look
    at how noisy data is changed by RBM here. Since RBM after training can be seen
    as a neural network, the weights of which are adjusted, the model can obtain reconstructed
    data by simply propagating input data (that is, noisy data) through a network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Deep Belief Nets (DBNs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DBNs are deep neural networks where logistic regression is added to RBMS as
    the output layer. Since the theory necessary for implementation has already been
    explained, we can go directly to the implementation. The package structure is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Belief Nets (DBNs)](img/B04779_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The flow of the program is very simple. The order is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up parameters for the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pre-training the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tuning the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing and evaluating the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Just as in RBM, the first step in setting up the main method is the declaration
    of variables and the code for creating demo data (the explanation is omitted here).
  prefs: []
  type: TYPE_NORMAL
- en: 'Please check that in the demo data, the number of units for an input layer
    is 60, a hidden layer has 2 layers, their combined number of units is 20, and
    the number of units for an output layer is 3\. Now, let''s look through the code
    from the *Building the Model* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The variable of `hiddenLayerSizes` is an array and its length represents the
    number of hidden layers in deep neural networks. The deep learning algorithm takes
    a huge amount of calculation, hence the program gives us an output of the current
    status so that we can see which process is proceeding. The variable of `hiddenLayerSizes`
    is an array and its length represents the number of hidden layers in deep neural
    networks. Each layer is constructed in the constructor.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please bear in mind that `sigmoidLayers` and `rbmLayers` are, of course, different
    objects but their weights and bias are shared.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because, as explained in the theory section, pre-training performs
    layer-wise training, whereas the whole model can be regarded as one neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing to do after building the model is pre-training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Pre-training needs to be processed with each `minibatch` but, at the same time,
    with each layer. Therefore, all training data is given to the `pretrain` method
    first, and then the data of each mini-batch is processed in the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Since the actual learning is done through CD-1 of RBM, the description of DBN
    within the code is very simple. In DBN (RBM), units of each layer have binary
    values, so the output method of `HiddenLayer` cannot be used because it returns
    double. Hence, the `outputBinomial` method is added to the class, which returns
    the `int` type (the code is omitted here). Once the pre-training is complete,
    the next step is fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be careful not to use training data that was used in the pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily fall into overfitting if we use the whole data set for both pre-training
    and fine-tuning. Therefore, the validation data set is prepared separately from
    the training dataset and is used for fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `finetune` method, the backpropagation algorithm in multi-layer neural
    networks is applied where the logistic regression is used for the output layer.
    To backpropagate unit values among multiple hidden layers, we define variables
    to maintain each layer''s inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The training part of DBN is just how it is seen in the preceding code. The hard
    part is probably the theory and implementation of RBM, so you might think it's
    not too hard when you just look at the code of DBN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since DBN after the training can be regarded as one (deep) neural network,
    you simply need to forward propagate data in each layer when you try to predict
    which class the unknown data belongs to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As for evaluation, no explanation should be needed because it's not much different
    from the previous classifier model.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have now acquired knowledge of one of the deep learning
    algorithms. You might be able to understand it more easily than expected. However,
    the difficult part of deep learning is actually setting up the parameters, such
    as setting how many hidden layers there are, how many units there are in each
    hidden layer, the learning rate, the iteration numbers, and so on. There are way
    more parameters to set than in the method of machine learning. Please remember
    that you might find this point difficult when you apply this to a realistic problem.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The method used in pre-training for SDA is called **Denoising Autoencoders**
    (**DA**). It can be said that DA is the method that emphasizes the role of equating
    inputs and outputs. What does this mean? The processing content of DA is as follows:
    DA adds some noise to input data intentionally and partially damages the data,
    and then DA performs learning as it restores corrupted data to the original input
    data. This intentional noise can be easily substantiated if the input data value
    is [0, 1]; by turning the value of the relevant part into 0 compulsorily. If a
    data value is out of this range, it can be realized, for example, by adding Gaussian
    noise, but in this book, we''ll think about the former [0, 1] case to understand
    the core part of the algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In DA as well, an input/output layer is called a visible layer. DA''s graphical
    model can be shown to be the same shape of RBM, but to get a better understanding,
    let''s follow this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Denoising Autoencoders](img/B04779_03_51.jpg) is the corrupted data,
    the input data with noise. Then, forward propagation to the hidden layer and the
    output layer can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_52.jpg)![Denoising Autoencoders](img/B04779_03_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Denoising Autoencoders](img/B04779_03_19.jpg) denotes the bias of the
    hidden layer and ![Denoising Autoencoders](img/B04779_03_23.jpg) the bias of the
    visible layer. Also, ![Denoising Autoencoders](img/B04779_03_20.jpg) denotes the
    sigmoid function. As seen in the preceding diagram, corrupting input data and
    mapping to a hidden layer is called **Encode** and mapping to restore the encoded
    data to the original input data is called **Decode**. Then, DA''s evaluation function
    can be denoted with a negative log likelihood function of the original input data
    and decoded data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Denoising Autoencoders](img/B04779_03_27.jpg) is the model parameter,
    the weight and the bias of the visible layer and the hidden layer. What we need
    to do is just find the gradients of these parameters against the evaluation function.
    To deform equations easily, we define the functions here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_55.jpg)![Denoising Autoencoders](img/B04779_03_56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_57.jpg)![Denoising Autoencoders](img/B04779_03_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using these functions, each gradient of a parameter can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_59.jpg)![Denoising Autoencoders](img/B04779_03_60.jpg)![Denoising
    Autoencoders](img/B04779_03_61.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, only two terms are required. Let''s derive them one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_62.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we utilized the derivative of the `sigmoid` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_63.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_64.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the following equation can be obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_65.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On the other hand, we can also get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_66.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the renewed equation for each parameter will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_67.jpg)![Denoising Autoencoders](img/B04779_03_68.jpg)![Denoising
    Autoencoders](img/B04779_03_69.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Denoising Autoencoders](img/B04779_03_70.jpg) is the number of iterations
    and ![Denoising Autoencoders](img/B04779_03_50.jpg) is the learning rate. Although
    DA requires a bit of technique for deformation, you can see that the theory itself
    is very simple compared to RBM.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's proceed with the implementation. The package structure is the same
    as the one for RBM.
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoders](img/B04779_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As for model parameters, in addition to the number of units in a hidden layer,
    the amount of noise being added to the input data is also a parameter in DA. Here,
    the corruption level is set at `0.3`. Generally, this value is often set at `0.1
    ~ 0.3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The flow from the building model to training is the same as RBM. Although this
    method of training is called `contrastiveDivergence` in RBM, it''s simply set
    as `train` in DA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The content of `train` is as explained in the theory section. First of all,
    add noise to the input data, then encode and decode it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The process of adding noise is, as previously explained, the compulsory turning
    of the value of the corresponding part of the data into `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The other processes are just simple activation and propagation, so we won''t
    go through them here. The calculation of the gradients follows math equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Compared to RBM, the implementation of DA is also quite simple. When you test
    (`reconstruct`) the model, you don''t need to corrupt the data. As in standard
    neural networks, you just need to forward propagate the given inputs based on
    the weights of the networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Stacked Denoising Autoencoders (SDA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SDA is deep neural networks with piled up DA layers. In the same way that DBN
    consists of RBMs and logistic regression, SDA consists of DAs and logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stacked Denoising Autoencoders (SDA)](img/B04779_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The flow of implementation is not that different between DBN and SDA. Even though
    there is a difference between RBM and DA in pre-training, the content of fine-tuning
    is exactly the same. Therefore, not much explanation might be needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method for pre-training is not that different, but please note that the
    point where the `int` type was used for DBN is changed to double type, as DA can
    handle `[0, 1]`, not binary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `predict` method after learning is also exactly the same as in DBN. Considering
    that both DBN and SDA can be treated as one multi-layer neural network after learning
    (that is, the pre-training and fine-tuning), it's natural that most of the processes
    are common.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, SDA can be implemented more easily than DBN, but the precision to be
    obtained is almost the same. This is the merit of SDA.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the problem of the previous neural networks algorithm
    and what the breakthrough was for deep learning. Also, you learned about the theory
    and implementation of DBN and SDA, the algorithm that fueled the boom of deep
    learning, and of RBM and DA used in each respective method.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at more deep learning algorithms. They take
    different approaches to obtain high precision rates and are well developed.
  prefs: []
  type: TYPE_NORMAL
