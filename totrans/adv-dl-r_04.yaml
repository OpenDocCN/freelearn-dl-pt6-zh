- en: Deep Neural Networks for Multi-Class Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When developing prediction and classification models, depending on the type
    of response or target variable, we come across two potential type of problems:
    the target variable is of categorical type (this is a classification type of problem)
    or the target variable is of a numeric type (this is a regression type of problem).
    It has been observed that about 70% of the data belongs to problems arising from
    classification categories and the remaining 30% are regression problems (here
    is the reference: [https://www.topcoder.com/role-of-statistics-in-data-science/](https://www.topcoder.com/role-of-statistics-in-data-science/)).
    In this chapter, we will provide steps for applying deep learning neural networks
    for classification problems. The steps are illustrated using the fetal cardiotocograms,
    or CTGs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief understanding of the fetal cardiotocogram (or CTG) dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Steps for data preparation, including normalization, data partitioning, and
    one-hot encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and fitting a deep neural network model for classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating classification model performance and making predictions using the
    model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning the model for performance optimization and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cardiotocogram dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will provide information about the data used for developing
    a multiclass classification model. We will use only one library, which is Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset (medical)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset used in this chapter is publicly available at the UCI Machine Learning
    Repository maintained by the School of Information and Computer Science at the University
    of California. You can access this at [https://archive.ics.uci.edu/ml/datasets/cardiotocography](https://archive.ics.uci.edu/ml/datasets/cardiotocography).
  prefs: []
  type: TYPE_NORMAL
- en: It is to be noted that this URL enables you to download an Excel data file.
    This file can be easily converted to a `.csv` format by saving the file as a `.csv`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'For data we should use the formatting which is used for `.csv`, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This data consists of fetal CTGs, and the target variable classifies a patient
    into one of three categories: normal, suspect, and pathological. There are 2,126
    rows in this dataset. The CTGs are classified by three expert obstetricians, and
    a consensus classification label is assigned to each of them as normal (N) (represented
    by 1), suspect (S) (represented by 2), and pathological (P) (represented by 3).
    There are 21 independent variables, and the main objective is to develop a classification
    model to correctly classify each patient into one of the three categories represented
    by N, S, and P.'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data for model building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will prepare the data for building the classification model.
    Data preparation will involve normalizing the data, partitioning the data into
    training and test data, and carrying out one-hot encoding of the response variable.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing numeric variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For developing deep network models, we carry out the normalization of numeric
    variables to bring them to a common scale. When dealing with several variables,
    it is likely that different variables have different scales—for example, there
    could be a variable that shows revenues earned by a company and the values could
    be in millions of dollars. In another example, there could be a variable that
    shows the dimension of a product in centimeters. Such extreme differences in scale
    create difficulties when training a network, and normalization helps to address
    this issue. For normalization, we will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding code, we first change the data to matrix format,
    and then we remove the default names by assigning `NULL` to the dimension names.
    In this step, the names of 22 variables will be changed to `V1`, `V2`, `V3`,...,
    `V22`. If you run `str(data)` at this stage, you will notice the change in format
    of the original data. We normalize the 21 independent variables using the `normalize`
    function, which is a part of the Keras package. When you run this line of code,
    you will notice that it uses TensorFlow as a backend. We also change the target
    variable, NSP, to numeric from the default integer type. In addition, in the same
    line of code, we also change values from `1`, `2`, and `3` to `0`, `1`, and `2`
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will partition this data into training and test datasets. To carry
    out data partitioning, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding code, to obtain the same samples in the training
    and test datasets for repeatability purposes, we use `set.seed` with a specific
    number, which in this case is `1234`. This will ensure that the reader can also
    obtain the same samples in the training and test data. For data partitioning,
    a 70:30 split is used here, but any other ratio can be used too. In machine learning
    applications, this is a commonly used step to ensure that the prediction model
    works well with unseen data that is stored in the form of test data. Training
    data is used for developing the model and test data is used to assess the performance
    of the model. Sometimes, a prediction model may perform very well or even perfectly
    well with training data; however, when it is evaluated with test data that has
    not been seen by the model, the performance may turn out to be very disappointing.
    In machine learning, this problem is termed as over fitting the model. Test data
    helps to assess and ensure that the prediction model can be reliably implemented
    for making the appropriate decisions.
  prefs: []
  type: TYPE_NORMAL
- en: We use `training` and `test` names to store independent variables and we use
    `trainingtarget` and `testtarget` names to store target variables stored in the
    22^(nd) column of the dataset. After data partitioning, we will have 1,523 observations
    in the training data and the remaining 603 observations will be in the test data.
    Note that although we use a 70:30 split here, the actual ratio after data partitioning
    may not be exactly 70:30.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After data partitioning, we will carry out a one-hot encoding of the response
    variable. One-hot encoding helps to represent a categorical variable in zeros
    and ones. The code and output for one-hot encoding is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, with the help of the `to_categorical`
    function from the Keras package, we convert the target variable to a binary class
    matrix, where the presence or absence of a class is simply represented by 1 or
    0 respectively. In this example, we have three classes for the target variable,
    which are converted to three dummy variables. This process is also called **one-hot
    encoding**. First, 10 rows from `testLabels` are printed. The first row indicates
    the normal category for the patient with (1,0,0), the sixth row indicates the
    suspect category for the patient with (0,1,0), and the fourth row provides an
    example of the pathologic category of a patient with (0,0,1).
  prefs: []
  type: TYPE_NORMAL
- en: Once we complete these steps for data preparation, we move to the next step,
    where we create the classification model to classify a patient as normal, suspect,
    or pathological.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and fitting a deep neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will develop the model architecture, compile the model,
    and then fit the model.
  prefs: []
  type: TYPE_NORMAL
- en: Developing model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code used for developing the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, we start by creating a sequential model using
    the `keras_model_sequential()` function, which allows a linear stack of layers
    to be added. Next, we add layers to the model using the pipe operator, `%>%`.
    This pipe operator takes information from the left as output and feeds that information
    as input to what is on the right. We use a fully connected or densely connected
    neural network using the `layer_dense` function and then specify various inputs.
    In this dataset, we have 21 independent variables, and as such, the `input_shape` function is
    specified as 21 neurons or units in the neural network. This layer is also termed
    as the input layer in the network. The first hidden layer has 8 units and the
    activation function that we use here is a rectified linear unit, or `relu`, which
    is the most popular activation function used in these situations. The first hidden
    layer is connected to the output layer with 3 units using the pipe operator. We
    use 3 units since our target variable has 3 classes. The activation function used
    in the output layer is `'softmax'`, which helps to keep the range of output values
    between 0 and 1\. Keeping the range of output values between 0 and 1 will help
    us to interpret results in the form of familiar probability values.
  prefs: []
  type: TYPE_NORMAL
- en: For typing the pipe operator, `%>%`, in RStudio, you can use the *Shift* + *Command*
    + *M* shortcut for Mac, and for Windows, *Shift* + *Ctrl* + *M*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain a summary of the model architecture that we have created, we can
    run the `summary` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Since the input layer has 21 units that are connected to each of the 8 units
    in the first hidden layer, we end up with 168 weights (21 x 8). We also obtain
    one bias term for each unit in the hidden layer, with a total of 8 such terms.
    So, at the first and only hidden layer stage, we have a total of 176 parameters
    (168 + 8). Similarly, 8 units in the hidden layer are connected to 3 units in
    the output layer, yielding 24 weights (8 x 3). This way, we have 24 weights and
    3 bias terms at the output layer that account for a total of 27 parameters. Finally,
    the total number of parameters for this neural network architecture will be 203.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To configure the learning process for the neural network, we compile the model
    by specifying the loss, optimizer, and metrics, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We use `loss` for specifying the objective function that we want to optimize.
    As shown in the preceding code, for the loss, we use `'categorical_crossentropy'`,
    since our target variable has three categories. For situations where the target
    variable has two categories, we use `binary_crossentropy`. For the optimizer,
    we use the `'adam'` optimization algorithm, which is a popular algorithm for deep
    learning. Its popularity is mainly due to the fact that it gives good results
    faster than other stochastic optimization methods, such as the **adaptive gradient
    algorithm** (**AdaGrad**) and **root mean square propagation** (**RMSProp**).
    We specify the metrics for evaluating the model performance during training and
    testing. For `metrics`, we use `accuracy` to assess the classification performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to fit the model, which we will do in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To fit the model, we make use of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen from the preceding code, we see the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: To fit the model, we provide training data that has data for 21 independent
    variables and `trainLabels`, which contains data for the target variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of iterations or epochs is specified as 200\. An epoch is a single
    pass of the training data followed by model assessment using validation data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To avoid overfitting, we specified that the validation split is 0.2, which means
    that 20% of the training data will be used to assess the model performance as
    the training proceeds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this 20% of data is the bottom 20% of the data points in the training
    data. We stored data on the loss and accuracy values for the training and validation
    data generated during the training of the model in `model_one` for later use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For `batch_size`, we used the default value of 32, which represents the number
    of samples that will be used per gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the training of the model proceeds, we get a visual display of plots for
    loss and accuracy based on training and validation data after each epoch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For accuracy, we would like the model to have higher values, as accuracy is
    a t`he-higher-the-better` type of metric, whereas for loss, which is a `the-lower-the-better`
    type of metric, we would like the model to have lower values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, we also obtained the numeric summary of the loss output based on
    the last 3 epochs, as shown in the preceding code output. For each epoch, we saw
    that 1,218 samples out of 1,523 samples of the training data (about 80%) were
    used for fitting the model. The remaining 20% of the data was used for calculating
    accuracy and loss values for the validation data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A word of caution. When using `validation_split`, note that the validation data
    is not selected randomly from the training data—for example, when `validation_split
    = 0.2`, the last 20% of the training data is used for validation and the first
    80% is used for training. Therefore, if the values of the target variable are
    not random, then `validation_split` may introduce bias in the classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the training process completes 200 epochs, we can plot the training progress
    in terms of loss and accuracy for training and validation data using the `plot`
    function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph provides a plot that has accuracy in the top window and
    loss in the lower window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7d38d49-2ace-4e8d-9365-ba2e65201ed1.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracy and loss for training and validation data
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding plot for loss and accuracy, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: From the plot for accuracy in the top graph, you can see that accuracy values
    increase significantly after about 25 epochs and then continue to increase gradually
    for the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For validation data, the progress is more uneven, with a drop in accuracy between
    the 25^(th) and 50^(th) epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A somewhat similar pattern is observed in the opposite direction for loss values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that if the training data accuracy increases with the number of epochs,
    but the validation data accuracy decreases, that would suggest an overfitting
    of the model. We do not see any major pattern suggesting model overfitting from
    this plot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation and predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use test data to evaluate the model performance. We
    can certainly calculate loss and accuracy values using the training data; however,
    the real test of a classification model is when it is used with unseen data. And
    since test data is kept separate from the model building process, we can now use
    it for model evaluation. We will first calculate loss and accuracy values with
    the test data and then develop a confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Loss and accuracy calculation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for obtaining loss and accuracy values using the test data along with
    the output is shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, using the `evaluate` function, we can
    obtain loss and accuracy values as `0.4439` and `0.8424` respectively. Using `colSums(testLabels)`,
    we can find that there are 460, 94, and 49 cases of normal, suspect, and pathological
    patients respectively in the test data. Converting these numbers to percentages
    using a total of 603 samples in the test data, we obtain 76.3%, 15.6%, and 8.1%
    respectively. The highest number of samples belongs to the normal category of
    patients, and we can use 76.3% as a benchmark for the model performance. If we
    do not use any model and simply classify all cases in the test data as belonging
    to the normal category of patients, then we will still be correct about 76.3%
    of the time since we will be right about all normal patients and incorrect about
    the other two categories.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the accuracy of our prediction will be as high as 76.3%; therefore,
    the model that we develop here should perform at least better than this benchmark
    number. If it functions below this number, then it is not likely to be of much
    practical use. Since we get an accuracy of 84.2% for the test data, we are definitely
    doing better than the benchmark value, but clearly we must also try to improve
    our model in order to perform even better. To do that, let's dig even deeper and
    learn about model performance for each category of the response variable with
    the help of a confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To obtain a confusion matrix, let''s start by making a prediction for the test
    data and save it in `pred`. We use `predict_classes` to make this prediction and
    then use the `table` function to create a summary of predicted versus actual values
    for the test data to create a confusion matrix, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding confusion matrix, shown as output", values `0`, `1`, and `2`
    represent normal, suspect, and pathological categories respectively. From the
    confusion matrix, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: There were `435` patients in the test data who were actually normal and the
    model also predicted them as being normal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, there were `51` correct predictions for the suspect group and `22`
    correct predictions for the pathological group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we add all the numbers on the diagonal of the confusion matrix, which are
    the correct classifications, we obtain 508 (435 + 51 + 22), or an accuracy level
    of 84.2% ((508 ÷ 603) x 100).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the confusion matrix, the off diagonal numbers indicate the number of patients
    who are misclassified. The highest number of misclassifications is 41, where the
    patients actually belong to the suspect group but the model incorrectly classified
    them in the normal category of patients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The instance of misclassification with the lowest number involved one patient
    who actually belonged to the normal category, but the model incorrectly classified
    this patient in the pathological category.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s also look at the predictions in terms of probabilities instead of only
    classes, which was the approach that we used previously. To predict probabilities,
    we can use the `predict_prob` function. We can then look at the first seven rows
    from the test data using the `cbind` function for comparison, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding output, we have probability values for three categories based
    on the model and we also have the predicted category represented by `pred` and
    the actual category represented by `testtarget` in the test data. From the preceding
    output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: For the first sample, the highest probability of `0.993` is for the normal category
    of patients, and that is the reason the predicted class is identified as `0`.
    Since this prediction matches the actual result in the test data, we treat this
    as the correct classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, since the fourth sample shows the highest probability of `0.7197`
    for the third category, the predicted class is labeled as `2`, which turns out
    to be a correct prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the sixth sample has the highest probability of `0.9466` for the first
    category represented by `0`, whereas the actual class is `1`. In this case, our
    model misclassifies the sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will explore the option of improving the classification performance
    of the model to obtain better accuracy. Two key strategies that we can follow
    are to increase the number of hidden layers for building a deeper neural network
    and to change the number of units in the hidden layer. We will explore these options
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we fine-tune the previous classification model to explore its
    functions and see whether its performance can be further improved.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with an additional hidden layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this experiment, we will add an additional hidden layer to the previous
    model. The code and output of the summary of the model is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code and output, we have added a second hidden layer
    with 5 units. In this hidden layer too, we use `relu `as the activation function.
    Note that as a result of this change, we have increased the total number of parameters
    from 203 in the previous model to 239 in this model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compile and then fit the model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, we have compiled the model with same settings
    that we used earlier. We have also kept the setting for the `fit` function the
    same as earlier. The model-output-related information is stored in `model_two`.
    The following diagram provides the plot of accuracy and loss for `model_two`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81a4aa6a-1a94-4f86-a533-997d3a1f4cb0.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracy and loss for training and validation data
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding diagram, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy values based on training and validation data remain relatively
    constant for the first few epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After about 20 epochs, the accuracy for the training data starts to increase
    and then continues to increase for the remaining epochs. The rate of increase,
    however, slows down after about 100 epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the accuracy based on validation data drops for approximately
    50 epochs, then starts to increase, and then becomes more or less constant after
    about 125 epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, loss values initially drop significantly for training data, but after
    about 50 epochs, the rate of decrease drops.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss values for the validation data drop during the initial few epochs and
    then increase and stabilize after about 25 epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using class predictions based on the test data, we can also obtain a confusion
    matrix to assess the performance of this classification model. The following code
    is used to obtain a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding confusion matrix, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: By comparing correct classifications for `0`, `1`, and `2` classes with the
    previous model, we notice that improvement is only seen for class `1`, whereas
    the correct classifications for classes `0` and `2` have, in fact, reduced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall accuracy for this model is 82.1%, which is below the accuracy value
    of 84.2% that we obtained earlier. So, our attempt to make our model slightly
    deeper did not improve accuracy, in this case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with a higher number of units in the hidden layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s fine-tune the first model by changing the number of units in the
    first and only hidden layer using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code and output, we have increased the number of units
    in the first and only hidden layer from `8` to `30`. The total number of parameters
    for this model is `753`. We compile and fit the model with the same setting that
    we used earlier. We store the accuracy and loss values while fitting the model
    in `model_three`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot provides the plot for accuracy and loss for training
    and validation data based on the new classification model, as shown in the following
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/469ca0d2-5a59-477c-b70e-e5e2c7c59d52.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracy and loss for training and validation data
  prefs: []
  type: TYPE_NORMAL
- en: 'We can make the following observations from the preceding plot:'
  prefs: []
  type: TYPE_NORMAL
- en: There is no evidence of overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After about 75 epochs, we do not see any major improvement in the model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The prediction of classes using the test data and confusion matrix is obtained
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding confusion matrix, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: We see improvements in the classification of 1 suspect and 2 pathological categories
    compared to the first model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The correct classifications for the `0`, `1`, and `2` categories are `424`,
    `55`, and `39` respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall accuracy using the test data comes to 85.9%, which is better than
    the first two models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also obtain percentages that show how often this model correctly classifies
    each class by dividing the number of correct classifications in each column by
    the total of that column. We find that this classification model correctly classifies
    normal, suspect, and pathological cases with percentages of about 92.2%, 58.5%,
    and 79.6% respectively. So the model performance is at its highest when correctly
    classifying normal patients; however, the model accuracy drops to just 58.5% when
    correctly classifying patients in the suspect category. From the confusion matrix,
    we can see that the highest number of samples associated with misclassification
    is 35\. Thus, there are 35 patients who actually belong to the suspect category,
    but the classification model incorrectly puts these patients in the normal category.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting using a deeper network with more units in the hidden layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After building three different neural network models with 203, 239, and 753
    parameters respectively, we will now build a deeper neural network model containing
    a larger number of units in the hidden layers. The code used for this experiment
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You can see from the preceding code and output that to try and improve the classification
    performance, this model has a total of 2,793 parameters. This model has three
    hidden layers with 40, 30, and 20 units in the three hidden layers. After each
    hidden layer, we have also added a dropout layer with dropout rates of 40%, 30%,
    and 20% to avoid overfitting—for example, with a dropout rate of 0.4 (or 40%)
    after the first hidden layer, 40% of the units in the first hidden layer are dropped
    to zero at random at the time of training. This helps to avoid any overfitting
    that may occur because of the higher number of units in the hidden layers. We
    compile the model and then run the model with same settings that we used earlier.
    We also store the loss and accuracy values after each epoch in `model_four`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A plot for accuracy and loss values for training and validation data is shown
    in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef6be428-c579-42c5-8a68-0580a8f217aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracy and loss for training and validation data
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding plot, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Training loss and accuracy values stay approximately constant after about 150
    epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy values for validation data are mainly flat after about 75 epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, for loss, we see some divergence between training and validation data
    after about 75 epochs, with loss from validation data increasing gradually. This
    suggests the presence of overfitting after about 75 epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now make predictions using test data and review the resulting confusion
    matrix to assess model performance, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding confusion matrix, the following observations can be made:'
  prefs: []
  type: TYPE_NORMAL
- en: The correct classifications for the `0`, `1`, and `2` categories are `431`,
    `53`, and `40` respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall accuracy comes to 86.9%, which is better than the first three models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also find that this classification model correctly classifies normal,
    suspect, and pathological cases with percentages of about 93.7%, 56.4%, and 81.6%
    respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting by addressing the class imbalance problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this dataset, the number of patients in the normal, suspect, and pathological
    categories is not the same. In the original dataset, the number of normal, suspect,
    and pathological patients are 1,655, 295, and 176, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will make use of the following code to develop a bar plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, we obtain the following bar plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/290b5c4d-c759-4931-bd19-0893b0770a47.png)'
  prefs: []
  type: TYPE_IMG
- en: Proportion of samples in each of the three classes
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding bar plot, the percentages of normal, suspect, and pathological
    patients are approximately 78%, 14%, and 8% respectively. When we compare these
    classes, we observe that the number of normal patients is about 5.6 times (1,655/295)
    greater than the number of suspect patients and about 9.4 times greater than the
    number of pathological patients. The dataset exhibiting a pattern where classes
    are not balanced but contain significantly different numbers of cases per class
    is described as having a class imbalance problem. The class that has a significantly
    higher number of cases may benefit from this at the time of training the model,
    but at the cost of the other classes.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, a classification model may contain a bias toward the class that
    has a significantly higher number of cases, and provide results with higher classification
    accuracy for this class compared to the other classes. When data is influenced
    by such a class imbalance, it is important to address the issue to avoid bias
    in the final classification model. In such situations, we can make use of class
    weights to address the class imbalance issue in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Very often, datasets that are used for developing classification models have
    an unequal number of samples for each class. Such class imbalance issues can easily
    be handled using the `class_weight` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code that includes `class_weight` to incorporate class imbalance information
    is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, we have specified a weight of `1` for
    the `normal` class, a weight of `5.6` for the suspect class, and a weight of `9.4`
    for the pathological class. Assigning these weights creates a level playing field
    for all three categories. We have kept all other settings the same as they were
    in the previous model. After training the network, the loss and accuracy values
    for each epoch are stored in `model_five`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss and accuracy plot for this experiment is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9b36df6-2112-4e24-9488-ac26dcab752e.png)'
  prefs: []
  type: TYPE_IMG
- en: From the accuracy and loss plot based on training and validation data, we do
    not see any obvious pattern suggesting overfitting. After about 100 epochs, we
    do not see any major improvement in model performance in terms of loss and accuracy
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the predictions from the model and the resulting confusion matrix
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding confusion matrix, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The correct classifications for the `0`, `1`, and `2` categories are `358`,
    `74`, and `41` respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall accuracy is now reduced to 78.4%, which is mainly due to the drop
    in accuracy for the normal class, since we increased the weights for the other
    two classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also find that this classification model correctly classifies normal,
    suspect, and pathological cases with percentages of about 77.8%, 78.7%, and 83.7%
    respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly, the biggest gains are for the suspect class, which is now correctly
    classified at the rate of 78.7% versus the earlier rate of only 56.4%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the pathological class, we do not see any major gain or loss in accuracy
    value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These results clearly indicate the influence of using weights to address class
    imbalance problems, as now the classification performance across the three classes
    is more consistent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving and reloading a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know that each time we run a model in Keras, the model starts with different
    starting points due to random initial weights*.* Once we arrive at a model with
    an acceptable level of performance and would like to reuse the same model in the
    future, we can save the model using the `save_model_hdf5` function. We can then
    load this same model using the `load_model_hdf5` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will allow us to save the model architecture and the model
    weights, and, if needed, will allow us to resume the training of the model from
    the previous training session.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to develop a neural network model that helps to
    solve a classification type of problem. We started with a simple classification
    model and explored how to change the number of hidden layers and the number of
    units in the hidden layers. The idea behind exploring and fine-tuning a classification
    model was to illustrate how to explore and improve the performance of the classification
    model. We also saw how to dig deeper to understand the performance of a classification
    model with the help of a confusion matrix. We purposefully looked at a relatively
    smaller neural network model at the beginning of this chapter and finished with
    an example of a relatively deeper neural network model. Deeper networks involving
    several hidden layers can also lead to overfitting problems, where a classification
    model may have excellent performance with training data but doesn't do well with
    testing data. To avoid such situations, we can make use of dropout layers after
    each dense layer, as was illustrated previously. We also illustrated the use of
    class weights for situations where the class imbalance could cause a classification
    model to be more biased toward a specific class. Finally, we also saw how we can
    save the model details for future use when we don't need to rerun the model.
  prefs: []
  type: TYPE_NORMAL
- en: For the models that we used in this chapter, there were certain parameters that
    we kept constant during the various experiments—for example, when compiling a
    model, we always used `adam` as an optimizer. One of the reasons for the popularity
    of using `adam` is that it doesn't require much tuning, and provides good results
    in less time; however, the reader is encouraged to try out other optimizers, such
    as `adagrad`, `adadelta`, and `rmsprop`, and observe the impact on the classification
    performance of the model. Another setting that we kept constant in this chapter
    is the batch size of 32 at the time of training the network. The reader is also
    encouraged to experiment with higher (such as 64) and lower (such as 16) batch
    sizes and observe what impact this has on the classification performance.
  prefs: []
  type: TYPE_NORMAL
- en: As we go on to future chapters, we will gradually develop more and more complex
    and deeper neural network models. Having addressed a classification model where
    the response variables are categorical, in the next chapter, we will go over the
    steps for developing and improving the prediction model for the regression type
    of problems, where the target variable is numeric.
  prefs: []
  type: TYPE_NORMAL
