<html><head></head><body><div id="book-columns"><div id="book-inner"><div class="chapter" title="Chapter 1. Introduction to Deep Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/><span class="koboSpan" id="kobo.1.1">Chapter 1. Introduction to Deep Learning </span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Deep Neural networks are</span><a id="id0" class="indexterm"/><span class="koboSpan" id="kobo.3.1"> currently capable of providing human level solutions to a variety of problems such as image recognition, speech recognition, machine translation, natural language processing, and many more.</span></p><p><span class="koboSpan" id="kobo.4.1">In this chapter, we will look at how neural networks, a biologically-inspired architecture has evolved throughout the years. </span><span class="koboSpan" id="kobo.4.2">Then we will cover some of the important concepts and terminology related to deep learning as a refresher for the subsequent chapters. </span><span class="koboSpan" id="kobo.4.3">Finally we will understand the intuition behind the creative nature of deep networks through a generative model.</span></p><p><span class="koboSpan" id="kobo.5.1">We will cover the following topics in this chapter:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.6.1">Evolution of deep learning</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.7.1">Stochastic Gradient Descent, ReLU, learning rate, and so on</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.8.1">Convolutional network, Recurrent Neural Network and LSTM</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.9.1">Difference between discriminative and generative models</span></li></ul></div><div class="section" title="Evolution of deep learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec08"/><span class="koboSpan" id="kobo.10.1">Evolution of deep learning</span></h1></div></div></div><p><span class="koboSpan" id="kobo.11.1">A lot of the important work on </span><a id="id1" class="indexterm"/><span class="koboSpan" id="kobo.12.1">neural networks happened in the 80's and 90's, but back then computers were slow and datasets very tiny. </span><span class="koboSpan" id="kobo.12.2">The research didn't really find many applications in the real world. </span><span class="koboSpan" id="kobo.12.3">As a result, in the first decade of the 21st century neural networks have completely disappeared from the world of machine learning. </span><span class="koboSpan" id="kobo.12.4">It's only in the last few years, first in speech recognition around 2009, and then in computer vision around 2012, that neural networks made a big comeback (with LeNet, AlexNet, and so on). </span><span class="koboSpan" id="kobo.12.5">What changed?</span></p><p><span class="koboSpan" id="kobo.13.1">Lots of data (big data) and cheap, fast GPU's. </span><span class="koboSpan" id="kobo.13.2">Today, neural networks are everywhere. </span><span class="koboSpan" id="kobo.13.3">So, if you're doing anything with data, analytics, or prediction, deep learning is definitely something that you want to get familiar with.</span></p><p><span class="koboSpan" id="kobo.14.1">See the following figure:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.15.1"><img src="graphics/B08086_01_01.jpg" alt="Evolution of deep learning"/></span><div class="caption"><p><span class="koboSpan" id="kobo.16.1">Figure-1: Evolution of deep learning</span></p></div></div><p><span class="koboSpan" id="kobo.17.1">Deep learning is an exciting</span><a id="id2" class="indexterm"/><span class="koboSpan" id="kobo.18.1"> branch of machine learning that uses data, lots of data, to teach computers how to do things only humans were capable of before, such as recognizing what's in an image, what people are saying when they are talking on their phones, translating a document into another language, and helping robots explore the world and interact with it. </span><span class="koboSpan" id="kobo.18.2">Deep learning has emerged as a central tool to solve perception problems and it's state of the art with computer vision and speech recognition.</span></p><p><span class="koboSpan" id="kobo.19.1">Today many companies have made deep learning a central part of their machine learning toolkit—Facebook, Baidu, Amazon, Microsoft, and Google are all using deep learning in their products because deep learning shines wherever there is lots of data and complex problems to solve.</span></p><p><span class="koboSpan" id="kobo.20.1">Deep learning is the name we often use for "deep neural networks" composed of several layers. </span><span class="koboSpan" id="kobo.20.2">Each layer is made of nodes. </span><span class="koboSpan" id="kobo.20.3">The computation happens in the nodes, where it combines input data with a set of parameters or weights, that either amplify or dampen that input. </span><span class="koboSpan" id="kobo.20.4">These input-weight products are then summed and the sum is passed through the </span><code class="literal"><span class="koboSpan" id="kobo.21.1">activation</span></code><span class="koboSpan" id="kobo.22.1"> function, to determine to what extent the value should progress through the network to affect the final prediction, such as an act of classification. </span><span class="koboSpan" id="kobo.22.2">A layer consists of a row of nodes that that turn on or off as the input is fed through the network. </span><span class="koboSpan" id="kobo.22.3">The input of the first layer becomes the input of the second layer and so on. </span><span class="koboSpan" id="kobo.22.4">Here's a diagram of what neural a network might look like:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.23.1"><img src="graphics/B08086_01_22.jpg" alt="Evolution of deep learning"/></span></div><p><span class="koboSpan" id="kobo.24.1">Let's get familiarized with </span><a id="id3" class="indexterm"/><span class="koboSpan" id="kobo.25.1">some deep neural network concepts and terminology.</span></p><div class="section" title="Sigmoid activation"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec07"/><span class="koboSpan" id="kobo.26.1">Sigmoid activation</span></h2></div></div></div><p><span class="koboSpan" id="kobo.27.1">The sigmoid activation function used in </span><a id="id4" class="indexterm"/><span class="koboSpan" id="kobo.28.1">neural networks has an output boundary of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.29.1">(0, 1)</span></em></span><span class="koboSpan" id="kobo.30.1">, and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.31.1">α</span></em></span><span class="koboSpan" id="kobo.32.1"> is the offset parameter to set the value at which the sigmoid evaluates to 0. </span></p><p><span class="koboSpan" id="kobo.33.1">The sigmoid function often works fine for gradient</span><a id="id5" class="indexterm"/><span class="koboSpan" id="kobo.34.1"> descent as long as the input data </span><span class="emphasis"><em><span class="koboSpan" id="kobo.35.1">x</span></em></span><span class="koboSpan" id="kobo.36.1"> is kept </span><a id="id6" class="indexterm"/><span class="koboSpan" id="kobo.37.1">within a limit. </span><span class="koboSpan" id="kobo.37.2">For large values of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.38.1">x</span></em></span><span class="koboSpan" id="kobo.39.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.40.1">y</span></em></span><span class="koboSpan" id="kobo.41.1"> is constant. </span><span class="koboSpan" id="kobo.41.2">Hence, the derivatives </span><span class="emphasis"><em><span class="koboSpan" id="kobo.42.1">dy/dx</span></em></span><span class="koboSpan" id="kobo.43.1"> (the gradient) equates to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.44.1">0</span></em></span><span class="koboSpan" id="kobo.45.1">, which is often termed as the </span><span class="strong"><strong><span class="koboSpan" id="kobo.46.1">vanishing gradient</span></strong></span><span class="koboSpan" id="kobo.47.1"> problem.</span></p><p><span class="koboSpan" id="kobo.48.1">This is a problem because when the gradient is 0, multiplying it with the loss (actual value - predicted value) also gives us 0 and ultimately networks stop learning.</span></p></div><div class="section" title="Rectified Linear Unit (ReLU)"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec08"/><span class="koboSpan" id="kobo.49.1">Rectified Linear Unit (ReLU)</span></h2></div></div></div><p><span class="koboSpan" id="kobo.50.1">A neural network can be built by combining some linear classifiers with some non-linear functions. </span><span class="koboSpan" id="kobo.50.2">The </span><span class="strong"><strong><span class="koboSpan" id="kobo.51.1">Rectified Linear Unit</span></strong></span><span class="koboSpan" id="kobo.52.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.53.1">ReLU</span></strong></span><span class="koboSpan" id="kobo.54.1">) has become very popular in the last few years. </span><span class="koboSpan" id="kobo.54.2">It computes</span><a id="id7" class="indexterm"/><span class="koboSpan" id="kobo.55.1"> the function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.56.1">f(x)=max(0,x)</span></em></span><span class="koboSpan" id="kobo.57.1">. </span><span class="koboSpan" id="kobo.57.2">In other words, the activation is simply thresholded at zero. </span><span class="koboSpan" id="kobo.57.3">Unfortunately, ReLU units can be fragile during training and can die, as a ReLU </span><a id="id8" class="indexterm"/><span class="koboSpan" id="kobo.58.1">neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again, and so the gradient flowing through the unit will forever be zero from that point on.</span></p><p><span class="koboSpan" id="kobo.59.1">To overcome this problem, a leaky </span><code class="literal"><span class="koboSpan" id="kobo.60.1">ReLU</span></code><span class="koboSpan" id="kobo.61.1"> function will have a small negative slope (of 0.01, or so) instead of zero when </span><span class="emphasis"><em><span class="koboSpan" id="kobo.62.1">x&lt;0</span></em></span><span class="koboSpan" id="kobo.63.1">:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.64.1"><img src="graphics/B08086_01_24.jpg" alt="Rectified Linear Unit (ReLU)"/></span></div><p><span class="koboSpan" id="kobo.65.1">where </span><span class="emphasis"><em><span class="koboSpan" id="kobo.66.1">αα</span></em></span><span class="koboSpan" id="kobo.67.1"> is a small constant.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.68.1"><img src="graphics/B08086_01_02.jpg" alt="Rectified Linear Unit (ReLU)"/></span><div class="caption"><p><span class="koboSpan" id="kobo.69.1">Figure-2: Rectified Linear Unit</span></p></div></div></div><div class="section" title="Exponential Linear Unit (ELU)"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec09"/><span class="koboSpan" id="kobo.70.1">Exponential Linear Unit (ELU)</span></h2></div></div></div><p><span class="koboSpan" id="kobo.71.1">The mean of ReLU </span><a id="id9" class="indexterm"/><span class="koboSpan" id="kobo.72.1">activation is not zero and hence sometimes</span><a id="id10" class="indexterm"/><span class="koboSpan" id="kobo.73.1"> makes learning difficult for the network. </span><span class="koboSpan" id="kobo.73.2">The </span><span class="strong"><strong><span class="koboSpan" id="kobo.74.1">Exponential Linear Unit</span></strong></span><span class="koboSpan" id="kobo.75.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.76.1">ELU)</span></strong></span><span class="koboSpan" id="kobo.77.1"> is similar to ReLU activation function when the input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.78.1">x</span></em></span><span class="koboSpan" id="kobo.79.1"> is positive, but for negative values it is a function bounded by a fixed value </span><span class="emphasis"><em><span class="koboSpan" id="kobo.80.1">-1</span></em></span><span class="koboSpan" id="kobo.81.1">, for </span><span class="emphasis"><em><span class="koboSpan" id="kobo.82.1">α=1</span></em></span><span class="koboSpan" id="kobo.83.1"> (the hyperparameter </span><span class="emphasis"><em><span class="koboSpan" id="kobo.84.1">α</span></em></span><span class="koboSpan" id="kobo.85.1"> controls the value to which an ELU saturates for negative inputs). </span><span class="koboSpan" id="kobo.85.2">This behavior helps to push the mean </span><a id="id11" class="indexterm"/><span class="koboSpan" id="kobo.86.1">activation of neurons closer to zero; that helps to learn representations that are more</span><a id="id12" class="indexterm"/><span class="koboSpan" id="kobo.87.1"> robust to noise.</span></p></div><div class="section" title="Stochastic Gradient Descent (SGD)"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec10"/><span class="koboSpan" id="kobo.88.1">Stochastic Gradient Descent (SGD)</span></h2></div></div></div><p><span class="koboSpan" id="kobo.89.1">Scaling batch gradient descent is cumbersome because it has to compute a lot if the dataset is big, and as a rule </span><a id="id13" class="indexterm"/><span class="koboSpan" id="kobo.90.1">of thumb, if computing your</span><a id="id14" class="indexterm"/><span class="koboSpan" id="kobo.91.1"> loss takes </span><span class="emphasis"><em><span class="koboSpan" id="kobo.92.1">n</span></em></span><span class="koboSpan" id="kobo.93.1"> floating point operations, computing its gradient takes about three times that to compute.</span></p><p><span class="koboSpan" id="kobo.94.1">But in practice we want to be able to train lots of data because on real problems we will always get more gains the more data we use. </span><span class="koboSpan" id="kobo.94.2">And because gradient descent is iterative and has to do that for many steps, that means that in order to update the parameters in a single step, it has to go through all the data samples and then do this iteration over the data tens or hundreds of times.</span></p><p><span class="koboSpan" id="kobo.95.1">Instead of computing the loss over entire data samples for every step, we can compute the average loss for a very small random fraction of the training data. </span><span class="koboSpan" id="kobo.95.2">Think between 1 and 1000 training samples each time. </span><span class="koboSpan" id="kobo.95.3">This technique is called </span><span class="strong"><strong><span class="koboSpan" id="kobo.96.1">Stochastic Gradient Descent</span></strong></span><span class="koboSpan" id="kobo.97.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.98.1">SGD</span></strong></span><span class="koboSpan" id="kobo.99.1">) and is at the core of deep learning. </span><span class="koboSpan" id="kobo.99.2">That's because SGD scales well with both data and model size.</span></p><p><span class="koboSpan" id="kobo.100.1">SGD gets its reputation for being black magic as it has lots of hyper-parameters to play and tune such as initialization parameters, learning rate parameters, decay, and momentum, and you have to get them right.</span></p><p><span class="koboSpan" id="kobo.101.1">AdaGrad is a simple modification of SGD, which implicitly does momentum and learning rate decay by itself. </span><span class="koboSpan" id="kobo.101.2">Using AdaGrad often makes learning less sensitive to hyper-parameters. </span><span class="koboSpan" id="kobo.101.3">But it often tends to be a little worse than precisely tuned SDG with momentum. </span><span class="koboSpan" id="kobo.101.4">It's still a very good option though, if you're just trying to get things to work:</span></p><p> </p><div class="mediaobject"><span class="koboSpan" id="kobo.102.1"><img src="graphics/B08086_01_04.jpg" alt="Stochastic Gradient Descent (SGD)"/></span><div class="caption"><p><span class="koboSpan" id="kobo.103.1">Figure-4a: Loss computation in batch gradient descent and SGD</span></p><p><span class="emphasis"><em><span class="koboSpan" id="kobo.104.1">Source</span></em></span><span class="koboSpan" id="kobo.105.1">: </span><a class="ulink" href="https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochasticgradient- descent"><span class="koboSpan" id="kobo.106.1">https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochasticgradient-
descent</span></a>
</p></div></div><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.107.1"><img src="graphics/B08086_01_05.jpg" alt="Stochastic Gradient Descent (SGD)"/></span><div class="caption"><p><span class="koboSpan" id="kobo.108.1">Figure 4b: Stochastic Gradient Descent and AdaGrad</span></p></div></div><p><span class="koboSpan" id="kobo.109.1">You can notice from </span><span class="emphasis"><em><span class="koboSpan" id="kobo.110.1">Figure 4a</span></em></span><span class="koboSpan" id="kobo.111.1"> that in case of batch gradient descent the </span><code class="literal"><span class="koboSpan" id="kobo.112.1">loss</span></code><span class="koboSpan" id="kobo.113.1">/</span><code class="literal"><span class="koboSpan" id="kobo.114.1">optimization</span></code><span class="koboSpan" id="kobo.115.1"> function is well minimized, whereas SGD calculates the loss by taking a random fraction of the data in each step and often oscillates around that point. </span><span class="koboSpan" id="kobo.115.2">In practice, it's not that</span><a id="id15" class="indexterm"/><span class="koboSpan" id="kobo.116.1"> bad and SGD often converges faster.</span></p></div><div class="section" title="Learning rate tuning"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec11"/><span class="koboSpan" id="kobo.117.1">Learning rate tuning</span></h2></div></div></div><p><span class="koboSpan" id="kobo.118.1">The </span><code class="literal"><span class="koboSpan" id="kobo.119.1">loss</span></code><span class="koboSpan" id="kobo.120.1"> function of the neural network can be related to a surface, where the weights of the network represent each</span><a id="id16" class="indexterm"/><span class="koboSpan" id="kobo.121.1"> direction you can move in. </span><span class="koboSpan" id="kobo.121.2">Gradient descent provides the steps in the current direction of the slope, and the learning rate</span><a id="id17" class="indexterm"/><span class="koboSpan" id="kobo.122.1"> gives the length of each step you take. </span><span class="koboSpan" id="kobo.122.2">The learning rate helps the network to abandons old beliefs for new ones.</span></p><p><span class="koboSpan" id="kobo.123.1">Learning rate tuning can be very strange. </span><span class="koboSpan" id="kobo.123.2">For example, you might think that using a higher learning rate means you learn more or that you learn faster. </span><span class="koboSpan" id="kobo.123.3">That's just not true. </span><span class="koboSpan" id="kobo.123.4">In fact, you can often take a model, lower the learning rate, and get to a better model faster.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.124.1"><img src="graphics/B08086_01_03.jpg" alt="Learning rate tuning"/></span><div class="caption"><p><span class="koboSpan" id="kobo.125.1">Figure-3: Learning rate</span></p></div></div><p><span class="koboSpan" id="kobo.126.1">You might be tempted to look at the learning curve that shows the loss over time to see how quickly the </span><a id="id18" class="indexterm"/><span class="koboSpan" id="kobo.127.1">network learns. </span><span class="koboSpan" id="kobo.127.2">Here the higher learning rate starts faster, but then it plateaus, whereas the lower learning rate keeps on going and gets better. </span><span class="koboSpan" id="kobo.127.3">It is a very familiar picture for anyone who has trained neural networks. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.128.1">Never trust how quickly you learn</span></em></span><span class="koboSpan" id="kobo.129.1">.</span></p></div><div class="section" title="Regularization"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec12"/><span class="koboSpan" id="kobo.130.1">Regularization</span></h2></div></div></div><p><span class="koboSpan" id="kobo.131.1">The first way to prevent </span><a id="id19" class="indexterm"/><span class="koboSpan" id="kobo.132.1">over fitting is by looking at the performance under </span><a id="id20" class="indexterm"/><span class="koboSpan" id="kobo.133.1">validation set, and stopping to train as soon as it stops improving. </span><span class="koboSpan" id="kobo.133.2">It's called early termination, and it's one way to prevent a neural network from over-optimizing on the training set. </span><span class="koboSpan" id="kobo.133.3">Another way is to apply regularization. </span><span class="koboSpan" id="kobo.133.4">Regularizing means applying artificial constraints on the network that implicitly reduce the number of free parameters while not making it more difficult to optimize.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.134.1"><img src="graphics/B08086_01_06.jpg" alt="Regularization"/></span><div class="caption"><p><span class="koboSpan" id="kobo.135.1">Figure 6a: Early termination</span></p></div></div><p><span class="koboSpan" id="kobo.136.1">In the skinny jeans analogy as shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.137.1">Figure 6b</span></em></span><span class="koboSpan" id="kobo.138.1">, think stretch pants. </span><span class="koboSpan" id="kobo.138.2">They fit just as well, but because they're flexible, they don't make things harder to fit in. </span><span class="koboSpan" id="kobo.138.3">The stretch pants of deep</span><a id="id21" class="indexterm"/><span class="koboSpan" id="kobo.139.1"> learning are sometime called </span><span class="strong"><strong><span class="koboSpan" id="kobo.140.1">L2 regularization</span></strong></span><span class="koboSpan" id="kobo.141.1">. </span><span class="koboSpan" id="kobo.141.2">The idea is to add another term to the loss, which</span><a id="id22" class="indexterm"/><span class="koboSpan" id="kobo.142.1"> penalizes large weights.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.143.1"><img src="graphics/B08086_01_07.jpg" alt="Regularization"/></span><div class="caption"><p><span class="koboSpan" id="kobo.144.1">Figure 6b: Stretch pant analogy of deep learning</span></p></div></div><div class="mediaobject"><span class="koboSpan" id="kobo.145.1"><img src="graphics/B08086_01_08.jpg" alt="Regularization"/></span><div class="caption"><p><span class="koboSpan" id="kobo.146.1">Figure 6c: L2 tegularization</span></p></div></div><p><span class="koboSpan" id="kobo.147.1">Currently, in deep learning practice, the widely used approach for preventing overfitting is to feed lots of</span><a id="id23" class="indexterm"/><span class="koboSpan" id="kobo.148.1"> data into the deep network.</span></p></div><div class="section" title="Shared weights and pooling"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec13"/><span class="koboSpan" id="kobo.149.1">Shared weights and pooling</span></h2></div></div></div><p><span class="koboSpan" id="kobo.150.1">Let say an image has a cat in it and it doesn't really matter where the cat is in the image, as it's still an image with</span><a id="id24" class="indexterm"/><span class="koboSpan" id="kobo.151.1"> a cat. </span><span class="koboSpan" id="kobo.151.2">If the network has to learn about cats in the left corner </span><a id="id25" class="indexterm"/><span class="koboSpan" id="kobo.152.1">and about cats in the right corner independently, that's a lot of work that it has to do. </span><span class="koboSpan" id="kobo.152.2">But objects and images are largely the </span><a id="id26" class="indexterm"/><span class="koboSpan" id="kobo.153.1">same whether they're on the left or on the right of the picture. </span><span class="koboSpan" id="kobo.153.2">That's what's called </span><span class="strong"><strong><span class="koboSpan" id="kobo.154.1">translation invariance</span></strong></span><span class="koboSpan" id="kobo.155.1">.</span></p><p><span class="koboSpan" id="kobo.156.1">The way of achieving</span><a id="id27" class="indexterm"/><span class="koboSpan" id="kobo.157.1"> this in networks is called </span><span class="strong"><strong><span class="koboSpan" id="kobo.158.1">weight sharing</span></strong></span><span class="koboSpan" id="kobo.159.1">. </span><span class="koboSpan" id="kobo.159.2">When networks know that two inputs can contain the same kind </span><a id="id28" class="indexterm"/><span class="koboSpan" id="kobo.160.1">of information, then it can share the </span><a id="id29" class="indexterm"/><span class="koboSpan" id="kobo.161.1">weights and train the weights jointly for those inputs. </span><span class="koboSpan" id="kobo.161.2">It is a very important idea. </span><span class="koboSpan" id="kobo.161.3">Statistical invariants are things that don't change on average across time or space, and are everywhere. </span><span class="koboSpan" id="kobo.161.4">For images, the idea of weight sharing will get us to study convolutional networks. </span><span class="koboSpan" id="kobo.161.5">For text and sequences in general, it will lead us to recurrent neural networks:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.162.1"><img src="graphics/B08086_01_09.jpg" alt="Shared weights and pooling"/></span><div class="caption"><p><span class="koboSpan" id="kobo.163.1">Figure 7a: Translation variance</span></p></div></div><div class="mediaobject"><span class="koboSpan" id="kobo.164.1"><img src="graphics/B08086_01_10.jpg" alt="Shared weights and pooling"/></span><div class="caption"><p><span class="koboSpan" id="kobo.165.1">Figure 7b: Weight sharing</span></p></div></div><p><span class="koboSpan" id="kobo.166.1">To reduce the spatial extent of the feature maps in the convolutional pyramid, a very small stride could run and take all the convolutions in a neighborhood and combine them somehow. </span><span class="koboSpan" id="kobo.166.2">This is known as </span><span class="strong"><strong><span class="koboSpan" id="kobo.167.1">pooling</span></strong></span><span class="koboSpan" id="kobo.168.1">.</span></p><p><span class="koboSpan" id="kobo.169.1">In max-pooling as</span><a id="id30" class="indexterm"/><span class="koboSpan" id="kobo.170.1"> shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.171.1">Figure 7d</span></em></span><span class="koboSpan" id="kobo.172.1">, at every point in the feature map, look at a small neighborhood</span><a id="id31" class="indexterm"/><span class="koboSpan" id="kobo.173.1"> around that point and compute the maximum of all the responses around it. </span><span class="koboSpan" id="kobo.173.2">There are some advantages to using max pooling. </span><span class="koboSpan" id="kobo.173.3">First, it doesn't add to your number of parameters. </span><span class="koboSpan" id="kobo.173.4">So, you don't risk an increasing over fitting. </span><span class="koboSpan" id="kobo.173.5">Second, it simply often yields more accurate models. </span><span class="koboSpan" id="kobo.173.6">However, since the convolutions that run below run at a lower stride, the model then becomes a lot more expensive to compute. </span><span class="koboSpan" id="kobo.173.7">Max-pooling extracts the most important feature, whereas average pooling sometimes can't extract good features because it takes all into account and results in an average value that may/may not be important for object detection-type tasks.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.174.1"><img src="graphics/B08086_01_11.jpg" alt="Shared weights and pooling"/></span><div class="caption"><p><span class="koboSpan" id="kobo.175.1">Figure 7c: Pooling</span></p></div></div><div class="mediaobject"><span class="koboSpan" id="kobo.176.1"><img src="graphics/B08086_01_12.jpg" alt="Shared weights and pooling"/></span><div class="caption"><p><span class="koboSpan" id="kobo.177.1">Figure 7d: Max and average pooling</span></p></div></div></div><div class="section" title="Local receptive field"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec14"/><span class="koboSpan" id="kobo.178.1">Local receptive field</span></h2></div></div></div><p><span class="koboSpan" id="kobo.179.1">A simple way to encode the local structure is to connect a submatrix of adjacent input neurons into one</span><a id="id32" class="indexterm"/><span class="koboSpan" id="kobo.180.1"> single hidden neuron belonging to the next layer. </span><span class="koboSpan" id="kobo.180.2">That single hidden neuron represents one local receptive field. </span><span class="koboSpan" id="kobo.180.3">Let's </span><a id="id33" class="indexterm"/><span class="koboSpan" id="kobo.181.1">consider CIFAR-10 images that have an input feature of size [32 x 32 x 3]. </span><span class="koboSpan" id="kobo.181.2">If the receptive field (or the filter size) is 4 x 4, then each neuron in the convolution layer will have weights to a [4 x 4 x 3] region in the input feature, for a total of 4*4*3 = 48 weights (and +1 bias parameter). </span><span class="koboSpan" id="kobo.181.3">The extent of the connectivity along the depth axis must be 3, since this is the depth (or number of channel: RGB) of the input feature.</span></p></div><div class="section" title="Convolutional network (ConvNet)"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec15"/><span class="koboSpan" id="kobo.182.1">Convolutional network (ConvNet)</span></h2></div></div></div><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.183.1">Convolutional Networks</span></strong></span><span class="koboSpan" id="kobo.184.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.185.1">ConvNets</span></strong></span><span class="koboSpan" id="kobo.186.1">) are neural networks that share their parameters/weights across </span><a id="id34" class="indexterm"/><span class="koboSpan" id="kobo.187.1">space. </span><span class="koboSpan" id="kobo.187.2">An image</span><a id="id35" class="indexterm"/><span class="koboSpan" id="kobo.188.1"> can be represented as a flat pancake that has width, height, and depth or number of channel (for RGB: having red, green, and blue channel the depth is 3, whereas for grayscale the depth is 1). </span></p><p><span class="koboSpan" id="kobo.189.1">Now let's slide a tiny neural network with </span><span class="emphasis"><em><span class="koboSpan" id="kobo.190.1">K</span></em></span><span class="koboSpan" id="kobo.191.1"> outputs across the image without changing the weights.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.192.1"><img src="graphics/B08086_01_13.jpg" alt="Convolutional network (ConvNet)"/></span><div class="caption"><p><span class="koboSpan" id="kobo.193.1">Figure 8a: Weight sharing across space</span></p></div></div><div class="mediaobject"><span class="koboSpan" id="kobo.194.1"><img src="graphics/B08086_01_14.jpg" alt="Convolutional network (ConvNet)"/></span><div class="caption"><p><span class="koboSpan" id="kobo.195.1">Figure 8b: Convolutional pyramid with layers of convolution</span></p></div></div><p><span class="koboSpan" id="kobo.196.1">On the output, a different image will be drawn with different width, different height, and different depth (from just R, G, B color channels to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.197.1">K</span></em></span><span class="koboSpan" id="kobo.198.1"> number</span><a id="id36" class="indexterm"/><span class="koboSpan" id="kobo.199.1"> of channels). </span><span class="koboSpan" id="kobo.199.2">This operation is</span><a id="id37" class="indexterm"/><span class="koboSpan" id="kobo.200.1"> known as convolution.</span></p><p><span class="koboSpan" id="kobo.201.1">A ConvNet is going to basically be a deep network with layers of convolutions that stack together to form a pyramid like structure. </span><span class="koboSpan" id="kobo.201.2">You can see from the preceding figure that the network takes an image as an input of dimension (width x height x depth) and then applys convolutions progressively over it to reduce the spatial dimension while increasing the depth, which is roughly equivalent to its semantic complexity. </span><span class="koboSpan" id="kobo.201.3">Let's understand some of the common terminology in convent.</span></p><p><span class="koboSpan" id="kobo.202.1">Each layer or depth in the image stack is called a feature map and patches or kernels are used for mapping three feature maps to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.203.1">K</span></em></span><span class="koboSpan" id="kobo.204.1"> feature maps. </span><span class="koboSpan" id="kobo.204.2">A stride is the number of pixels that is shifted each time you move your filter. </span><span class="koboSpan" id="kobo.204.3">Depending on the type of padding a stride of 1 makes the output roughly the same size as the input. </span><span class="koboSpan" id="kobo.204.4">A stride of 2 makes it about half the size. </span><span class="koboSpan" id="kobo.204.5">In the case of valid padding, a sliding filter don't cross the edge of the image, whereas in same-padding it goes off the edge and is padded with zeros to make the output map size exactly the same size as the input map:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.205.1"><img src="graphics/B08086_01_15.jpg" alt="Convolutional network (ConvNet)"/></span></div><div class="mediaobject"><span class="koboSpan" id="kobo.206.1"><img src="graphics/B08086_01_16.jpg" alt="Convolutional network (ConvNet)"/></span><div class="caption"><p><span class="koboSpan" id="kobo.207.1">Figure 8c: Different terminology related to convolutional network</span></p></div></div></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Deconvolution or transpose convolution"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec09"/><span class="koboSpan" id="kobo.1.1">Deconvolution or transpose convolution</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">In the case of a computer</span><a id="id38" class="indexterm"/><span class="koboSpan" id="kobo.3.1"> vision application where the resolution of final output is required to be larger than the input, deconvolution/transposed convolution is the </span><a id="id39" class="indexterm"/><span class="koboSpan" id="kobo.4.1">de-facto standard. </span><span class="koboSpan" id="kobo.4.2">This layer is used in very popular applications such as GAN, image super-resolution, surface depth estimation from image, optical flow estimation, and so on.</span></p><p><span class="koboSpan" id="kobo.5.1">CNN in general performs down-sampling, that is, they produce output of a lower resolution than the input, whereas in deconvolution the layer up-samples the image to get the same resolution as the input image. </span><span class="koboSpan" id="kobo.5.2">Note since a naive up-sampling inadvertently loses details, a better option is to have a trainable up-sampling convolutional layer whose parameters will change during training.</span></p><p><span class="koboSpan" id="kobo.6.1">Tensorflow method: </span><code class="literal"><span class="koboSpan" id="kobo.7.1">tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding, name)</span></code>
</p><div class="section" title="Recurrent Neural Networks and LSTM"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec16"/><span class="koboSpan" id="kobo.8.1">Recurrent Neural Networks and LSTM</span></h2></div></div></div><p><span class="koboSpan" id="kobo.9.1">The key idea behind </span><span class="strong"><strong><span class="koboSpan" id="kobo.10.1">Recurrent Neural Networks</span></strong></span><span class="koboSpan" id="kobo.11.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.12.1">RNN</span></strong></span><span class="koboSpan" id="kobo.13.1">) is to share parameters over time. </span><span class="koboSpan" id="kobo.13.2">Imagine that </span><a id="id40" class="indexterm"/><span class="koboSpan" id="kobo.14.1">you have a sequence of events, and at each point in time you want to make a decision about what's happened so far in this sequence. </span><span class="koboSpan" id="kobo.14.2">If the sequence is reasonably stationary, you can use the same classifier at each point in time. </span><span class="koboSpan" id="kobo.14.3">That simplifies things a lot already. </span><span class="koboSpan" id="kobo.14.4">But since this is a sequence, you also want to take into</span><a id="id41" class="indexterm"/><span class="koboSpan" id="kobo.15.1"> account the past-everything that happened before that point.</span></p><p><span class="koboSpan" id="kobo.16.1">RNN is going to have a single model responsible for summarizing the past and providing that information to your classifier. </span><span class="koboSpan" id="kobo.16.2">It basically ends up with a network that has a relatively simple repeating pattern, with part of the classifier connecting to the input at each time step and another part called the recurrent connection connecting you to the past at each step, as shown in the following figure:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.17.1"><img src="graphics/B08086_01_18.jpg" alt="Recurrent Neural Networks and LSTM"/></span><div class="caption"><p><span class="koboSpan" id="kobo.18.1">Figure 9a: Recurrent neural network</span></p></div></div><div class="mediaobject"><span class="koboSpan" id="kobo.19.1"><img src="graphics/B08086_01_19.jpg" alt="Recurrent Neural Networks and LSTM"/></span><div class="caption"><p><span class="koboSpan" id="kobo.20.1">Figure-9b: Long short-term memory (LSTM)</span></p></div></div><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.21.1">LSTM</span></strong></span><span class="koboSpan" id="kobo.22.1"> stands for </span><span class="strong"><strong><span class="koboSpan" id="kobo.23.1">long short-term memory</span></strong></span><span class="koboSpan" id="kobo.24.1">. </span><span class="koboSpan" id="kobo.24.2">Now, conceptually, a recurrent neural network consists of a </span><a id="id42" class="indexterm"/><span class="koboSpan" id="kobo.25.1">repetition of simple little units like this, which take as an input the past, a new</span><a id="id43" class="indexterm"/><span class="koboSpan" id="kobo.26.1"> input, and produce a new prediction and connect to the future. </span><span class="koboSpan" id="kobo.26.2">Now, what's in the middle of that is typically a simple set of layers with some weights and linearities.</span></p><p><span class="koboSpan" id="kobo.27.1">In LSTM as shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.28.1">Figure 9b</span></em></span><span class="koboSpan" id="kobo.29.1">, the gating values for each gate get controlled by a tiny logistic regression on the input parameters. </span><span class="koboSpan" id="kobo.29.2">Each of them has its own set of shared parameters. </span><span class="koboSpan" id="kobo.29.3">And there's an additional hyperbolic tension sprinkled to keep the outputs between -1 and 1. </span><span class="koboSpan" id="kobo.29.4">Also it's differentiable all the way, which means it can optimize the parameters very easily. </span><span class="koboSpan" id="kobo.29.5">All these little gates help the model keep its memory for longer when it needs to, and ignore things when it should.</span></p></div><div class="section" title="Deep neural networks"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec17"/><span class="koboSpan" id="kobo.30.1">Deep neural networks</span></h2></div></div></div><p><span class="koboSpan" id="kobo.31.1">The central idea of </span><a id="id44" class="indexterm"/><span class="koboSpan" id="kobo.32.1">deep learning is to add more layers and make your model deeper. </span><span class="koboSpan" id="kobo.32.2">There are lots of good reasons to do that. </span><span class="koboSpan" id="kobo.32.3">One is parameter efficiency. </span><span class="koboSpan" id="kobo.32.4">You can typically get much more performance with fewer parameters by going deeper rather than wider.</span></p><p><span class="koboSpan" id="kobo.33.1">Another one is that a lot of the natural phenomena that you might be interested in, tend to have a hierarchical structure, which deep models naturally capture. </span><span class="koboSpan" id="kobo.33.2">If you poke at a model for images, for example, and visualize what the model learns, you'll often find very simple things at the lowest layers, such as lines or edges.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.34.1"><img src="graphics/B08086_01_20.jpg" alt="Deep neural networks"/></span><div class="caption"><p><span class="koboSpan" id="kobo.35.1">Figure 10a: Deep neural networks</span></p></div></div><div class="mediaobject"><span class="koboSpan" id="kobo.36.1"><img src="graphics/B08086_01_21.jpg" alt="Deep neural networks"/></span><div class="caption"><p><span class="koboSpan" id="kobo.37.1">Figure 10b: Network layers capturing hierarchical structure of image</span></p></div></div><p><span class="koboSpan" id="kobo.38.1">A very typical architecture for a ConvNet is a few layers alternating convolutions and max pooling, followed by</span><a id="id45" class="indexterm"/><span class="koboSpan" id="kobo.39.1"> a few fully connected layers at the top. </span><span class="koboSpan" id="kobo.39.2">The first famous model to use this architecture was LeNet-5 designed by Yann Lecun for character recognition back in 1998.</span></p><p><span class="koboSpan" id="kobo.40.1">Modern convolutional networks such as AlexNet, which famously won the competitive ImageNet object recognition challenge in 2012, use a very similar architecture with a few wrinkles. </span><span class="koboSpan" id="kobo.40.2">Another notable form of pooling is average pooling. </span><span class="koboSpan" id="kobo.40.3">Instead of taking the max, just take an average over the window of pixels around a specific location.</span></p></div><div class="section" title="Discriminative versus generative models"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec18"/><span class="koboSpan" id="kobo.41.1">Discriminative versus generative models</span></h2></div></div></div><p><span class="koboSpan" id="kobo.42.1">A discriminative model learns the conditional probability distribution </span><span class="emphasis"><em><span class="koboSpan" id="kobo.43.1">p(y|x)</span></em></span><span class="koboSpan" id="kobo.44.1"> which could be interpreted as the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.45.1">probability of y given x</span></em></span><span class="koboSpan" id="kobo.46.1">. </span><span class="koboSpan" id="kobo.46.2">A discriminative classifier learns by observing data. </span><span class="koboSpan" id="kobo.46.3">It makes fewer</span><a id="id46" class="indexterm"/><span class="koboSpan" id="kobo.47.1"> assumptions on the distributions, but depends heavily on the quality of the data. </span><span class="koboSpan" id="kobo.47.2">The distribution </span><span class="emphasis"><em><span class="koboSpan" id="kobo.48.1">p(y|x)</span></em></span><span class="koboSpan" id="kobo.49.1"> simply classifies a given example x directly into a label </span><span class="emphasis"><em><span class="koboSpan" id="kobo.50.1">y</span></em></span><span class="koboSpan" id="kobo.51.1">. </span><span class="koboSpan" id="kobo.51.2">For example, in logistic regression all we have to do is to learn weights and bias that would minimize the squared loss.</span></p><p><span class="koboSpan" id="kobo.52.1">Whereas a generative model learns the joint probability distribution </span><span class="emphasis"><em><span class="koboSpan" id="kobo.53.1">p(x,y)</span></em></span><span class="koboSpan" id="kobo.54.1">, where </span><span class="emphasis"><em><span class="koboSpan" id="kobo.55.1">x</span></em></span><span class="koboSpan" id="kobo.56.1"> is the input data and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.57.1">y</span></em></span><span class="koboSpan" id="kobo.58.1"> is the label that you want to classify. </span><span class="koboSpan" id="kobo.58.2">A generative model can generate more samples by itself artificially, based on assumptions about the distribution of data. </span><span class="koboSpan" id="kobo.58.3">For example, in the Naive Bayes' model, we can learn </span><span class="emphasis"><em><span class="koboSpan" id="kobo.59.1">p(x)</span></em></span><span class="koboSpan" id="kobo.60.1"> from data, also </span><span class="emphasis"><em><span class="koboSpan" id="kobo.61.1">p(y)</span></em></span><span class="koboSpan" id="kobo.62.1">, the prior class probabilities, and we can also learn </span><span class="emphasis"><em><span class="koboSpan" id="kobo.63.1">p(x|y)</span></em></span><span class="koboSpan" id="kobo.64.1"> from the data using say maximum likelihood.</span></p><p><span class="koboSpan" id="kobo.65.1">Once we have </span><span class="emphasis"><em><span class="koboSpan" id="kobo.66.1">p(x)</span></em></span><span class="koboSpan" id="kobo.67.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.68.1">p(y)</span></em></span><span class="koboSpan" id="kobo.69.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.70.1">p(x|y)</span></em></span><span class="koboSpan" id="kobo.71.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.72.1">p(x, y)</span></em></span><span class="koboSpan" id="kobo.73.1"> is not difficult to find out. </span><span class="koboSpan" id="kobo.73.2">Now using Bayes' rule, we can replace the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.74.1">p(y|x)</span></em></span><span class="koboSpan" id="kobo.75.1"> with </span><span class="emphasis"><em><span class="koboSpan" id="kobo.76.1">(p(x|y)p(y))/p(x)</span></em></span><span class="koboSpan" id="kobo.77.1">. </span><span class="koboSpan" id="kobo.77.2">And since we are just interested in the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.78.1">arg max</span></em></span><span class="koboSpan" id="kobo.79.1">, the denominator can be removed, as that will be the same for every </span><span class="emphasis"><em><span class="koboSpan" id="kobo.80.1">y</span></em></span><span class="koboSpan" id="kobo.81.1">:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.82.1"><img src="graphics/B08086_01_25.jpg" alt="Discriminative versus generative models"/></span></div><p><span class="koboSpan" id="kobo.83.1">This is the equation we use in generative models, as </span><span class="emphasis"><em><span class="koboSpan" id="kobo.84.1">p(x, y) = p(x | y) p(y)</span></em></span><span class="koboSpan" id="kobo.85.1">, which explicitly models the actual distribution of each class.</span></p><p><span class="koboSpan" id="kobo.86.1">In practice, the discriminative models generally outperform generative models in classification tasks, but the generative model shines over discriminative models in creativity/generation tasks.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/><span class="koboSpan" id="kobo.1.1">Summary</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">So far you have refreshed various concepts related to deep learning and also learned how deep networks have evolved from the arena of supervised tasks of classifying an image, recognizing voice, text, and so on, towards the creative power through generative model. </span><span class="koboSpan" id="kobo.2.2">In the next chapter we will see how deep learning can be used for performing wonderful creativity tasks in the unsupervised domain using </span><span class="strong"><strong><span class="koboSpan" id="kobo.3.1">Generative Adversarial Networks</span></strong></span><span class="koboSpan" id="kobo.4.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.5.1">GANs</span></strong></span><span class="koboSpan" id="kobo.6.1">).</span></p></div></div></div></body></html>