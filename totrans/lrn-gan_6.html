<html><head></head><body><div id="book-columns"><div id="book-inner"><div class="chapter" title="Chapter 6. Taking Machine Learning to Production"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/><span class="koboSpan" id="kobo.1.1">Chapter 6. Taking Machine Learning to Production</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">A lot of machine learning and deep learning tutorials, text books, and videos focus on the training and evaluation of models only. </span><span class="koboSpan" id="kobo.2.2">But how do you take your trained model to production and use it for real-time scenarios or make it available to your customers?</span></p><p><span class="koboSpan" id="kobo.3.1">In this chapter, you will develop a facial image correction system using the </span><code class="literal"><span class="koboSpan" id="kobo.4.1">LFW</span></code><span class="koboSpan" id="kobo.5.1"> dataset to automatically correct corrupted images using your trained GAN model. </span><span class="koboSpan" id="kobo.5.2">Then, you will learn several techniques to deploy machine learning or deep learning models in production, both on data centers and clouds with microservice-based containerized environments. </span><span class="koboSpan" id="kobo.5.3">Finally, you will learn a way to run deep models in a serverless environment and with managed cloud services.</span></p><p><span class="koboSpan" id="kobo.6.1">We will cover the following topics in this chapter:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.7.1">Building an image correction system using DCGAN</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.8.1">The challenges of deploying machine learning models</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.9.1">Microservice architecture with containers</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.10.1">Various approaches to deploying deep learning models</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.11.1">Serving Keras-based deep models on Docker</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.12.1">Deploying deep models on the cloud with GKE</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.13.1">Serverless image recognition with audio using AWS Lambda and Polly</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.14.1">Running face detection with a cloud managed service</span></li></ul></div><div class="section" title="Building an image correction system using DCGAN"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec34"/><span class="koboSpan" id="kobo.15.1">Building an image correction system using DCGAN</span></h1></div></div></div><p><span class="koboSpan" id="kobo.16.1">Image correction and inpainting are related technologies used for filling in or completing missing or corrupted parts of images. </span><span class="koboSpan" id="kobo.16.2">Building a system that can |fill in the missing pieces broadly</span><a id="id271" class="indexterm"/><span class="koboSpan" id="kobo.17.1"> requires</span><a id="id272" class="indexterm"/><span class="koboSpan" id="kobo.18.1"> two pieces of information: </span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.19.1">Contextual information</span></strong></span><span class="koboSpan" id="kobo.20.1">: Helps</span><a id="id273" class="indexterm"/><span class="koboSpan" id="kobo.21.1"> to infer missing pixels based on information provided by the surrounding pixels</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.22.1">Perceptual information</span></strong></span><span class="koboSpan" id="kobo.23.1">: Helps to</span><a id="id274" class="indexterm"/><span class="koboSpan" id="kobo.24.1"> interpret the filled/completed portions as being normal, as seen in real life or other pictures</span></li></ul></div><p><span class="koboSpan" id="kobo.25.1">In this example, we will develop an image correction or completion system with the </span><span class="strong"><strong><span class="koboSpan" id="kobo.26.1">Labeled Face in the Wild</span></strong></span><span class="koboSpan" id="kobo.27.1"> (</span><code class="literal"><span class="koboSpan" id="kobo.28.1">LFW</span></code><span class="koboSpan" id="kobo.29.1">) dataset </span><a id="id275" class="indexterm"/><span class="koboSpan" id="kobo.30.1">using DCGAN. </span><span class="koboSpan" id="kobo.30.2">Refer to </span><a class="link" href="ch02.html" title="Chapter 2. Unsupervised Learning with GAN"><span class="koboSpan" id="kobo.31.1">Chapter 2</span></a><span class="koboSpan" id="kobo.32.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.33.1">Unsupervised Learning with GAN</span></em></span><span class="koboSpan" id="kobo.34.1">, for DCGAN and its architecture.</span></p><p><span class="koboSpan" id="kobo.35.1">Let's define some notation and </span><code class="literal"><span class="koboSpan" id="kobo.36.1">loss</span></code><span class="koboSpan" id="kobo.37.1"> function before diving into the steps for building an image correction system:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.38.1">x</span></em></span><span class="koboSpan" id="kobo.39.1">: Corrupted image.</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.40.1">M</span></em></span><span class="koboSpan" id="kobo.41.1">: Represents a binary mask that has a value of either 1 (meaning the part of the image we want to keep) or 0 (meaning the part of the image we want to complete/correct). </span><span class="koboSpan" id="kobo.41.2">The element-wise multiplication between the two matrices </span><span class="emphasis"><em><span class="koboSpan" id="kobo.42.1">x</span></em></span><span class="koboSpan" id="kobo.43.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.44.1">M</span></em></span><span class="koboSpan" id="kobo.45.1"> represented by </span><span class="inlinemediaobject"><span class="koboSpan" id="kobo.46.1"><img src="graphics/B08086_06_37.jpg" alt="Building an image correction system using DCGAN"/></span></span><span class="koboSpan" id="kobo.47.1"> returns the original part of the image.
</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.48.1">pdata</span></em></span><span class="koboSpan" id="kobo.49.1">: The unknown distribution of sampled data.</span></li></ul></div><p><span class="koboSpan" id="kobo.50.1">Once we have trained the discriminator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.51.1">D(x)</span></em></span><span class="koboSpan" id="kobo.52.1"> and generator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.53.1">G(z)</span></em></span><span class="koboSpan" id="kobo.54.1"> of DCGAN, we can leverage it to complete missing pixels in an image, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.55.1">x</span></em></span><span class="koboSpan" id="kobo.56.1">, by maximizing </span><span class="emphasis"><em><span class="koboSpan" id="kobo.57.1">D(x)</span></em></span><span class="koboSpan" id="kobo.58.1"> over those missing pixels.</span></p><p><span class="koboSpan" id="kobo.59.1">Contextual loss penalizes </span><span class="emphasis"><em><span class="koboSpan" id="kobo.60.1">G(z)</span></em></span><span class="koboSpan" id="kobo.61.1"> for not creating a similar image for the known pixel location in the input image by element-wise subtracting the pixels in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.62.1">x</span></em></span><span class="koboSpan" id="kobo.63.1"> from </span><span class="emphasis"><em><span class="koboSpan" id="kobo.64.1">G(z)</span></em></span><span class="koboSpan" id="kobo.65.1"> and finding the difference between them:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.66.1"><img src="graphics/B08086_06_38.jpg" alt="Building an image correction system using DCGAN"/></span></div><p><span class="koboSpan" id="kobo.67.1">Perceptual loss has the same criterion used in training DCGAN to make sure that the recovered image looks real:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.68.1"><img src="graphics/B08086_06_39.jpg" alt="Building an image correction system using DCGAN"/></span></div><p><span class="koboSpan" id="kobo.69.1">Next, we need to find an image from the generator, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.70.1">G(z)</span></em></span><span class="koboSpan" id="kobo.71.1">, that provides a reasonable reconstruction of the missing pixels. </span><span class="koboSpan" id="kobo.71.2">Then, the completed pixels </span><span class="inlinemediaobject"><span class="koboSpan" id="kobo.72.1"><img src="graphics/B08086_06_41.jpg" alt="Building an image correction system using DCGAN"/></span></span><span class="koboSpan" id="kobo.73.1"> can be added to the</span><a id="id276" class="indexterm"/><span class="koboSpan" id="kobo.74.1"> original pixels to generate the reconstructed image:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.75.1"><img src="graphics/B08086_06_40.jpg" alt="Building an image correction system using DCGAN"/></span></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip03"/><span class="koboSpan" id="kobo.76.1">Tip</span></h3><p><span class="koboSpan" id="kobo.77.1">Training a deep</span><a id="id277" class="indexterm"/><span class="koboSpan" id="kobo.78.1"> convolutional network over a CPU may be prohibitively slow, so it is recommended to use a CUDA-enabled GPU for deep learning activities involving images with convolution or transposed convolution.</span></p></div></div><div class="section" title="Steps for building an image correction system"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec58"/><span class="koboSpan" id="kobo.79.1">Steps for building an image correction system</span></h2></div></div></div><p><span class="koboSpan" id="kobo.80.1">Make sure you have</span><a id="id278" class="indexterm"/><span class="koboSpan" id="kobo.81.1"> downloaded the code for this chapter:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.82.1">The </span><code class="literal"><span class="koboSpan" id="kobo.83.1">DCGAN-ImageCorrection</span></code><span class="koboSpan" id="kobo.84.1"> project will have the following directory structure:</span><div class="mediaobject"><span class="koboSpan" id="kobo.85.1"><img src="graphics/B08086_06_01.jpg" alt="Steps for building an image correction system"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.86.1">Now download the </span><code class="literal"><span class="koboSpan" id="kobo.87.1">LFW</span></code><span class="koboSpan" id="kobo.88.1"> dataset (aligned with deep funnelling) from </span><a class="ulink" href="http://vis-www.cs.umass.edu/lfw"><span class="koboSpan" id="kobo.89.1">http://vis-www.cs.umass.edu/lfw</span></a><span class="koboSpan" id="kobo.90.1"> and extract its content under the </span><code class="literal"><span class="koboSpan" id="kobo.91.1">lfw</span></code><span class="koboSpan" id="kobo.92.1"> directory:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.93.1">wget http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.94.1">tar -xvzf lfw-funneled.tgz</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.95.1">Next, execute </span><code class="literal"><span class="koboSpan" id="kobo.96.1">create_tfrecords.py</span></code><span class="koboSpan" id="kobo.97.1"> to generate the TensorFlow standard format from the </span><code class="literal"><span class="koboSpan" id="kobo.98.1">LFW</span></code><span class="koboSpan" id="kobo.99.1"> images. </span><span class="koboSpan" id="kobo.99.2">Modify the path of your </span><code class="literal"><span class="koboSpan" id="kobo.100.1">LFW</span></code><span class="koboSpan" id="kobo.101.1"> image location in the Python file:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.102.1">base_path = &lt;Path to lfw directory&gt;</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.103.1">python create_tfrecords.py</span></strong></span>
</pre></div><p><span class="koboSpan" id="kobo.104.1">This will generate the </span><code class="literal"><span class="koboSpan" id="kobo.105.1">tfrecords</span></code><span class="koboSpan" id="kobo.106.1"> file in the </span><code class="literal"><span class="koboSpan" id="kobo.107.1">data</span></code><span class="koboSpan" id="kobo.108.1"> directory as follows:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.109.1"><img src="graphics/B08086_06_02.jpg" alt="Steps for building an image correction system"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.110.1">Now train the DCGAN model by executing the following command:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.111.1">python train_generate.py</span></strong></span>
</pre></div><p><span class="koboSpan" id="kobo.112.1">You can </span><a id="id279" class="indexterm"/><span class="koboSpan" id="kobo.113.1">modify the </span><code class="literal"><span class="koboSpan" id="kobo.114.1">max_itr</span></code><span class="koboSpan" id="kobo.115.1"> attribute in the Python files to determine the maximum number of iterations the training should continue for. </span><span class="koboSpan" id="kobo.115.2">Once the training is going on, after every 5,000 iterations, you will find the generated images under the </span><code class="literal"><span class="koboSpan" id="kobo.116.1">lfw-gen</span></code><span class="koboSpan" id="kobo.117.1"> directory, as follows:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.118.1"><img src="graphics/B08086_06_03.jpg" alt="Steps for building an image correction system"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.119.1">Finally, you can use the trained DCGAN model to correct corrupted images. </span><span class="koboSpan" id="kobo.119.2">You need to put your corrupted images under the </span><code class="literal"><span class="koboSpan" id="kobo.120.1">complete_src</span></code><span class="koboSpan" id="kobo.121.1"> directory and execute the following command:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.122.1">python image_correction.py --is_complete True --latest_ckpt  &lt;checkpoint number&gt;</span></strong></span>
</pre></div></li></ol></div><p><span class="koboSpan" id="kobo.123.1">You can also alter the type of masking by specifying </span><code class="literal"><span class="koboSpan" id="kobo.124.1">center</span></code><span class="koboSpan" id="kobo.125.1"> or </span><code class="literal"><span class="koboSpan" id="kobo.126.1">random</span></code><span class="koboSpan" id="kobo.127.1"> with the </span><code class="literal"><span class="koboSpan" id="kobo.128.1">masktype</span></code><span class="koboSpan" id="kobo.129.1"> attribute in the preceding command.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.130.1"><img src="graphics/B08086_06_04.jpg" alt="Steps for building an image correction system"/></span></div><p><span class="koboSpan" id="kobo.131.1">The preceding</span><a id="id280" class="indexterm"/><span class="koboSpan" id="kobo.132.1"> command will generate corrected or completed images under the complete directory, as follows:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.133.1"><img src="graphics/B08086_06_05.jpg" alt="Steps for building an image correction system"/></span></div></div><div class="section" title="Challenges of deploying models to production"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec59"/><span class="koboSpan" id="kobo.134.1">Challenges of deploying models to production</span></h2></div></div></div><p><span class="koboSpan" id="kobo.135.1">Most researchers </span><a id="id281" class="indexterm"/><span class="koboSpan" id="kobo.136.1">and machine learning practitioners focus on the training and evaluation side of machine learning or deep learning models. </span><span class="koboSpan" id="kobo.136.2">A real-world analogy of building models during research is similar to cooking at home, whereas building or deploying that model in production is like cooking for a wide variety of customers (whose taste changes over time) in a restaurant.</span></p><p><span class="koboSpan" id="kobo.137.1">Some of the common challenges that often arise during the production deployment of models are as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.138.1">Scalability</span></strong></span><span class="koboSpan" id="kobo.139.1">: A real-world</span><a id="id282" class="indexterm"/><span class="koboSpan" id="kobo.140.1"> production environment is quite different from a training or research environment. </span><span class="koboSpan" id="kobo.140.2">You often need to cater for a high volume of requests without impacting the performance. </span><span class="koboSpan" id="kobo.140.3">Your model should automatically scale up/out based on the traffic and then scale down/in when the traffic is low.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.141.1">Automated model training or updates</span></strong></span><span class="koboSpan" id="kobo.142.1">: Real-world data has temporal dynamics and, as your model enters the real-world production environment, the data starts looking </span><a id="id283" class="indexterm"/><span class="koboSpan" id="kobo.143.1">different from that on which the model was originally trained. </span><span class="koboSpan" id="kobo.143.2">This means you need to retrain your model (sometimes automatically) and then switch between models seamlessly.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.144.1">Interoperation between development languages</span></strong></span><span class="koboSpan" id="kobo.145.1">: Often, two different people or groups are responsible for researching (training) the model and productionizing it, and the</span><a id="id284" class="indexterm"/><span class="koboSpan" id="kobo.146.1"> language for research may be different from the preferred language for production. </span><span class="koboSpan" id="kobo.146.2">This causes a bunch of problems, as machine learning models have different implementations in different programming languages, even though the model is essentially the same.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.147.1">Knowledge of training set metadata</span></strong></span><span class="koboSpan" id="kobo.148.1">: Real-world production data might have missing values </span><a id="id285" class="indexterm"/><span class="koboSpan" id="kobo.149.1">and you will need to apply a missing value imputation technique to deal with this. </span><span class="koboSpan" id="kobo.149.2">Although, in production systems, you don't keep information about training data, but in order to correctly impute the missing values arriving in production test samples, you have to store the knowledge of the training set statistics needed for imputation.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.150.1">Real-time evaluation of model performance</span></strong></span><span class="koboSpan" id="kobo.151.1">: Evaluation of a model's performance in production </span><a id="id286" class="indexterm"/><span class="koboSpan" id="kobo.152.1">often requires you to collect ground truth data (or other real-time metrics) and generate dynamic pages as a model processes more data. </span><span class="koboSpan" id="kobo.152.2">Also, you </span><a id="id287" class="indexterm"/><span class="koboSpan" id="kobo.153.1">might need to carry out </span><span class="strong"><strong><span class="koboSpan" id="kobo.154.1">A/B</span></strong></span><span class="koboSpan" id="kobo.155.1"> testing by deploying two or more models serving the same functionality simultaneously to evaluate performance in production.</span></li></ul></div></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Microservice architecture using containers"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec35"/><span class="koboSpan" id="kobo.1.1">Microservice architecture using containers</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">In traditional </span><span class="strong"><strong><span class="koboSpan" id="kobo.3.1">monolithic</span></strong></span><span class="koboSpan" id="kobo.4.1"> architecture, an</span><a id="id288" class="indexterm"/><span class="koboSpan" id="kobo.5.1"> application puts all its functionality into single packages such as EAR or WAR </span><a id="id289" class="indexterm"/><span class="koboSpan" id="kobo.6.1">and deploys it on an application server (such as JBoss, Tomcat, or WebLogic). </span><span class="koboSpan" id="kobo.6.2">Even though a monolithic application has separate and distinguishable components, all are packaged under one roof.</span></p><div class="section" title="Drawbacks of monolithic architecture"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec60"/><span class="koboSpan" id="kobo.7.1">Drawbacks of monolithic architecture</span></h2></div></div></div><p><span class="koboSpan" id="kobo.8.1">Some of the common pitfalls of monolithic design are as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.9.1">The functional </span><a id="id290" class="indexterm"/><span class="koboSpan" id="kobo.10.1">components in monolithic architecture are packed under one application and are not isolated. </span><span class="koboSpan" id="kobo.10.2">Hence, changing a single component requires updating an entire application, thereby bringing the entire application to a halt. </span><span class="koboSpan" id="kobo.10.3">This is not desirable in a production scenario.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.11.1">Scaling a monolithic application is not efficient, because to scale you have to deploy each copy of the application (WAR or EAR) in various servers that will utilize the same amount of resources.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.12.1">Often, in the real world, one or two functional components are heavily used compared to other components. </span><span class="koboSpan" id="kobo.12.2">But in monolithic design, all components will utilize the same resources, hence it is hard to segregate highly used components to improve the performance of the overall application.</span></li></ul></div><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.13.1">Microservices</span></strong></span><span class="koboSpan" id="kobo.14.1"> is a technique that decomposes large software projects into loosely coupled modules/services that communicate with each other through simple APIs. </span><span class="koboSpan" id="kobo.14.2">A microservices-based architecture puts each functionality into separate services to overcome the drawbacks of monolithic design.</span></p></div><div class="section" title="Benefits of microservice architecture"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec61"/><span class="koboSpan" id="kobo.15.1">Benefits of microservice architecture</span></h2></div></div></div><p><span class="koboSpan" id="kobo.16.1">Some of the advantages of microservice design pattern are as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.17.1">Single responsibility principle</span></strong></span><span class="koboSpan" id="kobo.18.1">: Microservice architecture </span><a id="id291" class="indexterm"/><span class="koboSpan" id="kobo.19.1">makes sure that each functionality is deployed or exposed as a separate service</span><a id="id292" class="indexterm"/><span class="koboSpan" id="kobo.20.1"> through simple APIs.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.21.1">High scalability</span></strong></span><span class="koboSpan" id="kobo.22.1">: Highly </span><a id="id293" class="indexterm"/><span class="koboSpan" id="kobo.23.1">utilized or demanding services can be deployed in multiple servers to serve a high number of requests/traffic, thereby enhancing performance. </span><span class="koboSpan" id="kobo.23.2">This is difficult to achieve with single, large monolithic services.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.24.1">Improves fault tolerance</span></strong></span><span class="koboSpan" id="kobo.25.1">: The failure of </span><a id="id294" class="indexterm"/><span class="koboSpan" id="kobo.26.1">single modules/services doesn't affect the larger application, and you can quickly recover or bring back the failed module, since the module is running as a separate service. </span><span class="koboSpan" id="kobo.26.2">Whereas a monolithic or bulky service having errors in one component/module can impact other modules/functionality.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.27.1">Freedom of the technology stack</span></strong></span><span class="koboSpan" id="kobo.28.1">: Microservices allows you to choose the technology that is best </span><a id="id295" class="indexterm"/><span class="koboSpan" id="kobo.29.1">suited for a particular functionality and helps you to try out a new technology stack on an individual service.</span></li></ul></div><p><span class="koboSpan" id="kobo.30.1">The best way to deploy microservices-based applications is inside containers.</span></p><div class="section" title="Containers"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec06"/><span class="koboSpan" id="kobo.31.1">Containers</span></h3></div></div></div><p><span class="koboSpan" id="kobo.32.1">Containers are shareable,</span><a id="id296" class="indexterm"/><span class="koboSpan" id="kobo.33.1"> lightweight processes that sit on top of a host operating system and share the kernels (binaries and libraries) of the host OS. </span><span class="koboSpan" id="kobo.33.2">Containers solve a bunch of complex problems simultaneously</span><a id="id297" class="indexterm"/><span class="koboSpan" id="kobo.34.1"> through a layer of abstraction. </span><span class="koboSpan" id="kobo.34.2">The popularity of containers can be described by the wonderful triad: </span><span class="emphasis"><em><span class="koboSpan" id="kobo.35.1">Isolation</span></em></span><span class="koboSpan" id="kobo.36.1">! </span><span class="emphasis"><em><span class="koboSpan" id="kobo.37.1">Portability</span></em></span><span class="koboSpan" id="kobo.38.1">! </span><span class="emphasis"><em><span class="koboSpan" id="kobo.39.1">Repeatability</span></em></span><span class="koboSpan" id="kobo.40.1">!.</span></p></div><div class="section" title="Docker"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec07"/><span class="koboSpan" id="kobo.41.1">Docker</span></h3></div></div></div><p><span class="koboSpan" id="kobo.42.1">Docker is one of the hottest open </span><a id="id298" class="indexterm"/><span class="koboSpan" id="kobo.43.1">source projects and is a very popular containerization engine that allows a convenient way to pack</span><a id="id299" class="indexterm"/><span class="koboSpan" id="kobo.44.1"> your service/application with all dependencies together to be deployed locally or in the cloud.</span></p></div><div class="section" title="Kubernetes"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec08"/><span class="koboSpan" id="kobo.45.1">Kubernetes</span></h3></div></div></div><p><span class="koboSpan" id="kobo.46.1">Kubernetes is another open source project at Google and it provides orchestration to containers, allowing automated </span><a id="id300" class="indexterm"/><span class="koboSpan" id="kobo.47.1">horizontal</span><a id="id301" class="indexterm"/><span class="koboSpan" id="kobo.48.1"> scaling, service discovery, load balancing, and much more. </span><span class="koboSpan" id="kobo.48.2">Simply put, it automates the management of your containerized applications/services in the cloud.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/><span class="koboSpan" id="kobo.49.1">Note</span></h3><p><span class="koboSpan" id="kobo.50.1">We will refer to Docker as the container engine for illustration in this section, although other container engines would also provide similar features or functionality.</span></p></div></div></div></div><div class="section" title="Benefits of using containers"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec62"/><span class="koboSpan" id="kobo.51.1">Benefits of using containers</span></h2></div></div></div><p><span class="koboSpan" id="kobo.52.1">Some of the pros of using container are discussed as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.53.1">Continuous deployment and testing</span></strong></span><span class="koboSpan" id="kobo.54.1">: Often, release life cycles involving different environments, </span><a id="id302" class="indexterm"/><span class="koboSpan" id="kobo.55.1">such as development and production, have some</span><a id="id303" class="indexterm"/><span class="koboSpan" id="kobo.56.1"> differences because of different package versions or dependencies. </span><span class="koboSpan" id="kobo.56.2">Docker fills that gap by ensuring consistent environments by maintaining all configurations and dependencies internally from development to production. </span><span class="koboSpan" id="kobo.56.3">As a result, you can use the same container from development to production without any discrepancies or manual intervention.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.57.1">Multi-cloud platforms</span></strong></span><span class="koboSpan" id="kobo.58.1">: One of the greatest benefits of Docker is its portability across various</span><a id="id304" class="indexterm"/><span class="koboSpan" id="kobo.59.1"> environments and platforms. </span><span class="koboSpan" id="kobo.59.2">All major cloud providers, such as </span><span class="strong"><strong><span class="koboSpan" id="kobo.60.1">Amazon Web Services</span></strong></span><span class="koboSpan" id="kobo.61.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.62.1">AWS</span></strong></span><span class="koboSpan" id="kobo.63.1">) and </span><span class="strong"><strong><span class="koboSpan" id="kobo.64.1">Google Compute Platform</span></strong></span><span class="koboSpan" id="kobo.65.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.66.1">GCP</span></strong></span><span class="koboSpan" id="kobo.67.1">), have embraced Docker's availability by adding individual support (AWS ECS or Google GKE). </span><span class="koboSpan" id="kobo.67.2">Docker containers can run inside a Virtual Machine VM instance (Amazon EC2 or Google Compute Engine) provided the host OS supports Docker.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.68.1">Version control</span></strong></span><span class="koboSpan" id="kobo.69.1">: Docker</span><a id="id305" class="indexterm"/><span class="koboSpan" id="kobo.70.1"> containers work as a version control system just like </span><code class="literal"><span class="koboSpan" id="kobo.71.1">Git</span></code><span class="koboSpan" id="kobo.72.1">/</span><code class="literal"><span class="koboSpan" id="kobo.73.1">SVN</span></code><span class="koboSpan" id="kobo.74.1"> repositories, so that you can commit changes to your Docker images and version control then.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.75.1">Isolation and security</span></strong></span><span class="koboSpan" id="kobo.76.1">: Docker ensures that applications running inside containers are completely isolated and </span><a id="id306" class="indexterm"/><span class="koboSpan" id="kobo.77.1">segregated from each other, granting complete control over traffic flow and management. </span><span class="koboSpan" id="kobo.77.2">No Docker container has access to the processes running inside another container. </span><span class="koboSpan" id="kobo.77.3">From an architectural standpoint, each container has its own set of resources.</span></li></ul></div><p><span class="koboSpan" id="kobo.78.1">You can combine an advanced machine learning or deep learning application with the deployment capabilities of containers to make the system more efficient and shareable.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Various approaches to deploying deep models"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec36"/><span class="koboSpan" id="kobo.1.1">Various approaches to deploying deep models</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Machine learning is exciting and fun! </span><span class="koboSpan" id="kobo.2.2">It has its challenges though, both during the modeling part and also during deployment, when you want your model to serve real people and systems. </span></p><p><span class="koboSpan" id="kobo.3.1">Deploying machine</span><a id="id307" class="indexterm"/><span class="koboSpan" id="kobo.4.1"> learning models into production can be done in a wide variety of ways and the different ways of productionalizing</span><a id="id308" class="indexterm"/><span class="koboSpan" id="kobo.5.1"> machine learning models is really governed by various factors:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.6.1">Do you want your model to be part of real-time streaming analytics or batch analytics?</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.7.1">Do you want to have multiple models serving the same functionality or do you need to perform A/B testing on your models?</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.8.1">How often do you want your model to be updated?</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.9.1">How do you scale your model based on traffic?</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.10.1">How do you integrate with other services or fit the ML service into a pipeline?</span></li></ul></div><div class="section" title="Approach 1 - offline modeling and microservice-based containerized deployment"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec63"/><span class="koboSpan" id="kobo.11.1">Approach 1 - offline modeling and microservice-based containerized deployment</span></h2></div></div></div><p><span class="koboSpan" id="kobo.12.1">In this approach, you will train and evaluate your model offline and then use your pretrained model to build a </span><a id="id309" class="indexterm"/><span class="koboSpan" id="kobo.13.1">RESTful service and deploy it inside a container. </span><span class="koboSpan" id="kobo.13.2">Next, you can run the container within your data center or cloud depending on cost, security, scaling, and infrastructure requirement. </span><span class="koboSpan" id="kobo.13.3">This approach is well suited to when your machine learning or deep learning service will have continuous traffic flow and need to dynamically scale based on spikes in requests.</span></p></div><div class="section" title="Approach 2 - offline modeling and serverless deployment"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec64"/><span class="koboSpan" id="kobo.14.1">Approach 2 - offline modeling and serverless deployment</span></h2></div></div></div><p><span class="koboSpan" id="kobo.15.1">In this approach, you</span><a id="id310" class="indexterm"/><span class="koboSpan" id="kobo.16.1"> will train your model offline and deploy your service in a serverless environment such as AWS Lambda (where you will be charged only for invoking the API; you don't have to pay for running containers or VM instances on an hourly/minute basis). </span><span class="koboSpan" id="kobo.16.2">This approach is well suited to when your model service will not be used continuously but will instead be invoked after a certain time. </span><span class="koboSpan" id="kobo.16.3">But even if there is continuous traffic flow (depending on the number of requests you hit), this approach might still be cost-effective compared to approach 1.</span></p></div><div class="section" title="Approach 3 - online learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec65"/><span class="koboSpan" id="kobo.17.1">Approach 3 - online learning</span></h2></div></div></div><p><span class="koboSpan" id="kobo.18.1">Sometimes, you need to perform real-time streaming analytics by integrating your machine learning service with the </span><a id="id311" class="indexterm"/><span class="koboSpan" id="kobo.19.1">pipeline (such as putting it at the consumer end of a message queue having IOT sensor data). </span><span class="koboSpan" id="kobo.19.2">Data might change very frequently in real-time streaming situations. </span><span class="koboSpan" id="kobo.19.3">In that scenario, offline model training is not the right choice. </span><span class="koboSpan" id="kobo.19.4">Instead, you need your model to adapt automatically to the data as it is sees it—that is it will update weights/parameters based on data using something like SGD or its mini batch variant.</span></p></div><div class="section" title="Approach 4 - using a managed machine learning service"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec66"/><span class="koboSpan" id="kobo.20.1">Approach 4 - using a managed machine learning service</span></h2></div></div></div><p><span class="koboSpan" id="kobo.21.1">This approach is well suited to when</span><a id="id312" class="indexterm"/><span class="koboSpan" id="kobo.22.1"> you don't have the resources or team members to build machine learning models in-house. </span><span class="koboSpan" id="kobo.22.2">Instead, you utilize the available cloud-based managed machine learning or deep learning services, such as Google Cloud ML, Azure ML, AWS Rekognition, AWS Polly, Google Cloud Vision, and so on to fulfill your requirements by invoking simple API calls.</span></p><p><span class="koboSpan" id="kobo.23.1">Next, we will illustrate the deployment approaches mentioned previously through hands-on examples.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Serving Keras-based deep models on Docker"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec37"/><span class="koboSpan" id="kobo.1.1">Serving Keras-based deep models on Docker</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">In this example, we will </span><a id="id313" class="indexterm"/><span class="koboSpan" id="kobo.3.1">build an image identification</span><a id="id314" class="indexterm"/><span class="koboSpan" id="kobo.4.1"> system with a pretrained Keras InceptionV3 model and deploy it on a container in the local machine. </span><span class="koboSpan" id="kobo.4.2">Refer to </span><a class="link" href="ch04.html" title="Chapter 4. Building Realistic Images from Your Text"><span class="koboSpan" id="kobo.5.1">Chapter 4</span></a><span class="koboSpan" id="kobo.6.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">Building Realistic Images from Your Text</span></em></span><span class="koboSpan" id="kobo.8.1">, for more information about pretrained models. </span><span class="koboSpan" id="kobo.8.2">Our pretrained Keras model will run inside a Docker container exposed as a REST API using Flask.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.9.1"><img src="graphics/B08086_06_06.jpg" alt="Serving Keras-based deep models on Docker"/></span></div><p><span class="koboSpan" id="kobo.10.1">Make sure you have the </span><code class="literal"><span class="koboSpan" id="kobo.11.1">keras-microservice</span></code><span class="koboSpan" id="kobo.12.1"> project available and then perform the following steps to run a Keras-based deep model inside a </span><code class="literal"><span class="koboSpan" id="kobo.13.1">docker</span></code><span class="koboSpan" id="kobo.14.1"> container:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.15.1">First, check that the Dockerfile is in your current working directory and then build a Docker image:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.16.1">docker build -t keras-recognition-service .</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.17.1">Once the </span><a id="id315" class="indexterm"/><span class="koboSpan" id="kobo.18.1">Docker image is built </span><a id="id316" class="indexterm"/><span class="koboSpan" id="kobo.19.1">successfully, use the image to run a container with the </span><code class="literal"><span class="koboSpan" id="kobo.20.1">docker run</span></code><span class="koboSpan" id="kobo.21.1"> command:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.22.1">docker run -it --rm -d -p &lt;host port&gt;:&lt;container port&gt; -v &lt;host path&gt;:&lt;container path&gt; keras-recognition-service</span></strong></span>
</pre></div><p><span class="koboSpan" id="kobo.23.1">For example:</span></p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.24.1">docker run -it --rm -d -p 5000:80 -v /Users/kuntalg/knowledge:/deep/model keras-recognition-service</span></strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/><span class="koboSpan" id="kobo.25.1">Note</span></h3><p><span class="koboSpan" id="kobo.26.1">Inside the </span><code class="literal"><span class="koboSpan" id="kobo.27.1">docker</span></code><span class="koboSpan" id="kobo.28.1"> container, the </span><a id="id317" class="indexterm"/><span class="koboSpan" id="kobo.29.1">Keras model is running on port </span><code class="literal"><span class="koboSpan" id="kobo.30.1">5001</span></code><span class="koboSpan" id="kobo.31.1"> inside a WSGI HTTP Python server named </span><span class="strong"><strong><span class="koboSpan" id="kobo.32.1">Gunicorn</span></strong></span><span class="koboSpan" id="kobo.33.1">, which is load balanced by an </span><span class="strong"><strong><span class="koboSpan" id="kobo.34.1">Nginx</span></strong></span>
<a id="id318" class="indexterm"/><span class="koboSpan" id="kobo.35.1"> proxy server on port </span><code class="literal"><span class="koboSpan" id="kobo.36.1">80</span></code><span class="koboSpan" id="kobo.37.1">. </span><span class="koboSpan" id="kobo.37.2">We used the </span><code class="literal"><span class="koboSpan" id="kobo.38.1">–p</span></code><span class="koboSpan" id="kobo.39.1"> attribute previously to map the host port with the container port. </span><span class="koboSpan" id="kobo.39.2">Also, we used the </span><code class="literal"><span class="koboSpan" id="kobo.40.1">-v</span></code><span class="koboSpan" id="kobo.41.1"> volume attribute to map the host path with the container path, so that we can load the pretrained model from this path.</span></p></div></div><p><span class="koboSpan" id="kobo.42.1">Now it's time to test our image identification service by executing the </span><code class="literal"><span class="koboSpan" id="kobo.43.1">test.sh</span></code><span class="koboSpan" id="kobo.44.1"> script. </span><span class="koboSpan" id="kobo.44.2">The script contains a </span><code class="literal"><span class="koboSpan" id="kobo.45.1">curl</span></code><span class="koboSpan" id="kobo.46.1"> command to call and test the REST API of our exposed image identification service:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.47.1">#!/bin/bash
echo "Prediction for 1st Image:"
echo "--------------------------------"
(echo -n '{"data": "'; base64 test-1.jpg; echo '"}') | curl -X POST -H "Content-Type: application/json" -d @- http://127.0.0.1:5000

echo "Prediction for 2nd Image:"
echo "--------------------------------"
(echo -n '{"data": "'; base64 test-1.jpg; echo '"}') | curl -X POST -H "Content-Type: application/json" -d @- http://127.0.0.1:5000</span></pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.48.1">Finally, execute the script to generate a prediction from our Keras service:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.49.1"> ./test_requests.sh</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.50.1"><img src="graphics/B08086_06_07.jpg" alt="Serving Keras-based deep models on Docker"/></span></div></li></ol></div><p><span class="koboSpan" id="kobo.51.1">Voila! </span><span class="koboSpan" id="kobo.51.2">We have </span><a id="id319" class="indexterm"/><span class="koboSpan" id="kobo.52.1">successfully deployed our first Keras-based deep</span><a id="id320" class="indexterm"/><span class="koboSpan" id="kobo.53.1"> learning model inside a container.</span></p></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Deploying a deep model on the cloud with GKE"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec38"/><span class="koboSpan" id="kobo.1.1">Deploying a deep model on the cloud with GKE</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Once the deep</span><a id="id321" class="indexterm"/><span class="koboSpan" id="kobo.3.1"> learning model is created, deployed inside the container, and generating predictions locally, it's time to take the model to the cloud (for example, Google Cloud in this example) using Docker and Kubernetes.</span></p><p><span class="koboSpan" id="kobo.4.1">Perform the following steps to</span><a id="id322" class="indexterm"/><span class="koboSpan" id="kobo.5.1"> take a locally created containerized model to the cloud:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.6.1">Sign up for a Google Cloud trial account (</span><a class="ulink" href="https://cloud.google.com/free"><span class="koboSpan" id="kobo.7.1">https://cloud.google.com/free</span></a><span class="koboSpan" id="kobo.8.1">) and then create a </span><span class="strong"><strong><span class="koboSpan" id="kobo.9.1">New Project</span></strong></span><span class="koboSpan" id="kobo.10.1"> by typing a relevant </span><span class="strong"><strong><span class="koboSpan" id="kobo.11.1">Project name</span></strong></span><span class="koboSpan" id="kobo.12.1"> of your choice:</span><div class="mediaobject"><span class="koboSpan" id="kobo.13.1"><img src="graphics/B08086_06_08.jpg" alt="Deploying a deep model on the cloud with GKE"/></span></div><p><span class="koboSpan" id="kobo.14.1">Please note down the </span><span class="strong"><strong><span class="koboSpan" id="kobo.15.1">project ID</span></strong></span><span class="koboSpan" id="kobo.16.1"> containing your </span><span class="strong"><strong><span class="koboSpan" id="kobo.17.1">Project name</span></strong></span><span class="koboSpan" id="kobo.18.1"> along with some numeric digits of the format </span><code class="literal"><span class="koboSpan" id="kobo.19.1">&lt;project name&gt;-xxxxxx</span></code><span class="koboSpan" id="kobo.20.1">. </span><span class="koboSpan" id="kobo.20.2">We will need the </span><span class="strong"><strong><span class="koboSpan" id="kobo.21.1">project ID</span></strong></span><span class="koboSpan" id="kobo.22.1"> later for deploying our local model to the c</span></p></li><li class="listitem"><span class="koboSpan" id="kobo.23.1">d </span><a id="id323" class="indexterm"/><span class="koboSpan" id="kobo.24.1">SDK on your machine (</span><a class="ulink" href="https://cloud.google.com/sdk"><span class="koboSpan" id="kobo.25.1">https://cloud.google.com/sdk</span></a><span class="koboSpan" id="kobo.26.1">). </span><span class="koboSpan" id="kobo.26.2">And then install kubectl to</span><a id="id324" class="indexterm"/><span class="koboSpan" id="kobo.27.1"> manage the Kubernetes cluster:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.28.1">gcloud components install kubectl</span></strong></span>
</pre></div><p><span class="koboSpan" id="kobo.29.1">The </span><code class="literal"><span class="koboSpan" id="kobo.30.1">gcloud</span></code><span class="koboSpan" id="kobo.31.1"> command is included in the Google Cloud SDK.</span></p></li><li class="listitem"><span class="koboSpan" id="kobo.32.1">Set some environment variables with the </span><code class="literal"><span class="koboSpan" id="kobo.33.1">gcloud</span></code><span class="koboSpan" id="kobo.34.1"> command-line tool </span><code class="literal"><span class="koboSpan" id="kobo.35.1">config</span></code><span class="koboSpan" id="kobo.36.1"> command:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.37.1">gcloud config set project &lt;project ID&gt;</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.38.1">gcloud config set compute/zone &lt;zone name such as us-central1-b&gt;</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.39.1">export PROJECT_ID="$(gcloud config get-value project -q)</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.40.1">Now build the docker image with a tag or version (</span><code class="literal"><span class="koboSpan" id="kobo.41.1">v1</span></code><span class="koboSpan" id="kobo.42.1"> in this example):</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.43.1">docker build -t gcr.io/&lt;project ID&gt;/keras-recognition-service:v1 .</span></strong></span>
</pre></div><p><span class="koboSpan" id="kobo.44.1">For example:</span></p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.45.1">docker build -t gcr.io/deeplearning-123456/keras-recognition-service:v1 .</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.46.1">Next, upload</span><a id="id325" class="indexterm"/><span class="koboSpan" id="kobo.47.1"> the image built previously with the </span><code class="literal"><span class="koboSpan" id="kobo.48.1">docker push</span></code><span class="koboSpan" id="kobo.49.1"> command to the Google Container Registry:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.50.1">gcloud docker -- push gcr.io/&lt;project ID&gt;/keras-recognition-service:v1</span></strong></span>
</pre></div><p><span class="koboSpan" id="kobo.51.1">For example:</span></p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.52.1">gcloud docker -- push gcr.io/deeplearning-123456/keras-recognition-service:v1</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.53.1"><img src="graphics/B08086_06_09.jpg" alt="Deploying a deep model on the cloud with GKE"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.54.1">Once the container image is stored in a registry, we need to create a container cluster by specifying the number of Compute Engine VM instances. </span><span class="koboSpan" id="kobo.54.2">This cluster will be orchestrated and managed by Kubernetes. </span><span class="koboSpan" id="kobo.54.3">Execute the following command to create a two-node cluster named </span><code class="literal"><span class="koboSpan" id="kobo.55.1">dl-cluster</span></code><span class="koboSpan" id="kobo.56.1">:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.57.1">gcloud container clusters create dl-cluster --num-nodes=2</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.58.1">We will use the Kubernetes </span><code class="literal"><span class="koboSpan" id="kobo.59.1">kubectl</span></code><span class="koboSpan" id="kobo.60.1"> command-line tool to deploy and run an application on a Container Engine cluster, listening on port </span><code class="literal"><span class="koboSpan" id="kobo.61.1">80</span></code><span class="koboSpan" id="kobo.62.1">:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.63.1">gcloud container clusters get-credentials dl-cluster</span></strong></span>

<span class="strong"><strong><span class="koboSpan" id="kobo.64.1">kubectl run keras-recognition-service --image=gcr.io/deeplearning-123456/keras-recognition-service:v1 --port 80</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.65.1"><img src="graphics/B08086_06_10.jpg" alt="Deploying a deep model on the cloud with GKE"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.66.1">Now attach the </span><a id="id326" class="indexterm"/><span class="koboSpan" id="kobo.67.1">application running inside the container cluster to the load balancer, so that we can expose our image identification service to a real- world user:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.68.1">kubectl expose deployment keras-recognition-service --type=LoadBalancer --port 80 --target-port 80</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.69.1">Next, run the following </span><code class="literal"><span class="koboSpan" id="kobo.70.1">kubectl</span></code><span class="koboSpan" id="kobo.71.1"> command to get the external IP of our service: </span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.72.1">kubectl get service</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.73.1">Finally, execute the following command to get a prediction from our image recognition service hosted inside a container cluster hosted on the cloud:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.74.1">(echo -n '{"data": "'; base64 test-1.jpeg; echo '"}') | curl -X POST -H "Content-Type: application/json" -d @- http://&lt;External IP&gt;:80</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.75.1"><img src="graphics/B08086_06_11.png.jpg" alt="Deploying a deep model on the cloud with GKE"/></span></div></li></ol></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Serverless image recognition with audio using AWS Lambda and Polly"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec39"/><span class="koboSpan" id="kobo.1.1">Serverless image recognition with audio using AWS Lambda and Polly</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">In this example, we will build an</span><a id="id327" class="indexterm"/><span class="koboSpan" id="kobo.3.1"> audio-based image prediction system using TensorFlow pretrained InceptionV3 model and </span><a id="id328" class="indexterm"/><span class="koboSpan" id="kobo.4.1">deploy it on a serverless environment of AWS Lambda. </span><span class="koboSpan" id="kobo.4.2">We will run our image prediction code</span><a id="id329" class="indexterm"/><span class="koboSpan" id="kobo.5.1"> on AWS Lambda and load our pretrained model from S3, and then expose the service to our real-world</span><a id="id330" class="indexterm"/><span class="koboSpan" id="kobo.6.1"> customer through the AWS API gateway.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.7.1"><img src="graphics/B08086_06_12.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div><p><span class="koboSpan" id="kobo.8.1">Perform the following steps to build an audio-based image recognition system on a serverless platform:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.9.1">Sign up for an </span><a id="id331" class="indexterm"/><span class="koboSpan" id="kobo.10.1">AWS trial account (</span><a class="ulink" href="https://aws.amazon.com/free/"><span class="koboSpan" id="kobo.11.1">https://aws.amazon.com/free/</span></a><span class="koboSpan" id="kobo.12.1">) and navigate to the </span><span class="strong"><strong><span class="koboSpan" id="kobo.13.1">IAM</span></strong></span><span class="koboSpan" id="kobo.14.1"> service to create a new role for </span><a id="id332" class="indexterm"/><span class="koboSpan" id="kobo.15.1">AWS Lambda. </span><span class="koboSpan" id="kobo.15.2">Attach two new managed policies: </span><span class="strong"><strong><span class="koboSpan" id="kobo.16.1">S3FullAccess</span></strong></span><span class="koboSpan" id="kobo.17.1"> and </span><span class="strong"><strong><span class="koboSpan" id="kobo.18.1">PollyFullAccess</span></strong></span><span class="koboSpan" id="kobo.19.1"> alongside the inline policy of </span><span class="strong"><strong><span class="koboSpan" id="kobo.20.1">lambda_basic_execution</span></strong></span><span class="koboSpan" id="kobo.21.1">.</span><div class="mediaobject"><span class="koboSpan" id="kobo.22.1"><img src="graphics/B08086_06_13.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.23.1">Next, create</span><a id="id333" class="indexterm"/><span class="koboSpan" id="kobo.24.1"> an S3 bucket, where we will store our lambda code (consisting of custom Python packages such</span><a id="id334" class="indexterm"/><span class="koboSpan" id="kobo.25.1"> as </span><code class="literal"><span class="koboSpan" id="kobo.26.1">numpy</span></code><span class="koboSpan" id="kobo.27.1">, </span><code class="literal"><span class="koboSpan" id="kobo.28.1">scipy</span></code><span class="koboSpan" id="kobo.29.1">, </span><code class="literal"><span class="koboSpan" id="kobo.30.1">tensorflow</span></code><span class="koboSpan" id="kobo.31.1">, and so on). </span><span class="koboSpan" id="kobo.31.2">Also</span><a id="id335" class="indexterm"/><span class="koboSpan" id="kobo.32.1"> create three folders within your S3 bucket:</span><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal"><span class="koboSpan" id="kobo.33.1">code</span></code><span class="koboSpan" id="kobo.34.1">: We will </span><a id="id336" class="indexterm"/><span class="koboSpan" id="kobo.35.1">store our code for the lambda environment here</span></li><li class="listitem" style="list-style-type: disc"><code class="literal"><span class="koboSpan" id="kobo.36.1">audio</span></code><span class="koboSpan" id="kobo.37.1">: Our prediction audio will be saved in this location</span></li><li class="listitem" style="list-style-type: disc"><code class="literal"><span class="koboSpan" id="kobo.38.1">model</span></code><span class="koboSpan" id="kobo.39.1">: We will store our pretrained model in this location</span></li></ul></div><div class="mediaobject"><span class="koboSpan" id="kobo.40.1"><img src="graphics/B08086_06_14.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.41.1">Download</span><a id="id337" class="indexterm"/><span class="koboSpan" id="kobo.42.1"> the pretrained TensorFlow </span><a id="id338" class="indexterm"/><span class="koboSpan" id="kobo.43.1">model (</span><a class="ulink" href="http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz"><span class="koboSpan" id="kobo.44.1">http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz</span></a><span class="koboSpan" id="kobo.45.1">), extract</span><a id="id339" class="indexterm"/><span class="koboSpan" id="kobo.46.1"> it, and then</span><a id="id340" class="indexterm"/><span class="koboSpan" id="kobo.47.1"> upload the following files to the </span><code class="literal"><span class="koboSpan" id="kobo.48.1">S3</span></code><span class="koboSpan" id="kobo.49.1"> bucket </span><a id="id341" class="indexterm"/><span class="koboSpan" id="kobo.50.1">under the </span><code class="literal"><span class="koboSpan" id="kobo.51.1">model</span></code><span class="koboSpan" id="kobo.52.1"> directory:</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.53.1">classify_image_graph_def.pb
imagenet_synset_to_human_label_map.txt
imagenet_synset_to_human_label_map.txt</span></pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.54.1"><img src="graphics/B08086_06_15.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.55.1">The </span><code class="literal"><span class="koboSpan" id="kobo.56.1">lambda_tensorflow.zip</span></code><span class="koboSpan" id="kobo.57.1"> contains a </span><code class="literal"><span class="koboSpan" id="kobo.58.1">classify.py</span></code><span class="koboSpan" id="kobo.59.1"> file that will be invoked</span><a id="id342" class="indexterm"/><span class="koboSpan" id="kobo.60.1"> during the </span><code class="literal"><span class="koboSpan" id="kobo.61.1">lambda</span></code><span class="koboSpan" id="kobo.62.1"> function </span><a id="id343" class="indexterm"/><span class="koboSpan" id="kobo.63.1">execution. </span><span class="koboSpan" id="kobo.63.2">Change the bucket name, and inside the </span><code class="literal"><span class="koboSpan" id="kobo.64.1">classify.py</span></code><span class="koboSpan" id="kobo.65.1">, zip it again and upload it to the </span><a id="id344" class="indexterm"/><span class="koboSpan" id="kobo.66.1">S3 bucket under</span><a id="id345" class="indexterm"/><span class="koboSpan" id="kobo.67.1"> the </span><code class="literal"><span class="koboSpan" id="kobo.68.1">code</span></code><span class="koboSpan" id="kobo.69.1"> directory:</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.70.1">def lambda_handler(event, context):
    
    if not os.path.exists('/tmp/imagenetV3/'):
        os.makedirs('/tmp/imagenetV3/')

    # imagenet_synset_to_human_label_map.txt:
    #   Map from synset ID to a human readable string.
    </span><span class="koboSpan" id="kobo.70.2">strBucket = 'kg-image-prediction'
    strKey = 'models/imagenetV3/imagenet_synset_to_human_label_map.txt'
    strFile = '/tmp/imagenetV3/imagenet_synset_to_human_label_map.txt'
    downloadFromS3(strBucket,strKey,strFile)  
    print(strFile)

    # imagenet_2012_challenge_label_map_proto.pbtxt:
    #   Text representation of a protocol buffer mapping a label to synset ID.
    
    </span><span class="koboSpan" id="kobo.70.3">strBucket = 'kg-image-prediction'
    strKey = 'models/imagenetV3/imagenet_2012_challenge_label_map_proto.pbtxt'
    strFile = '/tmp/imagenetV3/imagenet_2012_challenge_label_map_proto.pbtxt'
    downloadFromS3(strBucket,strKey,strFile)
    print(strFile) 

    # classify_image_graph_def.pb:
    #   Binary representation of the GraphDef protocol buffer.
    </span><span class="koboSpan" id="kobo.70.4">strBucket = 'kg-image-prediction'
    strKey = 'models/imagenetV3/classify_image_graph_def.pb'
    strFile = '/tmp/imagenetV3/classify_image_graph_def.pb'
    downloadFromS3(strBucket,strKey,strFile)
    print(strFile)
    data = base64.b64decode(event['base64Image'])
    imageName= event['imageName']
    
    image=io.BytesIO(data)
    strBucket = 'kg-image-prediction'
    
    strKey = 'raw-image/tensorflow/'+imageName+'.png'
    uploadToS3(image, strBucket, strKey)
    print("Image file uploaded to S3")
    
    
    audioKey=imageName+'.mp3'
    print(audioKey)
    print("Ready to Run inference")

    strBucket = 'kg-image-prediction'
    strKey = 'raw-image/tensorflow/'+imageName+'.png'
    imageFile = '/tmp/'+imageName+'.png'
    downloadFromS3(strBucket,strKey,imageFile)
    print("Image downloaded from S3")

    strResult = run_inference_on_image(imageFile)

    # Invoke AWS Polly to generate Speech from text
    polly_client=boto3.client('polly')
    response = polly_client.synthesize_speech(Text =strResult,OutputFormat = "mp3",VoiceId = "Joanna")
    if "AudioStream" in response:
        output = os.path.join("/tmp", audioKey)
        with open(output, "wb") as file:
            file.write(response["AudioStream"].read())

    #Upload speech to S3
    print("Ready upload to S3 audio")
    strBucket = 'kg-image-prediction'
    strKey = 'audio/'+audioKey
    strFile = '/tmp/'+audioKey

    with open(strFile, 'rb') as data:
        uploadToS3(data,strBucket,strKey)
    # Clean up directory
    os.remove(imageFile)
    os.remove(strFile)
    
    
    return strResult</span></pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.71.1"><img src="graphics/B08086_06_16.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.72.1">Now </span><a id="id346" class="indexterm"/><span class="koboSpan" id="kobo.73.1">navigate to the </span><span class="strong"><strong><span class="koboSpan" id="kobo.74.1">Lambda</span></strong></span><span class="koboSpan" id="kobo.75.1"> service</span><a id="id347" class="indexterm"/><span class="koboSpan" id="kobo.76.1"> from the </span><a id="id348" class="indexterm"/><span class="koboSpan" id="kobo.77.1">web console to create a new Lambda function from scratch. </span><span class="koboSpan" id="kobo.77.2">Provide </span><span class="strong"><strong><span class="koboSpan" id="kobo.78.1">Name*</span></strong></span><span class="koboSpan" id="kobo.79.1"> and </span><span class="strong"><strong><span class="koboSpan" id="kobo.80.1">Description</span></strong></span><span class="koboSpan" id="kobo.81.1"> for the function; choose </span><span class="strong"><strong><span class="koboSpan" id="kobo.82.1">Runtime</span></strong></span><span class="koboSpan" id="kobo.83.1"> as </span><span class="strong"><strong><span class="koboSpan" id="kobo.84.1">Python 2.7</span></strong></span><span class="koboSpan" id="kobo.85.1"> and attach the </span><code class="literal"><span class="koboSpan" id="kobo.86.1">IAM</span></code><span class="koboSpan" id="kobo.87.1"> role created previously to this Lambda function.</span><div class="mediaobject"><span class="koboSpan" id="kobo.88.1"><img src="graphics/B08086_06_17.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.89.1">And then </span><a id="id349" class="indexterm"/><span class="koboSpan" id="kobo.90.1">specify</span><a id="id350" class="indexterm"/><span class="koboSpan" id="kobo.91.1"> the code (</span><code class="literal"><span class="koboSpan" id="kobo.92.1">lambda_tensorflow.zip</span></code><span class="koboSpan" id="kobo.93.1">) location in your Lambda function configuration:</span><div class="mediaobject"><span class="koboSpan" id="kobo.94.1"><img src="graphics/B08086_06_18.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.95.1">Also increase</span><a id="id351" class="indexterm"/><span class="koboSpan" id="kobo.96.1"> the </span><span class="strong"><strong><span class="koboSpan" id="kobo.97.1">Memory(MB)</span></strong></span><span class="koboSpan" id="kobo.98.1"> and </span><span class="strong"><strong><span class="koboSpan" id="kobo.99.1">Timeout</span></strong></span><span class="koboSpan" id="kobo.100.1"> of your Lambda function under the </span><span class="strong"><strong><span class="koboSpan" id="kobo.101.1">Advance Settings</span></strong></span><span class="koboSpan" id="kobo.102.1"> tab. </span><span class="koboSpan" id="kobo.102.2">The first</span><a id="id352" class="indexterm"/><span class="koboSpan" id="kobo.103.1"> time, the lambda execution will take some time due to the loading of the pretrained model from S3.</span><div class="mediaobject"><span class="koboSpan" id="kobo.104.1"><img src="graphics/B08086_06_19.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.105.1">Next,</span><a id="id353" class="indexterm"/><span class="koboSpan" id="kobo.106.1"> create a new API by </span><a id="id354" class="indexterm"/><span class="koboSpan" id="kobo.107.1">navigating to the </span><span class="strong"><strong><span class="koboSpan" id="kobo.108.1">API Gateway</span></strong></span><span class="koboSpan" id="kobo.109.1"> service:</span><div class="mediaobject"><span class="koboSpan" id="kobo.110.1"><img src="graphics/B08086_06_20.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.111.1">Then, click</span><a id="id355" class="indexterm"/><span class="koboSpan" id="kobo.112.1"> the </span><span class="strong"><strong><span class="koboSpan" id="kobo.113.1">Binary Support</span></strong></span><span class="koboSpan" id="kobo.114.1"> tab on</span><a id="id356" class="indexterm"/><span class="koboSpan" id="kobo.115.1"> the left panel of your API to add the following content type:</span><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.116.1">image/png</span></strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.117.1">image/jpeg</span></strong></span></li></ul></div><div class="mediaobject"><span class="koboSpan" id="kobo.118.1"><img src="graphics/B08086_06_21.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.119.1">Next, create a </span><span class="strong"><strong><span class="koboSpan" id="kobo.120.1">New Child Resource</span></strong></span><span class="koboSpan" id="kobo.121.1"> by </span><a id="id357" class="indexterm"/><span class="koboSpan" id="kobo.122.1">specifying the </span><span class="strong"><strong><span class="koboSpan" id="kobo.123.1">Resource Path</span></strong></span><span class="koboSpan" id="kobo.124.1"> (for example, </span><code class="literal"><span class="koboSpan" id="kobo.125.1">tensorflow-predict</span></code><span class="koboSpan" id="kobo.126.1">):</span><div class="mediaobject"><span class="koboSpan" id="kobo.127.1"><img src="graphics/B08086_06_22.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.128.1">Next, add a </span><a id="id358" class="indexterm"/><span class="koboSpan" id="kobo.129.1">method (</span><span class="strong"><strong><span class="koboSpan" id="kobo.130.1">POST</span></strong></span><span class="koboSpan" id="kobo.131.1">) to your child resource by clicking </span><span class="strong"><strong><span class="koboSpan" id="kobo.132.1">Create Method</span></strong></span><span class="koboSpan" id="kobo.133.1"> from the </span><span class="strong"><strong><span class="koboSpan" id="kobo.134.1">Action</span></strong></span> <a id="id359" class="indexterm"/><span class="koboSpan" id="kobo.135.1">menu. </span><span class="koboSpan" id="kobo.135.2">Add the Lambda function we created previously to AMP with this API resource. </span><span class="koboSpan" id="kobo.135.3">You</span><a id="id360" class="indexterm"/><span class="koboSpan" id="kobo.136.1"> may need to specify the correct region to find your Lambda function from the dropdown.</span></li><li class="listitem"><span class="koboSpan" id="kobo.137.1">Once the </span><span class="strong"><strong><span class="koboSpan" id="kobo.138.1">POST</span></strong></span><span class="koboSpan" id="kobo.139.1"> method is created, click on </span><span class="strong"><strong><span class="koboSpan" id="kobo.140.1">Integration Request</span></strong></span><span class="koboSpan" id="kobo.141.1"> and expand the </span><span class="strong"><strong><span class="koboSpan" id="kobo.142.1">Body Mapping Templates</span></strong></span><span class="koboSpan" id="kobo.143.1"> tab. </span><span class="koboSpan" id="kobo.143.2">Under </span><span class="strong"><strong><span class="koboSpan" id="kobo.144.1">Request body passthrough</span></strong></span><span class="koboSpan" id="kobo.145.1">, choose </span><span class="strong"><strong><span class="koboSpan" id="kobo.146.1">When there are no templates defined (recommended)</span></strong></span><span class="koboSpan" id="kobo.147.1">. </span><span class="koboSpan" id="kobo.147.2">Then, add an image/jpeg under </span><span class="strong"><strong><span class="koboSpan" id="kobo.148.1">Content-Type</span></strong></span><span class="koboSpan" id="kobo.149.1"> and add the following under the </span><span class="strong"><strong><span class="koboSpan" id="kobo.150.1">Generate template</span></strong></span><span class="koboSpan" id="kobo.151.1"> section:</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.152.1">{
"base64Image": "$input.body",
"imageName": "$input.params(imageName)"
}</span></pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.153.1"><img src="graphics/B08086_06_23.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.154.1">Finally,</span><a id="id361" class="indexterm"/><span class="koboSpan" id="kobo.155.1"> deploy the API from the </span><span class="strong"><strong><span class="koboSpan" id="kobo.156.1">Action</span></strong></span><span class="koboSpan" id="kobo.157.1"> menu and define the </span><span class="strong"><strong><span class="koboSpan" id="kobo.158.1">Stage name</span></strong></span><span class="koboSpan" id="kobo.159.1"> (for example, </span><code class="literal"><span class="koboSpan" id="kobo.160.1">prod</span></code><span class="koboSpan" id="kobo.161.1"> or </span><code class="literal"><span class="koboSpan" id="kobo.162.1">dev</span></code><span class="koboSpan" id="kobo.163.1">). </span><span class="koboSpan" id="kobo.163.2">Once your API is deployed, you will find your API URL as follows:</span><p>
<code class="literal"><span class="koboSpan" id="kobo.164.1">https://&lt;API ID&gt;.execute-api.&lt;region&gt;.amazonaws.com/</span></code>
</p></li><li class="listitem"><span class="koboSpan" id="kobo.165.1">Next, access </span><a id="id362" class="indexterm"/><span class="koboSpan" id="kobo.166.1">your API from</span><a id="id363" class="indexterm"/><span class="koboSpan" id="kobo.167.1"> the REST client, such as </span><span class="strong"><strong><span class="koboSpan" id="kobo.168.1">POSTMAN</span></strong></span><span class="koboSpan" id="kobo.169.1"> shown in this example, to invoke your image prediction service. </span><span class="koboSpan" id="kobo.169.2">In the </span><span class="strong"><strong><span class="koboSpan" id="kobo.170.1">API Request</span></strong></span><span class="koboSpan" id="kobo.171.1">, set the </span><span class="strong"><strong><span class="koboSpan" id="kobo.172.1">Content-Type</span></strong></span><span class="koboSpan" id="kobo.173.1"> as </span><span class="strong"><strong><span class="koboSpan" id="kobo.174.1">image/jpeg</span></strong></span><span class="koboSpan" id="kobo.175.1"> and add the parameter name </span><span class="strong"><strong><span class="koboSpan" id="kobo.176.1">imageName</span></strong></span><span class="koboSpan" id="kobo.177.1"> with a value (such as </span><code class="literal"><span class="koboSpan" id="kobo.178.1">animal</span></code><span class="koboSpan" id="kobo.179.1">). </span><span class="koboSpan" id="kobo.179.2">Add an image in the body as a </span><code class="literal"><span class="koboSpan" id="kobo.180.1">binary</span></code><span class="koboSpan" id="kobo.181.1"> file that our service will predict:</span><p>
<code class="literal"><span class="koboSpan" id="kobo.182.1">https://&lt;API ID&gt;.execute-api.&lt;region&gt;.amazonaws.com/prod/tensorflow-predict?imageName=animal</span></code>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.183.1"><img src="graphics/B08086_06_24.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div></li></ol></div><p><span class="koboSpan" id="kobo.184.1">Voila! </span><span class="koboSpan" id="kobo.184.2">You</span><a id="id364" class="indexterm"/><span class="koboSpan" id="kobo.185.1"> will see the following</span><a id="id365" class="indexterm"/><span class="koboSpan" id="kobo.186.1"> output from the serverless service in</span><a id="id366" class="indexterm"/><span class="koboSpan" id="kobo.187.1"> your Postman response:</span></p><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.188.1">"The image is identified as giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca (with score = 0.89107)"</span></strong></span>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.189.1"><img src="graphics/B08086_06_25.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div><p><span class="koboSpan" id="kobo.190.1">Also, audio</span><a id="id367" class="indexterm"/><span class="koboSpan" id="kobo.191.1"> of the predicted response will be generated and stored in the S3 bucket under the </span><code class="literal"><span class="koboSpan" id="kobo.192.1">audio</span></code><span class="koboSpan" id="kobo.193.1"> folder.</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.194.1"><img src="graphics/B08086_06_26.png.jpg" alt="Serverless image recognition with audio using AWS Lambda and Polly"/></span></div><div class="section" title="Steps to modify code and packages for lambda environments"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec67"/><span class="koboSpan" id="kobo.195.1">Steps to modify code and packages for lambda environments</span></h2></div></div></div><p><span class="koboSpan" id="kobo.196.1">If you need to add an </span><a id="id368" class="indexterm"/><span class="koboSpan" id="kobo.197.1">additional Python package for your service or update any existing package, perform the following:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.198.1">Launch an EC2 instance with </span><span class="strong"><strong><span class="koboSpan" id="kobo.199.1">Amazon Linux AMI 2017.03.1 (HVM), SSD Volume Type</span></strong></span><span class="koboSpan" id="kobo.200.1">:</span><div class="mediaobject"><span class="koboSpan" id="kobo.201.1"><img src="graphics/B08086_06_27.png.jpg" alt="Steps to modify code and packages for lambda environments"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.202.1">Log in to the EC2 instance and copy the current lambda code in the instance. </span><span class="koboSpan" id="kobo.202.2">Then, create a directory and extract the ZIP file inside that directory:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.203.1">mkdir lambda</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.204.1">cd lambda</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.205.1">unzip lambda_tensorflow.zip</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.206.1"><img src="graphics/B08086_06_28.png.jpg" alt="Steps to modify code and packages for lambda environments"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.207.1">To update any </span><a id="id369" class="indexterm"/><span class="koboSpan" id="kobo.208.1">existing package, first remove it and then install it with the </span><code class="literal"><span class="koboSpan" id="kobo.209.1">pip</span></code><span class="koboSpan" id="kobo.210.1"> command. </span><span class="koboSpan" id="kobo.210.2">To add a new package install with </span><code class="literal"><span class="koboSpan" id="kobo.211.1">pip</span></code><span class="koboSpan" id="kobo.212.1"> (if that package depends on shared </span><code class="literal"><span class="koboSpan" id="kobo.213.1">.so</span></code><span class="koboSpan" id="kobo.214.1"> libraries, then you need to create a </span><code class="literal"><span class="koboSpan" id="kobo.215.1">lib</span></code><span class="koboSpan" id="kobo.216.1"> folder and copy those files in it from the </span><code class="literal"><span class="koboSpan" id="kobo.217.1">//usr/lib</span></code><span class="koboSpan" id="kobo.218.1"> and </span><code class="literal"><span class="koboSpan" id="kobo.219.1">/usr/lib64</span></code><span class="koboSpan" id="kobo.220.1"> directory):</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.221.1">rm -rf tensorflow*</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.222.1">pip install tensorflow==1.2.0 -t /home/ec2-user/lambda</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.223.1">Then create the ZIP file of the full directory:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.224.1">zip –r lambda_tensorflow.zip *</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.225.1">And finally, copy the ZIP file to S3 and update your Lambda function by mentioning the new ZIP file location on S3.</span></li></ol></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/><span class="koboSpan" id="kobo.226.1">Note</span></h3><p><span class="koboSpan" id="kobo.227.1">You may need to strip down some packages or irrelevant directories from packages to make sure the total size of unzipped files in the </span><code class="literal"><span class="koboSpan" id="kobo.228.1">code</span></code><span class="koboSpan" id="kobo.229.1"> directory is less than 250 MB; otherwise, Lambda won't deploy your code.</span></p></div></div><p><span class="koboSpan" id="kobo.230.1">Go to the following link for more information about custom package deployment on Lambda</span><a class="ulink" href="http://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html"><span class="koboSpan" id="kobo.231.1">http://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html</span></a><span class="koboSpan" id="kobo.232.1">.</span></p></div><div class="section" title="Running face detection with a cloud managed service"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec68"/><span class="koboSpan" id="kobo.233.1">Running face detection with a cloud managed service</span></h2></div></div></div><p><span class="koboSpan" id="kobo.234.1">In this example, we will use </span><a id="id370" class="indexterm"/><span class="koboSpan" id="kobo.235.1">a deep learning-based managed cloud service for our label identification and face detection system. </span><span class="koboSpan" id="kobo.235.2">We will continue to leverage a serverless environment AWS Lambda and utilize a deep learning-based cloud managed service, AWS Rekognition for facial attribute identification.</span></p><p><span class="koboSpan" id="kobo.236.1">Perform the following steps to build a facial detection system using managed cloud deep learning services on a serverless platform:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.237.1">First, update the</span><a id="id371" class="indexterm"/><span class="koboSpan" id="kobo.238.1"> IAM Lambda execution role from the previous example and attach a new managed policy </span><span class="strong"><strong><span class="koboSpan" id="kobo.239.1">AmazonRekognitionFullAccess</span></strong></span><span class="koboSpan" id="kobo.240.1"> as follows:</span><div class="mediaobject"><span class="koboSpan" id="kobo.241.1"><img src="graphics/B08086_06_29.png.jpg" alt="Running face detection with a cloud managed service"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.242.1">Next, create a new Lambda function that will be used for building the facial detection service. </span><span class="koboSpan" id="kobo.242.2">Select </span><span class="strong"><strong><span class="koboSpan" id="kobo.243.1">Runtime*</span></strong></span><span class="koboSpan" id="kobo.244.1"> as </span><span class="strong"><strong><span class="koboSpan" id="kobo.245.1">Python 2.7</span></strong></span><span class="koboSpan" id="kobo.246.1"> and keep all other settings as default. </span><span class="koboSpan" id="kobo.246.2">Attach the updated IAM role with this Lambda function:</span><div class="mediaobject"><span class="koboSpan" id="kobo.247.1"><img src="graphics/B08086_06_30.png.jpg" alt="Running face detection with a cloud managed service"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.248.1">Then, paste the</span><a id="id372" class="indexterm"/><span class="koboSpan" id="kobo.249.1"> following code in the </span><span class="strong"><strong><span class="koboSpan" id="kobo.250.1">Lambda function code</span></strong></span><span class="koboSpan" id="kobo.251.1"> section area. </span><span class="koboSpan" id="kobo.251.2">Update the S3 </span><span class="strong"><strong><span class="koboSpan" id="kobo.252.1">Bucket Name</span></strong></span><span class="koboSpan" id="kobo.253.1"> and AWS </span><span class="strong"><strong><span class="koboSpan" id="kobo.254.1">Region</span></strong></span><span class="koboSpan" id="kobo.255.1"> information in the </span><code class="literal"><span class="koboSpan" id="kobo.256.1">boto3.client</span></code><span class="koboSpan" id="kobo.257.1"> of the code as follows:</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.258.1">from __future__ import print_function

import json
import urllib
import boto3
import base64
import io

print('Loading function')

s3 = boto3.client('s3')
rekognition = boto3.client("rekognition", &lt;aws-region name like us-west-1&gt;)

bucket=&lt;Put your Bucket Name&gt;
key_path='raw-image/'

def lambda_handler(event, context):
    
    output={}
    try:
        if event['operation']=='label-detect':
            print('Detecting label')
            fileName= event['fileName']
            bucket_key=key_path + fileName
            data = base64.b64decode(event['base64Image'])
            image=io.BytesIO(data)
            s3.upload_fileobj(image, bucket, bucket_key)
            rekog_response = rekognition.detect_labels(Image={"S3Object": {"Bucket": bucket,"Name": bucket_key,}},MaxLabels=5,MinConfidence=90,)
            for label in rekog_response['Labels']:
                output[label['Name']]=label['Confidence']
        else:
            print('Detecting faces')
            FEATURES_BLACKLIST = ("Landmarks", "Emotions", "Pose", "Quality", "BoundingBox", "Confidence")
            fileName= event['fileName']
            bucket_key=key_path + fileName
            data = base64.b64decode(event['base64Image'])
            image=io.BytesIO(data)
            s3.upload_fileobj(image, bucket, bucket_key)
            face_response = rekognition.detect_faces(Image={"S3Object": {"Bucket": bucket,  "Name": bucket_key, }}, Attributes=['ALL'],)
            for face in face_response['FaceDetails']:
                output['Face']=face['Confidence']
                for emotion in face['Emotions']:
                    output[emotion['Type']]=emotion['Confidence']
                for feature, data in face.iteritems():
                    if feature not in FEATURES_BLACKLIST:
                        output[feature]=data
    except Exception as e:
        print(e)
        raise e    
      
    return output      </span></pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.259.1">Once</span><a id="id373" class="indexterm"/><span class="koboSpan" id="kobo.260.1"> you have created the Lambda function, we will create an API gateway child resource for this service as follows:</span><div class="mediaobject"><span class="koboSpan" id="kobo.261.1"><img src="graphics/B08086_06_31.png.jpg" alt="Running face detection with a cloud managed service"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.262.1">Next, we will </span><a id="id374" class="indexterm"/><span class="koboSpan" id="kobo.263.1">add a method (</span><span class="strong"><strong><span class="koboSpan" id="kobo.264.1">PUT</span></strong></span><span class="koboSpan" id="kobo.265.1"> in this case) to our new child resource (</span><span class="strong"><strong><span class="koboSpan" id="kobo.266.1">predict</span></strong></span><span class="koboSpan" id="kobo.267.1">), then click on </span><span class="strong"><strong><span class="koboSpan" id="kobo.268.1">Integration Request</span></strong></span><span class="koboSpan" id="kobo.269.1"> of the </span><span class="strong"><strong><span class="koboSpan" id="kobo.270.1">PUT</span></strong></span><span class="koboSpan" id="kobo.271.1"> method.</span><div class="mediaobject"><span class="koboSpan" id="kobo.272.1"><img src="graphics/B08086_06_32.jpg" alt="Running face detection with a cloud managed service"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.273.1">Now, attach the </span><span class="strong"><strong><span class="koboSpan" id="kobo.274.1">Lambda Function</span></strong></span><span class="koboSpan" id="kobo.275.1"> created previously with this resource method. </span><span class="koboSpan" id="kobo.275.2">You need to select the AWS </span><span class="strong"><strong><span class="koboSpan" id="kobo.276.1">Lambda Region</span></strong></span><span class="koboSpan" id="kobo.277.1"> where you have created your </span><span class="strong"><strong><span class="koboSpan" id="kobo.278.1">Lambda Function</span></strong></span><span class="koboSpan" id="kobo.279.1"> to get the Lambda function name in the drop-down list:</span><div class="mediaobject"><span class="koboSpan" id="kobo.280.1"><img src="graphics/B08086_06_33.png.jpg" alt="Running face detection with a cloud managed service"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.281.1">Next, expand the </span><span class="strong"><strong><span class="koboSpan" id="kobo.282.1">Body Mapping Templates</span></strong></span><span class="koboSpan" id="kobo.283.1"> section and select </span><span class="strong"><strong><span class="koboSpan" id="kobo.284.1">When there are no templates defined (recommended)</span></strong></span><span class="koboSpan" id="kobo.285.1"> in the </span><span class="strong"><strong><span class="koboSpan" id="kobo.286.1">Request body passthrough</span></strong></span><span class="koboSpan" id="kobo.287.1"> section. </span><span class="koboSpan" id="kobo.287.2">Then,</span><a id="id375" class="indexterm"/><span class="koboSpan" id="kobo.288.1"> add a mapping template </span><span class="strong"><strong><span class="koboSpan" id="kobo.289.1">image/png</span></strong></span><span class="koboSpan" id="kobo.290.1"> in the </span><span class="strong"><strong><span class="koboSpan" id="kobo.291.1">Content-Type</span></strong></span><span class="koboSpan" id="kobo.292.1"> and paste the following code in the </span><span class="strong"><strong><span class="koboSpan" id="kobo.293.1">General template</span></strong></span><span class="koboSpan" id="kobo.294.1"> area:</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.295.1">{
"base64Image": "$input.body",
"operation": "$input.params('activity')",
"fileName": "$input.params('fileName')"
}</span></pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.296.1"><img src="graphics/B08086_06_34.png.jpg" alt="Running face detection with a cloud managed service"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.297.1">Now deploy your API Gateway resource API by clicking </span><span class="strong"><strong><span class="koboSpan" id="kobo.298.1">Deploy API</span></strong></span><span class="koboSpan" id="kobo.299.1"> from the </span><span class="strong"><strong><span class="koboSpan" id="kobo.300.1">Action</span></strong></span><span class="koboSpan" id="kobo.301.1"> menu. </span><span class="koboSpan" id="kobo.301.2">Once your resource is deployed, you will get an API of the gateway that you will use to invoke the face detection service. </span><span class="koboSpan" id="kobo.301.3">We will continue to use the REST</span><a id="id376" class="indexterm"/><span class="koboSpan" id="kobo.302.1"> client </span><span class="strong"><strong><span class="koboSpan" id="kobo.303.1">Postman</span></strong></span><span class="koboSpan" id="kobo.304.1"> (</span><a class="ulink" href="https://chrome.google.com/webstore/detail/postman/fhbjgbiflinjbdggehcddcbncdddomop?hl=en"><span class="koboSpan" id="kobo.305.1">https://chrome.google.com/webstore/detail/postman/fhbjgbiflinjbdggehcddcbncdddomop?hl=en</span></a><span class="koboSpan" id="kobo.306.1">) from our previous example, but you can use any other </span><a id="id377" class="indexterm"/><span class="koboSpan" id="kobo.307.1">REST client of your choice as well. </span><span class="koboSpan" id="kobo.307.2">The API Gateway URL will look as follows: </span><p>
<code class="literal"><span class="koboSpan" id="kobo.308.1">https://&lt;API ID&gt;.execute-api.&lt;AWS Region&gt;.amazonaws.com/prod/predict</span></code>
</p></li><li class="listitem"><span class="koboSpan" id="kobo.309.1">Add </span><span class="strong"><strong><span class="koboSpan" id="kobo.310.1">Content-Type</span></strong></span><span class="koboSpan" id="kobo.311.1"> as </span><span class="strong"><strong><span class="koboSpan" id="kobo.312.1">image/png</span></strong></span><span class="koboSpan" id="kobo.313.1"> and add two request parameter activities and filenames in the request. </span><span class="koboSpan" id="kobo.313.2">The parameter </span><span class="strong"><strong><span class="koboSpan" id="kobo.314.1">activity</span></strong></span><span class="koboSpan" id="kobo.315.1"> takes two values (</span><span class="strong"><strong><span class="koboSpan" id="kobo.316.1">label-detect</span></strong></span><span class="koboSpan" id="kobo.317.1"> for image recognition or label detection) and (</span><span class="strong"><strong><span class="koboSpan" id="kobo.318.1">face-detect</span></strong></span><span class="koboSpan" id="kobo.319.1"> for face detection). </span><span class="koboSpan" id="kobo.319.2">And the </span><span class="strong"><strong><span class="koboSpan" id="kobo.320.1">fileName</span></strong></span><span class="koboSpan" id="kobo.321.1"> parameter will be used to save the raw image to S3 with that name.</span><div class="mediaobject"><span class="koboSpan" id="kobo.322.1"><img src="graphics/B08086_06_35.png.jpg" alt="Running face detection with a cloud managed service"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.323.1">Now, invoke </span><a id="id378" class="indexterm"/><span class="koboSpan" id="kobo.324.1">your service to detect a label or face and get the response output in JSON as follows:</span><div class="mediaobject"><span class="koboSpan" id="kobo.325.1"><img src="graphics/B08086_06_36.png.jpg" alt="Running face detection with a cloud managed service"/></span></div></li></ol></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec40"/><span class="koboSpan" id="kobo.1.1">Summary</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">So far, you have learned and implemented various ways of deploying the trained deep model and making predictions for new data samples. </span><span class="koboSpan" id="kobo.2.2">You have also learned how to take your model from your local machine or data center to the cloud using Docker containers smoothly. </span><span class="koboSpan" id="kobo.2.3">I hope, throughout the book, with lots of hands-on examples using real-world public datasets, you have understood the concept of GANs, and its variant architecture (SSGAN, BEGAN, DCGAN, CycleGAN, StackGAN, DiscoGAN) well. </span><span class="koboSpan" id="kobo.2.4">Once you have played around with the code and examples in this book, I would definitely encourage you to do the following:</span></p><p><span class="koboSpan" id="kobo.3.1">Participate in the Kaggle Adversarial Network competition: </span><a class="ulink" href="https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack"><span class="koboSpan" id="kobo.4.1">https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack</span></a><span class="koboSpan" id="kobo.5.1">.</span></p><p><span class="koboSpan" id="kobo.6.1">Keep your knowledge updated about deep learning and GANs by attending or viewing the following conferences:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.7.1">Neural Informat</span></strong></span><span class="strong"><strong><span class="koboSpan" id="kobo.8.1">ion Processing Syst</span></strong></span><span class="strong"><strong><span class="koboSpan" id="kobo.9.1">ems</span></strong></span><span class="koboSpan" id="kobo.10.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.11.1">NIPS</span></strong></span><span class="koboSpan" id="kobo.12.1">): </span><a class="ulink" href="https://nips.cc/"><span class="koboSpan" id="kobo.13.1">https://nips.cc/</span></a></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.14.1">International Conference on Learning Representations</span></strong></span><span class="koboSpan" id="kobo.15.1"> (ICLR): </span><a class="ulink" href="HTTP://WWW.ICLR.CC/"><span class="koboSpan" id="kobo.16.1">HTTP://WWW.ICLR.CC/</span></a></li></ul></div></div></div></div></body></html>