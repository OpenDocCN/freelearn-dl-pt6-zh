<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Revisiting Deep Learning Architecture and Techniques</h1>
                </header>
            
            <article>
                
<p class="mce-root">Deep learning is part of a broader machine learning and artificial intelligence field that uses artificial neural networks. One of the main advantages of deep learning methods is that they help to capture complex relationships and patterns contained in data. When the relationships and patterns are not very complex, traditional machine learning methods may work well. However, with the availability of technologies that help to generate and process more and more unstructured data, such as images, text, and videos, deep learning methods have become increasingly popular as they are almost a default choice to deal with such data. Computer vision and <strong>natural language processing</strong> (<strong>NLP</strong>) are two areas that are seeing interesting applications in a wide variety of fields, such as driverless cars, language translation, computer games, and even creating new artwork. </p>
<p class="mce-root">Within the deep learning toolkit, we now have an increasing array of neural network techniques that can be applied to a specific type of task. For example, when developing image classification models, a special type of deep network called a <strong>convolutional neural network</strong> (<strong>CNN</strong>) has proved to be effective in capturing unique patterns that exist in image-related data. Similarly, another popular deep learning network called <strong>recurrent neural networks</strong> (<strong>RNNs</strong>) and its variants have been found useful in dealing with data involving sequences of words or integers. Another popular and interesting deep learning network called a <strong>generative adversarial network</strong> (<strong>GAN</strong>) has the capability to generate new images, speech, music, or artwork.</p>
<p class="mce-root">In this book, we will use these and other popular deep learning networks using R software. Each chapter presents a complete example that has been specifically developed to run on a regular laptop or computer. The main idea is to avoid getting bogged down by a huge amount of data needing advanced computing resources at the first stage of applying deep learning methods. You will be able to go over all the steps using the illustrated examples in this book. The examples used also include the best practices for each topic, and you will find them useful. You will also find a hands-on and applied approach helpful in quickly seeing the big picture when trying to replicate these deep learning methods when faced with a new problem.</p>
<p class="mce-root">This chapter provides an overview of the deep learning methods with R that are covered in this book. We will go over the following topics in this chapter:</p>
<ul>
<li>Deep learning with R</li>
<li>The process of developing a deep network model</li>
<li>Popular deep learning techniques with R and RStudio</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning with R</h1>
                </header>
            
            <article>
                
<p>We will start by looking at the popularity of deep learning networks and also take a look at a version of some of the important R packages used in this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning trend</h1>
                </header>
            
            <article>
                
<p>Deep learning techniques make use of neural network-based models and have seen increasing interest in the last few years.A Google trends website for the search term <strong>deep learning</strong> provides the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e00aa45a-06c8-4f2b-8d54-bb32d931d475.png"/></p>
<p>The preceding plot has 100 as the peak popularity of a search term, and other numbers are relative to this highest point. It can be observed that the interest in the term <strong>deep learning</strong> has gradually increased in popularity since around 2014. <span>For the last two years, it has enjoyed peak popularity</span>. One of the reasons for the popularity of deep learning networks is the availability of the free and open source libraries, TensorFlow and Keras.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Versions of key R packages used</h1>
                </header>
            
            <article>
                
<p>In this book, we will use the Keras R package that uses TensorFlow as a backend for building deep learning networks. An output from a typical R session, used for the examples illustrated in this book, providing various version-related information, is provided in the following code:</p>
<pre># Information from a Keras R session<br/>sessionInfo()<br/><br/>R version 3.6.0 (2019-04-26)<br/>Platform: x86_64-apple-darwin15.6.0 (64-bit)<br/>Running under: macOS 10.15<br/><br/>Matrix products: default<br/>BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib<br/>LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib<br/><br/>Random number generation:<br/> RNG: Mersenne-Twister <br/> Normal: Inversion <br/> Sample: Rounding <br/> <br/>locale:<br/>[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8<br/><br/>attached base packages:<br/>[1] stats graphics grDevices utils datasets methods base<br/><br/>other attached packages:<br/>[1] keras_2.2.4.1<br/><br/>loaded via a namespace (and not attached):<br/> [1] Rcpp_1.0.2 lattice_0.20-38 lubridate_1.7.4 zeallot_0.1.0 <br/> [5] grid_3.6.0 R6_2.4.0 jsonlite_1.6 magrittr_1.5 <br/> [9] tfruns_1.4 stringi_1.4.3 whisker_0.4 Matrix_1.2-17 <br/>[13] reticulate_1.13 generics_0.0.2 tools_3.6.0 stringr_1.4.0 <br/>[17] compiler_3.6.0 base64enc_0.1-3 tensorflow_1.14.0</pre>
<p>As seen previously, for this book we have used the 3.6 version of R that was released in April 2019. The nickname for this R version is Planting of a Tree. The version used for the Keras package is 2.2.4.1. In addition, all the application examples illustrated in the book have been run on a Mac computer with 8 GB of RAM. The main reason for using this specification is that it will allow a reader to go through all the examples without needing advanced computing resources to get started with any deep learning network covered in the book.</p>
<p>In the next section, we will go over the process of developing a deep network model that is broken down into five general steps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Process of developing a deep network model</h1>
                </header>
            
            <article>
                
<p>Developing a deep learning network model can be broken down into five key steps shown in the following flowchart:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5608bd8c-32f4-4d05-ad3d-42c2c0442a93.png" style="width:12.00em;height:19.75em;"/></p>
<p>Each step mentioned in the preceding flowchart can have varying requirements based on the type of data used, the type of deep learning network being developed, and also the main objective of developing a model. We will go over each step to develop a general idea about what is involved.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data for a deep network model</h1>
                </header>
            
            <article>
                
<p>Developing deep learning neural network models requires the variables to have a certain format. Independent variables may come with a varying scale, with some variable values in decimals and some other variables in thousands. Using such varying scales of variables is not very efficient when training a network. Before developing deep learning networks, we make changes such that the variables have similar scales. The process used for achieving this is called <strong>normalization</strong>. </p>
<p>Two commonly used methods for normalization are z-score normalization and min-max normalization. In z-score normalization, we subtract the mean from each value and divide it by the standard deviation. This transformation results in values that lie between -3 and +3 with a mean of 0 and a standard deviation of 1. For a min-max normalization, we subtract the minimum value from each data point, and then divide it by the range. This transformation converts data to having values between zero and one.</p>
<p>As an example, see the following plots, where we have obtained 10,000 data points randomly from a normal distribution with a mean of 35 and a standard deviation of 5:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9274ac87-fd1c-4e1a-8578-e5f9b3fbbe66.png"/></p>
<p>From the preceding plots, we can observe that after z-score normalization, the data points mostly lie between -3 and +3. Similarly, after min-max normalization, the range of values changes to data points between 0 and 1. However, the overall pattern seen in the original data is retained after both types of normalization.</p>
<p>Another important step in preparing data when using a categorical response variable is to carry out one-hot encoding. One-hot encoding converts a categorical variable to a new binary format that has values containing either 0 or 1. This is achieved very easily by using the <kbd>to_categorical()</kbd> function available in Keras.</p>
<p>Typically data processing steps for unstructured data, such as image or text, are more involved compared with a situation where we are dealing with structured data. In addition, the nature of data preparation steps can vary from one type of data to another. For example, the way we prepare image data for developing a deep learning classification model is likely to be very different from the way we prepare text data for developing a movie review sentiment classification model. However, one important thing to note is that before we can develop deep learning models from unstructured data, they need to be first converted into a structured format. An example of converting unstructured image data into a structured format is shown in the following screenshot, using a picture of the handwritten digit <em>five</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/95f65454-c5c4-4767-99a4-b9634f5c62aa.png" style="width:54.58em;height:25.58em;"/></p>
<p>As can be observed from the preceding screenshot, when we read an image file containing a black and white handwritten digit <em>five</em> with 28 x 28 dimensions in R, it gets converted to numbers in rows and columns, giving it a structured format. The right-hand side of the screenshot shows data with 28 rows and 28 columns. The numbers in the body of the table are pixel values that range from 0 to 255, where a value of zero represents the black color and 255 represents the white color in the picture. When developing deep learning models, we make use of some forms of such structured data that are derived from image data. </p>
<p>Once the data for developing the model is prepared in the required format, we can then develop the model architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing a deep learning model architecture</h1>
                </header>
            
            <article>
                
<p>Developing the architecture of a model involves defining various items, such as the type and number of layers for the network, the type of activation function, the number of units or neurons to use in the network, and also providing the data-related input/output values. An example of specifying a simple sequential model architecture using Keras in R is shown in the following code:</p>
<pre># Model architecture<br/>model &lt;- keras_model_sequential()<br/> model %&gt;% <br/> layer_dense(units = 8, activation = 'relu', input_shape = c(21)) %&gt;% <br/> layer_dense(units = 3, activation = 'softmax')</pre>
<p>Note that a sequential model allows us to develop models layer by layer. As seen from the preceding code, two layers of densely connected networks have been added as a part of the sequential model. Two important decisions while choosing a model architecture involve the number and type of layers and the type of activation function for a layer. The number and type of layers to use is guided by the nature and complexity of the data. For a fully connected network (also known as a multilayer perceptron), we can use a dense layer with the help of the <kbd>layer_dense</kbd> function available in Keras.</p>
<p>On the other hand, when working with image data, we are likely to use convolutional layers in the network, using the <kbd>layer_conv_2d</kbd> function. We will discuss more details about specific model architectures with examples in each chapter. </p>
<p>There are different types of activation functions that are used in deep learning networks. A rectified linear unit, or <kbd>relu</kbd>, is a popular activation function used in hidden layers, and it uses a very simple calculation. If the input is negative, it returns a value of zero and, for everything else, there is no change to the original value. As an example, let's look at the following code:</p>
<pre># RELU function and related plot<br/>x &lt;- rnorm(10000, 2, 10)<br/>y &lt;- ifelse(x&lt;0, 0, x)<br/>par(mfrow = c(1,2))<br/>hist(x)<br/>plot(x,y)</pre>
<p>The preceding code generates 10,000 random numbers from a normal distribution with a mean of two and a standard deviation of 10, and stores the results in <kbd>x</kbd>. And then negative values are changed to zero and stored in y. A histogram of x and a scatter plot for x and y are given in the following graphs:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/19566188-4019-4952-bcdc-d7ac1db23736.png" style="width:31.58em;height:24.67em;"/></p>
<p>It can be observed from the preceding histogram that x has values that are both positive and negative. The scatter plot, based on the original x values and the modified y value that is obtained after converting negative values to zero, visualizes the impact of the <kbd>relu</kbd> activation function. In the scatter plot, the data points to the left of x = 0 are flat and have a zero slope. The data points to the right of x = 0 have a perfect linear pattern with a slope of 1.</p>
<p>One of the main advantages of using the <kbd>relu</kbd> activation function is its simple calculation. For developing deep learning network models, this becomes an important factor as it helps to reduce computational cost. For many deep learning networks, a rectified linear unit is used as the default activation function.</p>
<p>Another popular activation function used for developing deep networks is <kbd>softmax</kbd>, which is usually used in the outer layer of the network. Let's look at the following code to understand it better:</p>
<pre># Softmax function and related plot<br/>x &lt;- runif(1000, 1, 5)<br/>y &lt;- exp(x)/sum(exp(x))<br/>par(mfrow=c(1,2))<br/>hist(x)<br/>plot(x,y)</pre>
<p>In the preceding code, we have taken a random sample of 1,000 values from a uniform distribution that lies between 1 and 5. To use the <kbd>softmax</kbd> function, we can divide the exponential of each input value x by the sum of the exponential values of x. The resulting histogram, based on the x values, and the scatter plot of x and y values are shown in the following graphs:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e6669b6e-51a5-4f48-be04-36981b9cecd1.png" style="width:36.42em;height:28.33em;"/></p>
<p>We can observe that the preceding histogram provides an approximate uniform pattern for the x values. The impact of the <kbd>softmax</kbd> function can be seen from the scatter plot where the output values now lie between 0 and 1. This conversion is very useful for interpreting the results in terms of probabilities as the values now are as follows:</p>
<ul>
<li>Lie between 0 and 1</li>
<li>The total of these probabilities is 1</li>
</ul>
<p>This aspect of the <kbd>softmax</kbd> activation function, where results can be interpreted in terms of probabilities, makes it a popular choice when developing deep learning classification models. It works well whether we use it for image classification or text classification problems.</p>
<p>Apart from these two activation functions, we also make use of others that may be more suitable for a specific deep leaning model.</p>
<p>Once a model architecture to be used is specified, the next step is to compile the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compiling the model</h1>
                </header>
            
            <article>
                
<p>Compiling a model typically involves specifying the loss function, choosing an optimizer, and specifying the metrics to be used. These choices, however, depend on the type of problem that is being addressed. The following code is an example of R for compiling a deep learning binary classification model:</p>
<pre>model %&gt;% <br/>   compile(loss = 'binary_crossentropy', <br/>   optimizer = 'adam',<br/>   metrics = 'accuracy')</pre>
<p>The preceding loss function specified is <kbd>binary_crossentropy</kbd>, which is used when the response variable has two classes. Binary cross-entropy can be calculated using the following formula:</p>
<p style="padding-left: 120px"><img class="fm-editor-equation" src="assets/46d4e0aa-51e4-44b7-b6b3-fe2a37dcd8c5.png" style="width:22.83em;height:1.50em;"/></p>
<p>In the preceding formula, y represents the actual class and <kbd>yhat</kbd> represents the prediction probability. Let's consider two examples using the following code:</p>
<pre># Example-1<br/>y &lt;- c(0, 0, 0, 1, 1, 1)<br/>yhat &lt;- c(0.2, 0.3, 0.1, 0.8, 0.9, 0.7)<br/>(loss &lt;- - y*log(yhat) - (1-y)*log(1-yhat))<br/><br/><strong>[1] 0.2231436 0.3566749 0.1053605 0.2231436 0.1053605 0.3566749</strong><br/><br/>mean(loss)<br/><br/><strong>[1] 0.228393</strong><br/><br/># Example-2<br/>yhat &lt;- c(0.2, 0.9, 0.1, 0.8, 0.9, 0.2)<br/>(loss &lt;- - y*log(yhat) - (1-y)*log(1-yhat))<br/><br/><strong>[1] 0.2231436 2.3025851 0.1053605 0.2231436 0.1053605 1.6094379</strong><br/><br/>mean(loss)<br/><br/><strong>[1] 0.761505</strong></pre>
<p>As seen in <kbd>Example-1</kbd>, there are a total of six cases represented by y where the first three cases indicate the actual class to be 0, and the next three cases have the actual class as 1. The prediction probabilities captured by <kbd>yhat</kbd> is the probability that a case belongs to category 1. In <kbd>Example-1</kbd>, the <kbd>yhat</kbd> values correctly classify all six cases, and the average of all loss values is about 0.228. In <kbd>Example-2</kbd>, the <kbd>yhat</kbd> values correctly classify only four cases, and the average of all loss values now increases to about 0.762. The binary cross-entropy loss function in this way helps to assess the classification performance of a model. The lower the loss value is, the better the classification performance, and the higher the loss value is, the worse the classification performance of the model.</p>
<p>There are various other loss functions that are used based on the type of problem for which the deep learning network is being developed. For classification models where the response variables have more than two classes, we make use of the <kbd>categorical_crossentropy</kbd> loss function. For regression problems with numeric response variables, the mean square error (<kbd>mse</kbd>) may be an appropriate loss function.</p>
<p>When specifying an optimizer to be used by the model, <kbd>adam</kbd> is a popular choice for deep learning networks, giving good results in a wide variety of situations. Other commonly used optimizers include <kbd>rmsprop</kbd> and <kbd>adagrad</kbd>. When a deep learning network is being trained, the parameters of the network are modified based on feedback obtained from the loss function. How this modification of parameters takes place is based on which optimizer is used. The choice of a suitable optimizer is therefore important in arriving at a suitable model.</p>
<p>When compiling the model, we also specify a suitable metric that will be used for monitoring the training process. For classification problems, <kbd>accuracy</kbd> is a one of the most commonly used metrics. For regression problems, the mean absolute error is a commonly specified metric.</p>
<p>Once we compile a model, we are ready to fit it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fitting the model</h1>
                </header>
            
            <article>
                
<p>Fitting or training of the model is carried out with the help of data. An example of a code used for fitting a classification model is provided as follows:</p>
<pre>model %&gt;%   <br/> fit(training, <br/>   trainLabels, <br/>   epochs = 200,<br/>   batch_size = 32, <br/>   validation_split = 0.2)</pre>
<p>In the preceding code, fitting a model includes <kbd>training</kbd>, which is the data on independent variables, and <kbd>trainLabels</kbd>, which contain labels for the response variable. The number of epochs is specified to indicate the number of iterations of all samples in the training data that will be used during the training process. Batch size refers to the number of samples from the training data to be used, after which the model parameters will be updated. In addition, we also specify any validation split, where a 0.2 or 20% split means that the last 20% of samples from the training data will be kept separate from the training process to assess the model performance.</p>
<p>When fitting a model, different layers in the network have a random initialization of weights. Due to this random initialization of network weights, if we fit a model again with the same data, same architecture, and same settings, we will get slightly different results. This will occur not only in a different session of R, but also in the same session when a model is trained again.</p>
<p>There are many situations where getting repeatable results is important. As an example, while publishing a deep learning-related article in a peer-reviewed international journal, you may need to generate more plots from the same model based on reviewer feedback. Another situation could be where a team working on the same project may like to share a model and also results with other members of the team. The easiest way to obtain the same results from the model is to save and then reload the model using the following code:</p>
<pre># Save/reload model<br/>save_model_hdf5(model, <br/> filepath, <br/> overwrite = TRUE,<br/> include_optimizer = TRUE)<br/>model_x &lt;- load_model_hdf5(filepath, <br/> custom_objects = NULL, <br/> compile = TRUE)</pre>
<p>We can save the model by specifying <kbd>filepath</kbd> and then reload when required. Saving a model allows us to obtain repeatable results when we use the model again. It also allows us to have a way to share the same model with others who can obtain exactly the same results, as well as helping in situations where each run takes a lot of time. Saving and reloading the model allows you to resume the training process when you train the model again.</p>
<p>Once a model is fit, its performance can be assessed using both training and testing data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Assessing the model performance</h1>
                </header>
            
            <article>
                
<p>Assessing the performance of a deep learning classification model requires developing a confusion matrix that summarizes predictions for actual and predicted classes. Consider an example where a classification model is developed to classify graduate school applicants in one of two categories where class 0 refers to  applications that have not been accepted, and class 1 refers to accepted applications. An example of a confusion matrix for this situation explaining the key concepts <span>is provided as follows</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/884c53eb-52d9-44bd-acef-15d06693329f.png" style="width:19.58em;height:13.58em;"/></p>
<p>In the preceding confusion matrix, there are 208 applicants who were actually not accepted and the model also correctly predicts that they should not be accepted. This cell in the confusion matrix is also called the <strong>true negative</strong>. Similarly, there are 29 applicants who were actually accepted and the model also correctly predicts that they should be accepted. This cell in the confusion matrix is called the <strong>true positive</strong>. We also have cells with numbers indicating the incorrect classification of applicants by the model. There are 15 applicants who were actually not accepted, but the model incorrectly predicts that they should be accepted and this cell is called a <strong>false negative</strong>.</p>
<p>Another name for making an error when incorrectly classifying category 0 as belonging to category 1 is a type-1 error. Finally, there are 73 applicants that were actually accepted but the model incorrectly predicts them to belong to the not-accepted category, and this cell is called a <strong>false positive</strong>. Aanother name for such an incorrect classification is a type-2 error.</p>
<p>From the confusion matrix, we can calculate the accuracy of the classification performance by adding numbers to the diagonal and dividing the numbers by the total. So, the accuracy based on the preceding matrix is (208+29)/(208+29+73+15), or 72.92%. Apart from the accuracy, we can also find out the model performance in correctly classifying each category. We can calculate the accuracy of correctly classifying category 1, also called sensitivity, as 29/(29+73), or 28.4%. Similarly, we can calculate the accuracy of correctly classifying category 0, also called specificity, as 208/(208+15), or 93.3%.</p>
<p>Note, that the confusion matrix can be used when developing a classification model. However, other situations may call for other suitable ways of assessing the deep learning network.</p>
<p>We can now briefly go over the deep learning techniques covered in this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning techniques with R and RStudio</h1>
                </header>
            
            <article>
                
<p>The term <strong>deep</strong> in deep learning refers to a neural network model having several layers, and the learning takes place with the help of data. And based on the type of data used, deep learning may be categorized into two major categories, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0b572444-8411-43e4-be7e-7afff4434d3f.png" style="width:33.17em;height:23.33em;"/></p>
<p>As shown in the preceding diagram, the type of data used for developing a deep neural network model can be of a structured or unstructured type. In <a href="c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml">Chapter 2</a>, <em>Deep Neural Networks for Multi-Class Classification</em>, we illustrate the use of a deep learning network for classification problems using structured data where the response variable is of the categorical type. In <a href="07c9aa4a-1c93-490a-bfcd-7c4bcde639d5.xhtml">Chapter 3</a>, <em>Deep Neural Networks for Regression</em>, we illustrate the use of a deep learning network for regression problems using structured data where the response is a continuous type of variable. Chapters 4 to 12 illustrate the use of deep learning networks for mainly two types of unstructured data that involve images and text. In chapters 4 to 8, we provide application examples of some popular deep learning networks using image data, which is regarded as an unstructured type of data. Finally, in chapters 9 to 12, we cover some popular deep learning networks that are useful with text data, which is another major category within unstructured data.</p>
<p>Now, let's briefly go over the examples and techniques covered in chapters 2 to 12.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-class classification</h1>
                </header>
            
            <article>
                
<p>There are many problems where the main objective is to develop a classification model that uses data to classify observations into two or more categories. For example, a patient may be classified as normal, suspect, or pathological based on the data on several variables. The deep learning network in this case will use data on several patients where the outcome is already available, and it will learn to classify a patient into one of the three categories.</p>
<p>Another example of a classification problem could be where students send applications to a graduate school. An application from a student may be accepted or rejected based on variables such as GPA, GRE, and ranking of the school during their undergraduate degree. Another interesting example could be where student-related data is used for developing a model that helps to classify first-year students into those that are likely to stay with the current school and those who are likely to transfer to another school. A similar model can be developed to classify customers who are likely to stay with a business or switch to a competitor. </p>
<p>One of the challenges involved while developing a classification model is that of class imbalance. For example, when dealing with medical data, the number of patients classified as normal may be much larger than the number of patients who are classified as pathological. Similarly, when applying to a graduate program at one of the top universities, it is very likely that the data contains a significantly higher number of cases where an applicant is not accepted. Deep network models are useful in addressing such concerns easily. The Keras library used in this book provides a user-friendly interface not only to address such issues easily, but also to help in obtaining suitable classification models with the help of fast experimentation.</p>
<p>In <a href="c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml">Chapter 2</a>, <em>Deep Neural Networks for Multi-Class Classification</em>, we provide an illustration of a multi-class deep learning classification model using R.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression problems</h1>
                </header>
            
            <article>
                
<p>Structured data involving numeric response variables is classified as a regression problem. For example, the price of a house in a city may depend on variables such as the age of the house, the crime rate in the city, the number of rooms, and the property tax rate. Although statistical methods, such as multiple linear regression and elastic net regression, can also be useful for these situations, deep learning networks offer certain advantages. One of the main advantages of using neural networks in general is that they can capture non-linearity. Unlike statistical methods that require certain assumptions to be met before we can use them, neural network-based models are more flexible to use and do not require many assumptions to be fulfilled.</p>
<p>Many applications involving regression problems also call for identifying variables or features that have a significant impact on the response variable. However with deep learning networks, such feature engineering is inbuilt, and it doesn't call for any extra effort in extracting important features. One thing to note regarding deep learning networks is that the larger the dataset being used, the more effective the resulting prediction model will be. In <a href="07c9aa4a-1c93-490a-bfcd-7c4bcde639d5.xhtml">Chapter 3</a>, <em>Deep Neural Networks for Regression</em>, we provide an illustration of a deep learning regression model using R.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image classification</h1>
                </header>
            
            <article>
                
<p>Image data is classified as an unstructured type of data. One of the popular applications of deep learning networks involves developing image classification and recognition models. Image classification has various applications, such as face recognition on smartphones or on social media networks, classification of medical image data, classification of handwritten digits, and self-driving cars. Note that it is not possible to develop a classification model directly from unstructured data. The unstructured data needs to be first converted into a structured form before deep learning networks can be developed. For example, a black and white image may have dimensions of 21 x 21 and thus contain data on 441 (21 x 21) pixels. Once we convert an image into numbers representing all the pixels, it becomes feasible to develop image classification models. Although humans can classify a type of dress, a person, or certain object very easily, even when the images may have different sizes or orientation, training a computer to do so is a challenging task.</p>
<p>The Keras library provides several easy-to-use features for processing image data that helps in developing deep learning image classification networks. The effectiveness of having deep networks or neural networks with many layers especially comes to the fore when it comes to image recognition and classification problems. In <a href="356e6d56-329c-433e-8b3e-969453363ee9.xhtml">Chapter 4</a>, <em>Image Classification and Recognition</em>, we provide an illustration of applying a deep learning image classification model using R.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional neural networks</h1>
                </header>
            
            <article>
                
<p>Image classification tasks become challenging when the number of categories increases and images within a category show significant variability. Such situations also require a larger number of samples so that features inherent in each category can be captured more accurately by the classification model. For example, a fashion retailer may have a large variety of fashion items and may be interested in developing a classification model from the image data of such fashion items. A special type of deep network, called a <strong>convolutional neural network</strong> (<strong>CNN</strong>), has proven to be highly effective in situations that call for large scale image classification and recognition tasks. CNNs are the most popular networks for such applications and are regarded as the gold standard for large-scale image classification problems. These networks are capable of capturing various minute details in an image with the help of different types of layers in the network. In <a href="7285aaf1-8ca5-4f1d-95d8-057ce1fbf5f9.xhtml">Chapter 5</a>, <em>Image Classification Using Convolutional Neural Networks</em>, we provide an illustration of applying a CNN to image classification using R.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autoencoders</h1>
                </header>
            
            <article>
                
<p>Deep learning methods that involve classification and prediction models using data that has a response or a dependent variable are part of supervised deep learning methods. When working with structured or unstructured data, there are situations where the response variable is either not available or not used. Applications of deep learning networks that do not use a response variable are classified as unsupervised deep learning methods. For example, an application of deep learning may involve image data from which we want to extract important features in order to achieve dimension reduction. Another example involves handwritten images that contain unwanted noise and a deep network is used for denoising the images. In such situations, autoencoder networks have been found to be very useful for performing unsupervised deep learning tasks.</p>
<p>Autoencoder neural networks make use of an encoder and decoder network. When the image data is passed through an encoder and the resulting dimension is lower than that of the original image, the network is forced to extract only the most important features from the input data. And then the decoder part of the network reconstructs the original data from whatever is available from the output of the encoder. In <a href="489413e8-85df-4912-b59a-bd119d93c967.xhtml">Chapter 6</a>, <em>Applying Autoencoder Neural Networks Using Keras</em>, we provide an illustration of applying an autoencoder neural network for dimension reduction, de-noising, and image correction when working with image data using R.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transfer learning</h1>
                </header>
            
            <article>
                
<p>Developing deep learning classification models when the image data has several categories is a challenging task. It becomes even more challenging when the number of images available is limited. In such situations, it may be possible to take advantage of an existing model that has been developed with the help of a much larger dataset and reuse the patterns it has learned by customizing it for another classification task. This reuse of a pretrained deep network model for a new classification task is known as transfer learning.</p>
<p>The Keras library provides various pretrained models for image classification tasks that are trained using over a million images, and that capture reusable features that can be applied to similar but new data. Transferring what a pretrained model has learned from a large number of samples to a model that is being built with a much smaller sample size helps to save computational resources. In addition, use of the transfer learning approach can help to outperform a model that is built from scratch using a smaller dataset. In <a href="c316ef95-6026-4e25-9dd4-7e3a191721d0.xhtml">Chapter 7</a>, <em>Image Classification for Small Data Using Transfer Learning</em>, we cover transfer learning and illustrate the utilization of a pre trained deep learning image classification model using R.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generative adversarial networks</h1>
                </header>
            
            <article>
                
<p>An article in The Verge (Reference: <a href="https://www.theverge.com/2018/10/25/18023266/ai-art-portrait-christies-obvious-sold">https://www.theverge.com/2018/10/25/18023266/ai-art-portrait-christies-obvious-sold</a>) reported that an artwork named <em>Portrait of Edmond Belamy</em> created using an artificial intelligence algorithm was sold for $432,500. This artwork was estimated to sell for about $7,000 to $10,000. The deep learning algorithm that was used to create this artwork is called a <strong>generative adversarial network</strong> (<strong>GAN</strong>). The unique attribute of generative adversarial networks is that two deep networks are made to compete against each other to generate something meaningful. The two networks that compete against each other and try to outsmart one another are called generator and discriminator networks.</p>
<p>Consider a situation where we want to generate new handwritten images of the digit <em>five</em>. A generative adversarial network in this case would involve a generator network that creates fake images of the handwritten digit <em>five</em> from simply random noise and sends it to a discriminator network. The fake images are mixed with genuine images and the discriminator network, which is trained to differentiate between real and fake images of the handwritten digit <em>five</em>, will try its best to successfully differentiate between real and fake images. These two networks are made to compete against each other until the generator network starts making realistic-looking fake images that the discriminator network finds increasingly difficult to differentiate between. In addition to image data, application of generative adversarial networks can be extended to generate new text or even new music. We will illustrate an application of a generative adversarial network to generate new images in <a href="7031c1cb-e20d-4e86-8667-393d0cceddca.xhtml">Chapter 8</a>, <em>Creating New Images Using Generative Adversarial Networks</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep network for text classification </h1>
                </header>
            
            <article>
                
<p>Text data has certain unique characteristics that makes it a very different type of unstructured data compared to image data. As mentioned earlier, unstructured data requires extra processing steps to arrive at a structured format that can be used for developing a deep learning classification network. One of the applications of deep learning with text data involves developing a deep neural network sentiment classification model.</p>
<p>To develop a sentiment classification model, labels capturing sentiment related to the text data are needed. For example, we may use text data on movie reviews and a related sentiment label (positive review or negative review) to develop a model that can be used to automate the process. Another example could be the development of a sentiment classification model using text data on tweets. Such a model can be useful in comparing sentiments contained in thousands of tweets or and after an important event. Examples of such events where sentiment classification models can be useful include sentiments contained in tweets before and after the release of a new smartphone by a company, and sentiments contained in tweets before and after the performance of a presidential candidate in a live debate. A deep network for a sentiment classification model using text data is illustrated in <a href="491ea3a8-47e9-48b4-8553-7387528c8594.xhtml">Chapter 9</a>, <em>Deep Networks for Text Classification</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent neural networks </h1>
                </header>
            
            <article>
                
<p>A unique characteristic of text data is the fact that the placement of words in a text sequence has some meaning. <strong>Recurrent neural networks</strong> (<strong>RNNs</strong>) are well suited to work with data involving such sequences. Recurrent networks allow output from the previous step to be passed as input to the following step. This process of feeding prior information at a step allows recurrent networks to have memory, which is very useful for dealing with data involving sequences. The name <strong>recurrent</strong> in RNN also comes from the fact that the output at a step depends on information from the previous step.</p>
<p>RNNs can be used to develop a sentiment classification model where the text data could be movie reviews, tweets, product reviews, and so on. Developing such a sentiment classification model will also need the labels that will be used for training the network. We go over steps for developing a recurrent neural network model for sentiment classification using R in <a href="acfbe36f-dae6-40ad-96b5-0b0e87ce0f8d.xhtml">Chapter 10</a>, <em>Text Classification Using Recurrent Neural Networks</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Long short-term memory network</h1>
                </header>
            
            <article>
                
<p><strong>Long short-term memory</strong> (<strong>LSTM</strong>) networks are a special type of recurrent neural network. LSTM networks are useful when data regarding the sequence of words or integers has long-term dependencies. For example, two words that are important for correctly classifying sentiment contained in a movie review may be separated by many words in a long sentence. A sentiment classification model using a regular RNN will have difficulty capturing such long-term dependency between words. A regular RNN is useful when dependency between words or integers in a sequence is immediate or when two important words are next to each other. </p>
<p>Apart from sentiment classification, the application of LSTM networks can also be useful for speech recognition, language translation, anomaly detection, time series forecasting, answering questions, and so on. An application of an LSTM network for movie review sentiment classification is illustrated in <a href="da73d1c6-4377-4a8f-9bee-01262444f136.xhtml">Chapter 11</a>, <em>Text Classification Using Long Short-Term Memory Network</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional recurrent networks</h1>
                </header>
            
            <article>
                
<p><strong>Convolutional neural networks</strong> (<strong>CNNs</strong>) are useful for capturing high-level local features from the image or text data, and LSTM networks can capture long-term dependencies in the data involving sequences. When we use both CNNs and a recurrent network in the same model architecture, it is called a <strong>convolutional recurrent neural network</strong> (<strong>CRNN</strong>). As an example, if we consider data on articles and their authors, we may be interested in developing an author classification model where we can train a network to take text data containing an article as input and then help to make a prediction in terms of probability regarding the author. For this, we can first use a one-dimensional convolutional layer to extract important features from the data. These extracted features can then be passed to the LSTM recurrent layer to obtain the hidden long-term dependencies, which, in turn, are passed to a fully connected dense layer. This dense layer can then obtain the probability of correct authorship. CRNNs can also be applied to problems related to natural language processing, speech, and video. In <a href="be0c6dfc-045c-4698-b36d-74eca5e0a629.xhtml">Chapter 12</a>, <em>Text Classification Using Convolutional Recurrent Networks</em>, we illustrate the use of CRNNs for developing a model that can classify an author, based on articles written by them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tips, tricks, and best practices</h1>
                </header>
            
            <article>
                
<p>In this book, we provide an illustration of applying several popular deep learning methods using R. When working on more complex problems requiring the application of deep learning networks, the use of certain supporting tools may sometimes be very helpful. TensorFlow provides such a tool; it is called <strong>TensorBoard</strong> and is useful for visualizing deep network training performance, especially in situations that call for experimentation. Similarly, there is a package called <strong>Local Interpretable Model-Agnostic Explanations</strong> (<strong>LIME</strong>) that can help with visualization and interpretation of specific predictions. We also get many outputs, such as summaries and plots, when developing a deep network model. There is a package called <strong>tfruns</strong> that can help to keep everything in one place for easy reference. There is a callback feature in the Keras package that helps with stopping a network training at a suitable time. We will discuss all these tips, tricks, and best practices in <a href="af4eb94d-f4fb-41df-9df8-797e4771484d.xhtml">Chapter 13</a>, <em>Tips, Tricks, and the Road Ahead</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Deep learning methods that make use of artificial neural networks have been increasing in popularity in recent years. A number of areas of application involving deep learning methods include driverless cars, image classification, natural language processing, and new image generation. We started this first chapter by looking at the popularity of the deep learning term as reported from a Google trend website. We described a general five-step process for applying deep learning methods and developed some broad ideas about details within each step. We then briefly looked at deep learning techniques covered in each chapter and situations in which they are applied, along with some best practices. </p>
<p>In the next chapter, we get started with an application example and illustrate steps for developing a deep network model for multi-class classification problems.</p>


            </article>

            
        </section>
    </body></html>