- en: Working with Other Frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](95863955-3504-48ab-a217-e95339a754d3.xhtml), *Working with Caffe*,
    we learnt about Caffe and its relationship with Caffe2\. We examined the Caffe
    and Caffe2 model file formats and looked at the process of importing a pre-trained
    Caffe model into Caffe2 using AlexNet as an example. In this chapter, we will
    look at how to export from, and import to, Caffe2 from other popular DL frameworks.
    And we will also look at how to enable other DL frameworks to use a model trained
    with Caffe2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The ONNX model format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for ONNX in Caffe2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to export a Caffe2 model to ONNX format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to import an ONNX model into Caffe2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to visualize ONNX models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open Neural Network Exchange
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Open Neural Network Exchange** (**ONNX**), typically pronounced as *on-niks*,
    is a format to represent a computation graph, with support for a wide variety
    of operators and data types. This format is general enough to support both neural
    networks and traditional ML models. Started by Facebook and Microsoft, this format
    has quickly gained a reputation as a popular format for the export and import
    of deep neural networks among most DL frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: Installing ONNX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ONNX source code can be found online at: [https://github.com/onnx/onnx](https://github.com/onnx/onnx)
    This includes definitions of the format and scripts to operate on ONNX files.
    Libraries and tools to convert from and to specific DL framework formats are usually
    provided by DL frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: DL frameworks with built-in support for ONNX include Caffe2, PyTorch, MXNet,
    and Chainer. There are also converters to convert to and from other DL frameworks,
    such as TensorFlow. There are runtimes that can use ONNX models on specialized
    hardware accelerators. For example, TensorRT provides an inference runtime with
    ONNX support for use on NVIDIA GPUs, and OpenVINO does the same for use on Intel
    CPUs. (We will discuss TensorRT and OpenVINO in [Chapter 6](800ca2c2-fb20-4ad3-9268-12bb0aa83b8a.xhtml),
    *Deploying Models to Accelerators for Inference*.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python library of ONNX can be installed easily, using the following command
    on Ubuntu:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check if the installation was successful by testing whether the following
    command at the shell executes successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ONNX format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ONNX is an open source format and its specification and source code can be found
    online at [https://github.com/onnx/onnx](https://github.com/onnx/onnx). In [Chapter
    4](95863955-3504-48ab-a217-e95339a754d3.xhtml), *Working with Caffe*, we observed
    how both Caffe2 and Caffe use Google ProtoBuf for defining the data structure
    for serialization and deserialization of their network structures and weights.
    ONNX also uses Google ProtoBuf. It supports both ProtoBuf versions 2 and 3.
  prefs: []
  type: TYPE_NORMAL
- en: The definition of a graph, such as that of a neural network or generally any
    ML model, defines the various operators that the graph is composed of, the operators'
    parameters and the relationship between the operators. The syntax and semantics
    of this information are defined in ONNX as two distinct representations. The **Intermediate
    Representation** (**IR**) defines constructs, such as graph, node, and tensor.
    The operators define the various types of possible operators in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: ONNX IR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ProtoBuf definition of the ONNX computation graph and its data types can
    be found defined in the `onnx/onnx.in.proto` file in the ONNX source code. These
    are also referred to as the IR of ONNX.
  prefs: []
  type: TYPE_NORMAL
- en: 'By examining the IR definition of ONNX in the preceding file we can see the
    following definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NodeProto`: Used to define each of the layers in a neural network or each
    of the nodes in other ML models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ModelProto`: Used to define a model and its associated graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GraphProto`: Used to define the **directed acyclic graph** (**DAG**) structure
    of a neural network or the graph of other ML models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorProto`: Used to define an N-dimensional tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TypeProto`: Used to define the ONNX data types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ONNX operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The definition of an operator in ONNX can be found in the `onnx/onnx-operators.in.proto`
    file in the ONNX source code. We can find the definitions of `OperatorProto`,
    `OperatorSetProto`, and `FunctionProto` in this file.
  prefs: []
  type: TYPE_NORMAL
- en: The actual definitions of all the operators supported in ONNX can be found in
    C++ source files named `defs.cc` in subdirectories under the `onnx/defs` directory
    in the ONNX source code. For example, many of the common neural network operators
    can be found defined in the `onnx/defs/math/defs.cc` and `onnx/defs/nn/defs.cc`
    files in the ONNX source code.
  prefs: []
  type: TYPE_NORMAL
- en: 'For another example, consider the ReLU operator that we introduced in [Chapter
    3](3c2dd7d3-b762-49a3-a5d6-0b791eadadb2.xhtml), *Training Networks*. This operator
    has the name `Relu` (note the lower case *lu*) in ONNX and is defined in the `onnx/defs/math/defs.cc`
    file in the ONNX source code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that every operator is defined using the `ONNX_OPERATOR_SET_SCHEMA`
    macro. This macro is defined in the `onnx/defs/schema.h` source file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that every operator definition has three components: name (`name`),
    version (`ver`) and implementation (`impl`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, for the example of the `Relu` operator we saw in the preceding definition,
    we can deduce the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name**: The name of the operator in ONNX. In this case, it is `Relu`. Note
    that individual DL frameworks might map this name to a distinct operator or layer
    name in their own DL framework. That is, the name in ONNX and the corresponding
    name in the DL framework may not always be the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version**: The version of the definition of this operator. In this case,
    it is version 6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A documentation string explaining what the operator does. In this case, it
    is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"Relu takes one input data (Tensor<T>) and produces one output data (Tensor<T>)
    where the rectified linear function, y = max(0, x), is applied to the tensor elementwise."'
  prefs: []
  type: TYPE_NORMAL
- en: The input operands. In this case, a single tensor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output operands. In this case, a single tensor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constraints on the data type of the tensor values. In this case, ONNX is stating
    that it only supports data types of float (32-bit), double (64-bit) and float16
    (16-bit, sometimes called half) for tensor values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function to infer the type and shape of the tensor operands. In this case,
    it states that the output tensor must have the same type and shape as the input
    tensor. It does this by using the function named `propagateShapeAndTypeFromFirstInput`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the example of the preceding definition of the Relu operator, we can see
    that every operator definition has a lot of documentation embedded in it. All
    of this is used to auto-generate the complete ONNX operator documentation. This
    auto-generated documentation can be found as the `docs/Operators.md` files in
    the ONNX source code. This is a useful reference when we are searching for a suitable
    ONNX operator or trying to understand the details of a particular ONNX operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the auto-generated documentation of the `Relu` operator that we
    considered previously appears as shown as follows in Figure 5.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56a9af30-3dbc-4a97-89d4-f8d536d158f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Auto-generated Relu operator documentation in ONNX'
  prefs: []
  type: TYPE_NORMAL
- en: ONNX in Caffe2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Caffe2 has built-in support for ONNX. This includes support for exporting Caffe2
    models to ONNX format and importing ONNX models directly for inference in Caffe2\.
    C++ source files related to Caffe2's support of ONNX can be found in the `onnx`
    directory in the Caffe2 source code. Python source files that provide the frontend
    and backend support for ONNX can be found in the `python/onnx` directory in the
    Caffe2 source code.
  prefs: []
  type: TYPE_NORMAL
- en: The `onnx/onnx_exporter.h` and `onnx/onnx_exporter.cc` contain the definitions
    necessary to export a Caffe2 model to ONNX format. Support for exporting from
    Caffe2 to ONNX includes details such as the mapping from Caffe2 to ONNX for operators,
    data types, and transformations of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in `onnx/onnx_exporter.cc` we find the following mapping of some
    Caffe2 operators to ONNX operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Every DL framework that uses ONNX will have such a mapping. This is because
    every DL framework tends to have its own distinct operator or layer naming and
    a distinct jargon of defining the operator characteristics and relationships between
    operators. So, a clear and complete mapping is necessary for a DL framework to
    be able to digest an ONNX model definition into its own graph definition.
  prefs: []
  type: TYPE_NORMAL
- en: From the mapping between Caffe2 and ONNX we can see that the Caffe2 `SpatialBN`
    operator is renamed as the `BatchNormalization` operator in ONNX. Similarly, the
    Caffe2 `Conv2D` operator is renamed as the `Conv` operator in ONNX.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting the Caffe2 model to ONNX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Caffe2 models can be easily exported to ONNX format using Python. This enables
    a vast number of other DL frameworks to use our Caffe2 models for training and
    inference. The `frontend` module provided by Caffe2-ONNX does all of the heavy
    lifting of the exporting. This module is located as the `python/onnx/frontend.py`
    file in the Caffe2 source code.
  prefs: []
  type: TYPE_NORMAL
- en: The `ch5/export_to_onnx.py` script provided along with this book's source code
    shows how to export an existing Caffe2 model to ONNX format. As an example, consider
    converting the Caffe2 model of AlexNet that we created in [Chapter 4](95863955-3504-48ab-a217-e95339a754d3.xhtml),
    *Working wi**th Caffe*. We exported the operators and the weights of this network
    in Caffe2 to the files `predict_net.pb` and `init_net.pb` files respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can invoke the ONNX conversion script, as follows, to convert this Caffe2
    model to an ONNX file named `alexnet.onnx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at the pertinent sections of this script that help us to export from
    Caffe2 to ONNX.
  prefs: []
  type: TYPE_NORMAL
- en: 'First are the imports, which are seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `caffe2.proto.caffe2_pb2` module has the functionality needed to import
    the Caffe2 models stored in the `protobuf` format. The `onnx` and `caffe2.python.onnx.frontend`
    modules have the functionality that's necessary to export to ONNX format.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following script we also define the name and shape of the inputs to
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 4](95863955-3504-48ab-a217-e95339a754d3.xhtml), *Working with Caffe*,
    you might have noticed that the input layer and parameters are annotated in the
    Caffe `protobuf` format. However, this information is not stored in both the Caffe2
    `protobuf` format and the ONNX format. We would need to explicitly indicate the
    name and shape of the input whenever we use a Caffe2 and ONNX model.
  prefs: []
  type: TYPE_NORMAL
- en: We used an AlexNet model in this example, which has input named `data`, and
    the input shape is `(1, 3, 227, 227)`. Note that not all models have this input
    shape. For example, popular CNN models have inputs with the shape `(1, 3, 224,
    224)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to read in the Caffe2 model files using the `caffe2_pb2` methods,
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We need to read in both the `predict_net.pb` and `init_net.pb` Caffe2 model
    files, representing the network and its weights respectively. We do this by using
    the familiar `ParserFromString` method, which originates from the Google ProtoBuf
    Python library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we should initialize the data type and tensor shape of the input and associate
    that information with the input name using a Python dictionary, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now convert the Caffe2 `protobuf` objects to an ONNX `protobuf` object
    using the `caffe2_net_to_onnx_model` method of the `frontend` module, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note how this conversion method needs the input information, stored in `value_info`,
    for the conversion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can serialize the ONNX `protobuf` object to a byte buffer using
    the ProtoBuf `SerializeToString` method and then write that buffer to disk, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The full source code of the `ch5/export_to_onnx.py` script is listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Using the ONNX model in Caffe2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we converted a Caffe2 model to ONNX format so that
    it could be used with other DL frameworks. In this section, we will learn how
    to use an ONNX model exported from other DL frameworks into Caffe2 for inference.
  prefs: []
  type: TYPE_NORMAL
- en: The `backend` module provided in the Caffe2 ONNX package enables this import
    of the ONNX model to Caffe2\. This can be seen in the `backend.py` file in the
    `python/onnx` directory in the Caffe2 source code.
  prefs: []
  type: TYPE_NORMAL
- en: The `ch5/run_onnx_model.py` script provided along with this book's source code
    demonstrates how to load an ONNX model to Caffe2, and run an inference on an input
    image using that model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script first imports the Python modules necessary to work with the images
    (`PIL.Image`), Caffe2, and ONNX (`caffe2.python.onnx.backend`) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `prepare_input_image` method reads in an image from the input file path
    and prepares it to be passed as a blob to Caffe2, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we first used the `PIL.Image` module to read in the image
    from the input file as a 3-channel byte values. We then resized the image to the
    size required by AlexNet and used NumPy, which made the rest of the image processing
    easier. PIL reads the image channels in the order `HWC` (height, width, channel)
    and the channels are in `RGB` order. But AlexNet expects the data to be laid out
    as `BGR` channels of `HW` size. So, we converted to that format. Finally, we subtracted
    the mean from the image values and then added in a batch dimension in front to
    reformat the data to `NCHW` format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Loading the ONNX model from a file is easy if you use the `load` method from
    the `onnx` package, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use the loaded ONNX model for inference directly, using the
    `predict_img_class` method described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We need to use the `run_model` method, provided by Caffe2 ONNX backend `caffe2.python.backend`,
    to pass the inputs and obtain the results after inference through this model.
    Because we used an ImageNet model, we should use a JSON file with the mapping
    from the ImageNet class index number to its class name. We should pick the class
    index with the highest probability value and find its ImageNet class name.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the ONNX model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with ONNX models, it can be useful to have a tool that can help
    in visualizing the network structure. ONNX ships with such a script called `net_drawer.py`.
    You can find this tool in the `onnx/onnx/tools` directory in the ONNX source repository.
    If you installed ONNX from its Python package, then you can find this script at
    `/usr/local/lib/python2.7/dist-packages/onnx/tools/net_drawer.py`.
  prefs: []
  type: TYPE_NORMAL
- en: This script can be applied to convert an ONNX file to a directed acyclic graph
    representation of the network in the GraphViz DOT format. For example, consider
    the ONNX file `alexnet.onnx` that we obtained in the earlier section on converting
    from the Caffe2 model to the ONNX model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can convert this AlexNet ONNX file to a DOT file using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To convert the DOT file to a PNG image file for viewing, use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The image thus produced, shows the visualization of AlexNet
  prefs: []
  type: TYPE_NORMAL
- en: Another excellent visualization tool for ONNX models is Netron. The usage of
    this tool is covered in [Chapter 7](91e4cdcf-24f6-4426-ac95-b6845c020d83.xhtml),
    *Caffe2 at the Edge and in the cloud*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we introduced the details of the ONNX format, a popular representation
    for DL models. We examined how it depicts the intermediate representation and
    operators. We then looked at support for ONNX in Caffe2\. Using AlexNet as the
    example, we looked at how to convert a Caffe2 model file to ONNX format. We also
    looked at the reverse process: importing an ONNX model file into Caffe2, and then
    using it for inference. Finally, we looked at a useful tool to visualize the graph
    representation of an ONNX file.'
  prefs: []
  type: TYPE_NORMAL
