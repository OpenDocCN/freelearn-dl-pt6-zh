["```py\n# Random noise setting for Generator\nZ = tf.placeholder(tf.float32, shape=[None, 100], name='Z')\n\n#Generator parameter settings\nG_W1 = tf.Variable(xavier_init([100, 128]), name='G_W1')\nG_b1 = tf.Variable(tf.zeros(shape=[128]), name='G_b1')\nG_W2 = tf.Variable(xavier_init([128, 784]), name='G_W2')\nG_b2 = tf.Variable(tf.zeros(shape=[784]), name='G_b2')\ntheta_G = [G_W1, G_W2, G_b1, G_b2]\n\n# Generator Network\ndef generator(z):\n    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n    G_prob = tf.nn.sigmoid(G_log_prob)\n\n    return G_prob\n```", "```py\n#Input Image MNIST setting for Discriminator [28x28=784]\nX = tf.placeholder(tf.float32, shape=[None, 784], name='X')\n\n#Discriminator parameter settings\nD_W1 = tf.Variable(xavier_init([784, 128]), name='D_W1')\nD_b1 = tf.Variable(tf.zeros(shape=[128]), name='D_b1')\nD_W2 = tf.Variable(xavier_init([128, 1]), name='D_W2')\nD_b2 = tf.Variable(tf.zeros(shape=[1]), name='D_b2')\ntheta_D = [D_W1, D_W2, D_b1, D_b2]\n\n# Discriminator Network\ndef discriminator(x):\n    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n    D_prob = tf.nn.sigmoid(D_logit)\n\nreturn D_prob, D_logit\n```", "```py\nG_sample = generator(Z)\n\nD_real, D_logit_real = discriminator(X)\nD_fake, D_logit_fake = discriminator(G_sample)\n\n# Loss functions according the GAN original paper\nD_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1\\. - D_fake))\nG_loss = -tf.reduce_mean(tf.log(D_fake))\n```", "```py\n# Only update D(X)'s parameters, so var_list = theta_D\nD_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n# Only update G(X)'s parameters, so var_list = theta_G\nG_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n\ndef sample_Z(m, n):\n    '''Uniform prior for G(Z)'''\n    return np.random.uniform(-1., 1., size=[m, n])\n\nfor it in range(1000000):\n    X_mb, _ = mnist.train.next_batch(mb_size)\n\n    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n```", "```py\npython dcgan.py --mode train --batch_size <batch_size>\npython dcgan.py --mode generate --batch_size <batch_size> --nice\n\n```", "```py\ndef generator_model():\n    model = Sequential()\n    model.add(Dense(input_dim=100, output_dim=1024))\n    model.add(Activation('tanh'))\n    model.add(Dense(128*7*7))\n    model.add(BatchNormalization())\n    model.add(Activation('tanh'))\n    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,)))\n    model.add(UpSampling2D(size=(2, 2)))\n    model.add(Conv2D(64, (5, 5), padding='same'))\n    model.add(Activation('tanh'))\n    model.add(UpSampling2D(size=(2, 2)))\n    model.add(Conv2D(1, (5, 5), padding='same'))\n    model.add(Activation('tanh'))\n    return model\n```", "```py\ndef discriminator_model():\n    model = Sequential()\n    model.add(Conv2D(64, (5, 5), padding='same',input_shape=(28, 28, 1)))\n    model.add(Activation('tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(128, (5, 5)))\n    model.add(Activation('tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(1024))\n    model.add(Activation('tanh'))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    return model\n```", "```py\n    cd SSGAN-Tensorflow/\n\n    ```", "```py\n    python evaler.py --dataset CIFAR10 --checkpoint ckpt_dir\n\n    ```", "```py\nz = tf.random_uniform([self.batch_size, n_z], minval=-1, maxval=1, dtype=tf.float32)\n```", "```py\n# Generator model function\n        def G(z, scope='Generator'):\n            with tf.variable_scope(scope) as scope:\n                print ('\\033[93m'+scope.name+'\\033[0m')\n                z = tf.reshape(z, [self.batch_size, 1, 1, -1])\n                g_1 = deconv2d(z, deconv_info[0], is_train, name='g_1_deconv') \n                print (scope.name, g_1)\n                g_2 = deconv2d(g_1, deconv_info[1], is_train, name='g_2_deconv')\n                print (scope.name, g_2)\n                g_3 = deconv2d(g_2, deconv_info[2], is_train, name='g_3_deconv')\n                print (scope.name, g_3)\n                g_4 = deconv2d(g_3, deconv_info[3], is_train, name='g_4_deconv', activation_fn='tanh')\n                print (scope.name, g_4)\n                output = g_4\n                assert output.get_shape().as_list() == self.image.get_shape().as_list(), output.get_shape().as_list()\n            return output\n\n# Deconvolution method\ndef deconv2d(input, deconv_info, is_train, name=\"deconv2d\", stddev=0.02,activation_fn='relu'):\n    with tf.variable_scope(name):\n        output_shape = deconv_info[0]\n        k = deconv_info[1]\n        s = deconv_info[2]\n        deconv = layers.conv2d_transpose(input,\n            num_outputs=output_shape,\n            weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n            biases_initializer=tf.zeros_initializer(),\n            kernel_size=[k, k], stride=[s, s], padding='VALID')\n        if activation_fn == 'relu':\n            deconv = tf.nn.relu(deconv)\n            bn = tf.contrib.layers.batch_norm(deconv, center=True, scale=True, \n                decay=0.9, is_training=is_train, updates_collections=None)\n        elif activation_fn == 'tanh':\n            deconv = tf.nn.tanh(deconv)\n        else:\n            raise ValueError('Invalid activation function.')\n        return deconv\n```", "```py\n# Discriminator model function\n        def D(img, scope='Discriminator', reuse=True):\n            with tf.variable_scope(scope, reuse=reuse) as scope:\n                if not reuse: print ('\\033[93m'+scope.name+'\\033[0m')\n                d_1 = conv2d(img, conv_info[0], is_train, name='d_1_conv')\n                d_1 = slim.dropout(d_1, keep_prob=0.5, is_training=is_train, scope='d_1_conv/')\n                if not reuse: print (scope.name, d_1)\n                d_2 = conv2d(d_1, conv_info[1], is_train, name='d_2_conv')\n                d_2 = slim.dropout(d_2, keep_prob=0.5, is_training=is_train, scope='d_2_conv/')\n                if not reuse: print (scope.name, d_2)\n                d_3 = conv2d(d_2, conv_info[2], is_train, name='d_3_conv')\n                d_3 = slim.dropout(d_3, keep_prob=0.5, is_training=is_train, scope='d_3_conv/')\n                if not reuse: print (scope.name, d_3)\n                d_4 = slim.fully_connected(\n                    tf.reshape(d_3, [self.batch_size, -1]), n+1, scope='d_4_fc', activation_fn=None)\n                if not reuse: print (scope.name, d_4)\n                output = d_4\n                assert output.get_shape().as_list() == [self.batch_size, n+1]\n                return tf.nn.softmax(output), output\n\n# Convolution method with dropout\ndef conv2d(input, output_shape, is_train, k_h=5, k_w=5, stddev=0.02, name=\"conv2d\"):\n    with tf.variable_scope(name):\n        w = tf.get_variable('w', [k_h, k_w, input.get_shape()[-1], output_shape],\n                initializer=tf.truncated_normal_initializer(stddev=stddev))\n        conv = tf.nn.conv2d(input, w, strides=[1, 2, 2, 1], padding='SAME')\n\n        biases = tf.get_variable('biases', [output_shape], initializer=tf.constant_initializer(0.0))\n        conv = lrelu(tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape()))\n        bn = tf.contrib.layers.batch_norm(conv, center=True, scale=True, \n            decay=0.9, is_training=is_train, updates_collections=None)\n    return bn\n\n# Leaky Relu method\ndef lrelu(x, leak=0.2, name=\"lrelu\"):\n   with tf.variable_scope(name):\n   f1 = 0.5 * (1 + leak)\n   f2 = 0.5 * (1 - leak)\nreturn f1 * x + f2 * abs(x)\n```", "```py\n# Discriminator/classifier loss\ns_loss = tf.reduce_mean(huber_loss(label, d_real[:, :-1]))\n```", "```py\nd_loss_real = tf.nn.softmax_cross_entropy_with_logits(logits=d_real_logits, labels=real_label)\n d_loss_fake = tf.nn.softmax_cross_entropy_with_logits(logits=d_fake_logits, labels=fake_label)\nd_loss = tf.reduce_mean(d_loss_real + d_loss_fake)\n```", "```py\n# Huber loss\ndef huber_loss(labels, predictions, delta=1.0):\n    residual = tf.abs(predictions - labels)\n    condition = tf.less(residual, delta)\n    small_res = 0.5 * tf.square(residual)\n    large_res = delta * residual - 0.5 * tf.square(delta)\n    return tf.where(condition, small_res, large_res)\n\n# Generator loss\ng_loss = tf.reduce_mean(tf.log(d_fake[:, -1]))\n\ng_loss += tf.reduce_mean(huber_loss(real_image, fake_image)) * self.recon_weight\n```", "```py\n# Optimizer for discriminator\nself.d_optimizer = tf.contrib.layers.optimize_loss(\nloss=self.model.d_loss,\nglobal_step=self.global_step,\nlearning_rate=self.learning_rate*0.5,\noptimizer=tf.train.AdamOptimizer(beta1=0.5),\nclip_gradients=20.0,\nname='d_optimize_loss',\nvariables=d_var\n)\n\n# Optimizer for generator\nself.g_optimizer = tf.contrib.layers.optimize_loss(\nloss=self.model.g_loss,\nglobal_step=self.global_step,\nlearning_rate=self.learning_rate,\noptimizer=tf.train.AdamOptimizer(beta1=0.5),\nclip_gradients=20.0,\nname='g_optimize_loss',\nvariables=g_var\n)\n```", "```py\nfor s in xrange(max_steps):\n             step, accuracy, summary, d_loss, g_loss, s_loss, step_time, prediction_train, gt_train, g_img = \\\n               self.run_single_step(self.batch_train, step=s, is_train=True)\n```", "```py\ndistance (current parameters, average of parameters over the last t batches)\n```", "```py\nPixelShuffle- arXiv: 1609.05158, 2016 \n```", "```py\nwhile lossD > A:\n  train D\nwhile lossG > B:\n  train G\n```"]