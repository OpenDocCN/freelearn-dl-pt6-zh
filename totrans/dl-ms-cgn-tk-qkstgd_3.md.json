["```py\ncd ch3\njupyter notebook\n```", "```py\nimport numpy as np\n\nnum_samples = 20000\n\nlabel_mapping = np.eye(2)\ny = label_mapping[np.random.choice(2,num_samples)].astype(np.float32)\nX = np.random.random(size=(num_samples, 4)).astype(np.float32)\n```", "```py\n[0, 1]\n[1, 0]\n```", "```py\nfrom cntk.layers import Dense, Sequential\nfrom cntk import input_variable, default_options\nfrom cntk.ops import sigmoid\nfrom cntk.losses import binary_cross_entropy\n\nwith default_options(activation=sigmoid):\n   model = Sequential([\n        Dense(6),\n        Dense(2)\n    ])\n\nfeatures = input_variable(4)\n\nz = model(features)\n```", "```py\nfrom cntk.learners import sgd\nfrom cntk.logging import ProgressPrinter\n\nprogress_writer = ProgressPrinter(0)\n\nlabels = input_variable(2)\nloss = binary_cross_entropy(z, labels)\nlearner = sgd(z.parameters, lr=0.1)\n\ntraining_summary = loss.train((X,y), parameter_learners=[learner], callbacks=[progress_writer])\n```", "```py\n average      since    average      since      examples\n loss       last     metric       last \n ------------------------------------------------------\nLearning rate per minibatch: 0.5\n 1.4        1.4          0          0           512\n 1.4        1.4          0          0          1536\n 1.39       1.39          0          0          3584\n 1.39       1.39          0          0          7680\n 1.39       1.39          0          0         15872\n```", "```py\ntraining_summary = loss.train((X,y), \n    parameter_learners=[learner], \n    callbacks=[progress_writer],\n    minibatch_size=512)\n```", "```py\nfrom cntk import default_options, input_variable\nfrom cntk.layers import Dense, Sequential\nfrom cntk.ops import log_softmax, sigmoid\n\nmodel = Sequential([\n    Dense(4, activation=sigmoid),\n    Dense(3, activation=log_softmax)\n])\n\nfeatures = input_variable(4)\n\nz = model(features)\n```", "```py\nimport numpy as np\nimport pandas as pd\n\ndf_source = pd.read_csv('iris.csv', \n    names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], \n    index_col=False)\n\nlabel_mapping = {\n    'Iris-setosa': 0,\n    'Iris-versicolor': 1,\n    'Iris-virginica': 2\n}\n\nX = df_source.iloc[:, :4].values\n\ny = df_source['species'].values\ny = np.array([one_hot(label_mapping[v], 3) for v in y])\n\nX = X.astype(np.float32)\ny = y.astype(np.float32)\n```", "```py\ndef one_hot(index, length):\n    result = np.zeros(length)\n    result[index] = index\n\n    return result\n```", "```py\nfrom cntk.losses import cross_entropy_with_softmax\nfrom cntk.learners import sgd \nfrom cntk.logging import ProgressPrinter\n\nprogress_writer = ProgressPrinter(0)\n\nlabels = input_variable(3)\nloss = cross_entropy_with_softmax(z, labels)\nlearner = sgd(z.parameters, 0.1)\n\ntrain_summary = loss.train((X,y), \n    parameter_learners=[learner], \n    callbacks=[progress_writer], \n    minibatch_size=16, \n    max_epochs=5)\n```", "```py\naverage      since    average      since      examples\n loss       last     metric       last \n ------------------------------------------------------\nLearning rate per minibatch: 0.1\n 1.1        1.1          0          0            16\n 0.835      0.704          0          0            48\n 0.993       1.11          0          0           112\n 1.14       1.14          0          0            16\n 0.902      0.783          0          0            48\n 1.03       1.13          0          0           112\n 1.19       1.19          0          0            16\n 0.94      0.817          0          0            48\n 1.06       1.16          0          0           112\n 1.14       1.14          0          0            16\n 0.907       0.79          0          0            48\n 1.05       1.15          0          0           112\n 1.07       1.07          0          0            16\n 0.852      0.744          0          0            48\n 1.01       1.14          0          0           112\n```", "```py\nfrom cntk.io import StreamDef, StreamDefs, MinibatchSource, CTFDeserializer, INFINITELY_REPEAT\n\nlabels_stream = StreamDef(field='labels', shape=3, is_sparse=False)\nfeatures_stream = StreamDef(field='features', shape=4, is_sparse=False)\n\ndeserializer = CTFDeserializer('iris.ctf', StreamDefs(labels=labels_stream, features=features_stream))\n\nminibatch_source = MinibatchSource(deserializer, randomize=True)\n```", "```py\n|features 0.1 2.0 3.1 5.4 |labels 0 1 0\n|features 2.3 4.1 5.1 5.2 |labels 1 0 1\n```", "```py\nimport pandas as pd\nimport numpy as np\n\ndf_source = pd.read_csv('iris.csv', \n    names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], \n    index_col=False)\n\nfeatures = df_source.iloc[:,:4].values\nlabels = df_source['species'].values\n\nlabel_mapping = {\n    'Iris-setosa': 0,\n    'Iris-versicolor': 1,\n    'Iris-virginica': 2\n}\n\nlabels = [one_hot(label_mapping[v], 3) for v in labels]\n```", "```py\ndef one_hot(index, length):\n    result = np.zeros(length)\n    result[index] = 1\n\n    return result\n```", "```py\nwith open('iris.ctf', 'w') as output_file:\n    for index in range(0, features.shape[0]):\n        feature_values = ' '.join([str(x) for x in np.nditer(features[index])])\n        label_values = ' '.join([str(x) for x in np.nditer(labels[index])])\n\n        output_file.write('|features {} |labels {}\\n'.format(feature_values, label_values))\n```", "```py\nfrom cntk.logging import ProgressPrinter\nfrom cntk.train import Trainer, training_session\n\nminibatch_size = 16\nsamples_per_epoch = 150\nnum_epochs = 30\n\ninput_map = {\n    features: minibatch_source.streams.features,\n    labels: minibatch_source.streams.labels\n}\n\nprogress_writer = ProgressPrinter(0)\n\ntrain_history = loss.train(minibatch_source, \n           parameter_learners=[learner],\n           model_inputs_to_streams=input_map,\n           callbacks=[progress_writer],\n           epoch_size=samples_per_epoch,\n           max_epochs=num_epochs)\n```", "```py\naverage since average since examples loss    last  metric  last ------------------------------------------------------ Learning rate per minibatch: 0.1\n1.21    1.21  0       0     32\n1.15    1.12  0       0     96\n1.09    1.09  0       0     32\n1.03    1.01  0       0     96\n0.999   0.999 0       0     32\n0.999   0.998 0       0     96\n0.972   0.972 0       0     32\n0.968   0.966 0       0     96\n0.928   0.928 0       0     32\n[...]\n```", "```py\nfrom cntk import default_options, input_variable\nfrom cntk.layers import Dense, Sequential\nfrom cntk.ops import log_softmax, sigmoid\n\nmodel = Sequential([\n    Dense(4, activation=sigmoid),\n    Dense(3, activation=log_softmax)\n])\n\nfeatures = input_variable(4)\n\nz = model(features)\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom cntk.losses import cross_entropy_with_softmax\nfrom cntk.logging import ProgressPrinter\nfrom cntk.learners import sgd\nfrom cntk.train import Trainer\n\nlabels = input_variable(3)\nloss = cross_entropy_with_softmax(z, labels)\nlearner = sgd(z. parameters, 0.1)\n\nprogress_writer = ProgressPrinter(0)\ntrainer = Trainer(z, (loss, None), learner, progress_writer)\n\ninput_data = pd.read_csv('iris.csv', \n    names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], \n    index_col=False, chunksize=16)\n\nfor df_batch in input_data:\n    feature_values = df_batch.iloc[:,:4].values\n    feature_values = feature_values.astype(np.float32)\n\n    label_values = df_batch.iloc[:,-1]\n    label_values = label_values.map(lambda x: label_mapping[x])\n    label_values = label_values.values\n\n    encoded_labels = np.zeros((label_values.shape[0], 3))\n    encoded_labels[np.arange(label_values.shape[0]), label_values] = 1.\n\n    trainer.train_minibatch({features: feature_values, labels: encoded_labels})\n```"]