- en: '*Chapter 7*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Long Short-Term Memory (LSTM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Describe the purpose of an LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the architecture of an LSTM in detail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop a simple binary classification model using LSTMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement neural language translation and develop an English-to-German translation
    model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter briefly introduces you to the LSTM architecture and its applications
    in the world of natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapters, we studied Recurrent Neural Networks (RNNs) and a
    specialized architecture called the Gated Recurrent Unit (GRU), which helps combat
    the vanishing gradient problem. LSTMs offer yet another way to tackle the vanishing
    gradient problem. In this chapter, we will take a look at the architecture of
    LSTMs and see how they enable a neural network to propagate gradients in a faithful
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we will look at an interesting application of LSTMs in the form
    of neural language translation, which will empower us to build a model that can
    be used to translate text given in one language to another language.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The vanishing gradient problem makes it difficult for the gradient to propagate
    from the later layers in the network to the early layers, causing the initial
    weights of the network to not change much from the initial values. Thus, the model
    doesn't learn well and leads to poor performance. LSTMs solve the issue by introducing
    a "memory" to the network, which leads to the retention of long-term dependencies
    in the text structure. However, LSTMs add memory in a way that is different from
    the GRU's method. In the following sections, we will see how LSTMs accomplish
    this task.
  prefs: []
  type: TYPE_NORMAL
- en: An LSTM helps a network to remember long-term dependencies in an explicit manner.
    As in the case of the GRU, this is achieved by introducing more variables in the
    structure of a simple RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Using LSTMs, we allow the network to transfer most of the knowledge from the
    activation of previous timesteps, a feat difficult to achieve with simple RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the structure of the simple RNN; it''s essentially an unfolding of the
    same unit and can be represented by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1: The repeating module in a standard RNN](img/C13783_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: The repeating module in a standard RNN'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The recurrence of block "**A**"in the diagram signifies that it is the same
    structure that is repeated over time. The input to each unit is an activation
    from the previous timestep (represented by the letter "**h**"). Another input
    is the sequence value at time "**t**" (represented by the letter "**x**").
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the case with a simple RNN, LSTMs also have a fixed, time-unfolding,
    repeating structure, but the repeated unit itself has a different structure. Each
    unit of an LSTM has several different kinds of modules that interoperate to impart
    memory to the model. An LSTM''s structure can be represented by the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2: The LSTM unit'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.2: The LSTM unit'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s also get familiar with the notations we''ll be using for the diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3: Notations used in the model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.3: Notations used in the model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The most essential component of an LSTM is the cell state, henceforth represented
    by the letter "**C**". The cell state can be depicted by a constant bold line
    on the upper end of the boxes in the following diagram. It is often convenient
    to think of this line as a conveyor belt running through different time instances
    and carrying some information. Although there are several operations that can
    affect the value that propagates through the cell state, in practice, it is very
    easy for the information from previous cell states to reach the next cell state.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4: Cell state'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.4: Cell state'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It would be useful to understand LSTMs as seen from the perspective of the modification
    of this cell state. As with GRUs, the components of LSTMs that allow the modification
    of the cell state are called "*gates*".
  prefs: []
  type: TYPE_NORMAL
- en: An LSTM operates over several steps, which are described in the sections that
    follow.
  prefs: []
  type: TYPE_NORMAL
- en: The Forget Gate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The forget gate is responsible for determining the cell state content that
    should be forgotten from the previous timestep. The expression for the forget
    gate is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5: Expression for the forget gate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.5: Expression for the forget gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The input at timestep **t** is multiplied by a new set of weights, **W_f**,
    with the dimensions (**n_h**, **n_x**). The activation from the previous timestep
    (**h[t-1]**) is multiplied by another new set of weights, **U_f**, with the dimensions
    (**n_h**, **n_h**). Note that the multiplications are matrix multiplications.
    These two terms are then added and passed through a sigmoid function to squish
    the output, **f[t]**, within a range of [0,1]. The output has the same number
    of dimensions as there are in cell state vector C (**n_h**,**1**). The forget
    gate outputs a ''1'' or a ''0'' for each dimension. A value of ''1'' signifies
    that all information from the previous cell state for this dimension should pass,
    retained, while a value ''0'' indicates that all information from the previous
    cell state for this dimension should be forgotten. Diagrammatically, it can be
    represented as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6: The forget gate](img/C13783_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: The forget gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So, how does the output of the forget gate impact the sentence construction?
    Let''s take a look at the generated sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '"*Jack goes for a walk when his daughter goes to bed*."'
  prefs: []
  type: TYPE_NORMAL
- en: The first subject in the sentence is 'Jack,' which connotes the male gender.
    The cell state representing the gender of the subject has a value corresponding
    to 'Male' (this could be 0 or 1). Now, up to the word 'his' in the sentence, the
    subject of the sentence does not change, and the cell state for the subject's
    gender continues having the 'male' value. The next word, however, 'daughter,'
    is a new subject and hence there is a need to forget the old value in the cell
    state that represents the gender. Note that even if the old gender state was female,
    there is still a need to forget this value so that a value corresponding to the
    new subject can be used.
  prefs: []
  type: TYPE_NORMAL
- en: The forget gate accomplishes the 'forget' operation by setting the subject gender
    value to 0 (that is, f[t] will output 0 for the said dimension).
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, the forget gate can be calculated with the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This code produces the following output for `h_prev` and `x`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7: Output for the previous state, ‘h_prev,’ and the current input,
    ‘x’'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.7: Output for the previous state, ''h_prev,'' and the current input,
    ''x'''
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can initialize some dummy values for `W_f` and `U_f`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8: Output of the matrix values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.8: Output of the matrix values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now the forget gate can be calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following values for `f[t]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9: Output of the forget gate, f[t]'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.9: Output of the forget gate, f[t]'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Input Gate and the Candidate Cell State
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At each timestep, a new candidate cell state is also calculated using the following
    expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10: Expression for candidate cell state'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.10: Expression for candidate cell state'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The input at timestep **t** is multiplied by a new set of weights, **W_c**,
    with the dimensions (**n_h**, **n_x**). The activation from the previous timestep
    (**h[t-1]**) is multiplied by another new set of weights, **U_c**, with the dimensions
    (**n_h**, **n_h**). Note that the multiplications are matrix multiplications.
    These two terms are then added and passed through a hyperbolic tan function to
    squish the output, **f[t]**, within a range of [-1,1]. The output, **C_candidate**,
    has the dimensions (**n_h**,**1**). In the diagram that follows, the candidate
    cell state is represented by C tilde:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11: Input gate and candidate state'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.11: Input gate and candidate state'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The candidate aims at calculating the cell state that it deduces from the current
    timestep. In our example sentence, this corresponds to calculating the new subject
    gender value. This candidate cell state is not passed as is to update the next
    cell state but is regulated by an input gate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input gate determines which values of the candidate cell state get passed
    on to the next cell state. The following expression can be used to calculate the
    input gate value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12: Expression for the input gate value'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.12: Expression for the input gate value'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The input at timestep **t** is multiplied by a new set of weights, **W_i**,
    with the dimensions (**n_h**, **n_x**). The activation from the previous timestep
    (**h[t-1]**) is multiplied by another new set of weights, **U_i**, with the dimensions
    (**n_h**, **n_h**). Note that the multiplications are matrix multiplications.
    These two terms are then added and passed through a sigmoid function to squish
    the output, **i[t]**, within a range of **[0,1]**. The output has the same number
    of dimensions as there are in cell state vector **C** (**n_h**, **1**). In our
    example sentence, after reaching the word 'daughter,' there is a need to update
    the cell state for the values that correspond to the gender of the subject. After
    having calculated the new candidate value for the subject gender through the candidate
    cell state, only the dimension corresponding to the subject gender is set to 1
    in the input gate vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code snippet for the candidate cell state and input gate is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following values for the matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13: Screenshot of values of matrices for candidate cell state and
    input gate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.13: Screenshot of values of matrices for candidate cell state and
    input gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The input gate can be calculated as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following value for `i`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14: Screenshot of output of input gate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.14: Screenshot of output of input gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To calculate the candidate cell state, we first initialize the `W_c` and `U_c`
    matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The values produced for these matrices are as given:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15: Screenshot for values of matrices W_c and U_c'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.15: Screenshot for values of matrices W_c and U_c'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can now use the update equation for the candidate cell state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The candidate cell state produces the following value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16: Screenshot of the candidate cell state'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.16: Screenshot of the candidate cell state'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Cell State Update
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At this point, we know what should be forgotten from the old cell state (forget
    gate), what should be allowed to affect the new cell state (input gate), and what
    value the candidate cell change should have (candidate cell state). Now, the cell
    state for the current timestep can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17: Expression for cell state update'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.17: Expression for cell state update'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding expression, '**hadamard**' represents element-wise multiplications.
    So, the forget gate gets multiplied element wise with the old cell state, allowing
    it to forget the gender of the subject in our example sentence. On the other hand,
    the input gate allows the new candidate value for the gender of the subject to
    affect the new cell state. These two terms are then added element-wise so that
    the current cell state now has a subject gender that corresponds to a value that
    corresponds to 'female.'
  prefs: []
  type: TYPE_NORMAL
- en: The next diagram depicts the operation
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18: Updated cell state'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.18: Updated cell state'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here is the code snippet for producing the current cell state.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, initialize a value for the previous cell state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The value becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19: Screenshot for output of updated cell state'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.19: Screenshot for output of updated cell state'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Output Gate and Current Activation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Note that all we have done is update the cell state until now. We need to generate
    the activation for the current state as well; that is, (**h[t]**). This is done
    using an output gate that is calculated as given:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20: Expression for output gate.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.20: Expression for output gate.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The input at timestep **t** is multiplied by a new set of weights, **W_o**,
    with the dimensions (**n_h**, **n_x**). The activation from the previous timestep
    (**h[t-1]**) is multiplied by another new set of weights, **U_o**, with the dimensions
    (**n_h**, **n_h**). Note that the multiplications are matrix multiplications.
    These two terms are then added and passed through a sigmoid function to squish
    the output, **o[t]**, within a range of [0,1]. The output has the same number
    of dimensions as there are in cell state vector **h** (**n_h**, **1**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The output gate is responsible for regulating the amount by which the current
    cell state is allowed to affect the activation value for the timestep. In our
    example sentence, it is worth propagating the information that depicts whether
    the subject is singular or plural such that the correct verb form may be used.
    For example, if the word following the word ''daughter'' is a verb such as ''goes,''
    it is important to use the correct form of the word, ''go''. Hence, the output
    gate allows relevant information to be passed on to the activation, which then
    goes as an input to the next timestep. In the next diagram, the output gate is
    represented as **o_t**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21: Output gate and current activation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.21: Output gate and current activation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following code snippet shows how the value for the output gate can be calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.22: Screenshot for output of matrices W_o and U_o'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.22: Screenshot for output of matrices W_o and U_o'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now the output can be calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The value of the output gate is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23: Screenshot of the value of the output gate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.23: Screenshot of the value of the output gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Once the output gate is evaluated, the value of the next activation can be
    calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24: Expression to calculate the value of the next activation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.24: Expression to calculate the value of the next activation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: First, a hyperbolic tangent function is applied to the current cell state. This
    limits the values in the vector between -1 and 1\. Then, an element-wise product
    of this value is done with the output gate value that was just calculated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the code snippet for calculating the current timestep activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This finally produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25: Screenshot for the current timestep activation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.25: Screenshot for the current timestep activation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now let's build a very simple binary classifier to demonstrate the use of an
    LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 27: Building an LSTM-Based Model to Classify an Email as Spam or Not
    Spam (Ham)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will be building an LSTM-based model that will help us
    classify emails as spam or genuine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the required Python packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note:'
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The LSTM unit has been imported the same way as you would for a simple RNN or
    GRU.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can now read the input file containing a column that contains text and another
    column that contains the label for the text depicting whether the text is spam
    or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The data looks as depicted here:![Figure 7.26: Screenshot of the output for
    spam classification'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_07_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.26: Screenshot of the output for spam classification'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are some irrelevant columns as well, but we only need the columns containing
    the text data and labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be as follows:![Figure 7.27: Screenshot for columns with
    text and labels'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_07_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.27: Screenshot for columns with text and labels'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can check the label distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The label distribuiton would look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.28: Screenshot for label distribution'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_07_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.28: Screenshot for label distribution'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can now map the label distribution to 0/1 so that it can be fed to a classifier.
    Also, an array is created to contain the texts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces output X and Y as follows:![Figure 7.29: Screenshot for output
    X'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_07_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.29: Screenshot for output X'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 7.30: Screenshot for output Y'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_07_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.30: Screenshot for output Y'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, we will restrict the maximum number of tokens to be generated for the
    100 most frequent words. We will initialize a tokenizer that assigns an integer
    value to each word being used in the text corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will produce a `text_tokenized` value:![](img/C13783_07_31.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 7.31: Screenshot for the output of tokenized values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that since we restricted the maximum number words to be 100, only the words
    in the text that fall within the top 100 most frequent words will be assigned
    an integer index. The rest of the works will be ignored. So, even though the first
    sequence in X has 20 words, there are 6 indices in the tokenized representation
    of this sentence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will allow a maximum sequence length of 50 words per sequence and
    pad the sequences that are shorter than this length. The longer sequences, on
    the other hand, get truncated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.32: Screenshot for padded sequences'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_07_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.32: Screenshot for padded sequences'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that the padding was done in the 'pre' mode, meaning that the initial part
    of the sequences get padded to make the sequence length equal to max_len.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we define the model with the LSTM layer having 64 hidden units and fit
    it to our sequence data with the respective target values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we start with an embedding layer, which ensures a fixed size for input
    to the network (20). We have a dense layer with a single sigmoid output, which
    indicates whether the target variable is 0 or 1\. We then compile the model with
    binary cross-entropy as the loss function and use Adam as the optimization strategy.
    After that, we fit the model to our data with a batch size of 128 and an epoch
    count of 10\. Note that we also keep aside 20% of the training data as validation
    data. This starts a training session:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.33: Screenshot of model fitting to 10 epochs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.33: Screenshot of model fitting to 10 epochs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After 10 epochs, a validation accuracy of 96% is achieved. This is remarkably
    good performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now try some test sequences and obtain the probability of the sequence
    being spam:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**Expected output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.34: Screenshot of the output of model prediction'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_34.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.34: Screenshot of the output of model prediction'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There is a very high probability of the test text being spam.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9: Building a Spam or Ham Classifier Using a Simple RNN'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will be building a spam-or-ham classifier using a simple RNN with the same
    hyperparameters as earlier and compare the performance with that of our LSTM-based
    solution. For a simple dataset such as this, a simple RNN would perform very close
    to an LSTM. However, this is usually not the case with more complex models, as
    we will see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Find the input file at https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2007/exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Import the required Python packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the input file containing a column that contains text and another column
    that contains the label for the text depicting whether the text is spam or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert to sequences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pad the sequences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the sequences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict the mail category on the new test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.35: Output for mail category prediction'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_35.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.35: Output for mail category prediction'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for the activity can be found on page 324.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Language Translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simple binary classifier described in the previous section is a basic use
    case for the area of natural language processing (NLP) and doesn't fully justify
    the use of any techniques that are more complex than using a simple RNN or even
    simpler techniques. However, there are many complex use cases for which it is
    imperative to use more complex units such as LSTMs. Neural language translation
    is one such application.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of a neural language translation task is to build a model that can
    translate a piece of text from a source language to a target language. Before
    starting with the code, let's discuss the architecture of this system.
  prefs: []
  type: TYPE_NORMAL
- en: Neural language translation represents a many-to-many NLP application, which
    means that there are many inputs to the system and the system produces many outputs
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, the number of inputs and outputs could be different as the same
    text can have a different number of words in the source and target language. The
    area of NLP that solves such problems is referred to as sequence-to-sequence modeling.
    The architecture consists of an encoder block and a decoder block. The following
    diagram represents the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.36: Neural translation model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_36.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.36: Neural translation model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The left part of the architecture is the encoder block, and the right part
    is the decoder block. The diagram attempts to translate an English sentence to
    German, as here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'English: I would like to go swimming'
  prefs: []
  type: TYPE_NORMAL
- en: 'German: Ich möchte schwimmen gehen'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Periods have been dropped from the preceding sentences for demonstration purposes
    only. Periods are also considered valid tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder block takes each word of the English (source language) sentence
    as input at a given timestep. Each unit of the encoder block is an LSTM. The only
    outputs for the encoder block are the final cell state and activations. These
    are jointly referred to as the thought vector. The thought vector is used to initialize
    the activation and cell state for the decoder block, which is another LSTM block.
    During the training phase, at each timestep, the decoder output is the next word
    in the sentence. This is represented by a dense softmax layer that has a value
    1 for the next word token and 0 for all the other entries in the vector.
  prefs: []
  type: TYPE_NORMAL
- en: The English sentence is fed to the encoder word by word, producing a final cell
    state and activation. During the training phase, the real output of the decoder
    at each timestep is known. This is simply the next German word in the sentence.
    Note that there is a '**BEGIN_**' token inserted at the sentence beginning and
    an '**_END**' token at the end of the sentence. The output for the '**BEGIN_**'
    token is the first word in the German sentence. This can be seen in the last diagram.
    At the time of training, the network is made to learn the translation word by
    word.
  prefs: []
  type: TYPE_NORMAL
- en: In the inference phase, the English input sentence is fed to the encoder block,
    producing a final cell state and activation. The decoder has the '**BEGIN_**'
    token as the input at the first timestep, along with the cell state and activations.
    Using these three inputs, a softmax output is produced for this timestep. In a
    well-trained network, the softmax value is the highest for the entry corresponding
    to the correct word. This next word is then fed as the input to the next timestep.
    This process is continued until an '**_END**' token is sampled or a maximum sentence
    length is reached.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's go through the code for the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We read in the file containing sentence pairs first. We also keep the number
    of pairs restricted to 20,000 for demonstration purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.37: Screenshot for the English-to-German translation of sentence
    pairs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_37.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.37: Screenshot for the English-to-German translation of sentence pairs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Each line has first the English sentence, followed by a tab character, and
    then the German translation of the sentence. Next, we''ll map all the numbers
    to a placeholder word, ''**NUMBER_PRESENT**'', and append the ''**BEGIN_** ''
    and '' **_END**'' tokens to each German sentence, as discussed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous snippet, we obtained the input and output texts. They look
    as depicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.38: Screenshot for input and output texts after mapping'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_38.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.38: Screenshot for input and output texts after mapping'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, we get the maximum length of the input and output sequences and get a
    list of all the words in the input and output corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'input_words and target_words look as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.39: Screenshot for input text and target words'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_39.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.39: Screenshot for input text and target words'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, we generate an integer index for each token in the input and output words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The values of these variables are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.40: Screenshot for output of integer index for each token'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_40.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.40: Screenshot for output of integer index for each token'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We now define the arrays for the encoder input data, which is a 2-dimensional
    matrix with as many rows as sentence pairs and as many columns as the maximum
    input sequence length. Similarly, the decoder input data is also a 2-dimensional
    matrix with as many rows as sentence pairs and as many columns as the maximum
    sequence length in the target corpus. We also need target output data, which is
    required during the training phase. This is a 3-dimensional matrix where the first
    dimension has the same value as the number of sentence pairs. The second dimension
    has the same number of elements as the maximum target sequence length. The third
    dimension represents the number of decoder tokens (the number of distinct words
    in the target corpus). We initialize these variables with zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We now populate these matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The values look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.41: Screenshot of matrix population'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_41.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.41: Screenshot of matrix population'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We will now define a model. For this exercise, we''ll use the functional API
    of Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the encoder block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: First, an Input layer with a flexible number of inputs is defined (with the
    None attribute). Then, an embedding layer is defined and applied to the encoder
    inputs. Next, an LSTM unit is defined with 50 hidden units and applied to the
    embedding layer. Note that the return_state parameter in the LSTM definition is
    set to True since we would like to obtain the final encoder states to be used
    for initializing decoder cell state and activations. The encoder LSTM is then
    applied to the embeddings and the states are collected back into variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s define the decoder block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The decoder takes in inputs and defines embedding layers in a way similar to
    that of the encoder. An LSTM block is then defined with the return_sequences and
    return_state parameters set to True. This is done since we wish to use the sequences
    and states for the decoder. A dense layer is then defined with a softmax activation
    and a number of outputs equal to the number of distinct tokens in the target corpus.
    We can now define a model that takes in the encoder and decoder inputs as its
    input and produces the decoder outputs as final outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following model summary is seen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.42: Screenshot of model summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_42.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.42: Screenshot of model summary'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can now fit the model for our inputs and outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We set a batch size of 128 with 20 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.43: Screenshot of model fitting with 20 epochs](img/C13783_07_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.43: Screenshot of model fitting with 20 epochs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model is now trained. Now, as described in our section on neural language
    translation, the inference phase follows a slightly different architecture from
    the one used during training. We first define the encoder model, which takes encoder_inputs
    (with embedding) as input and produces encoder_states as output. This makes sense
    as the output of the encoder block is the cell state and activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, a decoder inference model is defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The initial states of decoder_lstm, which was trained earlier, are set to the
    decoder_states_inputs variable, which will be set to encoder state output later
    on. Then, we pass decoder outputs through a dense softmax layer for getting the
    index of the predicted word and define the decoder inference model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The decoder model takes multiple inputs in the form of decoder_input (with embedding)
    and decoder states. The output is also a multivariable where the dense layer output
    and decoder states are returned. The states are required here as they need to
    passed on as input states for the sampling of the word at the next timestep.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the output of the dense layer will return a vector, we need a reverse
    lookup dictionary to map the index for the generated word to an actual word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The values in the dictionaries are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.44: Screenshot of dictionary values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_44.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.44: Screenshot of dictionary values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We now need to develop a sampling logic. Given a token representation for every
    word in an input sentence, we first get the output from encoder_model using these
    word tokens as inputs for the encoder. We also initialize the first input word
    to the decoder to be a '**BEGIN_**' token. We then sample a new word token using
    these values. The input to the decoder for the next timestep is this newly generated
    token. We continue in this fashion until we either sample the '**_END**' token
    or reach the maximum allowed output sequence length.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is encoding the input as a state vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we generate an empty target sequence of length 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we populate the first character of the target sequence with the start
    character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create a sampling loop for a batch of sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we sample a token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we state the exit condition "**either hit max length**":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we update the states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'In this instance, you can test the model by translating a user-defined English
    sentence to German:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is depicted in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.45: Screenshot of English-to-German translator](img/C13783_07_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.45: Screenshot of English-to-German translator'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is, indeed, the correct translation.
  prefs: []
  type: TYPE_NORMAL
- en: So, even a model trained on just 20,000 sequences for only 20 epochs is capable
    of producing good translations. With the current settings, the training session
    ran for about 90 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 10: Creating a French-to-English translation model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we aim to generate a language translator model that converts
    French text into English.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find the related files to the activity at https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2007/activity.
  prefs: []
  type: TYPE_NORMAL
- en: Read in the sentence pairs (check the GitHub repository for the file).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate input and output texts with the '**BEGIN_**' and '**_END**' words attached
    to the output sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the input and output texts into input and output sequence matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the encoder and decoder training models and train the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the encoder and decoder architecture for inference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the user input text (French: '' *Où est ma voiture?*''). The sample
    output text in English should be ''*Where is my car?*''. Refer to the ''*French.txt*''
    file from the GitHub repository for some sample French words.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.46: Output for French to English translator model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_07_46.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.46: Output for French to English translator model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for the activity can be found on page 327.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduced LSTM units as a possible remedy to the vanishing gradient problem.
    We then discussed the LSTM architecture in detail and built a simple binary classifier
    using it. We then delved into a neural nanguage translation application that utilizes
    LSTM units, and we built a French-to-English translator model using the techniques
    we explored. In the next chapter, we will discuss the current state of the art
    in the NLP sphere.
  prefs: []
  type: TYPE_NORMAL
