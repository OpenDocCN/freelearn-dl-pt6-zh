<html><head></head><body>
		<div id="_idContainer660">
			<h1 class="chapterNumber sigil_not_in_toc">11</h1>
			<h1 class="chapterTitle" xml:lang="en-GB" id="sigil_toc_id_172" lang="en-GB"><a id="_idTextAnchor241"/>AI for Business – Minimize Costs with Deep Q-Learning</h1>
</div>

			<p class="normal">It's great that you can implement a deep Q-learning
 model to build a self-driving car. Really, once again, huge 
congratulations to you for that. But I also want you to be able to use 
deep Q-learning to solve a real-world business problem. With this next 
application, you'll be more than ready to add value to your work or 
business by leveraging AI. Even though we'll once again use a specific 
application, this chapter will provide you with a general AI fram<a id="_idTextAnchor242"/>ework,
 a blueprint containing the general steps of the process you have to 
follow when solving a real-world problem with deep Q-learning. This 
chapter is very important to you and for your career; I don't want you 
to close this book before you feel confident with the skills you'll 
learn here. Let's smash this next application together!</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_173" lang="en-GB"><a id="_idTextAnchor243"/>Problem to solve</h2>
			<p class="normal">When I said we were going to solve a real-world business problem, I didn't overstate the problem; the problem we're<a id="_idIndexMarker435"/> about to tackle with deep Q-learning is very similar to the following, which was solved in the real world via deep Q-learning.</p>
			<p class="normal">In 2016, DeepMind AI minimized a big part of 
Google's yearly costs by reducing the Google Data Center's cooling bill 
by 40% using their DQN AI model (deep Q-learning). Check the link here:</p>
			<p class="normal"><a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/"><span class="url">https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40</span></a></p>
			<p class="normal">In this case study, we'll do something very 
similar. We'll set up our own server environment, and we'll build an AI 
that controls the cooling and heating of the server so that it stays in 
an optimal range of temperatures while using the minimum of energy, 
therefore minimizing the costs. </p>
			<p class="normal">Just as the DeepMind AI did, our goal will be to achieve at least 40% energy savings! Are you ready for this? Let's bring it on!</p>
			<p class="normal">As ever, my first question to you is: What's our first step?</p>
			<p class="normal">I'm sure by this <a id="_idIndexMarker436"/>point I don't need to spell out the answer. Let's get straight to building our environment<a id="_idTextAnchor244"/>!</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_174" lang="en-GB"><a id="_idTextAnchor245"/>Building the environment</h2>
			<p class="normal">Before we define the<a id="_idIndexMarker437"/> states, actions, and rewards, we need to set up the server and explain how it operates. We'll do that in several steps:</p>
			<ol>
				<li class="list">First, we'll list all the environment parameters and variables by which the server is controlled.</li>
				<li class="list">After that we'll set the essential assumptions of the problem, on which your AI will rely to provide a solution.</li>
				<li class="list">Then we'll specify how you'll simulate the whole process.</li>
				<li class="list">Finally, we'll explain the overall functioning of the server, and how the AI plays its rol<a id="_idTextAnchor246"/>e.</li>
			</ol>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_175" lang="en-GB"><a id="_idTextAnchor247"/>Parameters and variables of the server environment</h3>
			<p class="normal">Here is a list of all the parameters, which keep their values fixed, of the <a id="_idIndexMarker438"/>server environment:</p>
			<ol>
				<li class="list" value="1">The average atmospheric temperature for each month.</li>
				<li class="list">The optimal temperature range of the server, which we'll set as <img src="../Images/B14110_11_001.png" alt=""/>.</li>
				<li class="list">The minimum temperature, below which the server fails to operate, which we'll set as <img src="../Images/B14110_11_002.png" alt=""/>.</li>
				<li class="list">The maximum temperature, above which the server fails to operate, which we'll set as <img src="../Images/B14110_11_003.png" alt=""/>.</li>
				<li class="list">The minimum number of users in the server, which we'll set as 10.</li>
				<li class="list">The maximum number of users in the server, which we'll set as 100.</li>
				<li class="list">The maximum change of users in the server per minute, which we'll set as 5; so every minute, the <a id="_idIndexMarker439"/>server can only have a change of 5 extra users or 5 fewer users at most.</li>
				<li class="list">The minimum rate<a id="_idIndexMarker440"/> of data transmission in the server, which we'll set as 20.</li>
				<li class="list">The maximum rate of data transmission in the server, which we'll set as 300.</li>
				<li class="list">The maximum change of the rate of data transmission
 per minute, which we'll set as 10; so every minute, the rate of data 
transmission can only change by a maximum value of 10 in either 
direction.</li>
			</ol>
			<p class="normal">Next, we'll list all the variables, which have values that fluctuate over time, of the server environment:</p>
			<ol>
				<li class="list" value="1">The temperature of the server at a given minute.</li>
				<li class="list">The number of users connected to the server at a given minute.</li>
				<li class="list">The rate of data transmission at a given minute.</li>
				<li class="list">The energy spent by the AI onto the server (to cool it down or heat it up) at a given minute.</li>
				<li class="list">The energy that would be spent by the server's 
integrated cooling system to automatically bring the server's 
temperature back to the optimal range, whenever the server's temperature
 goes outside this optimal range. This is to keep track of how much 
energy a <strong class="bold">non-AI</strong> system would use, so we can compare our AI system to it.</li>
			</ol>
			<p class="normal">All these parameters and variables will be part of the environment, and will influence the actions of our AI.</p>
			<p class="normal">Next, we'll explain the two core assumptions of the
 environment. It's important to understand that these assumptions are 
not AI related, but just used to simplify the environment so that we can
 focus on creating a functional AI solution.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_176" lang="en-GB"><a id="_idTextAnchor248"/>Assumptions of the server environment</h3>
			<p class="normal">We'll rely on the following<a id="_idIndexMarker441"/> two essential assumptions:</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_177" lang="en-GB"><a id="_idTextAnchor249"/>Assumption 1 – We can approximate the server temperature</h4>
			<p class="normal">The temperature of the<a id="_idIndexMarker442"/>
 server can be approximated through Multiple Linear Regression, that is,
 by a linear function of the atmospheric temperature, the number of 
users and the rate of data transmission, like so:</p>
			<p class="normal"><em class="italics">server temperature</em> = <img src="../Images/B14110_11_004.png" alt=""/> + <img src="../Images/B14110_11_005.png" alt=""/> <em class="italics">atmospheric temperature</em> + <img src="../Images/B14110_11_006.png" alt=""/> <em class="italics">number of users</em> + <img src="../Images/B14110_11_007.png" alt=""/> <em class="italics">rate of data transmission</em></p>
			<p class="normal">where <img src="../Images/B14110_11_008.png" alt=""/>, <img src="../Images/B14110_11_009.png" alt=""/>, <img src="../Images/B14110_11_010.png" alt=""/>, and <img src="../Images/B14110_11_011.png" alt=""/>.</p>
			<p class="normal">The raison d'être of this assumption and the reason why <img src="../Images/B14110_11_012.png" alt=""/>, <img src="../Images/B14110_11_013.png" alt=""/>, and <img src="../Images/B14110_11_014.png" alt=""/>
 are intuitive to understand. It makes sense that when the atmospheric 
temperature increases, the temperature of the server increases. The more
 users that are connected to the server, the more energy the server has 
to spend handling them, and therefore the higher the temperature of the 
server will be. Finally, the more data is transmitted inside the server,
 the more energy the server has to spend processing it, and therefore 
the higher the temperature of the server will be.</p>
			<p class="normal">For simplicity's sake, we can just suppose that 
these correlations are linear. However, you could absolutely run the 
same simulation by assuming they were quadratic or logarithmic, and 
altering the code to reflect those equations. This is just my simulation
 of a virtual server environment; feel free to tweak it as you like!</p>
			<p class="normal">Let's assume further that after performing this 
Multiple Linear Regression, we obtained the following values of the 
coefficients: <img src="../Images/B14110_11_015.png" alt=""/>, <img src="../Images/B14110_11_016.png" alt=""/>, <img src="../Images/B14110_11_017.png" alt=""/>, and <img src="../Images/B14110_11_018.png" alt=""/>. Accordingly:</p>
			<p class="normal"><em class="italics">server temperature</em> = <em class="italics">atmospheric temperature</em> + <img src="../Images/B14110_11_019.png" alt=""/> <em class="italics">number of users</em> + <img src="../Images/B14110_11_020.png" alt=""/> <em class="italics">rate of data transmission</em></p>
			<p class="normal">Now, if we were<a id="_idIndexMarker443"/> 
facing this problem in real life, we could get the dataset of 
temperatures for our server and calculate these values directly. Here, 
we're just assuming values that are easy to code and understand, because
 our goal in this chapter is not to perfectly model a real server; it's 
to go through the steps of solving a real-world problem with AI.</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_178" lang="en-GB"><a id="_idTextAnchor250"/>Assumption 2 – We can approximate the energy costs</h4>
			<p class="normal">The energy spent by any <a id="_idIndexMarker444"/>cooling
 system, either our AI or the server's integrated cooling system that 
we'll compare our AI to, that changes the server's temperature from <img src="../Images/B14110_11_021.png" alt=""/> to <img src="../Images/B14110_11_022.png" alt=""/>
 within 1 unit of time (in our case 1 minute), can be approximated again
 through regression by a linear function of the server's absolute 
temperature change, as so:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_023.png" style="height:2em;" alt=""/></figure>
			<p class="normal">where:</p>
			<ol>
				<li class="list" value="1"><span class="mediaobject"><img src="../Images/B14110_11_024.png" alt=""/></span> is the energy spent by the system on the server between times <img src="../Images/B14110_11_025.png" style="height:1em;" alt=""/> and <img src="../Images/B14110_11_026.png" style="height:1em;" alt=""/> minute.</li>
				<li class="list"><span class="mediaobject"><img src="../Images/B14110_11_027.png" alt=""/></span> is the change in the server's temperature caused by the system, between times <img src="../Images/B14110_11_028.png" style="height:1em;" alt=""/> and <img src="../Images/B14110_11_029.png" style="height:1em;" alt=""/> minute.</li>
				<li class="list"><span class="mediaobject"><img src="../Images/B14110_11_030.png" alt=""/></span> is the temperature of the server at time <img src="../Images/B14110_11_031.png" style="height:1em;" alt=""/>.</li>
				<li class="list"><span class="mediaobject"><img src="../Images/B14110_11_032.png" alt=""/></span> is the temperature of the server at time <img src="../Images/B14110_11_026.png" style="height:1em;" alt=""/> minute.</li>
				<li class="list"><span class="mediaobject"><img src="../Images/B14110_11_034.png" style="height:1.25em;" alt=""/></span>.</li>
				<li class="list"><span class="mediaobject"><img src="../Images/B14110_11_035.png" alt=""/></span>.</li>
			</ol>
			<p class="normal">Let's explain why it intuitively makes sense to make this assumption with <img src="../Images/B14110_11_036.png" style="height:1em;" alt=""/>.
 That's simply because the more the AI or the old-fashioned integrated 
cooling system heats up or cools down the server, the more energy it 
spends to achieve that heat transfer. </p>
			<p class="normal">For example, imagine the server suddenly has overheating issues and just reached <img src="../Images/B14110_11_037.png" alt=""/>C;
 then within one unit of time (1 minute), either system will need much 
more energy to bring the server's temperature back to its optimal 
temperature, <img src="../Images/B14110_11_038.png" style="height:1em;" alt=""/>C, than to bring it back to <img src="../Images/B14110_11_039.png" style="height:1em;" alt=""/>C.</p>
			<p class="normal">For simplicity's sake, in this example we suppose that these correlations are linear, instead of calculating true <a id="_idIndexMarker446"/>values
 from a real dataset. In case you're wondering why we take the absolute 
value, that's simply because when the AI cools down the server, <img src="../Images/B14110_11_040.png" alt=""/>, so <img src="../Images/B14110_11_041.png" alt=""/>. Since an energy cost is always positive, we have to take the absolute value of <img src="../Images/B14110_11_042.png" alt=""/>.</p>
			<p class="normal">Keeping our desired simplicity in mind, we'll assume that the results of the regression are <img src="../Images/B14110_11_043.png" alt=""/> and <img src="../Images/B14110_11_044.png" alt=""/>, so that we get the following final equation based on Assumption 2:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_045.png" style="height:2em;" alt=""/></figure>
			<p class="normal">thus:</p>
			<p class="normal"><span class="mediaobject"><img src="../Images/B14110_11_046.png" alt=""/></span>, that is, if the server is heated up,</p>
			<p class="normal"><span class="mediaobject"><img src="../Images/B14110_11_047.png" alt=""/></span>, that is, if the server is cooled down.</p>
			<p class="normal">Now we've got our assumptions covered, let's 
explain how we'll simulate the operation of the server, with users 
logging on and off and data coming in and out. </p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_179" lang="en-GB"><a id="_idTextAnchor251"/>Simulation</h3>
			<p class="normal">The number of users <a id="_idIndexMarker447"/>and
 the rate of data transmission will randomly fluctuate, to simulate the 
unpredictable user activity and data requirements of an actual server. 
This leads to randomness in the temperature. The AI needs to learn how 
much cooling or heating power it should transfer to the server so as to 
not deteriorate the server performance, and at the same time, expend as 
little energy as possible by optimizing its heat transfer.</p>
			<p class="normal">Now that we have the full picture, I'll explain the overall functioning of the server and the AI inside this environment.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_180" lang="en-GB"><a id="_idTextAnchor252"/>Overall functioning</h3>
			<p class="normal">Inside a data center, we're<a id="_idIndexMarker448"/>
 dealing with a specific server that is controlled by the parameters and
 variables listed previously. Every minute, some new users log on to the
 server and some current users log off, therefore updating the number of
 active users in the server. Also, every minute some new data is 
transmitted into the server, and some existing data is transmitted 
outside the server, therefore updating the rate of data transmission 
happening inside the server.</p>
			<p class="normal">Hence, based on <em class="italics">Assumption 1</em>
 given earlier, the temperature of the server is updated every minute. 
Now please focus, because this is where you'll understand the huge role 
the AI has to play on the server.</p>
			<p class="normal">Two possible systems can regulate the temperature 
of the server: the AI, or the server's integrated cooling system. The 
server's integrated cooling system is an unintelligent system that 
automatically brings the server's temperature back inside its optimal 
temperature range.</p>
			<p class="normal">Every minute, the server's temperature is updated. 
If the server is using the integrated cooling system, that system 
watches to see what happens; that update can either leave the 
temperature within the range of optimal temperatures (<img src="../Images/B14110_11_048.png" alt=""/>), or move it outside this range. If it goes outside the optimal range, for example to <img src="../Images/B14110_11_049.png" style="height:1em;" alt=""/>C,
 the server's integrated cooling system automatically brings the 
temperature back to the closest bound of the optimal range, in this case
 <img src="../Images/B14110_11_038.png" style="height:1em;" alt=""/>C. 
For the purposes of our simulation, we're assuming that no matter how 
big the change in temperature is, the integrated cooling system can 
bring it back into the optimal range in under a minute. This is, 
obviously, an unrealistic assumption, but the purpose of this chapter is
 for you to build a functioning AI capable of solving the problem, not 
to perfectly simulate the thermal dynamics of a real server. Once we've 
completed our example together, I highly recommend that you tinker with 
the code and try to make it more realistic; for now, to keep things 
simple, we'll believe in our magically effective integrated cooling 
system.</p>
			<p class="normal">If the server is instead using the AI, then in that
 case the server's integrated cooling system is deactivated and it is 
the AI itself that updates the temperature of the server to regulate it 
the best way. The AI <a id="_idIndexMarker449"/>changes the 
temperature after making some prior predictions, not in a purely 
deterministic way as with the unintelligent integrated cooling system. 
Before there's an update to the number of users and the rate of data 
transmission, causing a change in the temperature of the server, the AI 
predicts if it should cool down the server, do nothing, or heat up the 
server, and acts. Then the temperature change happens and the AI 
reiterates.</p>
			<p class="normal">Since these two systems are distinct from one 
another, we can evaluate them separately to compare their performance; 
to train or run the AI on a server, while keeping track of how much 
energy the integrated cooling system would have used in the same 
circumstances.</p>
			<p class="normal">That brings us to the energy. Remember that one 
primary goal of the AI is to lower the energy cost of running this 
server. Accordingly, our AI has to try and use less energy than the 
unintelligent cooling system would use on the server. Since, based on <em class="italics">Assumption 2</em>
 given preceding, the energy spent on the server (by any system) is 
proportional to the change of temperature within one unit of time:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_051.png" style="height:2em;" alt=""/></figure>
			<p class="normal">thus:</p>
			<p class="normal"><span class="mediaobject"><img src="../Images/B14110_11_052.png" alt=""/></span>, that is, if the server is heated up,</p>
			<p class="normal"><span class="mediaobject"><img src="../Images/B14110_11_053.png" alt=""/></span>, that is, if the server is cooled down,</p>
			<p class="normal">then that means that the energy saved by the AI at each iteration <img src="../Images/B14110_11_054.png" style="height:1em;" alt=""/>
 (each minute) is equal to the difference in absolute changes of 
temperatures caused in the server between the unintelligent server's 
integrated cooling system and the AI from <img src="../Images/B14110_11_055.png" style="height:1em;" alt=""/> and <img src="../Images/B14110_11_056.png" style="height:1em;" alt=""/>:</p>
			<p class="normal">Energy saved by the AI between <img src="../Images/B14110_11_057.png" style="height:1em;" alt=""/> and <img src="../Images/B14110_11_058.png" style="height:1em;" alt=""/></p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_059.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_060.png" alt=""/></span></figure>
			<p class="normal">where:</p>
			<ol>
				<li class="list" value="1"><img src="../Images/B14110_11_061.png" alt=""/> is the change of temperature<a id="_idIndexMarker450"/> that the server's integrated cooling system would cause in the server during the iteration <img src="../Images/B14110_11_062.png" style="height:1em;" alt=""/>, that is, from <img src="../Images/B14110_11_063.png" style="height:1em;" alt=""/> to <img src="../Images/B14110_11_064.png" style="height:1em;" alt=""/> minute.</li>
				<li class="list"><img src="../Images/B14110_11_065.png" alt=""/> is the change of temperature that the AI would cause in the server during the iteration <img src="../Images/B14110_11_062.png" style="height:1em;" alt=""/>, that is, from <img src="../Images/B14110_11_062.png" style="height:1em;" alt=""/> to <img src="../Images/B14110_11_068.png" style="height:1em;" alt=""/> minute.</li>
			</ol>
			<p class="normal">The AIs goal is to save as much as it can every 
minute, therefore saving the maximum total energy over 1 full year of 
simulation, and eventually saving the business the maximum cost possible
 on their cooling/heating electricity bill. That's how we do business in
 the 21st century; with AI!</p>
			<p class="normal">Now that we fully understand how our server environment works, and how it's simulated, it's time to proceed<a id="_idIndexMarker451"/> with what absolutely must be done when defining an AI environment. You know the next steps already:</p>
			<ol>
				<li class="list" value="1">Defining the states.</li>
				<li class="list">Defining the actions.</li>
				<li class="list">Defining the rewards.</li>
			</ol>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_181" lang="en-GB"><a id="_idTextAnchor253"/>Defining the states</h3>
			<p class="normal">Remember, when you're doing <a id="_idIndexMarker452"/>deep
 Q-learning, the input state is always a 1D vector. (Unless you are 
doing deep convolutional Q-learning, in which case the input state is a 
2D image, but that's getting ahead of ourselves! Wait for <em class="italics">Chapter 12</em>, <em class="italics">Deep Convolution Q-Learning</em>).
 So, what will the input state vector be in this server environment? 
What information will it contain in order to describe well enough each 
state of the environment? These are the questions you must ask yourself 
when modeling an AI problem and building the environment. Try to answer 
these questions first on your own and figure out the input state vector 
in this case, and you can find out what we're using in the next 
paragraph. Hint: have a look again at the variable defined preceding.</p>
			<p class="normal">The input state <img src="../Images/B14110_11_069.png" alt=""/> at time <img src="../Images/B14110_11_070.png" alt=""/> is composed of the following three elements:</p>
			<ol>
				<li class="list" value="1">The temperature of the server at time <img src="../Images/B14110_11_031.png" alt=""/></li>
				<li class="list">The number of users in the server at time <img src="../Images/B14110_11_072.png" alt=""/></li>
				<li class="list">The rate of data transmission in the server at time <img src="../Images/B14110_11_028.png" alt=""/></li>
			</ol>
			<p class="normal">Thus, the input state will be an input vector of 
these three elements. Our future AI will take this vector as input, and 
will return an action to perform at each time, <img src="../Images/B14110_11_074.png" alt=""/>. Speaking of the actions, what are they<a id="_idIndexMarker453"/> going to be? Let's find out.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_182" lang="en-GB"><a id="_idTextAnchor254"/>Defining the actions</h3>
			<p class="normal">To figure out which<a id="_idIndexMarker454"/> 
actions to perform, we need to remember the goal, which is to optimally 
regulate the temperature of the server. The actions are simply going to 
be the temperature changes that the AI can cause inside the server, in 
order to heat it up or cool it down. In deep Q-learning, the actions 
must always be discrete; they can't be plucked from a range, we need a 
defined number of possible actions. Therefore, we'll consider five 
possible temperature changes, from <img src="../Images/B14110_11_075.png" alt=""/>C to <img src="../Images/B14110_11_076.png" alt=""/>C, so that we end up with five possible actions that the AI can perform to regulate the temperature of the server:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_01.png" alt=""/></figure>
			<p class="packt_figref">Figure 1: Defining the actions</p>
			<p class="normal">Great. Finally, let's see how we're going to reward and punish our AI.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_183" lang="en-GB"><a id="_idTextAnchor255"/>Defining the rewards</h3>
			<p class="normal">You might have <a id="_idIndexMarker455"/>guessed from the earlier <em class="italics">Overall functioning</em> section what the reward is going to be. The reward at iteration <img src="../Images/B14110_11_054.png" alt=""/>
 is the energy saved by the AI, with respect to how much energy the 
server's integrated cooling system would have spent; that is, the 
difference between the energy that the unintelligent cooling system 
would spend if the AI was deactivated, and the energy that the AI spends
 on the server:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_078.png" style="height:2em;" alt=""/></figure>
			<p class="normal">Since according to <em class="italics">Assumption 2</em>,
 the energy spent is equal to the change of the temperature induced in 
the server (by any system, including the AI or the unintelligent cooling
 system):</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_079.png" style="height:2em;" alt=""/></figure>
			<p class="normal">thus:</p>
			<p class="normal"><img src="../Images/B14110_11_080.png" alt=""/>, if the server is<a id="_idIndexMarker456"/> heated up,</p>
			<p class="normal"><img src="../Images/B14110_11_081.png" alt=""/>, if the server is cooled down,</p>
			<p class="normal">then we receive a reward at time <img src="../Images/B14110_11_028.png" alt=""/>
 that is the difference in the change of temperature caused in the 
server between unintelligent cooling system (that is when there is no 
AI) and the AI:</p>
			<p class="normal">Energy saved by the AI between <img src="../Images/B14110_11_083.png" alt=""/> and <img src="../Images/B14110_11_084.png" alt=""/></p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_085.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_086.png" alt=""/></span></figure>
			<p class="normal">where:</p>
			<ol>
				<li class="list" value="1"><span class="mediaobject"><img src="../Images/B14110_11_087.png" alt=""/></span> is the change of temperature that the server's integrated cooling system would cause in the server during the iteration <img src="../Images/B14110_11_088.png" alt=""/>, that is, from <img src="../Images/B14110_11_028.png" alt=""/> to <img src="../Images/B14110_11_026.png" alt=""/> minute.</li>
				<li class="list"><span class="mediaobject"><img src="../Images/B14110_11_091.png" alt=""/></span> is the change <a id="_idIndexMarker457"/>of temperature that the AI would cause in the server during the iteration <img src="../Images/B14110_11_062.png" alt=""/>, that is, from <img src="../Images/B14110_11_062.png" alt=""/> to <img src="../Images/B14110_11_094.png" alt=""/> minute.</li>
			</ol>
			<p class="normal"><strong class="bold">Important note</strong>: It's 
important to understand that the systems (our AI and the server's 
integrated cooling system) will be evaluated separately, in order to 
compute the rewards. Since at each time point the actions of the two 
different systems lead to different temperatures, we have to keep track 
of the two temperatures separately, as <img src="../Images/B14110_11_095.png" alt=""/> and <img src="../Images/B14110_11_096.png" alt=""/>.
 In other words, we're performing two separate simulations at the same 
time, following the same fluctuations of users and data; one for the AI,
 and one for the server's integrated cooling system.</p>
			<p class="normal">To complete this section, we'll do a small 
simulation of 2 iterations (that is, 2 minutes) as an example to make 
everything crystal clear.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_184" lang="en-GB"><a id="_idTextAnchor256"/>Final simulation example</h3>
			<p class="normal">Let's say that we're <a id="_idIndexMarker458"/>at time <img src="../Images/B14110_11_097.png" alt=""/> pm, and that the temperature of the server is <img src="../Images/B14110_11_098.png" alt=""/>C,
 both with the AI and without it. At this exact time, the AI predicts an
 action: 0, 1, 2, 3 or 4. Since, right now, the server's temperature is 
outside the optimal temperature range, <img src="../Images/B14110_11_099.png" alt=""/>, the AI will probably predict actions 0, 1 or 2. Let's say that it predicts 1, which corresponds to cooling the server down by <img src="../Images/B14110_11_100.png" alt=""/>C. Therefore, between <img src="../Images/B14110_11_097.png" alt=""/> pm and <img src="../Images/B14110_11_102.png" alt=""/> pm, the AI makes<a id="_idIndexMarker459"/> the server's temperature go from <img src="../Images/B14110_11_103.png" alt=""/> to <img src="../Images/B14110_11_104.png" alt=""/>:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_105.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_106.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_107.png" style="height:1em;" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_108.png" style="height:1em;" alt=""/></span></figure>
			<p class="normal">Thus, based on <em class="italics">Assumption 2</em>, the energy spent by the AI on the server is:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_109.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_110.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_111.png" style="height:1em;" alt=""/></span></figure>
			<p class="normal">Now only one piece of information is missing to 
compute the reward: the energy that the server's integrated cooling 
system would have spent if the AI was deactivated between 4:00 pm and 
4:01 pm. Remember that this unintelligent cooling system automatically 
brings the server's temperature<a id="_idIndexMarker460"/> back to the closest bound of the optimal temperature range <img src="../Images/B14110_11_112.png" alt=""/>. Since at <img src="../Images/B14110_11_097.png" alt=""/> pm the temperature was <img src="../Images/B14110_11_114.png" alt=""/>C, then the closest bound of the optimal temperature range at that time was <img src="../Images/B14110_11_115.png" alt=""/>C. Thus, the server's integrated cooling system would have changed the temperature from <img src="../Images/B14110_11_116.png" alt=""/> to <img src="../Images/B14110_11_117.png" alt=""/>, and the server's temperature change that would have occurred if there was no AI is:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_118.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_119.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_120.png" style="height:1em;" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_121.png" style="height:1em;" alt=""/></span></figure>
			<p class="normal">Based on <em class="italics">Assumption 2</em>, the energy that the unintelligent cooling system would have spent if there was no AI is:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_122.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_123.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_1231.png" alt=""/></span><span class="mediaobject"><img src="../Images/B14110_11_124.png" style="height:1.25em;" alt=""/></span></figure>
			<p class="normal">In conclusion, the reward the AI gets <a id="_idIndexMarker461"/>after playing this action at time <img src="../Images/B14110_11_097.png" alt=""/> pm is:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_126.png" style="height:1em;" alt=""/></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_127.png" style="height:1.5em;" alt=""/></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_128.png" style="height:1em;" alt=""/></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_129.png" style="height:1em;" alt=""/></figure>
			<p class="normal">I'm sure you'll have noticed that as it stands, our
 AI system doesn't involve itself with the optimal range of temperatures
 for the server; as I've mentioned before, everything comes from the 
rewards, and the AI doesn't get any reward for being inside the optimal 
range or any penalty for being outside it. Once we've built the AI 
completely, I recommend that you play around with the code and try 
adding some rewards or penalties that get the AI to stick close to the 
optimal range; but for now, to keep things simple and get our AI up and 
running, we'll leave the reward as entirely linked to energy saved.</p>
			<p class="normal">Then, between <img src="../Images/B14110_11_130.png" alt=""/> pm and <img src="../Images/B14110_11_131.png" alt=""/>
 pm, new things happen: some new users log on to the server, some 
existing users log off, some new data transmits into the server, and 
some existing data transmits out. Based on <em class="italics">Assumption 1</em>, these factors make the server's temperature change. Let's say that overall, they increase the server's temperature by <img src="../Images/B14110_11_132.png" alt=""/>C:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_133.png" style="height:1.5em;" alt=""/></figure>
			<p class="normal">Now, remember that we're evaluating two systems separately: our AI, and the server's integrated cooling system. Therefore <a id="_idIndexMarker462"/>we must compute the two temperatures we would get with each of these two systems separately, one without the other, at <img src="../Images/B14110_11_134.png" alt=""/> pm. Let's start with the AI.</p>
			<p class="normal">The temperature we get at <img src="../Images/B14110_11_102.png" alt=""/> pm when the AI is activated is:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_136.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_137.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_138.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_139.png" style="height:1em;" alt=""/></span></figure>
			<p class="normal">And the temperature we get at <img src="../Images/B14110_11_140.png" alt=""/> pm if the AI is not activated is:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_141.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_142.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_143.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_144.png" style="height:1em;" alt=""/></span></figure>
			<p class="normal">Now we have our two separate temperatures, which are <img src="../Images/B14110_11_145.png" alt=""/>= 31.5°<em class="italics">C </em>when the AI is activated, and <img src="../Images/B14110_11_146.png" alt=""/>= 29°<em class="italics">C</em> when<a id="_idIndexMarker463"/> the AI is not activated.</p>
			<p class="normal">Let's simulate what happens between <img src="../Images/B14110_11_147.png" alt=""/> pm and <img src="../Images/B14110_11_148.png" alt=""/>
 pm. Again, our AI will make a prediction, and since the server is 
heating up, let's say it predicts action 0, which corresponds to cooling
 down the server by <img src="../Images/B14110_11_149.png" alt=""/>, bringing it down to <img src="../Images/B14110_11_150.png" alt=""/>. Therefore, the energy spent by the AI between <img src="../Images/B14110_11_151.png" alt=""/> pm and <img src="../Images/B14110_11_152.png" alt=""/> pm is:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_153.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_154.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_155.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_156.png" style="height:1em;" alt=""/></span></figure>
			<p class="normal">Now regarding the server's integrated cooling system (that is, when there is no AI), since at <img src="../Images/B14110_11_157.png" alt=""/> pm we had <img src="../Images/B14110_11_158.png" alt=""/>, then the closest bound of the optimal range of temperatures is still <img src="../Images/B14110_11_159.png" alt=""/>, and so the energy that the server's unintelligent cooling system would spend <a id="_idIndexMarker464"/>between <img src="../Images/B14110_11_102.png" alt=""/> pm and <img src="../Images/B14110_11_161.png" alt=""/> pm is:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_162.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_163.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_164.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_165.png" style="height:1em;" alt=""/></span></figure>
			<p class="normal">Hence the reward obtained between <img src="../Images/B14110_11_166.png" alt=""/> pm and <img src="../Images/B14110_11_167.png" alt=""/> pm, which is only and entirely based on the amount of energy saved, is:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_168.png" style="height:1em;" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_169.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_170.png" style="height:1em;" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_171.png" style="height:1em;" alt=""/></span></figure>
			<p class="normal">Finally, the total reward <a id="_idIndexMarker465"/>obtained between <img src="../Images/B14110_11_172.png" alt=""/> pm and <img src="../Images/B14110_11_173.png" alt=""/> pm is:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_174.png" style="height:1em;" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_175.png" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_176.png" style="height:1em;" alt=""/></span></figure>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><span class="mediaobject"><img src="../Images/B14110_11_177.png" style="height:1em;" alt=""/></span></figure>
			<p class="normal">That was an example of the whole process happening 
for two minutes. In our implementation we'll run the same process over 
1000 epochs of 5-month periods for the training, and then, once our AI 
is trained, we'll run the same process over 1 full year of simulation 
for the testing.</p>
			<p class="normal">Now that we've defined and built the environment in detail, it's time for our AI to take action! This is where<a id="_idIndexMarker466"/>
 deep Q-learning comes into play. Our model will be more advanced than 
the previous one because I'm introducing some new tricks, called <strong class="bold">dropout</strong> and <strong class="bold">early stopping</strong>, which <a id="_idIndexMarker467"/>are great techniques for you to have in your toolkit; they usually improve the training performance of deep Q-learning.</p>
			<p class="normal">Don't forget, you'll also get an AI Blueprint, 
which will allow you to adapt what we do here to any other business 
problem that you want to solve with deep Q-learning.</p>
			<p class="normal">Ready? Let's smash this.</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_185" lang="en-GB"><a id="_idTextAnchor257"/>AI solution</h2>
			<p class="normal">Let's start by reminding ourselves of the whole deep Q-learning model, while adapting it to this case <a id="_idIndexMarker468"/>study,
 so that you don't have to scroll or turn many pages back into the 
previous chapters. Repetition is never bad; it sticks the knowledge 
into our heads more firmly. Here's the deep Q-learning algorithm for you
 again:</p>
			<p class="normal">Initialization:</p>
			<ol>
				<li class="list" value="1">The memory of the experience replay is initialized to an empty list, called <code class="Code-In-Text--PACKT-">memory</code> in the code (the <code class="Code-In-Text--PACKT-">dqn.py</code> Python file in the <code class="Code-In-Text--PACKT-">Chapter 11</code> folder of the GitHub repo).</li>
				<li class="list">We choose a maximum size for the memory, called <code class="Code-In-Text--PACKT-">max_memory</code> in the code (the <code class="Code-In-Text--PACKT-">dqn.py</code> Python file in the <code class="Code-In-Text--PACKT-">Chapter 11</code> folder of the GitHub repo).</li>
			</ol>
			<p class="normal">At each time <em class="italics">t</em> (each minute), we repeat the following process, until the end of the epoch:</p>
			<ol>
				<li class="list" value="1">We predict the Q-values of the current state <img src="../Images/B14110_11_178.png" alt=""/>.
 Since five actions can be performed (0 == Cooling 3°C, 1 == Cooling 
1.5°C, 2 == No Heat Transfer, 3 == Heating 1.5°C, 4 == Heating 3°C), we 
get five predicted Q-values.</li>
				<li class="list">We perform the action selected by the argmax 
method, which simply consists of selecting the action that has the 
highest of the five predicted Q-values:<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_179.png" style="height:2.5em;" alt=""/></figure></li>
				<li class="list">We get the reward <img src="../Images/B14110_11_180.png" alt=""/>, which is the difference <img src="../Images/B14110_11_181.png" alt=""/>.</li>
				<li class="list">We reach the next state <img src="../Images/B14110_07_0452.png" alt=""/>, which is composed of the three following elements:<ul><li class="Bullet-Within-Bullet--PACKT-">The temperature of the server at time <img src="../Images/B14110_11_183.png" alt=""/></li>
<li class="Bullet-Within-Bullet--PACKT-">The number of users in the server at time <img src="../Images/B14110_11_094.png" alt=""/></li>
<li class="Bullet-Within-Bullet--PACKT-">The rate of data<a id="_idIndexMarker469"/> transmission in the server at time <img src="../Images/B14110_11_185.png" alt=""/></li>
</ul></li>
				<li class="list">We append the transition <img src="../Images/B14110_11_186.png" alt=""/> in the memory.</li>
				<li class="list">We take a random batch <img src="../Images/B14110_11_187.png" alt=""/> of transitions. For all the transitions <img src="../Images/B14110_11_188.png" alt=""/> of the random batch <img src="../Images/B14110_11_189.png" alt=""/>:<ul><li class="Bullet-Within-Bullet--PACKT-">We get the predictions: <img src="../Images/B14110_11_190.png" alt=""/></li>
<li class="Bullet-Within-Bullet--PACKT-">We get the targets: <img src="../Images/B14110_11_191.png" alt=""/></li>
<li class="Bullet-Within-Bullet--PACKT-">We compute the loss between the predictions and the targets over the whole batch <img src="../Images/B14110_11_192.png" alt=""/>:
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_193.png" style="height:3em;" alt=""/></figure>
</li>
</ul></li>
			</ol>
			<p class="normal">And then finally we backpropagate this loss error 
back into the neural network, and through stochastic gradient descent we
 update the weights according to how much they contributed to the loss 
error.</p>
			<p class="normal">I hope the refresher was refreshing! Let's <a id="_idIndexMarker470"/>move on to the brain of the outfit.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_186" lang="en-GB"><a id="_idTextAnchor258"/>The brain</h3>
			<p class="normal">By the brain, I mean of course the artificial neural network of our AI.</p>
			<p class="normal">Our brain will be a fully connected neural network, composed of two<a id="_idIndexMarker471"/> hidden<a id="_idTextAnchor259"/>
 layers, the first one with 64 neurons, and the second one with 32 
neurons. As a reminder, this neural network takes as inputs the states 
of the environment, and returns as outputs the Q-values for each of the 
five possible actions.</p>
			<p class="normal">This particular design of a neural network, with 
two hidden layers of 64 and 32 neurons respectively, is considered 
something of a <strong class="bold">classic</strong> architecture. It's suitable to solve a lot of problems, and it will work well for us here.</p>
			<p class="normal">This artificial brain will be trained<a id="_idIndexMarker472"/> with a <strong class="bold">Mean Squared Error</strong> (<strong class="bold">MSE</strong>) loss, and an <code class="Code-In-Text--PACKT-">Adam</code>
 optimizer. The choice for the MSE loss is because we want to measure 
and reduce the squared difference between the predicted value and the 
target value, and the <code class="Code-In-Text--PACKT-">Adam</code> optimizer is a classic optimizer used, in practice, by default.</p>
			<p class="normal">Here is what this artificial brain looks like:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_02.png" alt=""/></figure>
			<p class="packt_figref">Figure 2: The artificial brain of our AI</p>
			<p class="normal">This artificial brain looks complex to create, but we can build it very easily thanks to the amazing Keras library. In<a id="_idIndexMarker473"/>
 the last chapter, we used PyTorch because it's the neural network 
library I'm more familiar with; but I want you to be able to use as many
 AI tools as possible, so in this chapter we're going to power on with 
Keras. Here's a preview of the full implementation containing the part 
that builds this brain all by itself (taken from the <code class="Code-In-Text--PACKT-">brain_nodropout.py</code> file):</p>
			<pre class="programlisting language-markup"><code class="hljs reasonml"># BUILDING THE BRAIN
<span class="hljs-keyword">class</span> <span class="hljs-constructor">Brain(<span class="hljs-params">object</span>)</span>:
    
    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD
    
    def <span class="hljs-constructor">__init__(<span class="hljs-params">self</span>, <span class="hljs-params">learning_rate</span> = 0.001, <span class="hljs-params">number_actions</span> = 5)</span>:
        self.learning_rate = learning_rate
        
        # BUILDING THE INPUT LAYER COMPOSED OF THE INPUT STATE
        states = <span class="hljs-constructor">Input(<span class="hljs-params">shape</span> = (3,)</span>)
        
        # BUILDING THE FULLY CONNECTED HIDDEN LAYERS
        x = <span class="hljs-constructor">Dense(<span class="hljs-params">units</span> = 64, <span class="hljs-params">activation</span> = '<span class="hljs-params">sigmoid</span>')</span>(states)
        y = <span class="hljs-constructor">Dense(<span class="hljs-params">units</span> = 32, <span class="hljs-params">activation</span> = '<span class="hljs-params">sigmoid</span>')</span>(x)
        
        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER
        q_values = <span class="hljs-constructor">Dense(<span class="hljs-params">units</span> = <span class="hljs-params">number_actions</span>, <span class="hljs-params">activation</span> = '<span class="hljs-params">softmax</span>')</span>(y)
        
        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT
        self.model = <span class="hljs-constructor">Model(<span class="hljs-params">inputs</span> = <span class="hljs-params">states</span>, <span class="hljs-params">outputs</span> = <span class="hljs-params">q_values</span>)</span>
        
        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER
        self.model.compile(loss = 'mse', optimizer = <span class="hljs-constructor">Adam(<span class="hljs-params">lr</span> = <span class="hljs-params">learning_rate</span>)</span>)
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal">As you can see, it only takes a <a id="_idIndexMarker474"/>couple
 of lines of code, and I'll explain every line of that code to you in a 
later section. Now let's move on to the implementation.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_187" lang="en-GB"><a id="_idTextAnchor260"/>Implementation</h3>
			<p class="normal">This implementation will be divided into five parts, each part having its own Python file. You <a id="_idIndexMarker475"/>can find the full implementation in the <code class="Code-In-Text--PACKT-">Cha<a id="_idTextAnchor261"/>pter 11</code>
 folder of the GitHub repository. These five parts constitute the 
general AI framework, or AI Blueprint, that should be followed whenever 
you build an environment to solve any business problem with deep 
reinforcement learning.</p>
			<p class="normal">Here they are, from Step 1 to Step 5:</p>
			<ul>
				<li class="list"><strong class="bold">Step 1</strong>: Building the environment (<code class="Code-In-Text--PACKT-">environment.py</code>)</li>
				<li class="list"><strong class="bold">Step 2</strong>: Building the brain (<code class="Code-In-Text--PACKT-">brain_nodropout.py</code> or <code class="Code-In-Text--PACKT-">brain_dropout.py</code>)</li>
				<li class="list"><strong class="bold">Step 3</strong>: Implementing the deep reinforcement learning algorithm, which in our case is a deep Q-learning model (<code class="Code-In-Text--PACKT-">dqn.py</code>)</li>
				<li class="list"><strong class="bold">Step 4</strong>: Training the AI (<code class="Code-In-Text--PACKT-">training_noearlystopping.py</code> or <code class="Code-In-Text--PACKT-">training_earlystopping.py</code>)</li>
				<li class="list"><strong class="bold">Step 5</strong>: Testing the AI (<code class="Code-In-Text--PACKT-">testing.py</code>)</li>
			</ul>
			<p class="normal">In order, those are the main steps of the general AI framework.</p>
			<p class="normal">We'll follow this AI Blueprint to implement the AI 
for our specific case in the following five sections, each corresponding
 to one of these five main steps. Within each step, we'll distinguish 
the sub-steps that are still part of the general AI framework from the 
sub-steps that are specific to our project by writing the titles of the 
code sections in capital letters for all the sub-steps of the general AI
 framework, and in lowercase letters for all the sub-steps specific to 
our project.</p>
			<p class="normal">That means that anytime you see a new code section 
where the title is written in capital letters, then it is the next 
sub-step of the general AI framework, which you should also follow when 
building an AI for your own business problem.</p>
			<p class="normal">This next step, building the<a id="_idIndexMarker476"/>
 environment, is the largest Python implementation file for this 
project. Make sure you're rested and your batteries are recharged, 
and as soon as you are ready, let's tackle this together!</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_188" lang="en-GB"><a id="_idTextAnchor262"/>Step 1 – Building the environment</h3>
			<p class="normal">In this first step, we are going to build the environment inside a class. Why<a id="_idIndexMarker477"/>
 a class? Because we would like our environment to be an object which we
 can easily create with any values we choose for some parameters.</p>
			<p class="normal">For example, we can create one environment object 
for a server that has a certain number of connected users and a certain 
rate of data at a specific time, and another environment object for a 
different server that has a different number of connected users and a 
different rate of data. Thanks to the advanced structure of this class, 
we can easily plug-and-play the environment objects we create on 
different servers which have their own parameters, regulating their 
temperatures with several different AIs, so that we can minimize the 
energy consumption of a whole data center, just as Google DeepMind did 
for Google's data centers with its DQN (deep Q-learning) algorithm.</p>
			<p class="normal">This class follows the following sub-steps, which are part of the general AI Framework inside Step 1 – Building the environment:</p>
			<ul>
				<li class="list"><strong class="bold">Step 1-1</strong>: Introducing and initializing all the parameters and variables of the environment.</li>
				<li class="list"><strong class="bold">Step 1-2</strong>: Making a method that updates the environment right after the AI plays an action.</li>
				<li class="list"><strong class="bold">Step 1-3</strong>: Making a method that resets the environment.</li>
				<li class="list"><strong class="bold">Step 1-4</strong>: Making a method that gives us at any time the current state, the last reward obtained, and whether the game is over.</li>
			</ul>
			<p class="normal">You'll find the whole implementation of this <code class="Code-In-Text--PACKT-">Environment</code>
 class in this section. Remember the most important thing: all the code 
sections with their titles written in capital letters are steps of the 
general AI framework/Blueprint, and all the code sections having their 
titles written in lowercase letters are specific to our case study.</p>
			<p class="normal">The implementation of the environment has 144 lines of code. I won't explain each line of code for two reasons:</p>
			<ol>
				<li class="list" value="1">It would make this chapter really overwhelming.</li>
				<li class="list">The code is very simple, is commented on for clarity, and just creates everything we've defined so far in this chapter.</li>
			</ol>
			<p class="normal">I'm confident you'll have no<a id="_idIndexMarker478"/>
 problems understanding it. Besides, the code section titles and the 
chosen variable names are clear enough to understand the structure and 
the flow of the code at face value. I'll walk you through the code 
broadly. Here we go!</p>
			<p class="normal">First, we start building the <code class="Code-In-Text--PACKT-">Environment</code> class with its first method, the <code class="Code-In-Text--PACKT-">__init__</code> method, which introduces and initializes all the parameters and variables, as we described earlier:</p>
			<pre class="programlisting language-markup"><code class="hljs ruby"><span class="hljs-comment"># BUILDING THE ENVIRONMENT IN A CLASS</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Environment</span>(<span class="hljs-title">object</span>):</span>
    
    <span class="hljs-comment"># INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE ENVIRONMENT</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, optimal_temperature = (<span class="hljs-number">18.0</span>, <span class="hljs-number">24.0</span>)</span></span>, initial_month = <span class="hljs-number">0</span>, initial_number_users = <span class="hljs-number">10</span>, initial_rate_data = <span class="hljs-number">60</span>):
        <span class="hljs-keyword">self</span>.monthly_atmospheric_temperatures = [<span class="hljs-number">1.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">7.0</span>, <span class="hljs-number">10.0</span>, <span class="hljs-number">11.0</span>, <span class="hljs-number">20.0</span>, <span class="hljs-number">23.0</span>, <span class="hljs-number">24.0</span>, <span class="hljs-number">22.0</span>, <span class="hljs-number">10.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">1.0</span>]
        <span class="hljs-keyword">self</span>.initial_month = initial_month
        <span class="hljs-keyword">self</span>.atmospheric_temperature = <span class="hljs-keyword">self</span>.monthly_atmospheric_temperatures[initial_month]
        <span class="hljs-keyword">self</span>.optimal_temperature = optimal_temperature
        <span class="hljs-keyword">self</span>.min_temperature = -<span class="hljs-number">20</span>
        <span class="hljs-keyword">self</span>.max_temperature = <span class="hljs-number">80</span>
        <span class="hljs-keyword">self</span>.min_number_users = <span class="hljs-number">10</span>
        <span class="hljs-keyword">self</span>.max_number_users = <span class="hljs-number">100</span>
        <span class="hljs-keyword">self</span>.max_update_users = <span class="hljs-number">5</span>
        <span class="hljs-keyword">self</span>.min_rate_data = <span class="hljs-number">20</span>
        <span class="hljs-keyword">self</span>.max_rate_data = <span class="hljs-number">300</span>
        <span class="hljs-keyword">self</span>.max_update_data = <span class="hljs-number">10</span>
        <span class="hljs-keyword">self</span>.initial_number_users = initial_number_users
        <span class="hljs-keyword">self</span>.current_number_users = initial_number_users
        <span class="hljs-keyword">self</span>.initial_rate_data = initial_rate_data
        <span class="hljs-keyword">self</span>.current_rate_data = initial_rate_data
        <span class="hljs-keyword">self</span>.intrinsic_temperature = <span class="hljs-keyword">self</span>.atmospheric_temperature + <span class="hljs-number">1.25</span> * <span class="hljs-keyword">self</span>.current_number_users + <span class="hljs-number">1.25</span> * <span class="hljs-keyword">self</span>.current_rate_data
        <span class="hljs-keyword">self</span>.temperature_ai = <span class="hljs-keyword">self</span>.intrinsic_temperature
        <span class="hljs-keyword">self</span>.temperature_noai = (<span class="hljs-keyword">self</span>.optimal_temperature[<span class="hljs-number">0</span>] + <span class="hljs-keyword">self</span>.optimal_temperature[<span class="hljs-number">1</span>]) / <span class="hljs-number">2.0</span>
        <span class="hljs-keyword">self</span>.total_energy_ai = <span class="hljs-number">0</span>.<span class="hljs-number">0</span>
        <span class="hljs-keyword">self</span>.total_energy_noai = <span class="hljs-number">0</span>.<span class="hljs-number">0</span>
        <span class="hljs-keyword">self</span>.reward = <span class="hljs-number">0</span>.<span class="hljs-number">0</span>
        <span class="hljs-keyword">self</span>.game_over = <span class="hljs-number">0</span>
        <span class="hljs-keyword">self</span>.train = <span class="hljs-number">1</span>
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal">You'll notice the <code class="Code-In-Text--PACKT-">self.monthly_atmospheric_temperatures</code> variable; that's a list containing the average monthly<a id="_idIndexMarker479"/> atmospheric temperatures for each of the 12 months: 1°C in January, 5°C in February, 7°C in March, and so on. </p>
			<p class="normal">The <code class="Code-In-Text--PACKT-">self.atmospheric_temperature</code>
 variable is the current average atmospheric temperature of the month 
we're in during the simulation, and it's initialized as the atmospheric 
temperature of the initial month, which we'll set later as January.</p>
			<p class="normal">The <code class="Code-In-Text--PACKT-">self.game_over</code>
 variable tells the AI whether or not we should reset the temperature of
 the server, in case it goes outside the allowed range of [-20°C, 80°C].
 If it does, <code class="Code-In-Text--PACKT-">self.game_over</code> will be set equal to 1, otherwise it will remain at 0.</p>
			<p class="normal">Finally, the <code class="Code-In-Text--PACKT-">self.train</code> variable tells us whether we're in training mode or inference mode. If we're in training mode, <code class="Code-In-Text--PACKT-">self.train = 1</code>. If we're in inference mode, <code class="Code-In-Text--PACKT-">self.train = 0</code>. The rest is just putting into code everything we defined in words at the beginning of this chapter.</p>
			<p class="normal">Let's move on!</p>
			<p class="normal">Now, we make the second method, <code class="Code-In-Text--PACKT-">update_env</code>, which updates the environment after the AI performs an action. This method takes three arguments as inputs:</p>
			<ol>
				<li class="list" value="1"><code class="Code-In-Text--PACKT-">direction</code>: A variable describing the direction of the heat transfer the AI imposes on the server, like so: if <code class="Code-In-Text--PACKT-">direction == 1</code>, the AI is heating up the server. If <code class="Code-In-Text--PACKT-">direction == -1</code>, the AI is cooling down the server. We'll need to have the value of this direction before calling the <code class="Code-In-Text--PACKT-">update_env</code> method, since this method is called after the action is performed.</li>
				<li class="list"><code class="Code-In-Text--PACKT-">energy_ai</code>:
 The energy spent by the AI to heat up or cool down the server at this 
specific time when the action is played. Based on assumption 2, it will 
be equal to the temperature change caused by the AI in the server.</li>
				<li class="list"><code class="Code-In-Text--PACKT-">month</code>: Simply the month we're in at the specific time when the action is played.</li>
			</ol>
			<p class="normal">The first actions the program takes inside this 
method are to compute the reward. Indeed, right after the action is 
played, we can immediately deduce the reward, since it is the difference
 between the energy that the server's integrated system would spend if 
there was no AI, and the energy spent by the AI:</p>
			<pre class="programlisting language-markup"><code class="hljs ruby">    <span class="hljs-comment"># MAKING A METHOD THAT UPDATES THE ENVIRONMENT RIGHT AFTER THE AI PLAYS AN ACTION</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_env</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, direction, energy_ai, month)</span></span>:
        
        <span class="hljs-comment"># GETTING THE REWARD</span>
        
        <span class="hljs-comment"># Computing the energy spent by the server's cooling system when there is no AI</span>
        energy_noai = <span class="hljs-number">0</span>
        <span class="hljs-keyword">if</span> (<span class="hljs-keyword">self</span>.temperature_noai &lt; <span class="hljs-keyword">self</span>.optimal_temperature[<span class="hljs-number">0</span>]):
            energy_noai = <span class="hljs-keyword">self</span>.optimal_temperature[<span class="hljs-number">0</span>] - <span class="hljs-keyword">self</span>.temperature_noai
            <span class="hljs-keyword">self</span>.temperature_noai = <span class="hljs-keyword">self</span>.optimal_temperature[<span class="hljs-number">0</span>]
        elif (<span class="hljs-keyword">self</span>.temperature_noai &gt; <span class="hljs-keyword">self</span>.optimal_temperature[<span class="hljs-number">1</span>]):
            energy_noai = <span class="hljs-keyword">self</span>.temperature_noai - <span class="hljs-keyword">self</span>.optimal_temperature[<span class="hljs-number">1</span>]
            <span class="hljs-keyword">self</span>.temperature_noai = <span class="hljs-keyword">self</span>.optimal_temperature[<span class="hljs-number">1</span>]
        <span class="hljs-comment"># Computing the Reward</span>
        <span class="hljs-keyword">self</span>.reward = energy_noai - energy_ai
        <span class="hljs-comment"># Scaling the Reward</span>
        <span class="hljs-keyword">self</span>.reward = <span class="hljs-number">1</span>e-<span class="hljs-number">3</span> * <span class="hljs-keyword">self</span>.reward
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal">You have probably noticed that we choose to scale 
the reward at the end. In short, scaling is bringing the values (here 
the rewards) down into a short range. For example, normalization is a 
scaling technique where all the values are brought down<a id="_idIndexMarker480"/> into a range between 0 and 1. Another widely used scaling technique is standardization, which will be explained a bit later on.</p>
			<p class="normal">Scaling is a common practice that is usually 
recommended in research papers when performing deep reinforcement 
learning, as it stabilizes training and improves the performance of the 
AI.</p>
			<p class="normal">After getting the reward, we reach the next state. Remember that each state is composed of the following elements:</p>
			<ol>
				<li class="list" value="1">The temperature of the server at time <img src="../Images/B14110_11_083.png" alt=""/></li>
				<li class="list">The number of users in the server at time <img src="../Images/B14110_11_083.png" alt=""/></li>
				<li class="list">The rate of data transmission in the server at time <img src="../Images/B14110_11_062.png" alt=""/></li>
			</ol>
			<p class="normal">So, as we reach the next state, we update each of 
these elements one by one, following the sub-steps highlighted as 
comments in this next code section:</p>
			<pre class="programlisting language-markup"><code class="hljs zephir">        <span class="hljs-comment"># GETTING THE NEXT STATE</span>
        
        <span class="hljs-comment"># Updating the atmospheric temperature</span>
        <span class="hljs-keyword">self</span>.atmospheric_temperature = <span class="hljs-keyword">self</span>.monthly_atmospheric_temperatures[month]
        <span class="hljs-comment"># Updating the number of users</span>
        <span class="hljs-keyword">self</span>.current_number_users += np.random.randint(-<span class="hljs-keyword">self</span>.max_update_users, <span class="hljs-keyword">self</span>.max_update_users)
        <span class="hljs-keyword">if</span> (<span class="hljs-keyword">self</span>.current_number_users &gt; <span class="hljs-keyword">self</span>.max_number_users):
            <span class="hljs-keyword">self</span>.current_number_users = <span class="hljs-keyword">self</span>.max_number_users
        elif (<span class="hljs-keyword">self</span>.current_number_users &lt; <span class="hljs-keyword">self</span>.min_number_users):
            <span class="hljs-keyword">self</span>.current_number_users = <span class="hljs-keyword">self</span>.min_number_users
        <span class="hljs-comment"># Updating the rate of data</span>
        <span class="hljs-keyword">self</span>.current_rate_data += np.random.randint(-<span class="hljs-keyword">self</span>.max_update_data, <span class="hljs-keyword">self</span>.max_update_data)
        <span class="hljs-keyword">if</span> (<span class="hljs-keyword">self</span>.current_rate_data &gt; <span class="hljs-keyword">self</span>.max_rate_data):
            <span class="hljs-keyword">self</span>.current_rate_data = <span class="hljs-keyword">self</span>.max_rate_data
        elif (<span class="hljs-keyword">self</span>.current_rate_data &lt; <span class="hljs-keyword">self</span>.min_rate_data):
            <span class="hljs-keyword">self</span>.current_rate_data = <span class="hljs-keyword">self</span>.min_rate_data
        <span class="hljs-comment"># Computing the Delta of Intrinsic Temperature</span>
        past_intrinsic_temperature = <span class="hljs-keyword">self</span>.intrinsic_temperature
        <span class="hljs-keyword">self</span>.intrinsic_temperature = <span class="hljs-keyword">self</span>.atmospheric_temperature + <span class="hljs-number">1.25</span> * <span class="hljs-keyword">self</span>.current_number_users + <span class="hljs-number">1.25</span> * <span class="hljs-keyword">self</span>.current_rate_data
        delta_intrinsic_temperature = <span class="hljs-keyword">self</span>.intrinsic_temperature - past_intrinsic_temperature
        <span class="hljs-comment"># Computing the Delta of Temperature caused by the AI</span>
        <span class="hljs-keyword">if</span> (direction == <span class="hljs-number">-1</span>):
            delta_temperature_ai = -energy_ai
        elif (direction == <span class="hljs-number">1</span>):
            delta_temperature_ai = energy_ai
        <span class="hljs-comment"># Updating the new Server's Temperature when there is the AI</span>
        <span class="hljs-keyword">self</span>.temperature_ai += delta_intrinsic_temperature + delta_temperature_ai
        <span class="hljs-comment"># Updating the new Server's Temperature when there is no AI</span>
        <span class="hljs-keyword">self</span>.temperature_noai += delta_intrinsic_temperature
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal">Then, we update the <code class="Code-In-Text--PACKT-">self.game_over</code>
 variable if needed, that is, if the temperature of the server goes 
outside the allowed range of [-20°C, 80°C]. This can happen if the 
server temperature goes below the minimum temperature of -20°C, or if 
the server temperature goes higher than the maximum temperature of 80°C.
 Plus we do two extra things: we bring the server temperature back into 
the optimal temperature<a id="_idIndexMarker481"/> range (closest bound), and since doing this spends some energy, we update the total energy spent by the AI (<code class="Code-In-Text--PACKT-">self.total_energy_ai</code>). That's exactly what is coded in the next code section:</p>
			<pre class="programlisting language-markup"><code class="hljs monkey"><span class="hljs-meta">        # GETTING GAME OVER</span>
        
        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">self</span>.temperature_ai &lt; <span class="hljs-built_in">self</span>.min_temperature):
            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">self</span>.train == <span class="hljs-number">1</span>):
                <span class="hljs-built_in">self</span>.game_over = <span class="hljs-number">1</span>
            <span class="hljs-keyword">else</span>:
                <span class="hljs-built_in">self</span>.total_energy_ai += <span class="hljs-built_in">self</span>.optimal_temperature[<span class="hljs-number">0</span>] - <span class="hljs-built_in">self</span>.temperature_ai
                <span class="hljs-built_in">self</span>.temperature_ai = <span class="hljs-built_in">self</span>.optimal_temperature[<span class="hljs-number">0</span>]
        elif (<span class="hljs-built_in">self</span>.temperature_ai &gt; <span class="hljs-built_in">self</span>.max_temperature):
            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">self</span>.train == <span class="hljs-number">1</span>):
                <span class="hljs-built_in">self</span>.game_over = <span class="hljs-number">1</span>
            <span class="hljs-keyword">else</span>:
                <span class="hljs-built_in">self</span>.total_energy_ai += <span class="hljs-built_in">self</span>.temperature_ai - <span class="hljs-built_in">self</span>.optimal_temperature[<span class="hljs-number">1</span>]
                <span class="hljs-built_in">self</span>.temperature_ai = <span class="hljs-built_in">self</span>.optimal_temperature[<span class="hljs-number">1</span>]
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal">Now, I know it seems unrealistic for the server to 
snap right back to 24 degrees from 80, or to 18 from -20, but this is an
 action the magically efficient integrated cooling system we defined 
earlier is perfectly capable of. Think of it as the AI switching to the 
integrated system for a moment in the case of a temperature disaster. 
Once again, this is an area that will benefit enormously from your 
ongoing tinkering once we've got the AI up and running; after that, you 
can play around with these figures as you like in the interests of a 
more realistic server model.</p>
			<p class="normal">Then, we update the two scores coming from the two separate simulations, which are:</p>
			<ol>
				<li class="list" value="1"><code class="Code-In-Text--PACKT-">self.total_energy_ai</code>: The total energy spent by the AI</li>
				<li class="list"><code class="Code-In-Text--PACKT-">self.total_energy_noai</code>: The total energy spent by the server's integrated cooling system when there is no AI.</li>
			</ol>
			<pre class="programlisting language-markup"><code class="hljs pf">        <span class="hljs-comment"># UPDATING THE SCORES</span>
        
        <span class="hljs-comment"># Updating the Total Energy spent by the AI</span>
        <span class="hljs-literal">self</span>.total_energy_ai += energy_ai
        <span class="hljs-comment"># Updating the Total Energy spent by the server's cooling system when there is no AI</span>
        <span class="hljs-literal">self</span>.total_energy_noai += energy_noai
</code></pre>
			
			
			
			
			
			<p class="normal">Then to improve the performance, we scale the next 
state by scaling each of its three elements (server temperature, number 
of users, and data transmission rate). To do so, we perform a simple<a id="_idIndexMarker482"/>
 standardization scaling technique, which simply consists of subtracting
 the minimum value of the variable, and then dividing by the maximum 
delta of the variable:</p>
			<pre class="programlisting language-markup"><code class="hljs monkey"><span class="hljs-meta">        # SCALING THE NEXT STATE</span>
        
        scaled_temperature_ai = (<span class="hljs-built_in">self</span>.temperature_ai - <span class="hljs-built_in">self</span>.min_temperature) / (<span class="hljs-built_in">self</span>.max_temperature - <span class="hljs-built_in">self</span>.min_temperature)
        scaled_number_users = (<span class="hljs-built_in">self</span>.current_number_users - <span class="hljs-built_in">self</span>.min_number_users) / (<span class="hljs-built_in">self</span>.max_number_users - <span class="hljs-built_in">self</span>.min_number_users)
        scaled_rate_data = (<span class="hljs-built_in">self</span>.current_rate_data - <span class="hljs-built_in">self</span>.min_rate_data) / (<span class="hljs-built_in">self</span>.max_rate_data - <span class="hljs-built_in">self</span>.min_rate_data)
        next_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])
</code></pre>
			
			
			
			
			
			<p class="normal">Finally, we end this <code class="Code-In-Text--PACKT-">update_env</code> method by returning the next state, the reward received, and whether the game is over or not:</p>
			<pre class="programlisting language-markup"><code class="hljs monkey"><span class="hljs-meta">        # RETURNING THE NEXT STATE, THE REWARD, AND GAME OVER</span>
        
        <span class="hljs-keyword">return</span> next_state, <span class="hljs-built_in">self</span>.reward, <span class="hljs-built_in">self</span>.game_over
</code></pre>
			
			
			<p class="normal">Great! We're done with this long, but important, 
method that updates the environment at each time step (each minute). Now
 there are two final and very easy methods to go: one that resets the 
environment, and one that gives us three pieces of information at any 
time: the current state, the last reward received, and whether or not 
the game is over.</p>
			<p class="normal">Here's the <code class="Code-In-Text--PACKT-">reset</code>
 method, which resets the environment when a new training episode 
starts, by resetting all the variables of the environment to their 
originally initialized values:</p>
			<pre class="programlisting language-markup"><code class="hljs oxygene">    # MAKING A <span class="hljs-function"><span class="hljs-keyword">METHOD</span> <span class="hljs-title">THAT</span> <span class="hljs-title">RESETS</span> <span class="hljs-title">THE</span> <span class="hljs-title">ENVIRONMENT</span>
    
    <span class="hljs-title">def</span> <span class="hljs-title">reset</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, new_month)</span>:</span>
        <span class="hljs-keyword">self</span>.atmospheric_temperature = <span class="hljs-keyword">self</span>.monthly_atmospheric_temperatures[new_month]
        <span class="hljs-keyword">self</span>.initial_month = new_month
        <span class="hljs-keyword">self</span>.current_number_users = <span class="hljs-keyword">self</span>.initial_number_users
        <span class="hljs-keyword">self</span>.current_rate_data = <span class="hljs-keyword">self</span>.initial_rate_data
        <span class="hljs-keyword">self</span>.intrinsic_temperature = <span class="hljs-keyword">self</span>.atmospheric_temperature + <span class="hljs-number">1.25</span> * <span class="hljs-keyword">self</span>.current_number_users + <span class="hljs-number">1.25</span> * <span class="hljs-keyword">self</span>.current_rate_data
        <span class="hljs-keyword">self</span>.temperature_ai = <span class="hljs-keyword">self</span>.intrinsic_temperature
        <span class="hljs-keyword">self</span>.temperature_noai = (<span class="hljs-keyword">self</span>.optimal_temperature[<span class="hljs-number">0</span>] + <span class="hljs-keyword">self</span>.optimal_temperature[<span class="hljs-number">1</span>]) / <span class="hljs-number">2.0</span>
        <span class="hljs-keyword">self</span>.total_energy_ai = <span class="hljs-number">0.0</span>
        <span class="hljs-keyword">self</span>.total_energy_noai = <span class="hljs-number">0.0</span>
        <span class="hljs-keyword">self</span>.reward = <span class="hljs-number">0.0</span>
        <span class="hljs-keyword">self</span>.game_over = <span class="hljs-number">0</span>
        <span class="hljs-keyword">self</span>.train = <span class="hljs-number">1</span>
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal">Finally, here's the <code class="Code-In-Text--PACKT-">observe</code> method, which lets us know at any given time the current state, the last reward received, and whether the game is over:</p>
			<pre class="programlisting language-markup"><code class="hljs oxygene">    # MAKING A <span class="hljs-function"><span class="hljs-keyword">METHOD</span> <span class="hljs-title">THAT</span> <span class="hljs-title">GIVES</span> <span class="hljs-title">US</span> <span class="hljs-title">AT</span> <span class="hljs-title">ANY</span> <span class="hljs-title">TIME</span> <span class="hljs-title">THE</span> <span class="hljs-title">CURRENT</span> <span class="hljs-title">STATE</span>, <span class="hljs-title">THE</span> <span class="hljs-title">LAST</span> <span class="hljs-title">REWARD</span> <span class="hljs-title">AND</span> <span class="hljs-title">WHETHER</span> <span class="hljs-title">THE</span> <span class="hljs-title">GAME</span> <span class="hljs-title">IS</span> <span class="hljs-title">OVER</span>
    
    <span class="hljs-title">def</span> <span class="hljs-title">observe</span><span class="hljs-params">(<span class="hljs-keyword">self</span>)</span>:</span>
        scaled_temperature_ai = (<span class="hljs-keyword">self</span>.temperature_ai - <span class="hljs-keyword">self</span>.min_temperature) / (<span class="hljs-keyword">self</span>.max_temperature - <span class="hljs-keyword">self</span>.min_temperature)
        scaled_number_users = (<span class="hljs-keyword">self</span>.current_number_users - <span class="hljs-keyword">self</span>.min_number_users) / (<span class="hljs-keyword">self</span>.max_number_users - <span class="hljs-keyword">self</span>.min_number_users)
        scaled_rate_data = (<span class="hljs-keyword">self</span>.current_rate_data - <span class="hljs-keyword">self</span>.min_rate_data) / (<span class="hljs-keyword">self</span>.max_rate_data - <span class="hljs-keyword">self</span>.min_rate_data)
        current_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])
        return current_state, <span class="hljs-keyword">self</span>.reward, <span class="hljs-keyword">self</span>.game_over
</code></pre>
			
			
			
			
			
			
			
			<p class="normal">Awesome! We're done with the first step of the implementation, building the<a id="_idIndexMarker483"/> environment. Now let's move on to the next step and start building the brain.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_189" lang="en-GB"><a id="_idTextAnchor263"/>Step 2 – Building the brain</h3>
			<p class="normal">In this step, we're <a id="_idIndexMarker484"/>going to build the artificial brain of our AI, which is nothing other than a fully connected neur<a id="_idTextAnchor264"/>al network. Here it is again:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_03.png" alt=""/></figure>
			<p class="packt_figref">Figure 3: The artificial brain of our AI</p>
			<p class="normal">We'll build this artificial brain inside a class 
for the same reason as before, which is to allow us to create several 
artificial brains, for different servers inside a data center. Maybe 
some servers will need different artificial brains with different 
hyper-parameters than other servers. That's why, thanks to this 
class/object advanced Python structure, we can easily switch from one 
brain to another, to regulate the temperature of a new server that 
requires an AI with different<a id="_idIndexMarker485"/> neural network parameters. That's the beauty of <strong class="bold">Object-Oriented Programming</strong> (<strong class="bold">OOP</strong>).</p>
			<p class="normal">We're building this artificial brain with the amazing Keras library. From this library, we use the <code class="Code-In-Text--PACKT-">Dense()</code> class to <a id="_idIndexMarker486"/>create
 our two fully connected hidden layers, the first one from 64 hidden 
neurons, and the second one from 32 neurons. Remember, this is a classic
 neural network architecture often used by default, as common practice, 
and seen in many research papers. At the end, we use the <code class="Code-In-Text--PACKT-">Dense()</code> class again to return the Q-values, which are the outputs of the artificial neural network.</p>
			<p class="normal">Later on, when we code the training and testing 
files, we'll use the argmax method to select the action that has the 
maximum Q-value. Then, we assemble all the components of the brain, 
including the inputs and outputs, by creating it as an object of the <code class="Code-In-Text--PACKT-">Model()</code>
 class (which is very useful in that we can save and load a model with 
specific weights). Finally, we'll compile it with a mean squared error 
loss and an Adam optimizer. I'll explain all this in more detail later.</p>
			<p class="normal">Here are the new steps of the general AI framework:</p>
			<ul>
				<li class="list"><strong class="bold">Step 2-1</strong>: Build the input layer, composed of the input states.</li>
				<li class="list"><strong class="bold">Step 2-2</strong>: Build a 
defined number of hidden layers with a defined number of neurons inside 
each layer, fully connected to the input layer and between each other.</li>
				<li class="list"><strong class="bold">Step 2-3</strong>: Build the output layer, fully connected to the last hidden layer.</li>
				<li class="list"><strong class="bold">Step 2-4</strong>: Assemble the full architecture inside a model object.</li>
				<li class="list"><strong class="bold">Step 2-5</strong>: Compile the model with a mean squared error loss function and a chosen optimizer.</li>
			</ul>
			<p class="normal">The implementation of this is<a id="_idIndexMarker487"/> presented to you in a choice of two different files:</p>
			<ol>
				<li class="list" value="1"><code class="Code-In-Text--PACKT-">brain_nodropout.py</code>:
 An implementation file that builds the artificial brain without the 
dropout regularization technique (I'll explain what it is very soon).</li>
				<li class="list"><code class="Code-In-Text--PACKT-">brain_dropout.py</code>: An implementation file that builds the artificial brain with the dropout regularization technique.</li>
			</ol>
			<p class="normal">First let me give you the implementation without dropout, and then I'll provide one with dropout and explain it.</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_190" lang="en-GB"><a id="_idTextAnchor265"/>Without dropout</h4>
			<p class="normal">Here is the full implementation of the <a id="_idIndexMarker488"/>artificial brain, without any dropout regularization technique:</p>
			<pre class="programlisting language-markup"><code class="hljs nix"><span class="hljs-comment"># AI for Business - Minimize cost with Deep Q-Learning   #1</span>
<span class="hljs-comment"># Building the Brain without Dropout   #2</span>
<span class="hljs-comment">#3</span>
<span class="hljs-comment"># Importing the libraries   #4</span>
from keras.layers <span class="hljs-built_in">import</span> Input, Dense   <span class="hljs-comment">#5</span>
from keras.models <span class="hljs-built_in">import</span> Model   <span class="hljs-comment">#6</span>
from keras.optimizers <span class="hljs-built_in">import</span> Adam   <span class="hljs-comment">#7</span>
   <span class="hljs-comment">#8</span>
<span class="hljs-comment"># BUILDING THE BRAIN   #9</span>
   <span class="hljs-comment">#10</span>
class Brain(object):   <span class="hljs-comment">#11</span>
    <span class="hljs-comment">#12</span>
    <span class="hljs-comment"># BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD   #13</span>
    <span class="hljs-comment">#14</span>
    def __init__(self, <span class="hljs-attr">learning_rate</span> = <span class="hljs-number">0.001</span>, <span class="hljs-attr">number_actions</span> = <span class="hljs-number">5</span>):   <span class="hljs-comment">#15</span>
        self.<span class="hljs-attr">learning_rate</span> = learning_rate   <span class="hljs-comment">#16</span>
        <span class="hljs-comment">#17</span>
        <span class="hljs-comment"># BUILDING THE INPUT LAYER COMPOSED OF THE INPUT STATE   #18</span>
        <span class="hljs-attr">states</span> = Input(<span class="hljs-attr">shape</span> = (<span class="hljs-number">3</span>,))   <span class="hljs-comment">#19</span>
        <span class="hljs-comment">#20</span>
        <span class="hljs-comment"># BUILDING THE FULLY CONNECTED HIDDEN LAYERS   #21</span>
        <span class="hljs-attr">x</span> = Dense(<span class="hljs-attr">units</span> = <span class="hljs-number">64</span>, <span class="hljs-attr">activation</span> = 'sigmoid')(states)   <span class="hljs-comment">#22</span>
        <span class="hljs-attr">y</span> = Dense(<span class="hljs-attr">units</span> = <span class="hljs-number">32</span>, <span class="hljs-attr">activation</span> = 'sigmoid')(x)   <span class="hljs-comment">#23</span>
        <span class="hljs-comment">#24</span>
        <span class="hljs-comment"># BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER   #25</span>
        <span class="hljs-attr">q_values</span> = Dense(<span class="hljs-attr">units</span> = number_actions, <span class="hljs-attr">activation</span> = 'softmax')(y)   <span class="hljs-comment">#26</span>
        <span class="hljs-comment">#27</span>
        <span class="hljs-comment"># ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT   #28</span>
        self.<span class="hljs-attr">model</span> = Model(<span class="hljs-attr">inputs</span> = states, <span class="hljs-attr">outputs</span> = q_values)   <span class="hljs-comment">#29</span>
        <span class="hljs-comment">#30</span>
        <span class="hljs-comment"># COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER   #31</span>
        self.model.compile(<span class="hljs-attr">loss</span> = 'mse', <span class="hljs-attr">optimizer</span> = Adam(<span class="hljs-attr">lr</span> = learning_rate))   <span class="hljs-comment">#32</span>
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal">Now, let's go through the code in detail.</p>
			<p class="normal"><strong class="bold">Line 5</strong>: We import the <code class="Code-In-Text--PACKT-">Input</code> and <code class="Code-In-Text--PACKT-">Dense</code> classes from the <code class="Code-In-Text--PACKT-">layers</code> module in the <code class="Code-In-Text--PACKT-">keras</code> library. The <code class="Code-In-Text--PACKT-">Input</code> class <a id="_idIndexMarker489"/>allows us to build the input layer, and the <code class="Code-In-Text--PACKT-">Dense</code> class allows us to build the fully-connected layers.</p>
			<p class="normal"><strong class="bold">Line 6</strong>: We import the <code class="Code-In-Text--PACKT-">Model</code> class from the <code class="Code-In-Text--PACKT-">models</code> module in the <code class="Code-In-Text--PACKT-">keras</code> library. It allows us to build the whole neural network model by assembling its different layers.</p>
			<p class="normal"><strong class="bold">Line 7</strong>: We import the <code class="Code-In-Text--PACKT-">Adam</code> class from the <code class="Code-In-Text--PACKT-">optimizers</code> module in the <code class="Code-In-Text--PACKT-">keras</code>
 library. It allows us to use the Adam optimizer, used to update the 
weights of the neural network through stochastic gradient descent, when 
backpropagating the loss error in each iteration of the training.</p>
			<p class="normal"><strong class="bold">Line 11</strong>: We introduce the <code class="Code-In-Text--PACKT-">Brain</code>
 class, which will contain not only the whole architecture of the 
artificial neural network, but also the connection of the model to the 
loss (Mean-Squared Error) and the Adam optimizer.</p>
			<p class="normal"><strong class="bold">Line 15</strong>: We introduce the <code class="Code-In-Text--PACKT-">__init__</code> method, which will be the only method of this class. We define<a id="_idIndexMarker490"/>
 the whole architecture of the neural network inside it, just 
by creating successive variables which together assemble the neural 
network. This method takes as inputs two arguments:</p>
			<ol>
				<li class="list" value="1">The learning rate (<code class="Code-In-Text--PACKT-">learning_rate</code>),
 which is a measure of how fast you want the neural network to learn 
(the higher the learning rate, the faster the neural network learns; but
 at the cost of quality). The default value is <code class="Code-In-Text--PACKT-">0.001</code>.</li>
				<li class="list">The number of actions (<code class="Code-In-Text--PACKT-">number_actions</code>),
 which is of course the number of actions that our AI can perform. Now 
you might be thinking: why do we need to put that as an argument? Well 
that's just in case you want to build another AI that can perform more 
or fewer actions. In which case you would simply need to change the 
value of the argument and that's it. Pretty practical, isn't it?</li>
			</ol>
			<p class="normal"><strong class="bold">Line 16</strong>: We create an object variable for the learning rate, <code class="Code-In-Text--PACKT-">self.learning_rate</code>, initialized as the value of the <code class="Code-In-Text--PACKT-">learning_rate</code> argument provided in the <code class="Code-In-Text--PACKT-">__init__</code> method (therefore the argument of the <code class="Code-In-Text--PACKT-">Brain</code> class when we create the object in the future).</p>
			<p class="normal"><strong class="bold">Line 19</strong>: We create the input states layer, called <code class="Code-In-Text--PACKT-">states</code>, as an object of the <code class="Code-In-Text--PACKT-">Input</code> class. Into this <code class="Code-In-Text--PACKT-">Input</code> class we enter one argument, <code class="Code-In-Text--PACKT-">shape = (3,)</code>,
 which simply tells that the input layer is a 1D vector composed of 
three elements (the server temperature, the number of users, and the 
data transmission rate).</p>
			<p class="normal"><strong class="bold">Line 22</strong>: We create the first fully-connected hidden layer, called <code class="Code-In-Text--PACKT-">x</code>, as an object of the <code class="Code-In-Text--PACKT-">Dense</code> class, which takes as input two arguments:</p>
			<ol>
				<li class="list" value="1"><code class="Code-In-Text--PACKT-">units</code>: The number of hidden neurons we want to have in this first hidden layer. Here, we choose to have 64 hidden neurons.</li>
				<li class="list"><code class="Code-In-Text--PACKT-">activation</code>:
 The activation function used to pass on the signal when 
forward-propagating the inputs into this first hidden layer. Here we 
choose, by default, a sigmoid activation function, which is as follows:</li>
			</ol>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_04.png" alt=""/></figure>
			<p class="packt_figref">Figure 4: The sigmoid activation function</p>
			<p class="normal">The ReLU activation function would also have worked
 well here; I encourage you to experiment! Note also how the connection 
from the input layer to this first hidden layer is made by calling the <code class="Code-In-Text--PACKT-">states</code> variable right after the <code class="Code-In-Text--PACKT-">Dense</code> class.</p>
			<p class="normal"><strong class="bold">Line 23</strong>: We create the second<a id="_idIndexMarker491"/> fully-connected hidden layer, called <code class="Code-In-Text--PACKT-">y</code>, as an object of the <code class="Code-In-Text--PACKT-">Dense</code> class, which takes as input the same two arguments:</p>
			<ol>
				<li class="list" value="1"><code class="Code-In-Text--PACKT-">units</code>: The number of hidden neurons we want to have in this second hidden layer. This time we choose to have 32 hidden neurons.</li>
				<li class="list"><code class="Code-In-Text--PACKT-">activation</code>:
 The activation function used to pass on the signal when 
forward-propagating the inputs into this first hidden layer. Here, 
again, we choose a sigmoid activation function.</li>
			</ol>
			<p class="normal">Note once again how the connection from the first hidden layer to this second hidden layer is made by calling the <code class="Code-In-Text--PACKT-">x</code> variable right after the <code class="Code-In-Text--PACKT-">Dense</code> class.</p>
			<p class="normal"><strong class="bold">Line 26</strong>: We create the output layer, called <code class="Code-In-Text--PACKT-">q_values</code>, fully connected to the second hidden layer, as an object of the <code class="Code-In-Text--PACKT-">Dense</code> class. This time, we input <code class="Code-In-Text--PACKT-">number_actions</code> units since the output layer contains the actions to play, and a <code class="Code-In-Text--PACKT-">softmax</code> activation function, as seen in <em class="italics">Chapter 5</em>, <em class="italics">Your First AI Model – Beware the Bandits!</em>, on the deep Q-learning theory.</p>
			<p class="normal"><strong class="bold">Line 29</strong>: Using the <code class="Code-In-Text--PACKT-">Model</code> class, we assemble the successive layers of the neural network, by just inputting the <code class="Code-In-Text--PACKT-">states</code> as the inputs, and the <code class="Code-In-Text--PACKT-">q_values</code> as the outputs.</p>
			<p class="normal"><strong class="bold">Line 32</strong>: Using the <code class="Code-In-Text--PACKT-">compile</code> method taken from the <code class="Code-In-Text--PACKT-">Model</code> class, we connect our model to the Mean-Squared Error loss and the<a id="_idIndexMarker492"/> Adam optimizer. The latter takes the <code class="Code-In-Text--PACKT-">learning_rate</code> argument as input.</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_191" lang="en-GB"><a id="_idTextAnchor266"/>With dropout</h4>
			<p class="normal">It'll be valuable for you to add one more <a id="_idIndexMarker493"/>powerful technique<a id="_idIndexMarker494"/> to your toolkit: <strong class="bold">dropout</strong>.</p>
			<p class="normal">Dropout is a regularization technique that prevents
 overfitting, which is the situation where the AI model performs well on
 the training set, but poorly on the test set. Dropout simply consists 
of deactivating a randomly selected portion of neurons during each step 
of forward- and back-propagation. That means not all the neurons learn 
the same way, which prevents the neural network from overfitting the 
training data.</p>
			<p class="normal">Adding dropout is very easy with <code class="Code-In-Text--PACKT-">keras</code>. You simply need to call the <code class="Code-In-Text--PACKT-">Dropout</code> class right after the <code class="Code-In-Text--PACKT-">Dense</code> class, and input the proportion of neurons you want to deactivate, like so:</p>
			<pre class="programlisting language-markup"><code class="hljs nix"><span class="hljs-comment"># AI for Business - Minimize cost with Deep Q-Learning</span>
<span class="hljs-comment"># Building the Brain with Dropout</span>
<span class="hljs-comment"># Importing the libraries</span>
from keras.layers <span class="hljs-built_in">import</span> Input, Dense, Dropout
from keras.models <span class="hljs-built_in">import</span> Model
from keras.optimizers <span class="hljs-built_in">import</span> Adam
<span class="hljs-comment"># BUILDING THE BRAIN</span>
class Brain(object):
    
    <span class="hljs-comment"># BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD</span>
    
    def __init__(self, <span class="hljs-attr">learning_rate</span> = <span class="hljs-number">0.001</span>, <span class="hljs-attr">number_actions</span> = <span class="hljs-number">5</span>):
        self.<span class="hljs-attr">learning_rate</span> = learning_rate
        
        <span class="hljs-comment"># BUILDING THE INPUT LAYER COMPOSED OF THE INPUT STATE</span>
        <span class="hljs-attr">states</span> = Input(<span class="hljs-attr">shape</span> = (<span class="hljs-number">3</span>,))
        
        <span class="hljs-comment"># BUILDING THE FIRST FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED</span>
        <span class="hljs-attr">x</span> = Dense(<span class="hljs-attr">units</span> = <span class="hljs-number">64</span>, <span class="hljs-attr">activation</span> = 'sigmoid')(states)
        <span class="hljs-attr">x</span> = Dropout(<span class="hljs-attr">rate</span> = <span class="hljs-number">0.1</span>)(x)
        
        <span class="hljs-comment"># BUILDING THE SECOND FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED</span>
        <span class="hljs-attr">y</span> = Dense(<span class="hljs-attr">units</span> = <span class="hljs-number">32</span>, <span class="hljs-attr">activation</span> = 'sigmoid')(x)
        <span class="hljs-attr">y</span> = Dropout(<span class="hljs-attr">rate</span> = <span class="hljs-number">0.1</span>)(y)
        
        <span class="hljs-comment"># BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER</span>
        <span class="hljs-attr">q_values</span> = Dense(<span class="hljs-attr">units</span> = number_actions, <span class="hljs-attr">activation</span> = 'softmax')(y)
        
        <span class="hljs-comment"># ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT</span>
        self.<span class="hljs-attr">model</span> = Model(<span class="hljs-attr">inputs</span> = states, <span class="hljs-attr">outputs</span> = q_values)
        
        <span class="hljs-comment"># COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER</span>
        self.model.compile(<span class="hljs-attr">loss</span> = 'mse', <span class="hljs-attr">optimizer</span> = Adam(<span class="hljs-attr">lr</span> = learning_rate))
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal">Here, we apply dropout to the first and <a id="_idIndexMarker495"/>second
 fully-connected layers, by deactivating 10% of their neurons each. Now,
 let's move on to the next step of our general AI framework: Step 3 – 
Implementing the deep reinforcement learning algorithm.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_192" lang="en-GB"><a id="_idTextAnchor267"/>Step 3 – Implementing the deep reinforcement learning algorithm</h3>
			<p class="normal">In this new implementation (given in the <code class="Code-In-Text--PACKT-">dqn.py</code> file), we simply have to follow the deep Q-learning algorithm provided before. Hence, this<a id="_idIndexMarker496"/> implementation follows the following sub-steps, which are part of the general AI framework:</p>
			<ul>
				<li class="list"><strong class="bold">Step 3-1</strong>: Introduce and initialize all the parameters and variables of the deep Q-learning model.</li>
				<li class="list"><strong class="bold">Step 3-2</strong>: Make a method that builds the memory in experience replay.</li>
				<li class="list"><strong class="bold">Step 3-3</strong>: Make a method that builds and returns two batches of 10 inputs and 10 targets.</li>
			</ul>
			<p class="normal">First, have a look at the whole code, and<a id="_idIndexMarker497"/> then I'll explain it line by line:</p>
			<pre class="programlisting language-markup"><code class="hljs oxygene"># AI <span class="hljs-keyword">for</span> Business - Minimize cost <span class="hljs-keyword">with</span> Deep Q-Learning   <span class="hljs-string">#1</span>
# Implementing Deep Q-Learning <span class="hljs-keyword">with</span> Experience Replay   <span class="hljs-string">#2</span>
<span class="hljs-string">#3</span>
# Importing the libraries   <span class="hljs-string">#4</span>
import numpy <span class="hljs-keyword">as</span> np   <span class="hljs-string">#5</span>
<span class="hljs-string">#6</span>
# IMPLEMENTING DEEP Q-LEARNING <span class="hljs-keyword">WITH</span> EXPERIENCE REPLAY   <span class="hljs-string">#7</span>
<span class="hljs-string">#8</span>
<span class="hljs-keyword">class</span> DQN(object):   <span class="hljs-string">#9</span>
    <span class="hljs-string">#10</span>
    # INTRODUCING <span class="hljs-keyword">AND</span> INITIALIZING ALL THE PARAMETERS <span class="hljs-keyword">AND</span> VARIABLES <span class="hljs-keyword">OF</span> THE DQN   <span class="hljs-string">#11</span>
    def __init__(<span class="hljs-keyword">self</span>, max_memory = <span class="hljs-number">100</span>, discount = <span class="hljs-number">0.9</span>):   <span class="hljs-string">#12</span>
        <span class="hljs-keyword">self</span>.memory = list()   <span class="hljs-string">#13</span>
        <span class="hljs-keyword">self</span>.max_memory = max_memory   <span class="hljs-string">#14</span>
        <span class="hljs-keyword">self</span>.discount = discount   <span class="hljs-string">#15</span>
<span class="hljs-string">#16</span>
    # MAKING A <span class="hljs-function"><span class="hljs-keyword">METHOD</span> <span class="hljs-title">THAT</span> <span class="hljs-title">BUILDS</span> <span class="hljs-title">THE</span> <span class="hljs-title">MEMORY</span> <span class="hljs-title">IN</span> <span class="hljs-title">EXPERIENCE</span> <span class="hljs-title">REPLAY</span>   #17
    <span class="hljs-title">def</span> <span class="hljs-title">remember</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, transition, game_over)</span>:</span>   <span class="hljs-string">#18</span>
        <span class="hljs-keyword">self</span>.memory.append([transition, game_over])   <span class="hljs-string">#19</span>
        <span class="hljs-keyword">if</span> len(<span class="hljs-keyword">self</span>.memory) &gt; <span class="hljs-keyword">self</span>.max_memory:   <span class="hljs-string">#20</span>
            del <span class="hljs-keyword">self</span>.memory[<span class="hljs-number">0</span>]   <span class="hljs-string">#21</span>
<span class="hljs-string">#22</span>
    # MAKING A <span class="hljs-function"><span class="hljs-keyword">METHOD</span> <span class="hljs-title">THAT</span> <span class="hljs-title">BUILDS</span> <span class="hljs-title">TWO</span> <span class="hljs-title">BATCHES</span> <span class="hljs-title">OF</span> <span class="hljs-title">INPUTS</span> <span class="hljs-title">AND</span> <span class="hljs-title">TARGETS</span> <span class="hljs-title">BY</span> <span class="hljs-title">EXTRACTING</span> <span class="hljs-title">TRANSITIONS</span> <span class="hljs-title">FROM</span> <span class="hljs-title">THE</span> <span class="hljs-title">MEMORY</span>   #23
    <span class="hljs-title">def</span> <span class="hljs-title">get_batch</span><span class="hljs-params">(<span class="hljs-keyword">self</span>, model, batch_size = 10)</span>:</span>   <span class="hljs-string">#24</span>
        len_memory = len(<span class="hljs-keyword">self</span>.memory)   <span class="hljs-string">#25</span>
        num_inputs = <span class="hljs-keyword">self</span>.memory[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].shape[<span class="hljs-number">1</span>]   <span class="hljs-string">#26</span>
        num_outputs = model.output_shape[-<span class="hljs-number">1</span>]   <span class="hljs-string">#27</span>
        inputs = np.zeros((min(len_memory, batch_size), num_inputs))   <span class="hljs-string">#28</span>
        targets = np.zeros((min(len_memory, batch_size), num_outputs))   <span class="hljs-string">#29</span>
        <span class="hljs-keyword">for</span> i, idx <span class="hljs-keyword">in</span> enumerate(np.random.randint(<span class="hljs-number">0</span>, len_memory, size = min(len_memory, batch_size))):   <span class="hljs-string">#30</span>
            current_state, action, reward, next_state = <span class="hljs-keyword">self</span>.memory[idx][<span class="hljs-number">0</span>]   <span class="hljs-string">#31</span>
            game_over = <span class="hljs-keyword">self</span>.memory[idx][<span class="hljs-number">1</span>]   <span class="hljs-string">#32</span>
            inputs[i] = current_state   <span class="hljs-string">#33</span>
            targets[i] = model.predict(current_state)[<span class="hljs-number">0</span>]   <span class="hljs-string">#34</span>
            Q_sa = np.max(model.predict(next_state)[<span class="hljs-number">0</span>])   <span class="hljs-string">#35</span>
            <span class="hljs-keyword">if</span> game_over:   <span class="hljs-string">#36</span>
                targets[i, action] = reward   <span class="hljs-string">#37</span>
            <span class="hljs-keyword">else</span>:   <span class="hljs-string">#38</span>
                targets[i, action] = reward + <span class="hljs-keyword">self</span>.discount * Q_sa   <span class="hljs-string">#39</span>
        return inputs, targets   <span class="hljs-string">#40</span>
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal"><strong class="bold">Line 5</strong>: We import the <code class="Code-In-Text--PACKT-">numpy</code> library, because we'll be working with <code class="Code-In-Text--PACKT-">numpy</code> arrays.</p>
			<p class="normal"><strong class="bold">Line 9</strong>: We introduce the <code class="Code-In-Text--PACKT-">DQN</code> class (<strong class="bold">DQN</strong> stands for <strong class="bold">Deep Q-Network</strong>), which contains the main parts of the deep Q-Learning algorithm, including <a id="_idIndexMarker498"/>experience replay.</p>
			<p class="normal"><strong class="bold">Line 12</strong>: We introduce the <code class="Code-In-Text--PACKT-">__init__</code> method, which creates the three following object variables of the <code class="Code-In-Text--PACKT-">DQN</code>
 model: the experience replay memory, the capacity (maximum size of the 
memory), and the discount factor in the formula of the target. It takes 
as arguments <code class="Code-In-Text--PACKT-">max_memory</code> (the capacity) and <code class="Code-In-Text--PACKT-">discount</code> (the discount factor), in case we want to build other experience replay <a id="_idIndexMarker499"/>memories
 with different capacities, or if we want to change the value of the 
discount factor in the computation of the target. The default values of 
these arguments are respectively <code class="Code-In-Text--PACKT-">100</code> and <code class="Code-In-Text--PACKT-">0.9</code>,
 which were chosen arbitrarily and turned out to work quite well; these 
are good arguments to experiment with, to see what difference it makes 
when you set them differently.</p>
			<p class="normal"><strong class="bold">Line 13</strong>: We create the experience replay memory object variable, <code class="Code-In-Text--PACKT-">self.memory</code>, and we initialize it as an empty list.</p>
			<p class="normal"><strong class="bold">Line 14</strong>: We create the object variable for the memory capacity, <code class="Code-In-Text--PACKT-">self.max_memory</code>, and we initialize it as the value of the <code class="Code-In-Text--PACKT-">max_memory</code> argument.</p>
			<p class="normal"><strong class="bold">Line 15</strong>: We create the object variable for the discount factor, <code class="Code-In-Text--PACKT-">self.discount</code>, and we initialize it as the value of the <code class="Code-In-Text--PACKT-">discount</code> argument.</p>
			<p class="normal"><strong class="bold">Line 18</strong>: We introduce the <code class="Code-In-Text--PACKT-">remember</code> method, which takes as input a transition to be added to the memory, and <code class="Code-In-Text--PACKT-">game_over</code>, which states whether or not this transition leads the server's temperature to go outside of the allowed range of temperatures.</p>
			<p class="normal"><strong class="bold">Line 19</strong>: Using the <code class="Code-In-Text--PACKT-">append</code> function called from the <code class="Code-In-Text--PACKT-">memory</code> list, we add the transition with the <code class="Code-In-Text--PACKT-">game_over</code> boolean into the memory (in the last position).</p>
			<p class="normal"><strong class="bold">Line 20</strong>: If, after adding this transition, the size of the memory exceeds the memory capacity (<code class="Code-In-Text--PACKT-">self.max_memory</code>).</p>
			<p class="normal"><strong class="bold">Line 21</strong>: We delete the<a id="_idIndexMarker500"/> first element of the memory.</p>
			<p class="normal"><strong class="bold">Line 24</strong>: We introduce the <code class="Code-In-Text--PACKT-">get_batch</code> method, which takes as inputs the model we built in the previous Python file (<code class="Code-In-Text--PACKT-">model</code>) and a batch size (<code class="Code-In-Text--PACKT-">batch_size</code>), and builds two batches of inputs and targets by extracting <code class="Code-In-Text--PACKT-">10</code> transitions from the memory (if the batch size is 10).</p>
			<p class="normal"><strong class="bold">Line 25</strong>: We get the current number of elements in the memory and put it into a new variable, <code class="Code-In-Text--PACKT-">len_memory</code>.</p>
			<p class="normal"><strong class="bold">Line 26</strong>: We get the 
number of elements in the input state vector (which is 3), but instead 
of directly entering 3, we access this number from the <code class="Code-In-Text--PACKT-">shape</code> attribute of the input state vector element of the memory, which we get by taking the <code class="Code-In-Text--PACKT-">[0][0][0]</code> indexes. Each element of the memory is structured as follows:</p>
			<p class="normal">[[<code class="Code-In-Text--PACKT-">current_state</code>, <code class="Code-In-Text--PACKT-">action</code>, <code class="Code-In-Text--PACKT-">reward</code>, <code class="Code-In-Text--PACKT-">next_state</code>], <code class="Code-In-Text--PACKT-">game_over</code>]</p>
			<p class="normal">Thus in <code class="Code-In-Text--PACKT-">[0][0][0]</code>, the first <code class="Code-In-Text--PACKT-">[0]</code> corresponds to the first element of the memory (meaning the first transition), the second <code class="Code-In-Text--PACKT-">[0]</code> corresponds to the tuple [<code class="Code-In-Text--PACKT-">current_state</code>, <code class="Code-In-Text--PACKT-">action</code>, <code class="Code-In-Text--PACKT-">reward</code>, <code class="Code-In-Text--PACKT-">next_state</code>], and so the third <code class="Code-In-Text--PACKT-">[0]</code> corresponds to the <code class="Code-In-Text--PACKT-">current_state</code> element of that tuple. Hence, <code class="Code-In-Text--PACKT-">self.memory[0][0][0]</code> corresponds to the first current state, and by adding <code class="Code-In-Text--PACKT-">.shape[1]</code>
 we get the number of elements in that input state vector. You might be 
wondering why we didn't enter 3 directly; that's because we want to 
generalize this code to any input state vector dimension you might want 
to have in your environment. For example, you might want to consider an 
input state with more information about your server, such as the 
humidity. Thanks to this line of code, you won't have to change anything
 regarding your new number of state elements.</p>
			<p class="normal"><strong class="bold">Line 27</strong>: We get the 
number of elements of the model output, meaning the number of actions. 
Just like on the previous line, instead of entering directly 5, we 
generalize by accessing this from the <code class="Code-In-Text--PACKT-">shape</code> attribute called from our <code class="Code-In-Text--PACKT-">model</code> object of the <code class="Code-In-Text--PACKT-">Model</code> class. <code class="Code-In-Text--PACKT-">-1</code> means that we get the last index of that <code class="Code-In-Text--PACKT-">shape</code> attribute, where the number of actions is contained.</p>
			<p class="normal"><strong class="bold">Line 28</strong>: We introduce and initialize the batch of inputs as a <code class="Code-In-Text--PACKT-">numpy</code> array, of <code class="Code-In-Text--PACKT-">batch_size</code> = 10 rows and 3 columns<a id="_idIndexMarker501"/>
 corresponding to input state elements, with only zeros. If the memory 
doesn't have 10 transitions yet, the number of rows will just be the 
length of the memory. </p>
			<p class="normal">If the memory already has at least 10 transitions, what we get with this line of code is the following:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_05.png" alt=""/></figure>
			<p class="packt_figref">Figure 5: Batch of inputs (1/2)</p>
			<p class="normal"><strong class="bold">Line 29</strong>: We introduce and initialize the batch of targets as a <code class="Code-In-Text--PACKT-">numpy</code> array of <code class="Code-In-Text--PACKT-">batch_size</code>
 = 10 rows and 5 columns corresponding to the five possible actions, 
with only zeros. Just like before, if the memory doesn't have 10 
transitions yet, the number of rows will just be the length of the 
memory. If the memory already has at least 10 transitions, what we get 
with this line of code is the following:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_06.png" alt=""/></figure>
			<p class="packt_figref">Figure 6: Batch of targets (1/3)</p>
			<p class="normal"><strong class="bold">Line 30</strong>: We do a double iteration inside the same <code class="Code-In-Text--PACKT-">for</code> loop. The first iterative variable <code class="Code-In-Text--PACKT-">i</code> goes from 0 to the batch size (or up to <code class="Code-In-Text--PACKT-">len_memory</code> if <code class="Code-In-Text--PACKT-">len_memory</code> &lt; <code class="Code-In-Text--PACKT-">batch_size</code>):</p>
			<p class="normal"><code class="Code-In-Text--PACKT-">i</code> = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9</p>
			<p class="normal">That way, <code class="Code-In-Text--PACKT-">i</code> will iterate each element of the batch. The second iterative variable <code class="Code-In-Text--PACKT-">idx</code> takes 10 random indexes of the memory, in order to extract 10 random transitions from the memory. Inside the <code class="Code-In-Text--PACKT-">for</code> loop, we populate the two batches of inputs and targets with their right values by iterating through each of their elements.</p>
			<p class="normal"><strong class="bold">Line 31</strong>: We get the<a id="_idIndexMarker502"/> transition of the sampled index <code class="Code-In-Text--PACKT-">idx</code> from the memory, composed of the current state, the action, the reward, and the next state. The reason we add <code class="Code-In-Text--PACKT-">[0]</code> is because an element of the memory is structured as follows:</p>
			<p class="normal">[[<code class="Code-In-Text--PACKT-">current_state</code>, <code class="Code-In-Text--PACKT-">action</code>, <code class="Code-In-Text--PACKT-">reward</code>, <code class="Code-In-Text--PACKT-">next_state</code>], <code class="Code-In-Text--PACKT-">game_over</code>]</p>
			<p class="normal">We'll get the <code class="Code-In-Text--PACKT-">game_over</code> value separately, in the next line of code.</p>
			<p class="normal"><strong class="bold">Line 32</strong>: We get the <code class="Code-In-Text--PACKT-">game_over</code> value corresponding to that same index <code class="Code-In-Text--PACKT-">idx</code> of the memory. As you can see, this time we add <code class="Code-In-Text--PACKT-">[1]</code> on the end to get the second element of a memory element:</p>
			<p class="normal">[[<code class="Code-In-Text--PACKT-">current_state</code>, <code class="Code-In-Text--PACKT-">action</code>, <code class="Code-In-Text--PACKT-">reward</code>, <code class="Code-In-Text--PACKT-">next_state</code>], <code class="Code-In-Text--PACKT-">game_over</code>]</p>
			<p class="normal"><strong class="bold">Line 33</strong>: We populate the batch of inputs with all the current states, in order to get this at the end of the <code class="Code-In-Text--PACKT-">for</code> loop:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_07.png" alt=""/></figure>
			<p class="packt_figref">Figure 7: Batch of inputs (2/2)</p>
			<p class="normal"><strong class="bold">Line 34</strong>: Now we start populating the batch of targets with the right values. First, we populate it with all the Q-values <img src="../Images/B14110_11_197.png" alt=""/>
 that the model predicts for the different state-action pairs: (current 
state, action 0), (current state, action 1), (current state, action 2), 
(current state, action 3), and (current state, action 4). Thus we first 
get this (at the end of the <code class="Code-In-Text--PACKT-">for</code> loop):</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_08.png" alt=""/></figure>
			<p class="packt_figref">Figure 8: Batch of targets (2/3)</p>
			<p class="normal">Remember that for the action that is played, the formula of the target must be this one:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_198.png" alt=""/></figure>
			<p class="normal">What we do in the<a id="_idIndexMarker503"/> 
following lines of code is to put this formula into the column of each 
action that was played within the 10 selected transitions. In other 
words, we get this:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_09.png" alt=""/></figure>
			<p class="packt_figref">Figure 9: Batch of targets (3/3)</p>
			<p class="normal">In that example, <strong class="bold">Action 1</strong> was performed in the first transition (<strong class="bold">Target 1</strong>), <strong class="bold">Action 3</strong> was performed in the second transition (<strong class="bold">Target 2</strong>), <strong class="bold">Action 0</strong> was performed in the third transition (<strong class="bold">Target 3</strong>), and so on. Let's populate this in the following lines of code.</p>
			<p class="normal"><strong class="bold">Line 35</strong>: We first start getting the <img src="../Images/B14110_11_199.png" alt=""/> part of the formula of the target:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_200.png" style="height:3em;" alt=""/></figure>
			<p class="normal"><strong class="bold">Line 36</strong>: We check if <code class="Code-In-Text--PACKT-">game_over</code>
 = 1, meaning that the server has gone outside the allowed range of 
server temperatures. Because if it has, there's actually no next state 
(because we basically reset<a id="_idIndexMarker504"/> the 
environment by putting the server's temperature back into the optimal 
range so we start from a new state); and therefore we shouldn't consider
 <img src="../Images/B14110_11_201.png" alt=""/>.</p>
			<p class="normal"><strong class="bold">Line 37</strong>: In that case, we only keep the <img src="../Images/B14110_11_202.png" alt=""/> part of the target.</p>
			<p class="normal"><strong class="bold">Line 38</strong>: However, if the game is not over (<code class="Code-In-Text--PACKT-">game_over</code> = 0)...</p>
			<p class="normal"><strong class="bold">Line 39</strong>: We keep the whole formula of the target, but of course only for the action that was performed, meaning <img src="../Images/B14110_11_203.png" alt=""/> here:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_204.png" style="height:3em;" alt=""/></figure>
			<p class="normal">Hence, we get the following batch of targets, as you saw earlier:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_10.png" alt=""/></figure>
			<p class="packt_figref">Figure 10: Batch of targets (3/3)</p>
			<p class="normal"><strong class="bold">Line 40</strong>: At last, we <code class="Code-In-Text--PACKT-">return</code> the final batches of <code class="Code-In-Text--PACKT-">inputs</code> and <code class="Code-In-Text--PACKT-">targets</code>.</p>
			<p class="normal">That was epic—you've successfully created an <a id="_idIndexMarker505"/>artificial brain. Now that you've done it, we're ready to start the training.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_193" lang="en-GB"><a id="_idTextAnchor268"/>Step 4: Training the AI</h3>
			<p class="normal">Now that our AI has a fully functional brain, it's time to train it. That's exactly what we do in this fourth Python<a id="_idIndexMarker506"/> implementation. You actually have a choice of two files to use for this:</p>
			<ol>
				<li class="list" value="1"><code class="Code-In-Text--PACKT-">training_noearlystopping.py</code>, which trains your AI on a full 1000 epochs of 5-months period.</li>
				<li class="list"><code class="Code-In-Text--PACKT-">training_earlystopping.py</code>, which<a id="_idIndexMarker507"/>
 trains your AI on 1000 epochs as well, but which can stop the training 
early if the performance no longer improves over the iterations. This 
technique is called <strong class="bold">early stopping</strong>.</li>
			</ol>
			<p class="normal">Both these implementations are long, but very 
simple. We start by setting all the parameters, then we build the 
environment by creating an object of the <code class="Code-In-Text--PACKT-">Environment()</code> class, then we build the brain of the AI by creating an object of the <code class="Code-In-Text--PACKT-">Brain()</code> class, then we build the deep Q-learning model by creating an object of the <code class="Code-In-Text--PACKT-">DQN()</code> class, and finally we launch the training connecting all these objects together over 1000 epochs of 5-month periods.</p>
			<p class="normal">You'll notice in the training loop that we also do 
some exploration when performing the actions, performing some random 
actions from time to time. In our case, this will be done 30% of the 
time, since we use an exploration parameter <img src="../Images/B14110_11_205.png" alt=""/>, and then we force the AI to perform a random action when we draw a random value between 0 and 1 that is below <img src="../Images/B14110_11_205.png" alt=""/>. The reason we do some exploration is because it improves the deep reinforcement learning process, as we discussed in <em class="italics">Chapter 9</em>, <em class="italics">Going Pro with Artificial Brains – Deep Q-Learning</em>,
 and the reason we don't use Softmax in this project is just to give you
 a look at how to implement a different exploration method.</p>
			<p class="normal">Later, you'll be introduced to another little improvement in the <code class="Code-In-Text--PACKT-">training_noearlystopping.py</code> file, where we use an early stopping technique which stops the training early if there's no improvement in the performance.</p>
			<p class="normal">Let's highlight the new steps which still belong to our general AI framework/Blueprint:</p>
			<ul>
				<li class="list"><strong class="bold">Step 4-1</strong>: Building the environment by creating an object of the <code class="Code-In-Text--PACKT-">Environment</code> class.</li>
				<li class="list"><strong class="bold">Step 4-2</strong>: Building the artificial brain by creating an object of the <code class="Code-In-Text--PACKT-">Brain</code> class.</li>
				<li class="list"><strong class="bold">Step 4-3</strong>: Building the <code class="Code-In-Text--PACKT-">DQN</code> model by creating an object of the <code class="Code-In-Text--PACKT-">DQN</code> class.</li>
				<li class="list"><strong class="bold">Step 4-4</strong>: Selecting the training mode.</li>
				<li class="list"><strong class="bold">Step 4-5</strong>: Starting the<a id="_idIndexMarker508"/> training with a <code class="Code-In-Text--PACKT-">for</code> loop over 100 epochs of 5-month periods.</li>
				<li class="list"><strong class="bold">Step 4-6</strong>: During each epoch we repeat the whole deep Q-learning process, while also doing some exploration 30% of the time.</li>
			</ul>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_194" lang="en-GB"><a id="_idTextAnchor269"/>No early stopping</h4>
			<p class="normal">Ready to implement this? Maybe get a good coffee or tea first because this is <a id="_idIndexMarker509"/>going
 to be a bit long (88 lines of code, but easy ones!). We'll start 
without early stopping and then at the end I'll explain how to add the 
early stopping technique. The file to follow along with is <code class="Code-In-Text--PACKT-">training_noearlystopping.py</code>. Since this is pretty long, let's do it section by section this time, starting with the first one:</p>
			<pre class="programlisting language-markup"><code class="hljs clean"># AI for Business - Minimize cost <span class="hljs-keyword">with</span> Deep Q-Learning   #<span class="hljs-number">1</span>
# Training the AI without Early Stopping   #<span class="hljs-number">2</span>
#<span class="hljs-number">3</span>
# Importing the libraries and the other python files   #<span class="hljs-number">4</span>
<span class="hljs-keyword">import</span> os   #<span class="hljs-number">5</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np   #<span class="hljs-number">6</span>
<span class="hljs-keyword">import</span> random <span class="hljs-keyword">as</span> rn   #<span class="hljs-number">7</span>
<span class="hljs-keyword">import</span> environment   #<span class="hljs-number">8</span>
<span class="hljs-keyword">import</span> brain_nodropout   #<span class="hljs-number">9</span>
<span class="hljs-keyword">import</span> dqn   #<span class="hljs-number">10</span>
</code></pre>
			
			
			
			
			
			
			
			
			
			<p class="normal"><strong class="bold">Line 5</strong>: We import the <code class="Code-In-Text--PACKT-">os</code>
 library, which will be used to set a seed for reproducibility so that 
if you run the training several times, you'll get the same result each 
time. You can, of course, choose to remove this when you tinker with the
 code yourself!</p>
			<p class="normal"><strong class="bold">Line 6</strong>: We import the <code class="Code-In-Text--PACKT-">numpy</code> library, since we'll work with <code class="Code-In-Text--PACKT-">numpy</code> arrays.</p>
			<p class="normal"><strong class="bold">Line 7</strong>: We import the <code class="Code-In-Text--PACKT-">random</code> library, which we'll use to do some exploration.</p>
			<p class="normal"><strong class="bold">Line 8</strong>: We import the <code class="Code-In-Text--PACKT-">environment.py</code> file, implemented in Step 1, which contains the whole defined environment.</p>
			<p class="normal"><strong class="bold">Line 9</strong>: We import the <code class="Code-In-Text--PACKT-">brain_nodropout.py</code> file, our artificial brain without dropout that we implemented in Step 2. This contains the whole neural network of our AI.</p>
			<p class="normal"><strong class="bold">Line 10</strong>: We import the <code class="Code-In-Text--PACKT-">dqn.py</code> file<a id="_idIndexMarker510"/> implemented in Step 3, which contains the main parts of the deep Q-learning algorithm, including experience replay.</p>
			<p class="normal">Moving on to the next section:</p>
			<pre class="programlisting language-markup"><code class="hljs makefile"><span class="hljs-comment"># Setting seeds for reproducibility   #12</span>
os.environ['PYTHONHASHSEED'] = '0'   <span class="hljs-comment">#13</span>
np.random.seed(42)   <span class="hljs-comment">#14</span>
rn.seed(12345)   <span class="hljs-comment">#15</span>
<span class="hljs-comment">#16</span>
<span class="hljs-comment"># SETTING THE PARAMETERS   #17</span>
epsilon = .3   <span class="hljs-comment">#18</span>
number_actions = 5   <span class="hljs-comment">#19</span>
direction_boundary = (number_actions - 1) / 2   <span class="hljs-comment">#20</span>
number_epochs = 100   <span class="hljs-comment">#21</span>
max_memory = 3000   <span class="hljs-comment">#22</span>
batch_size = 512   <span class="hljs-comment">#23</span>
temperature_step = 1.5   <span class="hljs-comment">#24</span>
<span class="hljs-comment">#25</span>
<span class="hljs-comment"># BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS   #26</span>
env = environment.Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)   <span class="hljs-comment">#27</span>
<span class="hljs-comment">#28</span>
<span class="hljs-comment"># BUILDING THE BRAIN BY SIMPLY CREATING AN OBJECT OF THE BRAIN CLASS   #29</span>
brain = brain_nodropout.Brain(learning_rate = 0.00001, number_actions = number_actions)   <span class="hljs-comment">#30</span>
<span class="hljs-comment">#31</span>
<span class="hljs-comment"># BUILDING THE DQN MODEL BY SIMPLY CREATING AN OBJECT OF THE DQN CLASS   #32</span>
dqn = dqn.DQN(max_memory = max_memory, discount = 0.9)   <span class="hljs-comment">#33</span>
<span class="hljs-comment">#34</span>
<span class="hljs-comment"># CHOOSING THE MODE   #35</span>
train = True   <span class="hljs-comment">#36</span>
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal"><strong class="bold">Lines 13, 14, and 15</strong>:
 We set seeds for reproducibility, to get the same results after several
 rounds of training. This is really only important so you can reproduce 
your findings—if you don't need to do that, some<a id="_idIndexMarker511"/> people prefer them and others don't. If you don't want the seeds you can just remove them.</p>
			<p class="normal"><strong class="bold">Line 18</strong>: We introduce the exploration factor <img src="../Images/B14110_11_207.png" alt=""/>, and we set it to <code class="Code-In-Text--PACKT-">0.3</code>,
 meaning that there will be 30% of exploration (performing random 
actions) vs. 70% of exploitation (performing the actions of the AI).</p>
			<p class="normal"><strong class="bold">Line 19</strong>: We set the number of actions to <code class="Code-In-Text--PACKT-">5</code>.</p>
			<p class="normal"><strong class="bold">Line 20</strong>: We set the 
direction boundary, meaning the action index below which we cool down 
the server, and above which we heat up the server. Since actions 0 and 
1 cool down the server, and actions 3 and 4 heat up the server, that 
direction boundary is (5-1)/2 = 2, which corresponds to the action that 
transfers no heat to the server (action 2).</p>
			<p class="normal"><strong class="bold">Line 21</strong>: We set the number of training epochs to <code class="Code-In-Text--PACKT-">100</code>.</p>
			<p class="normal"><strong class="bold">Line 22</strong>: We set the memory capacity, meaning its maximum size, to <code class="Code-In-Text--PACKT-">3000</code>.</p>
			<p class="normal"><strong class="bold">Line 23</strong>: We set the batch size to <code class="Code-In-Text--PACKT-">512</code>.</p>
			<p class="normal"><strong class="bold">Line 24</strong>: We introduce
 the temperature step, meaning the absolute temperature change that the 
AI cause onto the server by playing actions 0, 1, 3, or 4. And that's 
of course <code class="Code-In-Text--PACKT-">1.5</code>°C.</p>
			<p class="normal"><strong class="bold">Line 27</strong>: We create the <code class="Code-In-Text--PACKT-">environment</code> object, as an instance of the <code class="Code-In-Text--PACKT-">Environment</code> class which we call from the <code class="Code-In-Text--PACKT-">environment</code> file. Inside this <code class="Code-In-Text--PACKT-">Environment</code> class, we enter all the arguments of the <code class="Code-In-Text--PACKT-">init</code> method:</p>
			<pre class="programlisting language-markup"><code class="hljs ini"><span class="hljs-attr">optimal_temperature</span> = (<span class="hljs-number">18.0</span>, <span class="hljs-number">24.0</span>),
<span class="hljs-attr">initial_month</span> = <span class="hljs-number">0</span>,
<span class="hljs-attr">initial_number_users</span> = <span class="hljs-number">20</span>,
<span class="hljs-attr">initial_rate_data</span> = <span class="hljs-number">30</span>
</code></pre>
			
			
			
			<p class="normal"><strong class="bold">Line 30</strong>: We create the <code class="Code-In-Text--PACKT-">brain</code> object as an instance of the <code class="Code-In-Text--PACKT-">Brain</code> class, which we call from the <code class="Code-In-Text--PACKT-">brain_nodropout</code> file. Inside this <code class="Code-In-Text--PACKT-">Brain</code> class, we enter all the arguments of the <code class="Code-In-Text--PACKT-">init</code> method:</p>
			<pre class="programlisting language-markup"><code class="hljs ini"><span class="hljs-attr">learning_rate</span> = <span class="hljs-number">0.00001</span>,
<span class="hljs-attr">number_actions</span> = number_actions
</code></pre>
			
			<p class="normal"><strong class="bold">Line 33</strong>: We create the <code class="Code-In-Text--PACKT-">dqn</code> object as an instance of the <code class="Code-In-Text--PACKT-">DQN</code> class, which we call from the <code class="Code-In-Text--PACKT-">dqn</code> file. Inside this <code class="Code-In-Text--PACKT-">DQN</code> class we enter all the arguments of the <code class="Code-In-Text--PACKT-">init</code> method:</p>
			<pre class="programlisting language-markup"><code class="hljs ini"><span class="hljs-attr">max_memory</span> = max_memory,
<span class="hljs-attr">discount</span> = <span class="hljs-number">0.9</span>
</code></pre>
			
			<p class="normal"><strong class="bold">Line 36</strong>: We set the training mode to <code class="Code-In-Text--PACKT-">True</code>, because the next code section will contain the big <code class="Code-In-Text--PACKT-">for</code> loop that performs all the training.</p>
			<p class="normal">All good so far? Don't forget to take <a id="_idIndexMarker512"/>a break or a step back by reading the previous paragraphs again anytime you feel a bit overwhelmed or lost.</p>
			<p class="normal">Now let's begin the big training loop; that's the last code section of this file:</p>
			<pre class="programlisting language-markup"><code class="hljs angelscript"># TRAINING THE AI   #<span class="hljs-number">38</span>
env.train = train   #<span class="hljs-number">39</span>
model = brain.model   #<span class="hljs-number">40</span>
<span class="hljs-keyword">if</span> (env.train):   #<span class="hljs-number">41</span>
    # STARTING THE LOOP OVER ALL THE EPOCHS (<span class="hljs-number">1</span> Epoch = <span class="hljs-number">5</span> Months)   #<span class="hljs-number">42</span>
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, number_epochs):   #<span class="hljs-number">43</span>
        # INITIALIAZING ALL THE VARIABLES OF BOTH THE ENVIRONMENT AND THE TRAINING LOOP   #<span class="hljs-number">44</span>
        total_reward = <span class="hljs-number">0</span>   #<span class="hljs-number">45</span>
        loss = <span class="hljs-number">0.</span>   #<span class="hljs-number">46</span>
        new_month = np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">12</span>)   #<span class="hljs-number">47</span>
        env.reset(new_month = new_month)   #<span class="hljs-number">48</span>
        game_over = False   #<span class="hljs-number">49</span>
        current_state, _, _ = env.observe()   #<span class="hljs-number">50</span>
        timestep = <span class="hljs-number">0</span>   #<span class="hljs-number">51</span>
        # STARTING THE LOOP OVER ALL THE TIMESTEPS (<span class="hljs-number">1</span> Timestep = <span class="hljs-number">1</span> Minute) IN ONE EPOCH   #<span class="hljs-number">52</span>
        <span class="hljs-keyword">while</span> ((<span class="hljs-keyword">not</span> game_over) <span class="hljs-keyword">and</span> timestep &lt;= <span class="hljs-number">5</span> * <span class="hljs-number">30</span> * <span class="hljs-number">24</span> * <span class="hljs-number">60</span>):   #<span class="hljs-number">53</span>
            # PLAYING THE NEXT ACTION BY EXPLORATION   #<span class="hljs-number">54</span>
            <span class="hljs-keyword">if</span> np.random.rand() &lt;= epsilon:   #<span class="hljs-number">55</span>
                action = np.random.randint(<span class="hljs-number">0</span>, number_actions)   #<span class="hljs-number">56</span>
                <span class="hljs-keyword">if</span> (action - direction_boundary &lt; <span class="hljs-number">0</span>):   #<span class="hljs-number">57</span>
                    direction = <span class="hljs-number">-1</span>   #<span class="hljs-number">58</span>
                <span class="hljs-keyword">else</span>:   #<span class="hljs-number">59</span>
                    direction = <span class="hljs-number">1</span>   #<span class="hljs-number">60</span>
                energy_ai = abs(action - direction_boundary) * temperature_step   #<span class="hljs-number">61</span>
            # PLAYING THE NEXT ACTION BY INFERENCE   #<span class="hljs-number">62</span>
            <span class="hljs-keyword">else</span>:   #<span class="hljs-number">63</span>
                q_values = model.predict(current_state)   #<span class="hljs-number">64</span>
                action = np.argmax(q_values[<span class="hljs-number">0</span>])   #<span class="hljs-number">65</span>
                <span class="hljs-keyword">if</span> (action - direction_boundary &lt; <span class="hljs-number">0</span>):   #<span class="hljs-number">66</span>
                    direction = <span class="hljs-number">-1</span>   #<span class="hljs-number">67</span>
                <span class="hljs-keyword">else</span>:   #<span class="hljs-number">68</span>
                    direction = <span class="hljs-number">1</span>   #<span class="hljs-number">69</span>
                energy_ai = abs(action - direction_boundary) * temperature_step   #<span class="hljs-number">70</span>
            # UPDATING THE ENVIRONMENT AND REACHING THE NEXT STATE   #<span class="hljs-number">71</span>
            next_state, reward, game_over = env.update_env(direction, energy_ai, ( new_month + <span class="hljs-built_in">int</span>(timestep/(<span class="hljs-number">30</span>*<span class="hljs-number">24</span>*<span class="hljs-number">60</span>)) ) % <span class="hljs-number">12</span>)   #<span class="hljs-number">72</span>
            total_reward += reward   #<span class="hljs-number">73</span>
            # STORING THIS NEW TRANSITION INTO THE MEMORY   #<span class="hljs-number">74</span>
            dqn.remember([current_state, action, reward, next_state], game_over)   #<span class="hljs-number">75</span>
            # GATHERING IN TWO SEPARATE BATCHES THE INPUTS AND THE TARGETS   #<span class="hljs-number">76</span>
            inputs, targets = dqn.get_batch(model, batch_size = batch_size)   #<span class="hljs-number">77</span>
            # COMPUTING THE LOSS OVER THE TWO WHOLE BATCHES OF INPUTS AND TARGETS   #<span class="hljs-number">78</span>
            loss += model.train_on_batch(inputs, targets)   #<span class="hljs-number">79</span>
            timestep += <span class="hljs-number">1</span>   #<span class="hljs-number">80</span>
            current_state = next_state   #<span class="hljs-number">81</span>
        # PRINTING THE TRAINING RESULTS FOR EACH EPOCH   #<span class="hljs-number">82</span>
        print(<span class="hljs-string">"\n"</span>)   #<span class="hljs-number">83</span>
        print(<span class="hljs-string">"Epoch: {:03d}/{:03d}"</span>.format(epoch, number_epochs))   #<span class="hljs-number">84</span>
        print(<span class="hljs-string">"Total Energy spent with an AI: {:.0f}"</span>.format(env.total_energy_ai))   #<span class="hljs-number">85</span>
        print(<span class="hljs-string">"Total Energy spent with no AI: {:.0f}"</span>.format(env.total_energy_noai))   #<span class="hljs-number">86</span>
        # SAVING THE MODEL   #<span class="hljs-number">87</span>
        model.save(<span class="hljs-string">"model.h5"</span>)   #<span class="hljs-number">88</span>
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal"><strong class="bold">Line 39</strong>: We set the <code class="Code-In-Text--PACKT-">env.train</code> object variable (this is a variable of our <code class="Code-In-Text--PACKT-">environment</code> object) to the value of the <code class="Code-In-Text--PACKT-">train</code> variable entered just before, which is of course equal to <code class="Code-In-Text--PACKT-">True</code>, meaning we are indeed in training mode.</p>
			<p class="normal"><strong class="bold">Line 40</strong>: We get the<a id="_idIndexMarker513"/> model from our <code class="Code-In-Text--PACKT-">brain</code>
 object. This model contains the whole architecture of the neural 
network, plus its optimizer. It also has extra practical tools, like for
 example the <code class="Code-In-Text--PACKT-">save</code> and <code class="Code-In-Text--PACKT-">load</code> methods, which will allow us respectively to save the weights after the training or load them anytime in the future.</p>
			<p class="normal"><strong class="bold">Line 41</strong>: If we are in training mode…</p>
			<p class="normal"><strong class="bold">Line 43</strong>: We start the main training <code class="Code-In-Text--PACKT-">for</code> loop, iterating the training epochs from 1 to 100.</p>
			<p class="normal"><strong class="bold">Line 45</strong>: We set the total reward (total reward accumulated over the training iterations) to <code class="Code-In-Text--PACKT-">0</code>.</p>
			<p class="normal"><strong class="bold">Line 46</strong>: We set the loss to <code class="Code-In-Text--PACKT-">0</code> (<code class="Code-In-Text--PACKT-">0</code> because the loss will be a <code class="Code-In-Text--PACKT-">float</code>).</p>
			<p class="normal"><strong class="bold">Line 47</strong>: We set the starting month of the training, called <code class="Code-In-Text--PACKT-">new_month</code>, to a random integer between 0 and 11. For example, if the random integer is 2, we start the training in March.</p>
			<p class="normal"><strong class="bold">Line 48</strong>: By calling the <code class="Code-In-Text--PACKT-">reset</code> method from our <code class="Code-In-Text--PACKT-">env</code> object of the <code class="Code-In-Text--PACKT-">Environment</code> class built in Step 1, we reset the environment starting from that <code class="Code-In-Text--PACKT-">new_month</code>.</p>
			<p class="normal"><strong class="bold">Line 49</strong>: We set the <code class="Code-In-Text--PACKT-">game_over</code> variable to <code class="Code-In-Text--PACKT-">False</code>, because we're starting in the allowed range of server temperatures.</p>
			<p class="normal"><strong class="bold">Line 50</strong>: By calling the <code class="Code-In-Text--PACKT-">observe</code> method from our <code class="Code-In-Text--PACKT-">env</code> object of the <code class="Code-In-Text--PACKT-">Environment</code> class built in Step 1, we get the current state only, which is our starting state.</p>
			<p class="normal"><strong class="bold">Line 51</strong>: We set the first <code class="Code-In-Text--PACKT-">timestep</code> to <code class="Code-In-Text--PACKT-">0</code>. This is the first minute of the training.</p>
			<p class="normal"><strong class="bold">Line 53</strong>: We start the <code class="Code-In-Text--PACKT-">while</code>
 loop that will iterate all the timesteps (minutes) for the whole period
 of the epoch, which is 5 months. Therefore, we iterate through <code class="Code-In-Text--PACKT-">5 * 30 * 24 * 60</code> minutes; that is, 216,000 timesteps. </p>
			<p class="normal">If, however, during those timesteps we go outside the allowed range of server temperatures (that is, if <code class="Code-In-Text--PACKT-">game_over</code> = 1), then we stop the epoch and we start a new one.</p>
			<p class="normal">Lines 55 to 61 make sure the<a id="_idIndexMarker514"/>
 AI performs a random action 30% of the time. This is exploration. The 
trick to it in this case is to sample a random number between 0 and 1, 
and if this random number is between 0 and 0.3, the AI performs a random
 action. That means the AI will perform a random action 30% of the time,
 because this sampled number has a 30% chance to be between 0 and 0.3.</p>
			<p class="normal"><strong class="bold">Line 55</strong>: If a sampled number between 0 and 1 is below <img src="../Images/B14110_11_208.png" alt=""/>...</p>
			<p class="normal"><strong class="bold">Line 56</strong>: ... we play a random action index from 0 to 4.</p>
			<p class="normal"><strong class="bold">Line 57</strong>: Now that 
we've just performed an action, we compute the direction and the energy 
spent; remember that they're are the required arguments of the <code class="Code-In-Text--PACKT-">update_env</code> method of the <code class="Code-In-Text--PACKT-">Environment</code>
 class, which we'll call later to update the environment. The AI 
distinguishes between two cases by checking if the action is below or 
above the direction boundary of 2. If the action is below the direction 
boundary of 2, meaning the AI cools down the server...</p>
			<p class="normal"><strong class="bold">Line 58</strong>: ...then the heating direction is equal to <code class="Code-In-Text--PACKT-">-1</code> (cooling down).</p>
			<p class="normal"><strong class="bold">Line 59 and 60</strong>: Else the heating direction is equal to <code class="Code-In-Text--PACKT-">+1</code> (heating up).</p>
			<p class="normal"><strong class="bold">Line 61</strong>: We compute the energy spent by the AI onto the server, which according to Assumption 2 is:</p>
			<p class="normal">|<em class="italics">action</em> - <em class="italics">direction_boundary</em>| * <em class="italics">temperature_step</em> = |<em class="italics">action</em> - 2| * 1.5 <em class="italics">Joules</em></p>
			<p class="normal">For example, if the action is 4, then the AI heats 
up the server by 3°C, and so according to Assumption 2 the energy spent 
is 3 Joules. And we check indeed that |4-2|*1.5 = 3.</p>
			<p class="normal"><strong class="bold">Line 63</strong>: Now we play the actions by inference, meaning directly from our AI's predictions. The inference starts from the <code class="Code-In-Text--PACKT-">else</code> statement, which corresponds to the <code class="Code-In-Text--PACKT-">if</code> statement of line 55. This <code class="Code-In-Text--PACKT-">else</code> corresponds to the situation where the sampled number is between 0.3 and 1, which happens 70% of the time.</p>
			<p class="normal"><strong class="bold">Line 64</strong>: By calling the <code class="Code-In-Text--PACKT-">predict</code> method from our <code class="Code-In-Text--PACKT-">model</code> object (<code class="Code-In-Text--PACKT-">predict</code> is a pre-built method of the <code class="Code-In-Text--PACKT-">Model</code> class), we get the five predicted Q-values from our AI model.</p>
			<p class="normal"><strong class="bold">Line 65</strong>: Using the <code class="Code-In-Text--PACKT-">argmax</code> function from <code class="Code-In-Text--PACKT-">numpy</code>, we select the action that has the maximum Q-value among the five predicted ones at Line 64.</p>
			<p class="normal"><strong class="bold">Lines 66 to 70</strong>: We do exactly the same as in Lines 57 to 61, but this time with the action performed by inference.</p>
			<p class="normal"><strong class="bold">Line 72</strong>: Now we have everything ready to update the environment. We call the big <code class="Code-In-Text--PACKT-">update_env</code> method made in the <code class="Code-In-Text--PACKT-">Environment</code>
 class of Step 1, by inputting the heating direction, the energy spent 
by the AI, and the month we're in at that specific timestep of the <code class="Code-In-Text--PACKT-">while</code>
 loop. We get in return the next state, the reward received, and whether
 the game is over (that is, whether or not we went outside the optimal 
range of server temperatures).</p>
			<p class="normal"><strong class="bold">Line 73</strong>: We add this last reward received to the total reward.</p>
			<p class="normal"><strong class="bold">Line 75</strong>: By calling the <code class="Code-In-Text--PACKT-">remember</code> method from our <code class="Code-In-Text--PACKT-">dqn</code> object of the <code class="Code-In-Text--PACKT-">DQN</code> class built in Step 3, we store the new transition [[<code class="Code-In-Text--PACKT-">current_state</code>, <code class="Code-In-Text--PACKT-">action</code>, <code class="Code-In-Text--PACKT-">reward</code>, <code class="Code-In-Text--PACKT-">next_state</code>], <code class="Code-In-Text--PACKT-">game_over</code>] into the memory.</p>
			<p class="normal"><strong class="bold">Line 77</strong>: By calling the <code class="Code-In-Text--PACKT-">get_batch</code> method from our <code class="Code-In-Text--PACKT-">dqn</code> object of the <code class="Code-In-Text--PACKT-">DQN</code> class built in Step 3, we create two separate batches of <code class="Code-In-Text--PACKT-">inputs</code> and <code class="Code-In-Text--PACKT-">targets</code>, each one having 512 elements (since <code class="Code-In-Text--PACKT-">batch_size</code> = 512).</p>
			<p class="normal"><strong class="bold">Line 79</strong>: By calling the <code class="Code-In-Text--PACKT-">train_on_batch</code> method from our <code class="Code-In-Text--PACKT-">model</code> object (<code class="Code-In-Text--PACKT-">train_on_batch</code> is a pre-built method of the <code class="Code-In-Text--PACKT-">Model</code>
 class), we compute the loss error between the predictions and the 
targets over the whole batch. As a reminder, this loss error is the 
mean-squared error loss. Then in this same line, we add this loss error 
to the total loss of the epoch, in case we want to check how this total 
loss evolves over the epochs during the training.</p>
			<p class="normal"><strong class="bold">Line 80</strong>: We increment the <code class="Code-In-Text--PACKT-">timestep</code>.</p>
			<p class="normal"><strong class="bold">Line 81</strong>: We update the current state, which becomes the new state reached.</p>
			<p class="normal"><strong class="bold">Line 83</strong>: We print a new line to separate out the training results so we can look them over easily.</p>
			<p class="normal"><strong class="bold">Line 84</strong>: We print the epoch reached (the one we are in at this specific moment of the main training <code class="Code-In-Text--PACKT-">for</code> loop).</p>
			<p class="normal"><strong class="bold">Line 85</strong>: We print the total <a id="_idIndexMarker515"/>energy spent by the AI over that specific epoch (the one we are in at this specific moment of the main training <code class="Code-In-Text--PACKT-">for</code> loop).</p>
			<p class="normal"><strong class="bold">Line 86</strong>: We print the total energy spent by the server's integrated cooling system over that same specific epoch.</p>
			<p class="normal"><strong class="bold">Line 88</strong>: We save the 
model's weights at the end of the training, in order to load them in the
 future, anytime we want to use our pre-trained model to regulate a 
server's temperature.</p>
			<p class="normal">That's it for training our<a id="_idIndexMarker516"/> AI without early stopping; now let's have a look at what you'd need to change to implement it.</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_195" lang="en-GB"><a id="_idTextAnchor270"/>Early stopping</h4>
			<p class="normal">Now open the <code class="Code-In-Text--PACKT-">training_earlystopping.py</code> file. Compare it to the previous file; all the lines of code from 1 to 40 are the same. Then, in the last code<a id="_idIndexMarker517"/> section, <code class="Code-In-Text--PACKT-">TRAINING THE AI</code>,
 we have the same process, to which is added the early stopping 
technique. As a reminder, it consists of stopping the training if 
there's no more improvement of the performance, which could be assessed 
two different ways:</p>
			<ol>
				<li class="list" value="1">If the total reward of an epoch no longer increases much over the epochs.</li>
				<li class="list">If the total loss of an epoch no longer decreases much over the epochs.</li>
			</ol>
			<p class="normal">Let's see how we do this.</p>
			<p class="normal">First, we introduce four new variables just before the main training <code class="Code-In-Text--PACKT-">for</code> loop:</p>
			<pre class="programlisting language-markup"><code class="hljs makefile"><span class="hljs-comment"># TRAINING THE AI   #38</span>
env.train = train   <span class="hljs-comment">#39</span>
model = brain.model   <span class="hljs-comment">#40</span>
early_stopping = True   <span class="hljs-comment">#41</span>
patience = 10   <span class="hljs-comment">#42</span>
best_total_reward = -np.inf   <span class="hljs-comment">#43</span>
patience_count = 0   <span class="hljs-comment">#44</span>
if (env.train):   <span class="hljs-comment">#45</span>
    <span class="hljs-comment"># STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)   #46</span>
    for epoch in range(1, number_epochs):   <span class="hljs-comment">#47</span>
</code></pre>
			
			
			
			
			
			
			
			
			
			<p class="normal"><strong class="bold">Line 41</strong>: We introduce a new variable, <code class="Code-In-Text--PACKT-">early_stopping</code>, which is set equal to <code class="Code-In-Text--PACKT-">True</code>
 if we decide to activate the early stopping technique, meaning if we 
decide to stop the training when the performance no longer improves.</p>
			<p class="normal"><strong class="bold">Line 42</strong>: We introduce a new variable, <code class="Code-In-Text--PACKT-">patience</code>,
 which is the number of epochs we wait without performance improvement 
before stopping the training. Here we choose a patience of <code class="Code-In-Text--PACKT-">10</code>
 epochs, which means that if the best total reward of an epoch doesn't 
increase during the next 10 epochs, we will stop the training.</p>
			<p class="normal"><strong class="bold">Line 43</strong>: We introduce a new variable, <code class="Code-In-Text--PACKT-">best_total_reward</code>,
 which is the best total reward recorded over a full epoch. If we don't 
beat that best total reward before 10 epochs go by, the training stops. 
It's initialized to <code class="Code-In-Text--PACKT-">-np.inf</code>, which represents <code class="Code-In-Text--PACKT-">-infinity</code>. That's just<a id="_idIndexMarker518"/>
 a trick to say that nothing can be lower than that best total reward at
 the beginning. Then as soon as we get the first total reward over the 
first epoch, <code class="Code-In-Text--PACKT-">best_total_reward</code> becomes that first total reward.</p>
			<p class="normal"><strong class="bold">Line 44</strong>: We introduce a new variable, <code class="Code-In-Text--PACKT-">patience_count</code>, which is a counter starting from <code class="Code-In-Text--PACKT-">0</code>, and is incremented by 1 each time the total reward of an epoch doesn't beat the best total reward. If <code class="Code-In-Text--PACKT-">patience_count</code> reaches 10 (the patience), we stop the training. And if one epoch beats the best total reward, <code class="Code-In-Text--PACKT-">patience_count</code> is reset to 0.</p>
			<p class="normal">Then, the main training <code class="Code-In-Text--PACKT-">for</code> loop is the same as before, but just before saving the model we add the following:</p>
			<pre class="programlisting language-markup"><code class="hljs mipsasm">       <span class="hljs-comment"># EARLY STOPPING   #91</span>
        if (early_stopping):   <span class="hljs-comment">#32</span>
            if (total_reward &lt;= <span class="hljs-keyword">best_total_reward): </span>  <span class="hljs-comment">#93</span>
                patience_count += <span class="hljs-number">1</span>   <span class="hljs-comment">#94</span>
            elif (total_reward &gt; <span class="hljs-keyword">best_total_reward): </span>  <span class="hljs-comment">#95</span>
                <span class="hljs-keyword">best_total_reward </span>= total_reward   <span class="hljs-comment">#96</span>
                patience_count = <span class="hljs-number">0</span>   <span class="hljs-comment">#97</span>
            if (patience_count &gt;= patience):   <span class="hljs-comment">#98</span>
                print(<span class="hljs-string">"Early Stopping"</span>)   <span class="hljs-comment">#99</span>
                <span class="hljs-keyword">break </span>  <span class="hljs-comment">#100</span>
        <span class="hljs-comment"># SAVING THE MODEL   #101</span>
        model.save(<span class="hljs-string">"model.h5"</span>)   <span class="hljs-comment">#102</span>
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal"><strong class="bold">Line 92</strong>: If the <code class="Code-In-Text--PACKT-">early_stopping</code> variable is <code class="Code-In-Text--PACKT-">True</code>, meaning if the early stopping technique is activated…</p>
			<p class="normal"><strong class="bold">Line 93</strong>: And if the total reward of the current epoch (we are still in the main training <code class="Code-In-Text--PACKT-">for</code> loop that iterates the epochs) is lower than the best total reward of an epoch obtained so far…</p>
			<p class="normal"><strong class="bold">Line 94</strong>: ...we increment the <code class="Code-In-Text--PACKT-">patience_count</code> variable by <code class="Code-In-Text--PACKT-">1</code>.</p>
			<p class="normal"><strong class="bold">Line 95</strong>: However, if the total reward of the current epoch is higher than the best total reward of an epoch obtained so far…</p>
			<p class="normal"><strong class="bold">Line 96</strong>: ...we update the best total reward, which becomes that new total reward of the current epoch.</p>
			<p class="normal"><strong class="bold">Line 97</strong>: ...and we reset the <code class="Code-In-Text--PACKT-">patience_count</code> variable to <code class="Code-In-Text--PACKT-">0</code>.</p>
			<p class="normal"><strong class="bold">Line 98</strong>: Then in a new <code class="Code-In-Text--PACKT-">if</code> condition, we check that if the <code class="Code-In-Text--PACKT-">patience_count</code> variable goes higher than the patience of 10…</p>
			<p class="normal"><strong class="bold">Line 99</strong>: ...we print <code class="Code-In-Text--PACKT-">Early Stopping</code>,</p>
			<p class="normal"><strong class="bold">Line 100</strong>: ...and we stop<a id="_idIndexMarker519"/> the main training <code class="Code-In-Text--PACKT-">for</code> loop with a <code class="Code-In-Text--PACKT-">break</code> statement.</p>
			<p class="normal">That's the whole thing. Easy and intuitive, right? Now you know how to implement early stopping.</p>
			<p class="normal">After executing the code (I'll explain how to run 
this in a bit), we'll already see some good performances from our AI 
during the training, spending less energy than the server's integrated 
cooling system most of the time. But that's only training; now we need 
to see if we get good performance from the AI on a new 1-year 
simulation. That's where our next and final Python file comes into play.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_196" lang="en-GB"><a id="_idTextAnchor271"/>Step 5 – Testing the AI</h3>
			<p class="normal">Now we need to test the performance of our AI in a brand-new situation. To do so, we<a id="_idIndexMarker520"/>
 run a 1-year simulation in inference mode, meaning that there's no 
training happening at any time. Our AI only returns predictions over a 
full year of simulation. Then, thanks to our environment object, in the 
end we'll be able to see the total energy spent by the AI over the full 
year, as well as the total energy that would have been spent in the 
exact same year by the server's integrated cooling system. Finally, we 
compare these two total energies spent, by computing their relative 
difference (in %) which shows us precisely the total energy saved by the
 AI. Buckle up for the final results—we'll reveal them very soon!</p>
			<p class="normal">In terms of the AI blueprint, for the testing 
implementation we have almost the same process as the training 
implementation, except that this time we don't need to create a <code class="Code-In-Text--PACKT-">brain</code> object nor a <code class="Code-In-Text--PACKT-">DQN</code>
 model object; and, of course, we won't run the deep Q-learning process 
over some training epochs. However, we do have to create a new <code class="Code-In-Text--PACKT-">environment</code> object, and instead of creating a <code class="Code-In-Text--PACKT-">brain</code>,
 we'll load our artificial brain with its pre-trained weights from the 
previous training that we executed in Step 4 – Training the AI. Let's 
take a look at the final sub-steps of this final part of 
the AI framework/Blueprint:</p>
			<ul>
				<li class="list"><strong class="bold">Step 5-1</strong>: Build a new environment by creating an object of the <code class="Code-In-Text--PACKT-">Environment</code> class.</li>
				<li class="list"><strong class="bold">Step 5-2</strong>: Load the artificial brain with its pre-trained weights from the previous training.</li>
				<li class="list"><strong class="bold">Step 5-3</strong>: Choose the inference mode.</li>
				<li class="list"><strong class="bold">Step 5-4</strong>: Start the 1-year simulation.</li>
				<li class="list"><strong class="bold">Step 5-5</strong>: In each 
iteration (each minute), our AI only performs the action that results 
from its prediction, and no exploration or deep Q-learning training 
happens whatsoever.</li>
			</ul>
			<p class="normal">The implementation is a piece<a id="_idIndexMarker521"/> of cake to understand. It's actually the same as the training file, except that:</p>
			<ol>
				<li class="list" value="1">Instead of creating a <code class="Code-In-Text--PACKT-">brain</code> object from the <code class="Code-In-Text--PACKT-">Brain</code> class, we load the pre-trained weights resulting from the training.</li>
				<li class="list">Instead of running a training loop over 100 epochs 
of 5-month periods, we run an inference loop over a single 12-month 
period. Inside this inference loop, you'll find exactly the same code as
 the inference part of the training <code class="Code-In-Text--PACKT-">for</code> loop. You've got this!</li>
			</ol>
			<p class="normal">Have a look at the full testing implementation in the following code:</p>
			<pre class="programlisting language-markup"><code class="hljs nix"><span class="hljs-comment"># AI for Business - Minimize cost with Deep Q-Learning</span>
<span class="hljs-comment"># Testing the AI</span>

<span class="hljs-comment"># Installing Keras</span>
<span class="hljs-comment"># conda install -c conda-forge keras</span>

<span class="hljs-comment"># Importing the libraries and the other python files</span>
<span class="hljs-built_in">import</span> os
<span class="hljs-built_in">import</span> numpy as np
<span class="hljs-built_in">import</span> random as rn
from keras.models <span class="hljs-built_in">import</span> load_model
<span class="hljs-built_in">import</span> environment

<span class="hljs-comment"># Setting seeds for reproducibility</span>
os.environ['PYTHONHASHSEED'] = '<span class="hljs-number">0</span>'
np.random.seed(<span class="hljs-number">42</span>)
rn.seed(<span class="hljs-number">12345</span>)

<span class="hljs-comment"># SETTING THE PARAMETERS</span>
<span class="hljs-attr">number_actions</span> = <span class="hljs-number">5</span>
<span class="hljs-attr">direction_boundary</span> = (number_actions - <span class="hljs-number">1</span>) / <span class="hljs-number">2</span>
<span class="hljs-attr">temperature_step</span> = <span class="hljs-number">1.5</span>

<span class="hljs-comment"># BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS</span>
<span class="hljs-attr">env</span> = environment.Environment(<span class="hljs-attr">optimal_temperature</span> = (<span class="hljs-number">18.0</span>, <span class="hljs-number">24.0</span>), <span class="hljs-attr">initial_month</span> = <span class="hljs-number">0</span>, <span class="hljs-attr">initial_number_users</span> = <span class="hljs-number">20</span>, <span class="hljs-attr">initial_rate_data</span> = <span class="hljs-number">30</span>)

<span class="hljs-comment"># LOADING A PRE-TRAINED BRAIN</span>
<span class="hljs-attr">model</span> = load_model(<span class="hljs-string">"model.h5"</span>)

<span class="hljs-comment"># CHOOSING THE MODE</span>
<span class="hljs-attr">train</span> = False

<span class="hljs-comment"># RUNNING A 1 YEAR SIMULATION IN INFERENCE MODE</span>
env.<span class="hljs-attr">train</span> = train
current_state, _, <span class="hljs-attr">_</span> = env.observe()
for timestep <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">12</span> * <span class="hljs-number">30</span> * <span class="hljs-number">24</span> * <span class="hljs-number">60</span>):
    <span class="hljs-attr">q_values</span> = model.predict(current_state)
    <span class="hljs-attr">action</span> = np.argmax(q_values[<span class="hljs-number">0</span>])
    <span class="hljs-keyword">if</span> (action - direction_boundary &lt; <span class="hljs-number">0</span>):
        <span class="hljs-attr">direction</span> = -<span class="hljs-number">1</span>
    <span class="hljs-keyword">else</span>:
        <span class="hljs-attr">direction</span> = <span class="hljs-number">1</span>
    <span class="hljs-attr">energy_ai</span> = abs(action - direction_boundary) * temperature_step
    next_state, reward, <span class="hljs-attr">game_over</span> = env.update_env(direction, energy_ai, int(timestep / (<span class="hljs-number">30</span> * <span class="hljs-number">24</span> * <span class="hljs-number">60</span>)))
    <span class="hljs-attr">current_state</span> = next_state

<span class="hljs-comment"># PRINTING THE TRAINING RESULTS FOR EACH EPOCH</span>
print(<span class="hljs-string">"\n"</span>)
print(<span class="hljs-string">"Total Energy spent with an AI: {:.0f}"</span>.format(env.total_energy_ai))
print(<span class="hljs-string">"Total Energy spent with no AI: {:.0f}"</span>.format(env.total_energy_noai))
print(<span class="hljs-string">"ENERGY SAVED: {:.0f} %"</span>.format((env.total_energy_noai - env.total_energy_ai) / env.total_energy_noai * <span class="hljs-number">100</span>))
</code></pre>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<p class="normal">Everything's<a id="_idIndexMarker522"/> more or less the same as before; we just removed the parts related to the traini<a id="_idTextAnchor272"/>ng.</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_197" lang="en-GB"><a id="_idTextAnchor273"/>The demo</h2>
			<p class="normal">Given the different files we have, make sure to understand that there are four <a id="_idIndexMarker523"/>possible ways to run the program:</p>
			<ol>
				<li class="list" value="1">Without dropout and without early stopping</li>
				<li class="list">Without dropout and with early stopping</li>
				<li class="list">With dropout and without early stopping</li>
				<li class="list" value="4">With dropout and with early stopping</li>
			</ol>
			<p class="normal">Then, for each of these four combinations, the way 
to run this is the same: we first execute the training file, and then 
the testing file. In this demo section, we'll execute the 4th option, 
with both dropout and early stopping.</p>
			<p class="normal">Now how do we run this? We have two options: with or without Google Colab.</p>
			<p class="normal">I'll explain how to do it on Google Colab, and I'll
 even give you a Google Colab file where you only have to hit the play 
button. For those of you who want to execute this without Colab, on your
 favorite Python IDE, or through the terminal, let me explain how it's 
done. It's easy; you just need to download the main repository from 
GitHub, then in your Python IDE set the right working directory folder, 
which is the <code class="Code-In-Text--PACKT-">Chapter 11</code> folder, and then run the following two files in this order:</p>
			<ol>
				<li class="list" value="1"><code class="Code-In-Text--PACKT-">training_earlystopping.py</code>, inside which you should make sure to import <code class="Code-In-Text--PACKT-">brain_dropout</code> at line 9. This will execute the training, and you'll have to wait until that finishes (which will take about 10 minutes).</li>
				<li class="list"><code class="Code-In-Text--PACKT-">testing.py</code>, which will test the model on one full year of data.</li>
			</ol>
			<p class="normal">Now, back to Google Colab. First, open a<a id="_idIndexMarker524"/> new Colaboratory file, and call it <strong class="bold">Deep Q-Learning for Business</strong>. Then add all your files from the <code class="Code-In-Text--PACKT-">Chapter 11</code> folder of GitHub into this Colaboratory file, right here:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_11.png" alt=""/></figure>
			<p class="packt_figref">Figure 11: Google Colab – Step 1</p>
			<p class="normal">Unfortunately, it's not easy to add the different files manually. You can only do this by using the <code class="Code-In-Text--PACKT-">os</code> library, which<a id="_idIndexMarker525"/>
 we won't bother with. Instead, copy-paste the five Python 
implementations in five different cells of our Colaboratory file, in the
 following order:</p>
			<ol>
				<li class="list" value="1">A first cell containing the whole <code class="Code-In-Text--PACKT-">environment.py</code> implementation.</li>
				<li class="list">A second cell containing the whole <code class="Code-In-Text--PACKT-">brain_dropout.py</code> implementation.</li>
				<li class="list">A third cell containing the whole <code class="Code-In-Text--PACKT-">dqn.py</code> implementation.</li>
				<li class="list">A fourth cell containing the whole <code class="Code-In-Text--PACKT-">training_earlystopping.py</code> implementation.</li>
				<li class="list">And a last cell containing the whole <code class="Code-In-Text--PACKT-">testing.py</code> implementation.</li>
			</ol>
			<p class="normal">Here's what it looks like, after adding some snazzy titles:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_12.png" alt=""/></figure>
			<p class="packt_figref">Figure 12: Google Colab – Step 2</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_13.png" alt=""/></figure>
			<p class="packt_figref">Figure 13: Google Colab – Step 3</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_14.png" alt=""/></figure>
			<p class="packt_figref">Figure 14: Google Colab – Step 4</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_15.png" alt=""/></figure>
			<p class="packt_figref">Figure 15: Google Colab – Step 5</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_16.png" alt=""/></figure>
			<p class="packt_figref">Figure 16: Google Colab – Step 6</p>
			<p class="normal">Now before we execute each of these cells in the order one through five, we need to remove the <code class="Code-In-Text--PACKT-">import</code> commands of the Python files. The reason for this is that now that the implementations are<a id="_idIndexMarker526"/>
 in cells, they're like a single Python implementation, and we don't 
have to import the interdependent files in every single cell. First, 
remove the following three different rows in the training file:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_17.png" alt=""/></figure>
			<p class="packt_figref">Figure 17: Google Colab – Step 7</p>
			<p class="normal">After doing that, we end up with this:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_18.png" alt=""/></figure>
			<p class="packt_figref">Figure 18: Google Colab – Step 8</p>
			<p class="normal">Then, since we removed these imports, we also have to remove the three file<a id="_idIndexMarker527"/>names for the <code class="Code-In-Text--PACKT-">environment</code>, the <code class="Code-In-Text--PACKT-">brain</code>, and the <code class="Code-In-Text--PACKT-">dqn</code>, when creating the objects:</p>
			<p class="normal"><strong class="bold">First the environment</strong>:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_19.png" alt=""/></figure>
			<p class="packt_figref">Figure 19: Google Colab – Step 9</p>
			<p class="normal"><strong class="bold">Then the brain</strong>:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_20.png" alt=""/></figure>
			<p class="packt_figref">Figure 20: Google Colab – Step 10</p>
			<p class="normal"><strong class="bold">And finally the dqn</strong>:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_21.png" alt=""/></figure>
			<p class="packt_figref">Figure 21: Google Colab – Step 11</p>
			<p class="normal">Now the training file's good to go. In the testing file, we just have to remove two things, the <code class="Code-In-Text--PACKT-">environment</code> import at line 12:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_22.png" alt=""/></figure>
			<p class="packt_figref">Figure 22: Google Colab – Step 12</p>
			<p class="normal">and the <code class="Code-In-Text--PACKT-">environment.</code> at row 25:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_23.png" alt=""/></figure>
			<p class="packt_figref">Figure 23: Google Colab – Step 13</p>
			<p class="normal">That's it; now you're all set! You're ready<a id="_idIndexMarker528"/> to literally hit the play button on each of the cells from top to the bottom.</p>
			<p class="normal">First, execute the first cell. After executing it, no output is displayed. That's fine!</p>
			<p class="normal">Then execute the second cell:</p>
			<pre class="programlisting language-markup"><code class="hljs mathematica"><span class="hljs-keyword">Using</span> TensorFlow backend.
</code></pre>
			<p class="normal">After executing it, you can see the output <code class="Code-In-Text--PACKT-">Using TensorFlow backend.</code></p>
			<p class="normal">Then execute the third cell, after which no output is displayed.</p>
			<p class="normal">Now it gets a bit exciting! You're about to execute
 the training, and follow the training performance in real time. Do this
 by executing the fourth cell. After executing it, the training 
launches, and you should see the following results:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_24.png" alt=""/></figure>
			<p class="packt_figref">Figure 24: The output</p>
			<p class="normal">Don't worry <a id="_idIndexMarker529"/>about 
those warnings, everything's running the way it should. Since early 
stopping is activated, you'll reach the end of the training way before 
the 100 epochs, at the 15th epoch:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_25.png" alt=""/></figure>
			<p class="packt_figref">Figure 25: The output at the 15th epoch</p>
			<p class="normal">Note that the pre-trained weights are saved in <strong class="screen-text">Files</strong>, in the <code class="Code-In-Text--PACKT-">model.h5</code> file:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_11_26.png" alt=""/></figure>
			<p class="packt_figref">Figure 26: The model.h5 file</p>
			<p class="normal">The training<a id="_idIndexMarker530"/> results 
look promising. Most of the time the AI spends less energy than the 
alternative server's integrated cooling system. Check that this is still
 the case with a full test, on one new year of simulation.</p>
			<p class="normal">Execute the final cell and when it finishes 
running, (which takes approximately 3 minutes), you obtain in the 
printed results that the total energy consumption saved by the AI is…</p>
			<pre class="programlisting language-markup"><code class="hljs yaml"><span class="hljs-attr">Total Energy spent with an AI:</span> <span class="hljs-number">261985</span>
<span class="hljs-attr">Total Energy spent with no AI:</span> <span class="hljs-number">1978293</span>
<span class="hljs-attr">ENERGY SAVED:</span> <span class="hljs-number">87</span><span class="hljs-string">%</span>
</code></pre>
			
			
			<p class="normal"><strong class="bold">Total Energy saved by the AI = 87%</strong></p>
			<p class="normal">That's a lot of energy saved! Google DeepMind 
achieved similarly impressive results in 2016. If you look up the 
results by searching "DeepMind reduces Google cooling bill," you'll see 
that the result they achieved was 40%. Not bad! Of course, let's be 
critical: their server/ data center <a id="_idIndexMarker531"/>environment
 is much more complex than our server environment and has many more 
parameters, so even though they have one of the most talented AI teams 
in the world, they could only reduce the cooling bill by less than 50%.</p>
			<p class="normal">Our environment's very simple, and if you dig into 
it (which I recommend you do) you'll likely find that the variations of 
users and data, and therefore the variation of temperature, follow a 
uniform distribution. Accordingly, the server's temperature usually 
stays around the optimal range of temperatures. The AI understands that 
well, and thus chooses most of the time to take no action and cause no 
change of temperature, thus consuming very little energy. </p>
			<p class="normal">I highly recommend that you play around with your 
server cooling model; make it as complex as you like, and try out 
different rewards to see if you can cause different behaviors.</p>
			<p class="normal">Even though our environment is simple, you can be 
proud of your achievement. What matters is that you were able to build a
 deep Q-learning model for a real-world business problem. The 
environment itself is less important; what's most important is that you 
know how to connect a deep reinforcement learning model to an 
environment, and how to train the model inside.</p>
			<p class="normal">Now, after your successes with the self-driving car plus this business application, you know how to do just that!</p>
			<p class="normal">What we've built is excellent for our business 
client, as our AI will seriously reduce their costs. Remember that 
thanks to our object-oriented structure (working with classes and 
objects), we could very easily take the objects created in this 
implementation for one server, and then plug them into other servers, so
 that in the end we end up lowering the total energy consumption of a 
whole data center! That's how Google saved billions of dollars in 
energy-related costs, thanks to the <code class="Code-In-Text--PACKT-">DQN</code> model built by their DeepMind AI.</p>
			<p class="normal">My heartiest congratulations to you for smashing this new application. You've just made huge progress with your AI skills.</p>
			<p class="normal">Finally, here's the link to<a id="_idIndexMarker532"/>
 the Colaboratory file with this whole implementation as promised. You 
don't have to install anything, Keras and NumPy are already 
pre-installed (this is the beauty of Google Colab!):</p>
			<p class="normal"><a href="https://colab.research.google.com/drive/1KGAoT7S60OC3UGHNnrr_FuN5Hcil0cHk"><span class="url">https://colab.research.google.com/drive/1KGAoT7S60OC3UGHNnrr_FuN5Hcil0cHk</span></a></p>
			<p class="normal">Before we finish this chapter and move onto the 
world of deep convolutional Q-learning, let me give you a useful recap 
of the whole general AI blueprint when building a deep reinforcement 
learning model.</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_198" lang="en-GB">Re<a id="_idTextAnchor274"/>cap – The general AI framework/Blueprint</h2>
			<p class="normal">Let's recap the whole AI Blueprint, so that you can print it <a id="_idIndexMarker533"/>out and put it on your wall.</p>
			<p class="normal"><strong class="bold">Step 1: Building the environment</strong></p>
			<ul>
				<li class="list"><strong class="bold">Step 1-1</strong>: Introducing and initializing all the parameters and variables of the environment.</li>
				<li class="list"><strong class="bold">Step 1-2</strong>: Making a method that updates the environment right after the AI plays an action.</li>
				<li class="list"><strong class="bold">Step 1-3</strong>: Making a method that resets the environment.</li>
				<li class="list"><strong class="bold">Step 1-4</strong>: Making a method that gives us at any time the current state, the last reward obtained, and whether the game is over.</li>
			</ul>
			<p class="normal"><strong class="bold">Step 2: Building the brain</strong></p>
			<ul>
				<li class="list"><strong class="bold">Step 2-1</strong>: Building the input layer composed <a id="_idIndexMarker534"/>of the input states.</li>
				<li class="list"><strong class="bold">Step 2-2</strong>: Building 
the hidden layers with a chosen number of these layers and neurons 
inside each, fully connected to the input layer and between each other.</li>
				<li class="list"><strong class="bold">Step 2-3</strong>: Building the output layer, fully connected to the last hidden layer.</li>
				<li class="list"><strong class="bold">Step 2-4</strong>: Assembling the full architecture inside a model object.</li>
				<li class="list"><strong class="bold">Step 2-5</strong>: Compiling the model with a mean squared error loss function and a chosen optimizer (a good one is Adam).</li>
			</ul>
			<p class="normal"><strong class="bold">Step 3: Implementing the deep reinforcement learning algorithm</strong></p>
			<ul>
				<li class="list"><strong class="bold">Step 3-1</strong>: Introducing and initializing all the parameters and variables of the <code class="Code-In-Text--PACKT-">DQN</code> model.</li>
				<li class="list"><strong class="bold">Step 3-2</strong>: Making a method that builds the memory in experience replay.</li>
				<li class="list"><strong class="bold">Step 3-3</strong>: Making a method that builds and returns two batches of 10 inputs and 10 targets.</li>
			</ul>
			<p class="normal"><strong class="bold">Step 4: Training the AI</strong></p>
			<ul>
				<li class="list"><strong class="bold">Step 4-1</strong>: Building the environment by creating an object <a id="_idIndexMarker535"/>of the <code class="Code-In-Text--PACKT-">Environment</code> class built in Step 1.</li>
				<li class="list"><strong class="bold">Step 4-2</strong>: Building the artificial brain by creating an object of the <code class="Code-In-Text--PACKT-">Brain</code> class built in Step 2.</li>
				<li class="list"><strong class="bold">Step 4-3</strong>: Building the <code class="Code-In-Text--PACKT-">DQN</code> model by creating an object of the <code class="Code-In-Text--PACKT-">DQN</code> class built in Step 3.</li>
				<li class="list"><strong class="bold">Step 4-4</strong>: Choosing the training mode.</li>
				<li class="list"><strong class="bold">Step 4-5</strong>: Starting the training with a <code class="Code-In-Text--PACKT-">for</code> loop over a chosen number of epochs.</li>
				<li class="list"><strong class="bold">Step 4-6</strong>: During each epoch we repeat the whole deep Q-learning process, while also doing some exploration 30% of the time.</li>
			</ul>
			<p class="normal"><strong class="bold">Step 5: Testing the AI</strong></p>
			<ul>
				<li class="list"><strong class="bold">Step 5-1</strong>: Building a new environment by<a id="_idIndexMarker536"/> creating an object of the <code class="Code-In-Text--PACKT-">Environment</code> class built in Step 1.</li>
				<li class="list"><strong class="bold">Step 5-2</strong>: Loading the artificial brain with its pre-trained weights from the previous training.</li>
				<li class="list"><strong class="bold">Step 5-3</strong>: Choosing the inference mode.</li>
				<li class="list"><strong class="bold">Step 5-4</strong>: Starting the simulation.</li>
				<li class="list"><strong class="bold">Step 5-5</strong>: At each 
iteration (each minute), our AI only plays the action that results from 
its prediction, and no exploration or deep Q-learning training 
is happening whatsoever.</li>
			</ul>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_199" lang="en-GB">Summar<a id="_idTextAnchor275"/>y</h2>
			<p class="normal">In this chapter you re-applied deep Q-learning to a
 new business problem. You were supposed to find the best strategy to 
cool down and heat up the server. Before you started defining the AI 
strategy, you had to make some assumptions about your environment, for 
example the way the temperature is calculated. As inputs to your ANN, 
you had information about the server at any given time, like the 
temperature and data transmission. As outputs, your AI predicted whether
 to cool down or heat up our server by a certain amount. The reward was 
the energy saved with respect to the other, traditional cooling system. 
Your AI was able to save 87% energy.</p>
</body></html>