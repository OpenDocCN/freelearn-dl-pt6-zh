["```py\n# IMDB data\nimdb <- dataset_imdb(num_words = 500)\nc(c(train_x, train_y), c(test_x, test_y)) %<-% imdb\nlength(train_x); length(test_x)\n[1] 25000\n[1] 25000\n\ntable(train_y)\ntrain_y\n    0     1 \n12500 12500 \n\ntable(test_y)\ntest_y\n    0     1 \n12500 12500\n```", "```py\n# Sequence of integers\ntrain_x[[3]]\n  [1]   1  14  47   8  30  31   7   4 249 108   7   4   2  54  61 369\n [17]  13  71 149  14  22 112   4   2 311  12  16   2  33  75  43   2\n [33] 296   4  86 320  35   2  19 263   2   2   4   2  33  89  78  12\n [49]  66  16   4 360   7   4  58 316 334  11   4   2  43   2   2   8\n [65] 257  85   2  42   2   2  83  68   2  15  36 165   2 278  36  69\n [81]   2   2   8 106  14   2   2  18   6  22  12 215  28   2  40   6\n [97]  87 326  23   2  21  23  22  12 272  40  57  31  11   4  22  47\n[113]   6   2  51   9 170  23   2 116   2   2  13 191  79   2  89   2\n[129]  14   9   8 106   2   2  35   2   6 227   7 129 113\n\ntrain_x[[6]]\n [1]   1   2 128  74  12   2 163  15   4   2   2   2   2  32  85 156  45\n[18]  40 148 139 121   2   2  10  10   2 173   4   2   2  16   2   8   4\n[35] 226  65  12  43 127  24   2  10  10\n\nfor (i in 1:6) print(length(train_x[[i]]))\n\nOutput\n\n[1] 218\n[1] 189\n[1] 141\n[1] 550\n[1] 147\n[1] 43\n```", "```py\n# Padding and truncation\ntrain_x <- pad_sequences(train_x, maxlen = 100)\ntest_x <- pad_sequences(test_x, maxlen = 100)\n```", "```py\n# Sequence of integers\ntrain_x[3,]\n  [1]   2   4   2  33  89  78  12  66  16   4 360   7   4  58 316 334\n [17]  11   4   2  43   2   2   8 257  85   2  42   2   2  83  68   2\n [33]  15  36 165   2 278  36  69   2   2   8 106  14   2   2  18   6\n [49]  22  12 215  28   2  40   6  87 326  23   2  21  23  22  12 272\n [65]  40  57  31  11   4  22  47   6   2  51   9 170  23   2 116   2\n [81]   2  13 191  79   2  89   2  14   9   8 106   2   2  35   2   6\n [97] 227   7 129 113\n\ntrain_x[6,]\n  [1]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [17]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [33]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n [49]   0   0   0   0   0   0   0   0   0   1   2 128  74  12   2 163\n [65]  15   4   2   2   2   2  32  85 156  45  40 148 139 121   2   2\n [81]  10  10   2 173   4   2   2  16   2   8   4 226  65  12  43 127\n [97]  24   2  10  10\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential() \nmodel %>% \n         layer_embedding(input_dim = 500, output_dim = 32) %>%\n         layer_simple_rnn(units = 8) %>%  \n         layer_dense(units = 1, activation = \"sigmoid\")\n```", "```py\n# Model summary\nmodel\n\nOUTPUT\n\nModel\n________________________________________________________________________\nLayer (type)                    Output Shape                 Param # \n========================================================================\nembedding_21 (Embedding)        (None, None, 32)             16000 \n________________________________________________________________________\nsimple_rnn_23 (SimpleRNN)       (None, 8)                    328 \n________________________________________________________________________\ndense_24 (Dense)                (None, 1)                    9 \n========================================================================\nTotal params: 16,337\nTrainable params: 16,337\nNon-trainable params: 0\n________________________________________________________________________\n```", "```py\n# Compile model\nmodel %>% compile(optimizer = \"rmsprop\",\n         loss = \"binary_crossentropy\",\n         metrics = c(\"acc\"))\n```", "```py\n# Fit model\nmodel_one <- model %>% fit(train_x, train_y,\n         epochs = 10,\n         batch_size = 128,\n         validation_split = 0.2)\n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(train_x, train_y)\n$loss\n[1] 0.4057531\n\n$acc\n[1] 0.8206\n```", "```py\n# Prediction and confusion matrix\npred <- model %>% predict_classes(train_x)\ntable(Predicted=pred, Actual=imdb$train$y)\n Actual\nPredicted 0 1\n 0 9778 1762\n 1 2722 10738\n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(test_x, test_y)\n$loss\n[1] 0.4669374\n\n$acc\n[1] 0.77852\n```", "```py\n# Prediction and confusion matrix\npred1 <- model %>%   predict_classes(test_x)\ntable(Predicted=pred1, Actual=imdb$test$y)\n         Actual\nPredicted     0     1\n        0  9134  2171\n        1  3366 10329\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential() \nmodel %>% \n         layer_embedding(input_dim = 500, output_dim = 32) %>%\n         layer_simple_rnn(units = 32) %>% \n         layer_dense(units = 1, activation = \"sigmoid\")\n\n# Compile model\nmodel %>% compile(optimizer = \"rmsprop\",\n         loss = \"binary_crossentropy\",\n         metrics = c(\"acc\"))\n\n# Fit model\nmodel_two <- model %>% fit(train_x, train_y,\n         epochs = 10,\n         batch_size = 128,\n         validation_split = 0.2)\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential() \nmodel %>% \n         layer_embedding(input_dim = 500, output_dim = 32) %>%\n         layer_simple_rnn(units = 32, activation = \"relu\") %>% \n layer_dense(units = 1, activation = \"sigmoid\")\n\n# Compile model\nmodel %>% compile(optimizer = \"rmsprop\",\n loss = \"binary_crossentropy\",\n metrics = c(\"acc\"))\n\n# Fit model\nmodel_three <- model %>% fit(train_x, train_y,\n epochs = 10,\n batch_size = 128,\n validation_split = 0.2)\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential() %>% \n         layer_embedding(input_dim = 500, output_dim = 32) %>% \n         layer_simple_rnn(units = 32, \n                          return_sequences = TRUE, \n                          activation = 'relu') %>% \n         layer_simple_rnn(units = 32, \n                          return_sequences = TRUE, \n                          activation = 'relu') %>% \n         layer_simple_rnn(units = 32, \n                          activation = 'relu') %>%\n         layer_dense(units = 1, activation = \"sigmoid\")\n\n# Compile model\nmodel %>% compile(optimizer = \"rmsprop\",\n loss = \"binary_crossentropy\",\n metrics = c(\"acc\"))\n\n# Fit model\nmodel_four <- model %>% fit(train_x, train_y,\n0 epochs = 10,\n batch_size = 128,\n validation_split = 0.2)\n```", "```py\n# Summary of padding sequences\nz <- NULL\nfor (i in 1:25000) {z[i] <- print(length(train_x[[i]]))}\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   11.0   130.0   178.0   238.7   291.0  2494.0 \n\nz <- NULL\nfor (i in 1:25000) {z[i] <- print(length(test_x[[i]]))}\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    7.0   128.0   174.0   230.8   280.0  2315.0\n```", "```py\n# IMDB data\nc(c(train_x, train_y), c(test_x, test_y)) %<-% imdb\ntrain_x <- pad_sequences(train_x, maxlen = 200)  \ntest_x <- pad_sequences(test_x, maxlen = 200)\n\n# Model architecture\nmodel <- keras_model_sequential() %>% \n         layer_embedding(input_dim = 500, output_dim = 32) %>% \n         layer_simple_rnn(units = 32, \n                          return_sequences = TRUE, \n                          activation = 'relu') %>% \n         layer_simple_rnn(units = 32, \n                          return_sequences = TRUE, \n                          activation = 'relu') %>% \n         layer_simple_rnn(units = 32, \n                          return_sequences = TRUE, \n                          activation = 'relu') %>% \n         layer_simple_rnn(units = 32, \n                          activation = 'relu') %>%\n         layer_dense(units = 1, activation = \"sigmoid\")\n\n# Compile model\nmodel %>% compile(optimizer = \"rmsprop\",\n         loss = \"binary_crossentropy\",\n         metrics = c(\"acc\"))\n\n# Fit model\nmodel_five <- model %>% fit(train_x, train_y,\n         epochs = 10,\n         batch_size = 128,\n         validation_split = 0.2)\n```", "```py\n# Prediction and confusion matrix\npred1 <- model %>%   predict_classes(test_x)\ntable(Predicted=pred1, Actual=imdb$test$y)\n         Actual\nPredicted     0     1\n        0 10066  1819\n        1  2434 10681\n```"]