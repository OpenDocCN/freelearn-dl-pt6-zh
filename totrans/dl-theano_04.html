<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;4.&#xA0;Generating Text with a Recurrent Neural Net" id="1GKCM1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04" class="calibre1"/>Chapter 4. Generating Text with a Recurrent Neural Net</h1></div></div></div><p class="calibre8">In the previous chapter, you learned how to represent a discrete input into a vector so that neural nets have the power to understand discrete inputs as well as continuous ones.</p><p class="calibre8">Many real-world applications involve variable-length inputs, such as connected objects and automation (sort of Kalman filters, much more evolved); natural language processing (understanding, translation, text generation, and image annotation); human behavior reproduction (text handwriting generation and chat bots); and reinforcement learning.</p><p class="calibre8">Previous networks, named feedforward networks, are able to classify inputs of fixed dimensions only. To extend their power to variable-length inputs, a new category of networks <a id="id191" class="calibre1"/>has been designed: the <span class="strong"><strong class="calibre2">recurrent neural networks</strong></span> (<span class="strong"><strong class="calibre2">RNN</strong></span>) that are well suited for machine learning tasks on variable-length inputs or sequences.</p><p class="calibre8">Three well-known recurrent neural nets (simple RNN, GRU, and LSTM) are presented for the example of text generation. The topics covered in this chapter are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The case of sequences</li><li class="listitem">The mechanism of recurrent networks</li><li class="listitem">How to build a simple recurrent network</li><li class="listitem">Backpropagation through time</li><li class="listitem">Different types of RNN, LSTM, and GRU</li><li class="listitem">Perplexity and word error rate</li><li class="listitem">Training on text data for text generation</li><li class="listitem">Applications of recurrent networks</li></ul></div></div>

<div class="book" title="Chapter&#xA0;4.&#xA0;Generating Text with a Recurrent Neural Net" id="1GKCM1-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="Need for RNN"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch04lvl1sec43" class="calibre1"/>Need for RNN</h1></div></div></div><p class="calibre8">Deep <a id="id192" class="calibre1"/>learning networks for natural language is numerical and deals well with multidimensional arrays of floats and integers, as input values. For categorical values, such characters or words, the previous chapter demonstrated a technique known as embedding for transforming them into numerical values as well.</p><p class="calibre8">So far, all inputs have been fixed-sized arrays. In many applications, such as texts in natural language processing, inputs have one semantic meaning but can be represented by sequences of variable length.</p><p class="calibre8">There is a need to deal with variable-length sequences as shown in the following diagram:</p><div class="mediaobject"><img src="../images/00065.jpeg" alt="Need for RNN" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">
<span class="strong"><strong class="calibre2">Recurrent Neural Networks</strong></span> (<span class="strong"><strong class="calibre2">RNN</strong></span>) are the answer to variable-length inputs.</p><p class="calibre8">Recurrence <a id="id193" class="calibre1"/>can be seen as applying a feedforward network more than once at different time steps, with different incoming input data, but with a major difference, the presence of connections to the past, previous time steps, and in one goal, to refine the representation of input through time.</p><p class="calibre8">At each time step, the hidden layer output values represent an intermediate state of the network.</p><p class="calibre8">Recurrent connections define the transition for moving from one state to another, given an input, in order to refine the representation:</p><div class="mediaobject"><img src="../images/00066.jpeg" alt="Need for RNN" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Recurrent <a id="id194" class="calibre1"/>neural networks are suited for challenges involving sequences, such as texts, sounds and speech, hand writing, and time series.</p></div></div>
<div class="book" title="A dataset for natural language"><div class="book" id="1HIT82-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec44" class="calibre1"/>A dataset for natural language</h1></div></div></div><p class="calibre8">As a <a id="id195" class="calibre1"/>dataset, any text corpus can be used, such as Wikipedia, web articles, or even with symbols such as code or computer programs, theater plays, and poems; the model will catch and reproduce the different patterns in the data.</p><p class="calibre8">In this case, let's use tiny Shakespeare texts to predict new Shakespeare texts or at least, new texts written in a style inspired by Shakespeare; two levels of predictions are possible, but can be handled in the same way:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">At the character level</strong></span>: Characters <a id="id196" class="calibre1"/>belong to an alphabet that includes punctuation, and given the first few characters, the model predicts the next characters from an alphabet, including spaces to build words and sentences. There is no constraint for the predicted word to belong to a dictionary and the objective of training is to build words and sentences close to real ones.</li><li class="listitem"><span class="strong"><strong class="calibre2">At the word level</strong></span>: Words <a id="id197" class="calibre1"/>belong to a dictionary that includes punctuation, and given the first few words, the model predicts the next word out of a vocabulary. In this case, there is a strong constraint on the words since they belong to a dictionary, but not on sentences. We expect the model to focus more on capturing the syntax and meaning of the sentences than on the character level.</li></ul></div><p class="calibre8">In both modes, token designates character/word; dictionary, alphabet, or vocabulary designates (the list of possible values for the token);</p><p class="calibre8">The popular NLTK library, a Python module, is used to split texts into sentences and tokenize into words:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">conda</strong></span> install nltk</pre></div><p class="calibre8">In a <a id="id198" class="calibre1"/>Python shell, run the following command to download the English tokenizer in the <code class="email">book</code> package:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">import</strong></span> nltk
nltk.download("book")</pre></div><p class="calibre8">Let's parse the text to extract words:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> load <span class="strong"><strong class="calibre2">import</strong></span> parse_text
X_train, y_train, index_to_word <span class="strong"><strong class="calibre2">=</strong></span> parse_text("data/tiny-shakespear.txt", type="word")

<span class="strong"><strong class="calibre2">for</strong></span> i <span class="strong"><strong class="calibre2">in</strong></span> range(10):
  <span class="strong"><strong class="calibre2">print</strong></span> "x", " ".join([index_to_word[x] <span class="strong"><strong class="calibre2">for</strong></span> x <span class="strong"><strong class="calibre2">in</strong></span> X_train[i]])
  <span class="strong"><strong class="calibre2">print</strong></span> "y"," ".join([index_to_word[x] <span class="strong"><strong class="calibre2">for</strong></span> x <span class="strong"><strong class="calibre2">in</strong></span> y_train[i]])

<span class="strong"><em class="calibre12">Vocabulary size 9000</em></span>
<span class="strong"><em class="calibre12">Found 12349 unique words tokens.</em></span>
<span class="strong"><em class="calibre12">The least frequent word in our vocabulary is 'a-fire' and appeared 1 times.</em></span>
<span class="strong"><em class="calibre12">x START first citizen : before we proceed any further , hear me speak .</em></span>
<span class="strong"><em class="calibre12">y first citizen : before we proceed any further , hear me speak . END</em></span>
<span class="strong"><em class="calibre12">x START all : speak , speak .</em></span>
<span class="strong"><em class="calibre12">y all : speak , speak . END</em></span>
<span class="strong"><em class="calibre12">x START first citizen : you are all resolved rather to die than to famish ?</em></span>
<span class="strong"><em class="calibre12">y first citizen : you are all resolved rather to die than to famish ? END</em></span>
<span class="strong"><em class="calibre12">x START all : resolved .</em></span>
<span class="strong"><em class="calibre12">y all : resolved . END</em></span>
<span class="strong"><em class="calibre12">x START resolved .</em></span>
<span class="strong"><em class="calibre12">y resolved . END</em></span>
<span class="strong"><em class="calibre12">x START first citizen : first , you know caius marcius is chief enemy to the people .</em></span>
<span class="strong"><em class="calibre12">y first citizen : first , you know caius marcius is chief enemy to the people . END</em></span>
<span class="strong"><em class="calibre12">x START all : we know't , we know't .</em></span>
<span class="strong"><em class="calibre12">y all : we know't , we know't . END</em></span>
<span class="strong"><em class="calibre12">x START first citizen : let us kill him , and we 'll have corn at our own price .</em></span>
<span class="strong"><em class="calibre12">y first citizen : let us kill him , and we 'll have corn at our own price . END</em></span>
<span class="strong"><em class="calibre12">x START is't a verdict ?</em></span>
<span class="strong"><em class="calibre12">y is't a verdict ? END</em></span>
<span class="strong"><em class="calibre12">x START all : no more talking o n't ; let it be done : away , away !</em></span>
<span class="strong"><em class="calibre12">y all : no more talking o n't ; let it be done : away , away ! END</em></span>
</pre></div><p class="calibre8">Or <a id="id199" class="calibre1"/>the <code class="email">char</code> library:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> load <span class="strong"><strong class="calibre2">import</strong></span> parse_text
X_train, y_train, index_to_char <span class="strong"><strong class="calibre2">=</strong></span> parse_text("data/tiny-shakespear.txt", <span class="strong"><strong class="calibre2">type</strong></span>="char")

<span class="strong"><strong class="calibre2">for</strong></span> i <span class="strong"><strong class="calibre2">in</strong></span> range(10):
  <span class="strong"><strong class="calibre2">print</strong></span> "x",''.join([index_to_char[x] <span class="strong"><strong class="calibre2">for</strong></span> x <span class="strong"><strong class="calibre2">in</strong></span> X_train[i]])
  <span class="strong"><strong class="calibre2">print</strong></span> "y",''.join([index_to_char[x] <span class="strong"><strong class="calibre2">for</strong></span> x <span class="strong"><strong class="calibre2">in</strong></span> y_train[i]])

<span class="strong"><em class="calibre12">x ^first citizen: before we proceed any further, hear me speak</em></span>
<span class="strong"><em class="calibre12">y irst citizen: before we proceed any further, hear me speak.$</em></span>
<span class="strong"><em class="calibre12">x ^all: speak, speak</em></span>
<span class="strong"><em class="calibre12">y ll: speak, speak.$</em></span>
<span class="strong"><em class="calibre12">x ^first citizen: you are all resolved rather to die than to famish</em></span>
<span class="strong"><em class="calibre12">y irst citizen: you are all resolved rather to die than to famish?$</em></span>
<span class="strong"><em class="calibre12">x ^all: resolved</em></span>
<span class="strong"><em class="calibre12">y ll: resolved.$</em></span>
<span class="strong"><em class="calibre12">x ^resolved</em></span>
<span class="strong"><em class="calibre12">y esolved.$</em></span>
<span class="strong"><em class="calibre12">x ^first citizen: first, you know caius marcius is chief enemy to the people</em></span>
<span class="strong"><em class="calibre12">y irst citizen: first, you know caius marcius is chief enemy to the people.$</em></span>
<span class="strong"><em class="calibre12">x ^all: we know't, we know't</em></span>
<span class="strong"><em class="calibre12">y ll: we know't, we know't.$</em></span>
<span class="strong"><em class="calibre12">x ^first citizen: let us kill him, and we'll have corn at our own price</em></span>
<span class="strong"><em class="calibre12">y irst citizen: let us kill him, and we'll have corn at our own price.$</em></span>
<span class="strong"><em class="calibre12">x ^is't a verdict</em></span>
<span class="strong"><em class="calibre12">y s't a verdict?$</em></span>
<span class="strong"><em class="calibre12">x ^all: no more talking on't; let it be done: away, away</em></span>
<span class="strong"><em class="calibre12">y ll: no more talking on't; let it be done: away, away!$</em></span>
</pre></div><p class="calibre8">The additional start token (the <code class="email">START</code> word and the <code class="email">^</code> character) avoids having a void hidden state when the prediction starts. Another solution is to initialize the first hidden state with <span class="strong"><img src="../images/00067.jpeg" alt="A dataset for natural language" class="calibre23"/></span>.</p><p class="calibre8">The <a id="id200" class="calibre1"/>additional end token (the <code class="email">END</code> word and the <code class="email">$</code> character) helps the network learn to predict a stop when the sequence generation is predicted to be finished.</p><p class="calibre8">Last, the <code class="email">out of vocabulary</code> token (the <code class="email">UNKNOWN</code> word) replaces words that do not belong to the vocabulary to avoid big dictionaries.</p><p class="calibre8">In this example, we'll omit the validation dataset, but for any real-world application, keeping a part of the data for validation is a good practice.</p><p class="calibre8">Also, note that functions from <a class="calibre1" title="Chapter 2. Classifying Handwritten Digits with a Feedforward Network" href="part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 2</a>, <span class="strong"><em class="calibre12">Classifying Handwritten Digits with a Feedforward Network</em></span> for layer initialization <code class="email">shared_zeros</code> and <code class="email">shared_glorot_uniform</code> and from <a class="calibre1" title="Chapter 3. Encoding Word into Vector" href="part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 3</a>, <span class="strong"><em class="calibre12">Encoding Word into Vector</em></span> for model saving and loading <code class="email">save_params</code> and <code class="email">load_params</code> have been packaged into the <code class="email">utils</code> package:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> theano <span class="strong"><strong class="calibre2">import</strong></span> *
<span class="strong"><strong class="calibre2">import</strong></span> theano.tensor <span class="strong"><strong class="calibre2">as</strong></span> T
<span class="strong"><strong class="calibre2">from</strong></span> utils <span class="strong"><strong class="calibre2">import</strong></span> shared_zeros, shared_glorot_uniform,save_params,load_params</pre></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Simple recurrent network"><div class="book" id="1IHDQ2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec45" class="calibre1"/>Simple recurrent network</h1></div></div></div><p class="calibre8">An RNN <a id="id201" class="calibre1"/>is a network applied at multiple time steps but with a major difference: a connection to the previous state of layers at previous time steps named hidden states <span class="strong"><img src="../images/00068.jpeg" alt="Simple recurrent network" class="calibre23"/></span>:</p><div class="mediaobject"><img src="../images/00069.jpeg" alt="Simple recurrent network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">This can <a id="id202" class="calibre1"/>be written in the following form:</p><div class="mediaobject"><img src="../images/00070.jpeg" alt="Simple recurrent network" class="calibre9"/></div><p class="calibre10"> </p><div class="mediaobject"><img src="../images/00071.jpeg" alt="Simple recurrent network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">An RNN can be unrolled as a feedforward network applied on the sequence <span class="strong"><img src="../images/00072.jpeg" alt="Simple recurrent network" class="calibre23"/></span> as input and with shared parameters between different time steps.</p><p class="calibre8">Input and <a id="id203" class="calibre1"/>output's first dimension is time, while next dimensions are for the data dimension inside each step. As seen in the previous chapter, the value at a time step (a word or a character) can be represented either by an index (an integer, 0-dimensional) or a one-hot-encoding vector (1-dimensional). The former representation is more compact in memory. In this case, input and output sequences will be 1-dimensional represented by a vector, with one dimension, the time:</p><div class="informalexample"><pre class="programlisting">x <span class="strong"><strong class="calibre2">=</strong></span> T.ivector()
y <span class="strong"><strong class="calibre2">=</strong></span> T.ivector()</pre></div><p class="calibre8">The structure of the training program remains the same as in <a class="calibre1" title="Chapter 2. Classifying Handwritten Digits with a Feedforward Network" href="part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 2</a>, <span class="strong"><em class="calibre12">Classifying Handwritten Digits with a Feedforward Network</em></span> with feedforward network, except the model that we'll define with a recurrent module shares the same weights at different time steps:</p><div class="informalexample"><pre class="programlisting">embedding_size = len(index_)
n_hidden=500</pre></div><p class="calibre8">Let's define the hidden and input weights:</p><div class="informalexample"><pre class="programlisting">U <span class="strong"><strong class="calibre2">=</strong></span> shared_glorot_uniform(( embedding_size,n_hidden), <span class="strong"><strong class="calibre2">name</strong></span>="U")
W <span class="strong"><strong class="calibre2">=</strong></span> shared_glorot_uniform((n_hidden, n_hidden), <span class="strong"><strong class="calibre2">name</strong></span>="W")
bh <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros((n_hidden,), <span class="strong"><strong class="calibre2">name</strong></span>="bh")</pre></div><p class="calibre8">And the output weights:</p><div class="informalexample"><pre class="programlisting">V = shared_glorot_uniform(( n_hidden, embedding_size), <span class="strong"><strong class="calibre2">name</strong></span>="V")
by = shared_zeros((embedding_size,), <span class="strong"><strong class="calibre2">name</strong></span>="by")

params = [U,V,W,by,bh]

<span class="strong"><strong class="calibre2">def</strong></span> step(x_t, h_tm1):
    h_t = T.tanh(U[x_t] + T.dot( h_tm1, W) + bh)
    y_t = T.dot(h_t, V) + by
    <span class="strong"><strong class="calibre2">return</strong></span> h_t, y_t</pre></div><p class="calibre8">Initial state can be set to zero while using the start tokens:</p><div class="informalexample"><pre class="programlisting">h0 <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros((n_hidden,), <span class="strong"><strong class="calibre2">name</strong></span>='h0')
[h, y_pred], _ = theano.scan(step, <span class="strong"><strong class="calibre2">sequences</strong></span>=x, <span class="strong"><strong class="calibre2">outputs_info</strong></span>=[h0, <span class="strong"><strong class="calibre2">None</strong></span>], <span class="strong"><strong class="calibre2">truncate_gradient</strong></span>=10)</pre></div><p class="calibre8">It returns two tensors, where the first dimension is time and the second dimension is data values (0-dimensional in this case).</p><p class="calibre8">Gradient <a id="id204" class="calibre1"/>computation through the scan function is automatic in Theano and follows both direct and recurrent connections to the previous time step. Therefore, due to the recurrent connections, the error at a particular time step is propagated <a id="id205" class="calibre1"/>to the previous time step, a mechanism named <span class="strong"><strong class="calibre2">Backpropagation Through Time</strong></span> (<span class="strong"><strong class="calibre2">BPTT</strong></span>).</p><p class="calibre8">It has been observed that the gradients either explode or vanish after too many time steps. This is why the gradients are truncated after 10 steps in this example, and errors will not be backpropagated to further past time steps.</p><p class="calibre8">For the remaining steps, we keep the classification as before:</p><div class="informalexample"><pre class="programlisting">model <span class="strong"><strong class="calibre2">=</strong></span> T.nnet.softmax(y_pred)
y_out <span class="strong"><strong class="calibre2">=</strong></span> T.argmax(model, <span class="strong"><strong class="calibre2">axis</strong></span>=-1)
cost <span class="strong"><strong class="calibre2">=</strong></span> -T.mean(T.log(model)[T.arange(y.shape[0]), y])</pre></div><p class="calibre8">This returns a vector of values at each time step.</p></div>

<div class="book" title="Simple recurrent network">
<div class="book" title="LSTM network"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec15" class="calibre1"/>LSTM network</h2></div></div></div><p class="calibre8">One of <a id="id206" class="calibre1"/>the main difficulties with RNN is to capture long-term dependencies due to the vanishing/exploding gradient effect and truncated backpropagation.</p><p class="calibre8">To overcome <a id="id207" class="calibre1"/>this issue, researchers have been looking at a long list of potential solutions. A new kind of recurrent network was designed in 1997 with a memory unit, named a cell state, specialized in keeping and transmitting long-term information.</p><p class="calibre8">At each time step, the cell value can be updated partially with a candidate cell and partially erased thanks to a gate mechanism. Two gates, the update gate and the forget gate, decide how to update the cell, given the previously hidden state value and current input value:</p><div class="mediaobject"><img src="../images/00073.jpeg" alt="LSTM network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The candidate cell is computed in the same way:</p><div class="mediaobject"><img src="../images/00074.jpeg" alt="LSTM network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The new cell state is computed as follows:</p><div class="mediaobject"><img src="../images/00075.jpeg" alt="LSTM network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">For the new hidden state, an output gate decides what information in the cell value to output:</p><div class="mediaobject"><img src="../images/00076.jpeg" alt="LSTM network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The <a id="id208" class="calibre1"/>remaining stays equal with the simple RNN:</p><div class="mediaobject"><img src="../images/00077.jpeg" alt="LSTM network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">This <a id="id209" class="calibre1"/>mechanism allows the network to store some information to use a lot further in future than it was possible with a simple RNN.</p><p class="calibre8">Many variants of design for LSTM have been designed and it is up to you to test these variants on your problems to see how they behave.</p><p class="calibre8">In this example, we'll use a variant, where gates and candidates use both the previously hidden state and previous cell state.</p><p class="calibre8">In Theano, let's define the weights for:</p><p class="calibre8">- the input gate:</p><div class="informalexample"><pre class="programlisting">W_xi = shared_glorot_uniform(( embedding_size,n_hidden))
W_hi = shared_glorot_uniform(( n_hidden,n_hidden))
W_ci = shared_glorot_uniform(( n_hidden,n_hidden))
b_i = shared_zeros((n_hidden,))</pre></div><p class="calibre8">- the forget gate:</p><div class="informalexample"><pre class="programlisting">W_xf = shared_glorot_uniform(( embedding_size, n_hidden))
W_hf = shared_glorot_uniform(( n_hidden,n_hidden))
W_cf = shared_glorot_uniform(( n_hidden,n_hidden))
b_f = shared_zeros((n_hidden,))</pre></div><p class="calibre8">- the output gate:</p><div class="informalexample"><pre class="programlisting">W_xo = shared_glorot_uniform(( embedding_size, n_hidden))
W_ho = shared_glorot_uniform(( n_hidden,n_hidden))
W_co = shared_glorot_uniform(( n_hidden,n_hidden))
b_o = shared_zeros((n_hidden,))</pre></div><p class="calibre8">- the cell:</p><div class="informalexample"><pre class="programlisting">W_xc = shared_glorot_uniform(( embedding_size, n_hidden))
W_hc = shared_glorot_uniform(( n_hidden,n_hidden))
b_c = shared_zeros((n_hidden,))</pre></div><p class="calibre8">- the output layer:</p><div class="informalexample"><pre class="programlisting">W_y = shared_glorot_uniform(( n_hidden, embedding_size), <span class="strong"><strong class="calibre2">name</strong></span>="V")
b_y = shared_zeros((embedding_size,), <span class="strong"><strong class="calibre2">name</strong></span>="by")</pre></div><p class="calibre8">The array of all trainable parameters:</p><div class="informalexample"><pre class="programlisting">params <span class="strong"><strong class="calibre2">=</strong></span> [W_xi,W_hi,W_ci,b_i,W_xf,W_hf,W_cf,b_f,W_xo,W_ho,W_co,b_o,W_xc,W_hc,b_c,W_y,b_y]</pre></div><p class="calibre8">The step function to be placed inside the recurrent loop :</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> step(x_t, h_tm1, c_tm1):
    i_t = T.nnet.sigmoid(W_xi[x_t] + T.dot(W_hi, h_tm1) + T.dot(W_ci, c_tm1) + b_i)
    f_t = T.nnet.sigmoid(W_xf[x_t] + T.dot(W_hf, h_tm1) + T.dot(W_cf, c_tm1) + b_f)
    c_t = f_t <span class="strong"><strong class="calibre2">*</strong></span> c_tm1 + i_t <span class="strong"><strong class="calibre2">*</strong></span> T.tanh(W_xc[x_t] + T.dot(W_hc, h_tm1) + b_c)
    o_t = T.nnet.sigmoid(W_xo[x_t] + T.dot(W_ho, h_tm1) + T.dot(W_co, c_t) + b_o)
    h_t = o_t <span class="strong"><strong class="calibre2">*</strong></span> T.tanh(c_t)
    y_t = T.dot(h_t, W_y) + b_y
    <span class="strong"><strong class="calibre2">return</strong></span> h_t, c_t, y_t</pre></div><p class="calibre8">Let's create the recurrent loop with the scan operator :</p><div class="informalexample"><pre class="programlisting">h0 <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros((n_hidden,), name='h0')
c0 <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros((n_hidden,), name='c0')
[h, c, y_pred], _ <span class="strong"><strong class="calibre2">=</strong></span> theano.scan(step, <span class="strong"><strong class="calibre2">sequences</strong></span>=x,<span class="strong"><strong class="calibre2"> outputs_info</strong></span>=[h0, c0, None], <span class="strong"><strong class="calibre2">truncate_gradient</strong></span>=10)</pre></div></div></div>

<div class="book" title="Simple recurrent network">
<div class="book" title="Gated recurrent network"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec16" class="calibre1"/>Gated recurrent network</h2></div></div></div><p class="calibre8">The GRU is <a id="id210" class="calibre1"/>an alternative to LSTM, simplifying the mechanism without the use of an extra cell:</p><div class="mediaobject"><img src="../images/00078.jpeg" alt="Gated recurrent network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The code <a id="id211" class="calibre1"/>to build a gated recurrent network consists simply of defining the weights and the <code class="email">step</code> function, as before:</p><p class="calibre8">- Weights for the Update gate:</p><div class="informalexample"><pre class="programlisting">W_xz <span class="strong"><strong class="calibre2">=</strong></span> shared_glorot_uniform(( embedding_size,n_hidden))
W_hz <span class="strong"><strong class="calibre2">=</strong></span> shared_glorot_uniform(( n_hidden,n_hidden))
b_z <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros((n_hidden,))</pre></div><p class="calibre8">- Weights for the Reset gate:</p><div class="informalexample"><pre class="programlisting">W_xr <span class="strong"><strong class="calibre2">=</strong></span> shared_glorot_uniform(( embedding_size,n_hidden))
W_hr <span class="strong"><strong class="calibre2">=</strong></span> shared_glorot_uniform(( n_hidden,n_hidden))
b_r <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros((n_hidden,))</pre></div><p class="calibre8">- Weight for the Hidden layer:</p><div class="informalexample"><pre class="programlisting">W_xh <span class="strong"><strong class="calibre2">=</strong></span> shared_glorot_uniform(( embedding_size,n_hidden))
W_hh <span class="strong"><strong class="calibre2">=</strong></span> shared_glorot_uniform(( n_hidden,n_hidden))
b_h <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros((n_hidden,))</pre></div><p class="calibre8">- Weight for the Output layer:</p><div class="informalexample"><pre class="programlisting">W_y <span class="strong"><strong class="calibre2">=</strong></span> shared_glorot_uniform(( n_hidden, embedding_size),<span class="strong"><strong class="calibre2"> name</strong></span>="V")
b_y <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros((embedding_size,),<span class="strong"><strong class="calibre2"> name</strong></span>="by")</pre></div><p class="calibre8">The trainable parameters:</p><div class="informalexample"><pre class="programlisting">params <span class="strong"><strong class="calibre2">=</strong></span> [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_y, b_y]</pre></div><p class="calibre8">The step function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> step(x_t, h_tm1):
    z_t = T.nnet.sigmoid(W_xz[x_t] + T.dot(W_hz, h_tm1) + b_z)
    r_t = T.nnet.sigmoid(W_xr[x_t] + T.dot(W_hr, h_tm1) + b_r)
    can_h_t = T.tanh(W_xh[x_t] + r_t <span class="strong"><strong class="calibre2">*</strong></span> T.dot(W_hh, h_tm1) + b_h)
    h_t = (1 - z_t) <span class="strong"><strong class="calibre2">*</strong></span> h_tm1 + z_t <span class="strong"><strong class="calibre2">*</strong></span> can_h_t
    y_t = T.dot(h_t, W_y) + b_y
    <span class="strong"><strong class="calibre2">return</strong></span> h_t, y_t</pre></div><p class="calibre8">The recurrent loop:</p><div class="informalexample"><pre class="programlisting">h0 <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros((n_hidden,), <span class="strong"><strong class="calibre2">name</strong></span>='h0')
[h, y_pred], _ <span class="strong"><strong class="calibre2">= </strong></span>theano.scan(step, <span class="strong"><strong class="calibre2">sequences</strong></span>=x, <span class="strong"><strong class="calibre2">outputs_info</strong></span>=[h0, None], <span class="strong"><strong class="calibre2">truncate_gradient</strong></span>=10)</pre></div><p class="calibre8">Having <a id="id212" class="calibre1"/>introduced the major nets, we'll <a id="id213" class="calibre1"/>see how they perform on the text generation task.</p></div></div>
<div class="book" title="Metrics for natural language performance" id="1JFUC1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec46" class="calibre1"/>Metrics for natural language performance</h1></div></div></div><p class="calibre8">The <span class="strong"><strong class="calibre2">Word Error Rate</strong></span> (<span class="strong"><strong class="calibre2">WER</strong></span>) or <span class="strong"><strong class="calibre2">Character Error Rate</strong></span> (<span class="strong"><strong class="calibre2">CER</strong></span>) is equivalent to the designation <a id="id214" class="calibre1"/>of the accuracy error for the case of natural language.</p><p class="calibre8">Evaluation <a id="id215" class="calibre1"/>of language models is usually expressed with <a id="id216" class="calibre1"/>perplexity, which is simply:</p><div class="mediaobject"><img src="../images/00079.jpeg" alt="Metrics for natural language performance" class="calibre9"/></div><p class="calibre10"> </p></div>
<div class="book" title="Training loss comparison" id="1KEEU1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec47" class="calibre1"/>Training loss comparison</h1></div></div></div><p class="calibre8">During <a id="id217" class="calibre1"/>training, the learning rate might be strong after a certain number of epochs for fine-tuning. Decreasing the learning rate when the loss does not decrease anymore will help during the last steps of training. To decrease the learning rate, we need to define it as an input variable during compilation:</p><div class="informalexample"><pre class="programlisting">lr <span class="strong"><strong class="calibre2">=</strong></span> T.scalar(<span class="strong"><strong class="calibre2">'learning_rate'</strong></span>)
train_model <span class="strong"><strong class="calibre2">=</strong></span> theano.function(<span class="strong"><strong class="calibre2">inputs</strong></span>=[x,y,lr], <span class="strong"><strong class="calibre2">outputs</strong></span>=cost,<span class="strong"><strong class="calibre2">updates</strong></span>=updates)</pre></div><p class="calibre8">During training, we adjust the learning rate, decreasing it if the training loss is not better:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">if</strong></span> (len(train_loss) <span class="strong"><strong class="calibre2">&gt;</strong></span> 1 <span class="strong"><strong class="calibre2">and</strong></span> train_loss[-1] <span class="strong"><strong class="calibre2">&gt;</strong></span> train_loss[-2]):
    learning_rate <span class="strong"><strong class="calibre2">=</strong></span> learning_rate <span class="strong"><strong class="calibre2">*</strong></span> 0.5</pre></div><p class="calibre8">As a <a id="id218" class="calibre1"/>first experiment, let's see the impact of the size of the hidden layer on the training loss for a simple RNN:</p><div class="mediaobject"><img src="../images/00080.jpeg" alt="Training loss comparison" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">More hidden units improve training speed and might be better in the end. To check this, we should run it for more epochs.</p><p class="calibre8">Comparing the training of the different network types, in this case, we do not observe any improvement with LSTM and GRU:</p><div class="mediaobject"><img src="../images/00081.jpeg" alt="Training loss comparison" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">This <a id="id219" class="calibre1"/>might be due to the <code class="email">truncate_gradient</code> option or because the problem is too simple and not so memory-dependent.</p><p class="calibre8">Another parameter to tune is the minimum number of occurrences for a word to be a part of the dictionary. A higher number will learn on words that are more frequent, which is better.</p></div>
<div class="book" title="Example of predictions" id="1LCVG1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec48" class="calibre1"/>Example of predictions</h1></div></div></div><p class="calibre8">Let's predict <a id="id220" class="calibre1"/>a sentence with the generated model:</p><div class="informalexample"><pre class="programlisting">sentence = [0]
<span class="strong"><strong class="calibre2">while </strong></span>sentence[-1] <span class="strong"><strong class="calibre2">!=</strong></span> 1:
    pred = predict_model(sentence)[-1]
    sentence.append(pred)
<span class="strong"><strong class="calibre2">print</strong></span>(" ".join([ index_[w] for w in sentence[1:-1]]))</pre></div><p class="calibre8">Note that we take the most probable next word (argmax), while we must, in order to get some randomness, draw the next word following the predicted probabilities.</p><p class="calibre8">At 150 epochs, while the model has still not converged entirely with learning our Shakespeare writings, we can play with the predictions, initiating it with a few words, and <a id="id221" class="calibre1"/>see the network generate the end of the sentences:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">First citizen</strong></span>: A word , i know what a word</li><li class="listitem"><span class="strong"><strong class="calibre2">How</strong></span> now!</li><li class="listitem"><span class="strong"><strong class="calibre2">Do</strong></span> you not this asleep , i say upon this?</li><li class="listitem"><span class="strong"><strong class="calibre2">Sicinius</strong></span>: What, art thou my master?</li><li class="listitem"><span class="strong"><strong class="calibre2">Well,</strong></span> sir, come.</li><li class="listitem"><span class="strong"><strong class="calibre2">I have been</strong></span> myself</li><li class="listitem"><span class="strong"><strong class="calibre2">A most</strong></span> hose, you in thy hour, sir</li><li class="listitem"><span class="strong"><strong class="calibre2">He shall</strong></span> not this</li><li class="listitem"><span class="strong"><strong class="calibre2">Pray you</strong></span>, sir</li><li class="listitem"><span class="strong"><strong class="calibre2">Come</strong></span>, come, you</li><li class="listitem"><span class="strong"><strong class="calibre2">The crows</strong></span>?</li><li class="listitem"><span class="strong"><strong class="calibre2">I'll give</strong></span> you</li><li class="listitem"><span class="strong"><strong class="calibre2">What</strong></span>, ho!</li><li class="listitem"><span class="strong"><strong class="calibre2">Consider you</strong></span>, sir</li><li class="listitem"><span class="strong"><strong class="calibre2">No more</strong></span>!</li><li class="listitem"><span class="strong"><strong class="calibre2">Let us</strong></span> be gone, or your UNKNOWN UNKNOWN, i do me to do</li><li class="listitem"><span class="strong"><strong class="calibre2">We are</strong></span> not now</li></ul></div><p class="calibre8">From these examples, we notice that the model has learned to position punctuation correctly, adding a point, comma, question mark, or an exclamation mark at the right place to order direct objects, indirect objects, and adjectives correctly.</p><p class="calibre8">The original texts are composed of short sentences in a Shakespeare style. Bigger articles such as Wikipedia pages, as well as pushing the training further with a validation split to control overfitting will produce longer texts. <a class="calibre1" title="Chapter 10. Predicting Times Sequences with Advanced RNN" href="part0096_split_000.html#2RHM01-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 10</a>, <span class="strong"><em class="calibre12">Predicting Times Sequence with Advanced RNN</em></span>: will teach how to predict time sequences with Advanced RNN and present an advanced version of this chapter.</p></div>
<div class="book" title="Applications of RNN" id="1MBG21-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec49" class="calibre1"/>Applications of RNN</h1></div></div></div><p class="calibre8">This <a id="id222" class="calibre1"/>chapter introduced the simple RNN, LSTM, and GRU models. Such models have a wide range of applications in sequence generation or sequence understanding:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Text generation, such as automatic generation of Obama political speech (obama-rnn), for example with a text seed on jobs:<div class="blockquote"><blockquote class="blockquote1"><p class="calibre28">Good afternoon. God bless you. The United States will step up to the cost of a new challenges of the American people that will share the fact that we created the problem. They were attacked and so that they have to say that all the task of the final days of war that I will not be able to get this done. The promise of the men and women who were still going to take out the fact that the American people have fought to make sure that they have to be able to protect our part. It was a chance to stand together to completely look for the commitment to borrow from the American people. And the fact is the men and women in uniform and the millions of our country with the law system that we should be a strong stretcks of the forces that we can afford to increase our spirit of the American people and the leadership of our country who are on the Internet of American lives. Thank you very much. God bless you, and God bless the United States of America.</p></blockquote></div><p class="calibre24">You can check this example out in detail at <a class="calibre1" href="https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0#.4nee5wafe.">https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0#.4nee5wafe.</a>
</p></li><li class="listitem">Text <a id="id223" class="calibre1"/>annotation, for example, the <span class="strong"><strong class="calibre2">Part of Speech</strong></span> (<span class="strong"><strong class="calibre2">POS</strong></span>) tags: noun, verb, particle, adverb, and adjective.<p class="calibre24">Generating human handwriting: <a class="calibre1" href="http://www.cs.toronto.edu/~graves/handwriting.html">http://www.cs.toronto.edu/~graves/handwriting.html</a>
</p><div class="mediaobject"><img src="../images/00082.jpeg" alt="Applications of RNN" class="calibre9"/></div><p class="calibre27"> </p></li><li class="listitem">Drawing with Sketch-RNN (<a class="calibre1" href="https://github.com/hardmaru/sketch-rnn">https://github.com/hardmaru/sketch-rnn</a>)<div class="mediaobject"><img src="../images/00083.jpeg" alt="Applications of RNN" class="calibre9"/></div><p class="calibre27"> </p></li><li class="listitem"><span class="strong"><strong class="calibre2">Speech synthesis</strong></span>: A recurrent <a id="id224" class="calibre1"/>network will generate parameters for generating each phoneme in a speech or voice speaking. In the following image, time-frequency homogeneous blocs are classified in phonemes (or graphemes or letters):<div class="mediaobject"><img src="../images/00084.jpeg" alt="Applications of RNN" class="calibre9"/></div><p class="calibre27"> </p></li><li class="listitem">Music generation:<div class="book"><ul class="itemizedlist1"><li class="listitem">Melody generation at <a class="calibre1" href="https://github.com/tensorflow/magenta/tree/master/magenta/models/melody_rnn">https://github.com/tensorflow/magenta/tree/master/magenta/models/melody_rnn</a>.</li><li class="listitem">Mozart style music generation with Mozart-RNN at <a class="calibre1" href="http://www.hexahedria.com/2015/08/03/composing-music-withrecurrent-neural-networks/">http://www.hexahedria.com/2015/08/03/composing-music-withrecurrent-neural-networks/</a>.</li></ul></div><p class="calibre24">
</p></li><li class="listitem">Any <a id="id225" class="calibre1"/>classification of sequences, such as sentiment analysis (positive, negative, or neutral sentiments) that we'll address in <a class="calibre1" title="Chapter 5. Analyzing Sentiment with a Bidirectional LSTM" href="part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 5</a>, <span class="strong"><em class="calibre12">Analyzing</em></span><span class="strong"><em class="calibre12"> Sentiment with a Bidirectional LSTM</em></span>.</li><li class="listitem">Sequence encoding or decoding that we'll address in <a class="calibre1" title="Chapter 6. Locating with Spatial Transformer Networks" href="part0069_split_000.html#21PMQ2-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 6</a>, <span class="strong"><em class="calibre12">Locating with Spatial Transformer Networks</em></span>.</li></ul></div></div>
<div class="book" title="Related articles" id="1NA0K1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec50" class="calibre1"/>Related articles</h1></div></div></div><p class="calibre8">You can refer to the following links for more insight:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre12">The Unreasonable Effectiveness of Recurrent Neural Networks</em></span>, Andrej Karpathy May 21, 2015 (<a class="calibre1" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a>)</li><li class="listitem"><span class="strong"><em class="calibre12">Understanding LSTM Networks</em></span> on Christopher Colah's blog's, 2015 (<a class="calibre1" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>)</li><li class="listitem">Use of LSTM for audio classification: <span class="strong"><em class="calibre12">Connectionist Temporal Classification and Deep Speech: Scaling up end-to-end speech recognition</em></span> (<a class="calibre1" href="https://arxiv.org/abs/1412.5567">https://arxiv.org/abs/1412.5567</a>)</li><li class="listitem">Handwriting demo at <a class="calibre1" href="http://www.cs.toronto.edu/~graves/handwriting.html">http://www.cs.toronto.edu/~graves/handwriting.html</a></li><li class="listitem"><span class="strong"><em class="calibre12">General Sequence Learning using Recurrent Neural Networks</em></span> tutorial at <a class="calibre1" href="https://www.youtube.com/watch?v=VINCQghQRuM">https://www.youtube.com/watch?v=VINCQghQRuM</a></li><li class="listitem">On the difficulty of training Recurrent Neural Networks Razvan Pascanu, Tomas Mikolov, Yoshua Bengio 2012</li><li class="listitem">Recurrent Neural Networks Tutorial:<div class="book"><ul class="itemizedlist1"><li class="listitem">Introduction to RNNS</li><li class="listitem">Implementing RNN with Python, NumPy, and Theano</li><li class="listitem">Backpropagation through time and vanishing gradients</li><li class="listitem">Implementing a GRU/LSTM RNN with Python and Theano Denny Britz 2015 at <a class="calibre1" href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/</a></li></ul></div></li><li class="listitem">LONG SHORT-TERM MEMORY, Sepp Hochreiter, Jürgen Schmidhuber, 1997</li></ul></div></div>
<div class="book" title="Summary" id="1O8H61-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec51" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">Recurrent Neural Networks provides the ability to process variable-length inputs and outputs of discrete or continuous data.</p><p class="calibre8">While the previous feedforward networks were able to process only one input to one output (one-to-one scheme), recurrent neural nets introduced in this chapter offered the possibility to make conversions between variable-length and fixed-length representations adding new operating schemes for deep learning input/output: one-to-many, many-to-many, or many-to-one.</p><p class="calibre8">The range of applications of RNN is wide. For this reason, we'll study them more in depth in the further chapters, in particular how to enhance the predictive power of these three modules or how to combine them to build multi-modal, question-answering, or translation applications.</p><p class="calibre8">In particular, in the next chapter, we'll see a practical example using text embedding and recurrent networks for sentiment analysis. This time, there will also be an opportunity to review these recurrence units under another library Keras, a deep learning library that simplifies writing models for Theano.</p></div></body></html>