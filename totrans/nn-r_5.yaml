- en: Training and Visualizing a Neural Network in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen in [Chapters 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, and [Chapter 2](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4),
    *Learning Process in Neural Networks*, training a neural network model forms the
    basis for building a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Feed-forward and backpropagation are the techniques used to determine the weights
    and biases of the model. The weights can never be zero but the biases can be zero.
    To start with, the weights are initialized a random number, and by gradient descent,
    the errors are minimized; we get a set of best possible weights and biases for
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is trained using any of the R functions, we can pass on the
    independent variables to predict the target or unknown variable. In this chapter,
    we will use a publicly available dataset to train, test, and visualize a neural
    network model. The following items will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Training, testing, and evaluating a dataset using NN model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the NN model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early stopping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalization of NN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling of NN parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the chapter, we will understand how to train, test, and evaluate
    a dataset using NN model. We will learn how to visualize the NN model in R environment.
    We will cover the concepts like early stopping, avoiding overfitting, generalization
    of NN, and scaling of NN parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Data fitting with neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data fitting is the process of building a curve or a mathematical function
    that has the best match with a set of previously collected points. The curve fitting
    can relate to both interpolations, where exact data points are required, and smoothing,
    where a flat function is built that approximates the data. The approximate curves
    obtained from the data fitting can be used to help display data, to predict the
    values of a function where no data is available, and to summarize the relationship
    between two or more variables. In the following figure is shown a linear interpolation
    of collected data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Data fitting is the process of training a neural network on a set of inputs
    in order to produce an associated set of target outputs. Once the neural network
    has fit the data, it forms a generalization of the input-output relationship and
    can be used to generate outputs for inputs it was not trained on.
  prefs: []
  type: TYPE_NORMAL
- en: The fuel consumption of vehicles has always been studied by the major manufacturers
    of the entire planet. In an era characterized by oil refueling problems and even
    greater air pollution problems, fuel consumption by vehicles has become a key
    factor. In this example, we will build a neural network with the purpose of predicting
    the fuel consumption of the vehicles according to certain characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, use the `Auto` dataset contained in the `ISLR` package that we
    have already used in an example in [Chapter 3](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4),
    *Deep Learning Using Multilayer Neural Networks*. The `Auto` dataset contain gas
    mileage, horsepower, and other information for 392 vehicles. It is a data frame
    with 392 observations on the following nine variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mpg`: Miles per gallon'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cylinders`: Number of cylinders between 4 and 8'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`displacement`: Engine displacement (cubic inches)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`horsepower`: Engine horsepower'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight`: Vehicle weight (lbs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`acceleration`: Time to accelerate from 0 to 60 mph (sec)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`year`: Model year (modulo 100)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`origin`: Origin of car (American, European, Japanese)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: Vehicle name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the code that we will use in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As usual, we will analyze the code line-by-line, by explaining in detail all
    the features applied to capture the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first two lines of the initial code are used to load the libraries needed
    to run the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them. This function should be
    used only once and not every time you run the code.
  prefs: []
  type: TYPE_NORMAL
- en: The `neuralnet` library is used to train neural networks using backpropagation,
    **resilient backpropagation** (**RPROP**) with or without weight backtracking,
    or the modified **globally convergent version** (**GRPROP**). The function allows
    flexible settings through custom-choice of error and activation function. Furthermore,
    the calculation of generalized weights is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: The `ISLR` library contains a set of datasets freely usable for our examples.
    This is a series of data collected during major studies conducted by research
    centers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This command loads the `Auto` dataset, which, as we anticipated, is contained
    in the `ISLR` library, and saves it in a given dataframe. Use the `View` function
    to view a compact display of the structure of an arbitrary R object. The following
    screenshot shows some of the data contained in the `Auto` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00098.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the database consists of 392 rows and 9 columns. The rows represent
    392 commercial vehicles from 1970 to 1982\. The columns represent the 9 characteristics
    collected for each car, in order: `mpg`, `cylinders`, `displacement`, `horsepower`,
    `weight`, `acceleration`, `year`, `origin`, and `name`.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting with data analysis through the building and training of a neural
    network, we conduct an exploratory analysis to understand how data is distributed
    and extract preliminary knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can begin our explorative analysis by tracing a plot of predictors versus
    target. We recall in this respect that in our analysis, the predictors are the
    following variables: `cylinders`, `displacement`, `horsepower`, `weight`, `acceleration`,
    `year`, `origin`, and `name`. The target is the `mpg` variable that contains measurements
    of the miles per gallon of 392 sample cars.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to examine the weight and mileage of cars from three different
    origins, as shown in the next graph, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To plot the chart, we used the `plot()` function, specifying what to point
    on the *x* axis (`weight`), what to point on the *y* axis (`mpg`), and finally,
    based on which variable to group the data (`origin`), as shown in the following
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Remember the number in the `origin` column correspond at the following zone:
    1= America, 2=Europe, and 3=Japan). From the analysis of the previous graph, we
    can find that fuel consumption increases with weight gain. Let''s remember that
    the target measures the miles per gallon, so how many miles are going with a gallon
    of fuel. It follows that the greater the value of mpg (miles per gallon), the
    lower the fuel consumption.'
  prefs: []
  type: TYPE_NORMAL
- en: Another consideration that comes from plot analysis is that cars produced in
    America are heavier. In fact, in the right part of the chart (which corresponds
    to higher values of weight), there are only cars produced in that area.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if we focus our analysis on the left of the graph, in the upper part
    that corresponds to the lowest fuel consumption, we find in most cases Japanese
    and European cars. In conclusion, we can note that cars that have the lowest fuel
    consumption are Japanese.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see the other graphs, that is, what we get if we plot the remaining
    numeric predictors (`cylinders`, `displacement`, `horsepower`, and `acceleration`)
    versus target (`mpg`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For space reasons, we decided to place the four charts in one. R makes it easy
    to combine multiple plots into one general graph, using the `par()` function.
    Using the par( ) function, we can include the option mfrow=c(nrows, ncols) to
    create a matrix of nrows x ncols plots that are filled in by row. For example
    the option mfrow=c(3,2) creates a matrix plot with 3 rows and 2 columns. In addition,
    the option mfcol=c(nrows, ncols) fills in the matrix by columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure are shown 4 plot arranged in a matrix of 2 rows and
    two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00100.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From the analysis of the previous figure, we find confirmation of what has already
    been mentioned earlier. We can note that cars with higher horsepower have higher
    fuel consumption. The same thing we can say about the engine displacement; also
    in this case, vehicles with higher displacement have higher fuel consumption.
    Again, cars with higher horsepower and displacement values are produced in America.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, cars with higher acceleration values have lower fuel consumption.
    This fact is due to the lesser weight that such cars have. Usually, heavy cars
    are slower in acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4), *Learning
    Process in Neural Networks*, we scaled the data before building the network. On
    that occasion, we pointed out that it is good practice to normalize the data before
    training a neural network. With normalization, data units are eliminated, allowing
    you to easily compare data from different locations.
  prefs: []
  type: TYPE_NORMAL
- en: It is not always necessary to normalize numeric data. However, it has been shown
    that when numeric values are normalized, neural network formation is often more
    efficient and leads to better prediction. In fact, if numeric data are not normalized
    and the sizes of two predictors are very distant, a change in the value of a neural
    network weight has much more relative influence on higher value.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several standardization techniques; in [Chapter 2](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4),
    *Learning Process in Neural Networks*, we adopted min-max standardization. In
    this case, we will adopt *Z*-scores normalization. This technique consists of
    subtracting the mean of the column to each value in a column, and then dividing
    the result for the standard deviation of the column. The formula to achieve this
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In summary, the *Z* score (also called standard score) represents the number
    of standard deviations with which the value of an observation point or data is
    greater than the mean value of what is observed or measured. Values above the
    mean have positive *Z*-scores, while values below the mean have negative *Z*-scores.
    The *Z*-score is a quantity without dimension, obtained by subtracting the population
    mean from a single rough score and then dividing the difference for the standard
    deviation of the population.
  prefs: []
  type: TYPE_NORMAL
- en: Before applying the method chosen for normalization, you must calculate the
    mean and standard deviation values of each database column. To do this, we use
    the `apply` function. This function returns a vector or an array or a list of
    values obtained by applying a function to margins of an array or matrix. Let's
    understand the meaning of the arguments used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line allows us to calculate the mean of each variable going to the
    second line, allowing us to calculate the standard deviation of each variable.
    Let''s see how we used the function `apply()`. The first argument of the `apply`
    function specifies the dataset to apply the function to, in our case, the dataset
    named data. In particular, we have only considered the first six numeric variables;
    the other ones we will use for other purposes. The second argument must contain
    a vector giving the subscripts which the function will be applied over. In our
    case, one indicates rows and two indicates columns. The third argument must contain
    the function to be applied; in our case, the `mean()` function in the first row
    and the `sd()` function in the second row. The results are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To normalize the data, we use the `scale()` function, which is a generic function
    whose default method centers and/or scales the columns of a numeric matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the data transformed by normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now split the data for the training and the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first line of the code just suggested, the dataset is split into 70:30,
    with the intention of using 70 percent of the data at our disposal to train the
    network and the remaining 30 percent to test the network. In the second and third
    lines, the data of the dataframe named data is subdivided into two new dataframes,
    called `train_data` and `test_data`. Now we have to build the function to be submitted
    to the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the first line, we recover all the variable names in the `data_scaled` dataframe,
    using the `names()` function. In the second line, we build formula that we will
    use to train the network. What does this formula represent?
  prefs: []
  type: TYPE_NORMAL
- en: 'The models fitted by the `neuralnet()` function are specified in a compact
    symbolic form. The ~ operator is basic in the formation of such models. An expression
    of the form *y* ~ model is interpreted as a specification that the response *y*
    is modelled by a predictor specified symbolically by model. Such a model consists
    of a series of terms separated by + operators. The terms themselves consist of
    variable and factor names separated by : operators. Such a term is interpreted
    as the interaction of all the variables and factors appearing in the term. Let''s
    look at the formula we set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now we can build and train the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 3](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4), *Deep
    Learning Using Multilayer Neural Networks*, we said that to choose the optimal
    number of neurons, we need to know that:'
  prefs: []
  type: TYPE_NORMAL
- en: Small number of neurons will lead to high error for your system, as the predictive
    factors might be too complex for a small number of neurons to capture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large number of neurons will overfit your training data and not generalize well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of neurons in each hidden layer should be somewhere between the size
    of the input and the output layer, potentially the mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of neurons in each hidden layer shouldn't exceed twice the number
    of input neurons, as you are probably grossly overfit at this point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, we have five input variables (`cylinders`, `displacement`, `horsepower`,
    `weight`, and `acceleration`) and one variable output (`mpg`). We choose to set
    three neurons in the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The hidden argument accepts a vector with the number of neurons for each hidden
    layer, while the argument `linear.output` is used to specify whether we want to
    do regression (`linear.output=TRUE`) or classification (`linear.output=FALSE`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm used in `neuralnet()`, by default, is based on the resilient
    backpropagation without weight backtracking and additionally modifies one learning
    rate, either the learning rate associated with the smallest absolute gradient
    (`sag`) or the smallest learning rate (`slr`) itself. The `neuralnet()` function
    returns an object of class `nn`. An object of class `nn` is a list containing
    at most the components shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Components** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `call` | The matched call. |'
  prefs: []
  type: TYPE_TB
- en: '| `response` | Extracted from the `data` argument. |'
  prefs: []
  type: TYPE_TB
- en: '| `covariate` | The variables extracted from the data argument. |'
  prefs: []
  type: TYPE_TB
- en: '| `model.list` | A list containing the covariates and the `response` variables
    extracted from the `formula` argument. |'
  prefs: []
  type: TYPE_TB
- en: '| `err.fct` | The error function. |'
  prefs: []
  type: TYPE_TB
- en: '| `act.fct` | The activation function. |'
  prefs: []
  type: TYPE_TB
- en: '| `data` | The data argument. |'
  prefs: []
  type: TYPE_TB
- en: '| `net.result` | A list containing the overall result of the neural network
    for every repetition. |'
  prefs: []
  type: TYPE_TB
- en: '| `weights` | A list containing the fitted weights of the neural network for
    every repetition. |'
  prefs: []
  type: TYPE_TB
- en: '| `generalized.weights` | A list containing the generalized weights of the
    neural network for every repetition. |'
  prefs: []
  type: TYPE_TB
- en: '| `result.matrix` | A matrix containing the reached threshold, needed steps,
    error, AIC and BIC (if computed), and weights for every repetition. Each column
    represents one repetition. |'
  prefs: []
  type: TYPE_TB
- en: '| `startweights` | A list containing the startweights of the neural network
    for every repetition. |'
  prefs: []
  type: TYPE_TB
- en: 'To produce result summaries of the results of the model, we use the `summary()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'For each component of the neural network model are displayed three features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Length**: This is component length, that is how many elements of this type
    are contained in it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Class**: This contains specific indication on the component class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mode**: This is the type of component (numeric, list, function, logical,
    and so on)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To plot the graphical representation of the model with the weights on each connection,
    we can use the `plot()` function. The `plot()` function is a generic function
    for the representation of objects in R. Generic function means that it is suitable
    for different types of objects, from variables to tables to complex function outputs,
    producing different results. Applied to a nominal variable, it will produce a
    bar graph. Applied to a cardinal variable, it will produce a scatterplot. Applied
    to the same variable, but tabulated, that is, to its frequency distribution, it
    will produce a histogram. Finally, applied to two variables, a nominal and a cardinal,
    it will produce a boxplot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The neural network plot is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00102.gif)'
  prefs: []
  type: TYPE_IMG
- en: In the previous graph, the black lines (these lines start from input nodes)
    show the connections between each layer and the weights on each connection, while
    the blue lines (these lines start from bias nodes which are distinguished by number
    1) show the bias term added in each step. The bias can be thought of as the intercept
    of a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Though over time we have understood a lot about the mechanics that are the basis
    of the neural networks, in many respects, the model we have built and trained
    remains a black box. The fitting, weights, and model are not clear enough. We
    can be satisfied that the training algorithm is convergent and then the model
    is ready to be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can print on video, the weights and biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen, these are the same values that we can read in the network plot.
    For example, `cylinders.to.1layhid1 = 0.291091600669` is the weight for the connection
    between the input cylinders and the first node of the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can use the network to make predictions. For this, we had set aside 30
    percent of the data in the `test_data` dataframe. It is time to use it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In our case, we applied the function to the `test_data` dataset, using only
    the columns from `2` to `6`, representing the input variables of the network.
    To evaluate the network performance, we can use the **Mean Squared Error** (**MSE**)
    as a measure of how far away our predictions are from the real data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here `test_data$mpg` is the actual data and `predict_net_test$net.result` is
    the predicted data for the target of the analysis. Following is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks like a good result, but what do we compare it with? To get an idea
    of the accuracy of the network prediction, we can build a linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We build a linear regression model using the `lm` function. This function is
    used to fit linear models. It can be used to perform regression, single stratum
    analysis of variance, and analysis of covariance. To produce a summary of the
    results of model fitting obtained, we have used the `summary()` function, which
    returns the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we make the prediction with the linear regression model using the data
    contained in the `test_data` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we calculate the MSE for the regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: From the comparison between the two models (neural network model versus linear
    regression model), once again the neural network wins (0.26 versus 0.31).
  prefs: []
  type: TYPE_NORMAL
- en: 'We now perform a visual comparison by drawing on a graph the actual value versus
    the predicted value, first for neural network and then for linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The comparison between the performance of the neural network model (to the
    left) and the linear regression model (to the right) on the test set is plotted
    in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00103.gif)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the predictions by the neural network are more concentrated around
    the line than those by the linear regression model, even if you do not note a
    big difference.
  prefs: []
  type: TYPE_NORMAL
- en: Classifing breast cancer with a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The breast is made up of a set of glands and adipose tissue and is placed between
    the skin and the chest wall. In fact, it is not a single gland, but a set of glandular
    structures, called lobules, joined together to form a lobe. In a breast, there
    are 15 to 20 lobes. The milk reaches the nipple from the lobules through small
    tubes called milk ducts.
  prefs: []
  type: TYPE_NORMAL
- en: Breast cancer is a potentially serious disease if it is not detected and treated
    for a long time. It is caused by uncontrolled multiplication of some cells in
    the mammary gland that are transformed into malignant cells. This means that they
    have the ability to detach themselves from the tissue that has generated them
    to invade the surrounding tissues and eventually the other organs of the body.
    In theory, cancers can be formed from all types of breast tissues, but the most
    common ones are from glandular cells or from those forming the walls of the ducts.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of this example is to identify each of a number of benign or malignant
    classes. To do this, we will use the data contained in the dataset named `BreastCancer`
    (Wisconsin Breast Cancer Database) contained in the `mlbench` package. This data
    has been taken from the UCI Repository Of Machine Learning Databases at DNA samples
    arrive periodically as Dr. Wolberg reports his clinical cases. The database therefore
    reflects this chronological grouping of the data. This grouping information appears
    immediately, having been removed from the data itself. Each variable, except for
    the first, was converted into 11 primitive numerical attributes with values ranging
    from 0 through 10\. There are 16 missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataframes contain 699 observations on 11 variables—1 being a character
    variable, 9 being ordered or nominal, and 1 target class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Id`: Sample code number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Cl.thickness`: Clump thickness'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Cell.size`: Uniformity of cell size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Cell.shape`: Uniformity of cell shape'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Marg.adhesion`: Marginal adhesion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Epith.c.size`: Single epithelial cell size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Bare.nuclei`: Bare nuclei'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Bl.cromatin`: Bland chromatin'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Normal.nucleoli`: Normal nucleoli'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Mitoses`: Mitoses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Class`: Class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As said previously, the objective of this example is to identify each of a
    number of benign or malignant classes. The following is the code that we will
    use in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We begin analyzing the code line-by-line, by explaining in detail all the features
    applied to capture the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The first two lines of the initial code are used to load the libraries needed
    to run the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them. This function should be
    used only once and not every time you run the code.
  prefs: []
  type: TYPE_NORMAL
- en: The `mlbench` library contains a collection of artificial and real-world machine
    learning benchmark problems, including, for example, several datasets from the
    UCI Repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `neuralnet` library is used to train neural networks using backpropagation,
    RPROP with or without weight backtracking, or the modified GRPROP. The function
    allows flexible settings through custom-choice of error and activation function.
    Furthermore, the calculation of generalized weights is implemented. A brief description
    of the nnet package, extracted from the official documentation, is shown in the
    following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `neuralnet`: Training of neural networks |'
  prefs: []
  type: TYPE_TB
- en: '| **Description**: |'
  prefs: []
  type: TYPE_TB
- en: '| Training of neural networks using backpropagation, resilient backpropagation
    with (Riedmiller, 1994), or without weight backtracking (Riedmiller and Braun,
    1993), or the modified globally convergent version by Anastasiadis et al. (2005).
    The package allows flexible settings through custom-choice of error and activation
    function. |'
  prefs: []
  type: TYPE_TB
- en: '| **Details**: |'
  prefs: []
  type: TYPE_TB
- en: '| Package: `neuralnet` Type: Package'
  prefs: []
  type: TYPE_NORMAL
- en: 'Version: 1.33'
  prefs: []
  type: TYPE_NORMAL
- en: 'Date: 2016-08-05'
  prefs: []
  type: TYPE_NORMAL
- en: 'License: GPL-2 |'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Author(s)**: |'
  prefs: []
  type: TYPE_TB
- en: '| Stefan Fritsch Frauke Guenther'
  prefs: []
  type: TYPE_NORMAL
- en: Marc Suling
  prefs: []
  type: TYPE_NORMAL
- en: Sebastian M. Mueller |
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to the code, at this point we have to load the data to be analyzed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: With this command, we upload the data set named `BreastCancer`, as mentioned,
    in the `mlbench` library.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting with data analysis through the build and training of a neural
    network, we conduct an exploratory analysis to understand how the data is distributed
    and extract preliminary knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: With this command, we will see a brief summary using the `summary()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the `summary()` function is a generic function used to produce result
    summaries of the results of various model fitting functions. The function invokes
    particular methods which depend on the class of the first argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the function was applied to a dataframe and the results are shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00104.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The `summary()` function returns a set of statistics for each variable. In particular,
    it is useful to highlight the result provided for the `class` variable that contains
    the diagnosis of the cancer mass. In this case, 458 cases of benign `class` and
    241 cases of `malignant` class were detected. Another feature to highlight is
    the Bare.nuclei variable. For this variable, 16 cases of missing value were detected.
  prefs: []
  type: TYPE_NORMAL
- en: A missing value is one whose value is unknown. Missing values are represented
    in R by the `NA` symbol. `NA` is a special value whose properties are different
    from other values. `NA` is one of the very few reserved words in R; you cannot
    give anything this name. `NA` can arise when you read in an Excel spreadsheet
    with empty cells, for example. You will also see `NA` when you try certain operations
    that are illegal or don't make sense. Missing values do not necessarily arise
    from an error; often in real life, there is a lack of detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'A question arises spontaneously: do we have to worry about the presence of
    missing value? Unfortunately, yes, and this is due to the fact that almost every
    operation performed on an `NA` produces an `NA`. Then the presence of missing
    values in our dataset can cause errors in the calculations we will make later.
    This is why we are forced to remove the missing values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To remove missing values, we must first identify them. The `is.na()` function
    finds missing values for us; this function returns a logical vector of the same
    length as its argument, with *T* for missing values and *F* for non-missing. It''s
    fairly common to want to know the index of the missing values, and the `which()`
    function helps do this for us. To find all the rows in a dataframe with at least
    one `NA`, try this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `lapply()` function applies the function to each column and returns a list
    whose *i*-th element is a vector containing the indices of the elements which
    have missing values in column *i*. The `unlist()` function turns that list into
    a vector and `unique()` gets rid of the duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have the number of lines where the missing value (`NA`) appears, as
    we can see next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we know there are missing values in our database and we know where they
    are. We just have to remove those lines from the original dataset. To do this,
    we can use the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`na.omit`: Drops out any rows with missing values anywhere in them and forgets
    them forever'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na.exclude`: Drops out rows with missing values, but keeps track of where
    they were, so that when you make predictions, for example, you end up with a vector
    whose length is that of the original response'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will use the first option, so as to eliminate them forever:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm the removal of the rows where the missing values appeared, apply
    the `summary()` function again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00105.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see now, there is no missing value anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's go into our exploratory analysis. The first thing we can do is to
    plot the boxplots of the variables. A first idea is already made by looking at
    the results of the `summary()` function. Naturally, we will limit ourselves to
    numeric variables only.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following graph, the boxplots of the numeric variables (from 2° to 10°)
    contained in the cleaned dataset (`data_cleaned`) are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00106.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From the analysis of the previous graph, we can note that several variables
    have outliers, with the variable `Mitoses` being the one that has the largest
    number.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier values are numerically different from the rest of the collected data.
    Statistics derived from samples containing outliers can be misleading.
  prefs: []
  type: TYPE_NORMAL
- en: To better identify the presence of outlier, we can plot histograms of the variables
    in the database. A histogram is an accurate graphical representation of the distribution
    of numerical data. It is an estimate of the probability distribution of a continuous
    variable. To construct a histogram, the first step is to specify the range of
    values (that is, divide the entire range of values into a series of intervals),
    and then count how many values fall into each interval. The bins are usually specified
    as consecutive, non-overlapping intervals of a variable. The bins must be adjacent,
    and are often of equal size. With histogram, we can see where the middle is in
    your data distribution, how close the data lies around this middle, and where
    possible outliers are to be found.
  prefs: []
  type: TYPE_NORMAL
- en: 'In R environment, we can simply make a histogram by using the `hist()` function,
    which computes a histogram of the given data values. We must put the name of the
    dataset in between the parentheses of this function. To plot many graphs in the
    same window, we will use the `par()` function, already used in the previous examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the function `hist()` requires a vector as argument, we have transformed
    the values contained in the dataset columns into numeric vectors using the `as.numeric(`)
    function. This function creates or coerces objects of type `numeric`. In the following
    graphs are shown the histograms of the numeric variables (from 2° to 10°) contained
    in the cleaned dataset (`data_cleaned`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00107.gif)'
  prefs: []
  type: TYPE_IMG
- en: From the analysis of the histograms, it is possible to note that some variables
    have outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we did in the previous example, before building and training the network,
    we have to run the standardization of data. In this case, we will adopt min-max
    standardization.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, it is good practice to normalize the data before training a neural
    network. With normalization, data units are eliminated, allowing you to easily
    compare data from different locations.
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin, make a further check by using the `str()` function. This function
    provides a compact display of the internal structure of an object, a diagnostic
    function, and an alternative to the `summary()` function. Ideally, only one line
    for each basic structure is displayed. It is especially well suited to compactly
    display the (abbreviated) contents of (possibly nested) lists. The idea is to
    give reasonable output for any R object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As it is possible to note, the variables are present as a factor. We need to
    make a transformation for our calculations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We first identified the variables of the factor type and then we transformed
    them into numeric type. We can now standardize.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we will use the min-max method (usually called feature scaling)
    to get all the scaled data in the range *[0, 1]*. The formula to achieve this
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Before applying the method chosen for normalization, you must calculate the
    minimum and maximum values of each database column. To do this, we use the `apply()`
    function. This function returns a vector or an array or a list of values obtained
    by applying a function to margins of an array or matrix. Let's understand the
    meaning of the arguments used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The first argument of the apply function specifies the dataset to apply the
    function to, in our case, the dataset named `data`. The second argument must contain
    a vector giving the subscripts which the function will be applied over. In our
    case, one indicates rows and two indicates columns. The third argument must contain
    the function to be applied; in our case, the max function. What we will do next
    is to calculate the minimums for each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to normalize the data, we use the `scale()` function, which is a generic
    function whose default method centers and/or scales the columns of a numeric matrix,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm the standardization of data, let''s see the first 20 lines of the
    new matrix we created. To do this, we will use the `View()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see now, the data is between zero and one. At this point, we reconstruct
    the dataset, adding our target (that is the `class` variable), which represents
    the diagnosis of the cancer (`benign` or `malignant`). This topic requires our
    attention: as we have seen before, this variable (`class`) is categorical. Particularly
    in the data frame is present as a factor, so that we can properly use int the
    network we must necessarily transform it. Our target is a dichotomous variable
    (only two values: `benign` and `malignant`), so it can easily be transformed into
    two dummy variables.'
  prefs: []
  type: TYPE_NORMAL
- en: A dummy variable is one that takes the value `0` or `1` to indicate the absence
    or presence of some categorical effect that may be expected to shift the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: What we will do is create two new variables (`Cancerbenign` and `Cancermalignant`),
    starting with the `Class` variable representing our target. The `Cancerbenign`
    variable will contain values of one at each occurrence of the `benign` value present
    in the `Class` variable, and values of zero in other cases. In contrast, the `Cancermalignant`
    variable will contain values of one at each occurrence of the `malignant` value
    present in the `Class` variable and values of zero in other cases.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the two new dummy variables, we used the `model.matrix()` function.
    This function creates a model matrix by expanding factors to a set of dummy variables
    (depending on the contrasts), and expanding interactions similarly. Finally, we
    add the new variables to the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The time has come to train the network.
  prefs: []
  type: TYPE_NORMAL
- en: The network training phase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The artificial neural networks are composed of simple elements operating in
    parallel. Connections between network elements are fundamental as they decide
    network functions. These connections affect the result through its weight, which
    is regulated in the neural network training phase. In the following diagram is
    shown a comparison between serial and parallel processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, in the training phase, the network is regulated by changing the connection
    weights, so that a particular input will lead to a specific destination. For example,
    the network can be adjusted by comparing the output (what we calculate practically)
    and the target (what we want to get), until the network output matches the target.
    To get sufficiently reliable results, many input/target pairs are needed to form
    a network. In the following diagram is shown a simple flow chart of the training
    phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The way these weights are adjusted is defined by the particular algorithm we
    adopt. After highlighting the importance of the algorithm in network training,
    much interest must be placed in the preparation of the data to be provided to
    the network.
  prefs: []
  type: TYPE_NORMAL
- en: In the network training, the weights and bias must be tuned to optimize the
    network performance. It represents the most important phase of the whole process,
    as the better the network is, the better the generalization will be able to operate
    with new data, unknown to it. At this stage, part of the collected data is taken
    randomly (usually 70 percent of the available cases).
  prefs: []
  type: TYPE_NORMAL
- en: 'After the neural network training, we can use the network, in that phase a
    part of the collected data taken randomly (usually 30 perecnt of the available
    cases) is passed to the network to test it. Then the neural network object can
    be saved and used as many times as you want with any new data. In the following
    figure is shown how an original dataset has been divided:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This subdivision of data in code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first line of the code just suggested, the dataset is split into 70:30,
    with the intention of using 70 percent of the data at our disposal to train the
    network and the remaining 30 percent to test the network. In the second and third
    lines, the data of the dataframe named `data` is subdivided into two new dataframes,
    called `train_data` and `test_data`. Now we have to build the function to be submitted
    to the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: In the first line, we recover the names of the first nine variables in the `data_scaled`
    dataframe, using the `names()` function. In the second line, we build formula
    that we will use to train the network. What does this formula represent?
  prefs: []
  type: TYPE_NORMAL
- en: 'The models fitted by the `neuralnet()` function are specified in a compact
    symbolic form. The ~ operator is basic in the formation of such models. An expression
    of the form *y* ~ model is interpreted as a specification that the response *y*
    is modelled by a predictor specified symbolically by model. Such a model consists
    of a series of terms separated by + operators. The terms themselves consist of
    variable and factor names separated by : operators. Such a term is interpreted
    as the interaction of all the variables and factors appearing in the term. Let''s
    look at the formula we set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have everything we need, we can create and train the network. We recall
    the advice we gave in the previous example for the correct choice of number of
    neurons in the hidden layer. We have eight input variables (`Cl.thickness`, `Cell.size`,
    `Cell.shape`, `Marg.adhesion`, `Epith.c.size`, `Bare.nuclei`, `Bl.cromatin`, `Normal.nucleoli`,
    and `Mitoses`) and one variable output (`Cancer`). Then we choose to set five
    neurons in the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The `hidden` argument accepts a vector with the number of neurons for each hidden
    layer, while the argument `linear.output` is used to specify whether we want to
    do regression (`linear.output=TRUE`) or classification (`linear.output=FALSE`
    (our case)).
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm used in `neuralnet()`, by default, is based on the resilient backpropagation
    without weight backtracking, and additionally modifies one learning rate, either
    the learning rate associated with the smallest absolute gradient (`sag`) or the
    smallest learning rate (`slr`) itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'To plot the graphical representation of the model with the weights on each
    connection, we can use the `plot()` function, already widely explained in the
    previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The neural network plot is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous graph, the black lines (these lines start from input nodes)
    show the connections between each layer and the weights on each connection, while
    the blue lines (these lines start from bias nodes which are distinguished by number
    one) show the bias term added in each step. The bias can be thought of as the
    intercept of a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We finally have the network trained and ready for use. Now, we can use it to
    make our predictions. Remember, we've set aside 30 percent of available data and
    then use them to test the network. It's time to use it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'To predict data, we have used the compute function, which computes the outputs
    of all neurons for specific arbitrary covariate vectors, given a trained neural
    network. Let''s look at the results by printing the first ten lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, these are real numbers with several decimals. In order to compare
    them with the data contained in the dataset, we have to round them to the nearest
    integer. To do this, we will use the `round()` function that rounds the values
    in its first argument to the specified number of decimal places (default zero).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We now rebuild the starting variable. We no longer need the two dummy variables;
    they have done their job well, but now we no longer need them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can build the confusion matrix to check the performance of our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion matrix is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Although in a simple way, the matrix tells us that we only made eight errors.
    For more information about the confusion matrix, we can use the `CrossTable()`
    function contained in the `gmodels` package. As always, before loading the book,
    you need to install it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion matrix obtained by using the `CrossTable()` function is shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The cells falling on the main diagonal contain counts of examples where the
    classifier correctly categorized the examples. In the top-left cell, labeled `TN`,
    are the true negative results. These 132 of 205 values indicate cases where the
    cancer was `benign`, and the algorithm correctly identified it as such. The bottom-right
    cell, labeled `TP`, indicates the true positive results, where the classifier
    and the clinically determined label agree that the mass is `malignant`. A total
    of 65 of 205 predictions were true positives.
  prefs: []
  type: TYPE_NORMAL
- en: The cells falling on the other diagonal contain counts of examples where the
    classifier incorrectly categorized the examples. The three examples in the lower-left
    `FN` cell are false negative results; in this case, the predicted value was `benign`
    but the cancer was actually `malignant`. Errors in this direction could be extremely
    costly, as they might lead a patient to believe that she is cancer-free, when
    in reality the disease may continue to spread. The cell labeled `FP` would contain
    the false positive results, if there were any. These values occur when the model
    classifies a cancer as malignant when in reality it was benign. Although such
    errors are less dangerous than a false negative result, they should also be avoided
    as they could lead to additional financial burden on the health care system, or
    additional stress for the patient, as additional tests or treatment may have to
    be provided.
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping in neural network training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The epoch is a measure of each round trip from the forward propagation training
    and backpropagation update of weights and biases. The round trip of training has
    to stop once we have convergence (minimal error terms) or after a preset number
    of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Early stopping is a technique used to deal with overfitting of the model (more
    on overfitting in the next few pages). The training set is separated into two
    parts: one of them is to be used for training, while the other one is meant for
    validation purposes. We had separated our `IRIS` dataset into two parts: one 75
    percent and another 25 percent.'
  prefs: []
  type: TYPE_NORMAL
- en: With the training data, we compute the gradient and update the network weights
    and biases. The second set of data, the testing or validation data, is used to
    validate the model overfitting. If the error during validation increases for a
    specified number of iterations (`nnet.abstol`/`reltol`), the training is stopped
    and the weights and biases at that point are used by the model. This method is
    called *early stopping.*
  prefs: []
  type: TYPE_NORMAL
- en: An early stopping neural network ensemble generalization error is comparable
    with an individual neural network of optimal architecture that is trained by a
    traditional algorithm. The individual neural network needs a complex and perfect
    tuning to attain this generalization without early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding overfitting in the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fitting of the training data causes the model to determine the weights and
    biases along with the activation function values. When the algorithm does too
    well in some training dataset, it is said to be too much aligned to that particular
    dataset. This leads to high variance in the output values when the test data is
    very different from the training data. This high estimate variance is called **overfitting**.
    The predictions are affected due to the training data provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many possible ways to handle overfitting in neural networks. The
    first is regularization, similar to regression. There are two kinds of regularizations:'
  prefs: []
  type: TYPE_NORMAL
- en: L1 or lasso regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 or ridge regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max norm constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropouts in neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization introduces a cost term to impact the activation function. It
    tries to change most of the coefficients by bringing in more features with the
    objective function. Hence, it tries to push the coefficients for many variables
    to zero and reduce the cost term.
  prefs: []
  type: TYPE_NORMAL
- en: '**Lasso or L1 regularization or L1 penalty**: This has a penalty term, which
    uses the sum of absolute weights, so that the weights are optimized to reduce
    overfitting. **Least Absolute Shrinkage And Selection Operator** (**LASSO**) introduces
    the penalty weight to shrink the network weights towards zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L2 penalty or ridge regression**: This is similar to L1, but the penalty
    is based on squared weights instead of the sum of absolute weights. Larger weights
    get more penalty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For both cases, only weights are considered for optimization, and biases (or
    offsets or intercepts) are excluded from the exercise.
  prefs: []
  type: TYPE_NORMAL
- en: '**Max norm constraints**: This is another regularization technique, whereby
    we enforce an absolute upper bound on the magnitude of the incoming weight vector
    for every neuron and the projected gradient descent cannot modify the weights
    due to the constraint. Here, the parameter vector cannot grow out of control (even
    if the learning rates are too high) because the updates to the weights are always
    bounded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout**: This is another overfitting prevention technique. While training,
    dropout is implemented by keeping a neuron active with some probability *p* (a
    hyperparameter) or setting it to zero otherwise. This means that some neurons
    may not be present during training and hence dropout. The network is unaffected
    and becomes more accurate even in the absence of certain information. This prevents
    the network from becoming too dependent on any one (or any small combination)
    of the neurons. The process of dropout is explained in the following diagram.
    The red (or dark) neurons are the ones dropped out, and the neural network model
    survives without these neurons and offers less overfitting and greater accuracy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00116.gif)'
  prefs: []
  type: TYPE_IMG
- en: Generalization of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generalization is aimed at fitting the training data. It is an extension of
    the training we have done on the neural networks model. It seeks to minimize the
    sum of squared errors of the model on the training data (such as using ordinary
    least squares) and reduce the complexity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The methods of generalization are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping of training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retraining neural networks with different training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using random sampling, stratified sampling, or any good mix of target data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Training multiple neural networks and averaging out their outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling of data in neural network models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data scaling or normalization is a process of making model data in a standard
    format so that the training is improved, accurate, and faster. The method of scaling
    data in neural networks is similar to data normalization in any machine learning
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some simple methods of data normalization are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Z-score normalization**: As anticipated in previous sections, the arithmetic
    mean and standard deviation of the given data are calculated first. The standardized
    score or *Z*-score is then calculated as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00117.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *X* is the value of the data element, μ is the mean, and σ is the standard
    deviation. The *Z*-score or standard score indicates how many standard deviations
    the data element is from the mean. Since mean and standard deviation are sensitive
    to outliers, this standardization is sensitive to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Min-max normalization**: This calculates the following for each data element:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00118.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x*[*i*] is the data element, *min(x)* is the minimum of all data values,
    and *max(x)* is the maximum of all data values. This method transforms all the
    scores into a common range of [0, 1]. However, it suffers from outlier sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: '**Median and MAD**: The median and median absolute deviation (MAD) normalization
    calculates the normalized data value using the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x[i]* represents each data value. This method is insensitive to outliers
    and the points in the extreme tails of the distribution, and therefore it is robust.
    However, this technique does not retain the input distribution and does not transform
    the scores into a common numerical range.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble predictions using neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another approach to regularization involves combining neural network models
    and averaging out the results. The resultant model is the most accurate one.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network ensemble is a set of neural network models taking a decision
    by averaging the results of individual models. Ensemble technique is a simple
    way to improve generalization, especially when caused by noisy data or a small
    dataset. We train multiple neural networks and average their outputs.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we take 20 neural networks for the same learning problem, we
    adjust the various parameters in the training processing, and then the mean squared
    errors are compared with the mean squared errors of their average.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps followed:'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is loaded and divided into a train and test set. The percentage
    split can be varied for different neural net models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiple models are created with the different training sets and by adjusting
    the parameters in the `nnet()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the models are trained and errors in each model are tabulated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The average error is found for each row in test data and the mean square error
    is calculated for each model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The mean square error is compared with the mean square error of the average.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The best model is chosen from the comparison and is used further for prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This method allows us to play with the data and the function parameters to arrive
    at the optimal setting of the model. We can choose any number of models in the
    ensemble and do parallel processing of the models using R.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is highly reduced and the best parameters of the model are arrived
    at here.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the training and visualization of a simple neural
    network using R. Here, we can change the number of neurons, the number of hidden
    layers, the activation functions, and so on, to determine the training of the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: While dealing with a regression problem, the last layer is a single unit, which
    will give continuous values. For a classification problem, there are n terminal
    units, each representing the class of output with its probability. The breast
    cancer example had two output neurons to represent the two classes of values that
    are output from the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: We have learned how to train, test, and evaluate a dataset using NN model. We
    have also learned how to visualize the NN model in R environment. We have covered
    the concepts like early stopping, avoiding overfitting, generalization of NN,
    and scaling of NN parameters.
  prefs: []
  type: TYPE_NORMAL
