["```py\n    wget http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz -P /sharedfiles\n    ```", "```py\n    import pickle, gzip\n    with gzip.open(\"/sharedfiles/mnist.pkl.gz\", 'rb') as f:\n       train_set, valid_set, test_set = pickle.load(f)\n    ```", "```py\n    train_set[0].shape\n    *(50000, 784)*\n\n    train_set[1].shape\n    *(50000,)*\n\n    import matplotlib\n\n    import numpy \n\n    import matplotlib.pyplot as plt\n\n    plt.rcParams['figure.figsize'] = (10, 10)\n\n    plt.rcParams['image.cmap'] = 'gray'\n\n    for i in range(9):\n        plt.subplot(1,10,i+1)\n        plt.imshow(train_set[0][i].reshape(28,28))\n        plt.axis('off')\n        plt.title(str(train_set[1][i]))\n\n    plt.show()\n    ```", "```py\nimport theano\ntrain_set_x = theano.shared(numpy.asarray(train_set[0], dtype=theano.config.floatX))\ntrain_set_y = theano.shared(numpy.asarray(train_set[1], dtype='int32'))\n```", "```py\n    from theano import theano\n    import theano.tensor as T\n    ```", "```py\nbatch_size = 600\nn_in = 28 * 28\nn_out = 10\n\nx = T.matrix('x')\ny = T.ivector('y')\nW = theano.shared(\n            value=numpy.zeros(\n                (n_in, n_out),\n                dtype=theano.config.floatX\n            ),\n            name='W',\n            borrow=True\n        )\nb = theano.shared(\n    value=numpy.zeros(\n        (n_out,),\n        dtype=theano.config.floatX\n    ),\n    name='b',\n    borrow=True\n)\nmodel = T.nnet.softmax(T.dot(x, W) + b)\n```", "```py\ny_pred = T.argmax(model, axis=1)\n```", "```py\ncost = -T.mean(T.log(model)[T.arange(y.shape[0]), y])\n```", "```py\nerror = T.mean(T.neq(y_pred, y))\n```", "```py\nT.nnet.categorical_crossentropy(model, y_true).mean()\n```", "```py\nT.nnet.binary_crossentropy(model, y_true).mean()\n```", "```py\nT.sqr(model – y_true).mean()\n```", "```py\nT.abs_(model - y_true).mean()\n```", "```py\nT.switch(\n   T.lt(T.abs_(model - y_true) , 1\\. / sigma), \n   0.5 * sigma * T.sqr(model - y_true),\n   T.abs_(model - y_true) – 0.5 / sigma )\n.sum(axis=1).mean()\n```", "```py\nT.sqr(T.maximum(1\\. - y_true * model, 0.)).mean()\n```", "```py\nT.maximum(1\\. - y_true * model, 0.).mean()\n```", "```py\ng_W = T.grad(cost=cost, wrt=W)\ng_b = T.grad(cost=cost, wrt=b)\n\nlearning_rate=0.13\nindex = T.lscalar()\n\ntrain_model = theano.function(\n    inputs=[index],\n    outputs=[cost,error],\n    updates=[(W, W - learning_rate * g_W),(b, b - learning_rate * g_b)],\n    givens={\n        x: train_set_x[index * batch_size: (index + 1) * batch_size],\n        y: train_set_y[index * batch_size: (index + 1) * batch_size]\n    }\n)\n```", "```py\nn_epochs = 1000\nprint_every = 1000\n\nn_train_batches = train_set[0].shape[0] // batch_size\nn_iters = n_epochs * n_train_batches\ntrain_loss = np.zeros(n_iters)\ntrain_error = npzeros(n_iters)\n\nfor epoch in range(n_epochs):\n    for minibatch_index in range(n_train_batches):\n        iteration = minibatch_index + n_train_batches * epoch\n        train_loss[iteration], train_error[iteration] = train_model(minibatch_index)\n        if (epoch * train_set[0].shape[0] + minibatch_index) % print_every == 0 :\n            print('epoch {}, minibatch {}/{}, training error {:02.2f} %, training loss {}'.format(\n                epoch,\n                minibatch_index + 1,\n                n_train_batches,\n                train_error[iteration] * 100,\n                train_loss[iteration]\n            ))\n```", "```py\nT.max([x[:, n::n_pool] for n in range(n_pool)], axis=0)\n```", "```py\nbatch_size = 600\nn_in = 28 * 28\nn_hidden = 500\nn_out = 10\n\ndef shared_zeros(shape, dtype=theano.config.floatX, name='', n=None):\n    shape = shape if n is None else (n,) + shape\n    return theano.shared(np.zeros(shape, dtype=dtype), name=name)\n\ndef shared_glorot_uniform(shape, dtype=theano.config.floatX, name='', n=None):\n    if isinstance(shape, int):\n        high = np.sqrt(6\\. / shape)\n    else:\n        high = np.sqrt(6\\. / (np.sum(shape[:2]) * np.prod(shape[2:])))\n    shape = shape if n is None else (n,) + shape\n    return theano.shared(np.asarray(\n        np.random.uniform(\n            low=-high,\n            high=high,\n            size=shape),\n        dtype=dtype), name=name)\n\nW1 = shared_glorot_uniform( (n_in, n_hidden), name='W1' )\nb1 = shared_zeros( (n_hidden,), name='b1' )\n\nhidden_output = T.tanh(T.dot(x, W1) + b1)\n\nW2 = shared_zeros( (n_hidden, n_out), name='W2' )\nb2 = shared_zeros( (n_out,), name='b2' )\n\nmodel = T.nnet.softmax(T.dot(hidden_output, W2) + b2)\nparams = [W1,b1,W2,b2]\n```", "```py\ng_params = T.grad(cost=cost, wrt=params)\n```", "```py\nlearning_rate = 0.01\nupdates = [\n        (param, param - learning_rate * gparam)\n        for param, gparam in zip(params, g_params)\n    ]\n\ntrain_model = theano.function(\n    inputs=[index],\n    outputs=cost,\n    updates=updates,\n    givens={\n        x: train_set_x[index * batch_size: (index + 1) * batch_size],\n        y: train_set_y[index * batch_size: (index + 1) * batch_size]\n    }\n)\n```", "```py\nlayer0_input = x.reshape((batch_size, 1, 28, 28))\n```", "```py\nfrom theano.tensor.nnet import conv2d\n\nn_conv1 = 20\n\nW1 = shared_glorot_uniform( (n_conv1, 1, 5, 5) )\n\nconv1_out = conv2d(\n    input=layer0_input,\n    filters=W1,\n    filter_shape=(n_conv1, 1, 5, 5),\n    input_shape=(batch_size, 1, 28, 28)\n)\n```", "```py\nfrom theano.tensor.signal import pool\npooled_out = pool.pool_2d(input=conv1_out, ws=(2, 2), ignore_border=True)\n```", "```py\nn_conv2 = 50\n\nW2 = shared_glorot_uniform( (n_conv2, n_conv1, 5, 5) )\n\nconv2_out = conv2d(\n    input=pooled_out,\n    filters=W2,\n    filter_shape=(n_conv2, n_conv1, 5, 5),\n    input_shape=(batch_size, n_conv1, 12, 12)\n)\n\npooled2_out = pool.pool_2d(input=conv2_out, ds=(2, 2),ignore_border=True)\n```", "```py\nhidden_input = pooled2_out.flatten(2)\n\nn_hidden = 500\n\nW3 = shared_zeros( (n_conv2 * 4 * 4, n_hidden), name='W3' )\nb3 = shared_zeros( (n_hidden,), name='b3' )\n\nhidden_output = T.tanh(T.dot(hidden_input, W3) + b3)\n\nn_out = 10\n\nW4 = shared_zeros( (n_hidden, n_out), name='W4' )\nb4 = shared_zeros( (n_out,), name='b4' )\n\nmodel = T.nnet.softmax(T.dot(hidden_output, W4) + b4)\nparams = [W1,W2,W3,b3,W4,b4]\n```", "```py\nvalidate_model = theano.function(\n    inputs=[x,y],\n    outputs=[cost,error]\n)\n```", "```py\nif iteration % validation_interval == 0 :\n    val_index = iteration // validation_interval\n    valid_loss[val_index], valid_error[val_index] = np.mean([\n            validate_model(\n                valid_set[0][i * batch_size: (i + 1) * batch_size],\n                numpy.asarray(valid_set[1][i * batch_size: (i + 1) * batch_size], dtype=\"int32\")\n                )\n                for i in range(n_valid_batches)\n             ], axis=0)\n```", "```py\nepoch 0, minibatch 1/83, validation error 40.05 %, validation loss 2.16520105302\n\nepoch 24, minibatch 9/83, validation error 8.16 %, validation loss 0.288349323906\nepoch 36, minibatch 13/83, validation error 7.96 %, validation loss 0.278418215923\nepoch 48, minibatch 17/83, validation error 7.73 %, validation loss 0.272948684171\nepoch 60, minibatch 21/83, validation error 7.65 %, validation loss 0.269203903154\nepoch 72, minibatch 25/83, validation error 7.59 %, validation loss 0.26624627877\nepoch 84, minibatch 29/83, validation error 7.56 %, validation loss 0.264540277421\n...\nepoch 975, minibatch 76/83, validation error 7.10 %, validation loss 0.258190142922\nepoch 987, minibatch 80/83, validation error 7.09 %, validation loss 0.258411859162\n```", "```py\nepoch 0, minibatch 1/83, validation error 41.25 %, validation loss 2.35665753484\nepoch 24, minibatch 9/83, validation error 10.20 %, validation loss 0.438846310601\nepoch 36, minibatch 13/83, validation error 9.40 %, validation loss 0.399769391865\nepoch 48, minibatch 17/83, validation error 8.85 %, validation loss 0.379035864025\nepoch 60, minibatch 21/83, validation error 8.57 %, validation loss 0.365624915808\nepoch 72, minibatch 25/83, validation error 8.31 %, validation loss 0.355733696371\nepoch 84, minibatch 29/83, validation error 8.25 %, validation loss 0.348027150147\nepoch 96, minibatch 33/83, validation error 8.01 %, validation loss 0.34150374867\nepoch 108, minibatch 37/83, validation error 7.91 %, validation loss 0.335878048092\n...\nepoch 975, minibatch 76/83, validation error 2.97 %, validation loss 0.167824191041\nepoch 987, minibatch 80/83, validation error 2.96 %, validation loss 0.167092795949\n```", "```py\nepoch 0, minibatch 1/83, validation error 53.81 %, validation loss 2.29528842866\nepoch 24, minibatch 9/83, validation error 1.55 %, validation loss 0.048202780541\nepoch 36, minibatch 13/83, validation error 1.31 %, validation loss 0.0445762014715\nepoch 48, minibatch 17/83, validation error 1.29 %, validation loss 0.0432346871821\nepoch 60, minibatch 21/83, validation error 1.25 %, validation loss 0.0425786205451\nepoch 72, minibatch 25/83, validation error 1.20 %, validation loss 0.0413943211024\nepoch 84, minibatch 29/83, validation error 1.20 %, validation loss 0.0416557886347\nepoch 96, minibatch 33/83, validation error 1.19 %, validation loss 0.0414686980075\n...\nepoch 975, minibatch 76/83, validation error 1.08 %, validation loss 0.0477593478863\nepoch 987, minibatch 80/83, validation error 1.08 %, validation loss 0.0478142946085\n```", "```py\nlearning rate\nnumber of hidden neurons\nbatch size\n```", "```py\n    visualize_layer1 = theano.function(\n        inputs=[x,y],\n        outputs=conv1_out\n    )\n    ```", "```py\ndropout = 0.5\n\nif dropout > 0 :\n    mask = srng.binomial(n=1, p=1-dropout, size=hidden_input.shape)\n    # The cast is important because\n    # int * float32 = float64 which make execution slower\n    hidden_input = hidden_input * T.cast(mask, theano.config.floatX)\n```", "```py\ninfer_model = theano.function(\n    inputs=[x],\n    outputs=[y_pred]\n)\n```", "```py\ndef clip_norms(gs, c):\n    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n    return [ T.switch(T.ge(norm, c), g*c/norm, g) for g in gs]\n\nupdates = []\ngrads = T.grad(cost, params)\ngrads = clip_norms(grads, 50)\nfor p,g in zip(params,grads):\n    updated_p = p - learning_rate * g\n    updates.append((p, updated_p))\n```", "```py\nupdates = []\ngrads = T.grad(cost, params)\ngrads = clip_norms(grads, 50)\nfor p,g in zip(params,grads):\n    m = theano.shared(p.get_value() * 0.)\n    v = (momentum * m) - (learning_rate * g)\n    updates.append((m, v))\n    updates.append((p, p + v))\n```", "```py\nupdates = []\ngrads = T.grad(cost, params)\ngrads = clip_norms(grads, 50)\nfor p, g in zip(params, grads):\n    m = theano.shared(p.get_value() * 0.)\n    v = (momentum * m) - (learning_rate * g)\n    updates.append((m,v))\n    updates.append((p, p + momentum * v - learning_rate * g))\n```", "```py\nupdates = []\ngrads = T.grad(cost, params)\ngrads = clip_norms(grads, 50)\nfor p,g in zip(params,grads):\n    acc = theano.shared(p.get_value() * 0.)\n    acc_t = acc + g ** 2\n    updates.append((acc, acc_t))\n    p_t = p - (learning_rate / T.sqrt(acc_t + 1e-6)) * g\n    updates.append((p, p_t))\n```", "```py\nupdates = []\ngrads = T.grad(cost, params)\ngrads = clip_norms(grads, 50)\nfor p,g in zip(params,grads):\n    acc = theano.shared(p.get_value() * 0.)\n    acc_delta = theano.shared(p.get_value() * 0.)\n    acc_new = rho * acc + (1 - rho) * g ** 2\n    updates.append((acc,acc_new))\n    update = g * T.sqrt(acc_delta + 1e-6) / T.sqrt(acc_new + 1e-6)\n    updates.append((p, p - learning_rate * update))\n    updates.append((acc_delta, rho * acc_delta + (1 - rho) * update ** 2))\n```", "```py\nupdates = []\ngrads = T.grad(cost, params)\ngrads = clip_norms(grads, 50)\nfor p,g in zip(params,grads):\n    acc = theano.shared(p.get_value() * 0.)\n    acc_new = rho * acc + (1 - rho) * g ** 2\n    updates.append((acc, acc_new))\n    updated_p = p - learning_rate * (g / T.sqrt(acc_new + 1e-6))\n    updates.append((p, updated_p))\n```", "```py\nb1=0.9, b2=0.999, l=1-1e-8\nupdates = []\ngrads = T.grad(cost, params)\ngrads = clip_norms(grads, 50)  \nt = theano.shared(floatX(1.))\nb1_t = b1 * l **(t-1)\n\nfor p, g in zip(params, grads):\n    m = theano.shared(p.get_value() * 0.)\n    v = theano.shared(p.get_value() * 0.)\n    m_t = b1_t * m + (1 - b1_t) * g\n    v_t = b2 * v + (1 - b2) * g**2 \n    updates.append((m, m_t))\n    updates.append((v, v_t))\n    updates.append((p, p - (learning_rate * m_t / (1 - b1**t)) / (T.sqrt(v_t / (1 - b2**t)) + 1e-6)) )\nupdates.append((t, t + 1.))\n```"]