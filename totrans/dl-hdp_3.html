<html><head></head><body><div id="book-columns"><div id="book-inner"><div class="chapter" title="Chapter 3.  Convolutional Neural Network"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/><span class="koboSpan" id="kobo.1.1">Chapter 3.  Convolutional Neural Network </span></h1></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"><span class="koboSpan" id="kobo.2.1"> </span></td><td valign="top"><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.3.1">"The question of whether a computer can think is no more interesting than the question of whether a submarine can swim."</span></em></span>
</p></td><td valign="top"><span class="koboSpan" id="kobo.4.1"> </span></td></tr><tr><td valign="top"><span class="koboSpan" id="kobo.5.1"> </span></td><td colspan="2" align="right" valign="top" style="text-align: center"><span class="koboSpan" id="kobo.6.1">--</span><span class="attribution"><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">Edsger W. </span><span class="koboSpan" id="kobo.7.2">Dijkstra</span></em></span></span></td></tr></table></div><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.8.1">Convolutional neural network (CNN)</span></strong></span><span class="koboSpan" id="kobo.9.1">--doesn't it give an uncanny feeling about the combination of mathematics and biology with some negligible amount of computer science added? </span><span class="koboSpan" id="kobo.9.2">However, these type of networks have been some of the most dominant and powerful architectures in the field of computer vision. </span><span class="koboSpan" id="kobo.9.3">CNN started to gain its popularity after 2012, when there were huge improvements in the precision of classification, credit to some pioneer in the field of deep learning. </span><span class="koboSpan" id="kobo.9.4">Ever since then, a bunch of high-tech companies have been using deep CNN for various services. </span><span class="koboSpan" id="kobo.9.5">Amazon uses CNN for their product recommendations, Google uses it for their photo search, and Facebook primarily uses it for its automatic tagging algorithms.</span></p><p><span class="koboSpan" id="kobo.10.1">CNN [89] is a type of feed-forward neural network comprised of neurons, which have learnable weights and biases. </span><span class="koboSpan" id="kobo.10.2">These types of networks are basically used to process data, having the grid-like topology form. </span><span class="koboSpan" id="kobo.10.3">CNNs, as the name suggests, are a type of neural network where, unlike the general matrix multiplication, a special type of linear mathematical operation, convolution, is used in at least one of the subsequent layers. </span><span class="koboSpan" id="kobo.10.4">The architecture of CNN is designed to take the benefit of input with multidimensional structure. </span><span class="koboSpan" id="kobo.10.5">These include the 2D structure of an input image, speech signal, or even one-dimensional time series data. </span><span class="koboSpan" id="kobo.10.6">With all these advantages, CNN has been really successful with many practical applications. </span><span class="koboSpan" id="kobo.10.7">CNN is thus tremendously successful, specifically in fields such as natural language processing, recommender systems, image recognition, and video recognition.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/><span class="koboSpan" id="kobo.11.1">Note</span></h3><p><span class="koboSpan" id="kobo.12.1">A bias unit is an </span><span class="emphasis"><em><span class="koboSpan" id="kobo.13.1">extra</span></em></span><span class="koboSpan" id="kobo.14.1"> neuron that has a value of 1 and is added to each pre-output layer. </span><span class="koboSpan" id="kobo.14.2">These units are not connected to the previous layer and so do not represent any </span><span class="emphasis"><em><span class="koboSpan" id="kobo.15.1">activity</span></em></span><span class="koboSpan" id="kobo.16.1"> in a real sense.</span></p></div></div><p><span class="koboSpan" id="kobo.17.1">In this chapter, we will discuss the building block of CNN in-depth. </span><span class="koboSpan" id="kobo.17.2">We will initially discuss what convolution is and the need of convolution operations in the neural network. </span><span class="koboSpan" id="kobo.17.3">Under that topic, we will also address pooling operation, which is the most important component of CNN. </span><span class="koboSpan" id="kobo.17.4">The next topic of this chapter will point out the major challenges of CNN while dealing with large-scale data. </span><span class="koboSpan" id="kobo.17.5">The last part of this chapter will help the reader to learn how to design CNN using Deeplearning4j.</span></p><p><span class="koboSpan" id="kobo.18.1">The main topics of the chapter are listed as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.19.1">Understanding convolution</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.20.1">Background of a CNN</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.21.1">Basic layers of CNN</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.22.1">Distributed deep CNN</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.23.1">CNN with Deeplearning4j</span></li></ul></div><div class="section" title="Understanding convolution"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec18"/><span class="koboSpan" id="kobo.24.1">Understanding convolution</span></h1></div></div></div><p><span class="koboSpan" id="kobo.25.1">To understand the concept of convolution, let us take an example to determine the position of a lost mobile phone with the help of a laser sensor. </span><span class="koboSpan" id="kobo.25.2">Let's say the current location of the mobile phone at time </span><span class="emphasis"><em><span class="koboSpan" id="kobo.26.1">t</span></em></span><span class="koboSpan" id="kobo.27.1"> can be given by the laser as </span><span class="emphasis"><em><span class="koboSpan" id="kobo.28.1">f (t)</span></em></span><span class="koboSpan" id="kobo.29.1">. </span><span class="koboSpan" id="kobo.29.2">The laser gives different readings of the location for all the values of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.30.1">t</span></em></span><span class="koboSpan" id="kobo.31.1">. </span><span class="koboSpan" id="kobo.31.2">The laser sensors are generally noisy in nature, which is undesirable for this scenario. </span><span class="koboSpan" id="kobo.31.3">Therefore, to derive a less noisy measurement of the location of the phone, we need to calculate the average various measurements. </span><span class="koboSpan" id="kobo.31.4">Ideally, the more the measurements, the greater the accuracy of the location. </span><span class="koboSpan" id="kobo.31.5">Hence, we should undergo a weighted average, which provides more weight to the measurements.</span></p><p><span class="koboSpan" id="kobo.32.1">A weighted function can be given by the function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.33.1">w (b)</span></em></span><span class="koboSpan" id="kobo.34.1">, where </span><span class="emphasis"><em><span class="koboSpan" id="kobo.35.1">b</span></em></span><span class="koboSpan" id="kobo.36.1"> denotes the age of the measurement. </span><span class="koboSpan" id="kobo.36.2">To derive a new function that will provide a better estimate of the location of the mobile phone, we need to take the average of the weight at every moment.</span></p><p><span class="koboSpan" id="kobo.37.1">The new function can be given as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.38.1"><img src="graphics/image_03_001.jpg" alt="Understanding convolution"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.39.1">The preceding operation is termed as convolution. </span><span class="koboSpan" id="kobo.39.2">The conventional method of representing convolution is denoted by an asterisk or star, '*':</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.40.1"><img src="graphics/image_03_002.jpg" alt="Understanding convolution"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.41.1">Formally, convolution can be defined as an integral of the product of two functions, where one of the functions is reversed and shifted. </span><span class="koboSpan" id="kobo.41.2">Besides, taking the weighted averages, it may also be used for other purposes.</span></p><p><span class="koboSpan" id="kobo.42.1">In terms of convolutional network terminology, the function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.43.1">f</span></em></span><span class="koboSpan" id="kobo.44.1"> in our example is referred to as the input and the function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.45.1">w</span></em></span><span class="koboSpan" id="kobo.46.1">, the second parameter is called the kernel of the operation. </span><span class="koboSpan" id="kobo.46.2">The kernel is composed of a number of filters, which will be used on the input to get the output, referred to as </span><span class="emphasis"><em><span class="koboSpan" id="kobo.47.1">feature maps</span></em></span><span class="koboSpan" id="kobo.48.1">. </span><span class="koboSpan" id="kobo.48.2">In a more convenient way, the kernel can be seen as a membrane, which will allow only the desirable features of the input to pass through it. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.49.1">Figure 3.1</span></em></span><span class="koboSpan" id="kobo.50.1"> shows a pictorial view of the operation:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.51.1"><img src="graphics/image_03_003.jpg" alt="Understanding convolution"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.52.1">Figure 3.1: The figure shows a simple representation of a convolutional network where the input has to pass through the kernel to provide the feature map.</span></p><p><span class="koboSpan" id="kobo.53.1">In a practical scenario, as our example shows, the laser sensor cannot really provide the measurements at every given instant of time. </span><span class="koboSpan" id="kobo.53.2">Ideally, when a computer works on data, it only works at some regular intervals; hence, the time will be discrete. </span><span class="koboSpan" id="kobo.53.3">So, the sensor will generally provide the results at some defined interval of time. </span><span class="koboSpan" id="kobo.53.4">If we assume that the instrument provides output once/second, then the parameter </span><span class="emphasis"><em><span class="koboSpan" id="kobo.54.1">t</span></em></span><span class="koboSpan" id="kobo.55.1"> will only take integer values. </span><span class="koboSpan" id="kobo.55.2">With these assumptions, the functions </span><span class="emphasis"><em><span class="koboSpan" id="kobo.56.1">f</span></em></span><span class="koboSpan" id="kobo.57.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.58.1">w</span></em></span><span class="koboSpan" id="kobo.59.1"> will only be defined for integer values of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.60.1">t</span></em></span><span class="koboSpan" id="kobo.61.1">. </span><span class="koboSpan" id="kobo.61.2">The modified equation for discrete convolution can now be written as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.62.1"><img src="graphics/image_03_004.jpg" alt="Understanding convolution"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.63.1">In case of machine learning or deep learning applications, the inputs are generally a multidimensional array of data, and the kernel uses multidimensional arrays of different parameters taken by the algorithm. </span><span class="koboSpan" id="kobo.63.2">The basic assumption is that the values of the functions are non-zero only for a finite set of points for which we store the values, and zero elsewhere. </span><span class="koboSpan" id="kobo.63.3">So, the infinite summation can be represented as the summation for a range of a finite number of array elements. </span><span class="koboSpan" id="kobo.63.4">For example, for a 2D image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.64.1">I</span></em></span><span class="koboSpan" id="kobo.65.1"> as an input and a corresponding 2D kernel </span><span class="emphasis"><em><span class="koboSpan" id="kobo.66.1">K</span></em></span><span class="koboSpan" id="kobo.67.1">, the convolution function can be written as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.68.1"><img src="graphics/image_03_005.jpg" alt="Understanding convolution"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.69.1">So, with this, you have already got some background of convolution. </span><span class="koboSpan" id="kobo.69.2">In the next section of this chapter, we will discuss the application of convolution in a neural network and the building blocks of CNN.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Background of a CNN"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec19"/><span class="koboSpan" id="kobo.1.1">Background of a CNN</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">CNN, a particular form of deep learning models, is not a new concept, and they have been widely adopted by the vision community for a long time. </span><span class="koboSpan" id="kobo.2.2">The model worked well in recognizing the hand-written digit by LeCun et al in 1998 [90]. </span><span class="koboSpan" id="kobo.2.3">But unfortunately, due to the inability of CNNs to work with higher resolution images, its popularity has diminished with the course of time. </span><span class="koboSpan" id="kobo.2.4">The reason was mostly due to hardware and memory constraints, and also the lack of availability of large-scale training datasets. </span><span class="koboSpan" id="kobo.2.5">As the computational power increases with time, mostly due to the wide availability of CPUs and GPUs and with the generation of big data, various large-scale datasets, such as the MIT Places dataset (see Zhou et al., 2014), ImageNet [91] and so on. </span><span class="koboSpan" id="kobo.2.6">it became possible to train larger and complex models. </span><span class="koboSpan" id="kobo.2.7">This is initially shown by Krizhevsky et al [4] in their paper, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.3.1">Imagenet classification using deep convolutional neural networks</span></em></span><span class="koboSpan" id="kobo.4.1">. </span><span class="koboSpan" id="kobo.4.2">In that paper, they brought down the error rate with half-beating traditional approaches. </span><span class="koboSpan" id="kobo.4.3">Over the next few years, their paper became one of the most substantial papers in the field of computer vision. </span><span class="koboSpan" id="kobo.4.4">This popular network trained by Alex Krizhevsky, called AlexNet, could well have been the starting point of using deep networks in the field of computer vision.</span></p><div class="section" title="Architecture overview"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec24"/><span class="koboSpan" id="kobo.5.1">Architecture overview</span></h2></div></div></div><p><span class="koboSpan" id="kobo.6.1">We assume that the readers are already familiar with the traditional neural network. </span><span class="koboSpan" id="kobo.6.2">In this section, we will look at the general building blocks of CNN.</span></p><p><span class="koboSpan" id="kobo.7.1">The traditional neural network receives a single vector as input, and reaches the intermediate states through a series of latent (hidden) layers. </span><span class="koboSpan" id="kobo.7.2">Each hidden layer is composed of several neurons, where each neuron is fully connected to every other neuron of the previous layer. </span><span class="koboSpan" id="kobo.7.3">The last layer, called the 'output layer', is fully-connected, and it is responsible for the class scores. </span><span class="koboSpan" id="kobo.7.4">A regular neural network composed of three layers is shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.8.1">Figure 3.2</span></em></span><span class="koboSpan" id="kobo.9.1">:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.10.1"><img src="graphics/image_03_006.jpg" alt="Architecture overview"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.11.1">Figure 3.2: The figure shows the block diagram of a three-layer regular neural network. </span><span class="koboSpan" id="kobo.11.2">The neurons of every layer are fully-connected to every other layer of the previous layer.</span></p><p><span class="koboSpan" id="kobo.12.1">Regular neural networks face tremendous challenges while dealing with large-scale images. </span><span class="koboSpan" id="kobo.12.2">For example, in the CIFAR-10 RGB database, the dimension of the images are </span><span class="emphasis"><em><span class="koboSpan" id="kobo.13.1">32x32x3</span></em></span><span class="koboSpan" id="kobo.14.1">, hence, a single fully-connected neuron in a first hidden layer of the traditional neural network will have </span><span class="emphasis"><em><span class="koboSpan" id="kobo.15.1">32*32*3= 3072</span></em></span><span class="koboSpan" id="kobo.16.1"> number of weights. </span><span class="koboSpan" id="kobo.16.2">The number of weights, although seems to be reasonable at the outset, would really be a cumbersome task to manage with the increasing number of dimensions. </span><span class="koboSpan" id="kobo.16.3">For another RGB image, if the dimension becomes (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.17.1">300x300x3</span></em></span><span class="koboSpan" id="kobo.18.1">), the total number of weights of the neurons will result in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.19.1">300*300*3 = 270000</span></em></span><span class="koboSpan" id="kobo.20.1"> weights. </span><span class="koboSpan" id="kobo.20.2">Also, as the number of layers will increase, this number will also increase drastically, and would quickly lead to overfitting. </span><span class="koboSpan" id="kobo.20.3">Moreover, visualization of an image completely neglects the complex 2D spatial structure of the image. </span><span class="koboSpan" id="kobo.20.4">Therefore, the fully-connected concept of the neural network, right from the initial phase, does not seem to work with the larger dimensional datasets. </span><span class="koboSpan" id="kobo.20.5">So, we need to build a model that will overcome both of these limitations:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.21.1"><img src="graphics/image_03_007.jpg" alt="Architecture overview"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.22.1">Figure 3.3: The arrangement of CNN in 3D (width, height, and depth) is represented in the figure. </span><span class="koboSpan" id="kobo.22.2">Every layer converts the 3D input volume to the corresponding 3D output volume of neuron activations. </span><span class="koboSpan" id="kobo.22.3">The red input layer keeps the image, hence, its width and height would be the dimensions of the image, where the depth would be three (Red, Green, and Blue). </span><span class="koboSpan" id="kobo.22.4">Image sourced from Wikipedia.</span></p><p><span class="koboSpan" id="kobo.23.1">One way to solve this problem is to use convolution in place of matrix multiplication. </span><span class="koboSpan" id="kobo.23.2">Learning from a set of convolutional filters (kernel) is much easier than learning from the whole matrix (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.24.1">300x300x3</span></em></span><span class="koboSpan" id="kobo.25.1">). </span><span class="koboSpan" id="kobo.25.2">Unlike the traditional neural network, the layers of a CNN have their neurons arranged in three dimensions: width, height, and depth. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.26.1">Figure 3.3</span></em></span><span class="koboSpan" id="kobo.27.1"> shows the representation for this. </span><span class="koboSpan" id="kobo.27.2">For example, in the previous example of CIFAR-10, the image has a dimension of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.28.1">32x32x3</span></em></span><span class="koboSpan" id="kobo.29.1">, which is width, depth, and height respectively. </span><span class="koboSpan" id="kobo.29.2">In a CNN, instead of the neurons in a fully-connected nature, the neurons in a layer will only be connected to a subset of neurons in the previous layer. </span><span class="koboSpan" id="kobo.29.3">Details of this will be explained in the subsequent portion of this section. </span><span class="koboSpan" id="kobo.29.4">Moreover, the final output layer CIFAR-10 image will have the dimension </span><span class="emphasis"><em><span class="koboSpan" id="kobo.30.1">1x1x10</span></em></span><span class="koboSpan" id="kobo.31.1">, because the CNN will diminish the full image into a single vector of class score, placed along with the depth dimension.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Basic layers of CNN"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/><span class="koboSpan" id="kobo.1.1">Basic layers of CNN</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">A CNN is composed of a sequence of layers, where every layer of the network goes through a differentiable function to transform itself from one volume of activation to another. </span><span class="koboSpan" id="kobo.2.2">Four main types of layers are used to build a CNN: Convolutional layer, Rectified Linear Units layer, Pooling layer, and Fully-connected layer. </span><span class="koboSpan" id="kobo.2.3">All these layers are stacked together to form a full CNN.</span></p><p><span class="koboSpan" id="kobo.3.1">A regular CNN could have the following architecture:</span></p><p><span class="koboSpan" id="kobo.4.1">[INPUT - CONV - RELU - POOL - FC]</span></p><p><span class="koboSpan" id="kobo.5.1">However, in a deep CNN, there are generally more layers interspersed between these five basic layers.</span></p><p><span class="koboSpan" id="kobo.6.1">A classic deep neural network will have the following structure:</span></p><p><span class="koboSpan" id="kobo.7.1">Input -&gt; Conv-&gt;ReLU-&gt;Conv-&gt;ReLu-&gt;Pooling-&gt;ReLU-&gt;Conv-&gt;ReLu-&gt;Pooling-&gt;Fully Connected</span></p><p><span class="koboSpan" id="kobo.8.1">AlexNet, as mentioned in the earlier section, can be taken as a perfect example for this kind of structure. </span><span class="koboSpan" id="kobo.8.2">The architecture of AlexNet is shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.9.1">Figure 3.4</span></em></span><span class="koboSpan" id="kobo.10.1">. </span><span class="koboSpan" id="kobo.10.2">After every layer, an implicit ReLU non-linearity has been added. </span><span class="koboSpan" id="kobo.10.3">We will explain this in detail in the next section.</span></p><p><span class="koboSpan" id="kobo.11.1">One might wonder, why do we need multiple layers in a CNN? </span><span class="koboSpan" id="kobo.11.2">The next section of this chapter shall explain this as well:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.12.1"><img src="graphics/image_03_008.jpg" alt="Basic layers of CNN"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.13.1">Figure 3.4: An illustration of the depth and weight of the AlexNet is shown in the figure. </span><span class="koboSpan" id="kobo.13.2">The number inside the curly braces denotes the number of filters with dimensions written above.</span></p><div class="section" title="Importance of depth in a CNN"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec25"/><span class="koboSpan" id="kobo.14.1">Importance of depth in a CNN</span></h2></div></div></div><p><span class="koboSpan" id="kobo.15.1">In the paper [96], the author has put forward a few statistics, to show how deep networks help in gaining more accuracy of the output. </span><span class="koboSpan" id="kobo.15.2">As noted, the Architecture of Krizhevsky et al. </span><span class="koboSpan" id="kobo.15.3">model uses eight layers, which are trained on ImageNet. </span><span class="koboSpan" id="kobo.15.4">When the fully connected top layer (7th layer) is removed, it drops approximately 16 million parameters with a performance drop of 1.1%. </span><span class="koboSpan" id="kobo.15.5">Furthermore, when the top two layers (6th and 7th) are removed, nearly 50 million parameters get reduced along with a 5.7% drop in performance. </span><span class="koboSpan" id="kobo.15.6">Similarly, when the upper feature extractor layers (3rd and 4th) are removed, it results in a drop of around 1 million parameters with a performance drop of 3.0%. </span><span class="koboSpan" id="kobo.15.7">To get a better insight of the scenario, when the upper feature extractor layers and the fully connected (3rd, 4th, 6th, and 7th) layers are removed, the model was left with only four layers. </span><span class="koboSpan" id="kobo.15.8">In that case, a 33.5% drop in performance is experienced.</span></p><p><span class="koboSpan" id="kobo.16.1">Therefore, it can be easily concluded that we need deep convolutional network to increase the performance of the model. </span><span class="koboSpan" id="kobo.16.2">However, as already stated, a deep network is extremely difficult to manage in a centralized system due to limitations of memory and performance management. </span><span class="koboSpan" id="kobo.16.3">So, a distributed way of implementing deep CNN is required. </span><span class="koboSpan" id="kobo.16.4">In the subsequent sections of this chapter, we will explain how to implement this with the help of Deeplearning4j, and integrating the processing with Hadoop's YARN.</span></p></div><div class="section" title="Convolutional layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec26"/><span class="koboSpan" id="kobo.17.1">Convolutional layer</span></h2></div></div></div><p><span class="koboSpan" id="kobo.18.1">As illustrated in the architecture overview, the main purpose of convolution is to allow the model to work with a limited number of inputs at a particular time. </span><span class="koboSpan" id="kobo.18.2">Moreover, convolution supports three most important features, which substantially help in improving the performance of a deep learning model. </span><span class="koboSpan" id="kobo.18.3">The features are listed as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.19.1">Sparse connectivity</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.20.1">Parameter sharing</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.21.1">Equivariant representations</span></li></ul></div><p><span class="koboSpan" id="kobo.22.1">We will now describe each of these features in turn.</span></p><div class="section" title="Sparse connectivity"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec6"/><span class="koboSpan" id="kobo.23.1">Sparse connectivity</span></h3></div></div></div><p><span class="koboSpan" id="kobo.24.1">As already explained, the traditional network layers use matrix multiplication by a matrix of parameters with a different parameter describing the interaction between each output unit and input unit. </span><span class="koboSpan" id="kobo.24.2">On the other hand, CNNs use sparse connectivity, sometimes referred to as sparse interactions or sparse weights, for this purpose. </span><span class="koboSpan" id="kobo.24.3">This idea is attained by keeping the size of the kernel smaller than the input, which helps in reducing the time complexity of the algorithm. </span><span class="koboSpan" id="kobo.24.4">For example, for a large image dataset, the image could have thousands or millions of pixels; however, we can identify the small, significant features of the image, such as edges and contours from the kernels, which only have hundreds or tens of the whole pixels. </span><span class="koboSpan" id="kobo.24.5">Therefore, we need to keep only a small number of parameters, which, in turn, helps in the reduction of memory requirements of the models and datasets. </span><span class="koboSpan" id="kobo.24.6">The idea also alleviates the number of operations, which could enhance the overall computing power. </span><span class="koboSpan" id="kobo.24.7">This, in turn, decreases the running time complexity of the computation in a huge manner, which eventually ameliorates its efficiency. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.25.1">Figure 3.5</span></em></span><span class="koboSpan" id="kobo.26.1"> diagrammatically shows, how with the sparse connectivity approach, we can reduce the number of receptive fields of each neuron.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/><span class="koboSpan" id="kobo.27.1">Note</span></h3><p><span class="koboSpan" id="kobo.28.1">Each neuron in a Convolutional layer renders the response of the filters applied in the previous layer. </span><span class="koboSpan" id="kobo.28.2">The main purpose of these neurons is to pass the responses through some non-linearity. </span><span class="koboSpan" id="kobo.28.3">The total area of the previous layers, where that filter was applied, is termed as the receptive field of that neuron. </span><span class="koboSpan" id="kobo.28.4">So, the receptive field is always equivalent to the size of the filter.</span></p></div></div><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.29.1"><img src="graphics/B05883_03_05.jpg" alt="Sparse connectivity"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.30.1">Figure 3.5: The figure shows how the input units of M affect the output unit N3 with sparse connectivity.Unlike matrix multiplication, the number of receptive fields in the sparse connectivity approach reduce from five to three (M2, M3, and M4). </span><span class="koboSpan" id="kobo.30.2">The arrows indicate the parameter sharing approach too. </span><span class="koboSpan" id="kobo.30.3">The connections from one neuron are shared with two neurons in the model</span></p><p><span class="koboSpan" id="kobo.31.1">Therefore, with the sparse connectivity approach, the receptive fields for each layer are smaller than the receptive fields using the matrix multiplication approach. </span><span class="koboSpan" id="kobo.31.2">However, it is to be noted that for deep CNNs, the receptive field of the units is virtually larger than the receptive fields of the corresponding shallow networks. </span><span class="koboSpan" id="kobo.31.3">The reason is that all the units in the deep networks are indirectly connected to almost all the neurons of the network. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.32.1">Figure 3.6</span></em></span><span class="koboSpan" id="kobo.33.1"> shows a visual representation of such a scenario:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.34.1"><img src="graphics/B05883_03_06.jpg" alt="Sparse connectivity"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.35.1">Figure 3.6: Representation of sparse connectivity for deep layers of convolution neural networks. </span><span class="koboSpan" id="kobo.35.2">Unlike Figure 3.5, where unit N3 had three receptive fields, here the number of receptive fields of N3 has increased to five.</span></p><div class="section" title="Improved time complexity"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec0"/><span class="koboSpan" id="kobo.36.1">Improved time complexity</span></h4></div></div></div><p><span class="koboSpan" id="kobo.37.1">Similar to the example given in the previous section, if there are </span><span class="emphasis"><em><span class="koboSpan" id="kobo.38.1">p</span></em></span><span class="koboSpan" id="kobo.39.1"> inputs and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.40.1">q</span></em></span><span class="koboSpan" id="kobo.41.1"> outputs in a layer, then the matrix multiplication will require </span><span class="emphasis"><em><span class="koboSpan" id="kobo.42.1">(p*q)</span></em></span><span class="koboSpan" id="kobo.43.1"> number of parameters. </span><span class="koboSpan" id="kobo.43.2">The running time complexity of the algorithm will become </span><span class="emphasis"><em><span class="koboSpan" id="kobo.44.1">O (p*q)</span></em></span><span class="koboSpan" id="kobo.45.1">. </span><span class="koboSpan" id="kobo.45.2">With the sparsely connected approach, if we limit the number of upper limit connections associated with each output to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.46.1">n</span></em></span><span class="koboSpan" id="kobo.47.1">, then it will need only </span><span class="emphasis"><em><span class="koboSpan" id="kobo.48.1">n*q</span></em></span><span class="koboSpan" id="kobo.49.1"> parameters, and the runtime complexity will reduce to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.50.1">O (n*q)</span></em></span><span class="koboSpan" id="kobo.51.1">. </span><span class="koboSpan" id="kobo.51.2">For many real-life applications, the sparse connectivity approach provides good performance for the deep learning tasks while keeping the size of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.52.1">n &lt;&lt;p</span></em></span><span class="koboSpan" id="kobo.53.1">.</span></p></div></div><div class="section" title="Parameter sharing"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec7"/><span class="koboSpan" id="kobo.54.1">Parameter sharing</span></h3></div></div></div><p><span class="koboSpan" id="kobo.55.1">Parameter sharing can be defined as the process by which the same parameter for a function can be used for multiple functions in the model. </span><span class="koboSpan" id="kobo.55.2">In regular neural networks, each element of the weight matrix is applied exactly once, when calculating the output of a layer. </span><span class="koboSpan" id="kobo.55.3">The weight is multiplied by one element of the input, but never revisited. </span><span class="koboSpan" id="kobo.55.4">Parameter sharing can also be referred to as tied weights, as the value of the weight used to one input is tied to the value weight used for others. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.56.1">Figure 3.5</span></em></span><span class="koboSpan" id="kobo.57.1"> can also be viewed as an example for parameter sharing. </span><span class="koboSpan" id="kobo.57.2">For example, a particular parameter from </span><span class="strong"><strong><span class="koboSpan" id="kobo.58.1">M2</span></strong></span><span class="koboSpan" id="kobo.59.1"> is used with both </span><span class="strong"><strong><span class="koboSpan" id="kobo.60.1">N1</span></strong></span><span class="koboSpan" id="kobo.61.1"> and </span><span class="strong"><strong><span class="koboSpan" id="kobo.62.1">N3</span></strong></span><span class="koboSpan" id="kobo.63.1">.</span></p><p><span class="koboSpan" id="kobo.64.1">The main purpose of the operation is to control the number of free parameters in the Convolutional layer. </span><span class="koboSpan" id="kobo.64.2">In a CNN, each element of the kernel is used at almost every position of the input. </span><span class="koboSpan" id="kobo.64.3">One logical assumption for this is that if one of the features is desirable at some spatial position, then it should also be necessary to calculate the other positions.</span></p><p><span class="koboSpan" id="kobo.65.1">Since all the elements of a single depth slice share the same type of parametrization, the forward pass in each depth slice of the Convolutional layer can be measured as a convolutional of input volume with the weights of the neurons. </span><span class="koboSpan" id="kobo.65.2">The outcome of this convolution is an activation map. </span><span class="koboSpan" id="kobo.65.3">These collections of activation maps are stacked together with the association of depth dimension to result the output volume. </span><span class="koboSpan" id="kobo.65.4">Although the parameter sharing approach bestows the translation invariance of the CNN architecture, it does not enhance the runtime of forward propagation.</span></p><div class="section" title="Improved space complexity"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec1"/><span class="koboSpan" id="kobo.66.1">Improved space complexity</span></h4></div></div></div><p><span class="koboSpan" id="kobo.67.1">In parameter sharing, the runtime of the model still remains </span><span class="emphasis"><em><span class="koboSpan" id="kobo.68.1">O (n*q)</span></em></span><span class="koboSpan" id="kobo.69.1">. </span><span class="koboSpan" id="kobo.69.2">However, it helps to reduce the overall space complexity in a significant way, as the storage requirement of the model reduces to n number of parameters. </span><span class="koboSpan" id="kobo.69.3">Since </span><span class="emphasis"><em><span class="koboSpan" id="kobo.70.1">p</span></em></span><span class="koboSpan" id="kobo.71.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.72.1">q</span></em></span><span class="koboSpan" id="kobo.73.1"> are generally of similar sizes, the value of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.74.1">n</span></em></span><span class="koboSpan" id="kobo.75.1"> becomes almost negligible as compared to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.76.1">p*q</span></em></span><span class="koboSpan" id="kobo.77.1">.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/><span class="koboSpan" id="kobo.78.1">Note</span></h3><p><span class="koboSpan" id="kobo.79.1">Convolution is considerably more productive than the traditional dense matrix multiplication approach, both in terms of time complexity as well as space complexity.</span></p></div></div></div></div><div class="section" title="Equivariant representations"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec8"/><span class="koboSpan" id="kobo.80.1">Equivariant representations</span></h3></div></div></div><p><span class="koboSpan" id="kobo.81.1">In the convolution layer, due to parameter sharing, the layers possess a property termed as equivariance to translation. </span><span class="koboSpan" id="kobo.81.2">An equivariant function is defined as a function whose output changes in the same way the input does.</span></p><p><span class="koboSpan" id="kobo.82.1">Mathematically, if X and Y both belong to a same group G, then a function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.83.1">f: X </span><span class="inlinemediaobject"><span class="koboSpan" id="kobo.84.1"><img src="graphics/Arrow.jpg" alt="Equivariant representations"/></span></span><span class="koboSpan" id="kobo.85.1">
   Y</span></em></span><span class="koboSpan" id="kobo.86.1"> is said to be equivariant if </span><span class="emphasis"><em><span class="koboSpan" id="kobo.87.1">f (g.x) = g.f(x) for</span></em></span><span class="koboSpan" id="kobo.88.1"> all </span><span class="emphasis"><em><span class="koboSpan" id="kobo.89.1">g</span></em></span>
<span class="emphasis"><em><span class="inlinemediaobject"><span class="koboSpan" id="kobo.90.1"><img src="graphics/Belongs-t0.jpg" alt="Equivariant representations"/></span></span><span class="koboSpan" id="kobo.91.1">
 G</span></em></span><span class="koboSpan" id="kobo.92.1"> and all </span><span class="emphasis"><em><span class="koboSpan" id="kobo.93.1">x</span></em></span><span class="koboSpan" id="kobo.94.1"> in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.95.1">X</span></em></span><span class="koboSpan" id="kobo.96.1">.</span></p><p><span class="koboSpan" id="kobo.97.1">In case of convolution, if we take </span><span class="emphasis"><em><span class="koboSpan" id="kobo.98.1">g</span></em></span><span class="koboSpan" id="kobo.99.1"> to be any function, which shifts the input in equal magnitude, then the convolution function is equivariant to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.100.1">g</span></em></span><span class="koboSpan" id="kobo.101.1">. </span><span class="koboSpan" id="kobo.101.2">For example, let </span><span class="emphasis"><em><span class="koboSpan" id="kobo.102.1">I</span></em></span><span class="koboSpan" id="kobo.103.1"> be a function that gives the image color for any even coordinate. </span><span class="koboSpan" id="kobo.103.2">Let </span><span class="emphasis"><em><span class="koboSpan" id="kobo.104.1">h</span></em></span><span class="koboSpan" id="kobo.105.1"> be another function, which maps one image function to another image function, given by the following equation:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.106.1"><img src="graphics/image_03_011.jpg" alt="Equivariant representations"/></span></div><p>
</p><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.107.1">I</span><sup><span class="koboSpan" id="kobo.108.1">/</span></sup></em></span><span class="koboSpan" id="kobo.109.1"> is an image function that moves every pixel of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.110.1">I</span></em></span><span class="koboSpan" id="kobo.111.1"> five units to the right. </span><span class="koboSpan" id="kobo.111.2">Therefore, we have the following:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.112.1"><img src="graphics/image_03_012.jpg" alt="Equivariant representations"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.113.1">Now, if we apply this translation to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.114.1">I</span></em></span><span class="koboSpan" id="kobo.115.1">, followed by the convolution, the result would be exactly the same when we apply convolution to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.116.1">I</span><sup><span class="koboSpan" id="kobo.117.1">/</span></sup></em></span><span class="koboSpan" id="kobo.118.1">, followed by the transformation function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.119.1">h</span></em></span><span class="koboSpan" id="kobo.120.1"> to the output.</span></p><p><span class="koboSpan" id="kobo.121.1">In case of images, a convolution operation generates a two-dimensional map of all the definite features present in the input. </span><span class="koboSpan" id="kobo.121.2">So, similar to the earlier example, if we shift the object in the output by some fixed scale, the output representation will also move in the same scale. </span><span class="koboSpan" id="kobo.121.3">This concept is useful for some cases; for example, consider a group photo of cricket players of two different teams. </span><span class="koboSpan" id="kobo.121.4">We can find some common feature of the jersey in the image to detect some players. </span><span class="koboSpan" id="kobo.121.5">Now, the similar feature will obviously be present in others' t-shirts as well. </span><span class="koboSpan" id="kobo.121.6">So, it is quite practical to share the parameter across the entire image.</span></p><p><span class="koboSpan" id="kobo.122.1">Convolution also helps to process some special kinds of data, which are difficult, or rather not even possible, with the traditional fixed-shape matrix multiplication approach.</span></p></div></div><div class="section" title="Choosing the hyperparameters for Convolutional layers"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec27"/><span class="koboSpan" id="kobo.123.1">Choosing the hyperparameters for Convolutional layers</span></h2></div></div></div><p><span class="koboSpan" id="kobo.124.1">So far we have explained how each neuron in the convolution layers is connected to the input volume. </span><span class="koboSpan" id="kobo.124.2">In this section, we will discuss the ways of controlling the size of the output volume. </span><span class="koboSpan" id="kobo.124.3">In other words, controlling the number of neurons in the output volume, and how they are arranged.</span></p><p><span class="koboSpan" id="kobo.125.1">Basically, there are three hyperparameters, which control the size of the output volume of the Convolutional layers. </span><span class="koboSpan" id="kobo.125.2">They are: the depth, stride, and zero-padding.</span></p><p><span class="koboSpan" id="kobo.126.1">How do we know how many Convolutional layers should we use, what should be the size of the filters, or the values of stride and padding? </span><span class="koboSpan" id="kobo.126.2">These are extremely subjective questions, and their solutions are not at all trivial in nature. </span><span class="koboSpan" id="kobo.126.3">No researchers have set any standard parameter to choose these hyperparameters. </span><span class="koboSpan" id="kobo.126.4">A neural network generally and largely depends on the type of data used for training. </span><span class="koboSpan" id="kobo.126.5">This data can vary in size, complexity of the input raw image, type of image processing tasks, and many other criteria. </span><span class="koboSpan" id="kobo.126.6">One general line of thought by looking at the big dataset, is that one has to think how to choose the hyperparameters to deduce the correct combination, which creates abstractions of the images at proper scale. </span><span class="koboSpan" id="kobo.126.7">We'll discuss all these in this subsection.</span></p><div class="section" title="Depth"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec9"/><span class="koboSpan" id="kobo.127.1">Depth</span></h3></div></div></div><p><span class="koboSpan" id="kobo.128.1">In the output volume, depth is considered as an important parameter. </span><span class="koboSpan" id="kobo.128.2">The depth corresponds to the number of filters we would like to apply for each learning iteration on some changes in the input. </span><span class="koboSpan" id="kobo.128.3">If the first Convolutional layer takes a raw image as the input, then multiple neurons along the depth dimension might activate in the presence of various blobs of colors or different oriented edges. </span><span class="koboSpan" id="kobo.128.4">The set of neurons in the same regions of input are termed as a depth column.</span></p></div><div class="section" title="Stride"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec10"/><span class="koboSpan" id="kobo.129.1">Stride</span></h3></div></div></div><p><span class="koboSpan" id="kobo.130.1">Stride specifies the policy of allocation of depth columns around the spatial dimension (width and height). </span><span class="koboSpan" id="kobo.130.2">It basically controls how the filter convolves around the input volume. </span><span class="koboSpan" id="kobo.130.3">Stride can be formally defined as the amount by which the filter shifts during the convolution. </span><span class="koboSpan" id="kobo.130.4">Ideally, the value of stride should be an integer and not a fraction. </span><span class="koboSpan" id="kobo.130.5">Conceptually, this amount helps in deciding how much of the input image information one wants to retain before proceeding to the next layer. </span><span class="koboSpan" id="kobo.130.6">The more the stride, the more information that will be retained for the next layer.</span></p><p><span class="koboSpan" id="kobo.131.1">For example, when the stride is </span><span class="emphasis"><em><span class="koboSpan" id="kobo.132.1">1</span></em></span><span class="koboSpan" id="kobo.133.1">, a new depth column is allocated to spatial positions, one spatial unit apart. </span><span class="koboSpan" id="kobo.133.2">This produces large output volumes due to heavily overlapping receptive fields in between the columns. </span><span class="koboSpan" id="kobo.133.3">On the other hand, if the value of stride is increased, there will be less overlapping among the receptive fields, which results in spatially smaller dimensional output volume.</span></p><p><span class="koboSpan" id="kobo.134.1">We will take an example to simplify the concept a bit more. </span><span class="koboSpan" id="kobo.134.2">Let us imagine a </span><span class="emphasis"><em><span class="koboSpan" id="kobo.135.1">7*7</span></em></span><span class="koboSpan" id="kobo.136.1"> input volume and a </span><span class="emphasis"><em><span class="koboSpan" id="kobo.137.1">3*3</span></em></span><span class="koboSpan" id="kobo.138.1"> filter (we will ignore the third dimension for the sake of simplicity), with a stride of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.139.1">1</span></em></span><span class="koboSpan" id="kobo.140.1">. </span><span class="koboSpan" id="kobo.140.2">The output volume in this case would be of dimension </span><span class="emphasis"><em><span class="koboSpan" id="kobo.141.1">5*5</span></em></span><span class="koboSpan" id="kobo.142.1">, as shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.143.1">Figure 3.7</span></em></span><span class="koboSpan" id="kobo.144.1">. </span><span class="koboSpan" id="kobo.144.2">However, this looks somewhat straightforward. </span><span class="koboSpan" id="kobo.144.3">Now, with stride </span><span class="emphasis"><em><span class="koboSpan" id="kobo.145.1">2</span></em></span><span class="koboSpan" id="kobo.146.1">, keeping the other parameters the same, the output volume would have less dimensionality of the order </span><span class="emphasis"><em><span class="koboSpan" id="kobo.147.1">3*3</span></em></span><span class="koboSpan" id="kobo.148.1">. </span><span class="koboSpan" id="kobo.148.2">In this case, the receptive field will shift by </span><span class="emphasis"><em><span class="koboSpan" id="kobo.149.1">2</span></em></span><span class="koboSpan" id="kobo.150.1"> units, and hence, the volume will shrink to a dimension of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.151.1">3*3</span></em></span><span class="koboSpan" id="kobo.152.1">:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.153.1"><img src="graphics/image_03_013.jpg" alt="Stride"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.154.1">Figure 3.7: Illustration of how the filter convolves around the input volume of 7x7 with stride 1 resulting in 5x5 output volume.</span></p><p><span class="koboSpan" id="kobo.155.1">This is illustrated in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.156.1">Figure 3.8</span></em></span><span class="koboSpan" id="kobo.157.1">. </span><span class="koboSpan" id="kobo.157.2">All these calculations are based on some formula mentioned in the next topic of this section. </span><span class="koboSpan" id="kobo.157.3">Now, if we want to increase the stride further to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.158.1">3</span></em></span><span class="koboSpan" id="kobo.159.1">, we will have difficulties with spacing and making sure the receptive field fits on the input volume. </span><span class="koboSpan" id="kobo.159.2">Ideally, a programmer will raise the value of the stride only if lesser overlapping of the receptive fields is required, and if they need smaller spatial dimensions:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.160.1"><img src="graphics/image_03_014.jpg" alt="Stride"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.161.1">Figure 3.8: Illustration of how the filter convolves around the input volume of 7x7 with stride 2 resulting in 3x3 output volume.</span></p></div><div class="section" title="Zero-padding"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec11"/><span class="koboSpan" id="kobo.162.1">Zero-padding</span></h3></div></div></div><p><span class="koboSpan" id="kobo.163.1">We've already got enough information to infer that, as we keep applying more convolution layers to the input volume, the size of the output volume decreases further. </span><span class="koboSpan" id="kobo.163.2">However, in some cases, we might want to preserve almost all the information about the original input volume so that we can also extract the low-level features. </span><span class="koboSpan" id="kobo.163.3">In such scenarios, we pad the input volume with zeroes around the borders of the input volume.</span></p><p><span class="koboSpan" id="kobo.164.1">This size of zero-padding is considered as a hyperparameter. </span><span class="koboSpan" id="kobo.164.2">It can be defined as a hyperparameter, which is directly used to control the spatial size of the output volume in scenarios where we want to exactly preserve the spatial size of input volume.</span></p><p><span class="koboSpan" id="kobo.165.1">For example, if we apply a </span><span class="emphasis"><em><span class="koboSpan" id="kobo.166.1">5*5*3</span></em></span><span class="koboSpan" id="kobo.167.1"> filter to a </span><span class="emphasis"><em><span class="koboSpan" id="kobo.168.1">32*32*3</span></em></span><span class="koboSpan" id="kobo.169.1"> input volume, the output volume will reduce to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.170.1">28*28*3</span></em></span><span class="koboSpan" id="kobo.171.1">. </span><span class="koboSpan" id="kobo.171.2">However, let's say we want to use the same Convolutional layer, but need to keep the output volume to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.172.1">32*32*3</span></em></span><span class="koboSpan" id="kobo.173.1">. </span><span class="koboSpan" id="kobo.173.2">We will use a zero-padding of size</span><span class="emphasis"><em><span class="koboSpan" id="kobo.174.1"> 2 </span></em></span><span class="koboSpan" id="kobo.175.1">to this layer. </span><span class="koboSpan" id="kobo.175.2">This will give us an output volume of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.176.1">36*36*3</span></em></span><span class="koboSpan" id="kobo.177.1">, as shown in the following figure. </span><span class="koboSpan" id="kobo.177.2">Now, if we apply three convolution layers with a </span><span class="emphasis"><em><span class="koboSpan" id="kobo.178.1">5*5*3</span></em></span><span class="koboSpan" id="kobo.179.1"> filter, it will produce an output volume of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.180.1">32*32*3</span></em></span><span class="koboSpan" id="kobo.181.1">, hence maintaining the exact spatial size of the input volume. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.182.1">Figure 3.9</span></em></span><span class="koboSpan" id="kobo.183.1"> represents the pictorial views of the scenario:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.184.1"><img src="graphics/image_03_015.jpg" alt="Zero-padding"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.185.1">Figure 3.9: The input volume has a dimension of 32*32*3. </span><span class="koboSpan" id="kobo.185.2">The two borders of zeros will generate an input volume of 36*36*3. </span><span class="koboSpan" id="kobo.185.3">Further application of the Convolution layer, with three filters of size 5*5*3, having stride 1, will result in an output volume of 32*32*3.</span></p></div><div class="section" title="Mathematical formulation of hyperparameters"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec12"/><span class="koboSpan" id="kobo.186.1">Mathematical formulation of hyperparameters</span></h3></div></div></div><p><span class="koboSpan" id="kobo.187.1">This part of the chapter will introduce an equation to calculate the spatial size of the output volume based on the hyperparameters that we have discussed so far. </span><span class="koboSpan" id="kobo.187.2">The equation is extremely useful to choose the hyperparameter for the CNN, as these are the deciding factors to 'fit' the neurons in the network. </span><span class="koboSpan" id="kobo.187.3">The spatial size of the output volume can be written as a function of the input volume size (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.188.1">W</span></em></span><span class="koboSpan" id="kobo.189.1">), the receptive field size, or the filter size of the Convolutional layer neurons(</span><span class="emphasis"><em><span class="koboSpan" id="kobo.190.1">K</span></em></span><span class="koboSpan" id="kobo.191.1">), value of the applied stride(</span><span class="emphasis"><em><span class="koboSpan" id="kobo.192.1">S</span></em></span><span class="koboSpan" id="kobo.193.1">), and the amount of zero-padding used (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.194.1">P</span></em></span><span class="koboSpan" id="kobo.195.1">) on the border.</span></p><p><span class="koboSpan" id="kobo.196.1">The equation to compute the spatial size of output volume can be written as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.197.1"><img src="graphics/image_03_016.jpg" alt="Mathematical formulation of hyperparameters"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.198.1">Considering the examples given in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.199.1">Figure 3.7</span></em></span><span class="koboSpan" id="kobo.200.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.201.1">Figure 3.8</span></em></span><span class="koboSpan" id="kobo.202.1">, where </span><span class="emphasis"><em><span class="koboSpan" id="kobo.203.1">W=7</span></em></span><span class="koboSpan" id="kobo.204.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.205.1">K=3</span></em></span><span class="koboSpan" id="kobo.206.1">, and with no padding, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.207.1">P =0</span></em></span><span class="koboSpan" id="kobo.208.1">. </span><span class="koboSpan" id="kobo.208.2">For stride </span><span class="emphasis"><em><span class="koboSpan" id="kobo.209.1">1</span></em></span><span class="koboSpan" id="kobo.210.1">, we have </span><span class="emphasis"><em><span class="koboSpan" id="kobo.211.1">S=1</span></em></span><span class="koboSpan" id="kobo.212.1">, and this will give the following:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.213.1"><img src="graphics/image_03_017.jpg" alt="Mathematical formulation of hyperparameters"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.214.1">Similarly for stride </span><span class="strong"><strong><span class="koboSpan" id="kobo.215.1">2</span></strong></span><span class="koboSpan" id="kobo.216.1">, the equation will give a value of </span><span class="strong"><strong><span class="koboSpan" id="kobo.217.1">2</span></strong></span><span class="koboSpan" id="kobo.218.1">:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.219.1"><img src="graphics/image_03_018.jpg" alt="Mathematical formulation of hyperparameters"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.220.1">Hence, as shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.221.1">Figure 3.7</span></em></span><span class="koboSpan" id="kobo.222.1">, we would get an output of a spatial size of </span><span class="strong"><strong><span class="koboSpan" id="kobo.223.1">3</span></strong></span><span class="koboSpan" id="kobo.224.1">. </span><span class="koboSpan" id="kobo.224.2">However, with this configuration, when a stride of </span><span class="strong"><strong><span class="koboSpan" id="kobo.225.1">3 </span></strong></span><span class="koboSpan" id="kobo.226.1">is applied, it will not fit across the input volume, as this equation will return a fractional value </span><span class="strong"><strong><span class="koboSpan" id="kobo.227.1">2.333</span></strong></span><span class="koboSpan" id="kobo.228.1"> for the output volume:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.229.1"><img src="graphics/image_03_019.jpg" alt="Mathematical formulation of hyperparameters"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.230.1">This also signifies that the values of the hyperparameter have mutual constraints. </span><span class="koboSpan" id="kobo.230.2">The preceding example returns a fractional value, hence, the hyperparameters would be considered as invalid. </span><span class="koboSpan" id="kobo.230.3">However, we might resolve the issue by adding some zero-padding around the border.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/><span class="koboSpan" id="kobo.231.1">Note</span></h3><p><span class="koboSpan" id="kobo.232.1">The spatial arrangements of hyperparameters have mutual constraints.</span></p></div></div><div class="section" title="Effect of zero-padding"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec2"/><span class="koboSpan" id="kobo.233.1">Effect of zero-padding</span></h4></div></div></div><p><span class="koboSpan" id="kobo.234.1">As mentioned in the zero-padding section, its main purpose is to preserve the information of input volume to the next layer. </span><span class="koboSpan" id="kobo.234.2">To ensure same spatial size of input and output volume, the conventional formula for zero-padding, with a stride, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.235.1">S=1</span></em></span><span class="koboSpan" id="kobo.236.1">, is as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.237.1"><img src="graphics/image_03_020.jpg" alt="Effect of zero-padding"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.238.1">Taking the example given in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.239.1">Figure 3.9</span></em></span><span class="koboSpan" id="kobo.240.1">, we can verify the authenticity of the formula. </span><span class="koboSpan" id="kobo.240.2">In the example, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.241.1">W = 32</span></em></span><span class="koboSpan" id="kobo.242.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.243.1">K=5</span></em></span><span class="koboSpan" id="kobo.244.1">, and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.245.1">S=1</span></em></span><span class="koboSpan" id="kobo.246.1">. </span><span class="koboSpan" id="kobo.246.2">Therefore, to ensure the spatial output volume to be equal to 32, we choose the number of zero-padding as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.247.1"><img src="graphics/image_03_021.jpg" alt="Effect of zero-padding"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.248.1">So, with </span><span class="emphasis"><em><span class="koboSpan" id="kobo.249.1">P=2</span></em></span><span class="koboSpan" id="kobo.250.1">, the spatial size of the output volume is given as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.251.1"><img src="graphics/image_03_022.jpg" alt="Effect of zero-padding"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.252.1">So, this equation worked out well to preserve the same spatial dimension for the input volume and output volume.</span></p></div></div></div><div class="section" title="ReLU (Rectified Linear Units) layers"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec28"/><span class="koboSpan" id="kobo.253.1">ReLU (Rectified Linear Units) layers</span></h2></div></div></div><p><span class="koboSpan" id="kobo.254.1">In the convolution layer, the system basically computes the linear operations by doing element-wise multiplication and summations. </span><span class="koboSpan" id="kobo.254.2">Deep convolution usually performs the convolution operations followed by a non-linear operation after each layer. </span><span class="koboSpan" id="kobo.254.3">This is essential, because cascading linear operations produce another linear system. </span><span class="koboSpan" id="kobo.254.4">Adding non-linearity in between the layers corroborates a more expressive nature of the model than a linear model.</span></p><p><span class="koboSpan" id="kobo.255.1">Therefore, after each convolution layer, an activation layer is applied on the current output. </span><span class="koboSpan" id="kobo.255.2">So, the main objective of this activation layer is to introduce some non-linearity to the system. </span><span class="koboSpan" id="kobo.255.3">Modern CNNs use </span><span class="strong"><strong><span class="koboSpan" id="kobo.256.1">Rectified Linear Unit</span></strong></span><span class="koboSpan" id="kobo.257.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.258.1">ReLu</span></strong></span><span class="koboSpan" id="kobo.259.1">) as the activation function.</span></p><p><span class="koboSpan" id="kobo.260.1">In artificial neural networks, the activation function, the rectifier, is defined as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.261.1"><img src="graphics/image_03_023.jpg" alt="ReLU (Rectified Linear Units) layers"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.262.1">where </span><span class="emphasis"><em><span class="koboSpan" id="kobo.263.1">x</span></em></span><span class="koboSpan" id="kobo.264.1"> is the input to a neuron.</span></p><p><span class="koboSpan" id="kobo.265.1">A unit operating the rectifier is termed as ReLU. </span><span class="koboSpan" id="kobo.265.2">Earlier, many non-linear functions such as </span><span class="emphasis"><em><span class="koboSpan" id="kobo.266.1">tan h</span></em></span><span class="koboSpan" id="kobo.267.1">, sigmoid, and the like were used in the network, but in the last few years, researchers have identified that ReLU layers work much better, because they help the network to train a lot faster, without compromising the accuracy of the outcome. </span><span class="koboSpan" id="kobo.267.2">A significant improvement in the computational efficiency is a major factor for this.</span></p><p><span class="koboSpan" id="kobo.268.1">Furthermore, this layer enhances the non-linear properties of the model and other overall networks without having any impact on the receptive fields of the Convolutional layer.</span></p><p><span class="koboSpan" id="kobo.269.1">Recently, in 2013, Mass et al. </span><span class="koboSpan" id="kobo.269.2">[94] introduced a new version of non-linearity, termed as leaky-ReLU. </span><span class="koboSpan" id="kobo.269.3">Leaky-ReLU can be defined as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.270.1"><img src="graphics/image_03_024.jpg" alt="ReLU (Rectified Linear Units) layers"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.271.1">where </span><span class="emphasis"><em><span class="koboSpan" id="kobo.272.1">Î±</span></em></span><span class="koboSpan" id="kobo.273.1"> is a predetermined parameter. </span><span class="koboSpan" id="kobo.273.2">Later, in 2015, He et al [95] updated this equation by suggesting that the parameter Î± can also be trained, which leads to a much-improved model.</span></p><div class="section" title="Advantages of ReLU over the sigmoid function"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec13"/><span class="koboSpan" id="kobo.274.1">Advantages of ReLU over the sigmoid function</span></h3></div></div></div><p><span class="koboSpan" id="kobo.275.1">ReLU helps to alleviate the vanishing gradient problem, which is explained in detail in </span><a class="link" href="ch01.html" title="Chapter 1. Introduction to Deep Learning"><span class="koboSpan" id="kobo.276.1">
Chapter 1
</span></a><span class="koboSpan" id="kobo.277.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.278.1">Introduction to Deep Learning</span></em></span><span class="koboSpan" id="kobo.279.1">. </span><span class="koboSpan" id="kobo.279.2">ReLU applies the aforementioned function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.280.1">f(x)</span></em></span><span class="koboSpan" id="kobo.281.1"> to all the values of the input volume, and transforms all the negative activations to </span><span class="strong"><strong><span class="koboSpan" id="kobo.282.1">0</span></strong></span><span class="koboSpan" id="kobo.283.1">. </span><span class="koboSpan" id="kobo.283.2">For max function, the gradient is defined as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.284.1"><img src="graphics/image_03_025.jpg" alt="Advantages of ReLU over the sigmoid function"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.285.1">However, for the Sigmoid function, the gradient tends to vanish as we increase or decrease the value of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.286.1">x</span></em></span><span class="koboSpan" id="kobo.287.1">.</span></p><p><span class="koboSpan" id="kobo.288.1">The Sigmoid function is given as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.289.1"><img src="graphics/image_03_026.jpg" alt="Advantages of ReLU over the sigmoid function"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.290.1">The Sigmoid function has a range of [</span><span class="emphasis"><em><span class="koboSpan" id="kobo.291.1">0, 1</span></em></span><span class="koboSpan" id="kobo.292.1">], whereas the ReLU function has the range [</span><span class="emphasis"><em><span class="koboSpan" id="kobo.293.1">0, </span><span class="inlinemediaobject"><span class="koboSpan" id="kobo.294.1"><img src="graphics/Infinity.jpg" alt="Advantages of ReLU over the sigmoid function"/></span></span><span class="koboSpan" id="kobo.295.1">
</span></em></span><span class="koboSpan" id="kobo.296.1">]. </span><span class="koboSpan" id="kobo.296.2">Therefore, the Sigmoid function is applied to model the probability, while ReLU can be used to model all the positive numbers.</span></p></div></div><div class="section" title="Pooling layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec29"/><span class="koboSpan" id="kobo.297.1">Pooling layer</span></h2></div></div></div><p><span class="koboSpan" id="kobo.298.1">This layer is the third stage of a CNN. </span><span class="koboSpan" id="kobo.298.2">After applying some Rectified Linear Units later, the programmer might choose to apply a Pooling layer. </span><span class="koboSpan" id="kobo.298.3">The layer can also be referred to as a down-sampling layer.</span></p><p><span class="koboSpan" id="kobo.299.1">The pooling function is basically used to further modify the output of the layer. </span><span class="koboSpan" id="kobo.299.2">The primary function of the layer is to replace the output of the network at a certain location with a summarized statistics of the neighboring outputs. </span><span class="koboSpan" id="kobo.299.3">There are multiple options for this layer, Max-pooling being the most popular one. </span><span class="koboSpan" id="kobo.299.4">Max pooling operation [93] operates within a rectangular neighborhood, and reports the maximum output from it. </span><span class="koboSpan" id="kobo.299.5">Max-pooling basically takes a filter (generally of size </span><span class="emphasis"><em><span class="koboSpan" id="kobo.300.1">2x2</span></em></span><span class="koboSpan" id="kobo.301.1">) and stride of the same length, that is, </span><span class="strong"><strong><span class="koboSpan" id="kobo.302.1">2</span></strong></span><span class="koboSpan" id="kobo.303.1">. </span><span class="koboSpan" id="kobo.303.2">The filter is then applied to the input volume, and it outputs the maximum number in every region where the filter convolves around. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.304.1">Figure 3.10</span></em></span><span class="koboSpan" id="kobo.305.1"> shows a representation of the same thing. </span><span class="koboSpan" id="kobo.305.2">Other popular options for Pooling layers are the average of an </span><span class="emphasis"><em><span class="koboSpan" id="kobo.306.1">L2</span></em></span><span class="koboSpan" id="kobo.307.1"> normal of a rectangular neighborhood, average of a rectangular neighborhood or a weighted average, which is based on the distance from the central pixel:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.308.1"><img src="graphics/B05883_03_10.jpg" alt="Pooling layer"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.309.1">Figure 3.10: Example of Max-pool with a 2*2 filter and stride 2. </span><span class="koboSpan" id="kobo.309.2">Image sourced from Wikipedia.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note16"/><span class="koboSpan" id="kobo.310.1">Note</span></h3><p><span class="koboSpan" id="kobo.311.1">Invariance to local translation is extremely beneficial if we are interested in the neighboring features, rather than the exact position of the feature.</span></p></div></div><div class="section" title="Where is it useful, and where is it not?"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec14"/><span class="koboSpan" id="kobo.312.1">Where is it useful, and where is it not?</span></h3></div></div></div><p><span class="koboSpan" id="kobo.313.1">The intuitive reason behind the Pooling layer is that once some specific feature of the original input volume is known, its exact location becomes trivial as compared to its location relative to the other features. </span><span class="koboSpan" id="kobo.313.2">With the help of pooling, the representation becomes almost invariant to small translations of the input. </span><span class="koboSpan" id="kobo.313.3">Invariance to translation signifies that for a small amount of translation on the input, the values of most of the pooled output do not vary significantly.</span></p><p><span class="koboSpan" id="kobo.314.1">Invariance to local translation is extremely beneficial if we are interested in the neighboring features rather than the exact position of the feature. </span><span class="koboSpan" id="kobo.314.2">However, while dealing with computer vision tasks, it is required to be careful in the use of Pooling layer. </span><span class="koboSpan" id="kobo.314.3">Although pooling extensively helps in the reduction of complexity of the model, it might end up losing the location sensitivity of the model.</span></p><p><span class="koboSpan" id="kobo.315.1">Let us take an example of image processing, which involves identifying a box in an image. </span><span class="koboSpan" id="kobo.315.2">Pooling layer in this case will help if we simply target to determine the existence of the box in the image. </span><span class="koboSpan" id="kobo.315.3">However, if the problem statement is more concerned with locating the exact position of the box, we will have to be careful enough while using the Pooling layer. </span><span class="koboSpan" id="kobo.315.4">As another example, let us say we are working on a language model, and are interested in identifying the contextual similarity between two words. </span><span class="koboSpan" id="kobo.315.5">In this case, the use of Pooling layer is not advisable, as it will lose out on some valuable feature information.</span></p><p><span class="koboSpan" id="kobo.316.1">Therefore, it can be concluded that Pooling layer is basically used for reducing the computational complexity of the model. </span><span class="koboSpan" id="kobo.316.2">The Pooling layer is more like an averaging process, where we are more interested in a group of neighboring features. </span><span class="koboSpan" id="kobo.316.3">The layer can be applied in scenarios where we can afford to let go of some of the localized information.</span></p></div></div><div class="section" title="Fully connected layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec30"/><span class="koboSpan" id="kobo.317.1">Fully connected layer</span></h2></div></div></div><p><span class="koboSpan" id="kobo.318.1">The fully connected layer is the final layer of a CNN. </span><span class="koboSpan" id="kobo.318.2">The input volume for this layer comes from the output of the preceding Convolutional layer, ReLU, or Pooling layer. </span><span class="koboSpan" id="kobo.318.3">The fully connected layer takes this input and outputs an </span><span class="emphasis"><em><span class="koboSpan" id="kobo.319.1">N</span></em></span><span class="koboSpan" id="kobo.320.1"> dimensional vector, where </span><span class="emphasis"><em><span class="koboSpan" id="kobo.321.1">N</span></em></span><span class="koboSpan" id="kobo.322.1"> is the number of different classes present in the initial input datasets. </span><span class="koboSpan" id="kobo.322.2">The basic idea on which a fully connected layer works is that it works on the output received from the preceding layer, and identifies the specific feature that mostly correlates to a particular class. </span><span class="koboSpan" id="kobo.322.3">For example, if the model is predicting whether an image contains a cat or bird, it will have high values in the activation maps, which will represent some high-level features such as four legs or wings, respectively.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Distributed deep CNN"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec21"/><span class="koboSpan" id="kobo.1.1">Distributed deep CNN</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">This section of the chapter will introduce some extremely aggressive deep CNN architecture, associated challenges for these networks, and the need of much larger distributed computing to overcome this. </span><span class="koboSpan" id="kobo.2.2">This section will explain how Hadoop and its YARN can provide a sufficient solution for this problem.</span></p><div class="section" title="Most popular aggressive deep neural networks and their configurations"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec31"/><span class="koboSpan" id="kobo.3.1">Most popular aggressive deep neural networks and their configurations</span></h2></div></div></div><p><span class="koboSpan" id="kobo.4.1">CNNs have shown stunning results in image recognition in recent years. </span><span class="koboSpan" id="kobo.4.2">However, unfortunately, they are extremely expensive to train. </span><span class="koboSpan" id="kobo.4.3">In the case of a sequential training process, the convolution operation takes around 95% of the total running time. </span><span class="koboSpan" id="kobo.4.4">With big datasets, even with low-scale distributed training, the training process takes many days to complete. </span><span class="koboSpan" id="kobo.4.5">The award winning CNN, AlexNet with ImageNet in 2012, took nearly an entire week to train on with two GTX 580 3 GB GPUs. </span><span class="koboSpan" id="kobo.4.6">The following table displays few of the most popular distributed deep CNNs with their configuration and corresponding time taken for the training process to complete:</span></p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/><col/></colgroup><tbody><tr><td>
<p><span class="strong"><strong><span class="koboSpan" id="kobo.5.1">Models</span></strong></span></p>
</td><td>
<p><span class="strong"><strong><span class="koboSpan" id="kobo.6.1">Computing power</span></strong></span></p>
</td><td>
<p><span class="strong"><strong><span class="koboSpan" id="kobo.7.1">Datasets</span></strong></span></p>
</td><td>
<p><span class="strong"><strong><span class="koboSpan" id="kobo.8.1">Number of depth</span></strong></span></p>
</td><td>
<p><span class="strong"><strong><span class="koboSpan" id="kobo.9.1">Time taken for the training process</span></strong></span></p>
</td></tr><tr><td>
<p><span class="koboSpan" id="kobo.10.1">AlexNet</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.11.1">Two NVIDIA GTX 580 3 GB GPUs</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.12.1">Trained the network on ImageNet data, which contained over 15 million high-resolution images from a total of over 22,000 categories.</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.13.1">eight layers</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.14.1">Five to six days.</span></p>
</td></tr><tr><td>
<p><span class="koboSpan" id="kobo.15.1">ZFNet [97]</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.16.1">GTX 580 GPU</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.17.1">1.3 million images, spread over 1000 different classes.</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.18.1">eight layers</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.19.1">Twelve days.</span></p>
</td></tr><tr><td>
<p><span class="koboSpan" id="kobo.20.1">VGG Net [98]</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.21.1">4 Nvidia Titan Black GPUs</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.22.1">The dataset includes images of 1000 classes, and is split into three sets: training (1.3 M images), validation (50 K images), and testing (100 K images with held out class labels).</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.23.1">19 layers</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.24.1">Two to three weeks.</span></p>
</td></tr><tr><td>
<p><span class="koboSpan" id="kobo.25.1">GoogLeNet [99]</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.26.1">A few high-end GPUs</span></p>
</td><td>
<p>
</p><p><span class="koboSpan" id="kobo.27.1">1.2 million</span></p><p>
</p><p><span class="koboSpan" id="kobo.28.1">images for training.</span></p><p>
</p>
</td><td>
<p><span class="koboSpan" id="kobo.29.1">The network is 22 layers deep when counting only layers with parameters (or 27 layers if we also count pooling). </span><span class="koboSpan" id="kobo.29.2">The overall number of layers (independent building blocks) used for the construction of the network is about 100.</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.30.1">Within a week.</span></p>
</td></tr><tr><td>
<p><span class="koboSpan" id="kobo.31.1">Microsoft ResNet [100]</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.32.1">8 GPU machine</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.33.1">Trained on the 1.28 million training images, and evaluated on the 50k validation images.</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.34.1">152 layers.</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.35.1">Two to three weeks.</span></p>
</td></tr></tbody></table></div></div><div class="section" title="Training time - major challenges associated with deep neural networks"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec32"/><span class="koboSpan" id="kobo.36.1">Training time - major challenges associated with deep neural networks</span></h2></div></div></div><p><span class="koboSpan" id="kobo.37.1">From the preceding table, it can be surely inferred that there have been loads of efforts made by researchers to increase the accuracy of the outcome. </span><span class="koboSpan" id="kobo.37.2">One key point that comes out from the table is that the numbers of layers have been one of the major criteria to improve the accuracy. </span><span class="koboSpan" id="kobo.37.3">Microsoft's Resnet uses a neural network, as deep as 152 layers, which turns to be an extremely aggressive deep neural network. </span><span class="koboSpan" id="kobo.37.4">This architecture has set many new records in 2015 in classification, localization, and detection through the deep CNN. </span><span class="koboSpan" id="kobo.37.5">Apart from this, ResNet has also won the ILSVRC 2015 with an incredible improvement in the error rate of only 3.6%.</span></p><p><span class="koboSpan" id="kobo.38.1">Although deep convolutional networks are almost about to reach the expected mark of accuracy, the major concern in almost all these deep CNNs is the rightmost column of the table. </span><span class="koboSpan" id="kobo.38.2">Hence, it shows that the current challenge for training deep CNNs is to build a large-scale distributed framework to parallelize the training across many CPUs and GPUs over a fast interconnected network.</span></p></div><div class="section" title="Hadoop for deep CNNs"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec33"/><span class="koboSpan" id="kobo.39.1">Hadoop for deep CNNs</span></h2></div></div></div><p><span class="koboSpan" id="kobo.40.1">In this section, we will explain how Hadoop can be used to distribute the deep models at a large scale for faster processing.</span></p><p><span class="koboSpan" id="kobo.41.1">The running times of CNNs can be divided into two major categories:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.42.1">All the Convolutional layers present in the network consume around 90-95% of the computation. </span><span class="koboSpan" id="kobo.42.2">They use approximately 5% of the parameters, and have large representations. </span><span class="koboSpan" id="kobo.42.3">[101]</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.43.1">The rest of the computation, around 5-10%, is taken by the fully connected layers. </span><span class="koboSpan" id="kobo.43.2">They use almost 95% of the parameters, and have small representations.</span></li></ul></div><p><span class="koboSpan" id="kobo.44.1">Alex Krizhevsky in [101] has proposed an algorithm for training of CNN using distributed architecture. </span><span class="koboSpan" id="kobo.44.2">In a traditional CNN, the convolution operation itself consumes nearly the entire running time of the whole process; hence, data parallelism should be used for faster training. </span><span class="koboSpan" id="kobo.44.3">However, for a fully connected layer, it is advisable to use the model parallelism approach. </span><span class="koboSpan" id="kobo.44.4">We'll explain the algorithm using Hadoop and its YARN in this section of the chapter.</span></p><p><span class="koboSpan" id="kobo.45.1">In Hadoop, the distributed system workers sit on each of the blocks of HDFS, and process the data synchronously in parallel. </span><span class="koboSpan" id="kobo.45.2">We will use a small batch size of 1024 numbers of examples from the raw input image, which will be split into N multiple blocks of </span><span class="strong"><strong><span class="koboSpan" id="kobo.46.1">Hadoop Distributed File System</span></strong></span><span class="koboSpan" id="kobo.47.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.48.1">HDFS</span></strong></span><span class="koboSpan" id="kobo.49.1">). </span><span class="koboSpan" id="kobo.49.2">So, total </span><span class="emphasis"><em><span class="koboSpan" id="kobo.50.1">N</span></em></span><span class="koboSpan" id="kobo.51.1"> workers will be working for each small batch of data. </span><span class="koboSpan" id="kobo.51.2">The block size of HDFS would be kept as size </span><span class="emphasis"><em><span class="koboSpan" id="kobo.52.1">K</span></em></span><span class="koboSpan" id="kobo.53.1">. </span><span class="koboSpan" id="kobo.53.2">Now, what should be the size of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.54.1">K</span></em></span><span class="koboSpan" id="kobo.55.1">? </span><span class="koboSpan" id="kobo.55.2">Although, a small size of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.56.1">K</span></em></span><span class="koboSpan" id="kobo.57.1"> will increase the number of blocks and help to make the training even faster, a large number of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.58.1">N</span></em></span><span class="koboSpan" id="kobo.59.1"> will also eventually increase the volume of metadata that resides in the NameNode. </span><span class="koboSpan" id="kobo.59.2">One major drawback in this case, is </span><span class="strong"><strong><span class="koboSpan" id="kobo.60.1">Single Point of Failure</span></strong></span><span class="koboSpan" id="kobo.61.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.62.1">SPOF</span></strong></span><span class="koboSpan" id="kobo.63.1">) for Hadoop [81], which is more prone to smaller size of main memory for NameNode. </span><span class="koboSpan" id="kobo.63.2">However, with a bigger value of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.64.1">K</span></em></span><span class="koboSpan" id="kobo.65.1">, we will get a small number of blocks of HDFS, and hence, the number of workers working in parallel will be lesser in number. </span><span class="koboSpan" id="kobo.65.2">This will again make the training process slower. </span><span class="koboSpan" id="kobo.65.3">So, a better approach to choose the size of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.66.1">K</span></em></span><span class="koboSpan" id="kobo.67.1"> will primarily depend on the following three factors:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.68.1">The availability of the primary memory's size of your NameNode.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.69.1">The size of the input batch and the complexity of operations performed on each block of data.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.70.1">How important or valuable your intermediate outcome of the data is. </span><span class="koboSpan" id="kobo.70.2">Based on these criteria, we can set the replication factor. </span><span class="koboSpan" id="kobo.70.3">However, the higher the replication factor, the higher the load of the NameNode.</span></li></ul></div><p><span class="koboSpan" id="kobo.71.1">The blocks of HDFS are distributed across all the DataNodes of Hadoop where YARN will operate directly on them in parallel.</span></p><p><span class="koboSpan" id="kobo.72.1">The steps for distributed training of the Convolutional layer are as follows:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.73.1">Each of the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.74.1">N</span></em></span><span class="koboSpan" id="kobo.75.1"> blocks is given a different small data batch of 1024 examples from the raw input image.</span></li><li class="listitem"><span class="koboSpan" id="kobo.76.1">The same size of filter and stride is applied on each of the N numbers of blocks, which results in individual spatial output based on the values of the inputs.</span></li><li class="listitem"><span class="koboSpan" id="kobo.77.1">ReLU is applied on all of these, synchronously, in parallel to get some non-linearity in the outcome.</span></li><li class="listitem"><span class="koboSpan" id="kobo.78.1">Max-pooling, or any other down-sampling algorithm, if desired, is applied on these separate chunks of data based on the necessity of the outcome.</span></li><li class="listitem"><span class="koboSpan" id="kobo.79.1">The outputs (transformed parameters) for each iteration of the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.80.1">N</span></em></span><span class="koboSpan" id="kobo.81.1"> blocks are sent back to the master-node called Resource Manager, where their parameters are averaged. </span><span class="koboSpan" id="kobo.81.2">The newly updated parameter is sent back to each of the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.82.1">N</span></em></span><span class="koboSpan" id="kobo.83.1"> blocks to perform the actions again.</span></li><li class="listitem"><span class="koboSpan" id="kobo.84.1">Steps 2 to 5 are repeated for a predefined number of epochs.</span></li></ol></div><p><span class="koboSpan" id="kobo.85.1">For a Fully-connected layer, one of the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.86.1">N</span></em></span><span class="koboSpan" id="kobo.87.1"> number of workers working on any of the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.88.1">N</span></em></span><span class="koboSpan" id="kobo.89.1"> blocks of data for a small batch of input image will send the last-stage convolutional activities to all other (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.90.1">N-1</span></em></span><span class="koboSpan" id="kobo.91.1">) numbers of workers. </span><span class="koboSpan" id="kobo.91.2">The workers will then perform the fully-connected operation on this batch of 1024 examples, then initiate to back-propagate the gradients for these 1024 examples. </span><span class="koboSpan" id="kobo.91.3">The next worker, in parallel to this operation, will then send its last-stage Convolutional layer activities to the other workers similar to the earlier situation. </span><span class="koboSpan" id="kobo.91.4">The workers will again work on the fully-connected activities for the second batch of 1024 examples. </span><span class="koboSpan" id="kobo.91.5">The process will iterate until we get the outcome with the desired minimal error.</span></p><p><span class="koboSpan" id="kobo.92.1">In this method, the workers broadcast their last-stage Convolutional layer's information to all other workers. </span><span class="koboSpan" id="kobo.92.2">The main benefit of this approach is that a large proportion (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.93.1">(N-1)/N</span></em></span><span class="koboSpan" id="kobo.94.1">) of the communication can be suppressed, and it can be run in parallel with the calculation of the Fully-connected layer. </span><span class="koboSpan" id="kobo.94.2">The approach is extremely advantageous in terms of communication of the network.</span></p><p><span class="koboSpan" id="kobo.95.1">Therefore, it is very clear that Hadoop can be highly beneficial for providing a distributed environment for a CNN with the help of HDFS and Hadoop YARN.</span></p><p><span class="koboSpan" id="kobo.96.1">So, now that we are familiar with the approach of distributing the model in parallel with Hadoop, the next part of the section will discuss the coding part that each of the workers will be operating on each block of HDFS.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Convolutional layer using Deeplearning4j"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec22"/><span class="koboSpan" id="kobo.1.1">Convolutional layer using Deeplearning4j</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">This section of the chapter will provide the basic idea on how to write the code for CNN using Deeplearning4j. </span><span class="koboSpan" id="kobo.2.2">You'll be able to learn the syntax for using the various hyperparameters mentioned in this chapter.</span></p><p><span class="koboSpan" id="kobo.3.1">To implement CNN using Deeplearning4j, the whole idea can be split into three core phases: loading data or preparation of the data, network configuration, and training and evaluation of the model.</span></p><div class="section" title="Loading data"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec34"/><span class="koboSpan" id="kobo.4.1">Loading data</span></h2></div></div></div><p><span class="koboSpan" id="kobo.5.1">For CNNs, generally, we only work on the image data to train the model. </span><span class="koboSpan" id="kobo.5.2">In Deeplearning4j, the images can be read using </span><code class="literal"><span class="koboSpan" id="kobo.6.1">ImageRecordReader</span></code><span class="koboSpan" id="kobo.7.1">. </span><span class="koboSpan" id="kobo.7.2">The following code snippet shows how to load </span><span class="emphasis"><em><span class="koboSpan" id="kobo.8.1">16Ã16</span></em></span><span class="koboSpan" id="kobo.9.1"> color images for the model:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.10.1">RecordReader imageReader = new ImageRecordReader(16, 16, false);
imageReader.initialize(new FileSplit(new      
File(System.getProperty("user.home"), "image_location")));</span></pre><p><span class="koboSpan" id="kobo.11.1">After that, using </span><code class="literal"><span class="koboSpan" id="kobo.12.1">CSVRecordReader</span></code><span class="koboSpan" id="kobo.13.1">, we can load all the image labels from the input CSV files, as follows:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.14.1">int numLinesToSkip = 0;
String delimiter = ",";
RecordReader labelsReader = new    
CSVRecordReader((numLinesToSkip,delimiter);
labelsReader.initialize(new FileSplit(new   
File(System.getProperty("user.home"),"labels.csv_file_location"))</span></pre><p><span class="koboSpan" id="kobo.15.1">To combine both the images and labels data, we can use </span><code class="literal"><span class="koboSpan" id="kobo.16.1">ComposableRecordReader</span></code><span class="koboSpan" id="kobo.17.1">. </span><code class="literal"><span class="koboSpan" id="kobo.18.1">ComposableRecordReader</span></code><span class="koboSpan" id="kobo.19.1"> can also be beneficial in cases where we need to merge data from multiple sources:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.20.1">ComposableRecordReader(imageReader,labelsReader);</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.21.1">Similarly, instead of imageset, in some cases, if someone needs to load MNIST datasets into the model; for that, we can use the following part. </span><span class="koboSpan" id="kobo.21.2">This example uses a random number seed of </span><code class="literal"><span class="koboSpan" id="kobo.22.1">12345</span></code><span class="koboSpan" id="kobo.23.1">:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.24.1">DataSetIterator mnistTrain = new      
MnistDataSetIterator(batchSize,true,12345);
DataSetIterator mnistTest = new     
MnistDataSetIterator(batchSize,false,12345);</span></pre></div><div class="section" title="Model configuration"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec35"/><span class="koboSpan" id="kobo.25.1">Model configuration</span></h2></div></div></div><p><span class="koboSpan" id="kobo.26.1">The next part of the operation is to configure the CNN. </span><span class="koboSpan" id="kobo.26.2">Deeplearning4j provides a simple builder to define the deep neural network layer by layer, setting the different desired hyperparameters:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.27.1">MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder() MultiLayerConfiguration.Builder builder = new  
NeuralNetConfiguration.Builder()
</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.28.1">.seed(seed)
</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.29.1">.iterations(iterations)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.30.1">.regularization(true)
.l2(0.0005)
</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.31.1">.learningRate(0.01)</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.32.1">The first layer, Convolutional layer, can be called using the </span><code class="literal"><span class="koboSpan" id="kobo.33.1">ConvolutionLayer.Builder</span></code><span class="koboSpan" id="kobo.34.1"> method. </span><span class="koboSpan" id="kobo.34.2">The </span><code class="literal"><span class="koboSpan" id="kobo.35.1">.build()</span></code><span class="koboSpan" id="kobo.36.1"> function is used to build the layer. </span><code class="literal"><span class="koboSpan" id="kobo.37.1">.stride()</span></code><span class="koboSpan" id="kobo.38.1"> is used to set the amount of stride for this Convolutional layer:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.39.1">.layer(0, new ConvolutionLayer.Builder(5, 5)</span></strong></span>
</pre><p>
<code class="literal"><span class="koboSpan" id="kobo.40.1">nIn</span></code><span class="koboSpan" id="kobo.41.1"> and </span><code class="literal"><span class="koboSpan" id="kobo.42.1">nOut</span></code><span class="koboSpan" id="kobo.43.1"> signify the depth. </span><code class="literal"><span class="koboSpan" id="kobo.44.1">nIn</span></code><span class="koboSpan" id="kobo.45.1"> here is </span><code class="literal"><span class="koboSpan" id="kobo.46.1">nChannels</span></code><span class="koboSpan" id="kobo.47.1">, and </span><code class="literal"><span class="koboSpan" id="kobo.48.1">nOut</span></code><span class="koboSpan" id="kobo.49.1"> is the number of filters to be applied for the convolution:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.50.1">.nIn(nChannels)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.51.1">.stride(1, 1)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.52.1">.nOut(20)</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.53.1">To add the identity function as the activation function, we will define it in this way:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.54.1">.activation("identity")</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.55.1">.build())</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.56.1">To add a Pooling layer of type Max-pooling, we will call the </span><code class="literal"><span class="koboSpan" id="kobo.57.1">SubsamplingLayer.Builder()</span></code><span class="koboSpan" id="kobo.58.1"> method after the first layer:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.59.1">.layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType
.MAX)
</span><span class="strong"><strong><span class="koboSpan" id="kobo.60.1">.kernelSize(2,2)</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.61.1">.stride(2,2)</span></strong></span><span class="koboSpan" id="kobo.62.1">
.build())</span></pre><p><span class="koboSpan" id="kobo.63.1">The</span><span class="strong"><strong><span class="koboSpan" id="kobo.64.1"> Rectifier Linear Unit</span></strong></span><span class="koboSpan" id="kobo.65.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.66.1">ReLU</span></strong></span><span class="koboSpan" id="kobo.67.1">) layer can be added by calling new </span><code class="literal"><span class="koboSpan" id="kobo.68.1">DenseLayer.Builder().activation("relu")</span></code><span class="koboSpan" id="kobo.69.1">:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.70.1">.layer(4, new DenseLayer.Builder().activation("relu")</span></strong></span><span class="koboSpan" id="kobo.71.1">
.nOut(500).build())</span></pre><p><span class="koboSpan" id="kobo.72.1">The model can be initialized by calling the </span><code class="literal"><span class="koboSpan" id="kobo.73.1">init()</span></code><span class="koboSpan" id="kobo.74.1"> method as follows:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.75.1">MultiLayerNetwork model = new MultiLayerNetwork(getConfiguration());</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.76.1">model.init();</span></strong></span>
</pre></div><div class="section" title="Training and evaluation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec36"/><span class="koboSpan" id="kobo.77.1">Training and evaluation</span></h2></div></div></div><p><span class="koboSpan" id="kobo.78.1">As mentioned in the earlier section, for the training part, we need to divide the whole big dataset into a number of batches. </span><span class="koboSpan" id="kobo.78.2">The model will then work on those batches one by one in Hadoop. </span><span class="koboSpan" id="kobo.78.3">Let's say we divide the dataset into 5,000 batches, each batch of size 1024 examples. </span><span class="koboSpan" id="kobo.78.4">The 1024 examples will then split into multiple numbers of blocks where the workers will work in parallel. </span><span class="koboSpan" id="kobo.78.5">The split operation of the big dataset is done using the </span><code class="literal"><span class="koboSpan" id="kobo.79.1">RecordReaderDataSetIterator()</span></code><span class="koboSpan" id="kobo.80.1"> method. </span><span class="koboSpan" id="kobo.80.2">Let's first initialize the parameters needed to call the method as follows:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.81.1">int batchSize = 1024;  </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.82.1">int seed = 123;</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.83.1">int labelIndex = 4; </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.84.1">int iterations = 1</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.85.1">Let the total number of classes in the image be </span><code class="literal"><span class="koboSpan" id="kobo.86.1">10</span></code><span class="koboSpan" id="kobo.87.1">:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.88.1">int numClasses = 10;</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.89.1">Now, as we have set the number of parameters for </span><code class="literal"><span class="koboSpan" id="kobo.90.1">RecordReaderDataSetIterator()</span></code><span class="koboSpan" id="kobo.91.1">, we can call the method to set up the training platform:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.92.1">DataSetIterator iterator = new RecordReaderDataSetIterator(recordReader,batchSize,labelIndex,numClasses);</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.93.1">DataSet batchData= iterator.next();</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.94.1">batchData.shuffle();</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.95.1">In the training phase, we can randomly split the batch into train and test datasets. </span><span class="koboSpan" id="kobo.95.2">If we want 70 samples for the training set and the rest 30 for the test set, we can set this configuration by using the following:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.96.1">SplitTestAndTrain testAndTrain = batchData.splitTestAndTrain(0.70);  </span></strong></span><span class="koboSpan" id="kobo.97.1">
DataSet trainingData = testAndTrain.getTrain();
DataSet testData = testAndTrain.getTest();
        
trainAndTest =batchData.splitTestAndTrain(0.70);
trainInput = trainAndTest.getTrain();                

testInput.add(trainAndTest.getTest().getFeatureMatrix());</span></pre><p><span class="koboSpan" id="kobo.98.1">When the model is fully trained, for each batch, the test data can be saved so as to validate the model. </span><span class="koboSpan" id="kobo.98.2">Hence, by defining only one object of the </span><code class="literal"><span class="koboSpan" id="kobo.99.1">Evaluation</span></code><span class="koboSpan" id="kobo.100.1"> class, we will be able to collect the statistics of the entire dataset:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.101.1">Evaluation eval = new Evaluation(numOfClasses);</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.102.1">for (int i = 0; i &lt; testInput.size(); i++) 
{</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.103.1">    INDArray output = model.output(testInput.get(i));</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.104.1">    eval.eval(testLabels.get(i), output);</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.105.1">}</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.106.1">The model is now completely ready to be trained. </span><span class="koboSpan" id="kobo.106.2">This can be done by calling the </span><code class="literal"><span class="koboSpan" id="kobo.107.1">fit()</span></code><span class="koboSpan" id="kobo.108.1"> method as follows:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.109.1">model.fit(trainInput);</span></strong></span>
</pre></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec23"/><span class="koboSpan" id="kobo.1.1">Summary</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">CNNs, although not a new concept, has gained immense popularity in the last half a decade. </span><span class="koboSpan" id="kobo.2.2">The network primarily finds its application in the field of vision. </span><span class="koboSpan" id="kobo.2.3">The last few years have seen some major research on CNN by various technological companies such as Google, Microsoft, Apple, and the like, and also from various eminent researchers. </span><span class="koboSpan" id="kobo.2.4">Starting from the beginning, this chapter talked about the concept of convolution, which is the backbone of this type of network. </span><span class="koboSpan" id="kobo.2.5">Going forward, the chapter introduced the various layers of this network. </span><span class="koboSpan" id="kobo.2.6">Then it provided in-depth explanations for every associated layer of the deep CNN. </span><span class="koboSpan" id="kobo.2.7">After that, the various hyperparameters and their relations with the network were explained, both theoretically and mathematically. </span><span class="koboSpan" id="kobo.2.8">Later, the chapter talked about the approach of how to distribute the deep CNN across various machines with the help of Hadoop and its YARN. </span><span class="koboSpan" id="kobo.2.9">The last part discussed how to implement this network using Deeplearning4j for every worker working on each block of Hadoop.</span></p><p><span class="koboSpan" id="kobo.3.1">In the next chapter, we will discuss another popular deep neural network called recurrent neural network. </span><span class="koboSpan" id="kobo.3.2">Recurrent neural network has recently gained immense popularity mainly for its ability to model the sequences of variable length. </span><span class="koboSpan" id="kobo.3.3">Till now, this network is successfully implemented in different problems such as language modeling, handwriting recognition, speech recognition, and so on.</span></p></div></div></div></body></html>