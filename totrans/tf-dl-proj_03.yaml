- en: Caption Generation for Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Caption generation is one of the most important applications in the field of
    deep learning and has gained quite a lot of interest recently. Image captioning
    models involve a combination of both visual information along with natural language
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about:'
  prefs: []
  type: TYPE_NORMAL
- en: Recent advancements in the field of the caption generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How caption generation works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of caption generation models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is caption generation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Caption generation is the task of describing an image with natural language.
    Previously, caption generation models worked on object detection models combined
    with templates that were used to generate text for detected objects. With all
    the advancements in deep learning, these models have been replaced with a combination
    of convolutional neural networks and recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41534487-3f30-46d0-94e2-2c346a61bb88.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: https://arxiv.org/pdf/1609.06647.pdf
  prefs: []
  type: TYPE_NORMAL
- en: There are several datasets that help us create image captioning models.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring image captioning datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several datasets are available for captioning image task. The datasets are usually
    prepared by showing an image to a few persons and asking them to write a sentence
    each about the image. Through this method, several captions are generated for
    the same image. Having multiple options of captions helps in better generalization.
    The difficulty lies in the ranking of model performance. For each generation,
    preferably, a human has to evaluate the caption. Automatic evaluation is difficult
    for this task. Let's explore the `Flickr8` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Flickr8` is gathered from Flickr and is not permitted for commercial usage.
    Download the `Flickr8` dataset from [https://forms.illinois.edu/sec/1713398](https://forms.illinois.edu/sec/1713398).
    The descriptions can be found at [http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html](http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html).
    Download the text and images separately. Access to it can be obtained by filling
    in a form shown on the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cffe58f-56d6-4303-ab4a-5d017ebb7257.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An email will be sent with the download link. Once downloaded and extracted,
    the files should be like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are a couple of examples given in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d427f636-3f7d-4499-a65c-cb6e45b86f7e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding figure shows the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: A man in street racer armor is examining the tire of another racer's motor bike
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two racers drove the white bike down the road
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two motorists are riding along on their vehicle that is oddly designed and colored
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two people are in a small race car driving by a green hill
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two people in racing uniforms in a street car
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is example two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a07da3d8-9578-4836-91b7-a236cc829a20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding figure shows the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: A man in a black hoodie and jeans skateboards down a railing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A man skateboards down a steep railing next to some steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A person is sliding down a brick rail on a snowboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A person walks down the brick railing near a set of steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A snowboarder rides down a handrail without snow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, there are different captions provided for one image. The captions
    show the difficulty of the image captioning task.
  prefs: []
  type: TYPE_NORMAL
- en: Converting words into embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: English words have to be converted into embeddings for caption generation. An
    embedding is nothing but a vector or numerical representation of words or images.
    It is useful if words are converted to a vector form such that arithmetic can
    be performed using the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such an embedding can be learned by two methods, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d11a280-a213-4e63-80a6-2ec7971c36b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **CBOW** method learns the embedding by predicting a word given the surrounding
    words. The **Skip-gram** method predicts the surrounding words given a word, which
    is the reverse of **CBOW**. Based on the history, a target word can be trained,
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/918d0423-dbd7-4279-87d3-9574d66af009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once trained, the embedding can be visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63a42ddb-81c2-4564-9f58-f9fd2d53ce4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of words
  prefs: []
  type: TYPE_NORMAL
- en: This type of embedding can be used to perform vector arithmetic of words. This
    concept of word embedding will be helpful throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Image captioning approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several approaches to captioning images. Earlier methods used to construct
    a sentence based on the objects and attributes present in the image. Later, **recurrent
    neural networks** (**RNN**) were used to generate sentences. The most accurate
    method uses the attention mechanism. Let's explore these techniques and results
    in detail in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional random field
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Initially a method was tried with the **conditional random field** (**CRF**)
    constructing the sentence with the objects and attributes detected in the image.
    The steps involved in this process are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7418c864-3eaf-4ee1-9448-87f293d689ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'System flow for an example images (Source: http://www.tamaraberg.com/papers/generation_cvpr11.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'CRF has limited ability to come up with sentences in a coherent manner. The
    quality of generated sentences is not great, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c80448c0-73d4-402c-8c3c-5167ac91119a.png)'
  prefs: []
  type: TYPE_IMG
- en: The sentences shown here are too structured despite getting the objects and
    attributes correct.
  prefs: []
  type: TYPE_NORMAL
- en: Kulkarni et al., in the paper [http://www.tamaraberg.com/papers/generation_cvpr11.pdf](http://www.tamaraberg.com/papers/generation_cvpr11.pdf),
    proposed a method of finding the objects and attributes from an image and using it
    to generate text with a **conditional random field** (**CRF**).
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural network on convolution neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A recurrent neural network can be combined with convolutional neural network
    features to produce new sentences. This enables end-to-end training of the models.
    The following is the architecture of such a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c66418d9-a2bd-4cf8-8c61-a50790b04846.png)'
  prefs: []
  type: TYPE_IMG
- en: 'LSTM model (Source: https://arxiv.org/pdf/1411.4555.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several layers of **LSTM** used to produce the desired results. A
    few of the results produced by this model are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/477f57f6-4861-4c3d-912a-e77913dd64b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: https://arxiv.org/pdf/1411.4555.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: These results are better than the results produced by CRF. This shows the power
    of LSTM in generating sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reference: Vinyals et al., in the paper [https://arxiv.org/pdf/1411.4555.pdf](https://arxiv.org/pdf/1411.4555.pdf),
    proposed an end to end trainable deep learning for image captioning, which has
    CNN and RNN stacked back to back.'
  prefs: []
  type: TYPE_NORMAL
- en: Caption ranking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Caption ranking is an interesting way of selecting a caption from a set of
    captions. First, the images are ranked according to their features and corresponding
    captions are picked, as shown in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d6c5f49-2c4f-4ad7-8806-66b9de6d2775.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'The top images can be re-ranked using a different set of attributes. By getting
    more images, the quality can improve a lot as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8390766c-a13d-47c9-949c-39f9bd46af62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: The results are better with an increase in the number of images in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about caption ranking, refer: [http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf](http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Dense captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dense captioning is the problem of multiple captions on a single image. The
    following is the architecture of the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96c1aa2c-16be-47f6-8bb5-8d8039b2d61b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: This architecture produces good results.
  prefs: []
  type: TYPE_NORMAL
- en: For more understanding refer: Johnson et al., in the paper [https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf),
    proposed a method for dense captioning.
  prefs: []
  type: TYPE_NORMAL
- en: RNN captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The visual features can be used with sequence learning to form the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bd7bd01-b1a9-43a4-a024-b30cb6a4984d.png)'
  prefs: []
  type: TYPE_IMG
- en: This is an architecture for generating captions.
  prefs: []
  type: TYPE_NORMAL
- en: For details, refer: Donahue et al., in the paper [https://arxiv.org/pdf/1411.4389.pdf](https://arxiv.org/pdf/1411.4389.pdf),
    proposed **Long-term recurrent convolutional architectures** (**LRCN**) for the task of image captioning.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both the image and text can be mapped to the same embedding space to generate
    a caption.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32ad2629-93d0-4a4f-bf63-69e56fc63f33.png)'
  prefs: []
  type: TYPE_IMG
- en: A decoder is required to generate the caption.
  prefs: []
  type: TYPE_NORMAL
- en: Attention-based captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For detailed learning, refer: Xu et al., in the paper, [https://arxiv.org/pdf/1502.03044.pdf](https://arxiv.org/pdf/1502.03044.pdf),
    proposed a method for image captioning using an **attention mechanism**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention-based captioning has become popular recently as it provides better
    accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6f17ce4-f2c2-4668-86e6-2424644885af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This method trains an attention model in the sequence of the caption, thereby
    producing better results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79f3a9f7-3f04-4a27-9375-4f82267d442e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is a diagram of **LSTM** with attention-generating captions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f0220af-d7d0-4281-b1ba-092ef70358f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are several examples shown here, with an excellent visualization of objects
    unfolding in a time series manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed973502-76a8-4952-8809-e25aef013f7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Unfolding objects in time series manner
  prefs: []
  type: TYPE_NORMAL
- en: The results are really excellent!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a caption generation model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s read the dataset and transform it the way we need. Import the `os`
    library and declare the directory in which the dataset is present, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, define a function to open a file and return the lines present in the
    file as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the image paths of the training and testing datasets followed by the captions
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This should print the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the image-to-caption map has to be generated. This will help in training
    for easily looking up captions. Also, unique words present in the caption dataset
    will help to create the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, two maps have to be formed. One is word to index and the other is index
    to word map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The maximum number of words present in a caption is 38, which will help in
    defining the architecture. Next, import the libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create the `ImageModel` class for loading the VGG model with weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The weights are downloaded and stored. It may take some time at the first attempt.
    Next, a separate model is created so that a second fully connected layer is predicted.
    The following is a method for reading an image from a path and preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, define a method to load the image and do prediction. The predicted second
    fully connected layer can be reshaped to `4096`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Go through a list of image paths and create a list of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, store the extracted features as a pickle file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, initialize the class and extract both training and testing image features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the layers required to construct the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the vocabulary required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'For the final caption generation model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'For the language, a model is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The two different models are merged to form the final model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This model can be trained to generate captions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned image captioning techniques. First, we understood
    the embedding space of word vectors. Then, several approaches for image captioning
    were learned. Then came the implementation of the image captioning model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a look at the concept of **Generative Adversarial Networks**
    (**GAN**). GANs are intriguing and useful for generating images for various purposes.
  prefs: []
  type: TYPE_NORMAL
