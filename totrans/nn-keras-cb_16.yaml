- en: Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned about mapping input to a target—where,
    the input and output values are provided. In this chapter, we will be learning
    about reinforcement learning, where the objective that we want to achieve and
    the environment that we operate in are provided, but not any input or output mapping.
    The way in which reinforcement learning works is that we generate input values
    (the state in which the agent is) and the corresponding output values (the reward
    the agent achieves for taking certain actions in a state) by taking random actions
    at the start and gradually learning from the generated input data (actions in
    a state) and output values (rewards achieved by taking certain actions).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The optimal action to take in a simulated game with a non-negative reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The optimal action to take in a state in a simulated game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q-learning to maximize rewards when playing Frozen Lake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Q-learning to balance a cart pole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Q-learning to play the Space Invaders game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The optimal action to take in a simulated game with a non-negative reward
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will understand the way in which we can take the right action
    for a simulated game. Note that this exercise will primarily help you to grasp
    how reinforcement learning works.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's define the environment we are operating in this simulated setting.
  prefs: []
  type: TYPE_NORMAL
- en: You have three boxes, on which two players are playing a game. Player 1 marks
    a box with 1 and player 2 marks one with 2\. The player who is able to mark two
    consecutive boxes wins.
  prefs: []
  type: TYPE_NORMAL
- en: 'The empty board for this game looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6def8c7-0658-4359-b45a-8cae86613db7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the problem we just defined, only player 1 has an opportunity to win the
    game. The possible scenarios in which player 1 wins are either of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/129d43fc-ab45-4bc1-acc2-63b404e4cd47.png)'
  prefs: []
  type: TYPE_IMG
- en: From the problem setting, the intuitive way in which player 1 wins is when player
    1 chooses the middle box. This way, irrespective of which box is chosen by player
    2, player 1 will win on their subsequent move.
  prefs: []
  type: TYPE_NORMAL
- en: While the first step for player 1 is intuitive for us, in the next section we
    will learn about how an agent can automatically figure out the optimal first move.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy that we will adopt to solve this problem is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We initialize an empty board
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Player 1 chooses a box randomly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Player 2 chooses a box randomly from the remaining 2 boxes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depending on the box player 1 is left with, we update the reward for player
    1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If player 1 is able to place 1s in two consecutive boxes, he is a winner and
    will a reward of 1
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, player 1 will get a reward of 0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the preceding exercise 100 times, where the game is played and we store
    a reward for the given sequence of moves
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we will go ahead and calculate the average reward for the various first
    moves that were taken
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The box that was chosen in the first move, that has the highest average reward
    over 100 iterations is the optimal first move for player 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy defined above is coded as follows (the code file is available
    as `Finding_optimal_policy.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the game environment and the function to play the game:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are initializing an empty board with zero values and
    playing a random move named `samp`. Player 1 takes the first move and then player
    2 takes their turn, followed by player 1\. We fill up the empty board in this
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function to calculate the reward at the end of a game:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Play the game `100` times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the reward for choosing a certain first move:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: When you repeat the preceding code for multiple options for the first move,
    you will notice that the average reward is highest when occupying the second square.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal action to take in a state in a simulated game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous scenario, we considered a simplistic case where there is a
    reward when the objective is achieved. In this scenario, we will complicate game
    by having negative rewards too. However, the objective remains the same: maximizing the reward
    in the given problem setting where the environment has both positive and negative
    rewards.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The environment we are working on is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77011aa8-e3e7-41cc-9bf4-452707112100.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We start at the cell with **S** in it and our objective is to reach the cell
    where the reward is **+1**. In order to maximize the chances of achieving the reward,
    we will be using Bellman''s equation, which calculates the value of each cell
    in the preceding grid as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Value of current cell = reward of moving from the current cell to next cell
    + discount factor * value of next cell*'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in the current problem, the reward for moving to any cell other
    than the cell with a reward of **+1** is *0*.
  prefs: []
  type: TYPE_NORMAL
- en: The discount factor can be thought of as the energy expended moving from one
    cell to another. Thus, a cell that is far away from the rewarding cell will have
    a lower value compared to other cells in the current problem setting.
  prefs: []
  type: TYPE_NORMAL
- en: Once we calculate the value of each cell, we move to the cell that has the highest
    value of all the cells that an agent could move to.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy that we''ll adopt to calculate the value of each cell is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize an empty board.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the possible actions that an agent could take in a cell.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the state that an agent will be in, for the action the agent takes in
    the current cell.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the value of the current state, which depends on the reward for moving
    to the next state, as well as the value of the next state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the cell value of the current state based on the earlier calculation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, store the action taken in the current state to move to the next
    state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that, in the initial iterations, the values of cells that are far away
    from the end goal remain zero, while the values of cells that are adjacent to
    the end state rise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we iterate the previous steps multiple times, we will be in a position to
    update the cell values and, thus, in a position to decide the optimal route for
    the agent to follow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll code the strategy that we laid out in the previous
    section (the code file is available as `Finding_optimal_policy.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize an empty board:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the actions that can be taken in different states—where `D` represents
    moving down, `R` is right, `L` is left, and `U` is moving up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the function that extracts the next state given the current state and
    the action taken in the current state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the lists where the state, action, and rewards are appended:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Execute 100 actions at most in an episode (an episode is an instance of a game)
    where random action is taken in a cell (state) and calculate the value of the current
    state based on the reward for moving to the next state, as well as the value of
    next state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat the above exercise for `100` iterations (episodes/games) and calculate
    the value of each cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are taking random actions in a state and then calculating
    the next state for the action taken in the current state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are updating the value of a state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding results in the following final cell state values across all the
    cells:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/695f027e-2888-4b08-b8ca-dba64348fc4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on the preceding output, the agent can take take either the right action
    or the down action at the start of the game (where the agent starts from the top-left
    corner). However, if the agent takes the down action in the first step, it is
    better off taking the *r**ight* action in the next step, as the cell state value
    is higher for the state to the right compared to the state that is above the current
    cell state.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine the environment (the cells and their corresponding rewards) looks as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7b18107-739a-4819-b47b-2a8998b62a92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The actions that could be taken at different states areas follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The cell state values in various cells after iterating through the game multiple
    times is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96dfeacf-52d9-47f3-928c-275ff5b32948.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding results, we can see that the agent is better off taking an
    action down from the top-left corner than moving to its right, as the cell state
    value is higher for the cell state that is below the starting cell.
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning to maximize rewards when playing Frozen Lake
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in the previous sections, we have been taking random actions in a given
    state. Additionally, we have also been defining the environment and calculating
    the next state, actions, and the reward for a move via code. In this section,
    we will leverage OpenAI's Gym package to navigate through the Frozen Lake environment.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Frozen Lake environment looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f4064cc-944d-4d94-ab94-0a5c8b963098.png)'
  prefs: []
  type: TYPE_IMG
- en: The agent starts from the **S** state and the goal is to reach the **G** state
    by avoiding the **H** state as far as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding environment, there are 16 possible states that an agent can
    be in. Additionally, the agent can take four possible actions (move up, down,
    right, or left).
  prefs: []
  type: TYPE_NORMAL
- en: We'll define a q-table where there are 16 rows corresponding to the 16 states
    and four columns corresponding to the four actions that can be taken in each state.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we learned that:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Value of action taken in a state = reward + discount factor * value of the
    best possible action taken in the next state*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll modify the preceding formula as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Value of action taken in a state = value of action taken in a state + 1*(reward
    + discount factor* value of the best possible action taken in the next state - value
    of action taken in a state)*'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we'll replace the 1 with the learning rate, so that a value update
    of an action in a state does not change drastically. This is similar to the effect
    of having learning rates in neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '*Value of action taken in a state = value of action taken in a state + learning
    rate*(reward + discount factor* value of the best possible action taken in the
    next state - value of action taken in a state)*'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding, we can now update the q-table so that we can identify the
    optimal action that can be taken in different states.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy that we''ll adopt to solve this case study is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Register the environment in OpenAI's Gym
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize a zero array q-table shaped as 16 x 4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Employ an exploration-versus-exploitation approach in choosing an action in
    a given state:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we have merely explored possible overall actions as we randomly chose
    an action in a given state.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will explore the initial iterations as we are not sure of
    the optimal action to take during the initial few episodes of the game.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: However, as we learn more about the game, we exploit what we have learned in
    terms of possible actions to take while still taking random actions (with decreasing
    frequency as the number of episodes increases).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a given episode:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose an action depending on whether we try and explore or exploit
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the new state and reward, and check whether the game is over by taking
    the action chosen in the previous step
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize a learning rate parameter and discount factor
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the value of taking the preceding action in a state in the q-table by
    using the formula discussed earlier
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the preceding steps until the game is over
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, repeat the preceding steps for 1,000 different games
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the q-table to identify the optimal action to take in a given state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot the path of an agent as it takes the actions in a state as per the q-table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will code the strategy we discussed earlier (the code file
    is available as `Frozen_Lake_with_Q_Learning.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Gym is a toolkit for developing and comparing reinforcement learning algorithms.
    It supports teaching agents everything from walking to playing games such as Pong and Pinball.
  prefs: []
  type: TYPE_NORMAL
- en: More about Gym can be found at: [https://gym.openai.com/](https://gym.openai.com/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Register the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the created environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/eb411224-67a2-44b0-921e-610d1061e23a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding step renders (prints) the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code provides the number of state action pairs in the environment.
    In our case, given that it is a 4 x 4 grid, we have a total of 16 states. Thus,
    we have a total of 16 observations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code defines the number of actions that can be taken in a state
    in the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code samples an action from the possible set of actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code takes the action and generates the new state and the reward
    of the action, flags whether the game is done, and provides additional information
    for the step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code resets the environment so that the agent is back to the starting
    state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the q-table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We have initialized it to a shape of (16, 4) as there are 16 states and 4 possible
    actions in each state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run multiple iterations of playing a game:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize hyper-parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Play multiple episodes of the game:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the code below, we are defining the action to be taken. If `eps` (which is
    a random number generated between 0 to 1) is less than 0.5, we explore; otherwise,
    we exploit (to consider the best action in a q-table)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code below, we are fetching the new state and the reward, and flag whether
    the game is done by taking the action in the given step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code below, we are updating the q-table based on the action taken in
    a state. Additionally, we are also updating the state with the new state obtained
    after taking action in the current state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the following code, as the game is over, we proceed to a new episode of the game.
    However, we ensure that the randomness factor (`eps`), which is used in deciding
    whether we are going for exploration or exploitation, is updated.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have built the q-table, we now deploy the agent to maneuver in line
    with the optimal actions suggested by the q-table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The preceding gives the optimal path that the agent has to traverse to reach
    the end goal.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-learning to balance a cart pole
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we learned about taking an action based on q-table
    values. However, arriving at an optimal value is time-consuming, as the agent
    would have to play multiple times to arrive at the optimal q-table.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn about using a neural network so that we can arrive
    at the optimal values faster than what we achieved when we used Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this exercise, we will register the cart-pole environment where the possible
    actions are to move either right or left so that we balance the pole. Additionally
    the cart position, cart velocity, pole angle, and pole velocity at the tip is
    the information we have about the states.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rules of this game can be found here: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/).'
  prefs: []
  type: TYPE_NORMAL
- en: A pole is attached to a cart by an un-actuated joint, and the cart moves along
    a frictionless track. The system is controlled by applying a force of +1 or -1
    to the cart. The pendulum starts upright, and the goal is to prevent it from falling
    over. A reward of +1 is provided for every time-step that the pole remains upright.
    The episode ends when the pole is more than 15 degrees from the vertical, or the
    cart moves more than 2.4 units from the center.
  prefs: []
  type: TYPE_NORMAL
- en: In order to balance the cart-pole, we'll adopt the same strategy as we adopted
    in the previous section. However, the difference in deep q-learning is that we'll
    use a neural network to help us predict the optimal action that the agent needs
    to take.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way in which we train the neural network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll store the information on state values, the action taken, and the reward
    achieved:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward will be 1 if the game does not end (is not over) and 0 otherwise.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Initially, the model predicts based on randomly initialized weights, where the
    output layer of the model has two nodes that correspond to the new state's values
    for the two possible actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new state value will be based on the action that maximizes the value of
    new state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the game is not over, we update the current state's value with the sum of
    the reward and the product of the maximum state value of the new state and the
    discount factor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll now override the value of the action from the updated current state''s
    value that we obtained previously:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the action taken in the current step is wrong (that is, the game is over)
    the value of the action in the current state will be 0.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, the value of the target in the current step is a positive number.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This way, we are letting the model figure out the right action to take.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, we can consider this a way to specify that the action is wrong
    when the reward is zero. However, given that we are not sure whether it is the
    right action when the reward is 1, we'll just update it for the action we took
    and leave the new state's value (if we take the other action) untouched.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We append the state values to the input array, and also the values of taking
    one or an other action in the current state as the output array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We fit the model that minimizes the mean-squared error for the preceding data
    points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we keep reducing the exploration over an increasing number of episodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll code the strategy we discussed earlier as follows (the code file is
    available as `Deep_Q_learning_to_balance_a_cart_pole.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the environment and store the action size and state size in variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'A cart-pole environment looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a3a72ea-30b2-4b46-b545-908bc873ef5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the lists that need to be appended:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function that replays the game:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are defining a function that takes the neural network
    model, batch size, and epsilon (the parameter that signifies whether we'll explore
    or exploit). We are fetching a random sample of the size of `batch_size`. Note
    that you will learn about memory structure (which comprises state, action, reward,
    and `next_state`) in the next step. If the game is not done, we are updating the
    reward for taking the action that is taken; otherwise, the target will be 0 (as
    the reward would be 0 when the game is over).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the model predicts the value of taking a certain action (as the
    model has 2 nodes in the output—where each provides the output of taking one action
    over the other). The function returns the updated model and the coefficient of
    exploration/exploitation (epsilon).
  prefs: []
  type: TYPE_NORMAL
- en: 'Play the game over multiple episodes and append the scores obtained by the agent.
    Additionally, ensure that the actions taken by the agent are dictated by the model
    based on the epsilon value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are playing a total of 200 episodes where we are
    resetting the environment at the start of the episode. Additionally, we are reshaping
    the state so that it can be passed to the neural network model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding step, we are taking an action based on the exploration parameter
    (epsilon), where we take a random action (`env.actionspace.sample()`) in certain
    cases, and leverage the model''s predictions in other cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding step, we are performing an action and extracting the next
    state, the reward, and the information about whether the game is over. Additionally,
    we are overwriting the reward value with -10 if the game is over (which means
    the agent made an incorrect move). Further, we are extracting the next state and
    appending it into the memory. This way, we are creating a dataset for the model
    to be trained on, where the model takes the current state and reward to calculate
    the reward for one of the two possible actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, if the game is done, we  append the score (the number
    of steps taken during the game); otherwise, we update the model. Additionally,
    we are updating the model only when memory has as many data points as the pre-defined
    batch size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plotting the scores over increasing epochs looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8b05e8e-9ac2-4b2c-94cc-ee1ca482baad.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep Q-learning to play Space Invaders game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we used Deep Q-learning to play the Cart-Pole game.
    In this section, we will leverage Deep Q-learning to play Space Invaders, which
    is a more complex environment than Cart-Pole.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample screenshot of the Space Invaders game looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61dfcc9b-3271-482d-aef3-0eaebddf9a93.png)'
  prefs: []
  type: TYPE_IMG
- en: source: https://gym.openai.com/envs/SpaceInvaders-v0/
  prefs: []
  type: TYPE_NORMAL
- en: The objective of this exercise is to maximize the score obtained in a single
    game.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy that we''ll adopt to build an agent that is able to maximize the score
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the environment of the *Space Invaders-Atari2600* game.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Preprocess the image frame:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove pixels that do not necessarily impact the action prediction
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, pixels below the location of the player
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalize the input image.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Resize the image before passing it to the neural network model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stack frames as required by the Gym environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let the agent play the game over multiple episodes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the initial episodes, we'll have high exploration which decays over increasing
    episodes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The action that needs to be taken in a state depends on the value of the exploration
    coefficient.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Store the game state and the corresponding reward for the action taken in a
    state in memory.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the model depending on the reward that was received in the previous episodes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy that we discussed earlier is coded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the ROM that contains the Space Invaders game and also install the
    `retro` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the environment and extract the observation space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a function that preprocesses the frame (image/screenshot of the Space
    Invaders game):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a function that stacks frames given a state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the model hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a function that samples data from the total memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a function that returns the action that the agent needs to take:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a function that fine-tunes the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the neural network model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a623396-afa7-491b-96da-6853a83906e6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Loop through multiple episodes and keep playing the game while updating the
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the rewards obtained over increasing episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3f3345e3-c0b6-49bc-9009-b2f000a06908.png)'
  prefs: []
  type: TYPE_IMG
- en: From this, we can see that the model has learned to score over 800 in some episodes.
  prefs: []
  type: TYPE_NORMAL
