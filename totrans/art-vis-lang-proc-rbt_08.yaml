- en: '*Chapter 8*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object Recognition to Guide a Robot Using CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain how object recognition works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a network capable of recognizing objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build an object recognition system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter covers how object recognition works by building a network that
    would be capable of recognizing objects based on a video.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Object recognition** is an area of computer vision where a robot is capable
    of detecting objects in an environment using a camera or sensor that is capable
    of extracting images of the robot''s surroundings. From these images, software
    detects an object within every image and then recognizes the type of object. Machines
    are capable of recognizing objects from an image or a video captured by the robot''s
    sensors. This allows the robot to be aware of their environment.'
  prefs: []
  type: TYPE_NORMAL
- en: If a robot can recognize its environment and obtain this information using object
    recognition, it will be able to perform more complex tasks, such as grabbing objects
    or moving around in an environment. In *Chapter 9*, *Computer Vision for Robotics*,
    we will look at a robot performing these tasks in a virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: The task to be performed here is to detect specific objects within an image
    and recognize those objects. This type of computer vision problem is a bit different
    from the ones that we have looked at earlier in this book. In order to recognize
    a specific object, we have seen that labeling those objects and training a convolutional
    neural network, which was covered in *Chapter 5*, *Convolutional Neural Networks
    for Computer Vision*, which would work fine, but what about detecting these objects
    in the first place?
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we learned that objects we want to recognize have to be labeled
    with the corresponding class they belong to. Hence, in order to detect those objects
    within an image, a rectangle-shaped bounding box has to be drawn around them so
    that their location in the image is properly located. The neural network will
    then predict the bounding boxes and the label of those objects.
  prefs: []
  type: TYPE_NORMAL
- en: Labeling objects with bounding boxes is a tedious, tough task, so we are not
    going to show the process for labeling the images in a dataset with bounding boxes,
    or the process for training a neural network to recognize and detect those objects.
    Nevertheless, there is a library called `labelImg`, which you can access in this
    GitHub repository:ï·Ÿ [https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg).
    This allows you to create bounding boxes for every object within an image. Once
    you have the bounding boxes created, which in terms of data are known as coordinates,
    you can train a neural network to predict the bounding boxes and the corresponding
    label for every object within an image.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be using state-of-the-art methods of the YOLO network,
    which are ready to use and will save you from having to build your own algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Object Recognition and Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multiple object recognition and detection involves detecting and recognizing
    several objects within an image. This task involves labeling every single object
    with a bounding box and then recognizing the type of that object.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, there are many available pre-trained models that detect a lot
    of objects. The neural network called **YOLO** is one of the best models for this
    specific task and works in real time. YOLO will be explained in depth in the next
    chapter for the development of the simulator for the robot.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this chapter, the YOLO network that we want to use is trained to recognize
    and detect 80 different classes. These classes are:'
  prefs: []
  type: TYPE_NORMAL
- en: person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic
    light, fire hydrant, stop_sign, parking meter, bench, bird, cat, dog, horse, sheep,
    cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase,
    frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard,
    surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana,
    apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair,
    couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard,
    cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors,
    teddy bear, hair dryer, toothbrush.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure 8.1, you can see a sample of a street where people, cars, and buses
    have been detected using YOLO:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1: YOLO detection sample](img/C13550_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: YOLO detection sample'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this topic, we are going to build a multiple object recognition and detection
    system for static images.
  prefs: []
  type: TYPE_NORMAL
- en: First, we are going to do so using an OpenCV module called **DNN** (Deep Neural
    Network), which involves a few lines of code. Later on, we will use a library
    called **ImageAI**, which does the same but with less than 10 lines of code and
    will allow you to choose the specific objects you want to detect and recognize.
  prefs: []
  type: TYPE_NORMAL
- en: In order to implement YOLO with OpenCV, you will need to import the image using
    OpenCV, just like we covered in other chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 24: Building Your First Multiple Object Detection and Recognition
    Algorithm'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We are going to use a Google Colab notebook as this task does not involve training
    an algorithm, but rather using one.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we are going to implement a multiple object detection and
    recognition system using YOLO and OpenCV. We are going to code a detector and
    a recognizer system that takes an image as input and detects and recognizes objects
    within that image, then outputs the image with those detections drawn:'
  prefs: []
  type: TYPE_NORMAL
- en: Open up your Google Colab interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the following libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To input an image to this network, we need to use the `blobFromImage` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to load the classes of the dataset, which for YOLO are stored in `Models/yolov3.txt`,
    which you can find in `Chapter 8/Models` on GitHub. We read the classes like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate different colors for different classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the pre-trained model and the config file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an input blob:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the input blob for the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to declare the network, we use the `readNet` method from the `Models/yolov3.weights`,
    which is the weights of the network, and `Models/yolov3.cfg`, which is the architecture
    of the model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The method, class, weight, and architecture files can be found on GitHub in
    the `Lesson08/Models/` folder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have set this up, the only thing that is left in order to recognize
    and detect all the objects within an image is to run and execute the code, which
    is explained next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In order to get the output layers of the network, we declare the method mentioned
    in the following code and then run the interface to obtain the array of output
    layers, which contains several detections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function to draw a bounding box around the detected object with the
    class name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '''outs'' is an array of predictions. Later on in the exercise, we will see
    that we have to loop this array in order to get the bounding boxes and the confidences
    of each detection, along with the type of class.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Object detection algorithms often detect one object several times and that
    is a problem. This problem can be solved by using **non-max suppression**, which
    deletes the bounding boxes for every object with less confidence (the probability
    of the object being in the predicted class), after which the only bounding boxes
    that will remain are the ones with the highest confidence. After detecting the
    bounding boxes and the confidences, and declaring the corresponding thresholds,
    this algorithm can be run as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This step is one of the most important ones. Here, we are going to gather the
    confidence from every detection of every output layer (every object detected),
    the class ID, and the bounding boxes, but we''ll ignore detections with a confidence
    of less than 50%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For each detection from each output layer, get the confidence, the class ID,
    and bounding box params, and ignore weak detections (confidence < 0.5):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We loop over the list of indexes and use the method that we declared for printing
    to print every bounding box, every label, and every confidence on the input image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we show and save the resulting image. OpenCV has a method for showing
    it also; there is no need to use Matplotlib:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.2: YOLO detection sample](img/C13550_08_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.2: YOLO detection sample'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, we have to draw the bounding boxes, its classes, and the confidence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now let''s try some other examples using the steps mentioned previously. You
    can find the images in the `Dataset/obj-det/` folder. The outputs will be as shown
    in Figure 8.3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Image52926.jpg)![Figure 8.3: YOLO detection sample](img/Image52947.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: YOLO detection sample'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ImageAI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is another way to achieve this easily. You could use the **ImageAI** library,
    which is capable of performing object detection and recognition with a few lines
    of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The link to the GitHub repository for this library can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/OlafenwaMoses/ImageAI](https://github.com/OlafenwaMoses/ImageAI)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to install this library, you can do so by using pip with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To use this library, we need to import one class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We import the `ObjectDetection` class, which will work as a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, we declare the object of the class that is going to make the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The model that we are going to use has to be declared. For this library, we
    only get to use three models: RetinaNet, YOLOV3, and TinyYOLOV3\. YOLOV3 is the
    same model we used before and has moderate performance and accuracy with a moderate
    detection time.'
  prefs: []
  type: TYPE_NORMAL
- en: As for RetinaNet, it has higher performance and accuracy but a longer detection
    time.
  prefs: []
  type: TYPE_NORMAL
- en: TinyYOLOV3 is optimized for speed and has moderate performance and accuracy
    but a much faster detection time. This model will be used in the next topic because
    of its speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'You only have to change a couple of lines of code in order to get to work with
    any of these models. For YOLOV3, these lines are needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `.h5` file contains the weights and the architecture for the YOLOV3 neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the inference and get the corresponding detections, only a line of code
    is needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: What this line does is take an image as input and detect the bounding boxes
    of the objects in the image and their classes. It outputs a new image drawn with
    those detections, as well as a list of the detected objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how it detects the `sample.jpg` image that we used in the last exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: ImageAI YOLOV3 image detection](img/Image52955.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: ImageAI YOLOV3 image detection'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ImageAI also allows you to customize which objects you want to recognize. By
    default, it is also capable of detecting the same classes as YOLO, which is built
    using OpenCV, that is the 80 classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can customize it to only detect the objects that you want by passing an
    object as a parameter called `CustomObjects`, where you specify which objects
    you want the model to detect. Also, the method from the detector for recognizing
    those objects changes from `detectObjectsFromImage()` to `detectCustomObjectsFromImage()`.
    It is used like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 8.5: ImageAI YOLOV3 custom image detection](img/C13550_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: ImageAI YOLOV3 custom image detection'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Multiple Object Recognition and Detection in Video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multiple object recognition and detection in static images sounds amazing, but
    what about detecting and recognizing objects in a video?
  prefs: []
  type: TYPE_NORMAL
- en: You can download any video from the internet and try to detect and recognize
    all the objects that show up in the video.
  prefs: []
  type: TYPE_NORMAL
- en: The process to follow would be to get every frame of the video and for every
    frame, detect the corresponding objects and their labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Declare the corresponding libraries first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `imageai` library contains an object that allows the user to apply object
    detection and recognition to the video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We need V`ideoObjectDetection` so that we can detect objects in video. Moreover,
    Matplotlib is needed to show the detection process for every frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6: ImageAI one-frame object detection process](img/C13550_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: ImageAI one-frame object detection process'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we will first need to load the model. You can decide what model to load,
    depending on the speed you need the video to be processed at, with the precision
    required. YOLOV3 is in the middle, between RetinaNet and TinyYOLOV3, RetinaNet
    being the most precise but the slowest and TinyYOLOV3 the least precise but the
    fastest. We are going to stick to the YOLOV3 model but feel free to use the other
    two. The declaration after declaring the video object detection is the same as
    in the last topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Before running the video detector, we need to declare a function that will be
    applied to every frame processed. This function does not perform the detection
    algorithm, but it handles the detection process for every frame. And why do we
    have to handle the output of every frame after the object detection process? That
    is because we want to show the detection process frame by frame using Matplotlib..
  prefs: []
  type: TYPE_NORMAL
- en: 'Before declaring that method, we need to declare the colors that the objects
    will be printed on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are going to declare the method applied to every frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'First, as shown, the function is declared and the number of the frame, the
    array of detections, the number of occurrences of every object detected, and the
    frame are passed to it. Also, we declare the corresponding variables that we are
    going to use to print all the detections on every frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In this loop, the objects and their corresponding occurrences are stored. The
    colors that represent every object are also stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In this last piece of code, two plots are printed for every frame: one showing
    the image with the corresponding detections and the other with a chart containing
    the number of occurrences of every object detected and its percentage of the total
    of occurrences.'
  prefs: []
  type: TYPE_NORMAL
- en: This output is shown in Figure 8.6.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last cell, in order to execute the video detector, we write this couple
    of lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The first line initializes the Matplotlib plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second line starts the video detection. The arguments passed to the function
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_file_path`: The input video path'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_file_path`: The output video path'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frames_per_second`: Frames per second of the output video'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`per_frame_function`: The callback function after every process of detecting
    objects within a frame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minimum_percentage_probability`: The minimum probability value threshold,
    where only detections with the highest confidence are considered'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_detected_frame`: If set to True, the callback function receives the
    frame as a parameter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_progress`: If set to True, the process is logged in the console'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activity 8: Multiple Object Detection and Recognition in Video'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we are going to process a video frame by frame, detecting
    all possible objects within every frame and saving the output video to disk:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The video we will be using for this activity is uploaded on GitHub, in the
    `Dataset/videos/street.mp4` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Url : [https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson08/Dataset/videos/street.mp4](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson08/Dataset/videos/street.mp4)'
  prefs: []
  type: TYPE_NORMAL
- en: Open a Google Colab notebook, mount the disk, and navigate to where chapter
    8 is located.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install the library in the notebook, as it is not preinstalled, by using this
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Import the necessary libraries for the development of this activity and set
    `matplotlib`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Declare the model that you are going to use for detecting and recognizing objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find that information here:[https://github.com/OlafenwaMoses/ImageAI/blob/master/imageai/Detection/VIDEO.md](https://github.com/OlafenwaMoses/ImageAI/blob/master/imageai/Detection/VIDEO.md)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Also note that all models are stored in the `Models` folder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Declare the callback method that is going to be called after every frame is
    processed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run Matplotlib and the video detection processes on the `street.mp4` video that
    is inside the `Dataset/videos/` folder. You can also try out the `park.mp4` video,
    which is in the same directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity is available on page 326.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object recognition and detection is capable of identifying several objects within
    an image, to draw bounding boxes around those objects and predict the types of
    object they are.
  prefs: []
  type: TYPE_NORMAL
- en: The process of labeling the bounding boxes and their labels has been explained,
    but not in depth, due to the huge process required. Instead, we used state-of-the-art
    models to recognize and detect those objects.
  prefs: []
  type: TYPE_NORMAL
- en: YOLOV3 was the main model used in this chapter. OpenCV was used to explain how
    to run an object detection pipeline using its DNN module. ImageAI, an alternative
    library for object detection and recognition, has shown its potential for writing
    an object detection pipeline with a few lines and easy object customization.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the ImageAI object detection pipeline was put into practice by using
    a video, where every frame obtained from the video was passed through that pipeline
    to detect and identify objects from those frames and show them using Matplotlib.
  prefs: []
  type: TYPE_NORMAL
