- en: Recurrent and Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we have been studying feed-forward networks, where the data moves
    in one direction and there is no interconnection of nodes in each layer. In the
    presence of basic hypotheses that interact with some problems, the intrinsic unidirectional
    structure of feed-forward networks is strongly limiting. However, it is possible
    to start from it and create networks in which the results of computing one unit
    affect the computational process of the other. It is evident that algorithms that
    manage the dynamics of these networks must meet new convergence criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will introduce **Recurrent Neural Networks** (**RNN**),
    which are networks with cyclic data flows. We will also see **Convolutional Neural
    Networks** (**CNN**), which are standardized neural networks mainly used for image
    recognition. For both of these types of networks, we will do some sample implementations
    in R. The following topics are covered:'
  prefs: []
  type: TYPE_NORMAL
- en: RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `rnn` package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long Short-Term Memory** (**LSTM**) model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common CNN architecture--**LeNet**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the chapter, we will understand training, testing, and evaluating
    an RNN. We will learn how to visualize the RNN model in R environment. We will
    also be able to train an LSTM model. We will cover the concepts as CNN and common
    CNN architecture--LeNet.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Within the set of **Artificial Neural Networks** (**ANN**), there are several
    variants based on the number of hidden layers and data flow. One of the variants
    is RNN, where the connections between neurons can form a cycle. Unlike feed-forward
    networks, RNNs can use internal memory for their processing. RNNs are a class
    of ANNs that feature connections between hidden layers that are propagated through
    time in order to learn sequences. RNN use cases include the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: Stock market predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image captioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weather forecast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time-series-based forecasts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handwriting recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio or video processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robotics action sequencing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The networks we have studied so far (feed-forward networks) are based on input
    data that is powered to the network and converted into output. If it is a supervised
    learning algorithm, the output is a label that can recognize the input. Basically,
    these algorithms connect raw data to specific categories by recognizing patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent networks, on the other hand, take as their input not only current
    input data that is powered to the network but also what they have experienced
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: The decision made by a recurrent network at a specific instant affects the decision
    it will reach immediately afterwards. So, recurrent networks have two input sources--the
    present and the recent past--that combine to determine how they respond to new
    data, just as people do in life everyday.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recurrent networks are distinguished from feed-forward networks thanks to the
    feedback loop linked to their past decisions, thus accepting their output momentarily
    as inputs. This feature can be emphasized by saying that recurrent networks have
    memory. Adding memory to neural networks has a purpose: there is information in
    the sequence itself and recurrent networks use it to perform the tasks that feed-forward
    networks cannot.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RNN is a class of neural network where there are connections between neurons
    that form a directed cycle. A typical RNN is represented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00120.gif)'
  prefs: []
  type: TYPE_IMG
- en: Here, the output of one instance is taken as input for the next instance for
    the same neuron. The way the data is kept in memory and flows at different time
    periods makes RNNs powerful and successful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under RNNs, there are more variants in the way the data flows backwards:'
  prefs: []
  type: TYPE_NORMAL
- en: Fully recurrent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopfield
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elman and Jordan networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural history compressor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gated Recurrent Unit** (**GRU**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectional
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent MLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recurrent networks are designed to recognize patterns as a sequence of data
    and are helpful in prediction and forecasting. They can work on text, images,
    speech, and time series data. RNNs are among the powerful ANNs and represent the
    biological brain, including memory with processing power. Recurrent networks take
    inputs from the current input (like a feed-forward network) and the output that
    was calculated previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00121.gif)'
  prefs: []
  type: TYPE_IMG
- en: To understand this better, we consider the RNN as a network of neural networks,
    and the cyclic nature is **unfolded** in the following manner. The state of a
    neuron *h* is considered at different time periods (*-t-1*, *t*, *t+1* and so
    on) until convergence or the total number of epochs is reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vanilla is the first model of recurrent ANNs that was introduced. A vanilla
    RNN is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00122.gif)'
  prefs: []
  type: TYPE_IMG
- en: Other variants such as GRU or LSTM networks are more widespread given the simplicity
    of implementation, and they have demonstrated remarkable performance in a wide
    range of applications involving sequences such as language modeling, speech recognition,
    image captioning, and automatic translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs can be implemented in R through the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rnn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MxNetR`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorFlow` for R'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNNs are mainly used for sequence modeling. The inputs and outputs are treated
    as vectors (a matrix of numbers). For another level of understanding of RNNs,
    I advise you to go through the character sequencing example by Andrej Karpathy.
  prefs: []
  type: TYPE_NORMAL
- en: The features of RNN make it like an ANN with memory. The ANN memory is more
    like the human brain. With memory, we can make machines think from scratch and
    learn from their "memory." RNNs are basically ANNs with loops that allow information
    to persist in the network. The looping allows information to be passed from state
    t to state *t+1*.
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the preceding diagram, RNNs can be thought of as multiple copies
    of the same ANN, with the output of one passing on as input to the next one. When
    we persist the information, as the patterns change, RNN is able to predict the
    *t+1* value. This is particularly useful for analyzing time-series-based problems.
  prefs: []
  type: TYPE_NORMAL
- en: There is no specific labeling required; the value that is part of the input
    forms the time series variable, and RNN can learn the pattern and do the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The internal state of the RNN is updated for every time step of the learning
    process. The feed-forward mechanism in RNN is similar to ANN; however, the backpropagation
    is an error term correction following something called **Backpropagation Through
    Time** (**BPTT**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Backpropagation through time follows this pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: Unfold the RNN to contain *n* feed-forward networks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the weights *w* to random values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the following until the stopping criteria is met or you are done with
    the required number of epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set inputs to each network with values as *x[i.]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forward-propagate the inputs over the whole unfolded network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Back-propagate the error over the unfolded network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update all the weights in the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Average out the weights to find the final weight in the folded network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rnn package in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To implement RNN in an R environment, we can use the `rnn` package available
    through CRAN. This package is widely used to implement an RNN. A brief description
    of the `rnn` package, extracted from the official documentation, is shown in the
    following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **rnn**: Recurrent Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| **Description**: |'
  prefs: []
  type: TYPE_TB
- en: '| Implementation of an RNN in R |'
  prefs: []
  type: TYPE_TB
- en: '| **Details**: |'
  prefs: []
  type: TYPE_TB
- en: '| Package: `rnn` Type: Package'
  prefs: []
  type: TYPE_NORMAL
- en: 'Version: 0.8.0'
  prefs: []
  type: TYPE_NORMAL
- en: 'Date: 2016-09-11'
  prefs: []
  type: TYPE_NORMAL
- en: 'License: GPL-3 |'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Authors**: |'
  prefs: []
  type: TYPE_TB
- en: '| Bastiaan Quast Dimitri Fichou |'
  prefs: []
  type: TYPE_TB
- en: 'The main functions used from the `rnn` package are shown in this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `predict_rnn` | Predicts the output of an RNN model:`predict_rnn(model, X,
    hidden = FALSE, real_output = T, ...)` |'
  prefs: []
  type: TYPE_TB
- en: '| `run.rnn_demo` | A function to launch the `rnn_demo` app:`run.rnn_demo(port
    = NULL)` |'
  prefs: []
  type: TYPE_TB
- en: '| `trainr` | This trains the RNN. The model is used by the `predictr` function.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `predictr` | This predicts the output of an RNN model:`predictr(model, X,
    hidden = FALSE, real_output = T, ...)` |'
  prefs: []
  type: TYPE_TB
- en: As always, to be able to use a library, we must first install and then load
    it into our script.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them. This function should be
    used only once and not every time you run the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s install and load the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When we load the library (`library("rnn")`), we may receive the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Do not worry, as it''s nothing serious! R is just saying that, in order to
    run the `rnn` library, you also need to install the `digest` library. Remember
    it; in future, if such a problem happens, you now know how to solve it. Just add
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can launch the demo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run `run.rnn_demo()` after installing the `rnn` package, we can access
    a web page through `127.0.0.1:5876`, which allows us to run a demo of an RNN with
    preset values and also visually see how the parameters influence an RNN, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00123.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we will be able to set the parameters of our network and choose
    the appropriate values to be inserted into the boxes via its labels. The following
    parameters must be set correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '`time dimension`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training sample dimension`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`testing sample dimension`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`number of hidden layers`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Number of unit in the layer number 1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Number of unit in the layer number 2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learningrate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batchsize`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numepochs`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`momentum`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learningrate_decay`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After doing this, we just have to click on the train button and the command
    will be built and trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the results of the simulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00124.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `trainr` and `predictr` functions are the most important functions in the
    `rnn` package. The `trainr()` function trains a model with the set of `X` and
    `Y` parameters, which can be used for prediction using the `predictr()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `trainr()` function takes the following parameters. The output is a model
    that can be used for prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `Y` | Array of output values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dim 1`: Samples (must be equal to dim 1 of `X`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim 2`: Time (must be equal to dim 2 of `X`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim 3`: Variables (could be one or more, if a matrix, will be coerced to an
    array)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `X` | Array of input values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dim 1`: Samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim 2`: Time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim 3`: Variables (could be one or more; if it is a matrix, will be coerced
    to an array)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `learningrate` | Learning rate to be applied for weight iteration. |'
  prefs: []
  type: TYPE_TB
- en: '| `learningrate_decay` | Coefficient to apply to the learning rate at each
    epoch via the `epoch_annealing` function. |'
  prefs: []
  type: TYPE_TB
- en: '| `momemtum` | The coefficient of the last weight iteration to keep for faster
    learning. |'
  prefs: []
  type: TYPE_TB
- en: '| `hidden_dim` | The dimensions of the hidden layers. |'
  prefs: []
  type: TYPE_TB
- en: '| `network_type` | The type of network, which could be `rnn`, `gru` or `lstm`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `numepochs` | The number of iterations, that is, the number of times the
    whole dataset is presented to the network |'
  prefs: []
  type: TYPE_TB
- en: '| `sigmoid` | Method to be passed to the `sigmoid` function. |'
  prefs: []
  type: TYPE_TB
- en: '| `batch size` | Number of samples used at each weight iteration. Only one
    is supported for the moment. |'
  prefs: []
  type: TYPE_TB
- en: '| `epoch_function` | Vector of functions to be applied at each epoch loop.
    Use it to interact with the objects inside the list model or to print and plot
    at each epoch. It should return the model. |'
  prefs: []
  type: TYPE_TB
- en: '| `loss function` | Applied in each sample loop, vocabulary to verify. |'
  prefs: []
  type: TYPE_TB
- en: '| `...` | Arguments to be passed to methods, to be used in user defined functions.
    |'
  prefs: []
  type: TYPE_TB
- en: Now let's look at a simple example. This example included is in the official
    documentation of the CRAN `rnn` package to demonstrate the `trainr` and `predictr`
    functions and see the accuracy of the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: We have `X1` and `X` with random numbers in the range *0-127*. `Y` is initialized
    as `X1+X2`. After converting `X1`, `X2`, and `Y` to binary values, we use `trainr`
    to train `Y` based on `X(array of X1 and X2)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the model, we predict `B` based on another sample of `A1+A2`. The difference
    of errors is plotted as a histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, we will analyze the code line by line, explaining in detail all the
    features applied to capture the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line of the initial code are used to load the library needed to run
    the analysis. Let''s go to the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: These lines create training response numbers; these two vectors will be the
    inputs of the network we are about to build. We have used the `sample()` function
    to take a sample of the specified size from the elements of `x` either with or
    without replacement. The two vectors contain 7,000 random integer values between
    `1` and `127`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This command creates training response numbers; this is our target, or what
    we want to predict with the help of the network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'These three lines of code convert integers into binary sequences. We need to
    transform numbers into binaries before adding bit by bit. In the end, we get a
    sequence of eight values for each value, these values being `0` or `1`. To understand
    the transformation we analyze a preview of one of these variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go back to analyze the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This code creates a 3D array as required by the `trainr()` function. In this
    array, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dim 1`: Samples (must be equal to `dim 1` of inputs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim 2`: Time (must be equal to `dim 2` of inputs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim 3`: Variables (could be one or more; if it is a matrix, this will be coerced
    to the array)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `trainr()` function trains an RNN in native R. It takes a few minutes as
    the training happens based on `X` and `Y`. The following code shows the last 10
    trained epoch results displayed on the R prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the evolution of the algorithm by charting the error made by the
    algorithm to subsequent epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This graph shows the epoch versus error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00125.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Now the model is ready and we can use it to test the network. But first, we
    need to create some test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let us run the prediction for new data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert back to integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, plot the differences as a histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The histogram of errors is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00126.gif)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen here, the bin with more frequent is near zero to indicate that
    in most cases, the predictions coincide with the current values. All the other
    bins are related to the errors. We can therefore say that the network simulates
    the system with good performance.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen that RNNs have a memory that uses persistent previous information
    to be used in the current neural network processing. The previous information
    is used in the present task. However, the memory is short-term and we do not have
    a list of all of the previous information available for the neural node.
  prefs: []
  type: TYPE_NORMAL
- en: When we introduce a long-term memory into the RNN, we are able to remember a
    lot of previous information and use it for the current processing. This concept
    is called LSTM model of RNN, which has numerous use cases in video, audio, text
    prediction, and various other applications.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs were introduced by Hochreiter & Schmidhuber in 1997.
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM network is trained using **BPTT** and diminishes the vanishing gradient
    problem. LSTMs have powerful applications in time series predictions and can create
    large, recurrent networks to address difficult sequence problems in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM have **gates** that make the long/short term memory possible. These are
    contained in memory blocks connected through layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00127.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'There are three types of gates within a unit:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Gate:** Scales input to cell (write)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output Gate**: Scales output to cell (read)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forget Gate**: Scales old cell value (reset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each gate is like a switch that controls the read/write, thus incorporating
    the long-term memory function into the LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMs can be used to solve the following sequence prediction problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Direct sequence prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence to sequence prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key differences between GRU and LSTM are:'
  prefs: []
  type: TYPE_NORMAL
- en: A GRU has two gates, whereas an LSTM has three gates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GRUs don't possess any internal memory that is different from the exposed hidden
    state. They don't have the output gate, which is present in LSTMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no second nonlinearity applied when computing the output in GRU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another important set of neural networks in deep learning is CNN. They are designed
    specifically for image recognition and classification. CNNs have multiple layers
    of neural networks that extract information from images and determine the class
    they fall into.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a CNN can detect whether the image is a cat or not if it is trained
    with a set of images of cats. We will see the architecture and working of CNN
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: For a program, any image is a just a set of RGB numbers in a vector format.
    If we can make a neural network understand the pattern, it can form a CNN and
    detect images.
  prefs: []
  type: TYPE_NORMAL
- en: Regular neural nets are universal mathematical approximators that take an input,
    transform it through a series of functions, and derive the output. However, these
    regular neural networks do not scale well for an image analysis. For a 32 x 32
    pixel RGB image, the hidden layer would have *32*32*3=3072* weights. The regular
    neural nets work fine for this case. However, when the RGB image is scaled to
    size *200 x 200* pixel, the number of weights required in the hidden layer is
    *200*200*3=120,000* and the network does not perform well.
  prefs: []
  type: TYPE_NORMAL
- en: Enter CNN to solve this scalability problem. In CNN, the layers of a CNN have
    neurons arranged in three dimensions (**height**, **width**, and **depth**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a neural net and a CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00128.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'CNN is a sequence of layers of neural nets, wherein each layer transforms one
    volume of activations to another through a differentiable function. There are
    three types of layers that build the CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step #1 – filtering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The convolutional layer does the heavy math operations. In computer vision,
    a typical approach to processing an image is to convolute it with a filter to
    extract only the salient features in it. This is the first operation in a CNN.
    The input image is applied a filter logic to create an **activation map** or **feature
    map**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00129.gif)'
  prefs: []
  type: TYPE_IMG
- en: The convoluted feature vector is created by applying the kernel vector on each
    3 x 3 vector of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical steps for filtering are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Line up the feature and the image patch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply each image pixel by the corresponding feature pixel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add them up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide each sum by the total number of pixels in the feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the filtering is done, the next step is to compress the filtered pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step #2 – pooling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this step, we shrink the image stack. For each feature obtained in the convolutional
    step, we build up a matrix and now find the maximum in each chosen matrix to shrink
    the entire input. The steps are below:'
  prefs: []
  type: TYPE_NORMAL
- en: Pick a window size (usually 2 or 3).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick a stride moving range of pixels (usually 2).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Slide the window across the filtered images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each window, we take the maximum value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the slid window does not have the required number of cells as in the previous
    windows, we take whatever values are available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step #3 – ReLU for normalization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this step, we take the pooling output and for each pixel and apply the ReLU
    normalization to tweak the values. If any of the values is negative, we make it
    zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step #4 – voting and classification in the fully connected layer'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final layer is the fully connected layer and there is voting by the set
    of values to determine the class of the output. The fully connected layer is just
    a merged matrix of all the previous outputs.
  prefs: []
  type: TYPE_NORMAL
- en: This is the final layer and the output is determined based on the highest voted
    category.
  prefs: []
  type: TYPE_NORMAL
- en: By stacking up the layers in steps 1, 2, and 3, we form the convolution network,
    which can reduce the error term with backpropagation to give us the best prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The layers can be repeated multiple times and each layer output forms an input
    to the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'A classical CNN architecture would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'An example classification prediction using CNN is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00131.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will see an implementation of CNN using R in [Chapter 7](part0123.html#3L9L60-263fb608a19f4bb5955f37a7741ba5c4),
    *Use Cases of Neural Networks – Advanced Topics*.
  prefs: []
  type: TYPE_NORMAL
- en: Common CNN architecture - LeNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LeNet-5 is a convolutional network designed by Le Cun in the 1990s for handwritten
    and machine-printed character recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is the first successful application of convolutional networks. It has the
    following architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Humidity forecast using RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the first use case of RNNs, we see how we can train and predict an RNN using
    the `trainr()` function*.* Our purpose is to forecast the humidity of a certain
    location as a function of the day. The input file contains daily weather observations
    from multiple Australian weather stations. These observations are obtained from
    the Australian Commonwealth Bureau of Meteorology and are subsequently processed
    to create a relatively large sample dataset for illustrating analytics, data mining,
    and data science using R and the rattle.data package. The `weatherAUS` dataset
    is regularly updated and updates of this package usually correspond to updates
    to this dataset. The data is updated from the Bureau of Meteorology website. The
    `locationsAUS` dataset records the location of each weather station. The source
    dataset is copyrighted by the Australian Commonwealth Bureau of Meteorology and
    is used with permission.
  prefs: []
  type: TYPE_NORMAL
- en: 'A CSV version of this dataset is available at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://rattle.togaware.com/weatherAUS.csv](https://rattle.togaware.com/weatherAUS.csv)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `weatherAUS` dataset is a dataframe containing over 140,000 daily observations
    from over 45 Australian weather stations. This dataset contains the following
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Date`: The date of observation (a `Date` object).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Location`: The common name of the location of the weather station.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MinTemp`: The minimum temperature in degrees celsius.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MaxTemp`: The maximum temperature in degrees celsius.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Rainfall`: The amount of rainfall recorded for the day in mm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Evaporation`: The so-called class a pan evaporation (mm) in the 24 hours to
    9 a.m.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sunshine`: The number of hours of bright sunshine in the day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WindGustDir`: The direction of the strongest wind gust in the 24 hours to
    midnight.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WindGustSpeed`: The speed (km/h) of the strongest wind gust in the 24 hours
    to midnight.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Temp9am`: Temperature (degrees C) at 9 a.m.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RelHumid9am`: Relative humidity (percent) at 9 a.m.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Cloud9am`: Fraction of the sky obscured by clouds at 9 a.m. This is measured
    in oktas, which are a unit of eighths. It records how many eighths of the sky
    are obscured by cloud. A zero measure indicates completely clear sky whilst an
    8 indicates that it is completely overcast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WindSpeed9am`: Wind speed (km/hr) averaged over 10 minutes prior to 9 a.m.
    6 `weatherAUS`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pressure9am`: Atmospheric pressure (hpa) reduced to mean sea level at 9 a.m.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Temp3pm`: Temperature (degrees C) at 3 p.m.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RelHumid3pm`: Relative humidity (percent) at 3 p.m.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Cloud3pm`: Fraction of sky obscured by cloud (in oktas: eighths) at 3 p.m.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WindSpeed3pm`: Wind speed (km/hr) averaged over 10 minutes prior to 3 p.m.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pressure3pm`: Atmospheric pressure (hpa) reduced to mean sea level at 3 p.m.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChangeTemp`: Change in temperature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChangeTempDir`: Direction of change in temperature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChangeTempMag`: Magnitude of change in temperature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChangeWindDirect`: Direction of wind change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MaxWindPeriod`: Period of maximum wind.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RainToday`: Integer 1 if precipitation (mm) in the 24 hours to 9 a.m. exceeds
    1 mm, and 0 otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TempRange`: Difference between minimum and maximum temperatures (degrees C)
    in the 24 hours to 9 a.m.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PressureChange`: Change in pressure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RISK_MM`: The amount of rain. A kind of measure of the risk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RainTomorrow`: The target variable. Will it rain tomorrow?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our case, we will use only two of the many variables contained in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Date`: The date of observation (a `Date` object)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RelHumid9am`: Relative humidity (percent) at 9 a.m'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As said previously, the objective of this example is to forecast the humidity
    of a certain location as a function of the day. Here is the code that we will
    use in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin analyzing the code line by line, explaining in detail all the features
    applied to capture the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The first two lines of the initial code are used to load the libraries needed
    to run the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them. This function should be
    used only once and not every time you run the code.
  prefs: []
  type: TYPE_NORMAL
- en: The `rattle.data` library contains the datasets used as default examples by
    the `rattle` package. The datasets themselves can be used independently of the
    `rattle` package to illustrate analytics, data mining, and data science tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `rnn` library contains several functions for implementing an RNN in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'With this command, we upload the dataset named `weatherAUS`, as mentioned,
    contained in the `rattle.data` library. In the second line, the `view` function
    is used to invoke a spreadsheet-style data viewer on the dataframe object, as
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00133.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Returning to the code, as before, we use only two variables. In addition, the
    dataset contains data from different locations in Australia. We will limit our
    study to the first location (`Albury`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get a preliminary data analysis using the `summary()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `summary()` function returns a set of statistics for each variable. In
    particular, it is useful to highlight the result provided for the `Humidity9am`
    variable; this represents our target. For this variable, nine cases of missing
    value were detected. To remove the missing values, we will use the `na.omit()`
    function; it drops any rows with missing values and forgets them forever:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'With the second line of code, we limit our analysis to the first `3000` observations.
    Now we must set the input and the output data to the format required by the `trainr()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In this way, `x` will represent our input and `y` our target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'With this piece of code we construct a matrix of `30` lines and `100` columns
    with the data available. Recall is a size setting required for the function we
    will use for model building. We can now standardize this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'For this example, we have used the min-max method (usually called feature scaling)
    to get all the scaled data in the range *[0,1]*. The formula for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00134.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'During the normalization, we must calculate the minimum and maximum values
    of each database column. Then we transpose the matrix obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In these lines of code, the dataset is split into `70:30`, with the intention
    of using `70` percent of the data at our disposal to train the network and the
    remaining `30` percent to test the network. Now is the time to build and train
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `trainr()` function trains an RNN in R environment. We have used `16` neurons
    in the hidden layer and the number of epochs is `1,000`. The `trainr()` function
    takes a few minutes as the training happens based on `X` and `Y`. Here are the
    last 10 `Trained epoch` results as displayed on the R prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the evolution of the algorithm by charting the error made by the
    algorithm to subsequent epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This graph shows the **epoch** versus **error**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00135.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We finally have the network trained and ready for use; now we can use it to
    make our predictions. Remember, we''ve set aside 30 percent of the available data
    to test the network. It''s time to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to compare the results, let''s plot a graph showing the moisture content
    in the test set and the predicted results in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the actual values and predicted values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00136.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'From the analysis of the figure, it is possible to note one thing: the data
    is adapted to a good approximation to indicate that the model is able to predict
    the humidity conditions with good performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw RNNs and how to use internal memory for their processing.
    We also covered CNNs, which are standardized neural networks mainly used for image
    recognition. For RNNs, we studied some sample implementations in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned how to train, test, and evaluate an RNN. We also learned how to
    visualize the RNN model in an R environment. We discovered the LSTM model. We
    introduced the concepts as CNN and a common CNN architecture: LeNet.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see more use cases involving R implementations
    of neural networks and deep learning.
  prefs: []
  type: TYPE_NORMAL
