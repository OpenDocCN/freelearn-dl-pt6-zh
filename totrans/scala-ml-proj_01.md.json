["```py\nval train = \"data/insurance_train.csv\"\n```", "```py\nval spark = SparkSessionCreate.createSession()\nimport spark.implicits._\n```", "```py\nimport org.apache.spark.sql.SparkSession \n\nobject SparkSessionCreate { \n  def createSession(): SparkSession = { \n    val spark = SparkSession \n      .builder \n      .master(\"local[*]\") // adjust accordingly \n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") //change accordingly \n      .appName(\"MySparkSession\") //change accordingly \n      .getOrCreate() \n    return spark \n    }\n} \n```", "```py\n24/01/2018 11:11:10 \nERROR util.Shell: Failed to locate the winutils binary in the hadoop binary path\njava.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n```", "```py\n val trainInput = spark.read \n    .option(\"header\", \"true\") \n    .option(\"inferSchema\", \"true\") \n    .format(\"com.databricks.spark.csv\") \n    .load(train) \n    .cache \n```", "```py\nPrintln(trainInput.printSchema()) \nroot \n |-- id: integer (nullable = true) \n |-- cat1: string (nullable = true) \n |-- cat2: string (nullable = true) \n |-- cat3: string (nullable = true) \n  ... \n |-- cat115: string (nullable = true) \n |-- cat116: string (nullable = true)\n  ... \n |-- cont14: double (nullable = true) \n |-- loss: double (nullable = true) \n```", "```py\nprintln(df.count())\n>>>\n 188318 \n```", "```py\ndf.select(\"id\", \"cat1\", \"cat2\", \"cat3\", \"cont1\", \"cont2\", \"cont3\", \"loss\").show() \n>>> \n```", "```py\ndf.select(\"cat109\", \"cat110\", \"cat112\", \"cat113\", \"cat116\").show() \n>>> \n```", "```py\nval newDF = df.withColumnRenamed(\"loss\", \"label\") \n```", "```py\nnewDF.createOrReplaceTempView(\"insurance\") \n```", "```py\nspark.sql(\"SELECT avg(insurance.label) as AVG_LOSS FROM insurance\").show()\n>>>\n+------------------+\n| AVG_LOSS |\n+------------------+\n|3037.3376856699924|\n+------------------+\n```", "```py\nspark.sql(\"SELECT min(insurance.label) as MIN_LOSS FROM insurance\").show() \n>>>  \n+--------+\n|MIN_LOSS|\n+--------+\n| 0.67|\n+--------+\n```", "```py\nspark.sql(\"SELECT max(insurance.label) as MAX_LOSS FROM insurance\").show() \n>>> \n+---------+\n| MAX_LOSS|\n+---------+\n|121012.25|\n+---------+\n```", "```py\nimport org.apache.spark.ml.feature.{ StringIndexer, StringIndexerModel}\nimport org.apache.spark.ml.feature.VectorAssembler\n```", "```py\nvar trainSample = 1.0 \nvar testSample = 1.0 \nval train = \"data/insurance_train.csv\" \nval test = \"data/insurance_test.csv\" \nval spark = SparkSessionCreate.createSession() \nimport spark.implicits._ \nprintln(\"Reading data from \" + train + \" file\") \n\n val trainInput = spark.read \n        .option(\"header\", \"true\") \n        .option(\"inferSchema\", \"true\") \n        .format(\"com.databricks.spark.csv\") \n        .load(train) \n        .cache \n\n    val testInput = spark.read \n        .option(\"header\", \"true\") \n        .option(\"inferSchema\", \"true\") \n        .format(\"com.databricks.spark.csv\") \n        .load(test) \n        .cache \n```", "```py\nprintln(\"Preparing data for training model\") \nvar data = trainInput.withColumnRenamed(\"loss\", \"label\").sample(false, trainSample) \n```", "```py\nvar DF = data.na.drop() \nif (data == DF) \n  println(\"No null values in the DataFrame\")     \nelse{ \n  println(\"Null values exist in the DataFrame\") \n  data = DF \n} \nval seed = 12345L \nval splits = data.randomSplit(Array(0.75, 0.25), seed) \nval (trainingData, validationData) = (splits(0), splits(1)) \n```", "```py\ntrainingData.cache \nvalidationData.cache \n```", "```py\nval testData = testInput.sample(false, testSample).cache \n```", "```py\ndef isCateg(c: String): Boolean = c.startsWith(\"cat\") \ndef categNewCol(c: String): String = if (isCateg(c)) s\"idx_${c}\" else c \n```", "```py\ndef removeTooManyCategs(c: String): Boolean = !(c matches \"cat(109$|110$|112$|113$|116$)\")\n```", "```py\ndef onlyFeatureCols(c: String): Boolean = !(c matches \"id|label\") \n```", "```py\nval featureCols = trainingData.columns \n    .filter(removeTooManyCategs) \n    .filter(onlyFeatureCols) \n    .map(categNewCol) \n```", "```py\nval stringIndexerStages = trainingData.columns.filter(isCateg) \n      .map(c => new StringIndexer() \n      .setInputCol(c) \n      .setOutputCol(categNewCol(c)) \n      .fit(trainInput.select(c).union(testInput.select(c)))) \n```", "```py\nval assembler = new VectorAssembler() \n    .setInputCols(featureCols) \n    .setOutputCol(\"features\")\n```", "```py\nimport org.apache.spark.ml.regression.{LinearRegression, LinearRegressionModel} \nimport org.apache.spark.ml.{ Pipeline, PipelineModel } \nimport org.apache.spark.ml.evaluation.RegressionEvaluator \nimport org.apache.spark.ml.tuning.ParamGridBuilder \nimport org.apache.spark.ml.tuning.CrossValidator \nimport org.apache.spark.sql._ \nimport org.apache.spark.sql.functions._ \nimport org.apache.spark.mllib.evaluation.RegressionMetrics \n```", "```py\nval spark = SparkSessionCreate.createSession() \nimport spark.implicits._ \n```", "```py\nval numFolds = 10 \nval MaxIter: Seq[Int] = Seq(1000) \nval RegParam: Seq[Double] = Seq(0.001) \nval Tol: Seq[Double] = Seq(1e-6) \nval ElasticNetParam: Seq[Double] = Seq(0.001) \n```", "```py\nval model = new LinearRegression()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"label\") \n```", "```py\nprintln(\"Building ML pipeline\") \nval pipeline = new Pipeline()\n         .setStages((Preproessing.stringIndexerStages  \n         :+ Preproessing.assembler) :+ model)\n```", "```py\nval paramGrid = new ParamGridBuilder() \n      .addGrid(model.maxIter, MaxIter) \n      .addGrid(model.regParam, RegParam) \n      .addGrid(model.tol, Tol) \n      .addGrid(model.elasticNetParam, ElasticNetParam) \n      .build() \n```", "```py\nprintln(\"Preparing K-fold Cross Validation and Grid Search: Model tuning\") \nval cv = new CrossValidator() \n      .setEstimator(pipeline) \n      .setEvaluator(new RegressionEvaluator) \n      .setEstimatorParamMaps(paramGrid) \n      .setNumFolds(numFolds) \n```", "```py\nprintln(\"Training model with Linear Regression algorithm\") \nval cvModel = cv.fit(Preproessing.trainingData) \n```", "```py\nprintln(\"Evaluating model on train and validation set and calculating RMSE\") \nval trainPredictionsAndLabels = cvModel.transform(Preproessing.trainingData)\n                .select(\"label\", \"prediction\")\n                .map { case Row(label: Double, prediction: Double) \n                => (label, prediction) }.rdd \n\nval validPredictionsAndLabels = cvModel.transform(Preproessing.validationData)\n                                .select(\"label\", \"prediction\")\n                                .map { case Row(label: Double, prediction: Double) \n                                => (label, prediction) }.rdd \n\nval trainRegressionMetrics = new RegressionMetrics(trainPredictionsAndLabels) \nval validRegressionMetrics = new RegressionMetrics(validPredictionsAndLabels) \n```", "```py\nval bestModel = cvModel.bestModel.asInstanceOf[PipelineModel] \n```", "```py\nval results = \"n=====================================================================n\" + s\"Param trainSample: ${Preproessing.trainSample}n\" + \n      s\"Param testSample: ${Preproessing.testSample}n\" + \n      s\"TrainingData count: ${Preproessing.trainingData.count}n\" + \n      s\"ValidationData count: ${Preproessing.validationData.count}n\" + \n      s\"TestData count: ${Preproessing.testData.count}n\" +      \"=====================================================================n\" +   s\"Param maxIter = ${MaxIter.mkString(\",\")}n\" + \n      s\"Param numFolds = ${numFolds}n\" +      \"=====================================================================n\" +   s\"Training data MSE = ${trainRegressionMetrics.meanSquaredError}n\" + \n      s\"Training data RMSE = ${trainRegressionMetrics.rootMeanSquaredError}n\" + \n      s\"Training data R-squared = ${trainRegressionMetrics.r2}n\" + \n      s\"Training data MAE = ${trainRegressionMetrics.meanAbsoluteError}n\" + \n      s\"Training data Explained variance = ${trainRegressionMetrics.explainedVariance}n\" +      \"=====================================================================n\" +   s\"Validation data MSE = ${validRegressionMetrics.meanSquaredError}n\" + \n      s\"Validation data RMSE = ${validRegressionMetrics.rootMeanSquaredError}n\" + \n      s\"Validation data R-squared = ${validRegressionMetrics.r2}n\" + \n      s\"Validation data MAE = ${validRegressionMetrics.meanAbsoluteError}n\" + \n      s\"Validation data Explained variance = ${validRegressionMetrics.explainedVariance}n\" + \n      s\"CV params explained: ${cvModel.explainParams}n\" + \n      s\"LR params explained: ${bestModel.stages.last.asInstanceOf[LinearRegressionModel].explainParams}n\" +      \"=====================================================================n\" \n```", "```py\nprintln(results)\n>>> \nBuilding Machine Learning pipeline \nReading data from data/insurance_train.csv file \nNull values exist in the DataFrame \nTraining model with Linear Regression algorithm\n===================================================================== \nParam trainSample: 1.0 \nParam testSample: 1.0 \nTrainingData count: 141194 \nValidationData count: 47124 \nTestData count: 125546 \n===================================================================== \nParam maxIter = 1000 \nParam numFolds = 10 \n===================================================================== \nTraining data MSE = 4460667.3666198505 \nTraining data RMSE = 2112.0292059107164 \nTraining data R-squared = -0.1514435541595276 \nTraining data MAE = 1356.9375609756164 \nTraining data Explained variance = 8336528.638733305 \n===================================================================== \nValidation data MSE = 4839128.978963534 \nValidation data RMSE = 2199.802031766389 \nValidation data R-squared = -0.24922962724089603 \nValidation data MAE = 1356.419484419514 \nValidation data Explained variance = 8724661.329105612 \nCV params explained: estimator: estimator for selection (current: pipeline_d5024480c670) \nestimatorParamMaps: param maps for the estimator (current: [Lorg.apache.spark.ml.param.ParamMap;@2f0c9855) \nevaluator: evaluator used to select hyper-parameters that maximize the validated metric (current: regEval_00c707fcaa06) \nnumFolds: number of folds for cross validation (>= 2) (default: 3, current: 10) \nseed: random seed (default: -1191137437) \nLR params explained: aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2) \nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0, current: 0.001) \nfeaturesCol: features column name (default: features, current: features) \nfitIntercept: whether to fit an intercept term (default: true) \nlabelCol: label column name (default: label, current: label) \nmaxIter: maximum number of iterations (>= 0) (default: 100, current: 1000) \npredictionCol: prediction column name (default: prediction) \nregParam: regularization parameter (>= 0) (default: 0.0, current: 0.001) \nsolver: the solver algorithm for optimization. If this is not set or empty, default value is 'auto' (default: auto) \nstandardization: whether to standardize the training features before fitting the model (default: true) \ntol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6, current: 1.0E-6) \nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined) \n===================================================================== \n```", "```py\nprintln(\"Run prediction on the test set\") \ncvModel.transform(Preproessing.testData) \n      .select(\"id\", \"prediction\") \n      .withColumnRenamed(\"prediction\", \"loss\") \n      .coalesce(1) // to get all the predictions in a single csv file \n      .write.format(\"com.databricks.spark.csv\")\n      .option(\"header\", \"true\") \n      .save(\"output/result_LR.csv\")\n```", "```py\nimport org.apache.spark.ml.regression.{GBTRegressor, GBTRegressionModel} \nimport org.apache.spark.ml.{Pipeline, PipelineModel} \nimport org.apache.spark.ml.evaluation.RegressionEvaluator \nimport org.apache.spark.ml.tuning.ParamGridBuilder \nimport org.apache.spark.ml.tuning.CrossValidator \nimport org.apache.spark.sql._ \nimport org.apache.spark.sql.functions._ \nimport org.apache.spark.mllib.evaluation.RegressionMetrics \n```", "```py\nval NumTrees = Seq(5, 10, 15) \nval MaxBins = Seq(5, 7, 9) \nval numFolds = 10 \nval MaxIter: Seq[Int] = Seq(10) \nval MaxDepth: Seq[Int] = Seq(10) \n```", "```py\nval spark = SparkSessionCreate.createSession() \nimport spark.implicits._ \n```", "```py\nval model = new GBTRegressor()\n                .setFeaturesCol(\"features\")\n                .setLabelCol(\"label\") \n```", "```py\nval pipeline = new Pipeline().setStages((Preproessing.stringIndexerStages :+ Preproessing.assembler) :+ model) \n```", "```py\nval paramGrid = new ParamGridBuilder() \n      .addGrid(model.maxIter, MaxIter) \n      .addGrid(model.maxDepth, MaxDepth) \n      .addGrid(model.maxBins, MaxBins) \n      .build() \n```", "```py\nprintln(\"Preparing K-fold Cross Validation and Grid Search\") \nval cv = new CrossValidator() \n      .setEstimator(pipeline) \n      .setEvaluator(new RegressionEvaluator) \n      .setEstimatorParamMaps(paramGrid) \n      .setNumFolds(numFolds) \n```", "```py\nprintln(\"Training model with GradientBoostedTrees algorithm \") \nval cvModel = cv.fit(Preproessing.trainingData) \n```", "```py\nprintln(\"Evaluating model on train and test data and calculating RMSE\") \nval trainPredictionsAndLabels = cvModel.transform(Preproessing.trainingData).select(\"label\", \"prediction\").map { case Row(label: Double, prediction: Double) => (label, prediction) }.rdd \n\nval validPredictionsAndLabels = cvModel.transform(Preproessing.validationData).select(\"label\", \"prediction\").map { case Row(label: Double, prediction: Double) => (label, prediction) }.rdd \n\nval trainRegressionMetrics = new RegressionMetrics(trainPredictionsAndLabels) \nval validRegressionMetrics = new RegressionMetrics(validPredictionsAndLabels) \n```", "```py\nval bestModel = cvModel.bestModel.asInstanceOf[PipelineModel] \n```", "```py\nval featureImportances = bestModel.stages.last.asInstanceOf[GBTRegressionModel].featureImportances.toArray \nval FI_to_List_sorted = featureImportances.toList.sorted.toArray  \n```", "```py\nval output = \"n=====================================================================n\" + s\"Param trainSample: ${Preproessing.trainSample}n\" + \n      s\"Param testSample: ${Preproessing.testSample}n\" + \n      s\"TrainingData count: ${Preproessing.trainingData.count}n\" + \n      s\"ValidationData count: ${Preproessing.validationData.count}n\" + \n      s\"TestData count: ${Preproessing.testData.count}n\" +      \"=====================================================================n\" +   s\"Param maxIter = ${MaxIter.mkString(\",\")}n\" + \n      s\"Param maxDepth = ${MaxDepth.mkString(\",\")}n\" + \n      s\"Param numFolds = ${numFolds}n\" +      \"=====================================================================n\" +   s\"Training data MSE = ${trainRegressionMetrics.meanSquaredError}n\" + \n      s\"Training data RMSE = ${trainRegressionMetrics.rootMeanSquaredError}n\" + \n      s\"Training data R-squared = ${trainRegressionMetrics.r2}n\" + \n      s\"Training data MAE = ${trainRegressionMetrics.meanAbsoluteError}n\" + \n      s\"Training data Explained variance = ${trainRegressionMetrics.explainedVariance}n\" +      \"=====================================================================n\" +    s\"Validation data MSE = ${validRegressionMetrics.meanSquaredError}n\" + \n      s\"Validation data RMSE = ${validRegressionMetrics.rootMeanSquaredError}n\" + \n      s\"Validation data R-squared = ${validRegressionMetrics.r2}n\" + \n      s\"Validation data MAE = ${validRegressionMetrics.meanAbsoluteError}n\" + \n      s\"Validation data Explained variance = ${validRegressionMetrics.explainedVariance}n\" +      \"=====================================================================n\" +   s\"CV params explained: ${cvModel.explainParams}n\" + \n      s\"GBT params explained: ${bestModel.stages.last.asInstanceOf[GBTRegressionModel].explainParams}n\" + s\"GBT features importances:n ${Preproessing.featureCols.zip(FI_to_List_sorted).map(t => s\"t${t._1} = ${t._2}\").mkString(\"n\")}n\" +      \"=====================================================================n\" \n```", "```py\nprintln(results)\n >>> \n===================================================================== \nParam trainSample: 1.0 \nParam testSample: 1.0 \nTrainingData count: 141194 \nValidationData count: 47124 \nTestData count: 125546 \n===================================================================== \nParam maxIter = 10 \nParam maxDepth = 10 \nParam numFolds = 10 \n===================================================================== \nTraining data MSE = 2711134.460296872 \nTraining data RMSE = 1646.5522950385973 \nTraining data R-squared = 0.4979619968485668 \nTraining data MAE = 1126.582534126603 \nTraining data Explained variance = 8336528.638733303 \n===================================================================== \nValidation data MSE = 4796065.983773314 \nValidation data RMSE = 2189.9922337244293 \nValidation data R-squared = 0.13708582379658474 \nValidation data MAE = 1289.9808960385383 \nValidation data Explained variance = 8724866.468978886 \n===================================================================== \nCV params explained: estimator: estimator for selection (current: pipeline_9889176c6eda) \nestimatorParamMaps: param maps for the estimator (current: [Lorg.apache.spark.ml.param.ParamMap;@87dc030) \nevaluator: evaluator used to select hyper-parameters that maximize the validated metric (current: regEval_ceb3437b3ac7) \nnumFolds: number of folds for cross validation (>= 2) (default: 3, current: 10) \nseed: random seed (default: -1191137437) \nGBT params explained: cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false) \ncheckpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations (default: 10) \nfeaturesCol: features column name (default: features, current: features) \nimpurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance) \nlabelCol: label column name (default: label, current: label) \nlossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: squared, absolute (default: squared) \nmaxBins: Max number of bins for discretizing continuous features. Must be >=2 and >= number of categories for any categorical feature. (default: 32) \nmaxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5, current: 10) \nmaxIter: maximum number of iterations (>= 0) (default: 20, current: 10) \nmaxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. (default: 256) \nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0) \nminInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1\\. (default: 1) \npredictionCol: prediction column name (default: prediction) \nseed: random seed (default: -131597770) \nstepSize: Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default: 0.1) \nsubsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0) \nGBT features importance: \n   idx_cat1 = 0.0 \n   idx_cat2 = 0.0 \n   idx_cat3 = 0.0 \n   idx_cat4 = 3.167169394850417E-5 \n   idx_cat5 = 4.745749854188828E-5 \n... \n   idx_cat111 = 0.018960701085054904 \n   idx_cat114 = 0.020609596772820878 \n   idx_cat115 = 0.02281267960792931 \n   cont1 = 0.023943087007850663 \n   cont2 = 0.028078353534251005 \n   ... \n   cont13 = 0.06921704925937068 \n   cont14 = 0.07609111789104464 \n===================================================================== \n```", "```py\nprintln(\"Run prediction over test dataset\") \ncvModel.transform(Preproessing.testData) \n      .select(\"id\", \"prediction\") \n      .withColumnRenamed(\"prediction\", \"loss\") \n      .coalesce(1) \n      .write.format(\"com.databricks.spark.csv\") \n      .option(\"header\", \"true\") \n      .save(\"output/result_GBT.csv\") \n```", "```py\nimport org.apache.spark.ml.regression.{RandomForestRegressor, RandomForestRegressionModel} \nimport org.apache.spark.ml.{ Pipeline, PipelineModel } \nimport org.apache.spark.ml.evaluation.RegressionEvaluator \nimport org.apache.spark.ml.tuning.ParamGridBuilder \nimport org.apache.spark.ml.tuning.CrossValidator \nimport org.apache.spark.sql._ \nimport org.apache.spark.sql.functions._ \nimport org.apache.spark.mllib.evaluation.RegressionMetrics \n```", "```py\nval spark = SparkSessionCreate.createSession() \nimport spark.implicits._ \n```", "```py\nval NumTrees = Seq(5,10,15)  \nval MaxBins = Seq(23,27,30)  \nval numFolds = 10  \nval MaxIter: Seq[Int] = Seq(20) \nval MaxDepth: Seq[Int] = Seq(20) \n```", "```py\nval model = new RandomForestRegressor().setFeaturesCol(\"features\").setLabelCol(\"label\")\n```", "```py\nprintln(\"Building ML pipeline\") \nval pipeline = new Pipeline().setStages((Preproessing.stringIndexerStages :+ Preproessing.assembler) :+ model) \n```", "```py\nval paramGrid = new ParamGridBuilder() \n      .addGrid(model.numTrees, NumTrees) \n      .addGrid(model.maxDepth, MaxDepth) \n      .addGrid(model.maxBins, MaxBins) \n      .build() \n```", "```py\nprintln(\"Preparing K-fold Cross Validation and Grid Search: Model tuning\") \nval cv = new CrossValidator() \n      .setEstimator(pipeline) \n      .setEvaluator(new RegressionEvaluator) \n      .setEstimatorParamMaps(paramGrid) \n      .setNumFolds(numFolds) \n```", "```py\nprintln(\"Training model with Random Forest algorithm\")  \nval cvModel = cv.fit(Preproessing.trainingData) \n```", "```py\nprintln(\"Evaluating model on train and validation set and calculating RMSE\") \nval trainPredictionsAndLabels = cvModel.transform(Preproessing.trainingData).select(\"label\", \"prediction\").map { case Row(label: Double, prediction: Double) => (label, prediction) }.rdd \n\nval validPredictionsAndLabels = cvModel.transform(Preproessing.validationData).select(\"label\", \"prediction\").map { case Row(label: Double, prediction: Double) => (label, prediction) }.rdd \n\nval trainRegressionMetrics = new RegressionMetrics(trainPredictionsAndLabels) \nval validRegressionMetrics = new RegressionMetrics(validPredictionsAndLabels) \n```", "```py\nval bestModel = cvModel.bestModel.asInstanceOf[PipelineModel]\n```", "```py\nval featureImportances = bestModel.stages.last.asInstanceOf[RandomForestRegressionModel].featureImportances.toArray \nval FI_to_List_sorted = featureImportances.toList.sorted.toArray  \n```", "```py\nval output = \"n=====================================================================n\" + s\"Param trainSample: ${Preproessing.trainSample}n\" + \n      s\"Param testSample: ${Preproessing.testSample}n\" + \n      s\"TrainingData count: ${Preproessing.trainingData.count}n\" + \n      s\"ValidationData count: ${Preproessing.validationData.count}n\" + \n      s\"TestData count: ${Preproessing.testData.count}n\" +      \"=====================================================================n\" +   s\"Param maxIter = ${MaxIter.mkString(\",\")}n\" + \n      s\"Param maxDepth = ${MaxDepth.mkString(\",\")}n\" + \n      s\"Param numFolds = ${numFolds}n\" +      \"=====================================================================n\" +   s\"Training data MSE = ${trainRegressionMetrics.meanSquaredError}n\" + \n      s\"Training data RMSE = ${trainRegressionMetrics.rootMeanSquaredError}n\" + \n      s\"Training data R-squared = ${trainRegressionMetrics.r2}n\" + \n      s\"Training data MAE = ${trainRegressionMetrics.meanAbsoluteError}n\" + \n      s\"Training data Explained variance = ${trainRegressionMetrics.explainedVariance}n\" +      \"=====================================================================n\" +   s\"Validation data MSE = ${validRegressionMetrics.meanSquaredError}n\" + \n      s\"Validation data RMSE = ${validRegressionMetrics.rootMeanSquaredError}n\" + \n      s\"Validation data R-squared = ${validRegressionMetrics.r2}n\" + \n      s\"Validation data MAE = ${validRegressionMetrics.meanAbsoluteError}n\" + \n      s\"Validation data Explained variance =\n${validRegressionMetrics.explainedVariance}n\" +      \"=====================================================================n\" +   s\"CV params explained: ${cvModel.explainParams}n\" + \n      s\"RF params explained: ${bestModel.stages.last.asInstanceOf[RandomForestRegressionModel].explainParams}n\" + \n      s\"RF features importances:n ${Preproessing.featureCols.zip(FI_to_List_sorted).map(t => s\"t${t._1} = ${t._2}\").mkString(\"n\")}n\" +      \"=====================================================================n\" \n```", "```py\nprintln(results)\n>>>Param trainSample: 1.0\n Param testSample: 1.0\n TrainingData count: 141194\n ValidationData count: 47124\n TestData count: 125546\n Param maxIter = 20\n Param maxDepth = 20\n Param numFolds = 10\n Training data MSE = 1340574.3409399686\n Training data RMSE = 1157.8317412042081\n Training data R-squared = 0.7642745310548124\n Training data MAE = 809.5917285994619\n Training data Explained variance = 8337897.224852404\n Validation data MSE = 4312608.024875177\n Validation data RMSE = 2076.6819749001475\n Validation data R-squared = 0.1369507149716651\"\n Validation data MAE = 1273.0714382935894\n Validation data Explained variance = 8737233.110450774\n```", "```py\nprintln(\"Run prediction on the test set\") \ncvModel.transform(Preproessing.testData) \n      .select(\"id\", \"prediction\") \n      .withColumnRenamed(\"prediction\", \"loss\") \n      .coalesce(1) // to get all the predictions in a single csv file                 \n      .write.format(\"com.databricks.spark.csv\") \n      .option(\"header\", \"true\") \n      .save(\"output/result_RF.csv\") \n```", "```py\n// Estimator algorithm \nval model = new RandomForestRegressor() \n                    .setFeaturesCol(\"features\") \n                    .setLabelCol(\"label\") \n                    .setImpurity(\"gini\") \n                    .setMaxBins(20) \n                    .setMaxDepth(20) \n                    .setNumTrees(50) \nfittedModel = rf.fit(trainingData) \n```", "```py\nfittedModel.write.overwrite().save(\"model/RF_model\")  \nval sameModel = CrossValidatorModel.load(\"model/RF_model\") \n```", "```py\nsameModel.transform(Preproessing.testData) \n    .select(\"id\", \"prediction\") \n    .withColumnRenamed(\"prediction\", \"loss\") \n    .coalesce(1) \n    .write.format(\"com.databricks.spark.csv\") \n    .option(\"header\", \"true\") \n    .save(\"output/result_RF_reuse.csv\") \n```", "```py\nval cvModel = cv.fit(Preproessing.trainingData)   \n```", "```py\ncvModel.write.overwrite().save(\"model/RF_model\") \n```", "```py\n//Then we restore the same model back:\nval sameCV = CrossValidatorModel.load(\"model/RF_model\") \nNow when you try to restore the same model, Spark will automatically pick the best one. Finally, we reuse this model for making a prediction as follows:\nsameCV.transform(Preproessing.testData) \n      .select(\"id\", \"prediction\") \n      .withColumnRenamed(\"prediction\", \"loss\") \n      .coalesce(1) \n      .write.format(\"com.databricks.spark.csv\") \n      .option(\"header\", \"true\") \n      .save(\"output/result_RF_reuse.csv\")  \n```", "```py\n  ./bin/spark-submit \\\n      --class <main-class> \\\n      --master <master-url> \\\n      --deploy-mode <deploy-mode> \\\n      --conf <key>=<value> \\\n       ... # other options\n       <application-jar> \\\n       [application-arguments]\n```", "```py\n./bin/spark-submit \\\n   --class com.packt.ScalaML.InsuranceSeverityClaim.AllstateClaimsSeverityRandomForestRegressor\\\n   --master spark://207.184.161.138:7077 \\\n   --executor-memory 20G \\\n   --total-executor-cores 100 \\\n   /path/to/examples.jar\n```"]