- en: Chapter 10. Policy Gradient Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the final chapter of this book, we're going to introduce algorithms that
    directly optimize the policy network in reinforcement learning. These algorithms
    are collectively referred to as *policy gradient methods*. Since the policy network
    is directly optimized during training, the policy gradient methods belong to the
    family of *on-policy* reinforcement learning algorithms. Like value-based methods
    that we discussed in [Chapter 9](ch09.html "Chapter 9. Deep Reinforcement Learning"),
    *Deep Reinforcement Learning*, policy gradient methods can also be implemented
    as deep reinforcement learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: A fundamental motivation in studying the policy gradient methods is addressing
    the limitations of Q-Learning. We'll recall that Q-Learning is about selecting
    the action that maximizes the value of the state. With Q function, we're able
    to determine the policy that enables the agent to decide on which action to take
    for a given state. The chosen action is simply the one that gives the agent the
    maximum value. In this respect, Q-Learning is limited to a finite number of discrete
    actions. It's not able to deal with continuous action space environments. Furthermore,
    Q-Learning is not directly optimizing the policy. In the end, reinforcement learning
    is about finding that optimal policy that the agent will be able to use to decide
    on which action it should take in order to maximize the return.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, policy gradient methods are applicable to environments with discrete
    or continuous action spaces. In addition, the four policy gradient methods that
    we will be presenting in this chapter are directly optimizing the performance
    measure of the policy network. This results in a trained policy network that the
    agent can use to act in its environment optimally.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the goal of this chapter is to present:'
  prefs: []
  type: TYPE_NORMAL
- en: The policy gradient theorem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Four policy gradient methods: **REINFORCE**, **REINFORCE with baseline**, **Actor-Critic**,
    and **Advantage Actor-Critic**(**A2C**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A guide on how to implement the policy gradient methods in Keras in a continuous
    action space environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy gradient theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed in [Chapter 9](ch09.html "Chapter 9. Deep Reinforcement Learning"),
    *Deep Reinforcement Learning*, in Reinforcement Learning the agent is situated
    in an environment that is in state *s*[t''], an element of state space ![Policy
    gradient theorem](img/B08956_10_001.jpg). The state space ![Policy gradient theorem](img/B08956_10_001.jpg)
    may be discrete or continuous. The agent takes an action *a*[t] from the action
    space ![Policy gradient theorem](img/B08956_10_002.jpg) by obeying the policy,
    ![Policy gradient theorem](img/B08956_10_003.jpg). ![Policy gradient theorem](img/B08956_10_002.jpg)
    may be discrete or continuous. Because of executing the action *a*[t], the agent
    receives a reward *r* [t+1] and the environment transitions to a new state *s*
    [t+1]. The new state is dependent only on the current state and action. The goal
    of the agent is to learn an optimal policy ![Policy gradient theorem](img/B08956_10_004.jpg)
    that maximizes the *return* from all the states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradient theorem](img/B08956_10_17.jpg) (Equation 9.1.1)'
  prefs: []
  type: TYPE_IMG
- en: 'The return, ![Policy gradient theorem](img/B08956_10_095.jpg), is defined as
    the discounted cumulative reward from time *t* until the end of the episode or
    when the terminal state is reached:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradient theorem](img/B08956_10_19.jpg) (Equation 9.1.2)'
  prefs: []
  type: TYPE_IMG
- en: From *Equation 9.1.2*, the return can also be interpreted as a value of a given
    state by following the policy ![Policy gradient theorem](img/B08956_10_005.jpg).
    It can be observed from *Equation 9.1.1* that future rewards have lower weights
    compared to immediate rewards since generally ![Policy gradient theorem](img/B08956_10_006.jpg)
    where ![Policy gradient theorem](img/B08956_10_007.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have only considered learning the policy by optimizing a value-based
    function, *Q(s,a)*. Our goal in this chapter is to directly learn the policy by
    parameterizing ![Policy gradient theorem](img/B08956_10_008.jpg). By parameterization,
    we can use a neural network to learn the policy function. Learning the policy
    means that we are going to maximize a certain objective function, ![Policy gradient
    theorem](img/B08956_10_009.jpg) which is a performance measure with respect to
    parameter ![Policy gradient theorem](img/B08956_10_010.jpg). In episodic reinforcement
    learning, the performance measure is the value of the start state. In the continuous
    case, the objective function is the average reward rate.
  prefs: []
  type: TYPE_NORMAL
- en: Maximizing the objective function ![Policy gradient theorem](img/B08956_10_009.jpg)
    is achieved by performing *gradient ascent*. In gradient ascent, the gradient
    update is in the direction of the derivative of the function being optimized.
    So far, all our loss functions are optimized by minimization or by performing
    *gradient descent*. Later, in the Keras implementation, we're able to see that
    the gradient ascent can be performed by simply negating the objective function
    and performing gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of learning the policy directly is that it can be applied to
    both discrete and continuous action spaces. For discrete action spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradient theorem](img/B08956_10_026.jpg) (Equation 10.1.1)'
  prefs: []
  type: TYPE_IMG
- en: 'In that formula, *a*[*i*] is the *i*-th action. *a*[*i*] can be the prediction
    of a neural network or a linear function of state-action features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradient theorem](img/B08956_10_011.jpg) (Equation 10.1.2)'
  prefs: []
  type: TYPE_IMG
- en: '![Policy gradient theorem](img/B08956_10_012.jpg) is any function such as an
    encoder that converts the state-action to features.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradient theorem](img/B08956_10_013.jpg) determines the probability
    of each *a*[*i*]. For example, in the cartpole balancing problem in the previous
    chapter, the goal is to keep the pole upright by moving the cart along the 2D
    axis to the left or to the right. In this case, *a*[*0*] and *a*[*1*] are the
    probabilities of the left and right movements respectively. In general, the agent
    takes the action with the highest probability, ![Policy gradient theorem](img/B08956_10_014.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: For continuous action spaces, ![Policy gradient theorem](img/B08956_10_015.jpg)
    samples an action from a probability distribution given the state. For example,
    if the continuous action space is the range ![Policy gradient theorem](img/B08956_10_016.jpg),
    then ![Policy gradient theorem](img/B08956_10_17.jpg) is usually a Gaussian distribution
    whose mean and standard deviation are predicted by the policy network. The predicted
    action is a sample from this Gaussian distribution. To ensure that no invalid
    prediction is generated, the action is clipped between *-1.0* and *1.0*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, for continuous action spaces, the policy is a sample from a Gaussian
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradient theorem](img/B08956_10_018.jpg) (Equation 10.1.3)'
  prefs: []
  type: TYPE_IMG
- en: 'The mean, ![Policy gradient theorem](img/B08956_10_019.jpg), and standard deviation,
    ![Policy gradient theorem](img/B08956_10_020.jpg), are both functions of the state
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradient theorem](img/B08956_10_021.jpg) (Equation 10.1.4)'
  prefs: []
  type: TYPE_IMG
- en: '![Policy gradient theorem](img/B08956_10_022.jpg) (Equation 10.1.5)'
  prefs: []
  type: TYPE_IMG
- en: '![Policy gradient theorem](img/B08956_10_023.jpg) is any function that converts
    the state to its features. ![Policy gradient theorem](img/B08956_10_024.jpg) is
    the *softplus* function that ensures positive values of standard deviation. One
    way of implementing the state feature function, ![Policy gradient theorem](img/B08956_10_023.jpg),
    is using the encoder of an autoencoder network. At the end of this chapter, we
    will train an autoencoder and use the encoder part as the state feature function.
    Training a policy network is therefore a matter of optimizing the parameters ![Policy
    gradient theorem](img/B08956_10_025.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a continuously differentiable policy function, ![Policy gradient theorem](img/B08956_10_015.jpg),
    the policy gradient can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradient theorem](img/B08956_10_027.jpg) (Equation 10.1.6)'
  prefs: []
  type: TYPE_IMG
- en: '*Equation 10.1.6* is also known as the *policy gradient theorem*. It is applicable
    to both discrete and continuous action spaces. The gradient with respect to the
    parameter ![Policy gradient theorem](img/B08956_10_010.jpg) is computed from the
    natural logarithm of the policy action sampling scaled by the Q value. *Equation
    10.1.6* takes advantage of the property of the natural logarithm, ![Policy gradient
    theorem](img/B08956_10_33.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient theorem is intuitive in the sense that the performance gradient
    is estimated from the target policy samples and proportional to the policy gradient.
    The policy gradient is scaled by the Q value to encourage actions that positively
    contribute to the state value. The gradient is also inversely proportional to
    the action probability to penalize frequently occurring actions that do not contribute
    to the increase of performance measure.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will demonstrate the different methods of estimating
    the policy gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the proof of policy gradient theorem, please see [2] and lecture notes from
    David Silver on Reinforcement Learning, [http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: There are subtle advantages of policy gradient methods. For example, in some
    card-based games, value-based methods have no straightforward procedure in handling
    stochasticity, unlike policy-based methods. In policy-based methods, the action
    probability changes smoothly with the parameters. Meanwhile, value-based actions
    may suffer from drastic changes with respect to small changes in parameters. Lastly,
    the dependence of policy-based methods on parameters leads us to different formulations
    on how to perform gradient ascent on the performance measure. These are the four
    policy gradient methods to be presented in the succeeding sections.
  prefs: []
  type: TYPE_NORMAL
- en: Policy-based methods have their own disadvantages as well. They are generally
    harder to train because of the tendency to converge on a local optimum instead
    of the global optimum. In the experiments to be presented at the end of this chapter,
    it is easy for an agent to become comfortable and to choose actions that do not
    necessarily give the highest value. Policy gradient is also characterized by high
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient updates are frequently overestimated. Furthermore, training policy-based
    methods are time-consuming. The training requires thousands of episodes (that
    is, not sample efficient). Each episode only provides a small number of samples.
    Typical training in the implementation provided at the end of the chapter would
    take about an hour for 1,000 episodes on a GTX 1060 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we discuss the four policy gradient methods. While
    the discussion focuses on continuous action spaces, the concept is generally applicable
    to discrete action spaces. Due to similarities in the implementation of the policy
    and value networks of the four policy gradient methods, we will wait until the
    end of this chapter to illustrate the implementation into Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo policy gradient (REINFORCE) method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest policy gradient method is called REINFORCE [5], this is a Monte
    Carlo policy gradient method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_028.jpg) (Equation
    10.2.1)'
  prefs: []
  type: TYPE_IMG
- en: where *R*[*t*] is the return as defined in *Equation 9.1.2*. *R*[*t*] is an
    unbiased sample of ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_029.jpg)
    in the policy gradient theorem.
  prefs: []
  type: TYPE_NORMAL
- en: '*Algorithm* *10.2.1* summarizes the REINFORCE algorithm [2]. REINFORCE is a Monte Carlo
    algorithm. It does not require knowledge of the dynamics of the environment (that
    is, model-free). Only experience samples, ![Monte Carlo policy gradient (REINFORCE)
    method](img/B08956_10_030.jpg), are needed to optimally tune the parameters of
    the policy network, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_015.jpg).
    The discount factor, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_031.jpg),
    takes into consideration that rewards decrease in value as the number of steps
    increases. The gradient is discounted by ![Monte Carlo policy gradient (REINFORCE)
    method](img/B08956_10_032.jpg). Gradients taken at later steps have smaller contributions.
    The learning rate, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_033.jpg),
    is a scaling factor of the gradient update.'
  prefs: []
  type: TYPE_NORMAL
- en: The parameters are updated by performing gradient ascent using the discounted
    gradient and learning rate. As a Monte Carlo algorithm, REINFORCE requires that
    the agent completes an episode before processing the gradient updates. Due to
    its Monte Carlo nature, the gradient update of REINFORCE is characterized by high
    variance. At the end of this chapter, we will implement the REINFORCE algorithm
    into Keras.
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm 10.2.1 REINFORCE**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: A differentiable parameterized target policy network, ![Monte Carlo
    policy gradient (REINFORCE) method](img/B08956_10_015.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: Discount factor, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_035.jpg)
    and learning rate ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_033.jpg).
    For example, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_036.jpg)
    and ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_037.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_038.jpg),
    initial policy network parameters (for example, ![Monte Carlo policy gradient
    (REINFORCE) method](img/B08956_10_039.jpg)).'
  prefs: []
  type: TYPE_NORMAL
- en: Repeat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_36.jpg)
    by following ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_041.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for steps ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_042.jpg)
    do
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute return, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_37.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute discounted performance gradient, ![Monte Carlo policy gradient (REINFORCE)
    method](img/B08956_10_38.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform gradient ascent, ![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_39.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Monte Carlo policy gradient (REINFORCE) method](img/B08956_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2.1: Policy network'
  prefs: []
  type: TYPE_NORMAL
- en: In REINFORCE, the parameterized policy can be modeled by a neural network as
    shown in *Figure 10.2.1*. As discussed in the previous section, for the case of continuous
    action spaces, the state input is converted into features. The state features
    are the inputs of the policy network. The Gaussian distribution representing the
    policy function has a mean and standard deviation that are both functions of the
    state features. The policy network, ![Monte Carlo policy gradient (REINFORCE)
    method](img/B08956_10_043.jpg), could be an MLP, CNN, or an RNN depending on the
    nature of the state inputs. The predicted action is simply a sample from the policy
    function.
  prefs: []
  type: TYPE_NORMAL
- en: REINFORCE with baseline method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The REINFORCE algorithm can be generalized by subtracting a baseline from the
    return, ![REINFORCE with baseline method](img/B08956_10_044.jpg). The baseline
    function, *B(s* *t* *)* can be any function as long as it does not depend on *a*[t]
    The baseline does not alter the expectation of the performance gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![REINFORCE with baseline method](img/B08956_10_046.jpg) (Equation 10.3.1)'
  prefs: []
  type: TYPE_IMG
- en: '*Equation 10.3.1* implies that ![REINFORCE with baseline method](img/B08956_10_047.jpg)
    since ![REINFORCE with baseline method](img/B08956_10_048.jpg) is not a function
    of ![REINFORCE with baseline method](img/B08956_10_049.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the introduction of baseline does not change the expectation, it reduces
    the variance of the gradient updates. The reduction in variance generally accelerates
    learning. In most cases, we use the value function, ![REINFORCE with baseline
    method](img/B08956_10_44.jpg) as the baseline. If the return is overestimated,
    the scaling factor is proportionally reduced by the value function resulting to
    a lower variance. The value function is also parameterized, ![REINFORCE with baseline
    method](img/B08956_10_050.jpg) and is jointly trained with the policy network.
    In continuous action spaces, the state value can be a linear function of state
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![REINFORCE with baseline method](img/B08956_10_051.jpg) (Equation 10.3.2)'
  prefs: []
  type: TYPE_IMG
- en: '*Algorithm* *10.3.1* summarizes the REINFORCE with baseline method [1]. This
    is similar to REINFORCE except that the return is replaced by ![REINFORCE with
    baseline method](img/B08956_10_001.jpg). The difference is we are now training
    two neural networks. As shown in *Figure 10.3.1*, in addition to the policy network,
    ![REINFORCE with baseline method](img/B08956_10_034.jpg), the value network, ![REINFORCE
    with baseline method](img/B08956_10_052.jpg), is also trained at the same time.
    The policy network parameters are updated by the performance gradient, ![REINFORCE
    with baseline method](img/B08956_10_053.jpg), while the value network parameters
    are adjusted by the value gradient, ![REINFORCE with baseline method](img/B08956_10_054.jpg).
    Since REINFORCE is a Monte Carlo algorithm, it follows that the value function
    training is also a Monte Carlo algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rates are not necessarily the same. Note that the value network
    is also performing gradient ascent. We illustrate how to implement REINFORCE with baseline
    using Keras at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm 10.3.1 REINFORCE with baseline**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: A differentiable parameterized target policy network, ![REINFORCE
    with baseline method](img/B08956_10_015.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: A differentiable parameterized value network, ![REINFORCE with baseline
    method](img/B08956_10_055.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: Discount factor, ![REINFORCE with baseline method](img/B08956_10_035.jpg),
    the learning rate ![REINFORCE with baseline method](img/B08956_10_033.jpg) for
    the performance gradient and learning rate for the value gradient, ![REINFORCE
    with baseline method](img/B08956_10_056.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: ![REINFORCE with baseline method](img/B08956_10_057.jpg), initial
    policy network parameters (for example, ![REINFORCE with baseline method](img/B08956_10_058.jpg)).
    ![REINFORCE with baseline method](img/B08956_10_059.jpg), initial value network
    parameters (for example, ![REINFORCE with baseline method](img/B08956_10_060.jpg)).'
  prefs: []
  type: TYPE_NORMAL
- en: Repeat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode ![REINFORCE with baseline method](img/B08956_10_061.jpg)
    by following ![REINFORCE with baseline method](img/B08956_10_015.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for steps ![REINFORCE with baseline method](img/B08956_10_042.jpg) do
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute return, ![REINFORCE with baseline method](img/B08956_10_063.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract baseline, ![REINFORCE with baseline method](img/B08956_10_064.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute discounted value gradient, ![REINFORCE with baseline method](img/B08956_10_065.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform gradient ascent, ![REINFORCE with baseline method](img/B08956_10_066.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute discounted performance gradient, ![REINFORCE with baseline method](img/B08956_10_067.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform gradient ascent, ![REINFORCE with baseline method](img/B08956_10_068.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![REINFORCE with baseline method](img/B08956_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3.1: Policy and value networks'
  prefs: []
  type: TYPE_NORMAL
- en: Actor-Critic method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In REINFORCE with baseline method, the value is used as a baseline. It is not
    used to train the value function. In this section, we''ll introduce a variation
    of REINFORCE with baseline called the Actor-Critic method. The policy and value
    networks played the roles of actor and critic networks. The policy network is
    the actor deciding which action to take given the state. Meanwhile, the value
    network evaluates the decision made by the actor or the policy network. The value
    network acts as a critic which quantifies how good or bad the chosen action made
    by the actor is. The value network evaluates the state value, ![Actor-Critic method](img/B08956_10_069.jpg),
    by comparing it with the sum of the received reward, ![Actor-Critic method](img/B08956_10_070.jpg),
    and the discounted value of the observed next state, ![Actor-Critic method](img/B08956_10_071.jpg).
    The difference, ![Actor-Critic method](img/B08956_10_072.jpg), is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Actor-Critic method](img/B08956_10_073.jpg) (Equation 10.4.1)'
  prefs: []
  type: TYPE_IMG
- en: where we dropped the subscripts of ![Actor-Critic method](img/B08956_10_070.jpg)
    and ![Actor-Critic method](img/B08956_10_074.jpg) for simplicity. *Equation 10.4.1*
    is similar to the temporal differencing in Q-Learning discussed in [Chapter 9](ch09.html
    "Chapter 9. Deep Reinforcement Learning"), *Deep Reinforcement Learning*. The
    next state value is discounted by ![Actor-Critic method](img/B08956_10_075.jpg)
    Estimating distant future rewards is difficult. Therefore, our estimate is based
    only on the immediate future, ![Actor-Critic method](img/B08956_10_076.jpg). This
    has been known as *bootstrapping* technique. The bootstrapping technique and the
    dependence on state representation in *Equation 10.4.1* often accelerates learning
    and reduces variance. From *Equation 10.4.1*, we notice that the value network
    evaluates the current state, ![Actor-Critic method](img/B08956_10_077.jpg), which
    is due to the previous action, ![Actor-Critic method](img/B08956_10_078.jpg),
    of the policy network. Meanwhile, the policy gradient is based on the current
    action, ![Actor-Critic method](img/B08956_10_079.jpg). In a sense, the evaluation
    is delayed by one step.
  prefs: []
  type: TYPE_NORMAL
- en: '*Algorithm* *10.4.1* summarizes the Actor-Critic method [2]. Apart from the
    evaluation of the state value which is used to train both the policy and value
    networks, the training is done online. At every step, both networks are trained.
    This is unlike REINFORCE and REINFORCE with baseline where the agent completes
    an episode before the training is performed. The value network is consulted twice.
    Firstly, during the value estimate of the current state and secondly for the value
    of the next state. Both values are used in the computation of gradients. *Figure
    10.4.1* shows the Actor-Critic network. We will implement the Actor-Critic method
    in Keras at the end of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm 10.4.1 Actor-Critic**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: A differentiable parameterized target policy network, ![Actor-Critic
    method](img/B08956_10_080.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: A differentiable parameterized value network, ![Actor-Critic method](img/B08956_10_081.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: Discount factor, ![Actor-Critic method](img/B08956_10_007.jpg),
    the learning rate ![Actor-Critic method](img/B08956_10_033.jpg) for the performance
    gradient, and the learning rate for the value gradient, ![Actor-Critic method](img/B08956_10_056.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: ![Actor-Critic method](img/B08956_10_038.jpg), initial policy network
    parameters (for example, ![Actor-Critic method](img/B08956_10_039.jpg)). ![Actor-Critic
    method](img/B08956_10_082.jpg), initial value network parameters (for example,
    ![Actor-Critic method](img/B08956_10_083.jpg)).'
  prefs: []
  type: TYPE_NORMAL
- en: Repeat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for steps ![Actor-Critic method](img/B08956_10_042.jpg) do
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample an action ![Actor-Critic method](img/B08956_10_087.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the action and observe reward ![Actor-Critic method](img/B08956_10_070.jpg)
    and next state ![Actor-Critic method](img/B08956_10_088.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate state value estimate, ![Actor-Critic method](img/B08956_10_089.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute discounted value gradient, ![Actor-Critic method](img/B08956_10_090.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform gradient ascent, ![Actor-Critic method](img/B08956_10_091.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute discounted performance gradient, ![Actor-Critic method](img/B08956_10_092.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform gradient ascent, ![Actor-Critic method](img/B08956_10_093.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Actor-Critic method](img/B08956_10_094.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: '![Actor-Critic method](img/B08956_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4.1: Actor-critic network'
  prefs: []
  type: TYPE_NORMAL
- en: Advantage Actor-Critic (A2C) method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the Actor-Critic method from the previous section, the objective is for
    the value function to evaluate the state value correctly. There are other techniques
    to train the value network. One obvious method is to use **MSE** (**mean squared
    error**) in the value function optimization, similar to the algorithm in Q-Learning.
    The new value gradient is equal to the partial derivative of the MSE between the
    return, ![Advantage Actor-Critic (A2C) method](img/B08956_10_095.jpg), and the
    state value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Advantage Actor-Critic (A2C) method](img/B08956_10_61.jpg) (Equation 10.5.1)'
  prefs: []
  type: TYPE_IMG
- en: As ![Advantage Actor-Critic (A2C) method](img/B08956_10_62.jpg), the value network
    prediction gets more accurate. We call this variation of the Actor-Critic algorithm
    as A2C. A2C is a single threaded or synchronous version of the **Asynchronous
    Advantage Actor-Critic** (**A3C**) by [3]. The quantity ![Advantage Actor-Critic
    (A2C) method](img/B08956_10_63.jpg) is called *Advantage*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Algorithm* *10.5.1* summarizes the A2C method. There are some differences
    between A2C and Actor-Critic. Actor-Critic is online or is trained on per experience
    sample. A2C is similar to Monte Carlo algorithms REINFORCE and REINFORCE with
    baseline. It is trained after one episode has been completed. Actor-Critic is
    trained from the first state to the last state. A2C training starts from the last
    state and ends on the first state. In addition, the A2C policy and value gradients
    are no longer discounted by ![Advantage Actor-Critic (A2C) method](img/B08956_10_032.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding network for A2C is similar to *Figure 10.4.1* since we only
    changed the method of gradient computation. To encourage agent exploration during
    training, A3C algorithm [3] suggests that the gradient of the weighted entropy
    value of the policy function is added to the gradient function, ![Advantage Actor-Critic
    (A2C) method](img/B08956_10_69.jpg). Recall that entropy is a measure of information
    or uncertainty of an event.
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm 10.5.1 Advantage Actor-Critic (A2C)**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: A differentiable parameterized target policy network, ![Advantage
    Actor-Critic (A2C) method](img/B08956_10_015.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: A differentiable parameterized value network, ![Advantage Actor-Critic
    (A2C) method](img/B08956_10_096.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: Discount factor, ![Advantage Actor-Critic (A2C) method](img/B08956_10_035.jpg),
    the learning rate ![Advantage Actor-Critic (A2C) method](img/B08956_10_033.jpg)
    for the performance gradient, the learning rate for the value gradient, ![Advantage
    Actor-Critic (A2C) method](img/B08956_10_056.jpg) and entropy weight, ![Advantage
    Actor-Critic (A2C) method](img/B08956_10_097.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: ![Advantage Actor-Critic (A2C) method](img/B08956_10_038.jpg), initial
    policy network parameters (for example, ![Advantage Actor-Critic (A2C) method](img/B08956_10_039.jpg)).
    ![Advantage Actor-Critic (A2C) method](img/B08956_10_098.jpg), initial value network
    parameters (for example, ![Advantage Actor-Critic (A2C) method](img/B08956_10_099.jpg)).'
  prefs: []
  type: TYPE_NORMAL
- en: Repeat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode ![Advantage Actor-Critic (A2C) method](img/B08956_10_061.jpg)
    by following ![Advantage Actor-Critic (A2C) method](img/B08956_10_015.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Advantage Actor-Critic (A2C) method](img/B08956_10_100.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: for steps ![Advantage Actor-Critic (A2C) method](img/B08956_10_101.jpg) do
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute return, ![Advantage Actor-Critic (A2C) method](img/B08956_10_66.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute value gradient, ![Advantage Actor-Critic (A2C) method](img/B08956_10_102.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accumulate gradient, ![Advantage Actor-Critic (A2C) method](img/B08956_10_103.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute performance gradient, ![Advantage Actor-Critic (A2C) method](img/B08956_10_104.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform gradient ascent, ![Advantage Actor-Critic (A2C) method](img/B08956_10_105.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Policy Gradient methods with Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The four policy gradient methods (*Algorithms* *10.2.1* to *10.5.1*) discussed
    in the previous sections use identical policy and value network models. The policy
    and value networks in *Figures 10.2.1* to *10.4.1* have the same configurations.
    The four policy gradient methods differ only in:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance and value gradients formula
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we discuss the implementation in Keras of *Algorithms* *10.2.1*
    to *10.5.1* in one code, since they share many common routines.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The complete code can be found on [https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras).
  prefs: []
  type: TYPE_NORMAL
- en: But before discussing the implementation, let's briefly explore the training
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy Gradient methods with Keras](img/B08956_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6.1 MountainCarContinuous-v0 OpenAI gym environment
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Q-Learning, policy gradient methods are applicable to both discrete and
    continuous action spaces. In our example, we'll demonstrate the four policy gradient
    methods on a continuous action space case example, `MountainCarContinuous-v0`
    of OpenAI gym, [https://gym.openai.com](https://gym.openai.com). In case you are
    not familiar with OpenAI gym, please see [Chapter 9](ch09.html "Chapter 9. Deep
    Reinforcement Learning"), *Deep Reinforcement Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A snapshot of `MountainCarContinuous-v0` 2D environment is shown in *Figure
    10.6.1*. In this 2D environment, a car with a not too powerful engine is between
    two mountains. In order to reach the yellow flag on top of the mountain on the
    right, it must drive back and forth to gain enough momentum. The more energy (that
    is, the greater the absolute value of action) that is applied to the car, the
    smaller (or, the more negative) is the reward. The reward is always negative,
    and it is only positive upon reaching the flag. In that case, the car receives
    a reward of +100\. However, every action is penalized by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The continuous range of valid action values is [-1.0, 1.0]. Beyond the range,
    the action is clipped to its minimum or maximum value. Therefore, it makes no
    sense to apply an action value that is greater than 1.0 or less than -1.0\. The `MountainCarContinuous-v0`
    environment state has two elements:'
  prefs: []
  type: TYPE_NORMAL
- en: Car position
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Car velocity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The state is converted to state features by an encoder. The predicted action
    is the output of the policy model given the state. The output of the value function
    is the predicted value of the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy Gradient methods with Keras](img/B08956_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6.2 Autoencoder model
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy Gradient methods with Keras](img/B08956_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6.3 Encoder model
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy Gradient methods with Keras](img/B08956_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6.4 Decoder model
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figures 10.2.1* to *10.4.1*, before building the policy and value
    networks, we must first create a function that converts the state to features.
    This function is implemented by an encoder of an autoencoder similar to the ones
    implemented in [Chapter 3](ch03.html "Chapter 3. Autoencoders"), *Autoencoders*.
    *Figure 10.6.2* shows an autoencoder made of an encoder and a decoder. In *Figure
    10.6.3*, the encoder is an MLP made of `Input(2)-Dense(256, activation='relu')-Dense(128,
    activation='relu')-Dense(32)`. Every state is converted into a 32-dim feature
    vector. In *Figure* *10.6.4*, the decoder is also an MLP but made of `Input(32)-Dense(128,
    activation='relu')-Dense(256, activation='relu')-Dense(2)`. The autoencoder is
    trained for 10 epochs with an **MSE**, loss function, and Keras default Adam optimizer.
    We sampled 220,000 random states for the train and test dataset and applied 200k/20k
    train-test split. After training, the encoder weights are saved for future use
    in the policy and value networks training. *Listing 10.6.1* shows the methods
    for building and training the autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 10.6.1, `policygradient-car-10.1.1.py` shows us the methods for building
    and training the autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Policy Gradient methods with Keras](img/B08956_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6.5: Policy model (actor model)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the `MountainCarContinuous-v0` environment, the policy (or actor) model
    predicts the action that must be applied on the car. As discussed in the first
    section of this chapter on policy gradient methods, for continuous action spaces
    the policy model samples an action from a Gaussian distribution, ![Policy Gradient
    methods with Keras](img/B08956_10_106.jpg). In Keras, this is implemented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The action is clipped between its minimum and maximum possible values.
  prefs: []
  type: TYPE_NORMAL
- en: The role of the policy network is to predict the mean and standard deviation
    of the Gaussian distribution. *Figure 10.6.5* shows the policy network to model
    ![Policy Gradient methods with Keras](img/B08956_10_015.jpg). It's worth noting
    that the encoder model has pretrained weights that are frozen. Only the mean and
    standard deviation weights receive the performance gradient updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The policy network is basically the implementation of *Equations* *10.1.4*
    and *10.1.5* that are repeated here for convenience:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy Gradient methods with Keras](img/B08956_10_108.jpg) (Equation 10.1.4)'
  prefs: []
  type: TYPE_IMG
- en: '![Policy Gradient methods with Keras](img/B08956_10_109.jpg) (Equation 10.1.5)'
  prefs: []
  type: TYPE_IMG
- en: 'where ![Policy Gradient methods with Keras](img/B08956_10_023.jpg) is the encoder,
    ![Policy Gradient methods with Keras](img/B08956_10_110.jpg) are the weights of
    the mean''s `Dense(1)` layer, and ![Policy Gradient methods with Keras](img/B08956_10_111.jpg)
    are the weights of the standard deviation''s `Dense(1)` layer. We used a modified
    *softplus* function, ![Policy Gradient methods with Keras](img/B08956_10_112.jpg),
    to avoid zero standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The policy model builder is shown in the following listing. Also included in
    this listing are the log probability, entropy, and value models which we will
    discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 10.6.2, `policygradient-car-10.1.1.py` shows us the method for building
    the policy (actor), `logp`, entropy, and value models from the encoded state features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Policy Gradient methods with Keras](img/B08956_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6.6: Gaussian log probability model of the policy'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy Gradient methods with Keras](img/B08956_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6.7: Entropy model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the policy network, ![Policy Gradient methods with Keras](img/B08956_10_015.jpg),
    we must also have the action log probability (`logp`) network ![Policy Gradient
    methods with Keras](img/B08956_10_70.jpg) since this is actually what calculates
    the gradient. As shown in *Figure 10.6.6*, the `logp` network is simply the policy
    network where an additional `Lambda(1)` layer computes the log probability of
    the Gaussian distribution given action, mean, and standard deviation. The `logp`
    network and actor (policy) model share the same set of parameters. The `Lambda`
    layer does not have any parameter. It is implemented by the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Training the `logp` network trains the actor model as well. In the training
    methods that are discussed in this section, only the `logp` network is trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 10.6.7*, the entropy model also shares parameters with
    the policy network. The output `Lambda(1)` layer computes the entropy of the Gaussian
    distribution given the mean and standard deviation using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The entropy model is only used by the A2C method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy Gradient methods with Keras](img/B08956_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6.8: A value model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Preceding figure shows the value model. The model also uses the pre-trained
    encoder with frozen weights to implement following equation which is repeated
    here for convenience:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy Gradient methods with Keras](img/B08956_10_113.jpg) (Equation 10.3.2)'
  prefs: []
  type: TYPE_IMG
- en: '![Policy Gradient methods with Keras](img/B08956_10_114.jpg) are the weights
    of the `Dense(1)` layer, the only layer that receives value gradient updates.
    *Figure 10.6.8* represents ![Policy Gradient methods with Keras](img/B08956_10_096.jpg)
    in *Algorithms 10.3.1* to *10.5.1*. The value model can be built in a few lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: These lines are also implemented in method `build_actor_critic()`, which is
    shown in *Listing 10.6.2*.
  prefs: []
  type: TYPE_NORMAL
- en: After building the network models, the next step is training. In *Algorithms*
    *10.2.1* to *10.5.1*, we perform objective function maximization by gradient ascent.
    In Keras, we perform loss function minimization by gradient descent. The loss
    function is simply the negative of the objective function being maximized. The
    gradient descent is the negative of gradient ascent. *Listing 10.6.3* shows the
    `logp` and value loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: We can take advantage of the common structure of the loss functions to unify
    the loss functions in *Algorithms* *10.2.1* to *10.5.1*. The performance and value
    gradients differ only in their constant factors. All performance gradients have
    the common term, ![Policy Gradient methods with Keras](img/B08956_10_74.jpg).
    This is represented by `y_pred` in the policy log probability loss function, `logp_loss()`.
    The factor to the common term, ![Policy Gradient methods with Keras](img/B08956_10_74.jpg),
    depends on which algorithm and is implemented as `y_true`. Table *10.6.1* shows
    the values of `y_true`. The remaining term is the weighted gradient of entropy,
    ![Policy Gradient methods with Keras](img/B08956_10_69.jpg). It is implemented
    as the product of `beta` and `entropy` in the `logp_loss()` function. Only A2C
    uses this term, so by default, `beta=0.0`. For A2C, `beta=0.9`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 10.6.3, `policygradient-car-10.1.1.py`: The loss functions of `logp`
    and value networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '| Algorithm | `y_true of logp_loss` | `y_true of value_loss` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 10.2.1 REINFORCE | ![Policy Gradient methods with Keras](img/B08956_10_115.jpg)
    | Not applicable |'
  prefs: []
  type: TYPE_TB
- en: '| 10.3.1 REINFORCE with baseline | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg)
    | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| 10.4.1 Actor-Critic | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg)
    | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| 10.5.1 A2C | ![Policy Gradient methods with Keras](img/B08956_10_63.jpg)
    | ![Policy Gradient methods with Keras](img/B08956_10_095.jpg) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.6.1: y_true value of logp_loss and value_loss'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Similarly, the value loss functions of *Algorithms* *10.3.1* and *10.4.1* have
    the same structure. The value loss functions are implemented in Keras as `value_loss()`
    as shown in *Listing 10.6.3*. The common gradient factor ![Policy Gradient methods
    with Keras](img/B08956_10_75.jpg) is represented by the tensor `y_pred`. The remaining
    factor is represented by `y_true`. The `y_true` values are also shown in *Table
    10.6.1*. REINFORCE does not use a value function. A2C uses the MSE loss function
    to learn the value function. In A2C, `y_true` represents the target value or ground
    truth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 10.6.4, `policygradient-car-10.1.1.py` shows us, REINFORCE, REINFORCE
    with baseline, and A2C are trained by episode. The appropriate return is computed
    first before calling the main train routine in *Listing 10.6.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 10.6.5, `policygradient-car-10.1.1.py` shows us the main `train` routine
    used by all the policy gradient algorithms. Actor-critic calls this every experience
    sample while the rest call this during train per episode routine in *Listing 10.6.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: With all network models and loss functions in place, the last part is the training
    strategy, which is different for each algorithm. Two train functions are used
    as shown in *Listings 10.6.4* and *10.6.5*. *Algorithms* *10.2.1*, *10.3.1*, and
    *10.5.1* wait for a complete episode to finish before training, so it runs both
    `train_by_episode()` and `train()`. The complete episode is saved in `self.memory`.
    Actor-Critic *Algorithm* *10.4.1* trains per step and only runs `train()`.
  prefs: []
  type: TYPE_NORMAL
- en: Each algorithm processes its episode trajectory in a different way.
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | `y_true` formula | `y_true` in Keras |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 10.2.1 REINFORCE | ![Policy Gradient methods with Keras](img/B08956_10_115.jpg)
    | `reward * discount_factor` |'
  prefs: []
  type: TYPE_TB
- en: '| 10.3.1 REINFORCE with baseline | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg)
    | `(reward - self.value(state)[0]) * discount_factor` |'
  prefs: []
  type: TYPE_TB
- en: '| 10.4.1 Actor-Critic | ![Policy Gradient methods with Keras](img/B08956_10_117.jpg)
    | `(reward - self.value(state)[0] + gamma*next_value) * discount_factor` |'
  prefs: []
  type: TYPE_TB
- en: '| 10.5.1 A2C | ![Policy Gradient methods with Keras](img/B08956_10_63.jpg)and
    ![Policy Gradient methods with Keras](img/B08956_10_095.jpg) | `(reward - self.value(state)[0])`and`reward`
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.6.2: y_true value in Table 10.6.1'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For REINFORCE methods and A2C, the `reward` is actually the return as computed
    in `train_by_episode()`. `discount_factor = gamma**step`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both REINFORCE methods compute the return, ![Policy Gradient methods with Keras](img/B08956_10_37.jpg),
    by replacing the reward value in the memory as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This then trains the policy (actor) and value models (with baseline only) for
    each step beginning with the first step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training strategy of A2C is different in the sense that it computes gradients
    from the last step to the first step. Hence, the return accumulates beginning
    from the last step reward or the last next state value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `reward` variable in the list is also replaced by return. It is initialized
    by `reward` if the terminal state is reached (that is, the car touches the flag)
    or the next state value for non-terminal states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the Keras implementation, all the routines that we mentioned are implemented
    as methods in the `PolicyAgent` class. The role of the `PolicyAgent` is to represent
    the agent implementing policy gradient methods including building and training
    the network models and predicting the action, log probability, entropy, and state
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Following listing shows how one episode unfolds when the agent executes and
    trains the policy and value models. The `for` loop is executed for 1000 episodes.
    An episode terminates upon reaching 1000 steps or when the car touches the flag.
    The agent executes the action predicted by the policy at every step. After each
    episode or step, the training routine is called.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 10.6.6, `policygradient-car-10.1.1.py`: The agent runs for 1000 episodes
    to execute the action predicted by the policy at every step and perform training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Performance evaluation of policy gradient methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The four policy gradients methods were evaluated by training the agent for 1,000
    episodes. We define 1 training session as 1,000 episodes of training. The first
    performance metric is measured by accumulating the number of times the car reached
    the flag in 1,000 episodes. *Figures 10.7.1* to *10.7.4* shows five training sessions
    per method.
  prefs: []
  type: TYPE_NORMAL
- en: In this metric, A2C reached the flag with the greatest number of times followed
    by REINFORCE with baseline, Actor-Critic, and REINFORCE. The use of baseline or
    critic accelerates the learning. Note that these are training sessions with the
    agent continuously improving its performance. There were cases in the experiments
    where the agent's performance did not improve with time.
  prefs: []
  type: TYPE_NORMAL
- en: The second performance metric is based on the requirement that the `MountainCarContinuous-v0`
    is considered solved if the total reward per episode is at least 90.0\. From the
    five training sessions per method, we selected one training session with the highest
    total reward for the last 100 episodes (episodes 900 to 999). *Figures 10.7.5*
    to *10.7.8* show the results of the four policy gradient methods. REINFORCE with
    baseline is the only method that was able to consistently achieve a total reward
    of about 90 after 1,000 episodes of training. A2C has the second-best performance
    but could not consistently reach at least 90 for the total rewards.
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7.1: The number of times the mountain car reached the flag using
    REINFORCE method'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7.2: The number of times the mountain car reached the flag using
    REINFORCE with baseline method'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7.3: The number of times the mountain car reached the flag using
    the Actor-Critic method'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7.4: The number of times the mountain car reached the flag using
    the A2C method'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7.5: Total rewards received per episode using REINFORCE method'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_17_a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7.6: Total rewards received per episode using REINFORCE with baseline
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7.7: Total rewards received per episode using the Actor-Critic method'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance evaluation of policy gradient methods](img/B08956_10_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7.8: The total rewards received per episode using the A2C method'
  prefs: []
  type: TYPE_NORMAL
- en: In the experiments conducted, we used the same learning rate, `1e-3`, for log
    probability and value networks optimization. The discount factor is set to 0.99,
    except for A2C which is easier to train at a 0.95 discount factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reader is encouraged to run the trained network by executing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Following table shows other modes of running `policygradient-car-10.1.1.py`.
    The weights file (that is, `*.h5`) can be replaced by your own pre-trained weights
    file. Please consult the code to see the other potential options:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Purpose | Run |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Train REINFORCE from scratch |'
  prefs: []
  type: TYPE_TB
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Train REINFORCE with baseline from scratch |'
  prefs: []
  type: TYPE_TB
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Train Actor-Critic from scratch |'
  prefs: []
  type: TYPE_TB
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Train A2C from scratch |'
  prefs: []
  type: TYPE_TB
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Train REINFORCE from previously saved weights |'
  prefs: []
  type: TYPE_TB
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Train REINFORCE with baseline from previously saved weights |'
  prefs: []
  type: TYPE_TB
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Train Actor-Critic from previously saved weights |'
  prefs: []
  type: TYPE_TB
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Train A2C from previously saved weights |'
  prefs: []
  type: TYPE_TB
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10.7.1: Different options in running policygradient-car-10.1.1.py'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As a final note, the implementation of the policy gradient methods in Keras
    has some limitations. For example, training the actor model requires resampling
    the action. The action is first sampled and applied to the environment to observe
    the reward and next state. Then, another sample is taken for training the log
    probability model. The second sample is not necessarily the same as the first
    one, but the reward that is used for training comes from the first sampled action,
    which can introduce stochastic error in the computation of gradients.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is Keras is gaining a lot of support from TensorFlow in the form
    of `tf.keras`. Transitioning from Keras to a more flexible and powerful machine
    learning library, like TensorFlow, has been made a lot easier. If you started
    with Keras and wanted to build low-level custom machine learning routines, the
    APIs of Keras and `tf.keras` share strong similarities.
  prefs: []
  type: TYPE_NORMAL
- en: There is a small learning curve in using Keras in TensorFlow. Furthermore, in `tf.keras`,
    you're able to take advantage of the new easy to use Dataset and Estimators APIs
    of TensorFlow. This simplifies a lot of the code and model reuse that ends up with
    a clean pipeline. With the new eager execution mode of TensorFlow, it becomes
    even easier to implement and debug Python codes in `tf.keras` and TensorFlow.
    Eager execution allows the execution of codes without building a computational
    graph as we did in this book. It also allows code structures similar to a typical
    Python program.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've covered the policy gradient methods. Starting with the
    policy gradient theorem, we formulated four methods to train the policy network.
    The four methods, REINFORCE, REINFORCE with baseline, Actor-Critic, and A2C algorithms
    were discussed in detail. We explored how the four methods could be implemented
    in Keras. We then validated the algorithms by examining the number of times the
    agent successfully reached its goal and in terms of the total rewards received
    per episode.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to Deep Q-Network [3] that we discussed in the previous chapter, there
    are several improvements that can be done on the fundamental policy gradient algorithms.
    For example, the most prominent one is the A3C [4] which is a multi-threaded version
    of A2C. This enables the agent to get exposed to different experiences simultaneously
    and to optimize the policy and value networks asynchronously. However, in the
    experiments conducted by OpenAI, [https://blog.openai.com/baselines-acktr-a2c/](https://blog.openai.com/baselines-acktr-a2c/),
    there is no strong advantage of A3C over A2C since the former could not take advantage
    of the strong GPUs available nowadays.
  prefs: []
  type: TYPE_NORMAL
- en: Given that this is the end of the book, it's worth noting that the area of deep
    learning is huge, and to cover all the advances in one book like this is impossible.
    What we've done is carefully selected the advanced topics that I believe will
    be useful in a wide range of applications and that you, the reader will be able
    to easily build on. The implementations in Keras that have been illustrated throughout
    this book will allow you to carry on and apply the techniques in your own work
    and research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sutton and Barto. *Reinforcement Learning: An Introduction*, [http://incompleteideas.net/book/bookdraft2017nov5.pdf](http://incompleteideas.net/book/bookdraft2017nov5.pdf),
    (2017).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mnih, Volodymyr, and others. *Human-level control through deep reinforcement
    learning*, *Nature* 518.7540 (2015): 529.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mnih, Volodymyr, and others. *Asynchronous methods for deep reinforcement learning*, *International
    conference on machine learning*, 2016.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Williams and Ronald J. *Simple statistical gradient-following algorithms for
    connectionist reinforcement learning*, *Machine learning* 8.3-4 (1992): 229-256.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
