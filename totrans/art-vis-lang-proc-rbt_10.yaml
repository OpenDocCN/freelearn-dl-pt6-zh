- en: '*Appendix*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: About
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section is included to assist the students to perform the activities in
    the book. It includes detailed steps that are to be performed by the students
    to achieve the objectives of the activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 1: Fundamentals of Robotics'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 1: Robot Positioning Using Odometry with Python'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For further observations, you can change the wheels' diameter to 15 cm and check
    the difference in the output. Similarly, you can change other input values and
    check the difference in the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 2: Introduction to Computer Vision'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 2: Classify 10 Types of Clothes from the Fashion-MNIST Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: Open up your Google Colab interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder for the book, download the `Dataset` folder from GitHub, and
    upload it into the folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the drive and mount it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once you have mounted your drive for the first time, you will have to enter
    the authorization code mentioned by clicking on the URL given by Google and pressing
    the **Enter** key on your keyboard:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.39: Image displaying the Google Colab authorization step](img/C13550_05_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.38: Image displaying the Google Colab authorization step'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now that you have mounted the drive, you need to set the path of the directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset and show five samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.40: Loading datasets with five samples](img/C1355_02_40.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.39: Loading datasets with five samples'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/C1355_02_41.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.40: Samples of images from the Fashion-MNIST dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Preprocess the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the architecture of the neural network using `Dense` layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note:'
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The entire code file for this activity can be found on GitHub in the Lesson02
    | Activity02 folder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compile and train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy obtained is **88.72%**. This problem is harder to solve, so that's
    why we have achieved less accuracy than in the last exercise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.42: Prediction for clothes using Neural Networks](img/C1355_02_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.41: Prediction for clothes using Neural Networks'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It has classified the bag and the t-shirt correctly, but it has failed to classify
    the boots and the trousers. These samples are very different from the ones that
    it was trained for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 3: Fundamentals of Natural Language Processing'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 3: Process a Corpus'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `sklearn` `TfidfVectorizer` and `TruncatedSVD` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With `spaCy`, let''s add some new stop words, tokenize the corpus, and remove
    the stop words. The new corpus without these words will be stored in a new variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the TF-IDF matrix. I''m going to add some parameters to improve the
    results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform the LSA algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With pandas, we are shown a sorted `DataFrame` with the weights of the terms
    of each concept and the name of each feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.25: Output example of the most relevant words in a concept (f1)](img/C1355_03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.26: Output example of the most relevant words in a concept (f1)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Do not worry if the keywords are not the same as yours, if the keywords represent
    a concept, it is a valid result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 4: Neural Networks with NLP'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 4: Predict the Next Character in a Sequence'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries we need to solve the activity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the sequence of characters and multiply it by 100:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `char2id` dictionary to relate every character with an integer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Divide the sentence of characters into time series. The maximum length of time
    series will be five, so we will have vectors of five characters. Also, we are
    going to create the upcoming vector. The y_labels variable is the size of our
    vocabulary. We will use this variable later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So far, we have the sequences variable, which is an array of arrays, with the
    time series of characters. char is an array with the upcoming character. Now,
    we need to encode these vectors, so let''s define a method to encode an array
    of characters using the information of char2id:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Encode the variables into one-hot vectors. The shape of this is x = (2695,5,27)
    and y = (2695,27):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.35: Variables encoded into OneHotVectors](img/C1355_04_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.35: Variables encoded into OneHotVectors'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Split the data into train and test sets. To do this, we are going to use the
    `train_test_split` method of sklearn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.36: Splitting the data into train and test sets](img/C1355_04_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.36: Splitting the data into train and test sets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With the data ready to be inserted in the neural network, create a Sequential
    model with two layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First layer: LSTM with eight neurons (the activation is tanh). input_shape
    is the maximum length of the sequences and the size of the vocabulary. So, because
    of the shape of our data, we do not need to reshape anything.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Second layer: Dense with 27 neurons. This is how we successfully complete the
    activity. Using a LeakyRelu activation will give you a good score. But why? Our
    output has many zeroes, so the network could fail and just return a vector of
    zeroes. Using LeakyRelu prevents this problem:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model. The batch_size we use is 32, and we have 25 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.37: Training with a batch_size of 32 and 25 epochs](img/C1355_04_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.37: Training with a batch_size of 32 and 25 epochs'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Compute the error of your model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.38: Error shown in the model](img/C1355_04_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.38: Error shown in the model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Predict the test data and see the average percentage of hits. With this model,
    you will obtain an average of more than 90%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.39: Predicting the test data](img/C1355_04_39.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.39: Predicting the test data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: To end this activity, we need to create a function that accepts a sequence of
    characters and returns the next predicted value. To decode the prediction of the
    model, we first code a decode method. This method just search in the prediction
    the higher value and take the key character in the char2id dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a method to predict the next character in a given sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, introduce the sequence ''tyuio'' to predict the upcoming character.
    It will return ''p'':'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.40: Final output with the predicted sequence](img/C1355_04_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.40: Final output with the predicted sequence'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Congratulations! You have finished the activity. You can predict a value outputting
    a temporal sequence. This is also very important in finances, that is, when predicting
    future prices or stock movements.
  prefs: []
  type: TYPE_NORMAL
- en: You can change the data and predict what you want. If you add a linguistic corpus,
    you will generate text from your own RNN language model. So, our future conversational
    agent could generate poems or news text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 5: Convolutional Neural Networks for Computer Vision'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 5: Making Use of Data Augmentation to Classify correctly Images of
    Flowers'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: Open your Google Colab interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To output some samples from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.19: Samples from the dataset](img/C1355_05_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.23: Samples from the dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, we will normalize and perform one-hot encoding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Splitting the training and testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import libraries and build the CNN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Declare ImageDataGenerator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After which, we will evaluate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy achieved is **91.68%**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Try the model with unseen data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.20: Prediction of roses result from Activity 1](img/C1355_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.24: Prediction of roses result from Activity05'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The detailed code for this activity can be found on GitHub - [https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Activity05/Activity05.ipynb](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Activity05/Activity05.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 6: Robot Operating System (ROS)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 6: Simulators and Sensor'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating the packages and files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the implementation of the image obtainer node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you can see, this node is very similar to the one in *Exercise 21*, *Publishers
    and Subscribers*. The only differences are:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A counter is used for showing only one image of twenty received.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We enter `1000 (ms)` as the `Key()` parameter so that each image is shown for
    a second.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This is the implementation of the movement node:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To execute the file, we will execute the code mentioned here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run both nodes and check the system functioning. You should see the robot turning
    on itself while images of what it sees are shown. This is a sequence of the execution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.9: The first sequence of the execution of activity nodes](img/C1355_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: The first sequence of the execution of activity nodes'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 6.10: The second sequence of the execution of activity nodes](img/C1355_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: The second sequence of the execution of activity nodes'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 6.11: The third sequence of the execution of activity nodes](img/C1355_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: The third sequence of the execution of activity nodes'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The output will look similar but not exactly look as the one mentioned in figures
    6.10, 6.11, and 6.12.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have completed the activity and at the end, you will have
    an output which is like figures 6.8, 6.9, and 6.10\. By completing this activity
    successfully, you have been able to implement and work with nodes that let you
    subscribe to a camera which will show images in the virtual environment. You also
    learned to rotate a robot on itself that lets you view these images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 7: Build a Text-Based Dialogue System (Chatbot)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 7: Create a Conversational Agent to Control a Robot'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: Open up your Google Colab interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder for the book and download the `utils`, `responses`, and `training`
    folder from Github and upload it in the folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import drive and mount it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Every time you use a new collaborator, mount the drive to the desired folder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once you have mounted your drive for the first time, you will need to enter
    the authorization code mentioned by clicking on the URL mentioned by Google and
    press the Enter key on your keyboard:![Figure 7.24: Image displaying the Google
    Colab authorization step](img/C13550_05_08.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 7.28: Image displaying the Google Colab authorization step'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that you have mounted the drive, you need to set the path of the directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note:'
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The path mentioned in step 5 may change as per your folder setup on Google Drive.
    The path will always begin with cd /content/drive/My Drive/
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import the chatbot_intro file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the GloVe model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'List the responses and training sentences files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 7.25: A list of intent documents](img/C1355_07_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.29: A list of intent documents'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Create document vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![7.26: Shape of doc_vectors](img/C1355_07_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '7.30: Shape of doc_vectors'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Predict the intent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![7.27: Predicted intent](img/C1355_07_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '7.31: Predicted intent'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Congratulations! You finished the activity. You can add more intents if you
    want to and train the GloVe model to achieve better results. By creating a function
    with all the code, you programmed and developing a movement node in ROS, you can
    order your robot to make movements and turn around.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 8: Object Recognition to Guide a Robot Using CNNs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 8: Multiple Object Detection and Recognition in Video'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mount the drive:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Declare the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Declare the callback method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run Matplotlib and the video detection process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as shown in the following frames:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.7: ImageAI video object detection output](img/C1355_08_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.7: ImageAI video object detection output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the model detects objects more or less properly. Now you can
    see the output video in your chapter 8 root directory with all the object detections
    in it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There is an additional video added in the `Dataset/videos` folder – `park.mp4`.
    You can use the steps just mentioned and recognize objects in this video as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 9: Computer Vision for Robotics'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 9: A Robotic Security Guard'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new package in your catkin workspace to contain the integration node.
    Do it with this command to include the correct dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Switch to the package folder and create a new `scripts` directory. Then, create
    the Python file and make it executable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the implementation of the first node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Libraries importation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Node, subscriber, and network initialization:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Main function of the node:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Method for making predictions on images:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Publish the predictions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Program entry:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the implementation of the second node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Libraries importation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Class definition:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Node initialization and subscriber definition:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The callback function for obtaining published data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the alarm if a person is detected in the data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Program entry:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you need to set the destination to the scripts folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute the movement.py file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open a new terminal and execute the command to get the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run both nodes at the same time. This is an execution example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gazebo situation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.16: Example situation for the activity](img/C1355_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.16: Example situation for the activity'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'First node output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17: First activity node output](img/C1355_09_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.17: First activity node output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Second node output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18: Second activity node output](img/C1355_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.18: Second activity node output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
