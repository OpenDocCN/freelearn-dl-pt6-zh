["```py\n# Libraries\nlibrary(keras)\nlibrary(EBImage)\n\n# Reading and plotting images\nsetwd(\"~/Desktop/image18\")\ntemp = list.files(pattern=\"*.jpg\")\nmypic <- list()\nfor (i in 1:length(temp)) {mypic[[i]] <- readImage(temp[i])}\npar(mfrow = c(3,6))\nfor (i in 1:length(temp)) plot(mypic[[i]])\npar(mfrow = c(1,1))\n```", "```py\n# Exploring 5th image data\nprint(mypic[[5]])\n\nOUTPUT\nImage \n colorMode    : Color \n storage.mode : double \n dim          : 299 169 3 \n frames.total : 3 \n frames.render: 1 \n\nimageData(object)[1:5,1:6,1]\n [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    1    1    1    1    1\n[2,]    1    1    1    1    1    1\n[3,]    1    1    1    1    1    1\n[4,]    1    1    1    1    1    1\n[5,]    1    1    1    1    1    1\n\nhist(mypic[[5]])\n```", "```py\n# Exploring 16th image data\nprint(mypic[[16]])\n\nOUTPUT\n\nImage \n colorMode : Color \n storage.mode : double \n dim : 318 159 3 \n frames.total : 3 \n frames.render: 1 \n\nimageData(object)[1:5,1:6,1]\n [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020\n[2,] 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020\n[3,] 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020\n[4,] 0.2588235 0.2588235 0.2588235 0.2588235 0.2588235 0.2588235\n[5,] 0.2588235 0.2588235 0.2588235 0.2588235 0.2588235 0.2588235\n```", "```py\n# Resizing\nfor (i in 1:length(temp)) {mypic[[i]] <- resize(mypic[[i]], 28, 28)}\n```", "```py\n# Plot images\npar(mfrow = c(3,6))\nfor (i in 1:length(temp)) plot(mypic[[i]])\npar(mfrow = c(1,1)\n```", "```py\n# Reshape\nfor (i in 1:length(temp)) {mypic[[i]] <- array_reshape(mypic[[i]], c(28, 28,3))}\nstr(mypic)\n\nOUTPUT\n\nList of 18\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 0.953 0.953 0.953 0.953 0.953 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 0.328 ...\n $ : num [1:28, 1:28, 1:3] 0.26 0.294 0.312 0.309 0.289 ...\n $ : num [1:28, 1:28, 1:3] 0.49 0.49 0.49 0.502 0.502 ...\n $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ..\n```", "```py\n# Training Data\na <- c(1:3, 7:9, 13:15)\ntrainx <- NULL\nfor (i in a) {trainx <- rbind(trainx, mypic[[i]]) }\nstr(trainx)\n\nOUTPUT\n\nnum [1:9, 1:2352] 1 1 1 1 0.953 ...\n\n# Validation data\nb <- c(4, 10, 16)\nvalidx <- NULL\nfor (i in b) {validx <- rbind(validx, mypic[[i]]) }\nstr(validx)\n\nOUTPUT\n\nnum [1:3, 1:2352] 1 1 0.26 1 1 ...\n\n# Test Data\nc <- c(5:6, 11:12, 17:18)\ntestx <- NULL\nfor (i in c) {testx <- rbind(testx, mypic[[i]])}\nstr(testx)\n\nOUTPUT\n\nnum [1:6, 1:2352] 1 1 1 1 0.49 ...\n```", "```py\n# Labels\ntrainy <- c(0,0,0,1,1,1,2,2,2)\nvalidy <- c(0,1,2)\ntesty <- c(0,0,1,1,2,2)\n\n# One-hot encoding\ntrainLabels <- to_categorical(trainy)\nvalidLabels <- to_categorical(validy)\ntestLabels <- to_categorical(testy)\ntrainLabels\n\nOUTPUT\n [,1] [,2] [,3]\n [1,]    1    0    0\n [2,]    1    0    0\n [3,]    1    0    0\n [4,]    0    1    0\n [5,]    0    1    0\n [6,]    0    1    0\n [7,]    0    0    1\n [8,]    0    0    1\n [9,]    0    0    1\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 256, activation = 'relu', input_shape = c(2352)) %>% \n  layer_dense(units = 128, activation = 'relu') %>% \n  layer_dense(units = 3, activation = 'softmax')\nsummary(model)\n\nOUTPUT\n______________________________________________________________________\nLayer (type)                   Output Shape              Param # \n======================================================================\ndense_1 (Dense)                (None, 256)               602368 \n______________________________________________________________________\ndense_2 (Dense)                (None, 128)               32896 \n_____________________________________________________________________\ndense_3 (Dense)                (None, 3)                  387 \n======================================================================\nTotal params: 635,651\nTrainable params: 635,651\nNon-trainable params: 0\n_______________________________________________________________________\n```", "```py\n# Compile model\nmodel %>% compile(loss = 'categorical_crossentropy',\n  optimizer = 'adam',\n  metrics = 'accuracy')\n```", "```py\n# Fit model\nmodel_one <- model %>% fit(trainx, \n                         trainLabels, \n                         epochs = 30, \n                         batch_size = 32, \n                         validation_data =  list(validx, validLabels))\nplot(model_one)\n```", "```py\n# Model evaluation\nmodel %>% evaluate(trainx, trainLabels)\n\nOUTPUT\n12/12 [==============================] - 0s 87us/step\n$loss\n[1] 0.055556579307\n\n$acc\n[1] 1\n\n# Confusion matrix\npred <- model %>%   predict_classes(trainx)\ntable(Predicted=pred, Actual=trainy)\n\nOUTPUT\n Actual\nPredicted 0 1 2\n 0 3 0 0\n 1 0 3 0\n 2 0 0 3\n```", "```py\n# Prediction probabilities\nprob <- model %>%   predict_proba(trainx) \ncbind(prob, Predicted_class = pred, Actual = trainy)\n\nOUTPUT                                                \n                                                    Predicted_class Actual\n [1,] 0.9431666135788 0.007227868307 0.049605518579          0        0\n [2,] 0.8056846261024 0.005127847660 0.189187481999          0        0\n [3,] 0.9556384682655 0.001881886506 0.042479615659          0        0\n [4,] 0.0018005876336 0.988727569580 0.009471773170          1        1\n [5,] 0.0002136278927 0.998095452785 0.001690962003          1        1\n [6,] 0.0008950306219 0.994426369667 0.004678600468          1        1\n [7,] 0.0367377623916 0.010597365908 0.952664911747          2        2\n [8,] 0.0568452328444 0.011656147428 0.931498587132          2        2\n [9,] 0.0295505002141 0.011442330666 0.959007143974          2        2\n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(testx, testLabels)\n\nOUTPUT\n6/6 [==============================] - 0s 194us/step\n$loss\n[1] 0.5517520905\n\n$acc\n[1] 0.8333333\n\n# Confusion matrix\npred <- model %>%   predict_classes(testx)\ntable(Predicted=pred, Actual=testy)\n\nOUTPUT\n Actual\nPredicted 0 1 2\n 0 2 0 0\n 1 0 1 0\n 2 0 1 2\n```", "```py\n# Prediction probabilities\nprob <- model %>%   predict_proba(testx) \ncbind(prob, Predicted_class = pred, Actual = testy)\n\nOUTPUT                                         \n\n Predicted_class Actual\n[1,] 0.587377548218 0.02450981364 0.38811263442           0      0\n[2,] 0.532718658447 0.04708640277 0.42019486427           0      0\n[3,] 0.115497209132 0.18486714363 0.69963568449           2      1\n[4,] 0.001700860681 0.98481327295 0.01348586939           1      1\n[5,] 0.230999588966 0.03030913882 0.73869132996           2      2\n[6,] 0.112148292363 0.02054920420 0.86730253696           2      2\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential() \nmodel %>% \n  layer_dense(units = 512, activation = 'relu', input_shape = c(2352)) %>% \n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = 256, activation = 'relu') %>%\n  layer_dropout(rate = 0.1) %>%\n  layer_dense(units = 3, activation = 'softmax')\nsummary(model)\n\nOUTPUT\n_______________________________________________________________________\nLayer (type)                    Output Shape             Param # \n=======================================================================\ndense_1 (Dense)                  (None, 512)               1204736 \n_______________________________________________________________________\ndropout_1 (Dropout)              (None, 512)               0 \n_______________________________________________________________________\ndense_2 (Dense)                  (None, 256)              131328 \n_______________________________________________________________________\ndropout_2 (Dropout)              (None, 256)               0 \n_______________________________________________________________________\ndense_3 (Dense)                  (None, 3)                 771 \n=======================================================================\nTotal params: 1,336,835\nTrainable params: 1,336,835\nNon-trainable params: 0\n_______________________________________________________________________\n\n# Compile model\nmodel %>% compile(loss = 'categorical_crossentropy',\n  optimizer = 'adam',\n  metrics = 'accuracy')\n\n# Fit model\nmodel_two <- model %>% fit(trainx, \n trainLabels, \n epochs = 30, \n batch_size = 32, \n validation_data = list(validx, validLabels))\nplot(model_two)\n```", "```py\n# Loos and accuracy\nmodel %>% evaluate(trainx, trainLabels)\nOUTPUT\n12/12 [==============================] - 0s 198us/step\n$loss\n[1] 0.03438224643\n\n$acc\n[1] 1\n\n# Confusion matrix\npred <- model %>%   predict_classes(trainx)\ntable(Predicted=pred, Actual=trainy)\n\nOUTPUT\n Actual\nPredicted 0 1 2\n 0 3 0 0\n 1 0 3 0\n 2 0 0 3\n```", "```py\n# Prediction probabilities\nprob <- model %>%   predict_proba(trainx) \ncbind(prob, Predicted_class = pred, Actual = trainy)\n\nOUTPUT        \n Predicted_class Actual\n [1,] 0.97638195753098 0.0071088117547 0.01650915294886     0         0\n [2,] 0.89875286817551 0.0019298568368 0.09931717067957     0         0\n [3,] 0.98671281337738 0.0004396488657 0.01284754090011     0         0\n [4,] 0.00058794603683 0.9992876648903 0.00012432398216     1         1\n [5,] 0.00005639552546 0.9999316930771 0.00001191849515     1         1\n [6,] 0.00020669832884 0.9997472167015 0.00004611289114     1         1\n [7,] 0.03771930187941 0.0022936603054 0.95998704433441     2         2\n [8,] 0.08463590592146 0.0022607713472 0.91310334205627     2         2\n [9,] 0.03016609139740 0.0019471622072 0.96788680553436     2         2\n```", "```py\n# Loss and accuracy\nmodel %>% evaluate(testx, testLabels)\n\nOUTPUT\n\n6/6 [==============================] - 0s 345us/step\n$loss\n[1] 0.40148338683\n\n$acc\n[1] 0.8333333\n\n# Confusion matrix\npred <- model %>%   predict_classes(testx)\ntable(Predicted=pred, Actual=testy)\n\nOUTPUT\n Actual\nPredicted 0 1 2\n 0 2 0 0\n 1 0 1 0\n 2 0 1 2\n```", "```py\n# Prediction probabilities\nprob <- model %>%   predict_proba(testx) \ncbind(prob, Predicted_class = pred, Actual = testy)\n\nOUTPUT\n Predicted_class Actual\n[1,] 0.7411330938339 0.015922509134 0.242944419384           0      0\n[2,] 0.7733710408211 0.021422179416 0.205206796527           0      0\n[3,] 0.3322730064392 0.237866103649 0.429860889912           2      1\n[4,] 0.0005808877177 0.999227762222 0.000191345287           1      1\n[5,] 0.2163420319557 0.009395645000 0.774262309074           2      2\n[6,] 0.1447975188494 0.002772571286 0.852429926395           2      2\n```"]