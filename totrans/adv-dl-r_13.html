<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Networks for Text Classification</h1>
                </header>
            
            <article>
                
<p class="mce-root">Text data belongs to the unstructured category of data. When developing deep network models, we need to complete additional preprocessing steps due to the unique nature of such data. In this chapter, you will learn about the steps you'll need to follow to develop text classification models using deep neural networks. This process will be illustrated with easy– to– follow examples. Text data, such as customer comments, product reviews, and movie reviews, plays an important role in businesses, and text classification is an important deep learning problem.</p>
<p>In this chapter, we will discuss two text datasets, learn how to prepare text data when developing deep network classification models, look at IMDb movie review data, develop a deep network architecture, fit and evaluate the model, and discuss some tips and best practices. More specifically, in this chapter, we will cover the following topics:</p>
<ul>
<li>Text datasets</li>
<li>Preparing the data for model building</li>
<li>Developing deep neural networks</li>
<li>Model evaluation and prediction</li>
<li>Performance optimization tips and best practices</li>
</ul>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text datasets</h1>
                </header>
            
            <article>
                
<p>Text data can be used when we want to practice developing deep network models. Such data can be obtained from several publicly available sources<span>. We will go over two such</span> resources in this section<span>:</span></p>
<ul>
<li>The UCI machine learning repository</li>
<li>Text data within Keras</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The UCI machine learning repository</h1>
                </header>
            
            <article>
                
<p>The following link provides a variety of datasets that contain text sentences that have been extracted from reviews of products (from <a href="https://www.amazon.com/">amazon.com</a>), reviews of movies (from <a href="https://www.imdb.com/">IMDB.com</a>), and reviews of restaurants (from <a href="https://www.yelp.com/">yelp.com</a>): <a href="https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences">https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences</a>.</p>
<p>Each sentence is labeled in terms of the sentiment that was expressed in the reviews. This sentiment is either positive or negative. For each website, there are 500 positive and 500 negative sentences, which means there are 3,000 labeled sentences in total. This data can be used to develop a sentiment classification deep networking model that can help us automatically classify a customer review as either positive or negative.</p>
<p>The following are some examples of negative reviews from IMDb that have been labeled as 0:</p>
<ul>
<li>A very, very, very slow-moving, aimless movie about a distressed, drifting young man</li>
<li>Not sure who was more lost—the flat characters or the audience, nearly half of whom walked out</li>
<li>Attempting artiness with black and white and clever camera angles, the movie disappointed—became even more ridiculous—<span>a</span>s the acting was poor and the plot and lines almost non-existent</li>
<li>Very little music or anything to speak of </li>
</ul>
<p>The following are some examples of positive reviews from IMDb that have been labeled as 1:</p>
<ul>
<li>The best scene in the movie was when Gerardo was trying to find a song that kept running through his head</li>
<li>Saw the movie today and thought it was a good effort, good messages for kids</li>
<li>Loved the casting of Jimmy Buffet as the science teacher</li>
<li>And those baby owls were adorable</li>
<li>The movie showed a lot of Florida at its best, made it look very appealing</li>
</ul>
<p>The following are some examples of negative reviews from Amazon that are labeled as 0:</p>
<ul>
<li>So there is no way for me to plug it in here in the US unless I go by a converter</li>
<li>Tied to charger for conversations lasting more than 45 minutes. MAJOR PROBLEMS!!</li>
<li>I have to jiggle the plug to get it to line up right to get decent volume</li>
<li>If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one</li>
<li>I advise EVERYONE DO NOT BE FOOLED!</li>
</ul>
<p>The following are some examples of positive reviews from Amazon that are labeled as 1:</p>
<ul>
<li>Good case, Excellent value</li>
<li>Great for the jawbone</li>
<li>The mic is great</li>
<li>If you are Razr owner...you must have this!</li>
<li>And the sound quality is great</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text data within Keras</h1>
                </header>
            
            <article>
                
<p>There are two text datasets available within Keras, as follows:</p>
<ul>
<li><span class="st">The</span> <strong><span class="st">Internet Movie Database</span></strong> (<strong>IMDb</strong>), which contains movie review sentiment classification </li>
<li>Reuters Newswire's topics classification data</li>
</ul>
<p>The IMDb review data contains 25,000 reviews that have been classified as containing positive or negative sentiments. This data has already been preprocessed, with each review encoded as a sequence of integers. Reuters Newswire's topics classification data contains 11,228 newswires, and these have also been preprocessed, with each encoded as a sequence of integers. The newswires have been classified into 46 groups or topics, such as livestock, gold, and housing, jobs.</p>
<p>The following is an example of a positive movie review from the IMDb data from Keras:</p>
<p><em>"lavish production values and solid performances in this straightforward adaption of jane ? satirical classic about the marriage game within and between the classes in ? 18th century england northam and paltrow are a ? mixture as friends who must pass through ? and lies to discover that they love each other good humor is a ? virtue which goes a long way towards explaining the ? of the aged source material which has been toned down a bit in its harsh ? i liked the look of the film and how shots were set up and i thought it didn't rely too much on ? of head shots like most other films of the 80s and 90s do very good results."</em></p>
<p><span>The following</span> is an example of a negative movie review from the IMDb data from Keras:</p>
<p><em>"worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon bashing my head in i stuck that damn movie in the ? and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and ruining a whole day of my life."</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data for model building</h1>
                </header>
            
            <article>
                
<p>The steps we need to follow in order to prepare the data for model building are as follows:</p>
<ol>
<li>Tokenization</li>
<li>Converting text into integers</li>
<li>Padding and truncation</li>
</ol>
<p>To illustrate the steps involved in data preparation, we will make use of a very small text dataset involving five tweets related to when the Apple iPhone X released in September 2017. We will use this small dataset to understand the steps that are involved in data preparation and then we will switch to a larger IMDb dataset in order to build a deep network classification model. The following are the five tweets that we are going to store in <kbd>t1</kbd> to <kbd>t5</kbd>:</p>
<pre>t1 &lt;- "I'm not a huge $AAPL fan but $160 stock closes down $0.60 for the day on huge volume isn't really bearish"<br/>t2 &lt;- "$AAPL $BAC not sure what more dissapointing: the new iphones or the presentation for the new iphones?"<br/>t3 &lt;- "IMO, $AAPL animated emojis will be the death of $SNAP."<br/>t4 &lt;- "$AAPL get on board. It's going to 175. I think wall st will have issues as aapl pushes 1 trillion dollar valuation but 175 is in the cards"<br/>t5 &lt;- "In the AR vs. VR battle, $AAPL just put its chips behind AR in a big way."</pre>
<p>The preceding tweets include text that's in both lowercase and uppercase, punctuation, numbers, and special characters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tokenization</h1>
                </header>
            
            <article>
                
<p>Each word or number in the tweet is a token, and the process of splitting tweets into tokens is called <strong>tokenization</strong>. The code that's used to carry out tokenization is as follows:</p>
<pre>tweets &lt;- c(t1, t2, t3, t4, t5)<br/>token &lt;- text_tokenizer(num_words = 10) %&gt;%    <br/>         fit_text_tokenizer(tweets)<br/>token$index_word[1:3]<br/>$`1`<br/>[1] "the"<br/><br/>$`2`<br/>[1] "aapl"<br/><br/>$`3`<br/>[1] "in"</pre>
<p><span>From the preceding code, we can see the following:</span></p>
<ul>
<li>We started by saving five tweets in <kbd>tweets</kbd>.</li>
<li>For the tokenization process, we specified <kbd>num_words</kbd> as <kbd>10</kbd> to indicate we want to use 10 of the most frequent words and ignore any others.</li>
</ul>
<ul>
<li>Although we specified that we will have <kbd>10</kbd> frequent words, the maximum value of integers that will be used is actually going to be 10 - 1 = 9.</li>
<li>We used <kbd>fit_text_tokenizer</kbd>, which automatically converts text into lowercase and removes any punctuation from the tweets.</li>
<li>We observed that the top three most frequent words in these five tweets are <kbd>the</kbd>, <kbd>aapl</kbd>, and <kbd>in</kbd>.</li>
</ul>
<div class="packt_infobox">Note that words that have a high frequency may or may not be important for text classification.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Converting text into sequences of integers</h1>
                </header>
            
            <article>
                
<p>The following code is used to convert text into sequences of integers. The output is also provided:</p>
<pre>seq &lt;- texts_to_sequences(token, tweets)<br/><strong>seq</strong><br/><strong>[[1]]</strong><br/><strong>[1] 4 5 6 2 7 8 1 9 6</strong><br/><br/><strong>[[2]]</strong><br/><strong>[1] 2 4 1 1 8 1</strong><br/><br/><strong>[[3]]</strong><br/><strong>[1] 2 1</strong><br/><br/><strong>[[4]]</strong><br/><strong>[1] 2 9 2 7 3 1</strong><br/><br/><strong>[[5]]</strong><br/><strong>[1] 3 1 2 3 5</strong><br/></pre>
<p><span>From the preceding code, we can see the following:</span></p>
<ul>
<li>We have used <kbd>texts_to_sequences</kbd> to convert tweets into sequences of integers.</li>
<li>Since we've chosen the most frequent words for tokens to be <kbd>10</kbd>, the integers within each sequence of integers have a maximum value of 9.</li>
<li>For each tweet, the number of integers in the sequence is less than how many words there are due to only the most frequent words being used.</li>
</ul>
<ul>
<li>The sequences of integers have different lengths, ranging from 2 to 9.</li>
<li>For the purpose of developing a classification model, all of the sequences need to be the same length. This is achieved by performing padding or truncation.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Padding and truncation</h1>
                </header>
            
            <article>
                
<p>The code for making all the sequences of integers equal is as follows:</p>
<pre>pad_seq &lt;- pad_sequences(seq, maxlen = 5)<br/>pad_seq<br/><strong>     [,1] [,2] [,3] [,4] [,5]</strong><br/><strong>[1,]    7    8    1    9    6</strong><br/><strong>[2,]    4    1    1    8    1</strong><br/><strong>[3,]    0    0    0    2    1</strong><br/><strong>[4,]    9    2    7    3    1</strong><br/><strong>[5,]    3    1    2    3    5</strong></pre>
<p><span>From the preceding code, we can see the following:</span></p>
<ul>
<li>We have used <kbd>pad_sequences</kbd> so that all of the sequences of integers are equal in length.</li>
<li>When we specify the maximum length of all the sequences (using <kbd>maxlen</kbd>) to be 5, this will truncate sequences that are longer than 5 and add zeros to sequences that are shorter than 5.</li>
<li>Note that the default setting for padding here is "pre". This means that when a sequence is longer than 5, truncation will effect integers at the beginning of the sequence. We can observe this for the first sequence in the preceding output, where 4, 5, 6, and 2 have been removed.</li>
<li>Similarly, for the third sequence, which has a length of two, three zeros have been added to the beginning of the sequence. </li>
</ul>
<p>There may be situations where you may prefer to truncate or add zeroes to the end of the sequences of integers. The code to achieve this is as follows:</p>
<pre>pad_seq &lt;- pad_sequences(seq, maxlen = 5, padding = 'post')<br/>pad_seq <br/><strong>     [,1] [,2] [,3] [,4] [,5]</strong><br/><strong>[1,]    7    8    1    9    6</strong><br/><strong>[2,]    4    1    1    8    1</strong><br/><strong>[3,]    2    1    0    0    0</strong><br/><strong>[4,]    9    2    7    3    1</strong><br/><strong>[5,]    3    1    2    3    5</strong></pre>
<p>In the preceding code, we have specified the padding as <kbd>post</kbd>. The impact of this type of padding can be seen in the output, where zeros have been added to the end of sequence 3, which adds up to less than 5.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing a tweet sentiment classification model</h1>
                </header>
            
            <article>
                
<p>To develop a tweet sentiment classification model, we need labels for each tweet. However, getting labels that accurately reflect tweet sentiment is challenging. Let's take a look at some existing lexicons for sentiment classification and see why it isn't easy to get appropriate labels. With just five tweets, it isn't possible to develop a sentiment classification model. However, the idea here is to look at the process of arriving at an appropriate label for each tweet. This will help us appreciate the challenges involved in obtaining accurate labels. To automatically extract sentiment scores for each tweet, we will make use of the <kbd>syuzhet</kbd> package. We will also make use of commonly used lexicons for this purpose. The <strong>National Research Council</strong> (<strong>NRC</strong>) lexicon helps capture various emotions based on certain words. We will use the following code to obtain a sentiment score for the five tweets:</p>
<pre>library(syuzhet) <br/>get_nrc_sentiment(tweets) <br/>  anger anticipation disgust fear joy sadness surprise trust negative positive<br/>1     1            0       0    1   0       0        0     0        0        0<br/>2     0            0       0    0   0       0        0     0        0        0<br/>3     1            1       1    1   1       1        1     0        1        1<br/>4     0            1       0    0   0       0        0     0        0        0<br/>5     1            0       0    0   0       0        0     0        1        0</pre>
<p>The first tweet results in a score of 1 for both anger and fear. Although it contains the word <kbd>'bearish'</kbd>, if we were to read this tweet, we would determine that it's actually positive.</p>
<p>Let's look at the following code, which contains sentiment scores for the words <kbd>'bearish'</kbd>, <kbd>'death'</kbd>, and <kbd>'animated'</kbd>:</p>
<pre>get_nrc_sentiment('bearish') <br/>  anger anticipation disgust fear joy sadness surprise trust negative positive<br/>1     1            0       0    1   0       0        0     0        0        0<br/><br/>get_nrc_sentiment('death') <br/>  anger anticipation disgust fear joy sadness surprise trust negative positive<br/>1     1            1       1    1   0       1        1     0        1        0<br/><br/>get_nrc_sentiment('animated') <br/>  anger anticipation disgust fear joy sadness surprise trust negative positive<br/>1     0            0       0    0   1       0        0     0        0        1</pre>
<p>From the preceding code, we can determine the following:</p>
<ul>
<li>The overall score for the first tweet is based on the word italics, and nothing else.</li>
<li>The third tweet has a score of 1 for each category except trust.</li>
<li>From reading the tweet, it is obvious to us that the person writing this tweet actually feels that animated emojis will be positive for Apple and will be negative for Snapchat.</li>
<li>The sentiment scores are based on two words in this tweet: death and animated. They fail to capture the real sentiment that's expressed in the third tweet, which is very positive for Apple.</li>
</ul>
<p>When we manually label each of the five tweets with a negative sentiment, which is represented by 0, and a positive sentiment, which is represented by 1, we are likely to arrive at 1, 0, 1, 1, and 1 for our scores. Let's use the <span>following </span>code to arrive at these sentiment scores by using the <kbd>syuzhet</kbd>, <kbd>bing</kbd>, and <kbd>afinn</kbd> lexicons:</p>
<pre>get_sentiment(tweets, method="syuzhet")<br/>[1]  0.00  0.80 -0.35  0.00 -0.25<br/><br/>get_sentiment(tweets, method="bing")<br/>[1] -1  0 -1 -1  0<br/><br/>get_sentiment(tweets, method="afinn")<br/>[1]  4  0 -2  0  0</pre>
<p>Looking at results from the <kbd>syuzhet</kbd>, <kbd>bing</kbd>, and <kbd>afinn</kbd> lexicons, we can observe the following:</p>
<ul>
<li>The results vary significantly from the actual sentiments contained in the tweets. Thus, trying to automatically label a tweet with an appropriate sentiment score is difficult.</li>
<li>We saw that automatically labeling text sequences is a challenging problem. However, one solution is to label a very large number of text sequences, such as tweets, manually and then use that to develop a sentiment classification model.</li>
<li>In addition, it is important to note that such a sentiment classification model will only be helpful for the specific types of text data that were used to develop the model.</li>
<li>It isn't possible to use the same model for different text sentiment classification applications.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing deep neural networks</h1>
                </header>
            
            <article>
                
<p>Although we won't be developing a classification model based on just five tweets, let's look at the code for our model's architecture:</p>
<pre>model &lt;- keras_model_sequential()<br/>model %&gt;% layer_embedding(input_dim = 10, <br/>                          output_dim = 8, <br/>                          input_length = 5) <br/>summary(model)<br/><br/>OUTPUT<br/><strong>__________________________________________________________________________________</strong><br/><strong>Layer (type)                        Output Shape                     Param #      </strong><br/><strong>==================================================================================</strong><br/><strong>embedding_1 (Embedding)             (None, 5, 8)                     80           </strong><br/><strong>==================================================================================</strong><br/><strong>Total params: 80</strong><br/><strong>Trainable params: 80</strong><br/><strong>Non-trainable params: 0</strong><br/>________________________________________________________________________________<br/><br/>print(model$get_weights(), digits = 2)<br/><strong>[[1]]</strong><br/><strong>         [,1]    [,2]    [,3]   [,4]    [,5]    [,6]    [,7]    [,8]</strong><br/><strong> [1,]  0.0055 -0.0364 -0.0475  0.049 -0.0139 -0.0114 -0.0452 -0.0298</strong><br/><strong> [2,]  0.0398 -0.0143 -0.0406  0.023 -0.0496 -0.0124  0.0087 -0.0104</strong><br/><strong> [3,]  0.0370 -0.0321 -0.0491 -0.021 -0.0214  0.0391  0.0428 -0.0398</strong><br/><strong> [4,] -0.0257  0.0294  0.0433  0.048  0.0259 -0.0323 -0.0308  0.0224</strong><br/><strong> [5,] -0.0079 -0.0255  0.0164  0.023 -0.0486  0.0273  0.0245 -0.0020</strong><br/><strong> [6,]  0.0372  0.0464  0.0454 -0.020  0.0086 -0.0375 -0.0188  0.0395</strong><br/><strong> [7,]  0.0293  0.0305  0.0130  0.037 -0.0324 -0.0069 -0.0248  0.0178</strong><br/><strong> [8,] -0.0116 -0.0087 -0.0344  0.027  0.0132  0.0430 -0.0196 -0.0356</strong><br/><strong> [9,]  0.0314 -0.0315  0.0074 -0.044 -0.0198 -0.0135 -0.0353  0.0081</strong><br/><strong>[10,]  0.0426  0.0199 -0.0306 -0.049  0.0259 -0.0341 -0.0155  0.0147</strong><br/></pre>
<p><span>From the preceding code, we can observe the following:</span></p>
<ul>
<li>We initialized the model using <kbd>keras_model_sequential()</kbd>.</li>
<li>We specified the input dimension as 10, which is the number of most frequent words.</li>
<li>The output dimension of 8 leads to the number of parameters being 10 x 8 = 80.</li>
<li>The input length is the length of the sequence of integers.</li>
<li>We can get the weights for these 80 parameters using <kbd>model$get_weights()</kbd>.</li>
</ul>
<div class="packt_infobox">Note that these weights will change every time the model is initialized.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Obtaining IMDb movie review data</h1>
                </header>
            
            <article>
                
<p>Now, we will make use of IMDb movie review data, where the sentiment for each review has already been labeled as positive or negative. The code for accessing the IMDb movie review data from Keras is as follows:</p>
<pre>imdb &lt;- dataset_imdb(num_words = 500)  <br/>c(c(train_x, train_y), c(test_x, test_y)) %&lt;-% imdb<br/>z &lt;- NULL<br/>for (i in 1:25000) {z[i] &lt;- print(length(train_x[[i]]))}<br/>summary(z)<br/>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. <br/>   11.0   130.0   178.0   238.7   291.0  2494.0 </pre>
<p><span>From the preceding code, we can observe the following:</span></p>
<ul>
<li>We have used <kbd>train_x</kbd> and <kbd>train_y</kbd> to store the data in sequences of integers and labels representing positive or negative sentiment, respectively.</li>
<li>We used a similar convention for the test data, too.</li>
<li>Both the training and test data consist of 25,000 reviews each.</li>
<li>The summary of the sequence length shows that the minimum length for the movie reviews based on the most frequent words is 11 and that the maximum sequence length is <kbd>2494</kbd>.</li>
<li>The median sequence length is <kbd>178</kbd>.</li>
<li>The median value is less than the mean, which suggests that this data will be skewed to the right and will have a longer tail on the right-hand side.</li>
</ul>
<p>The histogram for the sequence length of the training data can be plotted as follows:<br/></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cc0f5389-0333-4e2d-9f2f-845016ae562f.png" style="width:29.75em;height:29.92em;"/></p>
<p>The preceding histogram for the length of the sequence of integers shows a right-skewed pattern. Most of the sequences have less than 500 integers.</p>
<p>Next, we will make the length of the sequence of integers equal using the following code:</p>
<pre>train_x &lt;- pad_sequences(train_x, maxlen = 100)<br/>test_x &lt;- pad_sequences(test_x, maxlen = 100)</pre>
<p><span>From the preceding code, we can observe the following:</span></p>
<ul>
<li>We have used <kbd>maxlen</kbd> of 100 to standardize the length of each sequence to 100 integers.</li>
<li>Sequences longer than 100 will have any additional integers truncated or removed, and sequences shorter than 100 will have zeros added to artificially increase the length of the sequence so that it reaches 100. We do this for both the train and test sequences.</li>
</ul>
<p>Now, we are ready to build a classification model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a classification model</h1>
                </header>
            
            <article>
                
<p>For the model architecture and model summary, we will use the following code:</p>
<pre>model &lt;- keras_model_sequential()<br/>model %&gt;% layer_embedding(input_dim = 500, <br/>                          output_dim = 16, <br/>                          input_length = 100) %&gt;%<br/>         layer_flatten() %&gt;% <br/>         layer_dense(units = 16, activation = 'relu') %&gt;%<br/>         layer_dense(units = 1, activation = "sigmoid")<br/>summary(model)<br/><br/>OUTPUT<br/><strong>___________________________________________________________________</strong><br/><strong>Layer (type)                  Output Shape              Param #    </strong><br/><strong>===================================================================</strong><br/><strong>embedding_12 (Embedding)      (None, 100, 16)           8000       </strong><br/><strong>___________________________________________________________________</strong><br/><strong>flatten_3 (Flatten)           (None, 1600)              0          </strong><br/><strong>___________________________________________________________________</strong><br/><strong>dense_6 (Dense)               (None, 16)                25616      </strong><br/><strong>___________________________________________________________________</strong><br/><strong>dense_7 (Dense)               (None, 1)                 17         </strong><br/><strong>===================================================================</strong><br/><strong>Total params: 33,633</strong><br/><strong>Trainable params: 33,633</strong><br/><strong>Non-trainable params: 0</strong><br/>___________________________________________________________________</pre>
<p><span>From the preceding code, we can observe the following:</span></p>
<ul>
<li>Here, we've added <kbd>layer_flatten()</kbd> after <kbd>layer_embedding()</kbd>.</li>
<li>This is followed by a dense layer with 16 nodes and a <kbd>relu</kbd> activation function.</li>
<li>The summary of the model shows that there are <kbd>33,633</kbd> parameters in total.</li>
</ul>
<p>Now, we can compile the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compiling the model</h1>
                </header>
            
            <article>
                
<p>We need to use the following code to compile the model:</p>
<pre>model %&gt;% compile(optimizer = "rmsprop",<br/>          loss = "binary_crossentropy",<br/>          metrics = c("acc"))</pre>
<p>From the preceding code, we can observe the following:</p>
<ul>
<li>We have used the <kbd>rmsprop</kbd> optimizer to compile the model.</li>
<li>For loss, we have used <kbd>binary_crossentropy</kbd> since the response has two values, that is, positive or negative. Metrics will make use of accuracy.</li>
</ul>
<p>Now, let's start fitting the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fitting the model</h1>
                </header>
            
            <article>
                
<p>We need to use the following code to fit the model:</p>
<pre>model_1 &lt;- model %&gt;% fit(train_x, train_y,<br/>                         epochs = 10,<br/>                         batch_size = 128,<br/>                         validation_split = 0.2)<br/>plot(model_1)</pre>
<p>As shown in the preceding code, we're using <kbd>train_x</kbd> and <kbd>train_y</kbd> to fit the model, as well as <kbd>10</kbd> epochs and a batch size of <kbd>128</kbd>. We are using 20% of the training data to assess the model's performance in terms of loss and accuracy values. After fitting the model, we obtain a plot for loss and accuracy, as shown in the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/59f959bb-5acb-4af9-ba42-2e9562be05bf.png" style="width:30.67em;height:33.00em;"/></p>
<p>From the preceding plot, we can observe the following:</p>
<ul>
<li>The plot for loss and accuracy shows divergence between the training and validation data after about four epochs.</li>
<li>Divergence between the training and validation data is observed for both the loss and accuracy values.</li>
<li>We won't be using this model since there is clear evidence that there's an overfitting problem.</li>
</ul>
<p>To overcome this overfitting problem, we need to modify the preceding code so that it appears as follows:</p>
<pre>model &lt;- keras_model_sequential()<br/>model %&gt;% layer_embedding(input_dim = 500, <br/>                          output_dim = 16, <br/>                          input_length = 100) %&gt;%<br/>         layer_flatten() %&gt;% <br/>         layer_dense(units = 16, activation = 'relu') %&gt;%<br/>         layer_dense(units = 1, activation = "sigmoid")<br/>model %&gt;% compile(optimizer = "rmsprop",<br/>          loss = "binary_crossentropy",<br/>          metrics = c("acc"))<br/>model_2 &lt;- model %&gt;% fit(train_x, train_y,<br/>                         epochs = 10,<br/>                         batch_size = 512,<br/>                         validation_split = 0.2)<br/>plot(model_2)</pre>
<p>Looking at the preceding code, we can observe the following:</p>
<ul>
<li>We're re-running the model and making only one change; that is, we're increasing the batch size to 512</li>
<li>We keep everything else the same and then fit the model using the training data</li>
</ul>
<p>After fitting the model, the loss and accuracy values that are stored in <kbd>model_2</kbd> are plotted, as shown in the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f61f34c6-0df2-48cd-9f6d-c345ffca0069.png" style="width:30.92em;height:33.25em;"/></p>
<p>From the preceding plot, we can observe the following:</p>
<ul>
<li>The loss and accuracy values show better results this time.</li>
<li>The curves for training and validation are closer to each other for both loss and accuracy.</li>
<li>In addition, the loss and accuracy values that are based on validation data don't show the severe deterioration that we had observed for the previous model, where the values for the last three epochs are flat here.</li>
<li>We were able to overcome the problem of overfitting by making minor changes to our code.</li>
</ul>
<p>We will use this model for evaluation and prediction.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model evaluation and prediction</h1>
                </header>
            
            <article>
                
<p>Now, we will evaluate the model using training and test data to obtain the loss, accuracy, and confusion matrices. Our objective is to obtain a model that can classify sentiment contained in movie reviews as either positive or negative.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation using training data</h1>
                </header>
            
            <article>
                
<p>The code to obtain the loss and accuracy values from the training data is as follows:</p>
<pre>model %&gt;% evaluate(train_x, train_y)<br/>$loss<br/>[1] 0.3745659<br/>$acc<br/>[1] 0.83428</pre>
<p>As we can see, for training data, the loss and accuracy are <kbd>0.375</kbd> and <kbd>0.834</kbd>, respectively. To look deeper into the model's sentiment classification performance, we need to develop a confusion matrix. To do so, use the following code:</p>
<pre>pred &lt;- model %&gt;%   predict_classes(train_x)<br/>table(Predicted=pred, Actual=imdb$train$y)<br/>         Actual<br/>Predicted     0     1<br/>        0 11128  2771<br/>        1  1372  9729</pre>
<p>In the preceding code, we are predicting that the classes for the training data are using the model and comparing the results with the actual sentiment classes of the movie reviews. This is summarized in a confusion matrix. We can make the following observations about the confusion matrix:</p>
<ul>
<li>The model correctly predicts the negative sentiments contained in 11,128 movie reviews.</li>
<li>The model correctly predicts the positive sentiments contained in 9,729 movie reviews.</li>
<li>Misclassifying a positive review as a negative review is higher (2,771) than misclassifying movie reviews that have a negative sentiment and have been incorrectly classified as positive (1,372). </li>
</ul>
<p>Next, we'll repeat this process with the test data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation using test data</h1>
                </header>
            
            <article>
                
<p>The code to obtain the loss and accuracy values from the test data is as follows:</p>
<pre>model %&gt;% evaluate(test_x, test_y)<br/>$loss<br/>[1] 0.4431483<br/>$acc<br/>[1] 0.79356</pre>
<p>As we can see, in terms of the test data, the loss and accuracy are <kbd>0.443</kbd> and <kbd>0.794</kbd>, respectively. These results are slightly inferior to the ones that were obtained for the training data. We can predict classes for the <kbd>test</kbd> data using the model and compare them with the actual classes of the movie reviews. This can be summarized in a confusion matrix, as follows:</p>
<pre>pred1 &lt;- model %&gt;%   predict_classes(test_x)<br/>table(Predicted=pred1, Actual=imdb$test$y)<br/>         Actual<br/>Predicted     0     1<br/>        0 10586  3247<br/>        1  1914  9253</pre>
<p>From the preceding confusion matrix, we can observe the following:</p>
<ul>
<li>Overall, this model seems to be more accurate in correctly predicting negative movie reviews (10,586) compared to positive movie reviews (9,253).</li>
<li>This pattern is consistent with the results that were obtained with the training data.</li>
<li>In addition, although 79% accuracy for test data is decent, there is still scope for improving the model's sentiment classification performance.</li>
</ul>
<p>In the next section, we will explore performance optimization tips and best practices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance optimization tips and best practices</h1>
                </header>
            
            <article>
                
<p>Now that we've obtained the test data's movie review classification accuracy, that is, 79%, we can work on improving this accuracy even further. Arriving at such an improvement may involve experimenting with the parameters in the model's architecture, the parameters that were used when we compiled the model, and/or the settings that were used while we were fitting a model. In this section, we will carry out an experiment by changing the maximum length of the sequence of words and, at the same time, use a different optimizer compared to what we used in the previous model. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Experimenting with the maximum sequence length and the optimizer</h1>
                </header>
            
            <article>
                
<p>Let's start by creating <kbd>train</kbd> and <kbd>test</kbd> data for the sequence of integers representing movie reviews and their labels using the following code:</p>
<pre>c(c(train_x, train_y), c(test_x, test_y)) %&lt;-% imdb<br/>z &lt;- NULL<br/>for (i in 1:25000) {z[i] &lt;- print(length(train_x[[i]]))}<br/>summary(z)<br/>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. <br/>   11.0   130.0   178.0   238.7   291.0  2494.0 </pre>
<p>In the preceding code, we're storing the length of the sequences based on the training data in <kbd>z</kbd>. By doing this, we get a summary of <kbd>z</kbd>. From here, we can obtain numeric summary values such as the minimum, first quartile, median, mean, third quartile, and maximum. The median value for the sequence of words is 178. In the previous sections, we used a maximum length of 100 at the time of padding the sequences so that they were of equal length. We will increase this to 200 in this experiment so that we have a number closer to the median value, as shown in the following code:</p>
<pre>imdb &lt;;- dataset_imdb(num_words = 500)  <br/>c(c(train_x, train_y), c(test_x, test_y)) %&lt;-% imdb<br/>train_x &lt;- pad_sequences(train_x, maxlen = 200) <br/>test_x &lt;- pad_sequences(test_x, maxlen = 200)<br/>model &lt;- keras_model_sequential()<br/>model %&gt;% layer_embedding(input_dim = 500, <br/>                          output_dim = 16, <br/>                          input_length = 200) %&gt;%<br/>         layer_flatten() %&gt;% <br/>         layer_dense(units = 16, activation = 'relu') %&gt;%<br/>         layer_dense(units = 1, activation = "sigmoid")<br/>model %&gt;% compile(optimizer = "adamax",  <br/>                  loss = "binary_crossentropy",<br/>                  metrics = c("acc"))<br/>model_3 &lt;- model %&gt;% fit(train_x, train_y,<br/>                         epochs = 10,<br/>                         batch_size = 512,<br/>                         validation_split = 0.2)<br/>plot(model_3)</pre>
<p>Another change we'll make is using the <kbd>adamax</kbd> optimizer when compiling the model. Note that this a variant of the popular <kbd>adam</kbd> optimizer. We keep everything else the same. After training the model, we plot the resulting loss and accuracy, as shown in the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6cae3c9d-98a4-41e9-8448-6936c2ce052c.png" style="width:34.67em;height:37.33em;"/></p>
<p>From the preceding plot for loss and accuracy, we can observe the following:</p>
<ul>
<li>The loss and accuracy values for the training and validation data show rapid improvements for about four epochs.</li>
<li>After four epochs, these improvements slow down for the training data.</li>
<li>For the validation data, the loss and accuracy values become flat for the last few epochs.</li>
<li>The plot doesn't show any cause for concern regarding overfitting.</li>
</ul>
<p>Next, we need to calculate the loss and accuracy based on the test data using the following code:</p>
<pre>model %&gt;% evaluate(test_x, test_y)<br/>$loss<br/>[1] 0.3906249<br/>$acc<br/>[1] 0.82468</pre>
<p class="mce-root">Looking at the preceding code, we can observe the following:</p>
<ul>
<li>The model's loss and accuracy, based on the test data, are <kbd>0.391</kbd> and <kbd>0.825</kbd>, respectively.</li>
<li>Both numbers indicate improvements compared to the performance we retrieved in the previous section.</li>
</ul>
<p>To look into the model's sentiment classification performance even further, we can use the following code:</p>
<pre>pred1 &lt;- model %&gt;%   predict_classes(test_x)<br/>table(Predicted=pred1, Actual=imdb$test$y)<br/>         Actual<br/>Predicted     0     1<br/>        0  9970  1853<br/>        1  2530 10647</pre>
<p>From the preceding confusion matrix, which is based on movie reviews of test data, we can observe the following:</p>
<ul>
<li>The correct classifications of negative (9,970) and positive movie reviews (10,647) are much closer now.</li>
<li>The correct classification of positive movie reviews is slightly better compared to the correct classification of negative reviews.</li>
<li>This model misclassifies a negative movie review as positive at a slightly higher rate (2,530) compared to a positive review being misclassified as a negative review (1,853).</li>
</ul>
<p>Here, experimenting with the maximum sequence length and the type of optimizer that's used to compile the model resulted in improved sentiment classification performance. You are encouraged to continue experimenting and improve the model's sentiment classification performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we started by developing deep neural networks for text classification. Due to the unique characteristics of text data, several extra preprocessing steps are required before a deep neural network sentiment classification model can be developed. We used a small sample of five tweets to go over the preprocessing steps, including tokenization, converting text data into a sequence of integers, and padding/truncation to arrive at the same sequence length. We also highlighted that automatically labeling text sequences with the appropriate sentiment is a challenging problem and general lexicons may be unable to provide useful results.</p>
<p>To develop a deep network sentiment classification model, we switched to a larger and ready-to-use IMDb movie review dataset that's available as part of Keras. To optimize the model's performance, we also experimented with parameters such as the maximum sequence length at the time of data preparation, as well as the type of optimizer that's used for compiling the model. These experiments yielded decent results; however, we will continue to explore this data so that we can improve the model's sentiment classification performance on the deep network model even further.</p>
<p>In the next chapter, we will make use of the recurrent neural network classification model, which is better suited to working with data involving sequences.</p>


            </article>

            
        </section>
    </body></html>