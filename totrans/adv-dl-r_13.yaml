- en: Deep Networks for Text Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text data belongs to the unstructured category of data. When developing deep
    network models, we need to complete additional preprocessing steps due to the
    unique nature of such data. In this chapter, you will learn about the steps you'll
    need to follow to develop text classification models using deep neural networks.
    This process will be illustrated with easy– to– follow examples. Text data, such
    as customer comments, product reviews, and movie reviews, plays an important role
    in businesses, and text classification is an important deep learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss two text datasets, learn how to prepare text
    data when developing deep network classification models, look at IMDb movie review
    data, develop a deep network architecture, fit and evaluate the model, and discuss
    some tips and best practices. More specifically, in this chapter, we will cover
    the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Text datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data for model building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing deep neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation and prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Text data can be used when we want to practice developing deep network models.
    Such data can be obtained from several publicly available sources. We will go
    over two such resources in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: The UCI machine learning repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text data within Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The UCI machine learning repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following link provides a variety of datasets that contain text sentences
    that have been extracted from reviews of products (from [amazon.com](https://www.amazon.com/)),
    reviews of movies (from [IMDB.com](https://www.imdb.com/)), and reviews of restaurants
    (from [yelp.com](https://www.yelp.com/)): [https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences).
  prefs: []
  type: TYPE_NORMAL
- en: Each sentence is labeled in terms of the sentiment that was expressed in the
    reviews. This sentiment is either positive or negative. For each website, there
    are 500 positive and 500 negative sentences, which means there are 3,000 labeled
    sentences in total. This data can be used to develop a sentiment classification
    deep networking model that can help us automatically classify a customer review
    as either positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some examples of negative reviews from IMDb that have been
    labeled as 0:'
  prefs: []
  type: TYPE_NORMAL
- en: A very, very, very slow-moving, aimless movie about a distressed, drifting young
    man
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not sure who was more lost—the flat characters or the audience, nearly half
    of whom walked out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attempting artiness with black and white and clever camera angles, the movie
    disappointed—became even more ridiculous—as the acting was poor and the plot and
    lines almost non-existent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very little music or anything to speak of
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some examples of positive reviews from IMDb that have been
    labeled as 1:'
  prefs: []
  type: TYPE_NORMAL
- en: The best scene in the movie was when Gerardo was trying to find a song that
    kept running through his head
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saw the movie today and thought it was a good effort, good messages for kids
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loved the casting of Jimmy Buffet as the science teacher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And those baby owls were adorable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The movie showed a lot of Florida at its best, made it look very appealing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some examples of negative reviews from Amazon that are labeled
    as 0:'
  prefs: []
  type: TYPE_NORMAL
- en: So there is no way for me to plug it in here in the US unless I go by a converter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tied to charger for conversations lasting more than 45 minutes. MAJOR PROBLEMS!!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have to jiggle the plug to get it to line up right to get decent volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have several dozen or several hundred contacts, then imagine the fun
    of sending each of them one by one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I advise EVERYONE DO NOT BE FOOLED!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some examples of positive reviews from Amazon that are labeled
    as 1:'
  prefs: []
  type: TYPE_NORMAL
- en: Good case, Excellent value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Great for the jawbone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mic is great
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are Razr owner...you must have this!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And the sound quality is great
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text data within Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two text datasets available within Keras, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Internet Movie Database** (**IMDb**), which contains movie review sentiment
    classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reuters Newswire's topics classification data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The IMDb review data contains 25,000 reviews that have been classified as containing
    positive or negative sentiments. This data has already been preprocessed, with
    each review encoded as a sequence of integers. Reuters Newswire's topics classification
    data contains 11,228 newswires, and these have also been preprocessed, with each
    encoded as a sequence of integers. The newswires have been classified into 46
    groups or topics, such as livestock, gold, and housing, jobs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a positive movie review from the IMDb data from
    Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"lavish production values and solid performances in this straightforward adaption
    of jane ? satirical classic about the marriage game within and between the classes
    in ? 18th century england northam and paltrow are a ? mixture as friends who must
    pass through ? and lies to discover that they love each other good humor is a
    ? virtue which goes a long way towards explaining the ? of the aged source material
    which has been toned down a bit in its harsh ? i liked the look of the film and
    how shots were set up and i thought it didn''t rely too much on ? of head shots
    like most other films of the 80s and 90s do very good results."*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a negative movie review from the IMDb data from
    Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"worst mistake of my life br br i picked this movie up at target for 5 because
    i figured hey it''s sandler i can get some cheap laughs i was wrong completely
    wrong mid way through the film all three of my friends were asleep and i was still
    suffering worst plot worst script worst movie i have ever seen i wanted to hit
    my head up against a wall for an hour then i''d stop and you know why because
    it felt damn good upon bashing my head in i stuck that damn movie in the ? and
    watched it burn and that felt better than anything else i''ve ever done it took
    american psycho army of darkness and kill bill just to get over that crap i hate
    you sandler for actually going through with this and ruining a whole day of my
    life."*'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data for model building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps we need to follow in order to prepare the data for model building
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Converting text into integers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Padding and truncation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To illustrate the steps involved in data preparation, we will make use of a
    very small text dataset involving five tweets related to when the Apple iPhone
    X released in September 2017\. We will use this small dataset to understand the
    steps that are involved in data preparation and then we will switch to a larger
    IMDb dataset in order to build a deep network classification model. The following
    are the five tweets that we are going to store in `t1` to `t5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding tweets include text that's in both lowercase and uppercase, punctuation,
    numbers, and special characters.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each word or number in the tweet is a token, and the process of splitting tweets
    into tokens is called **tokenization**. The code that''s used to carry out tokenization
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We started by saving five tweets in `tweets`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the tokenization process, we specified `num_words` as `10` to indicate we
    want to use 10 of the most frequent words and ignore any others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although we specified that we will have `10` frequent words, the maximum value
    of integers that will be used is actually going to be 10 - 1 = 9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used `fit_text_tokenizer`, which automatically converts text into lowercase
    and removes any punctuation from the tweets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We observed that the top three most frequent words in these five tweets are
    `the`, `aapl`, and `in`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that words that have a high frequency may or may not be important for text
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Converting text into sequences of integers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code is used to convert text into sequences of integers. The
    output is also provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We have used `texts_to_sequences` to convert tweets into sequences of integers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we've chosen the most frequent words for tokens to be `10`, the integers
    within each sequence of integers have a maximum value of 9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each tweet, the number of integers in the sequence is less than how many
    words there are due to only the most frequent words being used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sequences of integers have different lengths, ranging from 2 to 9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the purpose of developing a classification model, all of the sequences need
    to be the same length. This is achieved by performing padding or truncation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padding and truncation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for making all the sequences of integers equal is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We have used `pad_sequences` so that all of the sequences of integers are equal
    in length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we specify the maximum length of all the sequences (using `maxlen`) to
    be 5, this will truncate sequences that are longer than 5 and add zeros to sequences
    that are shorter than 5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the default setting for padding here is "pre". This means that when
    a sequence is longer than 5, truncation will effect integers at the beginning
    of the sequence. We can observe this for the first sequence in the preceding output,
    where 4, 5, 6, and 2 have been removed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, for the third sequence, which has a length of two, three zeros have
    been added to the beginning of the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There may be situations where you may prefer to truncate or add zeroes to the
    end of the sequences of integers. The code to achieve this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have specified the padding as `post`. The impact of
    this type of padding can be seen in the output, where zeros have been added to
    the end of sequence 3, which adds up to less than 5.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a tweet sentiment classification model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To develop a tweet sentiment classification model, we need labels for each
    tweet. However, getting labels that accurately reflect tweet sentiment is challenging.
    Let''s take a look at some existing lexicons for sentiment classification and
    see why it isn''t easy to get appropriate labels. With just five tweets, it isn''t
    possible to develop a sentiment classification model. However, the idea here is
    to look at the process of arriving at an appropriate label for each tweet. This
    will help us appreciate the challenges involved in obtaining accurate labels.
    To automatically extract sentiment scores for each tweet, we will make use of
    the `syuzhet` package. We will also make use of commonly used lexicons for this
    purpose. The **National Research Council** (**NRC**) lexicon helps capture various
    emotions based on certain words. We will use the following code to obtain a sentiment
    score for the five tweets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first tweet results in a score of 1 for both anger and fear. Although it
    contains the word `'bearish'`, if we were to read this tweet, we would determine
    that it's actually positive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following code, which contains sentiment scores for the
    words `''bearish''`, `''death''`, and `''animated''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can determine the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The overall score for the first tweet is based on the word italics, and nothing
    else.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third tweet has a score of 1 for each category except trust.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From reading the tweet, it is obvious to us that the person writing this tweet
    actually feels that animated emojis will be positive for Apple and will be negative
    for Snapchat.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The sentiment scores are based on two words in this tweet: death and animated.
    They fail to capture the real sentiment that''s expressed in the third tweet,
    which is very positive for Apple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we manually label each of the five tweets with a negative sentiment, which
    is represented by 0, and a positive sentiment, which is represented by 1, we are
    likely to arrive at 1, 0, 1, 1, and 1 for our scores. Let''s use the following code
    to arrive at these sentiment scores by using the `syuzhet`, `bing`, and `afinn`
    lexicons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at results from the `syuzhet`, `bing`, and `afinn` lexicons, we can
    observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The results vary significantly from the actual sentiments contained in the tweets.
    Thus, trying to automatically label a tweet with an appropriate sentiment score
    is difficult.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We saw that automatically labeling text sequences is a challenging problem.
    However, one solution is to label a very large number of text sequences, such
    as tweets, manually and then use that to develop a sentiment classification model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, it is important to note that such a sentiment classification model
    will only be helpful for the specific types of text data that were used to develop
    the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It isn't possible to use the same model for different text sentiment classification
    applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing deep neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although we won''t be developing a classification model based on just five
    tweets, let''s look at the code for our model''s architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We initialized the model using `keras_model_sequential()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We specified the input dimension as 10, which is the number of most frequent
    words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output dimension of 8 leads to the number of parameters being 10 x 8 = 80.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input length is the length of the sequence of integers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can get the weights for these 80 parameters using `model$get_weights()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that these weights will change every time the model is initialized.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining IMDb movie review data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will make use of IMDb movie review data, where the sentiment for each
    review has already been labeled as positive or negative. The code for accessing
    the IMDb movie review data from Keras is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We have used `train_x` and `train_y` to store the data in sequences of integers
    and labels representing positive or negative sentiment, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used a similar convention for the test data, too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the training and test data consist of 25,000 reviews each.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The summary of the sequence length shows that the minimum length for the movie
    reviews based on the most frequent words is 11 and that the maximum sequence length
    is `2494`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The median sequence length is `178`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The median value is less than the mean, which suggests that this data will be
    skewed to the right and will have a longer tail on the right-hand side.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The histogram for the sequence length of the training data can be plotted as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc0f5389-0333-4e2d-9f2f-845016ae562f.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding histogram for the length of the sequence of integers shows a right-skewed
    pattern. Most of the sequences have less than 500 integers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will make the length of the sequence of integers equal using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We have used `maxlen` of 100 to standardize the length of each sequence to 100
    integers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequences longer than 100 will have any additional integers truncated or removed,
    and sequences shorter than 100 will have zeros added to artificially increase
    the length of the sequence so that it reaches 100\. We do this for both the train
    and test sequences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we are ready to build a classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Building a classification model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the model architecture and model summary, we will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we've added `layer_flatten()` after `layer_embedding()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is followed by a dense layer with 16 nodes and a `relu` activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The summary of the model shows that there are `33,633` parameters in total.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we can compile the model.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to use the following code to compile the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We have used the `rmsprop` optimizer to compile the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For loss, we have used `binary_crossentropy` since the response has two values,
    that is, positive or negative. Metrics will make use of accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's start fitting the model.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to use the following code to fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, we''re using `train_x` and `train_y` to fit
    the model, as well as `10` epochs and a batch size of `128`. We are using 20%
    of the training data to assess the model''s performance in terms of loss and accuracy
    values. After fitting the model, we obtain a plot for loss and accuracy, as shown
    in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59f959bb-5acb-4af9-ba42-2e9562be05bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The plot for loss and accuracy shows divergence between the training and validation
    data after about four epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Divergence between the training and validation data is observed for both the
    loss and accuracy values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We won't be using this model since there is clear evidence that there's an overfitting
    problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To overcome this overfitting problem, we need to modify the preceding code
    so that it appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We're re-running the model and making only one change; that is, we're increasing
    the batch size to 512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We keep everything else the same and then fit the model using the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After fitting the model, the loss and accuracy values that are stored in `model_2`
    are plotted, as shown in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f61f34c6-0df2-48cd-9f6d-c345ffca0069.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy values show better results this time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The curves for training and validation are closer to each other for both loss
    and accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, the loss and accuracy values that are based on validation data
    don't show the severe deterioration that we had observed for the previous model,
    where the values for the last three epochs are flat here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We were able to overcome the problem of overfitting by making minor changes
    to our code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use this model for evaluation and prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation and prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will evaluate the model using training and test data to obtain the loss,
    accuracy, and confusion matrices. Our objective is to obtain a model that can
    classify sentiment contained in movie reviews as either positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation using training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code to obtain the loss and accuracy values from the training data is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, for training data, the loss and accuracy are `0.375` and `0.834`,
    respectively. To look deeper into the model''s sentiment classification performance,
    we need to develop a confusion matrix. To do so, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are predicting that the classes for the training
    data are using the model and comparing the results with the actual sentiment classes
    of the movie reviews. This is summarized in a confusion matrix. We can make the
    following observations about the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: The model correctly predicts the negative sentiments contained in 11,128 movie
    reviews.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model correctly predicts the positive sentiments contained in 9,729 movie
    reviews.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Misclassifying a positive review as a negative review is higher (2,771) than
    misclassifying movie reviews that have a negative sentiment and have been incorrectly
    classified as positive (1,372).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we'll repeat this process with the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation using test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code to obtain the loss and accuracy values from the test data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, in terms of the test data, the loss and accuracy are `0.443`
    and `0.794`, respectively. These results are slightly inferior to the ones that
    were obtained for the training data. We can predict classes for the `test` data
    using the model and compare them with the actual classes of the movie reviews.
    This can be summarized in a confusion matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding confusion matrix, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this model seems to be more accurate in correctly predicting negative
    movie reviews (10,586) compared to positive movie reviews (9,253).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pattern is consistent with the results that were obtained with the training
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, although 79% accuracy for test data is decent, there is still scope
    for improving the model's sentiment classification performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will explore performance optimization tips and best
    practices.
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've obtained the test data's movie review classification accuracy,
    that is, 79%, we can work on improving this accuracy even further. Arriving at
    such an improvement may involve experimenting with the parameters in the model's
    architecture, the parameters that were used when we compiled the model, and/or
    the settings that were used while we were fitting a model. In this section, we
    will carry out an experiment by changing the maximum length of the sequence of
    words and, at the same time, use a different optimizer compared to what we used
    in the previous model.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with the maximum sequence length and the optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by creating `train` and `test` data for the sequence of integers
    representing movie reviews and their labels using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we''re storing the length of the sequences based on
    the training data in `z`. By doing this, we get a summary of `z`. From here, we
    can obtain numeric summary values such as the minimum, first quartile, median,
    mean, third quartile, and maximum. The median value for the sequence of words
    is 178\. In the previous sections, we used a maximum length of 100 at the time
    of padding the sequences so that they were of equal length. We will increase this
    to 200 in this experiment so that we have a number closer to the median value,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Another change we''ll make is using the `adamax` optimizer when compiling the
    model. Note that this a variant of the popular `adam` optimizer. We keep everything
    else the same. After training the model, we plot the resulting loss and accuracy,
    as shown in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cae3c9d-98a4-41e9-8448-6936c2ce052c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot for loss and accuracy, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy values for the training and validation data show rapid
    improvements for about four epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After four epochs, these improvements slow down for the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the validation data, the loss and accuracy values become flat for the last
    few epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The plot doesn't show any cause for concern regarding overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we need to calculate the loss and accuracy based on the test data using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The model's loss and accuracy, based on the test data, are `0.391` and `0.825`,
    respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both numbers indicate improvements compared to the performance we retrieved
    in the previous section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To look into the model''s sentiment classification performance even further,
    we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding confusion matrix, which is based on movie reviews of test
    data, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The correct classifications of negative (9,970) and positive movie reviews (10,647)
    are much closer now.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The correct classification of positive movie reviews is slightly better compared
    to the correct classification of negative reviews.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model misclassifies a negative movie review as positive at a slightly higher
    rate (2,530) compared to a positive review being misclassified as a negative review
    (1,853).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, experimenting with the maximum sequence length and the type of optimizer
    that's used to compile the model resulted in improved sentiment classification
    performance. You are encouraged to continue experimenting and improve the model's
    sentiment classification performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by developing deep neural networks for text classification.
    Due to the unique characteristics of text data, several extra preprocessing steps
    are required before a deep neural network sentiment classification model can be
    developed. We used a small sample of five tweets to go over the preprocessing
    steps, including tokenization, converting text data into a sequence of integers,
    and padding/truncation to arrive at the same sequence length. We also highlighted
    that automatically labeling text sequences with the appropriate sentiment is a
    challenging problem and general lexicons may be unable to provide useful results.
  prefs: []
  type: TYPE_NORMAL
- en: To develop a deep network sentiment classification model, we switched to a larger
    and ready-to-use IMDb movie review dataset that's available as part of Keras.
    To optimize the model's performance, we also experimented with parameters such
    as the maximum sequence length at the time of data preparation, as well as the
    type of optimizer that's used for compiling the model. These experiments yielded
    decent results; however, we will continue to explore this data so that we can
    improve the model's sentiment classification performance on the deep network model
    even further.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will make use of the recurrent neural network classification
    model, which is better suited to working with data involving sequences.
  prefs: []
  type: TYPE_NORMAL
