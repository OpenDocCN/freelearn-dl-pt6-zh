["```py\nimport numpy as np\n\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([1, -2, 2])\nresult = np.convolve(x, y)\nprint result\n```", "```py\nimport numpy as np\nfrom scipy import signal\n\na = np.matrix('1 3 1; 0 -1 1; 2 2 -1')\nprint(a)\nw = np.matrix('1 2; 0 -1')\nprint(w)\n\nf = signal.convolve2d(a, w)\nprint(f)\n```", "```py\nf = signal.convolve2d(a, w, 'valid')\n```", "```py\nsudo pip install scikit-learn\n```", "```py\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.constraints import maxnorm\nfrom keras.optimizers import SGD\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras.datasets import cifar10\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\n```", "```py\nK.set_image_dim_ordering('th')\n (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n```", "```py\nseed = 7\n np.random.seed(seed)\n```", "```py\nX_train = X_train.astype('float32')\n X_test = X_test.astype('float32')\n\n X_train = X_train / 255.0\n X_test = X_test / 255.0\n```", "```py\ny_train = np_utils.to_categorical(y_train)\n y_test = np_utils.to_categorical(y_test)\n num_classes = y_test.shape[1]\n```", "```py\nmodel = Sequential()\nmodel.add(Conv2D(32,(3,3), input_shape = (3,32,32), padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(32,(3,3), padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(64,(3,3), padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\n```", "```py\nepochs = 25\n lrate = 0.01\n decay = lrate/epochs\n sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n```", "```py\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n```", "```py\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n```", "```py\nscores = model.evaluate(X_test,y_test,verbose=0)\n print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n```", "```py\nfrom keras.layers import Activation\n from keras.layers import BatchNormalization\n```", "```py\nmodel = Sequential()\nmodel.add(Conv2D(32, (3,3), padding='same', input_shape=x_train.shape[1:]))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, (3,3), padding='same'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(64, (3,3), padding='same'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3,3), padding='same'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(128, (3,3), padding='same'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3,3), padding='same'))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\nmodel.add(Dense(num_classes, activation='softmax'))\n```", "```py\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    )\ndatagen.fit(X_train)\n```", "```py\nbatch_size = 64\n\nmodel.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n                 steps_per_epoch=X_train.shape[0] // batch_size,epochs=125,\\\n                 verbose=1,validation_data=(X_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n```", "```py\nweight_decay = 1e-4\nmodel = Sequential()\nmodel.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=X_train.shape[1:]))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\nmodel.add(Dense(num_classes, activation='softmax'))\n```", "```py\nval trainDataSetIterator =\n                 new CifarDataSetIterator(2, 5000, true)\n val testDataSetIterator =\n                 new CifarDataSetIterator(2, 200, false)\n```", "```py\ndef defineModelConfiguration(): MultiLayerConfiguration =\n     new NeuralNetConfiguration.Builder()\n        .seed(seed)\n        .cacheMode(CacheMode.DEVICE)\n        .updater(new Adam(1e-2))\n        .biasUpdater(new Adam(1e-2*2))\n        .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer)\n        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n        .l1(1e-4)\n        .l2(5 * 1e-4)\n        .list\n        .layer(0, new ConvolutionLayer.Builder(Array(4, 4), Array(1, 1), Array(0, 0)).name(\"cnn1\").convolutionMode(ConvolutionMode.Same)\n            .nIn(3).nOut(64).weightInit(WeightInit.XAVIER_UNIFORM).activation(Activation.RELU)\n            .biasInit(1e-2).build)\n        .layer(1, new ConvolutionLayer.Builder(Array(4, 4), Array(1, 1), Array(0, 0)).name(\"cnn2\").convolutionMode(ConvolutionMode.Same)\n            .nOut(64).weightInit(WeightInit.XAVIER_UNIFORM).activation(Activation.RELU)\n            .biasInit(1e-2).build)\n        .layer(2, new SubsamplingLayer.Builder(PoolingType.MAX, Array(2,2)).name(\"maxpool2\").build())\n\n        .layer(3, new ConvolutionLayer.Builder(Array(4, 4), Array(1, 1), Array(0, 0)).name(\"cnn3\").convolutionMode(ConvolutionMode.Same)\n            .nOut(96).weightInit(WeightInit.XAVIER_UNIFORM).activation(Activation.RELU)\n            .biasInit(1e-2).build)\n        .layer(4, new ConvolutionLayer.Builder(Array(4, 4), Array(1, 1), Array(0, 0)).name(\"cnn4\").convolutionMode(ConvolutionMode.Same)\n            .nOut(96).weightInit(WeightInit.XAVIER_UNIFORM).activation(Activation.RELU)\n            .biasInit(1e-2).build)\n\n        .layer(5, new ConvolutionLayer.Builder(Array(3,3), Array(1, 1), Array(0, 0)).name(\"cnn5\").convolutionMode(ConvolutionMode.Same)\n            .nOut(128).weightInit(WeightInit.XAVIER_UNIFORM).activation(Activation.RELU)\n            .biasInit(1e-2).build)\n        .layer(6, new ConvolutionLayer.Builder(Array(3,3), Array(1, 1), Array(0, 0)).name(\"cnn6\").convolutionMode(ConvolutionMode.Same)\n            .nOut(128).weightInit(WeightInit.XAVIER_UNIFORM).activation(Activation.RELU)\n            .biasInit(1e-2).build)\n\n        .layer(7, new ConvolutionLayer.Builder(Array(2,2), Array(1, 1), Array(0, 0)).name(\"cnn7\").convolutionMode(ConvolutionMode.Same)\n            .nOut(256).weightInit(WeightInit.XAVIER_UNIFORM).activation(Activation.RELU)\n            .biasInit(1e-2).build)\n        .layer(8, new ConvolutionLayer.Builder(Array(2,2), Array(1, 1), Array(0, 0)).name(\"cnn8\").convolutionMode(ConvolutionMode.Same)\n            .nOut(256).weightInit(WeightInit.XAVIER_UNIFORM).activation(Activation.RELU)\n            .biasInit(1e-2).build)\n        .layer(9, new SubsamplingLayer.Builder(PoolingType.MAX, Array(2,2)).name(\"maxpool8\").build())\n\n        .layer(10, new DenseLayer.Builder().name(\"ffn1\").nOut(1024).updater(new Adam(1e-3)).biasInit(1e-3).biasUpdater(new Adam(1e-3*2)).build)\n        .layer(11,new DropoutLayer.Builder().name(\"dropout1\").dropOut(0.2).build)\n        .layer(12, new DenseLayer.Builder().name(\"ffn2\").nOut(1024).biasInit(1e-2).build)\n        .layer(13,new DropoutLayer.Builder().name(\"dropout2\").dropOut(0.2).build)\n        .layer(14, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n            .name(\"output\")\n            .nOut(numLabels)\n            .activation(Activation.SOFTMAX)\n            .build)\n        .backprop(true)\n        .pretrain(false)\n        .setInputType(InputType.convolutional(height, width, channels))\n        .build\n```", "```py\nval conf = defineModelConfiguration\n val model = new MultiLayerNetwork(conf)\n model.init\n```", "```py\nval epochs = 10\n for(idx <- 0 to epochs) {\n     model.fit(trainDataSetIterator)\n }\n```", "```py\nval eval = new Evaluation(testDataSetIterator.getLabels)\n while(testDataSetIterator.hasNext) {\n     val testDS = testDataSetIterator.next(batchSize)\n     val output = model.output(testDS.getFeatures)\n     eval.eval(testDS.getLabels, output)\n }\n println(eval.stats)\n```", "```py\n// Init the Spark context\n val sparkConf = new SparkConf\n sparkConf.setMaster(master)\n   .setAppName(\"Object Recognition Example\")\n val sc = new JavaSparkContext(sparkConf)\n\n // Parallelize data\n val trainDataList = mutable.ArrayBuffer.empty[DataSet]\n while (trainDataSetIterator.hasNext) {\n   trainDataList += trainDataSetIterator.next\n }\n val paralleltrainData = sc.parallelize(trainDataList)\n\n // Create the TrainingMaster\n var batchSizePerWorker: Int = 16\n val tm = new\n   ParameterAveragingTrainingMaster.Builder(batchSizePerWorker)\n   .averagingFrequency(5)\n   .workerPrefetchNumBatches(2)\n   .batchSizePerWorker(batchSizePerWorker)\n   .build\n\n // Training\n val sparkNet = new SparkDl4jMultiLayer(sc, conf, tm)\n for (i <- 0 until epochs) {\n   sparkNet.fit(paralleltrainData)\n   println(\"Completed Epoch {}\", i)\n }\n```"]