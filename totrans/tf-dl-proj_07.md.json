["```py\ndef download_and_decompress(url, storage_path, storage_dir):\n   import os.path\n   directory = storage_path + \"/\" + storage_dir\n   zip_file = directory + \".zip\"\n   a_file = directory + \"/cornell movie-dialogs corpus/README.txt\"\n   if not os.path.isfile(a_file):\n       import urllib.request\n       import zipfile\n       urllib.request.urlretrieve(url, zip_file)\n       with zipfile.ZipFile(zip_file, \"r\") as zfh:\n           zfh.extractall(directory)\n   return\n```", "```py\nimport re\ndef read_conversations(storage_path, storage_dir):\n   filename = storage_path + \"/\" + storage_dir + \"/cornell movie-dialogs corpus/movie_conversations.txt\"\n   with open(filename, \"r\", encoding=\"ISO-8859-1\") as fh:\n       conversations_chunks = [line.split(\" +++$+++ \") for line in fh]\n   return [re.sub('[\\[\\]\\']', '', el[3].strip()).split(\", \") for el in conversations_chunks]\n```", "```py\ndef read_lines(storage_path, storage_dir):\n   filename = storage_path + \"/\" + storage_dir + \"/cornell movie-dialogs corpus/movie_lines.txt\"\n   with open(filename, \"r\", encoding=\"ISO-8859-1\") as fh:\n       lines_chunks = [line.split(\" +++$+++ \") for line in fh]\n   return {line[0]: line[-1].strip() for line in lines_chunks}\n```", "```py\ndef get_tokenized_sequencial_sentences(list_of_lines, line_text):\n   for line in list_of_lines:\n       for i in range(len(line) - 1):\n           yield (line_text[line[i]].split(\" \"), line_text[line[i+1]].split(\" \"))\n```", "```py\ndef retrieve_cornell_corpora(storage_path=\"/tmp\", storage_dir=\"cornell_movie_dialogs_corpus\"):\n   download_and_decompress(\"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\",      \n                     storage_path,\n                           storage_dir)\n   conversations = read_conversations(storage_path, storage_dir)\n   lines = read_lines(storage_path, storage_dir)\n   return tuple(zip(*list(get_tokenized_sequencial_sentences(conversations, lines))))\n```", "```py\nfrom corpora_tools import *\nfrom corpora_downloader import retrieve_cornell_corpora\nsen_l1, sen_l2 = retrieve_cornell_corpora()\nprint(\"# Two consecutive sentences in a conversation\")\nprint(\"Q:\", sen_l1[0])\nprint(\"A:\", sen_l2[0])\nprint(\"# Corpora length (i.e. number of sentences)\")\nprint(len(sen_l1))\nassert len(sen_l1) == len(sen_l2)\n```", "```py\n# Two consecutive sentences in a conversation\nQ: ['Can', 'we', 'make', 'this', 'quick?', '', 'Roxanne', 'Korrine', 'and', 'Andrew', 'Barrett', 'are', 'having', 'an', 'incredibly', 'horrendous', 'public', 'break-', 'up', 'on', 'the', 'quad.', '', 'Again.']\nA: ['Well,', 'I', 'thought', \"we'd\", 'start', 'with', 'pronunciation,', 'if', \"that's\", 'okay', 'with', 'you.']\n# Corpora length (i.e. number of sentences)\n221616\n```", "```py\nclean_sen_l1 = [clean_sentence(s) for s in sen_l1]\nclean_sen_l2 = [clean_sentence(s) for s in sen_l2]\nfilt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(clean_sen_l1, clean_sen_l2)\nprint(\"# Filtered Corpora length (i.e. number of sentences)\")\nprint(len(filt_clean_sen_l1))\nassert len(filt_clean_sen_l1) == len(filt_clean_sen_l2)\n```", "```py\n# Filtered Corpora length (i.e. number of sentences)\n140261\n```", "```py\ndict_l1 = create_indexed_dictionary(filt_clean_sen_l1, dict_size=15000, storage_path=\"/tmp/l1_dict.p\")\ndict_l2 = create_indexed_dictionary(filt_clean_sen_l2, dict_size=15000, storage_path=\"/tmp/l2_dict.p\")\nidx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\nidx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\nprint(\"# Same sentences as before, with their dictionary ID\")\nprint(\"Q:\", list(zip(filt_clean_sen_l1[0], idx_sentences_l1[0])))\nprint(\"A:\", list(zip(filt_clean_sen_l2[0], idx_sentences_l2[0])))\n```", "```py\n[sentences_to_indexes] Did not find 16823 words\n[sentences_to_indexes] Did not find 16649 words\n# Same sentences as before, with their dictionary ID\nQ: [('well', 68), (',', 8), ('i', 9), ('thought', 141), ('we', 23), (\"'\", 5), ('d', 83), ('start', 370), ('with', 46), ('pronunciation', 3), (',', 8), ('if', 78), ('that', 18), (\"'\", 5), ('s', 12), ('okay', 92), ('with', 46), ('you', 7), ('.', 4)]\nA: [('not', 31), ('the', 10), ('hacking', 7309), ('and', 23), ('gagging', 8761), ('and', 23), ('spitting', 6354), ('part', 437), ('.', 4), ('please', 145), ('.', 4)]\n```", "```py\ndata_set = prepare_sentences(idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2)\nprint(\"# Prepared minibatch with paddings and extra stuff\")\nprint(\"Q:\", data_set[0][0])\nprint(\"A:\", data_set[0][1])\nprint(\"# The sentence pass from X to Y tokens\")\nprint(\"Q:\", len(idx_sentences_l1[0]), \"->\", len(data_set[0][0]))\nprint(\"A:\", len(idx_sentences_l2[0]), \"->\", len(data_set[0][1]))\n```", "```py\n# Prepared minibatch with paddings and extra stuff\nQ: [0, 68, 8, 9, 141, 23, 5, 83, 370, 46, 3, 8, 78, 18, 5, 12, 92, 46, 7, 4]\nA: [1, 31, 10, 7309, 23, 8761, 23, 6354, 437, 4, 145, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n# The sentence pass from X to Y tokens\nQ: 19 -> 20\nA: 11 -> 22\n```", "```py\ndef build_dataset(use_stored_dictionary=False):\n   sen_l1, sen_l2 = retrieve_cornell_corpora()\n   clean_sen_l1 = [clean_sentence(s) for s in sen_l1][:30000] ### OTHERWISE IT DOES NOT RUN ON MY LAPTOP\n   clean_sen_l2 = [clean_sentence(s) for s in sen_l2][:30000] ### OTHERWISE IT DOES NOT RUN ON MY LAPTOP\n   filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(clean_sen_l1, clean_sen_l2, max_len=10)\n   if not use_stored_dictionary:\n       dict_l1 = create_indexed_dictionary(filt_clean_sen_l1, dict_size=10000, storage_path=path_l1_dict)\n       dict_l2 = create_indexed_dictionary(filt_clean_sen_l2, dict_size=10000, storage_path=path_l2_dict)\n   else:\n       dict_l1 = pickle.load(open(path_l1_dict, \"rb\"))\n       dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n   dict_l1_length = len(dict_l1)\n   dict_l2_length = len(dict_l2)\n   idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n   idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n   max_length_l1 = extract_max_length(idx_sentences_l1)\n   max_length_l2 = extract_max_length(idx_sentences_l2)\n   data_set = prepare_sentences(idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2)\n   return (filt_clean_sen_l1, filt_clean_sen_l2), \\\n           data_set, \\\n           (max_length_l1, max_length_l2), \\\n           (dict_l1_length, dict_l2_length)\n```", "```py\n[sentences_to_indexes] Did not find 0 words\n[sentences_to_indexes] Did not find 0 words\nglobal step 100 learning rate 1.0 step-time 7.708967611789704 perplexity 444.90090078460474\neval: perplexity 57.442316329639176\nglobal step 200 learning rate 0.990234375 step-time 7.700247814655302 perplexity 48.8545568311572\neval: perplexity 42.190180314697045\nglobal step 300 learning rate 0.98046875 step-time 7.69800933599472 perplexity 41.620538109894945\neval: perplexity 31.291903031786116\n...\n...\n...\nglobal step 2400 learning rate 0.79833984375 step-time 7.686293318271639 perplexity 3.7086356605442767\neval: perplexity 2.8348589631663046\nglobal step 2500 learning rate 0.79052734375 step-time 7.689657487869262 perplexity 3.211876894960698\neval: perplexity 2.973809378544393\nglobal step 2600 learning rate 0.78271484375 step-time 7.690396382808681 perplexity 2.878854805600354\neval: perplexity 2.563583924617356\n```", "```py\nimport pickle\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport data_utils\nfrom corpora_tools import clean_sentence, sentences_to_indexes, prepare_sentences\nfrom train_chatbot import get_seq2seq_model, path_l1_dict, path_l2_dict\nmodel_dir = \"/home/abc/chat/chatbot_model\"\ndef prepare_sentence(sentence, dict_l1, max_length):\n   sents = [sentence.split(\" \")]\n   clean_sen_l1 = [clean_sentence(s) for s in sents]\n   idx_sentences_l1 = sentences_to_indexes(clean_sen_l1, dict_l1)\n   data_set = prepare_sentences(idx_sentences_l1, [[]], max_length, max_length)\n   sentences = (clean_sen_l1, [[]])\n   return sentences, data_set\n```", "```py\ndef decode(data_set):\nwith tf.Session() as sess:\n   model = get_seq2seq_model(sess, True, dict_lengths, max_sentence_lengths, model_dir)\n   model.batch_size = 1\n   bucket = 0\n   encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n     {bucket: [(data_set[0][0], [])]}, bucket)\n   _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n                                   target_weights, bucket, True)\n   outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n   if data_utils.EOS_ID in outputs:\n       outputs = outputs[1:outputs.index(data_utils.EOS_ID)]\ntf.reset_default_graph()\nreturn \" \".join([tf.compat.as_str(inv_dict_l2[output]) for output in outputs])\n```", "```py\nif __name__ == \"__main__\":\n   dict_l1 = pickle.load(open(path_l1_dict, \"rb\"))\n   dict_l1_length = len(dict_l1)\n   dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n   dict_l2_length = len(dict_l2)\n   inv_dict_l2 = {v: k for k, v in dict_l2.items()}\n   max_lengths = 10\n   dict_lengths = (dict_l1_length, dict_l2_length)\n   max_sentence_lengths = (max_lengths, max_lengths)\n   from bottle import route, run, request\n   @route('/api')\n   def api():\n       in_sentence = request.query.sentence\n     _, data_set = prepare_sentence(in_sentence, dict_l1, max_lengths)\n       resp = [{\"in\": in_sentence, \"out\": decode(data_set)}]\n       return dict(data=resp)\n   run(host='127.0.0.1', port=8080, reloader=True, debug=True)\n```", "```py\n$> python3 â€“u test_chatbot_aas.py\n```", "```py\n$> curl -X GET -G http://127.0.0.1:8080/api --data-urlencode \"sentence=how are you?\"\n{\"data\": [{\"out\": \"i ' m here with you .\", \"in\": \"where are you?\"}]}\n$> curl -X GET -G http://127.0.0.1:8080/api --data-urlencode \"sentence=are you here?\"\n{\"data\": [{\"out\": \"yes .\", \"in\": \"are you here?\"}]}\n$> curl -X GET -G http://127.0.0.1:8080/api --data-urlencode \"sentence=are you a chatbot?\"\n{\"data\": [{\"out\": \"you ' for the stuff to be right .\", \"in\": \"are you a chatbot?\"}]}\n$> curl -X GET -G http://127.0.0.1:8080/api --data-urlencode \"sentence=what is your name ?\"\n{\"data\": [{\"out\": \"we don ' t know .\", \"in\": \"what is your name ?\"}]}\n$> curl -X GET -G http://127.0.0.1:8080/api --data-urlencode \"sentence=how are you?\"\n{\"data\": [{\"out\": \"that ' s okay .\", \"in\": \"how are you?\"}]}\n```"]