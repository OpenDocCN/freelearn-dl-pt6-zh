<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;9.&#xA0;Selecting Relevant Inputs or Memories with the Mechanism of Attention"><div class="book" id="2MP362-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09" class="calibre1"/>Chapter 9. Selecting Relevant Inputs or Memories with the Mechanism of Attention</h1></div></div></div><p class="calibre8">This chapter introduces a mechanism of attention to neural network performance, and enables networks to improve their performance by focusing on relevant parts of their inputs or memories.</p><p class="calibre8">With such a mechanism, translations, annotations, explanations, and segmentations, as seen in previous chapter, enjoy greater accuracy.</p><p class="calibre8">Inputs and outputs of a neural network may also be connected to <span class="strong"><em class="calibre12">reads</em></span> and <span class="strong"><em class="calibre12">writes</em></span> to an external memory. These networks, <span class="strong"><strong class="calibre2">memory networks</strong></span>, are enhanced with an external memory and capable of deciding what information, and from where, to store or retrieve.</p><p class="calibre8">In this chapter, we'll discuss:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The mechanism of attention</li><li class="listitem">Aligning translations</li><li class="listitem">Focus in images</li><li class="listitem">Neural Turing Machines</li><li class="listitem">Memory networks</li><li class="listitem">Dynamic memory networks</li></ul></div></div>

<div class="book" title="Chapter&#xA0;9.&#xA0;Selecting Relevant Inputs or Memories with the Mechanism of Attention">
<div class="book" title="Differentiable mechanism of attention"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch09lvl1sec83" class="calibre1"/>Differentiable mechanism of attention</h1></div></div></div><p class="calibre8">When <a id="id355" class="calibre1"/>translating a sentence, describing the content of an image, annotating a sentence, or transcribing an audio, it sounds natural to focus on one part at a time of the input sentence or image, to get the sense of the block and transform it, before moving to the next part, under a certain order for global understanding.</p><p class="calibre8">For example, in the German language, under certain conditions, verbs come at the end of the sentence, so, when translating to English, once the subject has been read and translated, a good machine translation neural network could move its focus to the end of the <a id="id356" class="calibre1"/>sentence to find the verb and translate it into English. This process of matching input positions to current output predictions is possible through the <span class="strong"><em class="calibre12">mechanism of attention</em></span>.</p><p class="calibre8">First, let's come back to classification networks that have been designed with a softmax layer (see <a class="calibre1" title="Chapter 2. Classifying Handwritten Digits with a Feedforward Network" href="part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 2</a>, <span class="strong"><em class="calibre12">Classifying Handwritten Digits with a Feedforward Network</em></span>) that outputs a non-negative weight vector <span class="strong"><img src="../images/00130.jpeg" alt="Differentiable mechanism of attention" class="calibre23"/></span> that sums to <span class="strong"><em class="calibre12">1</em></span> given an input X:</p><div class="mediaobject"><img src="../images/00131.jpeg" alt="Differentiable mechanism of attention" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Then:</p><div class="mediaobject"><img src="../images/00132.jpeg" alt="Differentiable mechanism of attention" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The objective of classification is to have <span class="strong"><img src="../images/00133.jpeg" alt="Differentiable mechanism of attention" class="calibre23"/></span> as close as possible to <span class="strong"><em class="calibre12">1</em></span> for the correct class <span class="strong"><em class="calibre12">k</em></span>, and near zero for the other classes.</p><p class="calibre8">But <span class="strong"><img src="../images/00133.jpeg" alt="Differentiable mechanism of attention" class="calibre23"/></span> is a probability distribution, and can also be used as a weight vector to pay attention to some values of a memory vector <span class="strong"><img src="../images/00134.jpeg" alt="Differentiable mechanism of attention" class="calibre23"/></span> at a position <span class="strong"><em class="calibre12">k</em></span>:</p><div class="mediaobject"><img src="../images/00135.jpeg" alt="Differentiable mechanism of attention" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">It returns <span class="strong"><img src="../images/00136.jpeg" alt="Differentiable mechanism of attention" class="calibre23"/></span> if the weights focus on position <span class="strong"><em class="calibre12">k</em></span>. Depending on the sharpness of the weights, the output will be more or less blurry.</p><p class="calibre8">This <a id="id357" class="calibre1"/>mechanism of addressing the value of the vector <span class="strong"><em class="calibre12">m</em></span> at a particular position is an <span class="strong"><strong class="calibre2">attention mechanism</strong></span>: that is, it's linear, differentiable, and has <a id="id358" class="calibre1"/>a back-propagation gradient descent for training on specific tasks.</p></div></div>

<div class="book" title="Chapter&#xA0;9.&#xA0;Selecting Relevant Inputs or Memories with the Mechanism of Attention">
<div class="book" title="Differentiable mechanism of attention">
<div class="book" title="Better translations with attention mechanism"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec25" class="calibre1"/>Better translations with attention mechanism</h2></div></div></div><p class="calibre8">The <a id="id359" class="calibre1"/>applications for attention mechanisms are very large. To get a better understanding, let us first illustrate it with the example of machine translation. Attention mechanism aligns the source sentence and the target sentence (predicted translation), and avoids translation degradation for long sentences:</p><div class="mediaobject"><img src="../images/00137.jpeg" alt="Better translations with attention mechanism" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">In the previous chapter, we addressed the machine translation with an encoder-decoder framework and a fixed-length encoded vector <span class="strong"><em class="calibre12">c</em></span> provided by the encoder to the decoder. With the attention mechanism, if each step of the encoding recurrent network produces a hidden state <span class="strong"><em class="calibre12">h</em></span>
<span class="strong"><em class="calibre12">i</em></span>, the vector provided to the decoder at each decoding time step <span class="strong"><em class="calibre12">t</em></span> will be variable and given by:</p><div class="mediaobject"><img src="../images/00138.jpeg" alt="Better translations with attention mechanism" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">With <span class="strong"><img src="../images/00139.jpeg" alt="Better translations with attention mechanism" class="calibre23"/></span> the alignment coefficients produced by a softmax function:</p><div class="mediaobject"><img src="../images/00140.jpeg" alt="Better translations with attention mechanism" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Depending <a id="id360" class="calibre1"/>on the previous hidden state of the decoder <span class="strong"><img src="../images/00141.jpeg" alt="Better translations with attention mechanism" class="calibre23"/></span> and the encoding hidden states <span class="strong"><img src="../images/00142.jpeg" alt="Better translations with attention mechanism" class="calibre23"/></span>, the embedded dot product between the previous decoder hidden state and each encoder hidden state produces a weight that describes how they should match:</p><div class="mediaobject"><img src="../images/00143.jpeg" alt="Better translations with attention mechanism" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">After a few epochs of training, the model predicts each next word by focusing on a part of the input:</p><div class="mediaobject"><img src="../images/00144.jpeg" alt="Better translations with attention mechanism" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">To learn <a id="id361" class="calibre1"/>to align better, it is possible to use the alignment annotations present in the dataset, and add a cross entropy loss for the weights produced by the attention mechanism, to be used in the first epochs of training.</p></div></div></div>

<div class="book" title="Chapter&#xA0;9.&#xA0;Selecting Relevant Inputs or Memories with the Mechanism of Attention">
<div class="book" title="Differentiable mechanism of attention">
<div class="book" title="Better annotate images with attention mechanism"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec26" class="calibre1"/>Better annotate images with attention mechanism</h2></div></div></div><p class="calibre8">The same <a id="id362" class="calibre1"/>mechanism of attention can be applied to the tasks of annotating images or transcribing audio.</p><p class="calibre8">For images, the attention mechanism focuses on the relevant part of the features at each predicting time step:</p><div class="mediaobject"><img src="../images/00145.jpeg" alt="Better annotate images with attention mechanism" class="calibre9"/><div class="caption"><p class="calibre29">Show, attend and tell: neural image caption generation with visual attention</p></div></div><p class="calibre10"> </p><p class="calibre8">Let's have a look at the point of attention on images for a trained model:</p><div class="mediaobject"><img src="../images/00146.jpeg" alt="Better annotate images with attention mechanism" class="calibre9"/><div class="caption"><p class="calibre29">(<span class="strong"><em class="calibre12">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</em></span>, by Kelvin Xu et al., 2015)</p></div></div><p class="calibre10"> </p></div></div></div>
<div class="book" title="Store and retrieve information in Neural Turing Machines"><div class="book" id="2NNJO2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec84" class="calibre1"/>Store and retrieve information in Neural Turing Machines</h1></div></div></div><p class="calibre8">Attention <a id="id363" class="calibre1"/>mechanism can be used as an access to a part of memory in the memory-augmented networks.</p><p class="calibre8">The <a id="id364" class="calibre1"/>concept of memory in Neural Turing Machines has been inspired by both neuroscience and computer hardware.</p><p class="calibre8">RNN hidden states to store information is not capable of storing sufficiently large amounts of data and retrieving it, even when the RNN is augmented with a memory cell, such as in the case of LSTM.</p><p class="calibre8">To solve <a id="id365" class="calibre1"/>this problem, <span class="strong"><strong class="calibre2">Neural Turing Machines</strong></span> (<span class="strong"><strong class="calibre2">NTM</strong></span>) have been first designed with an <span class="strong"><strong class="calibre2">external memory bank</strong></span> and read/write heads, whilst retaining the magic of being trained via gradient descent.</p><p class="calibre8">Reading the <a id="id366" class="calibre1"/>memory bank is given by an attention on the variable memory bank as the attention on inputs in the previous examples:</p><div class="mediaobject"><img src="../images/00147.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Which can be illustrated the following way:</p><div class="mediaobject"><img src="../images/00148.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">While <a id="id367" class="calibre1"/>writing a value to the <a id="id368" class="calibre1"/>memory bank consists of assigning our new value to part of the memory, thanks to another attention mechanism:</p><div class="mediaobject"><img src="../images/00149.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre9"/></div><p class="calibre10"> </p><div class="mediaobject"><img src="../images/00150.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8"> describes the information to store, and <span class="strong"><img src="../images/00151.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre23"/></span> the information to erase, and are each the size of the memory bank:</p><div class="mediaobject"><img src="../images/00152.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The <a id="id369" class="calibre1"/>read and write heads are <a id="id370" class="calibre1"/>designed as in a hard drive and their mobility is imagined by the attention weights <span class="strong"><img src="../images/00153.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre23"/></span> and <span class="strong"><img src="../images/00154.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre23"/></span>.</p><p class="calibre8">The memory <span class="strong"><img src="../images/00155.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre23"/></span> will evolve at every timestep as the cell memory of a LSTM; but, since the memory bank is designed to be large, the network tends to store and organize the incoming <a id="id371" class="calibre1"/>data at every timestep with less interference than for any classical RNN.</p><p class="calibre8">The process to work with the memory is naturally been driven with a recurrent neural network acting as a <span class="strong"><strong class="calibre2">controller</strong></span> at each time step:</p><div class="mediaobject"><img src="../images/00156.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The <a id="id372" class="calibre1"/>controller network outputs at each timestep:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The positioning or attention coefficients for each write/read head</li><li class="listitem">The value to store or erase for the write heads</li></ul></div><p class="calibre8">The <a id="id373" class="calibre1"/>original NTM proposes two approaches to define the <span class="strong"><em class="calibre12">head positioning</em></span>, also named <span class="strong"><em class="calibre12">addressing</em></span>, defined by the weights <span class="strong"><img src="../images/00157.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre23"/></span>:</p><div class="book"><ul class="itemizedlist"><li class="listitem">A content-based positioning, to place similar content in the same area of the memory, which is useful for retrieval, sorting or counting tasks:<div class="mediaobject"><img src="../images/00158.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre9"/></div><p class="calibre27"> </p></li><li class="listitem">
A location-based positioning, which is based on previous position of the head, and can be used in copy tasks. A gate <span class="strong"><img src="../images/00159.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre23"/></span> defines the influence of the previous weights versus newly generated weights to compute the position of the head. A shift weight <span class="strong"><img src="../images/00160.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre23"/></span> defines how much to translate from the position with respect to this position.
</li></ul></div><p class="calibre8">Last, a sharpening weight <span class="strong"><img src="../images/00161.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre23"/></span> reduces the blur on the head position:</p><div class="mediaobject"><img src="../images/00162.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre9"/></div><p class="calibre10"> </p><div class="mediaobject"><img src="../images/00163.jpeg" alt="Store and retrieve information in Neural Turing Machines" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">All <a id="id374" class="calibre1"/>operations are differentiable.</p><p class="calibre8">Many <a id="id375" class="calibre1"/>more than two heads are possible, in particular for tasks such as the addition of two stored values where a single read head would be limiting.</p><p class="calibre8">These NTM have demonstrated better capability than LSTM in tasks such as retrieving the next item in an input sequence, repeating the input sequence many times, or sampling from distribution.</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Memory networks" id="2OM4A1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec85" class="calibre1"/>Memory networks</h1></div></div></div><p class="calibre8">Answering questions or resolving problems given a few facts or a story have led to the design of <a id="id376" class="calibre1"/>a new type of networks, <span class="strong"><strong class="calibre2">memory networks</strong></span>. In this case, the facts or the story are embedded into a memory bank, as if they were inputs. To solve tasks that require the facts to be ordered or to create transitions between the facts, memory networks use a recurrent reasoning process in multiple steps or hops on the memory banks.</p><p class="calibre8">First, the query or question <span class="strong"><em class="calibre12">q</em></span> is converted into a constant input embedding:</p><div class="mediaobject"><img src="../images/00164.jpeg" alt="Memory networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">While, at each step of the reasoning, the facts <span class="strong"><em class="calibre12">X</em></span> to answer the question are embedded into two memory banks, where the embedding coefficients are a function of the timestep:</p><div class="mediaobject"><img src="../images/00165.jpeg" alt="Memory networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">To <a id="id377" class="calibre1"/>compute attention weights:</p><div class="mediaobject"><img src="../images/00166.jpeg" alt="Memory networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">And:</p><div class="mediaobject"><img src="../images/00167.jpeg" alt="Memory networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Selected with the attention:</p><div class="mediaobject"><img src="../images/00168.jpeg" alt="Memory networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The output at each reasoning time step is then combined with the identity connection, as seen previously to improve the efficiency of the recurrency:</p><div class="mediaobject"><img src="../images/00169.jpeg" alt="Memory networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">A linear layer and classification softmax layer are added to the last <span class="strong"><img src="../images/00170.jpeg" alt="Memory networks" class="calibre23"/></span>:</p><div class="mediaobject"><img src="../images/00171.jpeg" alt="Memory networks" class="calibre9"/></div><p class="calibre10"> </p></div>

<div class="book" title="Memory networks" id="2OM4A1-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="Episodic memory with dynamic memory networks"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec27" class="calibre1"/>Episodic memory with dynamic memory networks</h2></div></div></div><p class="calibre8">Another <a id="id378" class="calibre1"/>design has been introduced with dynamic memory networks. First, the N facts are concatenated with a separator token and then encoded with a RNN: the output of the RNN <a id="id379" class="calibre1"/>at each separation <span class="strong"><img src="../images/00172.jpeg" alt="Episodic memory with dynamic memory networks" class="calibre23"/></span>
<span class="strong"><em class="calibre12"> </em></span>is used as input embedding. This way to encode facts is more natural and also preserves time dependency. The question is also encoded with an RNN to produce a vector <span class="strong"><em class="calibre12">q</em></span>.</p><p class="calibre8">Secondly, the memory bank is replaced with an episodic memory, relying on an attention mechanism mixed with an RNN, in order to preserve time dependency between the facts as well:</p><div class="mediaobject"><img src="../images/00173.jpeg" alt="Episodic memory with dynamic memory networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The gates <span class="strong"><img src="../images/00174.jpeg" alt="Episodic memory with dynamic memory networks" class="calibre23"/></span> are given by a multilayer perceptron depending on the previous state of reasoning <span class="strong"><img src="../images/00175.jpeg" alt="Episodic memory with dynamic memory networks" class="calibre23"/></span>, the question and the input embedding <span class="strong"><img src="../images/00176.jpeg" alt="Episodic memory with dynamic memory networks" class="calibre23"/></span> as inputs.</p><p class="calibre8">The reasoning occurs the same way with a RNN:</p><div class="mediaobject"><img src="../images/00177.jpeg" alt="Episodic memory with dynamic memory networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The following picture illustrates the interactions between inputs and outputs to compute the episodic memories:</p><div class="mediaobject"><img src="../images/00178.jpeg" alt="Episodic memory with dynamic memory networks" class="calibre9"/><div class="caption"><p class="calibre29">Ask Me Anything: dynamic memory networks for natural language processing</p></div></div><p class="calibre10"> </p><p class="calibre8">To benchmark these networks, Facebook research has synthetized the bAbI dataset, using NLP <a id="id380" class="calibre1"/>tools to create facts, questions, and answers for some random modeled stories. The dataset is composed of different tasks to test different reasoning skills, such as reasoning on one, two, or three facts, in time, size, or position, counting, listing, or understanding relations between <a id="id381" class="calibre1"/>arguments, negations, motivations, and finding paths.</p><p class="calibre8">As for guided alignment in machine translation, when the dataset also contains the annotations for the facts leading to the answer, it is also possible to use supervised training for:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The attention mechanism</li><li class="listitem">When to stop the reasoning loop, producing a stop token, when the number of facts used is sufficient to answer the question</li></ul></div></div></div>
<div class="book" title="Further reading" id="2PKKS1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec86" class="calibre1"/>Further reading</h1></div></div></div><p class="calibre8">You can refer to these topics for more insights:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre12">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</em></span>,<span class="strong"><em class="calibre12"> </em></span>Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher, 2015</li><li class="listitem"><span class="strong"><em class="calibre12">Attention and Augmented Recurrent Neural Networks</em></span>, Chris Olah, Shan Carter, Sept 2016 <a class="calibre1" href="http://distill.pub/2016/augmented-rnns/">http://distill.pub/2016/augmented-rnns/</a></li><li class="listitem"><span class="strong"><em class="calibre12">Guided Alignment training for Topic Aware Neural Machine Translation</em></span>, Wenhu Chen, Evgeny Matusov, Shahram Khadivi, Jan-Thorsten Peter, Jul 2016</li><li class="listitem"><span class="strong"><em class="calibre12">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</em></span>, Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio, Fev 2015</li><li class="listitem"><span class="strong"><em class="calibre12">Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</em></span>, Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merriënboer, Armand Joulin, Tomas Mikolov,2015</li><li class="listitem"><span class="strong"><em class="calibre12">Memory Networks</em></span>, Jason Weston, Sumit Chopra, Antoine Bordes,2014</li><li class="listitem"><span class="strong"><em class="calibre12">End-To-End Memory Networks</em></span>, Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, 2015</li><li class="listitem"><span class="strong"><em class="calibre12">Neural Turing Machines</em></span>, Alex Graves, Greg Wayne, Ivo Danihelka, 2014</li><li class="listitem"><span class="strong"><em class="calibre12">Deep Visual-Semantic Alignments for Generating Image Descriptions</em></span>, Andrej Karpathy, Li Fei-Fei, 2014</li></ul></div></div>
<div class="book" title="Summary" id="2QJ5E1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec87" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">The attention mechanism is a smart option to help neural networks select the right information and focus to produce the correct output. It can be placed either directly on the inputs or the features (inputs processed by a few layers). Accuracies in the cases of translation, image annotation, and speech recognition, are increased, in particular when the dimension of the inputs is important.</p><p class="calibre8">Attention mechanism has led to new types of networks enhanced with external memory, working as an input/output, from which to read or to which to write. These networks have proved to be very powerful in question-answering challenges, into which most tasks in natural language processing can can be cast: tagging, classification, sequence-to-sequence, or question answering tasks.</p><p class="calibre8">In the next chapter, we'll see more advanced techniques and their application to the more general case of recurrent neural networks, to improve accuracy.</p></div></body></html>