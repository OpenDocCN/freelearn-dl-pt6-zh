- en: Chapter 2. Algorithms for Machine Learning – Preparing for Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you read through how deep learning has been developed
    by looking back through the history of AI. As you should have noticed, machine
    learning and deep learning are inseparable. Indeed, you learned that deep learning
    is the developed method of machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, as a pre-exercise to understand deep learning well, you will
    see the mode details of machine learning, and in particular, you will learn the
    actual code for the method of machine learning, which is closely related to deep
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The core concepts of machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of popular machine learning algorithms, especially focusing on neural
    networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Theories and implementations of machine learning algorithms related to deep
    learning: perceptrons, logistic regression, and multi-layer perceptrons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will insert the source code of machine learning and deep learning with Java
    from this chapter. The version of JDK used in the code is 1.8, hence Java versions
    greater than 8 are required. Also, IntelliJ IDEA 14.1 is used for the IDE. We
    will use the external library from [Chapter 5](ch05.html "Chapter 5. Exploring
    Java Deep Learning Libraries – DL4J, ND4J, and More"), *Exploring Java Deep Learning
    Libraries – DL4J, ND4J, and More*, so we are starting with a new Maven project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The root package name of the code used in this book is `DLWJ`, the initials
    of *Deep Learning with Java*, and we will add a new package or a class under `DLWJ`
    as required. Please refer to the screenshot below, which shows the screen immediately
    after the new project is made:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting started](img/B04779_02_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There will be some names of variables and methods in the code that don't follow
    the Java coding standard. This is to improve your understanding together with
    some characters in the formulas to increase readability. Please bear this in mind
    in advance.
  prefs: []
  type: TYPE_NORMAL
- en: The need for training in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have already seen that machine learning is a method of pattern recognition.
    Machine learning reaches an answer by recognizing and sorting out patterns from
    the given learning data. It may seem easy when you just look at the sentence,
    but the fact is that it takes quite a long time for machine learning to sort out
    unknown data, in other words, to build the appropriate model. Why is that? Is
    it that difficult to just sort out? Does it even bother to have a "learning" phase
    in between?
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is, of course, yes. It is extremely difficult to sort out data appropriately.
    The more complicated a problem becomes, the more it becomes impossible to perfectly
    classify data. This is because there are almost infinite patterns of categorization
    when you simply say "pattern classifier." Let''s look at a very simple example
    in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The need for training in machine learning](img/B04779_02_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are two types of data, circles and triangles, and the unknown data, the
    square. You don't know which group the square belongs to in the two-dimensional
    coordinate space, so the task is to find out which group the square belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might instantly know that there seems to be a boundary that separates two
    data types. And if you decide where to set this boundary, it looks like you should
    be able to find out to which group the square belongs. Well then, let''s decide
    the boundary. In reality, however, it is not so easy to clearly define this boundary.
    If you want to set a boundary, there are various lines to consider, as you can
    see in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The need for training in machine learning](img/B04779_02_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Additionally, depending on the placement of the boundary, you can see that the
    square might be allocated to a different group or pattern. Furthermore, it is
    also possible to consider that the boundary might be a nonlinear boundary.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, what a machine does in training is choose the most likely
    boundary from these possible patterns. It will automatically learn how to sort
    out patterns when processing massive amounts of training data one after another.
    In other words, it adjusts the parameters of a mathematical model and eventually
    decides the boundary. The boundary decided by machine learning is called the **decision
    boundary** and is not necessarily a linear or nonlinear boundary. A decision boundary
    can also be a hyperplane if it classifies the data best. The more complicated
    the distribution of the data is, the more likely it is that the decision boundary
    would be nonlinear boundary or a hyperplane. A typical case is the multi-dimensional
    classification problem. We have already faced such difficulty by just setting
    a boundary in this simple problem, so it's not hard to imagine that it would be
    very time-consuming to solve a more complicated problem.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised and unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we saw that there could be millions of boundaries even
    for a simple classification problem, but it is difficult to say which one of them
    is the most appropriate. This is because, even if we could properly sort out patterns
    in the known data, it doesn't mean that unknown data can also be classified in
    the same pattern. However, you can increase the percentage of correct pattern
    categorization. Each method of machine learning sets a standard to perform a better
    pattern classifier and decides the most possible boundary—the decision boundary—to
    increase the percentage. These standards are, of course, greatly varied in each
    method. In this section, we'll see what all the approaches we can take are.
  prefs: []
  type: TYPE_NORMAL
- en: First, machine learning can be broadly classified into **supervised learning**
    and **unsupervised learning**. The difference between these two categories is
    the dataset for machine learning is labeled data or unlabeled data. With supervised
    learning, a machine uses labeled data, the combination of input data and output
    data, and mentions which pattern each type of data is to be classified as. When
    a machine is given unknown data, it will derive what pattern can be applied and
    classify the data based on labeled data, that is, the past correct answers. As
    an example, in the field of image recognition, when you input some images into
    a machine, if you prepare and provide a certain number of images of a cat, labeled
    `cat`, and the same number of images of a human, labeled `human`, for a machine
    to learn, it can judge by itself which group out of cat or human (or none of them)
    that an image belongs to. Of course, just deciding whether the image is a cat
    or a human doesn't really provide a practical use, but if you apply the same approach
    to other fields, you can create a system that can automatically tag who is who
    in a photo uploaded on social media. As you can now see, in supervised training,
    the learning proceeds when a machine is provided with the correct data prepared
    by humans in advance.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, with unsupervised learning, a machine uses unlabeled data.
    In this case, only input data is given. Then, what the machine learns is patterns
    and rules that the dataset includes and contains. The purpose of unsupervised
    learning is to grasp the structure of the data. It can include a process called
    **clustering**, which classifies a data constellation in each group that has a
    common character, or the process of extracting the correlation rule. For example,
    imagine there is data relating to a user's age, sex, and purchase trend for an
    online shopping website. Then, you might find out that the tastes of men in their
    20s and women in their 40s are close, and you want to make use of this trend to
    improve your product marketing. We have a famous story here—it was discovered
    from unsupervised training that a large number of people buy beer and diapers
    at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: You now know there are big differences between supervised learning and unsupervised
    learning, but that's not all. There are also different learning methods and algorithms
    for each learning method, respectively. Let's look at some representative examples
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machine (SVM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You could say that SVM is the most popular supervised training method in machine
    learning. The method is still used for broad fields in the data mining industry.
    With SVM, data from each category located the closest to other categories is marked
    as the standard, and the decision boundary is determined using the standard so
    that the sum of the Euclidean distance from each marked data and the boundary
    is maximized. This marked data is called **support vectors**. Simply put, SVM
    sets the decision boundary in the middle point where the distance from every pattern
    is maximized. Therefore, what SVM does in its algorithm is known as **maximizing
    the margin**. The following is the figure of the concept of SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support Vector Machine (SVM)](img/B04779_02_80.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you only hear this statement, you might think "is that it?" but what makes
    SVM the most valuable is a math technique: the kernel trick, or the kernel method.
    This technique takes the data that seems impossible to be classified linearly
    in the original dimension and intentionally maps it to a higher dimensional space
    so that it can be classified linearly without any difficulties. Take a look at
    the following figure so you can understand how the kernel trick works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support Vector Machine (SVM)](img/B04779_02_56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We have two types of data, represented by circles and triangles, and it is obvious
    that it would be impossible to separate both data types linearly in a two-dimensional
    space. However, as you can see in the preceding figure, by applying the kernel
    function to the data (strictly speaking, the feature vectors of training data),
    whole data is transformed into a higher dimensional space, that is, a three-dimensional
    space, and it is possible to separate them with a two-dimensional plane.
  prefs: []
  type: TYPE_NORMAL
- en: While SVM is useful and elegant, it has one demerit. Since it maps the data
    into a higher dimension, the number of calculations often increases, so it tends
    to take more time in processing as the calculation gets more complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Markov Model (HMM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HMM is an unsupervised training method that assumes data follows the **Markov
    process**. The Markov process is a stochastic process in which a future condition
    is decided solely on the present value and is not related to the past condition.
    HMM is used to predict which state the observation comes from when only one observation
    is visible.
  prefs: []
  type: TYPE_NORMAL
- en: The previous explanation alone may not help you fully understand how HMM works,
    so let's look at an example. HMM is often used to analyze a base sequence. You
    may know that a base sequence consists of four nucleotides, for example, A, T,
    G, C, and the sequence is actually a string of these nucleotides. You won't get
    anything just by looking through the string, but you do have to analyze which
    part is related to which gene. Let's say that if any base sequence is lined up
    randomly, then each of the four characters should be output by one-quarter when
    you cut out any part of the base sequence.
  prefs: []
  type: TYPE_NORMAL
- en: However, if there is a regularity, for example, where C tends to come next to
    G or the combination of ATT shows up frequently, then the probability of each
    character being output would vary accordingly. This regularity is the probability
    model and if the probability of being output relies only on an immediately preceding
    base, you can find out genetic information (= state) from a base sequence (= observation)
    using HMM.
  prefs: []
  type: TYPE_NORMAL
- en: Other than these bioinformatic fields, HMM is often used in fields where time
    sequence patterns, such as syntax analysis of **natural language processing**
    (**NLP**) or sound signal processing, are needed. We don't explore HMM deeper
    here because its algorithm is less related to deep learning, but you can reference
    a very famous book, *Foundations of statistical natural language processing*,
    from MIT Press if you are interested.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Neural networks are a little different to the machine learning algorithms.
    While other methods of machine learning take an approach based on probability
    or statistics, neural networks are algorithms that imitate the structure of a
    human brain. A human brain is made of a neuron network. Take a look at the following
    figure to get an idea of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural networks](img/B04779_02_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: One neuron is linked to the network through another neuron and takes electrical
    stimulation from the synapse. When that electricity goes above the threshold,
    it gets ignited and transmits the electrical stimulation to the next neuron linked
    to the network. Neural networks distinguish things based on how electrical stimulations
    are transmitted.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks have originally been the type of supervised learning that represents
    this electrical stimulation with numbers. Recently, especially with deep learning,
    various types of neural networks algorithms have been introduced, and some of
    them are unsupervised learning. The algorithm increases the predictability by
    adjusting the weight of the networks through the process of learning. Deep learning
    is an algorithm based on neural networks. More details on neural networks will
    be explained later, with implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression is one of the statistical regression models of variables
    with the Bernoulli distribution. While SVM and neural networks are classification
    models, logistic regression is a regression model, yet it certainly is one of
    the supervised learning methods. Although logistic regression has a different
    base of thinking, as a matter of fact, it can be thought of as one of the neural
    networks when you look at its formula. Details on logistic regression will also
    be explained with implementations later.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, each machine learning method has unique features. It's important
    to choose the right algorithm based on what you would like to know or what you
    would would like to use the data for. You can say the same of deep learning. Deep
    learning has different methods, so not only should you consider which the best
    method among them is, but you should also consider that there are some cases where
    you should not use deep learning. It's important to choose the best method for
    each case.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just for your reference, there is another method of machine learning called
    **reinforcement learning**. While some categorize reinforcement learning as unsupervised
    learning, others declare that all three learning algorithms, supervised learning,
    unsupervised learning, and reinforcement learning, should be divided into different
    types of algorithms, respectively. The following image shows the basic framework
    of reinforcement learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement learning](img/B04779_02_49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An agent takes an action based on the state of an environment and an environment
    will change based on the action. A mechanism with some sort of reward is provided
    to an agent following the change of an environment and the agent learns a better
    choice of act (decision-making).
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning application flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have looked at the methods that machine learning has and how these methods
    recognize patterns. In this section, we''ll see which flow is taken, or has to
    be taken, by data mining using machine learning. A decision boundary is set based
    on the model parameters in each of the machine learning methods, but we can''t
    say that adjusting the model parameters is the only thing we have to care about.
    There is another troublesome problem, and it is actually the weakest point of
    machine learning: feature engineering. Deciding which features are to be created
    from raw data, that is, the analysis subject, is a necessary step in making an
    appropriate classifier. And doing this, which is the same as adjusting the model
    parameters, also requires a massive amount of trial and error. In some cases,
    feature engineering requires far more effort than deciding a parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, when we simply say "machine learning," there are certain tasks that need
    to be completed in advance as preprocessing to build an appropriate classifier
    to deal with actual problems. Generally speaking, these tasks can be summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Deciding which machine learning method is suitable for a problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding what features should be used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding which setting is used for model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only when these tasks are completed does machine learning become valuable as
    an application.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how do you decide the suitable features and parameters? How do you get
    a machine to learn? Let''s first take a look at the following diagram as it might
    be easier for you to grasp the whole picture of machine learning. This is the
    summary of a learning flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning application flow](img/B04779_02_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from the preceding image, the learning phase of machine learning
    can be roughly divided into these two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Literally, model parameters are renewed and adjusted in the training phase and
    the machine examines the merit of a model in the test phase. We have no doubt
    that the research or experiment will hardly ever succeed with just one training
    and one test set. We need to repeat the process of training → test, training →
    test … until we get the right model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the preceding flowchart in order. First, you need to divide
    the raw data into two: a training dataset and a test dataset. What you need to
    be very careful of here is that the training data and the test data are separated.
    Let''s take an example so you can easily imagine what this means: you are trying
    to predict the daily price of S&P 500 using machine learning with historical price
    data. (In fact, predicting the prices of financial instruments using machine learning
    is one of the most active research fields.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that you have historical stock price data from 2001 to 2015 as raw data,
    what would happen if you performed the training with all the data from 2001 to
    2015 and similarly performed the test for the same period? The situation would
    occur that even if you used simple machine learning or feature engineering, the
    probability of getting the right prediction would be 70%, or even higher at 80%
    or 90%. Then, you might think: *What a great discovery! The market is actually
    that simple! Now I can be a billionaire!*'
  prefs: []
  type: TYPE_NORMAL
- en: But this would end as short-lived elation. The reality doesn't go that well.
    If you actually start investment management with that model, you wouldn't get
    the performance you were expecting and would be confused. This is obvious if you
    think about it and pay a little attention. If a training dataset and a test dataset
    are the same, you do the test with the data for which you already know the answer.
    Therefore, it is a natural consequence to get high precision, as you have predicted
    a correct answer using a correct answer. But this doesn't make any sense for a
    test. If you would like to evaluate the model properly, be sure to use data with
    different time periods, for example, you should use the data from 2001 to 2010
    for the training dataset and 2011 to 2015 for the test. In this case, you perform
    the test using the data you don't know the answer for, so you can get a proper
    prediction precision rate. Now you can avoid going on your way to bankruptcy,
    believing in investments that will never go well.
  prefs: []
  type: TYPE_NORMAL
- en: So, it is obvious that you should separate a training dataset and a test dataset
    but you may not think this is a big problem. However, in the actual scenes of
    data mining, the case often occurs that we conduct an experiment with the same
    data without such awareness, so please be extra careful. We've talked about this
    in the case of machine learning, but it also applies to deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: If you divide a whole dataset into two datasets, the first dataset to be used
    is the training dataset. To get a better precision rate, we first need to think
    about creating features in the training dataset. This feature engineering partly
    depends on human experience or intuition. It might take a long time and a lot
    of effort before you can choose the features to get the best results. Also, each
    machine learning method has different types of data formats of features to be
    accepted because the theory of models and formulas are unique to each method.
    As an example, we have a model that can only take an integer, a model that can
    only take a non-negative number/value, and a model that can only take real numbers
    from 0 to 1\. Let's look back at the previous example of stock prices. Since the
    value of the price varies a lot within a broader range, it may be difficult to
    make a prediction with a model that can only take an integer.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we have to be careful to ensure that there is compatibility between
    the data and the model. We don't say we can't use a model that can take all the
    real numbers from 0 if you would like to use a stock price as is for features.
    For example, if you divide all the stock price data by the maximum value during
    a certain period, the data range can fit into 0-1, hence you can use a model that
    can only take real numbers from 0 to 1\. As such, there is a possibility that
    you can apply a model if you slightly change the data format. You need to keep
    this point in mind when you think about feature engineering. Once you create features
    and decide which method of machine learning to apply, then you just need to examine
    it.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, features are, of course, important variables when deciding
    on the precision of a model; however, a model itself, in other words a formula
    within the algorithm, also has parameters. Adjusting the speed of learning or
    adjusting how many errors to be allowed are good examples of this. The faster
    the learning speed, the less time it takes to finish the calculation, hence it's
    better to be fast. However, making the learning speed faster means that it only
    provides solutions in brief. So, we should be careful not to lose our expected
    precision rates. Adjusting the permissible range of errors is effective for the
    case where a noise is blended in the data. The standard by which a machine judges
    "is this data weird?" is decided by humans.
  prefs: []
  type: TYPE_NORMAL
- en: Each method, of course, has a set of peculiar parameters. As for neural networks,
    how many neurons there should be in one of the parameters is a good example. Also,
    when we think of the kernel trick in SVM, how we set the kernel function is also
    one of the parameters to be determined. As you can see, there are so many parameters
    that machine learning needs to define, and which parameter is best cannot be found
    out in advance. In terms of how we define model parameters in advance, there is
    a research field that focuses on the study of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need to test many combinations of parameters to examine which
    combination can return the best precision. Since it takes a lot of time to test
    each combination one by one, the standard flow is to test multiple models with
    different parameter combinations in concurrent processing and then compare them.
    It is usually the case that a range of parameters that should be set to some extent
    is decided, so it's not that the problem can't be solved within a realistic time
    frame.
  prefs: []
  type: TYPE_NORMAL
- en: When the model that can get good precision is ready in the training dataset,
    next comes the test step. The rough flow of the test is to apply the same feature
    engineering applied to the training dataset and the same model parameters respectively
    and then verify the precision. There isn't a particularly difficult step in the
    test. The calculation doesn't take time either. It's because finding a pattern
    from data, in other words optimizing a parameter in a formula, creates a calculation
    cost. However, once a parameter adjustment is done, then the calculation is made
    right away as it only applies the formula to new datasets. The reason for performing
    a test is, simply put, to examine whether a model is too optimized by the training
    dataset. What does this mean? Well, in machine learning, there are two patterns
    where a training set goes well but a test set doesn't.
  prefs: []
  type: TYPE_NORMAL
- en: The first case is incorrect optimization by classifying noisy data blended into
    a training dataset. This can be related to the adjustment of a permissible range
    of errors mentioned earlier in this chapter. Data in the world is not usually
    clean. It can be said that there is almost no data that can be properly classified
    into clean patterns. The prediction of stock prices is a good example again. Stock
    prices usually repeat moderate fluctuations from previous stock prices, but sometimes
    they suddenly surge or drop sharply. And, there is, or should be, no regularity
    in this irregular movement. Another case is if you would like to predict the yield
    of a crop for a country; the data of the year affected by abnormal weather should
    be largely different from the normal years' data. These examples are extreme and
    easy to understand, but most for a data in the real world also contains noises,
    making it difficult to classify data into proper patterns. If you just do training
    without adjusting the parameters of machine learning, the model forces it to classify
    the noise data into a pattern. In this case, data from the training dataset might
    be classified correctly, but since noise data in the training dataset is also
    classified and the noise doesn't exist in the test dataset, the predictability
    in a test should be low.
  prefs: []
  type: TYPE_NORMAL
- en: The second case is incorrect optimizing by classifying data that is characteristic
    only in a training dataset. For example, let's think about making an app of English
    voice inputs. To build your app, you should prepare the data of pronunciation
    for various words as a training dataset. Now, let's assume you prepared enough
    voice data of British English native speakers and were able to create a high precision
    model that could correctly classify the pronunciation in the training dataset.
    The next step is a test. Since it's a test, let's use the voice data of American
    English native speakers for the means of providing different data. What would
    be the result then? You probably wouldn't get good precision. Furthermore, if
    you try the app to recognize the pronunciation of non-native speakers of English,
    the precision would be much lower. As you know, English has different pronunciations
    for different areas. If you don't take this into consideration and optimize the
    model with the training data set of British English, even though you may get a
    good result in the training set, you won't get a good result in the test set and
    it won't be useful for the actual application.
  prefs: []
  type: TYPE_NORMAL
- en: These two problems occur because the machine learning model learns from a training
    dataset and fits into the dataset too much. This problem is literally called the
    **overfitting problem**, and you should be very careful to avoid it. The difficulty
    of machine learning is that you have to think about how to avoid this overfitting
    problem besides the feature engineering. These two problems, overfitting and feature
    engineering, are partially related because poor feature engineering would fail
    into overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid the problem of overfitting, there's not much to do except increase
    the amount of data or the number of tests. Generally, the amount of data is limited,
    so the methods of increasing the number of tests are often performed. The typical
    example is **K-fold cross-validation**. In K-fold cross-validation, all the data
    is divided into K sets at the beginning. Then, one of the datasets is picked as
    a test dataset and the rest, K-1, are put as training datasets. Cross-validation
    performs the verification on each dataset divided into K for K times, and the
    precision is measured by calculating the average of these K results. The most
    worrying thing is that both a training dataset and a test dataset may happen to
    have good precision by chance; however, the probability of this accident can be
    decreased in K-fold cross-validation as it performs a test several times. You
    can never worry too much about overfitting, so it's necessary that you verify
    results carefully.
  prefs: []
  type: TYPE_NORMAL
- en: Well, you have now read through the flow of training and test sets and learned
    key points to be kept in mind. These two mainly focus on data analysis. So, for
    example, if your purpose is to pull out the meaningful information from the data
    you have and make good use of it, then you can go through this flow. On the other
    hand, if you need an application that can cope with a further new model, you need
    an additional process to make predictions with a model parameter obtained in a
    training and a test set. As an example, if you would like to find out some information
    from a dataset of stock prices and analyze and write a market report, the next
    step would be to perform training and test sets. Or, if you would like to predict
    future stock prices based on the data and utilize it as an investment system,
    then your purpose would be to build an application using a model obtained in a
    training and a test set and to predict a price based on the data you can get anew
    every day, or from every period you set. In the second case, if you would like
    to renew the model with the data that is newly added, you need to be careful to
    complete the calculation of the model building by the time the next model arrives.
  prefs: []
  type: TYPE_NORMAL
- en: Theories and algorithms of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, you saw the general flow of when we perform data analysis
    with machine learning. In this section, theories and algorithms of neural networks,
    one of the methods of machine learning, are introduced as a preparation toward
    deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we simply say "neural networks", their history is long. The first
    published algorithm of neural networks was called **perceptron**, and the paper
    released in 1957 by Frank Rosenblatt was named *The Perceptron: A Perceiving and
    Recognizing Automaton (Project Para)*. From then on, many methods were researched,
    developed, and released, and now neural networks are one of the elements of deep
    learning. Although we simply say "neural networks," there are various types and
    we''ll look at the representative methods in order now.'
  prefs: []
  type: TYPE_NORMAL
- en: Perceptrons (single-layer neural networks)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The perceptron algorithm is the model that has the simplest structure in the
    algorithms of neural networks and it can perform linear classification for two
    classes. We can say that it's the prototype of neural networks. It is the algorithm
    that models human neurons in the simplest way.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure is a schematic drawing of the general model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Perceptrons (single-layer neural networks)](img/B04779_02_61.jpg) shows
    the input signal, ![Perceptrons (single-layer neural networks)](img/B04779_02_62.jpg)
    shows the weight corresponding to each input signal, and ![Perceptrons (single-layer
    neural networks)](img/B04779_02_63.jpg) shows the output signal. ![Perceptrons
    (single-layer neural networks)](img/B04779_02_64.jpg) is the activation function.
    ![Perceptrons (single-layer neural networks)](img/B04779_02_65.jpg) shows, literally,
    the meaning of calculating the sum of data coming from the input. Please bear
    in mind that ![Perceptrons (single-layer neural networks)](img/B04779_02_61.jpg)
    applies a processing of nonlinear conversion with feature engineering in advance,
    that is, ![Perceptrons (single-layer neural networks)](img/B04779_02_61.jpg)is
    an engineered feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the output of perceptron can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_01.jpg)![Perceptrons
    (single-layer neural networks)](img/B04779_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_66.jpg) is called
    the step function. As shown in the equation, Perceptron returns the output by
    multiplying each factor of the feature vector by weight, calculating the sum of
    them, and then activating the sum with the step function. The output is the result
    estimated by Perceptron. During the training, you will compare this result with
    the correct data and feed back the error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![Perceptrons (single-layer neural networks)](img/B04779_02_67.jpg) be
    the value of the labeled data. Then, the formula can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If some labeled data belongs to class 1, ![Perceptrons (single-layer neural
    networks)](img/B04779_02_68.jpg), we have ![Perceptrons (single-layer neural networks)](img/B04779_02_69.jpg).
    If it belongs to class 2, ![Perceptrons (single-layer neural networks)](img/B04779_02_70.jpg),
    we have ![Perceptrons (single-layer neural networks)](img/B04779_02_71.jpg). Also,
    if the input data is classified correctly, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, putting these equations together, we have the following equation of properly
    classified data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, you can increase the predictability of Perceptron by minimizing
    the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Perceptrons (single-layer neural networks)](img/B04779_02_72.jpg) is
    called the error function. ![Perceptrons (single-layer neural networks)](img/B04779_02_73.jpg)
    shows the set of misclassification. To minimize the error function, gradient descent,
    or steepest descent, an optimization algorithm is used to find a local minimum
    of a function using gradient descent. The equation can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Perceptrons (single-layer neural networks)](img/B04779_02_74.jpg) is
    the learning rate, a common parameter of the optimization algorithm that adjusts
    the learning speed, and ![Perceptrons (single-layer neural networks)](img/B04779_02_75.jpg)
    shows the number of steps of the algorithm. In general, the smaller the value
    of the learning rate, the more probable it is that the algorithm falls into a
    local minimum because the model can't override the old value much. If the value
    is too big, however, the model parameters can't converge because the values fluctuate
    too widely. Therefore, practically, the learning rate is set to be big at the
    beginning and then dwindle with each iteration. On the other hand, with perceptrons,
    it is proved that the algorithm converges irrespective of the value of the learning
    rate when the data set is linearly separable, and thus the value is set to be
    1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at an implementation. The package structure is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's have a look at the content of `Perceptrons.java` as shown in the previous
    image. We will look into the main methods one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the parameters and constants that are needed for learning.
    As explained earlier, the learning rate (defined as `learningRate` in the code)
    can be 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Needless to say, machine learning and deep learning need a dataset to be learned
    and classified. Here, since we would like to focus on implementations deeply related
    to the theory of perceptrons, a sample dataset is generated within the source
    code and is used for the training and test sets, the class called `GaussianDistribution`
    is defined, and it returns a value following the normal distribution or Gaussian
    distribution. As for the source code itself, we don''t mention it here as you
    can see it in `GaussianDistribution.java`. We set the dimensions of the learning
    data in `nIn = 2` and define two types of instances as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can get the values that follow the normal distributions with a mean of `-2.0`
    and a variance of `1.0` by `g1.random()` and a mean of `2.0` and a variance of
    `1.0` by `g2.random()`.
  prefs: []
  type: TYPE_NORMAL
- en: With these values, 500 data attributes are generated in class 1 obtained by
    `[ g1.random(), g2.random() ]` and another 500 generated in class 2 obtained by
    `[ g2.random(), g1.random() ]`. Also, please bear in mind that each value of the
    class 1 label is `1` and of the class 2 label is `-1`. Almost all the data turns
    out to be a value around `[-2.0, 2.0]` for class 1 and `[2.0, -2.0]` for class
    2; hence, they can be linearly separated, but some data can be blended near the
    other class as noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have prepared the data, we can move on to building the model. The number
    of units in the input layer, `nIn`, is the argument used here to decide the model
    outline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the actual `Perceptrons` constructor. The parameter of the perceptrons
    model is only the weight, `w`, of the network—very simple —as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is finally the training. The iteration of learning continues
    until it reaches enough numbers of the learning set in advance or classifies all
    the training data correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `train` method, you can write down the gradient descent algorithm as
    we just explained. Here, the `w` parameter of the network is updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have done enough numbers of learning and finish, the next step is
    to perform the test. First, let''s check which class the test data is classified
    by in the well-trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `predict` method, simply activate the input through the network. The
    step function used here is defined in `ActivationFunction.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Subsequently, we evaluate the model using the test data. You might need more
    explanation to perform this part.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, the performance of the method of machine learning is measured by
    the indicator of accuracy, precision, and recall based on the confusion matrix.
    The confusion matrix summarizes the results of a comparison of the predicted class
    and the correct class in the matrix and is shown as the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | p_predicted | n_predicted |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **p_actual** | True positive (TP) | False negative (FN) |'
  prefs: []
  type: TYPE_TB
- en: '| **n_actual** | False positive (FP) | True negative (TN) |'
  prefs: []
  type: TYPE_TB
- en: 'The three indicators are shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptrons (single-layer neural networks)](img/B04779_02_08.jpg)![Perceptrons
    (single-layer neural networks)](img/B04779_02_09.jpg)![Perceptrons (single-layer
    neural networks)](img/B04779_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Accuracy shows the proportion of the data that is correctly classified for
    all the data, while precision shows the proportion of the actual correct data
    to the data predicted as positive, and recall is the proportion of the data predicted
    as positive to the actual positive data. Here is the code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: When you compile `Perceptron.java` and run it, you can get 99.0% for accuracy,
    98.0% for precision, and 100% for recall. This means that actual positive data
    is classified correctly but that there has been some data wrongly predicted as
    positive when it is actually negative. In this source code, since the data set
    is for demonstration, K-fold cross-validation is not included. The dataset in
    the example above is programmatically generated and has little noise data. Therefore,
    accuracy, precision, and recall are all high because the data can be well classified.
    However, as mentioned above, you have to look carefully at results, especially
    when you have great results.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression is, as you can assume from the name, the regression model.
    But when you look at the formula, you can see that logistic regression is the
    linear separation model that generalizes perceptrons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression can be regarded as one of the neural networks. With perceptrons,
    the step function is used for the activation function, but in logistic regression,
    the (logistic) sigmoid function is used. The equation of the sigmoid function
    can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B04779_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The graph of this function can be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B04779_02_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `sigmoid` function maps any values of a real number to a value from 0 to
    1\. Therefore, the output of the logistic regression can be regarded as the posterior
    probability for each class. The equations can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B04779_02_12.jpg)![Logistic regression](img/B04779_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These equations can be combined to make:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B04779_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Logistic regression](img/B04779_02_15.jpg) is the correct data. You
    may have noticed that the range of the data is different from the one of perceptrons.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the previous equation, the `likelihood` function, which estimates the
    maximum likelihood of the model parameters, can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B04779_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B04779_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, not only the weight of the network but the bias ![Logistic regression](img/B04779_02_76.jpg)
    are also parameters that need to be optimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we need to do now is maximize the likelihood function, but the calculation
    is worrying because the function has a mathematical product. To make the calculation
    easier, we take the logarithm (log) of the likelihood function. Additionally,
    we substitute the sign to turn the object to minimizing the negative log `likelihood`
    function. Since the log is the monotonic increase, the magnitude correlation doesn''t
    change. The equation can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B04779_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can see the error function at the same time. This type of function is called
    a cross-entropy error function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to perceptrons, we can optimize the model by computing the gradients
    of the model parameters, ![Logistic regression](img/B04779_02_57.jpg) and ![Logistic
    regression](img/B04779_02_76.jpg). The gradients can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B04779_02_19.jpg)![Logistic regression](img/B04779_02_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With these equations, we can update the model parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B04779_02_21.jpg)![Logistic regression](img/B04779_02_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Theoretically, we have no problem using the equations just mentioned and implementing
    them. As you can see, however, you have to calculate the sum of all the data to
    compute the gradients for each iteration. This will hugely increase the calculation
    cost once the size of a dataset becomes big.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, another method is usually applied that partially picks up some data
    from the dataset, computes the gradients by calculating the sum only with picked
    data, and renews the parameters. This is called **stochastic gradient descent**
    (**SGD**) because it stochastically chooses a subset of the data. This subset
    of the dataset used for one renewal is called a **mini-batch**.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SGD using a mini-batch is sometimes called **mini-batch stochastic gradient
    descent** (**MSGD**). Online training that learns to randomly choose one data
    from the dataset is called SGD to distinguish one from the other. In this book,
    however, both MSGD and SGD are called SGD, as both become the same when the size
    of the mini-batch is 1\. Since learning by each data does increase the calculation
    cost, it's better to use mini-batches.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the implementation of logistic regression, since it can be covered
    with multi-class logistic regression introduced in the next section, we won't
    write the code here. You can refer to the code of multi-class logistic regression
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression can also be applied to multi-class classification. In two-class
    classification, the activation function is the sigmoid function, and you can classify
    the data by evaluating the output value shifting from 0 to 1\. How, then, can
    we classify data when the number of classes is K? Fortunately, it is not difficult.
    We can classify multi-class data by changing the equation for the output to the
    K-dimensional class-membership probability vector, and we use the `softmax` function
    to do so, which is the multivariate version of the sigmoid function. The posterior
    probability of each class can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-class logistic regression](img/B04779_02_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With this, the same as two-class cases, you can get the likelihood function
    and the negative log likelihood function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-class logistic regression](img/B04779_02_24.jpg)![Multi-class logistic
    regression](img/B04779_02_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Multi-class logistic regression](img/B04779_02_26.jpg) , ![Multi-class
    logistic regression](img/B04779_02_27.jpg). Also, ![Multi-class logistic regression](img/B04779_02_28.jpg)
    is the Kth element of the correct data vector, ![Multi-class logistic regression](img/B04779_02_29.jpg),
    which corresponds to the *n* *th* training data. If an input data belongs to the
    class *k*, the value of ![Multi-class logistic regression](img/B04779_02_28.jpg)
    is 1; the value is 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradients of the loss function against the model parameters, the weight vector,
    and the bias, can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-class logistic regression](img/B04779_02_30.jpg)![Multi-class logistic
    regression](img/B04779_02_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s look through the source code to better understand the theory. You
    can see some variables related to mini-batches besides the ones necessary for
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is the process to shuffle training data so the data of each
    mini-batch is to be applied randomly to SGD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we can see the multi-class classification problem, we generate a sample
    dataset with three classes. In addition to mean values and variances used in perceptrons,
    we also use the dataset according to normal distribution with the mean of `0.0`
    and the variance of 1.0 for the training data and the test data for class 3\.
    In other words, each class''s data follows normal distributions with the mean
    of `[-2.0, 2.0]`, `[2.0, -1.0]` and `[0.0, 0.0]` and the variance of `1.0`. We
    defined the training data as the `int` type and the test data as the Integer type
    for the labeled data. This is to process the test data easier when evaluating
    the model. Also, each piece of labeled data is defined as an array because it
    follows multi-class classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we classify the training data into a mini-batch using `minibatchIndex`,
    which was defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have prepared the data, let''s practically build a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The model parameters of logistic regression are `W`, weight of the network,
    and bias `b`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The training is done with each mini-batch. If you set `minibatchSize = 1`,
    you can make the training so-called online training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the learning rate gradually decreases so that the model can converge.
    Now, for the actual training `train` method, you can briefly divide it into two
    parts, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the gradient of `W` and `b` using the data from the mini-batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update `W` and `b` with the gradients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: At the end of the `train` method, `return dY`, the error value of the predicted
    data and the correct data is returned. This is not mandatory for logistic regression
    itself but it is necessary in the machine learning and the deep learning algorithms
    introduced later.
  prefs: []
  type: TYPE_NORMAL
- en: Next up for the training is the test. The process of performing the test doesn't
    really change from the one for perceptrons.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, with the `predict` method, let''s predict the input data using the trained
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `predict` method and the `output` method called are written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: First, input data is activated with the `output` method. As you can see from
    the bottom of the output, the activation function uses the `softmax` function.
    `softmax` is defined in `ActivationFunction.java`, and with this function the
    array showing the probability of each class is returned, hence you just need to
    get the index within the array of the element that has the highest probability.
    The index represents the predicted class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s evaluate the model. Again, the confusion matrix is introduced
    for model evaluation, but be careful as you need to find the precision or recall
    for each class this time because we have multi-class classification here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Multi-layer perceptrons (multi-layer neural networks)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Single-layer neural networks have a huge problem. Perceptrons or logistic regressions
    are efficient for problems that can be linearly classified but they can''t solve
    nonlinear problems at all. For example, they can''t even solve the simplest XOR
    problem seen in the figure here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since most of the problems in the real world are nonlinear, perceptrons and
    logistic regression aren''t applicable for practical uses. Hence, the algorithm
    was improved to correspond to nonlinear problems. These are multi-layer perceptrons
    (or **multi-layer neural networks**, **MLPs**). As you can see from the name,
    by adding another layer, called a hidden layer, between the input layer and the
    output layer, the networks have the ability to express various patterns. This
    is the graphical model of an MLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What is most important here is not to introduce the skip-layer connection. In
    neural networks, it is better for both theory and implementation to keep the model
    as having a feed-forward network structure. By sticking to these rules, and by
    increasing the number of hidden layers, you can approximate arbitrary functions
    without making the model too complicated mathematically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how we compute the output. It looks complicated at first glance
    but it accumulates the layers and the scheme of the network''s weight or activation
    in the same way, so you simply have to combine the equation of each layer. Each
    output can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_77.jpg)
    is the activation function of the hidden layer and ![Multi-layer perceptrons (multi-layer
    neural networks)](img/B04779_02_78.jpg) is the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As has already been introduced, in the case of multi-class classification,
    the activation function of the output layer can be calculated efficiently by using
    the `softmax` function, and the error function is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As for a single layer, it's fine just to reflect this error in the input layer,
    but for the multi-layer, neural networks cannot learn as a whole unless you reflect
    the error in both the hidden layer and input layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, in feed-forward networks, there is an algorithm known as `backpropagation`,
    which enables the model to propagate this error efficiently by tracing the network
    forward and backward. Let''s look at the mechanism of this algorithm. To make
    the equation more readable, we''ll think about the valuation of an error function
    in the online training, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can now think about just the gradient of this, ![Multi-layer perceptrons
    (multi-layer neural networks)](img/B04779_02_79.jpg). Since all the data in a
    dataset in most cases of practical application is independent and identically
    distributed, we have no problem defining it as we just mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each unit in the feed-forward network is shown as the sum of the weight of
    the network connected to the unit, hence the generalized term can be represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_35.jpg)![Multi-layer
    perceptrons (multi-layer neural networks)](img/B04779_02_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Be careful, as ![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_61.jpg)
    here is not only the value of the input layer (of course, this can be the value
    of the input layer). Also, ![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_77.jpg)
    is the nonlinear activation function. The gradient of weights and the gradient
    of the bias can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_37.jpg)![Multi-layer
    perceptrons (multi-layer neural networks)](img/B04779_02_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let the notation defined in the next equation be introduced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_40.jpg)![Multi-layer
    perceptrons (multi-layer neural networks)](img/B04779_02_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, when we compare the equations, the output unit can be described
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, each unit of the hidden layer is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_43.jpg)![Multi-layer
    perceptrons (multi-layer neural networks)](img/B04779_02_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the **backpropagation formula** is introduced. As such, delta is called
    the **backpropagated** error. By computing the `backpropagated` error, the weights
    and bias can be calculated. It may seem difficult when you look at the formula,
    but what it basically does is receive feedback on errors from a connected unit
    and renew the weight, so it's not that difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at an implementation with a simple XOR problem as an example.
    You will have better understanding when you read the source code. The structure
    of the package is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-layer perceptrons (multi-layer neural networks)](img/B04779_02_55.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The basic flow of the algorithm is written in `MultiLayerPerceptrons.java`,
    but the actual part of backpropagation is written in `HiddenLayer.java`. We use
    multi-class logistic regression for the output layer. Since there is no change
    in `LogisticRegression.java`, the code is not shown in this section. In `ActivationFunction.java`,
    derivatives of the sigmoid function and hyperbolic tangent are added. The hyperbolic
    tangent is also the activation function that is often used as an alternative to
    the sigmoid. Also, in `RandomGenerator.java`, the method to generate random numbers
    with a uniform distribution is written. This is to randomly initialize the weight
    of the hidden layer, and it is quite an important part because a model often falls
    into a local optimum and fails to classify the data depending on these initial
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the content of `MultiLayerPerceptrons.java`. In `MultiLayerPereptrons.java`,
    differently defined classes are defined respectively for each layer: `HiddenLayer`
    class is used for the hidden layer and `LogisticRegression` class for the output
    layer. Instances of these classes are defined as `hiddenLayer` and `logisticLayer`,
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters of the MLP are the weights `W` and bias `b` of the hidden layer,
    `HiddenLayer`, and the output layer, `LogisticRegression`. Since the output layer
    is the same as the one previously introduced, we won''t look at the code here.
    The constructor of `HiddenLayer` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`W` is initialized, randomly matching the number of the units. This initialization
    is actually tricky as it makes you face the local minima problem more often if
    the initial values are not well distributed. Therefore, in a practical scene,
    it often happens that the model is tested with some random seeds. The activation
    function can be applied to either the sigmoid function or the hyperbolic tangent
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The training of the MLP can be given by forward propagation and backward propagation
    through the neural networks in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The part of `hiddenLayer.backward` gives the hidden layer backpropagation of
    the prediction error, `dY`, from a logistic regression. Be careful, as the input
    data of a logistic regression is also necessary for the backpropagation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You might think the algorithm is complex and difficult because the arguments
    seem complicated, but what we do here is almost the same as what we do with the
    `train` method of logistic regression: we calculate the gradients of `W` and `b`
    with the unit of the mini-batch and update the model parameters. That''s it. So,
    can an MLP learn the XOR problem? Check the result by running `MultiLayerPerceptrons.java`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result only outputs the percentages of the accuracy, precision, and recall
    of the model, but for example, if you dump the prediction data with the `predict`
    method of `LogisticRegression`, you can see how much it actually predicts the
    probability, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We've just shown that MLPs can approximate the function of XOR. Moreover, it
    is proven that MLPs can approximate any functions. We don't follow the math details
    here, but you can easily imagine that the more units MLPs have, the more complicated
    functions they could express and approximate.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, as preparation for deep learning, we dug into neural networks,
    which are one of the algorithms of machine learning. You learned about three representative
    algorithms of single-layer neural networks: perceptrons, logistic regression,
    and multi-class logistic regression. We see that single-layer neural networks
    can''t solve nonlinear problems, but this problem can be solved with multi-layer
    neural networks—the networks with a hidden layer(s) between the input layer and
    output layer. An intuitive understanding of why MLPs can solve nonlinear problems
    says that the networks can learn more complicated logical operations by adding
    layers and increasing the number of units, and thus having the ability to express
    more complicated functions. The key to letting the model have this ability is
    the backpropagation algorithm. By backpropagating the error of the output to the
    whole network, the model is updated and adjusted to fit in the training data with
    each iteration, and finally optimized to approximate the function for the data.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you'll learn the concepts and algorithms of deep learning.
    Since you've now acquired a foundational understanding of machine learning algorithms,
    you'll have no difficulty learning about deep learning.
  prefs: []
  type: TYPE_NORMAL
