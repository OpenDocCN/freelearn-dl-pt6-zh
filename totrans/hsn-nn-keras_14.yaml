- en: Contemplating Present and Future Developments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout the course of this book, we had the good fortune to explore together
    an intriguing idea that populates, and currently dominates, the realm of **Artificial
    Intelligence** (**AI**): **Artificial Neural Networks** (**ANNs**). On our journey,
    we had the opportunity to get detailed insight into the functioning of neural
    models, including the feed-forward, convolutional, and recurrent networks, and
    thereby **Long Short-Term Memory** (**LSTM**). We continued our journey by subsequently
    exploring self-supervised methods, including **Reinforcement Learning **(**RL**)
    with deep Q-networks, as well as autoencoders. We finalized our excursion by going
    over the intuition behind generative models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Sharing representations with transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning on Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concluding our experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limits of current neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encouraging sparse representation learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic optimization and evolutionary algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-network predictions and ensemble models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The future of AI and neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The road ahead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problems with classical computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advent of quantum computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantum neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technology and society
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contemplating the future
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing representations with transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One powerful paradigm that we have not yet had the pleasure of discussing is
    the notion of **transfer learning**. In our excursions, we saw various methods
    and techniques that allow neural networks to induct powerful and accurate representations
    from the data they see.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet, what if we wanted to transfer these learned representations to other networks?
    This can be quite useful if we are tackling a task where not a lot of training
    data is available beforehand. Essentially, transfer learning seeks to leverage
    commonalities among different learning tasks that may share similar statistical
    features. Consider the following case: you are a radiologist who wants to use
    a **Convolutional Neural Network** (**CNN**) to classify different pulmonary diseases,
    using images of chest X-rays. The only problem is you only have about a hundred
    labeled images of chest X-rays. Since you can''t go about ordering X-rays for
    any unsuspecting patient to augment your dataset, you are required to get creative.
    Maybe you have different images of the same phenomenon (such as MRIs and CT scans),
    or perhaps you have a lot of X-ray images from different body parts. So, why not
    use these?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we know that earlier layers in CNNs learn the same low-level features
    (such as edges, line segments, and curvatures), why not simply reuse these learned
    features from a different task and fine tune that model to our new learning task
    ? In many cases, transfer learning can save a lot of time, when compared to training
    a network from scratch, and is a very useful tool to have in your deep learning repertoire.
    In that spirit, let''s explore one very last hands-on example: implementing a
    simple transfer learning workflow on Keras.'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning on Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore a very simplistic transfer learning methodology
    in Keras. The idea behind this is simple: why waste precious computation resources
    on learning the repetitive low-level features common to almost all images?'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the famous `CIFAR10` dataset to illustrate our implementation, by
    making it our task to classify images pertaining to any of the 10 image categories
    present in the dataset. However, we will augment our learning experience by using
    layers from pretrained networks and adding them to a network of our own. To do
    this, we will import a very deep CNN, that has already been trained on expensive
    **Graphics Processing Units** (**GPUs**) for hundreds of hours and simply fine-tune
    it to our use case. The model in question that we will use is the same **VGG net**
    we used back in [Chapter 4](https://cdp.packtpub.com/hands_on_neural_networks_with_keras/wp-admin/post.php?post=655&action=edit),
    *Convolutional Neural Networks,* to visualize how a neural network sees a cheetah.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, however, we will be slicing it open and picking out some of its
    intermediate layers to splice into our own model, thereby transferring what it
    has learnt to a new task. We will begin by making some imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Loading a pretrained model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We define our image dimensions and load in the VGG16 model with the training
    weights it achieved on the **ImageNet** classification task (ILSVRC), excluding
    its input layer. We do this since the images we will train the network on differ
    in dimension from the original ones it was trained on. In the following code block,
    we can visually summarize the model object that we loaded in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c97cb657-860e-4ae4-91fb-61494c2a7237.png)'
  prefs: []
  type: TYPE_IMG
- en: This is quite a big model. In fact, it has about 20 million trainable parameters!
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining intermediate layers from a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Thanks to Keras''s Lego-like modular interface, we can do some really cool
    things, such as break apart the aforementioned model and reuse its layers as part
    of another network. This will be our next step, and it can be easily achieved
    using the functional API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The result obtained will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50903a31-7d23-45ac-b8e4-443bbea2bd26.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that all we had to do is initiate a model object using the functional
    API and pass it the first 12 layers of the VGG net. This is achieved by using
    the `.get_layer()` method on the VGG model object and passing it a layer name.
    Recall that the name of an individual layer can be verified by using the `.summary()`
    method on a given model object.
  prefs: []
  type: TYPE_NORMAL
- en: Adding layers to a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we have retrieved the pretrained intermediate layers from the VGG net.
    Next, we can connect more sequential layers to these pretrained layers. The idea
    behind this is to use the representations learned by the pretrained layers and
    build upon them, thereby augmenting the classification task with knowledge from
    a different learning task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To add more layers to our model, we will again use the functional API syntax
    to create a simple feed-forward network, which takes the output values from the
    selected VGG net layers and flattens them into 2D arrays, before passing them
    forward to densely connected layers with 1,024 neurons. This layer then connects
    to a heavy-dropout layer, where half of the neural connections from the previous
    layer is ignored while training.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have another dense layer of 1,024 neurons before reaching the final
    output layer. The output layer is equipped with 10 neurons, pertaining to the
    number of classes in our training data, as well as a softmax activation function,
    which will generate a 10-way probability score for each observation seen by the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have defined the layers we wish to add to the network, we can once
    again use the functional API syntax to merge the two separate models together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2966191-d843-44a6-89a5-fb91b8b0bf4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Most important, we must freeze the layer weights of the VGG model, so as to
    benefit from the representations it has encoded during its previous training session
    on those nice expensive GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we only chose to freeze the first four layers and decided to let the
    rest of the architecture retrain on this new learning task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Other approaches may choose to keep the entire model architecture frozen and
    only reinitialize the weights of the last layer of the model. We encourage you
    to try freezing different numbers of layers, and exploring how this changes the
    network's learning experience, by visualizing the loss convergence, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, different learning tasks may require different approaches. It naturally
    depends on a multitude of factors, such as the similarity between tasks, similarity
    between the training data, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: A common rule of thumb is to only reinitialize the weights of the last layer
    if very little data is available on the target learning task. Conversely, if a
    lot of data is available on the target task, then it is even conceivable to reinitialize
    weights for the entire network during training. In this case, you would have simply
    used a pretrained model and reimplemented it for a different use case. As always
    with deep learning, the answer lies in experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and preprocessing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we preprocess the CIFAR10 images and vectorize the labels, as we have
    been doing throughout the course of this book. There is nothing special here to
    note:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We first load the images in the first code block. In the second block, we normalize
    the pixel values to float values between `0` and `1`. Finally, in the last block,
    we one-hot encode our labels. Now, our network is ready to be compiled and trained.
  prefs: []
  type: TYPE_NORMAL
- en: Training the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we compile our model, with the categorical cross-entropy `loss` function
    and the `Adam` optimizer. Then, we can initiate the training session, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following will be the output obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba9db3f4-a560-4a89-a32d-816fa0b6ecc6.png)'
  prefs: []
  type: TYPE_IMG
- en: The model was trained in batches of 128 images for 10 epochs. The validation
    accuracy achieved was about 85%, which was considerably better than the same model
    trained from scratch. You can try this out yourself, by unfreezing the layers
    we froze, before training the model. There we have it. Now you have implemented
    a transfer learning workflow in Keras and are able to reuse neural networks for
    use cases requiring pretraining or fine tuning
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Experiment with different model depths by retrieving more blocks from the pretrained
    VGG net. Does the accuracy improve substantially with deeper models? Vary where
    you pick layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the number of trainable layers; how does this affect the convergence
    of `loss`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try out a different model out of the 10 pretrained ones available in `keras.applications`
    to build a classifier using the notion of transfer learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Listen to Andrew Ng talk about transfer learning: [https://www.youtube.com/watch?v=yofjFQddwHE](https://www.youtube.com/watch?v=yofjFQddwHE).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Concluding our experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Such accounts bring an end to our explorations and experimentations with various
    neural network architectures. Yet, there is still a lot more to discuss and discover.
    After all, while our journey together comes close to fruition, yours has just
    begun! There are countless more use cases, architectural variations, and implementational
    details that we could go on to explore, yet doing so will deviate from our initial
    ambitions for this work. We wanted to achieve a detailed understanding of what
    neural networks actually do, how they operate, and under what circumstances they
    may be used, respectively. Furthermore, we want to develop an internal intuition
    of what is actually happening inside these networks, and why these architectures
    work as well as they do. The remainder of this chapter will be dedicated to solidifying
    this notion, allowing you to better relate to the underlying idea of representation
    learning and applying this notion to any future use cases you may want to address
    using neural networks. Finally, we will also take this opportunity to address
    some of the latest developments in the field of ANNs, and how different business
    and institutions alike have crafted utility for this technology. Finally, we will
    also attempt to take a step into the future and speculate on how coming developments
    may affect the scientific, economic, and social landscape, in the advent of phenomena
    such as big data, and potential technological leaps such as quantum computing.
  prefs: []
  type: TYPE_NORMAL
- en: Learning representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we addressed the topic of representations and how this affects the task
    of learning in [Chapter 1](https://cdp.packtpub.com/hands_on_neural_networks_with_keras/wp-admin/post.php?post=24&action=edit),
    *Overview of Neural Networks*, we can now afford for our discussion to deepen
    further, given the hands-on practical examples we have executed since.
  prefs: []
  type: TYPE_NORMAL
- en: By now, we are all well aware that the success of any **Machine Learning** (**ML**)
    algorithm (including deep learning algorithms such as neural networks) is directly
    dependent on the manner in which we chose to represent the data it is shown. What's
    the deal here?
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the importance of representations and their impact on information
    processing, recall that we saw a succinct example earlier on in this book. We
    performed mathematical operations such as long division using Roman numerals,
    revealing the difficulty of carrying out such a task using suboptimal representations.
    Indeed, the way we choose to represent information directly impacts the way we
    process it, the sort of operations we are able to perform on it, and the kind
    of understanding we may derive.
  prefs: []
  type: TYPE_NORMAL
- en: DNA and technology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider another example: the DNA molecule. **Deoxyribonucleic Acid** (**DNA**)
    is a molecular structure made up of two intertwined chainlike threads, known as
    the **double helix formation**. The molecule can be broken down into **simpler
    monomeric units** (or **nucleoids**), forming base pairs composed of two of the
    four nitrogen-based building blocks (these being **Adenine** (**A**), **Guanine**
    (**G**), **Thymine** (**T**), and **Cytosine** (**C**)).'
  prefs: []
  type: TYPE_NORMAL
- en: Many of you may be wondering at this point, "*what does this have to do with
    the subject at hand?"* Well, as it turns out, this molecular structure holds the
    blueprints of all lifeforms on this planet. The molecule governs how cells divide,
    become more complex structures, all of the way up to the preferences and behavior
    of the flora and fauna here on our home planet.
  prefs: []
  type: TYPE_NORMAL
- en: Needless to say, this quadrinary system for representing information has found
    a way to encode and copy instructions to produce all life we see around us! No
    representation format devised by humans so far has ever come close to simulating
    the grand sphere of life as we know it. In fact, we still struggle to simulate
    realistically immersive game environments for entertainment purposes. Curiously,
    by many estimates, the DNA molecule itself can be represented using our own binary
    system, with about 1.5 gigabytes of data. Think about it, 1.5 gigabytes of data,
    or one single Blueray disk, is capable of storing all the of instructions for
    life itself. But that's about all we can do. We can't exactly instruct the Blueray
    disk to incessantly replicate itself into the complexity we embody and see around
    us every day. Hardware considerations aside, a paramount reason why we cannot
    replicate the operations of life in this manner is due to the representation of
    the data itself! Hence, the way we represent data has severe implications on the
    kind of transformations we may perform, resulting in ever more complex information-processing
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Limits of current neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similarly, in ML, it is hypothesized that different representations of data
    allow the capturing of different explanatory factors of variation present therein.
    The neural networks we saw were excellent at inducing efficient representations
    from their input values and leveraging these representations for all sorts of
    learning tasks. Yet, these input values themselves had to undergo a deluge of
    preprocessing considerations, transforming raw data into a format more palatable
    to the networks.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, the deficiency of neural networks relates to their heavy dependence
    on such preprocessing and feature-engineering considerations to learn useful representations
    from the given data. On their own, they are unable to extract and categorize discriminative
    elements from raw input values. Often, behind every neural network, there is a
    human.
  prefs: []
  type: TYPE_NORMAL
- en: We are still required to use our ingenuity, domain knowledge, and curiosity
    in order to overcome this deficiency. Eventually, however, we will strive to devise
    systems that require minimal human intervention (in the form of feature engineering,
    for example) and to truly understand the raw data present in the world. Designing
    such a system is one of the paramount goals in the field of AI, and one that we
    hope you can help advance.
  prefs: []
  type: TYPE_NORMAL
- en: For the time being, however, we will cover some useful concepts that allow us
    to design better representations from raw data, thereby designing better learning
    experiences for our artificial counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: Engineering representations for machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The topic of representation learning addresses exactly this. Intuitively, we
    ask ourselves this: *How can we make it easier for machines to extract useful
    information from data?* This notion is intrinsically linked to the idea that there
    exist certain generalizable assumptions about the world that may be applied to
    better interpret and synthesize the raw data available. These generalized assumptions,
    in conjunction with experimentative techniques, allow us to design good representations
    and discard bad ones. They serve as principles of experimentation when designing
    a preprocessing workflow for learning algorithms such as neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: How should a good representation be?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Intuitively, a good representation should be capable of disentangling the main
    factors of variation that cause an occurrence. Hence, one approach may be to augment
    the analytics workflow in a manner so as to make it easier for machines to spot
    these factors of variance.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers, over the decades, have amassed a set of heuristic assumptions applicable
    in the field of deep learning that allow us to do exactly this. Next, we will
    reproduce a subset of such heuristics, or regularization strategies, that are
    known to augment the learning experience of deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a comprehensive technical review of all of the considerations involved
    in representation learning, please refer to this excellent paper by some of the
    pioneers of deep learning—*Representation Learning: A Review and New Perspectives*
    (Y Bengio, A Courville, Pascal Vincent, 2016): [https://arxiv.org/pdf/1206.5538.pdf](https://arxiv.org/pdf/1206.5538.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing and data treatments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you must already be well aware, neural networks are quite picky eaters. Namely,
    there are two staple operations that need to be performed before our data can
    be fed to a neural network: **vectorization** and **normalization**.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall that vectorization simply means that all of the inputs and target variables
    of your data must be in a tensor format, containing floating-point values (or
    in specific cases, such as the Boston Housing Price Regression example, integers).
    We previously achieved this by populating a matrix of zeros using indexed values
    (as in the sentiment classification example), or by one-hot encoding our variables.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides vectorization, another consideration we had to undertake was normalization
    of our input data. This was more of a standard practice in most ML workflows and
    consisted of transforming our input variables into a small, homogeneous range
    of values. We achieved this in tasks such as image processing by normalizing the
    pixel values between 0 and 1\. In cases where our input variables were on different
    scales (such as with the Boston example), we had to implement an independent feature-wise
    normalization strategy. Forgoing such steps may cause gradient updates that do
    not converge to a global minimum, making it much more difficult for a network
    to learn. In general, a rule of thumb can be to try independent feature normalization,
    ensuring a feature-wise mean of 0 and a standard deviation of 1.
  prefs: []
  type: TYPE_NORMAL
- en: Smoothness of the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks struggle the least with predictions if they are shown a data
    distribution that is locally smooth. What does this mean? Simply put, if an input, *x*,produces
    an output, *y*, a point close to this input will produce an output proportionally
    close to *y*. This is the property of smoothness and greatly augments learning
    architecture such as neural networks, allowing them to capture better representations
    from such data. Unfortunately, however, having this property in your data distribution
    is not the only criteria for neural networks to learn good representations; the
    curse of dimensionality, for example, is still one that would need to be addressed,
    by feature selection or dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Adding something such as a smoothening factor to your data, for example, can
    largely benefit the learning process, as we did when predicting stock market prices
    with LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: Encouraging sparse representation learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you were training a network to classify pictures of cats and dogs. Over
    the course of training, the intermediate layers will learn different representations
    or features from the input values (such as cat ears, dog eyes, and so on), combining
    them in a probabilistic fashion to detect the presence of an output class (that
    is, whether a picture is of a cat or a dog).
  prefs: []
  type: TYPE_NORMAL
- en: Yet, while performing inference on an individual image, do we need the feature
    that detects cat ears to ascertain that this particular image is of a dog? The
    answer in almost all cases is a resounding no. Most of the time, we can assume
    that most features that a network learns during training are actually not relevant
    for each individual prediction. Hence, we want our network to learn sparse representations
    for each input, a resulting tensor representation where most entries are zero
    (denoting perhaps the presence or absence of the corresponding features).
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, sparsity is a very desirable property for learned representations.
    Not only does this allow us to have a smaller number of neurons active when representing
    a phenomenon (thereby increasing the efficiency of our network), but it also helps
    the network to better untangle the main factors of variance present within the
    data itself.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, sparsity allows the network to recognize learned features in data,
    without being perturbed by small variations occurring in the inputs. Implementation
    wise, sparsity simply enforces the value of most learned features to zero, when
    representing any individual input. Sparse representations can be learned through
    operations such as one-hot encoding, non-linear transformations as imposed by
    the activation functions, or by other means of penalizing derivatives of the intermediate
    layers, with respect to the input values.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, it is assumed that deeper model architectures give access to higher
    representational power, allowing us to hierarchically organize abstract representations
    for predictive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: However, as we know, deeper architectures are prone to overfitting, and hence
    can be challenging to train, requiring keen attention to aspects such as regularization
    (as seen with the regularization strategies explored in [Chapter 3](https://cdp.packtpub.com/hands_on_neural_networks_with_keras/wp-admin/post.php?post=26&action=edit),
    *Signal Processing - Data Analysis with Neural Networks*). How can we assess exactly
    how many layers to initialize, with the appropriate number of neurons and relevant
    regularization strategies to use? Given the complexity involved in designing the
    right architecture, it can be very time consuming to experiment with different
    model hyperparameters to find the right network specifications to solve the task
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: While we have discussed general intuitions on designing more robust architectures,
    using techniques such as dropout and batch normalization, we can't help but wonder
    whether there is a way to automate this entire tedious process. It would even
    be tempting to apply deep learning to this process itself, where it not a discretely
    constrained optimization problem (as opposed the continuous optimization problems
    we have so far been solving, using gradient decent).
  prefs: []
  type: TYPE_NORMAL
- en: Automatic optimization and evolutionary algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fortunately, many tools exist that allow such automatic parameter optimization. **Talos**
    ([https://github.com/autonomio/talos](https://github.com/autonomio/talos)) is
    one such tool built on top of the Keras library, made available as open source
    on GitHub. It allows you to predefine a set of hyperparameters (such as different
    number of layers, neurons per layer, and activation functions), after which the
    tool will automatically train and compare those Keras models to assess which one
    performs better.
  prefs: []
  type: TYPE_NORMAL
- en: Other solutions such as **Hyperas** ([https://github.com/maxpumperla/hyperas](https://github.com/maxpumperla/hyperas)
    ) or **auto_ML** ([https://auto-ml.readthedocs.io/en/latest/](https://auto-ml.readthedocs.io/en/latest/))
    allow similar functionalities and can help drastically reduce development time,
    allowing you to discover what hyperparameters work best for your task. In fact,
    you can use such tools and make your own genetic algorithms that help you select
    from a pool of hyperparameters, train and evaluate a network, then select the
    best of those network architectures, randomly mutate some hyperparameters of the
    selected networks, and repeat the training and evaluation all over again. Eventually,
    such an algorithm can produce increasingly complex architectures to solve a given
    problem, just as evolution does in nature. While a detailed overview of such methods
    is well beyond the scope of this book, we take the liberty of linking a simplistic
    implementation of such an approach next, which allows evolving network parameters
    in order to find ideal configurations.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Evolutionary algorithms and neural networks**: [http://www.weiss-gerhard.info/publications/C22.pdf](http://www.weiss-gerhard.info/publications/C22.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation of evolutionary neural networks**: [https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164](https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-network predictions and ensemble models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another way to get the best of neural networks is by using ensemble models.
    The idea is quite simple: why use one network when you can use many? In other
    words, why not design different neural networks, each sensitive to specific representations
    in the input data? Then, we can average out their predictions, getting a more
    generalizable and parsimonious prediction than using just one network.'
  prefs: []
  type: TYPE_NORMAL
- en: We can even attribute weights to each network, by pegging each network's prediction
    to the test accuracy it achieves on the task. Then, we can take a weighted average
    of the predictions (weighted with their relative accuracies) from each network
    to get to a more comprehensive prediction altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, we just look at the data with different eyes; each network, by
    virtue of its design, may pay attention to different factors of variance, perhaps
    ignored by its other counterparts. This method is fairly straightforward and simple
    to implement and only requires designing separate networks, with good intuition
    on what kind of representations each network can be expected to capture. After
    that, it is simply a question of adding appropriate weights to each individual
    network's prediction and averaging out the results.
  prefs: []
  type: TYPE_NORMAL
- en: The future of AI and neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the course of this book, we have dived deep into a specific realm
    of AI, nested within ML, that we call deep learning. This caveat of machine intelligence
    takes a connectionist approach, combining the predictive power of distributed
    representations, in turn learned by a deep neural network.
  prefs: []
  type: TYPE_NORMAL
- en: While deep learning neural networks have risen to prominence, since the advent
    of GPU, accelerated computing, and the availability of big data, many considerations
    have gone into improving the intuition and implementation behind these architectures,
    since their re-ascension to popularity, about a decade ago (Hinton et al, 2008).
    Yet, still, there exist many complex tasks that deep learning is not yet able
    to adequately tackle.
  prefs: []
  type: TYPE_NORMAL
- en: Global vectors approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, the sequence of mathematical transformations on given input values
    is simply not enough to learn an effective function mapping them to some output
    values. Already, many such examples exist, especially in the domain of **natural
    language processing** (**NLP**). While we restricted our NLP use cases to simple
    word vectorization, this approach can be limiting for some use cases requiring
    the understanding of complex dependencies that exist in human language.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, a popular approach is to symbolically attribute properties to words
    and attribute values to these properties so as to allow comparison with other
    words. This is the basic intuition behind a technique known as **Global Vectors**
    (**GloVe**) used as a text-preprocessing vectorization technique, before data
    is fed to neural networks. Such an approach perhaps alludes to how the use of
    deep learning will evolve in the future. This specific workflow illustrates the
    use of principles from both distributed and symbolic representations to discover,
    understand, and solve complex problems, such as the logical reasoning involved
    in machine question-answering.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the future, it is very likely that we start using principles from various
    disciplines of AI, in conjunction with the power of distributed representation
    that deep learning brings to the table, to design systems that are truly and generally
    intelligent. Such systems can then go about learning tasks in an autonomous manner,
    with the enhanced capability to tackle complex problems. It could, for example,
    conduct research following scientific methodology, thereby automating human knowledge
    discovery. In short, deep learning is here to stay, and will likely be complemented
    by other subfields of AI, to develop very powerful computing systems.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware hurdles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yet, before we get to that stage of AI, surely there are other improvements
    to be made. Recall that deep learning became popular not only because we learned
    techniques to represent and process data on a higher level, but also because our
    hardware improved drastically. We now have access to the processing power that
    would have cost us millions about a few decades ago, for literally a few thousand
    dollars. Similarly, there may yet be another hardware hurdle for humanity to overcome
    before we can design truly intuitive and logically superior systems, capable of
    solving humanity's grand problems.
  prefs: []
  type: TYPE_NORMAL
- en: Many have speculated that this giant leap will materialize in the form of quantum
    computing. While covering this topic in depth is a bit beyond the scope of this
    book (and the proficiencies of this author), we could not help but include a short
    parenthesis to illustrate the benefits and complexities involved in importing
    neural networks to an emerging computing paradigm, with promising prospects.
  prefs: []
  type: TYPE_NORMAL
- en: The road ahead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the previous diagram depicting the advances in processing power might
    make us look back in nostalgia over how far we have come, this same nostalgia
    will be wiped away quite fast as soon as we realize how far we still have to go.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the preceding diagram, the computational power of the systems we
    have implemented so far are nowhere near that of a human brain. The neural networks
    that we devised (at least in this book) had a number of neurons ranging anywhere
    from a million (the equivalent of what you would find in a cockroach) to about
    ten million (close to what is common for an adult zebra fish).
  prefs: []
  type: TYPE_NORMAL
- en: Attempting to train a network that parallels a human mind, at least in the number
    of neurons used, is currently beyond the scope of human engineering, as of the
    date of this book. It simply surpasses our current computing capacity. Moreover,
    it is important to note that this comparison naturally ignores the detail that
    the neurons in each of these learning systems (artificial versus biological) are
    different, both in form and function.
  prefs: []
  type: TYPE_NORMAL
- en: Biological neurons operate much differently than their artificial counterparts
    and are influenced by quantum systems such as molecular chemistry. The exact nature
    of information processing and storage in biological neurons is still not fully
    understood by modern neuroscience. So, how can we simulate what we don't yet fully
    comprehend? One answer to this dilemma could be to design more powerful computers,
    capable of representing and transforming information in ways more suited for the
    domain. This brings us to the phenomenon of quantum computing.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with classical computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In simplistic terms, quantum mechanics is a field that deals with the study
    of things that are very small, isolated, and cold. While this may not create an
    appealing picture at first, consider the problem we are facing currently. Already,
    the exponential growth of the number of transistors in a chip, as predicted by
    Moore's law, seems to be slowing down.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this important? These transistors are actually what permits us to compute!
    From simple data storage to the complex mathematical operations native to neural
    networks, all data representation in classical computers is by virtue of these
    semiconductor devices. We use them to amplify and switch electric signals, thereby
    creating logic gates capable of tracking the presence of charged electrons (1)
    or absence thereof (0). These switches can be manipulated to create binary digits,
    or bits, that represent a unit of information. In essence, this binary system
    forms the basis of all digital encoding, exploiting the physical properties of
    transistors to store and process information. It is the language of machines,
    which allows representing and processing information.
  prefs: []
  type: TYPE_NORMAL
- en: From the very first fully digital and programmable computer (Z3, 1938 ), to
    the latest supercomputers (IBMs Summit, 2018), this fundamental language of representation
    has not changed. For all intents and purposes, the lingua franca of machines has
    remained based on the binary system for about a century.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, as we discussed earlier, different representations allow us to perform
    different operations. Hence, perhaps it is time for us to revise the fundamental
    manner in which we represent data. Given the fact that transistors can only get
    so small, we are slowly but surely reaching the limits of classical computing.
    Hence, what better place to look for solutions than the infinitesimally small
    and bizarre world of quantum mechanics.
  prefs: []
  type: TYPE_NORMAL
- en: The advent of quantum computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While classical computers use binary representations to encode information into
    bits, their quantum counterparts use the laws of physics to encode information
    in **Q-Bits**. There are many approaches toward designing such systems. You can,
    for instance, use microwave pulses to alter the spin momentum of an electron,
    to represent and store information.
  prefs: []
  type: TYPE_NORMAL
- en: Quantum superposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As it turns out, this may allow us to leverage interesting quantum phenomena
    to represent operations that have no known classical counterpart. Operations such
    as **quantum superposition**, where two different quantum states may be added
    together to produce a third state, valid on its own. Hence, unlike its classical
    counterpart, a Q-Bit can have three states: (0), (1), and (1/0,), where the third
    represents a state only achievable through the property of quantum superposition.'
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, this allows us to represent much more information, opening doors
    for us to tackle problems from higher-complexity classes (such as simulating intelligence,
    for example).
  prefs: []
  type: TYPE_NORMAL
- en: Distinguishing Q-Bits from classical counterparts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Other quantum properties also exist that distinguish the Q-Bit from its classical
    counterpart. For example, two Q-Bits can enter an entangled state, where the spin
    of the electrons of each Q-Bit is set to continuously point in opposite directions.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this a big deal? Well, these two Q-Bits can then be separated by billions
    of miles, while still seemingly maintaining a link between each other. We know,
    by virtue of the laws of physics, that the spin of each electron will always point
    in opposite directions when observed, regardless of the distance between the Q-Bits
    themselves.
  prefs: []
  type: TYPE_NORMAL
- en: This entangled state is interesting, because there is no classical operation
    that can represent the idea of two different bits having no specified value, yet
    always remaining the opposite value of each other. These concepts have the potential
    of revolutionizing fields such as communication and cryptography, on top of the
    exponential computing power they bring to the table. The more Q-Bits a quantum
    computer can leverage, the more non-classical operations it can use to represent
    and process data. In essence, these are some of the pivotal underlining ideas
    behind quantum computing.
  prefs: []
  type: TYPE_NORMAL
- en: Quantum neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many of you may be thinking that all of this is nice, but we ourselves are
    surely decades away from being able to use a quantum computer, let alone design
    neural networks on it. While healthy skepticism is always nice, it does not do
    justice to the efforts of contemporary researchers, scientists, and businesses
    working around the clock to bring such systems to life. It may surprise you to
    know, for example, that anybody in the world with an internet connection today
    has free access to a quantum computer, using the link right here (courtesy of
    IBM): [https://quantumexperience.ng.bluemix.net/qx/editor](https://quantumexperience.ng.bluemix.net/qx/editor).'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, researchers such as Francesco Tacchino and his colleagues have already
    used this service to implement quantum neural networks for classification tasks!
    They were able to implement the world's first quantum perceptron, similar in spirit
    to the perceptron we saw in [Chapter 2](https://cdp.packtpub.com/hands_on_neural_networks_with_keras/wp-admin/post.php?post=37&action=edit), *A
    Deeper Dive into Neural Networks*, yet augmented with the laws of quantum mechanics.
    They used IBM's **Q-5** **Tenerife** superconducting quantum processor, which
    allows the manipulation of up to five Q-Bits, to train a classifier to detect
    simple patterns such as line segments.
  prefs: []
  type: TYPE_NORMAL
- en: While this may sound trivial at first, the implications of their work are quite
    significant. They were able to decisively show how a quantum computer allows an
    exponential increase in the number of dimensions it can process. For instance,
    while a classical perceptron is capable of processing input values of *n *dimensions,
    its quantum counterpart designed by these researchers was able to process 2N dimensions!
    Such implementations pave the way for future researchers to implement more complex
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, the realm of quantum neural networks is still in infancy, since quantum
    computers themselves have a lot of improvements to undergo. However, active research
    currently focuses on many areas of importing neural nets to the quantum world,
    ranging from straightforward extensions of connected layers, to quantum optimization
    algorithms that are better at navigating the loss landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Some have even speculated that quantum phenomena such as tunneling may be used
    to, quite literally, tunnel through the loss landscape to converge to optimal
    network weights extremely quickly! This truly represents the dawn of a new age
    for ML and AI. Once these systems have been thoroughly tried and tested, we may
    be able to represent truly complex patterns in novel ways, with implications beyond
    our current imagination.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A paper on quantum neural networks: [https://arxiv.org/pdf/1811.02266.pdf](https://arxiv.org/pdf/1811.02266.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A QNN paper by Google: [https://arxiv.org/pdf/1802.06002.pdf](https://arxiv.org/pdf/1802.06002.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Google Quantum AI blog: [https://ai.googleblog.com/2018/12/exploring-quantum-neural-networks.html](https://ai.googleblog.com/2018/12/exploring-quantum-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quantum optimization algorithms: [https://ieeexplore.ieee.org/abstract/document/6507335](https://ieeexplore.ieee.org/abstract/document/6507335)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technology and society
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today, we stand at the intersection of very interesting times. Times that some
    claim will define the future of humankind and change the way we perceive and interact
    with the world altogether. Automation, cognitive technologies, AI, and quantum
    computing are but a few among the sea of disruptive technologies, constantly causing
    organizations to reassess their value chains and better themselves in the way
    they impact the world.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps people will be able to work more efficiently, organize their time better,
    and devote their lives to activities that uniquely complement their skill sets,
    thereby delivering optimal value to the society they participate in. Or, perhaps,
    there is more of a dystopic future ahead of us, where such technologies are used
    to disenfranchise the masses, observe and control human behavior, and limit our
    freedom. While the technology itself is simply analogous to any tool previously
    invented by humans, the way we choose to use these tools will have reverberating
    consequences for all the involved stakeholders. Ultimately, the choice is ours.
    Luckily, we are at the dawn of this new era, and so we can still steer the direction
    of progress in a sustainable and inclusive manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, organizations across the globe are rushing to find ways to reap
    the fruit from such technologies before it is too late for them to adapt, leading
    to all sorts of concerns spanning from transparency to legality and ethics. These
    dilemmas surface despite the fact that we are still in the infancy phase of AI.
    In essence, all the methods and techniques we explored through the course of this
    book are narrow AI technologies. They are specific systems capable of solving
    narrow components of a workflow, be it to solve specific computer vision tasks
    or to answer certain types of questions in natural language. This is very different
    than the idea of AI, in its literal sense: an intelligence that is autonomous
    and can learn in a self-sufficient manner, without outsiders directly manipulating
    its internal learning algorithm. It is an intelligence that can grow and evolve,
    similar in spirit to the journey of a human baby to an adult, albeit at a different
    rate.'
  prefs: []
  type: TYPE_NORMAL
- en: Contemplating our future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider a new-born human baby. At first, it is even incapable of breathing
    and has to be motivated to do so by a few friendly spanks, delivered by the attending
    physician. For the first few months, this being does not seem to do anything remarkable
    and is incapable of independent movement, let alone thought. Yet, slowly, this
    same baby develops an internal model of the world around it. It becomes better
    and better at distinguishing all this light it sees and the cacophony of sounds
    it hears. Soon, it starts recognizing things such as movement, perhaps in the
    guise of a friendly face, hovering around with deliciously gooey substances. A
    bit later, it develops a premature internal physics engine, through the observation
    of the world around it. It then uses these representations to first crawl, then
    toddle, and eventually even walk, progressively updating its internal physics
    engine to represent more and more complex models of the world. Soon enough, it
    is able to perform somersaults, compose elaborate poetry, and peruse causes such
    as mathematics, history, philosophy, or even AI science.
  prefs: []
  type: TYPE_NORMAL
- en: Do note that nobody is exactly tuning a CNN to make the baby see better, or
    increasing the size of an LSTM architecture, for the baby to write better poetry.
    The individual was able to do so without any direct external intervention, by
    simply observing things around itself, listening to people, and learning by doing.
    While there are a multitude of things going on under the hood of a human baby
    as it journeys to adulthood, almost all of which are quite beyond the scope of
    this work, this example demonstrates how far we still are from creating something
    that can truly parallel our own intellect.
  prefs: []
  type: TYPE_NORMAL
- en: The same type of baby can eventually learn to drive cars, and with a little
    bit of help, solve complex problems such as world hunger or interplanetary travel!
    This is truly an intelligent organism. The artificial counterparts that we explored
    in our book are not yet worthy of comparison to the former form of intelligence,
    simply due to their narrow applicability. They are but pieces of a puzzle, a manner
    of approaching information processing, often for a specific cognitive domain.
    Perhaps one day, these narrow technologies will be united in a comprehensive system,
    splicing a multitude of such technologies together, creating something even greater
    than the components within. In fact, this is currently happening, as we have seen
    throughout this book already. For example, we saw how convolutional architectures
    may be merged with other neural network architectures such as LSTMs, for complex
    visual information processing involving a temporal component, as in making the
    right moves in a game.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the question then still remains: will such architectures truly become intelligent?
    This may be a question for the philosophers of today, but it is also one for the
    scientists of tomorrow. As these systems evolve, and conquer more and more realms
    that were previously through attainable only by humans, we will eventually face
    such existential questions about these machines and ourselves. Are we really that
    different? Are we just very complex computers, carrying out arithmetic operations
    through biology? Or is there more to intelligence and consciousness than mere
    computation? Sadly, we do not have all the answers, yet this does make for an
    exciting journey ahead for our species.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reiterated what we have learned in this book and saw how
    we can improve the existing techniques. We then moved on to see the future of
    deep learning and gained insight into quantum computing.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this journey has been informative. Thanks for reading and all the best!
  prefs: []
  type: TYPE_NORMAL
