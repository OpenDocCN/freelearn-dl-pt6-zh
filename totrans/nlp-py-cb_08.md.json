["```py\nimport nltk\nimport threading\nimport queue\nimport feedparser\nimport uuid\n```", "```py\nthreads = []\n```", "```py\nqueues = [queue.Queue(), queue.Queue()]\n```", "```py\ndef extractWords():\n```", "```py\nurl = 'https://timesofindia.indiatimes.com/rssfeeds/1081479906.cms'\n```", "```py\nfeed = feedparser.parse(url)\n```", "```py\nfor entry in feed['entries'][:5]:\n```", "```py\ntext = entry['title']\n```", "```py\nif 'ex' in text:\n  continue\n```", "```py\nwords = nltk.word_tokenize(text)\n```", "```py\ndata = {'uuid': uuid.uuid4(), 'input': words}\n```", "```py\nqueues[0].put(data, True)\n```", "```py\nprint(\">> {} : {}\".format(data['uuid'], text))\n```", "```py\ndef extractPOS():\n```", "```py\nwhile True:\n```", "```py\nif queues[0].empty():\n  break\n```", "```py\nelse:\n```", "```py\ndata = queues[0].get()\n```", "```py\nwords = data['input']\npostags = nltk.pos_tag(words)\n```", "```py\nqueues[0].task_done()\n```", "```py\nqueues[1].put({'uuid': data['uuid'], 'input': postags}, True)\n```", "```py\ndef extractNE():\n```", "```py\nwhile True:\n```", "```py\nif queues[1].empty():\n  break\n```", "```py\nelse:\n  data = queues[1].get()\n```", "```py\npostags = data['input']\nqueues[1].task_done()\n```", "```py\nchunks = nltk.ne_chunk(postags, binary=False)\n\nprint(\"  << {} : \".format(data['uuid']), end = '')\n  for path in chunks:\n    try:\n      label = path.label()\n      print(path, end=', ')\n      except:\n        pass\n      print()\n```", "```py\ndef runProgram():\n```", "```py\ne = threading.Thread(target=extractWords())\ne.start()\nthreads.append(e)\n```", "```py\np = threading.Thread(target=extractPOS())\np.start()\nthreads.append(p)\n```", "```py\n    n = threading.Thread(target=extractNE())\n    n.start()\n    threads.append(n)\n```", "```py\n    queues[0].join()\n    queues[1].join()\n```", "```py\n    for t in threads:\n        t.join()\n```", "```py\nif __name__ == '__main__':\n    runProgram()\n```", "```py\nimport nltk\nimport math\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n```", "```py\nclass TextSimilarityExample:\n```", "```py\ndef __init__(self):\n```", "```py\nself.statements = [\n  'ruled india',\n  'Chalukyas ruled Badami',\n  'So many kingdoms ruled India',\n  'Lalbagh is a botanical garden in India'\n]\n```", "```py\n    def TF(self, sentence):\n        words = nltk.word_tokenize(sentence.lower())\n        freq = nltk.FreqDist(words)\n        dictionary = {}\n        for key in freq.keys():\n            norm = freq[key]/float(len(words))\n            dictionary[key] = norm\n        return dictionary\n```", "```py\n    def IDF(self):\n        def idf(TotalNumberOfDocuments, NumberOfDocumentsWithThisWord):\n            return 1.0 + math.log(TotalNumberOfDocuments/NumberOfDocumentsWithThisWord)\n        numDocuments = len(self.statements)\n        uniqueWords = {}\n        idfValues = {}\n        for sentence in self.statements:\n            for word in nltk.word_tokenize(sentence.lower()):\n                if word not in uniqueWords:\n                    uniqueWords[word] = 1\n                else:\n                    uniqueWords[word] += 1\n        for word in uniqueWords:\n            idfValues[word] = idf(numDocuments, uniqueWords[word])\n        return idfValues\n```", "```py\n    def TF_IDF(self, query):\n        words = nltk.word_tokenize(query.lower())\n        idf = self.IDF()\n        vectors = {}\n        for sentence in self.statements:\n            tf = self.TF(sentence)\n            for word in words:\n                tfv = tf[word] if word in tf else 0.0\n                idfv = idf[word] if word in idf else 0.0\n                mul = tfv * idfv\n                if word not in vectors:\n                    vectors[word] = []\n                vectors[word].append(mul)\n        return vectors\n```", "```py\n    def displayVectors(self, vectors):\n        print(self.statements)\n        for word in vectors:\n            print(\"{} -> {}\".format(word, vectors[word]))\n```", "```py\n    def cosineSimilarity(self):\n        vec = TfidfVectorizer()\n        matrix = vec.fit_transform(self.statements)\n        for j in range(1, 5):\n            i = j - 1\n            print(\"\\tsimilarity of document {} with others\".format(i))\n            similarity = cosine_similarity(matrix[i:j], matrix)\n            print(similarity)\n```", "```py\n    def demo(self):\n        inputQuery = self.statements[0]\n        vectors = self.TF_IDF(inputQuery)\n        self.displayVectors(vectors)\n        self.cosineSimilarity()\n```", "```py\nsimilarity = TextSimilarityExample()\nsimilarity.demo()\n```", "```py\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom gensim import corpora, models\nimport nltk\nimport feedparser\n```", "```py\nclass IdentifyingTopicExample:\n```", "```py\n    def getDocuments(self):\n```", "```py\n        url = 'https://sports.yahoo.com/mlb/rss.xml'\n        feed = feedparser.parse(url)\n```", "```py\n        self.documents = []\n```", "```py\n        for entry in feed['entries'][:5]:\n```", "```py\n            text = entry['summary']\n```", "```py\n            if 'ex' in text:\n                continue\n```", "```py\n            self.documents.append(text)\n```", "```py\n            print(\"-- {}\".format(text))\n```", "```py\n        print(\"INFO: Fetching documents from {} completed\".format(url))\n```", "```py\n    def cleanDocuments(self):\n```", "```py\n        tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n```", "```py\n        en_stop = set(stopwords.words('english'))\n```", "```py\n        self.cleaned = []\n```", "```py\n        for doc in self.documents:\n```", "```py\n            lowercase_doc = doc.lower()\n```", "```py\n            words = tokenizer.tokenize(lowercase_doc)\n```", "```py\n            non_stopped_words = [i for i in words if not i in en_stop]\n```", "```py\n            self.cleaned.append(non_stopped_words)\n```", "```py\n        print(\"INFO: Cleaning {} documents completed\".format(len(self.documents)))\n```", "```py\n    def doLDA(self):\n```", "```py\n        dictionary = corpora.Dictionary(self.cleaned)\n```", "```py\n\n     corpus = [dictionary.doc2bow(cleandoc) for cleandoc in self.cleaned]\n```", "```py\n        ldamodel = models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary)\n```", "```py\n        print(ldamodel.print_topics(num_topics=2, num_words=4))\n```", "```py\n    def run(self):\n        self.getDocuments()\n        self.cleanDocuments()\n        self.doLDA()\n```", "```py\nif __name__ == '__main__':\n    topicExample = IdentifyingTopicExample()\n    topicExample.run()\n```", "```py\nfrom gensim.summarization import summarize\nfrom bs4 import BeautifulSoup\nimport requests\n```", "```py\nurls = {\n    'Daff: Unproven Unification of Suffix Trees and Redundancy': 'http://scigen.csail.mit.edu/scicache/610/scimakelatex.21945.none.html',\n    'CausticIslet: Exploration of Rasterization': 'http://scigen.csail.mit.edu/scicache/790/scimakelatex.1499.none.html'\n}\n```", "```py\nfor key in urls.keys():\n```", "```py\n    url = urls[key]\n```", "```py\n    r = requests.get(url)\n```", "```py\n    soup = BeautifulSoup(r.text, 'html.parser')\n```", "```py\n    data = soup.get_text()\n```", "```py\n    pos1 = data.find(\"1  Introduction\") + len(\"1  Introduction\")\n```", "```py\n    pos2 = data.find(\"2  Related Work\")\n```", "```py\n    text = data[pos1:pos2].strip()\n```", "```py\n    print(\"PAPER URL: {}\".format(url))\n    print(\"TITLE: {}\".format(key))\n```", "```py\n    print(\"GENERATED SUMMARY: {}\".format(summarize(text)))\n```", "```py\n    print()\n```", "```py\nimport nltk\nfrom nltk.chunk import tree2conlltags\nfrom nltk.corpus import names\nimport random\n```", "```py\nclass AnaphoraExample:\n```", "```py\n    def __init__(self):\n```", "```py\n        males = [(name, 'male') for name in names.words('male.txt')]\n        females = [(name, 'female') for name in names.words('female.txt')]\n```", "```py\n        combined = males + females\n        random.shuffle(combined)\n```", "```py\n        training = [(self.feature(name), gender) for (name, gender) in combined]\n```", "```py\n        self._classifier = nltk.NaiveBayesClassifier.train(training)\n```", "```py\n    def feature(self, word):\n        return {'last(1)' : word[-1]}\n```", "```py\n    def gender(self, word):\n        return self._classifier.classify(self.feature(word))\n```", "```py\n    def learnAnaphora(self):\n```", "```py\n        sentences = [\n            \"John is a man. He walks\",\n            \"John and Mary are married. They have two kids\",\n            \"In order for Ravi to be successful, he should follow John\",\n            \"John met Mary in Barista. She asked him to order a Pizza\"\n        ]\n```", "```py\n\n        for sent in sentences:\n```", "```py\n            chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)), binary=False)\n```", "```py\n            stack = []\n```", "```py\n            print(sent)\n```", "```py\n            items = tree2conlltags(chunks)\n```", "```py\n            for item in items:\n```", "```py\n                if item[1] == 'NNP' and (item[2] == 'B-PERSON' or item[2] == 'O'):\n                    stack.append((item[0], self.gender(item[0])))\n```", "```py\n                elif item[1] == 'CC':\n                    stack.append(item[0])\n```", "```py\n                elif item[1] == 'PRP':\n                    stack.append(item[0])\n```", "```py\n            print(\"\\t {}\".format(stack))\n```", "```py\nanaphora = AnaphoraExample()\nanaphora.learnAnaphora()\n```", "```py\nimport nltk\n```", "```py\n\ndef understandWordSenseExamples():\n```", "```py\n    words = ['wind', 'date', 'left']\n\n   print(\"-- examples --\")\n    for word in words:\n        syns = nltk.corpus.wordnet.synsets(word)\n        for syn in syns[:2]:\n            for example in syn.examples()[:2]:\n                print(\"{} -> {} -> {}\".format(word, syn.name(), example))\n```", "```py\ndef understandBuiltinWSD():\n```", "```py\n    print(\"-- built-in wsd --\")\n    maps = [\n        ('Is it the fish net that you are using to catch fish ?', 'fish', 'n'),\n        ('Please dont point your finger at others.', 'point', 'n'),\n        ('I went to the river bank to see the sun rise', 'bank', 'n'),\n    ]\n```", "```py\n    for m in maps:\n        print(\"Sense '{}' for '{}' -> '{}'\".format(m[0], m[1], nltk.wsd.lesk(m[0], m[1], m[2])))\n```", "```py\nif __name__ == '__main__':\n    understandWordSenseExamples()\n    understandBuiltinWSD()\n```", "```py\nimport nltk\nimport nltk.sentiment.sentiment_analyzer\n```", "```py\ndef wordBasedSentiment():\n```", "```py\n    positive_words = ['love', 'hope', 'joy']\n```", "```py\n    text = 'Rainfall this year brings lot of hope and joy to Farmers.'.split()\n```", "```py\n    analysis = nltk.sentiment.util.extract_unigram_feats(text, positive_words)\n```", "```py\n    print(' -- single word sentiment --')\n    print(analysis)\n```", "```py\ndef multiWordBasedSentiment():\n```", "```py\n    word_sets = [('heavy', 'rains'), ('flood', 'bengaluru')]\n```", "```py\n    text = 'heavy rains cause flash flooding in bengaluru'.split()\n```", "```py\n\n    analysis = nltk.sentiment.util.extract_bigram_feats(text, word_sets)\n```", "```py\n    print(' -- multi word sentiment --')\n    print(analysis)\n```", "```py\ndef markNegativity():\n```", "```py\n    text = 'Rainfall last year did not bring joy to Farmers'.split()\n```", "```py\n    negation = nltk.sentiment.util.mark_negation(text)\n```", "```py\n\n    print(' -- negativity --')\n    print(negation)\n```", "```py\nif __name__ == '__main__':\n    wordBasedSentiment()\n    multiWordBasedSentiment()\n    markNegativity()\n```", "```py\nimport nltk\nimport nltk.sentiment.util\nimport nltk.sentiment.sentiment_analyzer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n```", "```py\ndef mySentimentAnalyzer():\n```", "```py\n\n    def score_feedback(text):\n```", "```py\n\n        positive_words = ['love', 'genuine', 'liked']\n```", "```py\n\n        if '_NEG' in ' '.join(nltk.sentiment.util.mark_negation(text.split())):\n            score = -1\n```", "```py\n\n        else:\n            analysis = nltk.sentiment.util.extract_unigram_feats(text.split(), positive_words)\n```", "```py\n            if True in analysis.values():\n                score = 1\n            else:\n                score = 0\n```", "```py\n        return score\n```", "```py\n\n    feedback = \"\"\"I love the items in this shop, very genuine and quality is well maintained.\n    I have visited this shop and had samosa, my friends liked it very much.\n    ok average food in this shop.\n    Fridays are very busy in this shop, do not place orders during this day.\"\"\"\n```", "```py\n    print(' -- custom scorer --')\n    for text in feedback.split(\"\\n\"):\n        print(\"score = {} for >> {}\".format(score_feedback(text), text))\n```", "```py\ndef advancedSentimentAnalyzer():\n```", "```py\n    sentences = [\n        ':)',\n        ':(',\n        'She is so :(',\n        'I love the way cricket is played by the champions',\n        'She neither likes coffee nor tea',\n    ]\n```", "```py\n    senti = SentimentIntensityAnalyzer()\n\n  print(' -- built-in intensity analyser --')\n    for sentence in sentences:\n        print('[{}]'.format(sentence), end=' --> ')\n        kvp = senti.polarity_scores(sentence)\n        for k in kvp:\n            print('{} = {}, '.format(k, kvp[k]), end='')\n        print()\n```", "```py\n\nif __name__ == '__main__':\n    advancedSentimentAnalyzer()\n    mySentimentAnalyzer()\n```", "```py\nimport nltk\n```", "```py\ndef builtinEngines(whichOne):\n```", "```py\n    if whichOne == 'eliza':\n        nltk.chat.eliza.demo()\n    elif whichOne == 'iesha':\n        nltk.chat.iesha.demo()\n    elif whichOne == 'rude':\n        nltk.chat.rude.demo()\n    elif whichOne == 'suntsu':\n        nltk.chat.suntsu.demo()\n    elif whichOne == 'zen':\n        nltk.chat.zen.demo()\n    else:\n        print(\"unknown built-in chat engine {}\".format(whichOne))\n```", "```py\ndef myEngine():\n```", "```py\n    chatpairs = (\n        (r\"(.*?)Stock price(.*)\",\n            (\"Today stock price is 100\",\n            \"I am unable to find out the stock price.\")),\n        (r\"(.*?)not well(.*)\",\n            (\"Oh, take care. May be you should visit a doctor\",\n            \"Did you take some medicine ?\")),\n        (r\"(.*?)raining(.*)\",\n            (\"Its monsoon season, what more do you expect ?\",\n            \"Yes, its good for farmers\")),\n        (r\"How(.*?)health(.*)\",\n            (\"I am always healthy.\",\n            \"I am a program, super healthy!\")),\n        (r\".*\",\n            (\"I am good. How are you today ?\",\n            \"What brings you here ?\"))\n    )\n```", "```py\n    def chat():\n        print(\"!\"*80)\n        print(\" >> my Engine << \")\n        print(\"Talk to the program using normal english\")\n        print(\"=\"*80)\n        print(\"Enter 'quit' when done\")\n        chatbot = nltk.chat.util.Chat(chatpairs, nltk.chat.util.reflections)\n        chatbot.converse()\n```", "```py\n    chat()\n```", "```py\nif __name__ == '__main__':\n    for engine in ['eliza', 'iesha', 'rude', 'suntsu', 'zen']:\n        print(\"=== demo of {} ===\".format(engine))\n        builtinEngines(engine)\n        print()\n    myEngine()\n```"]