- en: NLP Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, several topics were covered concerning the undertaking
    of DL distributed training in a Spark cluster. The concepts presented there are
    common to any network model. Starting from this chapter, specific use cases for
    RNNs or LSTMs will be looked at first, and then CNNs will be covered. This chapter
    starts by introducing the following core concepts of **Natural Language Processing**
    (**NLP**):'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentence segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part-of-speech tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Named entity extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The theory behind the concepts in the preceding list will be detailed before
    finally presenting two complete Scala examples of NLP, one using Apache Spark
    and the Stanford core NLP library, and the other using the Spark core and the
    `Spark-nlp` library (which is built on top of Apache Spark MLLib). The goal of
    the chapter is to make readers familiar with NLP, before moving to implementations
    based on DL (RNNs) through DL4J and/or Keras/Tensorflow in combination with Spark,
    which will be the core topic of the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP is the field of using computer science and AI to process and analyze natural
    language data and then make machines able to interpret it as humans do. During
    the 1980s, when this concept started to get hyped, language processing systems
    were designed by hand coding a set of rules. Later, following increases in calculation
    power, a different approach, mostly based on statistical models, replaced the
    original one. A later ML approach (supervised learning first, also semi-supervised
    or unsupervised at present time) brought advances in this field, such as voice
    recognition software and human language translation, and will probably lead to
    more complex scenarios, such as natural language understanding and generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how NLP works. The first task, called the speech-to-text process, is
    to understand the natural language received. A built-in model performs speech
    recognition, which does the conversion from natural to programming language. This
    happens by breaking down speech into very small units and then comparing them
    to previous units coming speech that has been input previously. The output determines
    the words and sentences that most probably have been said. The next task, called
    **part-of-speech** (**POS**) tagging (or word-category disambiguation in some
    literature), identifies words as their grammatical forms (nouns, adjectives, verbs,
    and so on) using a set of lexicon rules. At the end of these two phases, a machine
    should understand the meaning of the input speech. A possible third task of an
    NLP process is text-to-speech conversion: at the end, the programming language
    is converted into a textual or audible format understandable by humans. That''s
    the ultimate goal of NLP: to build software that analyzes, understands, and can
    generate human languages in a natural way, making computers communicate as if
    they were humans.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a piece of text, there are three things that need to be considered and
    understood when implementing NLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic information**: The specific meaning of a single word. Consider,
    for example, the word *pole*, which could have different meanings (one end of
    a magnet, a long stick, and others). In a sentence like *extreme right and extreme
    left are the two poles of the political system,* in order to understand the correct
    meaning, it is important to know the relevant definition of pole. A reader would
    easily infer which one it is, but a machine can''t without ML or DL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Syntax information**: The phrase structure. Consider the sentence *William
    joined the football team already with long international experience*. Depending
    on how it is read, it has different meanings (it could be William or the football
    team that has long international experience).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context information**: The context where a word or a phrase appears. Consider,
    for example, the adjective *low*. It is often positive when part of a context
    of convenience (for example, *This mobile phone has a low price*), but it is almost
    always negative when talking about supplies (for example, *Supplies of drinkable
    water are running low*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following subsections will explain the main concepts of NLP supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tokenization means defining what a word is in NLP ML algorithms. Given a text,
    tokenization is the task of cutting it down into pieces, called **tokens**, while
    at the same time also removing particular characters (such as punctuation or delimiters).
    For example, given this input sentence in the English language:'
  prefs: []
  type: TYPE_NORMAL
- en: '`To be, or not to be, that is the question`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of tokenization would produce the following 11 tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '`To be or or not to be that is the question`'
  prefs: []
  type: TYPE_NORMAL
- en: 'One big challenge with tokenization is about what the correct tokens to use
    are. In the previous example, it was easy to decide: we cut down on white spaces
    and removed all the punctuation characters. But what if the input text isn''t
    in English? For some other languages, such as Chinese for example, where there
    are no white spaces, the preceding rules don''t work. So, any ML/DL model training
    for NLP should consider the specific rules for a language.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But even when limited to a single language, let''s say English, there could
    be tricky cases. Consider the following example sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '`David Anthony O''Leary is an Irish football manager and former player`'
  prefs: []
  type: TYPE_NORMAL
- en: 'How do you manage the apostrophe? There are five possible tokenizations in
    this case for `O''Leary`. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`leary`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`oleary`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`o''leary`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`o'' leary`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`o leary`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: But which one is the desired one? A simple strategy that comes quickly to mind
    could be to just split out all the non-alphanumeric characters in sentences. So,
    getting the `o` and `leary` tokens would be acceptable, because doing a Boolean
    query search with those tokens would match three cases out of five. But what about
    this following sentence?
  prefs: []
  type: TYPE_NORMAL
- en: '`Michael O''Leary has slammed striking cabin crew at Aer Lingus saying “they
    aren''t being treated like Siberian salt miners”.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'For *aren''t, *there are four possible tokenizations, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`aren''t`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`arent`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`are n''t`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`aren t`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, while the `o` and `leary` split looks fine, what about the `aren` and
    `t` split? This last one doesn't look good; a Boolean query search with those
    tokens would match two cases only out of four.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and issues with tokenization are language-specific. A deep knowledge
    of the language of the input documents is required in this context.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sentence segmentation is the process of splitting up a text into sentences.
    From its definition, it seems a straightforward process, but several difficulties
    can occur with it, for example, the presence of punctuation marks that can be
    used to indicate different things:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Streamsets Inc. has released the new Data Collector 3.5.0\. One of the new
    features is the MongoDB lookup processor.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the preceding text, you can see that the same punctuation mark (`.`)
    is used for three different things, not just as a sentence separator. Some languages,
    such as Chinese for example, come with unambiguous sentence-ending markers, while
    others don''t. So a strategy needs to be set. The quickest and dirtiest approach
    to locate the end of a sentence in a case like that in the previous example is
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: If it is a full stop, then it ends a sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the token preceding a full stop is present in a hand-precompiled list of
    abbreviations, then the full stop doesn't end the sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the next token after a full stop is capitalized, then the full stop ends
    the sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This gets more than 90% of sentences correct, but something smarter can be done,
    such as rule-based boundary disambiguation techniques (automatically learn a set
    of rules from input documents where the sentence breaks have been pre-marked),
    or better still, use a neural network (this can achieve more than 98% accuracy).
  prefs: []
  type: TYPE_NORMAL
- en: POS tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'POS tagging in NLP is the process of marking a word, depending on its definition
    and context as well, inside a text as corresponding to a particular POS. Speech
    has nine main parts—nouns, verbs, adjectives, articles, pronouns, adverbs, conjunctions,
    prepositions, and interjections. Each of them is divided into sub-classes. This
    process is more complex than tokenization and sentence segmentation. POS tagging
    can''t be generic because, depending on the context, the same word could have
    a different POS tag in sentences belonging to the same text, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Please lock the door and don''t forget the key in the lock.`'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the word `lock` is used twice with two different meanings (as a verb and
    as a noun) in the same sentence. Differences across languages should be considered
    as well. So this is a process that can't be handled manually, but it should be
    machine-based. The algorithms used can be rule-based or stochastic. Rule-based
    algorithms, in order to assign tags to unknown (or at least ambiguous) words,
    make use of contextual information. Disambiguation is achieved by analyzing different
    linguistic features of a word, such as the preceding and following words. A rule-based
    model is trained from a starting set of rules and data, and attempts to infer
    execution instructions for POS tagging. Stochastic taggers relate to different
    approaches; basically, any model that includes probability or frequency can be
    labeled this way. A simple stochastic tagger can disambiguate words based only
    on the probability a word occurs with a particular tag. More complex stochastic
    taggers are more efficient, of course. One of the most popular is the hidden Markov
    model ([https://en.wikipedia.org/wiki/Hidden_Markov_model](https://en.wikipedia.org/wiki/Hidden_Markov_model)),
    a statistical model in which the system being modeled is assumed to be a Markov
    process ([https://en.wikipedia.org/wiki/Markov_chain](https://en.wikipedia.org/wiki/Markov_chain))
    with hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: Named entity extraction (NER)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NER is the sub-task of NLP, the goal of which is to locate and classify named
    entities in a text into predefined categories. Let''s give an example. We have
    the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Guglielmo is writing a book for Packt Publishing in 2018.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'An NER process on it will produce the following annotated text:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[Guglielmo][Person] is writing a book for [Packt Publishing][Organization]
    in [2018][Time] .`'
  prefs: []
  type: TYPE_NORMAL
- en: Three entities have been detected, a person, `Guglielmo`, a two-token organization, `Packt
    Publishing`, and a temporal expression, `2018`.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, NER has been applied to structured text, but lately the number
    of use cases for unstructured text has grown.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with automating this process implementation have been case sensitivity
    (earlier algorithms often failed to recognize, for example, that Guglielmo Iozzia
    and GUGLIELMO IOZZIA are the same entity), different uses of punctuation marks,
    and missing separation characters. Implementations of NER systems use linguistic
    grammar-based techniques or statistical models and ML. Grammar-based systems can
    give better precision, but have a huge cost in terms of months of work by experienced
    linguists, plus they have low recall. ML-based systems have a high recall, but
    require a large amount of manually annotated data for their training. Unsupervised
    approaches are coming through order to drastically reduce the effort of data annotation.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge for this process is the context domain—several studies have
    demonstrated that NER systems developed for one domain (reaching high performance
    on it) typically don't perform well the same way in other domains. An NER system
    that has been trained in a Twitter content, for example, can't be applied, expecting
    the same performance and accuracy, to medical records. This applies for both rule-based
    and statistical/ML systems; a considerable effort is needed when tuning NER systems
    in a new domain in order for them to reach the same level of performance they
    had in the original domain where they were successfully trained.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chunking in NLP is the process of extracting phrases from text. It is used because
    simple tokens may not represent the real meaning of the text under examination.
    As an example, consider the phrase *Great Britain*; while the two separate words
    make sense, it is more advisable to use *Great Britain* as a single word. Chunking
    works on top of POS tagging; typically POS tags are input, while chunks are the
    output from it. This process is very similar to the way the human brain chunks
    information together to make it easier to process and understand. Think about
    the way you memorize, for example, sequences of numbers (such as debit card pins,
    telephone numbers, and others); you don't tend to memorize them as individual
    numbers, but try to group them together in a way that makes them easier to remember.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunking can be up or down. Chunking up tends more to abstraction; chunking
    down tends to look for more specific details. As an example, consider the following
    scenario in a call with a ticket sales and distribution company. The operator
    asks the question "*which kind of tickets would you like to purchase?*" The customer''s
    answer, "*Concert tickets*", is chunking up, because it moves towards a higher
    level of abstraction. Then, the operator asks more questions, such as "*which
    genre*," "*which artist or group*," "*for which dates and locations*,", "*for
    how many people*,", "*which sector*,", and so on, in order to get more details
    and fulfill the customer''s needs (this is chunking down). At the end, you can
    think about chunking as a hierarchy of sets. For a given context, there is always
    a higher-level set, which has subsets, and each subset can have other subsets.
    For example, consider a programming language as a higher-level subset; you can
    then have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Programming Language`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Scala (Subset of Programming Language)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Scala 2.11 (Subset of Scala)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Trait (a specific concept of Scala)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Iterator (a core Scala trait)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parsing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Parsing in NLP is the process of determining the syntactic structure of a text.
    It works by analyzing the text constituent words and it bases itself on the underlying
    grammar of the specific language in which the text has been written. The outcome
    of parsing is a parse tree of each sentence part of the input text. A parse tree
    is an ordered, rooted tree that represents the syntactic structure of a sentence
    according to some context-free grammar (a set of rules that describe all the possible
    strings in a given formal language). Let''s make an example. Consider the English
    language and the following example grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sentence -> noun-phrase, verb-phrase`'
  prefs: []
  type: TYPE_NORMAL
- en: '`noun-phrase -> proper-noun`'
  prefs: []
  type: TYPE_NORMAL
- en: '`noun-phrase -> determiner, noun`'
  prefs: []
  type: TYPE_NORMAL
- en: '`verb-phrase -> verb, noun-phrase`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the phrase `Guglielmo wrote a book` and apply the parsing process
    to it. The output would be a parse tree like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22e7c67c-532a-4dd6-839b-9ffe6f8fb299.png)'
  prefs: []
  type: TYPE_IMG
- en: Currently, the approaches to automated machine-based parsing are statistical,
    probabilistic, or ML.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on NLP with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, some examples of the implementation of NLP (and its core concepts
    as described in the previous sections) with Apache Spark are going to be detailed.
    These examples don't include DL4J or other DL frameworks, as NLP with multilayer
    neural networks will be the main topic of the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: While one of the core components of Spark, MLLib, is an ML library, it doesn't
    provide any facility for NLP. So, you need to use some other NLP library or framework
    on top of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on NLP with Spark and Stanford core NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first example covered in this chapter involves a Scala Spark wrapper of
    the Stanford core NLP ([https://github.com/stanfordnlp/CoreNLP](https://github.com/stanfordnlp/CoreNLP))
    library, which is open source and released with the GNU general public licence
    v3 ([https://www.gnu.org/licenses/gpl-3.0.en.html](https://www.gnu.org/licenses/gpl-3.0.en.html)).
    It is a Java library that provides a set of natural language analysis tools. Its
    basic distribution provides model files for the analysis of English, but the engine
    is compatible with models for other languages as well. It is stable and ready
    for production, and widely used across different areas of academic and industry.
    Spark CoreNLP ([https://github.com/databricks/spark-corenlp](https://github.com/databricks/spark-corenlp))
    is a wrapper of the Stanford Core NLP Java library for Apache Spark. It has been
    implemented in Scala. The Stanford Core NLP annotators have been wrapped as Spark
    DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'The spark-corenlp library''s current release contains a single Scala class
    function, which provides all of the high-level wrapper methods, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cleanXml`: Takes as input an XML document and removes all the XML tags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenize`: Tokenizes the input sentence into words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssplit`: Splits its input document into sentences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pos`: Generates the POS tags of its input sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lemma`: Generates the word lemmas of its input sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ner`: Generates the named entity tags of its input sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`depparse`: Generates the semantic dependencies of its input sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`coref`: Generates the `coref` chains of its input document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`natlog`: Generates the natural logic notion of polarity for each token in
    its input sentence. The possible returned values are up, down, or flat.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openie`: Generates a list of open IE triples as flat quadruples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sentiment`: Measures the sentiment of its input sentence. The scale goes from
    zero (strongly negative) to four (strongly positive).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first thing to do is to set up the dependencies for this example. It has
    dependencies on Spark SQL and the Stanford core NLP 3.8.0 (the importing of the
    models needs to be explicitly specified through the `Models` classifier), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When you need to work with a single language, let''s say Spanish, you can also
    choose to import the models for that language only, through its specific classifier,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: No library is available for `spark-corenlp` on Maven central. So, you have to
    build its JAR file starting from the GitHub source code, and then add it to the
    classpath of your NLP application or, if your applications rely on an artifact
    repository, such as JFrog Artifactory ([https://jfrog.com/artifactory/](https://jfrog.com/artifactory/)),
    Apache Archiva ([https://archiva.apache.org/index.cgi](https://archiva.apache.org/index.cgi)),
    or Sonatype Nexus OSS ([https://www.sonatype.com/nexus-repository-oss](https://www.sonatype.com/nexus-repository-oss)),
    store the JAR file there and then add the dependency to it in your project build
    file the same way as for any other dependency available in Maven central.
  prefs: []
  type: TYPE_NORMAL
- en: 'I previously mentioned that `spark-corenlp` wraps Stanford core NLP annotators
    as DataFrames. So, the first thing to do in the source code is to create a `SparkSession`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Create now a `Sequence` ([https://www.scala-lang.org/api/current/scala/collection/Seq.html](https://www.scala-lang.org/api/current/scala/collection/Seq.html))
    for the input text content (in XML format) and then transform it into a DataFrame,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Given this input, we could do different NLP operations using the `functions`
    available methods, such as cleaning from the tags the input XML included in the
    `text` field of the `input` DataFrame, splitting each sentence into single words,
    generating the named entity tags for each sentence, and measuring the sentiment
    of each sentence, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end, we print the output of those operations (`output` itself is a DataFrame),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to stop and destroy the `SparkSession`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing this example, the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3407710-8b0e-48b1-8834-ea8e69520571.png)'
  prefs: []
  type: TYPE_IMG
- en: The XML content has been cleared from the tags, the sentences have been split
    into their single words as expected, and for some words (`Birmingham`, `Mumbai`)
    a named entity tag (`LOCATION`) has been generated. And, the sentiment is positive
    (`3`) for both input sentences!
  prefs: []
  type: TYPE_NORMAL
- en: This approach is a good way to start with NLP in Scala and Spark; the API provided
    by this library is simple and high level, and gives time to people to assimilate
    the core NLP concepts quickly, while leveraging the Spark DataFrames capabilities.
    But there are downsides with it. Where there is a need to implement more complex
    and custom NLP solutions, the available API is too simple to tackle them. Also
    a licensing problem could occur if your final system isn't for internal purposes
    only, but your company plan is to sell and distribute your solution to customers;
    the Stanford core NLP library and `spark-corenlp` models depend on, and are released
    under, the full GNU GPL v3 license, which forbids redistributing it as part of
    proprietary software. The next section presents a more viable alternative for
    Scala and Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on NLP with Spark NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another alternative library to integrate with Apache Spark in order to do NLP
    is `spark-nlp` ([https://nlp.johnsnowlabs.com/](https://nlp.johnsnowlabs.com/))
    by the John Snow labs ([https://www.johnsnowlabs.com/](https://www.johnsnowlabs.com/)).
    It is open source and released under Apache License 2.0, so is different from
    `spark-corenlp` because its licensing model makes it possible to redistribute
    it as part of a commercial solution. It has been implemented in Scala on top of
    the Apache Spark ML module and it is available in Maven central. It provides NLP
    annotations for machine learning pipelines that at the same time are easy to understand
    and use, have great performance, and can scale easily in a distributed environment.
  prefs: []
  type: TYPE_NORMAL
- en: The release I am referring to in this section is 1.6.3 (the latest at the time
    this book was written).
  prefs: []
  type: TYPE_NORMAL
- en: 'The core concept of `spark-nlp` is the Spark ML pipeline ([https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Pipeline.html](https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Pipeline.html)). A
    pipeline consists of a sequence of stages. Each stage could be a transformer ([https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Transformer.html](https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Transformer.html))
    or an estimator ([https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Estimator.html](https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/ml/Estimator.html)).
    Transformers transform an input dataset into another, while estimators fit a model
    to data. When the fit method of a pipeline is invoked, its stages are then executed
    in order. Three types of pre-trained pipelines are available: the basic, advanced,
    and sentiment. The library provides also several pre-trained models for NLP and
    several annotators. But, let''s start with a simple first example in order to
    clarify the details of the main concepts of `spark-nlp`. Let''s try to implement
    a basic pipeline for ML-based named entity tag extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example depends on the Spark SQL and MLLib components and the
    `spark-nlp` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to start a `SparkSession` before anything else, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Before creating the pipeline, we need to implement its single stages. The first
    one is `com.johnsnowlabs.nlp.DocumentAssembler`, to specify the column of the
    application input to parse and the output column name (which will be the input
    column for the next stage), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The next stage is a `Tokenizer` (`com.johnsnowlabs.nlp.annotators.Tokenizer`),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After this stage, any input sentences should have been split into single words.
    We need to clean up those tokens, so the next stage is a `normalizer` (`com.johnsnowlabs.nlp.annotators.Normalizer`),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use one of the pre-trained models of the `spark-nlp` library to
    generate the named entity tags, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we are using the `NerDLModel` class (`com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel`),
    which behind the scenes uses a TensorFlow pre-trained model. The named entity
    tags generated by that model are in IOB format ([https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))),
    so we need to make them in a more human readable format. We can achieve this using
    the `NerConverter` class (`com.johnsnowlabs.nlp.annotators.ner.NerConverter`),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The last stage is to finalize the output of the pipeline, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: For this, we use the `Finisher` transformer (`com.johnsnowlabs.nlp.Finisher`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now create the pipeline using the stages created so far, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You have already probably noticed that each stage's output column is the input
    for the next stage's input column. That's because the stages for a pipeline are
    executed in the exact order they are listed in the input `Array` for the `setStages`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now feed the application with some sentences, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The same as for the `spark-corenlp` example in the previous section, we have
    created a `Sequence` for the input text content and then transformed it into a
    Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'By invoking the `fit` method of the `pipeline`, we can execute all of its stages,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'And, we get the resulting DataFrame output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b167214a-e7d3-4fe5-9c23-5ea43298325b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we take a closer look it is seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a93289e-b733-44a7-af1d-657fb05d34e1.png)'
  prefs: []
  type: TYPE_IMG
- en: An `ORGANIZATION` named entity tag has been generated for the word `Packt` and
    a `PERSON` named entity tag has been generated for the word `Guglielmo`.
  prefs: []
  type: TYPE_NORMAL
- en: '`spark-nlp` also provides a class, `com.johnsnowlabs.util.Benchmark`, to perform
    the benchmarking of pipeline execution, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we stop the `SparkSession` at the end of the pipeline execution, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let's now do something more complex. The pipeline in this second example does
    tokenization with n-grams ([https://en.wikipedia.org/wiki/N-gram](https://en.wikipedia.org/wiki/N-gram)),
    sequences of *n* tokens (typically words) from a given text or speech. The dependencies
    for this example are the same as for the previous one presented in this section—Spark
    SQL, Spark MLLib, and `spark-nlp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `SparkSession` and configure some Spark properties, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The first three stages for the pipeline are the same as for the previous example,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Put a `finisher` stage before using the n-gram stage, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The n-gram stage uses the `NGram` class ([https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.ml.feature.NGram](https://spark.apache.org/docs/2.2.1/api/scala/index.html#org.apache.spark.ml.feature.NGram))
    of Spark MLLib, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`NGram` is a feature transformer that converts an input array of strings into
    an array of n-grams. The chosen value for *n* in this example is `3`. We need
    now a further `DocumentAssembler` stage for the n-gram results, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s implement the pipeline, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now feed the application with the same input sentences as for the previous
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'And execute the stages of the pipeline, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the results to the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ab368f7-ea37-4f79-9492-4665a7b935c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we stop the `SparkSession`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The final example is an ML sentiment analysis using the Vivek Narayanan ([https://github.com/vivekn](https://github.com/vivekn))
    model. Sentiment analysis, which is a practical application of NLP, is the process
    of identifying and categorizing, through a computer, the opinion expressed in
    a text, in order to determine whether its writer's/speaker's attitude towards
    a product or topic is positive, negative, or neutral. In particular, for this
    example, we are going to train and validate the model on movie reviews. The dependencies
    for this example are the usual ones—Spark SQL, Spark MLLib, and `spark-nlp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, create a `SparkSession` (while also configuring some Spark properties),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We then need two datasets, one for training and one for testing. For simplicity,
    we define the training dataset as a `Sequence` and then transform it into a DataFrame,
    where the columns are the review text and the associated sentiment, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now define the stages for the pipeline. The first three stages are exactly
    the same as for the previous example pipelines (`DocumentAssembler`, `Tokenizer`, and
    `Normalizer`), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use the `com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentApproach`
    annotator, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the pipeline using the stages previously defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'And then start the training, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training has completed, we can use the following testing dataset to
    test it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7332a1d8-0bb5-490a-b091-870b2c270648.png)'
  prefs: []
  type: TYPE_IMG
- en: Two sentences that are part of the testing dataset have been properly marked
    as negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is of course possible to do a benchmark for sentiment analysis, too, through
    the `spark-nlp` `Benchmark` class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: At the end of this section, we can state that `spark-nlp` provides more features
    than `spark-corenlp`*,* is well integrated with Spark MLLib, and, thanks to its
    licensing model, doesn't present the same issues concerning the distribution of
    the application/system in which it is used and integrated. It is a stable library
    and ready for production in Spark environments. Unfortunately, most of its documentation
    is missing and the existing documentation is minimal and not well maintained,
    despite project development being active.
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand how a single feature works and how to combine them together,
    you have to go through the source code in GitHub. The library also uses existing
    ML models implemented through Python frameworks and provides a Scala class to
    represent them, hiding the underlying model implementation details from developers.
    This will work in several use case scenarios, but in order to build more robust
    and efficient models, you would probably have to move to implementing your own
    neural network model. Only DL4J will give you that level of freedom in development
    and training with Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we became familiar with the main concepts of NLP and started
    to get hands-on with Spark, exploring two potentially useful libraries, `spark-corenlp` and
    `spark-nlp`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how it is possible to achieve the same or better
    results by implementing complex NLP scenarios in Spark though DL (mostly RNN-based).
    We will explore different implementations by using DL4J, TensorFlow, Keras, the
    TensorFlow backend, and DL4J + Keras model imports.
  prefs: []
  type: TYPE_NORMAL
