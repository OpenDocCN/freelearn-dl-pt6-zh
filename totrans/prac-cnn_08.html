<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">GAN: Generating New Images with CNN</h1>
                </header>
            
            <article>
                
<p class="mce-root">Generally, a neural network needs labeled examples to learn effectively. Unsupervised learning approaches to learn from unlabeled data have not worked very well. A <strong>generative adversarial network</strong>, or simply a <strong>GAN</strong>, is part of an unsupervised learning approach but based on differentiable generator networks. GANs were first invented by Ian Goodfellow and others in 2014. Since then they have become extremely popular. This is based on game theory and has two players or networks: a generator network and b) a discriminator network, both competing against each other. This dual network game theory-based approach vastly improved the process of learning from unlabeled data. The generator network produces fake data and passes it to a discriminator. The discriminator network also sees real data and predicts whether the data it receives is fake or real. So, the generator is trained so that it can easily produce data that is very close to real data in order to fool the discriminator network. The discrimin<span>ator network is trained to classify which data is real and which data is fake. So, eventually, a generator network learns to produce data that is very, very close to real data. GANs are going to be widely popular in the music and arts domains.</span></p>
<div class="packt_tip"><span>According to </span><span>Goodfellow, "<em>You can think of generative models as giving Artificial Intelligence a form of imagination</em>."</span></div>
<p class="mce-root"><span>The following are a couple of examples of GANs:</span></p>
<ul>
<li>Pix2pix</li>
<li>CycleGAN</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pix2pix - Image-to-Image translation GAN</h1>
                </header>
            
            <article>
                
<p>This network uses a <strong>conditional generative adversarial network</strong> (<strong>cGAN</strong>) to learn mapping from the input and output of an image. Some of the examples that can be done from the original paper are as follows:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="475" src="assets/5bd742bd-4360-4fb0-acaf-595d92808a92.jpeg" width="450"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"> Pix2pix examples of cGANs </div>
<p>In the handbags example, the network learns how to color a black and white image. Here, the training dataset has the input image in black and white and the target image is the color version.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CycleGAN </h1>
                </header>
            
            <article>
                
<p>CycleGAN is also an image-to-image translator but without input/output pairs. For example, to generate photos from paintings, convert a horse image into a zebra image:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/60b9b3b9-a03d-4a53-acbf-49563705690e.jpeg"/></div>
<div class="packt_tip"><span>In a discriminator network, use of dropout is important. Otherwise, it may produce a poor result.</span></div>
<p>The generator network takes random noise as input and produces a realistic image as output. Running a generator network for different kinds of random noise produces different types of realistic images. The second network, which is known as the <strong>discriminator network</strong>, is very similar to a regular neural net classifier. This network is trained on real images, although training a GAN is quite different from a supervised training method. In supervised training, each image is labeled first before being shown to the model. For example, if the input is a dog image, we tell the model this is a dog. In case of a generative model, we show the model a lot of images and ask it to make more such similar images from the same probability distribution. Actually, the second discriminator network helps the generator network to achieve this.</p>
<p class="mce-root">The discriminator outputs the probability that the image is real or fake from the generator network. In other words, it tries to assign a probability close to 1 for a real image and a probability close to 0 for fake images. Meanwhile, the generator does the opposite. It is trained to output images that will have a probability close to 1 by the discriminator. Over time, the generator produces more realistic images and fools the discriminator:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="349" src="assets/31c3ec0c-b8e9-407f-9e05-9764f2b33178.png" width="543"/></div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a GAN model</h1>
                </header>
            
            <article>
                
<p>Most machine learning models explained in earlier chapters are based on optimization, that is, we minimize the cost function over its parameter space. GANs are different because of two networks: the generator G and the discriminator D. Each has its own cost. An easy way to visualize GAN is the cost of the discriminator is the negative of the cost of the generator. In GAN, we can define a value function that the generator has to minimize and the discriminator has to maximize. The training process for a generative model is quite different from the supervised training method. GAN is sensitive to the initial weights. So we need to use batch normalization. B<span>atch normalization</span> makes the model stable, besides improving performance. Here, we train two models, the generative model and the discriminative model, simultaneously. Generative model G captures data distribution and discriminative model D estimates the probability of a sample that came from training data rather than G.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">GAN – code example</h1>
                </header>
            
            <article>
                
<p>In the following example, we build and train a GAN model using an MNIST dataset and using TensorFlow. Here, we will use a special version of the ReLU activation function known as <span><strong>Leaky ReLU</strong>. The output is a new type of handwritten digit:</span></p>
<div class="packt_tip"><span>Leaky ReLU is a variation of the ReLU activation function given by the formula <span class="math"><span class="mrow"><em><span class="mi">f</span><span class="mo">(</span><span class="mi">x</span><span class="mo">) </span><span class="mo">= </span><span class="mi">m</span><span class="mi">a</span><span class="mi">x</span><span class="mo">(</span><span class="mi">α</span><span class="mo">∗</span><span class="mi">x</span><span class="mo">, </span><span class="mi">x</span></em><span class="mo"><em>)</em>. So the output for the negative value for <em>x</em> is <em>alpha * x </em>and the output for positive <em>x</em> is <em>x</em>.</span></span></span></span></div>
<pre>#import all necessary libraries and load data set<br/>%matplotlib inline<br/><br/>import pickle as pkl<br/>import numpy as np<br/>import tensorflow as tf<br/>import matplotlib.pyplot as plt<br/><br/>from tensorflow.examples.tutorials.mnist import input_data<br/>mnist = input_data.read_data_sets('MNIST_data')</pre>
<p>In order to build this network, we need two inputs, one for the generator and one for the discriminator. In the following code, we create placeholders for <kbd>real_input</kbd> for the discriminator and <kbd>z_input</kbd> for the generator, with the input sizes as <kbd>dim_real</kbd> and <kbd>dim_z</kbd>, respectively:</p>
<pre>#place holder for model inputs <br/>def model_inputs(dim_real, dim_z):<br/>    real_input = tf.placeholder(tf.float32, name='dim_real')<br/>    z_input = tf.placeholder(tf.float32, name='dim_z')<br/>    <br/>    return real_input, z_input</pre>
<p><span>Here, input <kbd>z</kbd> is a random vector to the generator which turns this vector into an image. Then we add a hidden layer, which is a leaky ReLU layer, to allow gradients to flow backwards. Leaky ReLU is just like a normal ReLU (for negative values emitting zero) except that there is a small non-zero output for negative input values. The generator performs better with the <kbd>tanh</kbd><kbd>sigmoid</kbd> function. Generator output is <kbd>tanh</kbd> output. So, we'll have to rescale the MNIST images to be between -1 and 1, instead of 0 and 1. With this knowledge, we can build the generator network:</span></p>
<pre>#Following code builds Generator Network<br/>def generator(z, out_dim, n_units=128, reuse=False, alpha=0.01):<br/>    ''' Build the generator network.<br/>    <br/>        Arguments<br/>        ---------<br/>        z : Input tensor for the generator<br/>        out_dim : Shape of the generator output<br/>        n_units : Number of units in hidden layer<br/>        reuse : Reuse the variables with tf.variable_scope<br/>        alpha : leak parameter for leaky ReLU<br/>        <br/>        Returns<br/>        -------<br/>        out: <br/>    '''<br/>    with tf.variable_scope('generator', reuse=reuse) as generator_scope: # finish this<br/>        # Hidden layer<br/>        h1 = tf.layers.dense(z, n_units, activation=None )<br/>        # Leaky ReLU<br/>        h1 = tf.nn.leaky_relu(h1, alpha=alpha,name='leaky_generator')<br/>        <br/>        # Logits and tanh output<br/>        logits = tf.layers.dense(h1, out_dim, activation=None)<br/>        out = tf.tanh(logits)<br/>        <br/>        return out</pre>
<p>The discriminator network is the same as the generator except that output layer is a <kbd>sigmoid</kbd> function:</p>
<pre><br/>def discriminator(x, n_units=128, reuse=False, alpha=0.01):<br/>    ''' Build the discriminator network.<br/>    <br/>        Arguments<br/>        ---------<br/>        x : Input tensor for the discriminator<br/>        n_units: Number of units in hidden layer<br/>        reuse : Reuse the variables with tf.variable_scope<br/>        alpha : leak parameter for leaky ReLU<br/>        <br/>        Returns<br/>        -------<br/>        out, logits: <br/>    '''<br/>    with tf.variable_scope('discriminator', reuse=reuse) as discriminator_scope:# finish this<br/>        # Hidden layer<br/>        h1 = tf.layers.dense(x, n_units, activation=None )<br/>        # Leaky ReLU<br/>        h1 = tf.nn.leaky_relu(h1, alpha=alpha,name='leaky_discriminator')<br/>        <br/>        logits = tf.layers.dense(h1, 1, activation=None)<br/>        out = tf.sigmoid(logits)<br/>        <br/>        return out, logits</pre>
<p class="mce-root"><span>To build the network, use the following code:</span></p>
<pre class="mce-root">#Hyperparameters<br/># Size of input image to discriminator<br/>input_size = 784 # 28x28 MNIST images flattened<br/># Size of latent vector to generator<br/>z_size = 100<br/># Sizes of hidden layers in generator and discriminator<br/>g_hidden_size = 128<br/>d_hidden_size = 128<br/># Leak factor for leaky ReLU<br/>alpha = 0.01<br/># Label smoothing <br/>smooth = 0.1</pre>
<p><span>We want to share weights between real and fake data, so we need to reuse the variables:</span></p>
<pre>#Build the network<br/>tf.reset_default_graph()<br/># Create our input placeholders<br/>input_real, input_z = model_inputs(input_size, z_size)<br/><br/># Build the model<br/>g_model = generator(input_z, input_size, n_units=g_hidden_size, alpha=alpha)<br/># g_model is the generator output<br/><br/>d_model_real, d_logits_real = discriminator(input_real, n_units=d_hidden_size, alpha=alpha)<br/>d_model_fake, d_logits_fake = discriminator(g_model, reuse=True, n_units=d_hidden_size, alpha=alpha)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating loss </h1>
                </header>
            
            <article>
                
<p><span>For the discriminator, the total loss is the sum of the losses for real and fake images. The losses will be sigmoid cross-entropyies, which we can get using the TensorFlow <kbd>tf.nn.sigmoid_cross_entropy_with_logits</kbd>. Then we compute the mean for all the images in the batch. So the losses will look like this:</span></p>
<pre>tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))</pre>
<p>To help the discriminator generalize better, the <kbd>labels</kbd> can be reduced a bit from 1.0 to 0.9, by for example, using the parameter <kbd>smooth</kbd><em>. </em>This is known as <strong>label smoothing</strong>, and is typically used with classifiers to improve performance. <span>The discriminator loss for the fake data is similar. The <kbd>logits</kbd> are <kbd>d_logits_fake</kbd></span><span>, which we got from passing the generator output to the discriminator. These fake <kbd>logits</kbd> are used with <kbd>labels</kbd> of all zeros. Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.</span></p>
<p><span>Finally, the generator losses are using <kbd>d_logits_fake</kbd><em>, </em>the fake image <kbd>logits</kbd>. But now the <kbd>labels</kbd> are all 1s. The generator is trying to fool the discriminator, so it wants the discriminator to output ones for fake images:</span></p>
<pre># Calculate losses<br/>d_loss_real = tf.reduce_mean(<br/>                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, <br/>                                                          labels=tf.ones_like(d_logits_real) * (1 - smooth)))<br/>d_loss_fake = tf.reduce_mean(<br/>                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, <br/>                                                          labels=tf.zeros_like(d_logits_real)))<br/>d_loss = d_loss_real + d_loss_fake<br/><br/>g_loss = tf.reduce_mean(<br/>             tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,<br/>                                                     labels=tf.ones_like(d_logits_fake)))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding the optimizer</h1>
                </header>
            
            <article>
                
<p><span>We need to update the generator and discriminator variables separately. So, first get all the variables of the graph and then, as we explained earlier, we can get only generator variables from the generator scope and, similarly, discriminator variables from the discriminator scope:</span></p>
<pre># Optimizers<br/>learning_rate = 0.002<br/><br/># Get the trainable_variables, split into G and D parts<br/>t_vars = tf.trainable_variables()<br/>g_vars = [var for var in t_vars if var.name.startswith('generator')]<br/>d_vars = [var for var in t_vars if var.name.startswith('discriminator')]<br/><br/>d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)<br/>g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)</pre>
<p><span>To train the network, use:</span></p>
<pre>batch_size = 100<br/>epochs = 100<br/>samples = []<br/>losses = []<br/># Only save generator variables<br/>saver = tf.train.Saver(var_list=g_vars)<br/>with tf.Session() as sess:<br/>    sess.run(tf.global_variables_initializer())<br/>    for e in range(epochs):<br/>        for ii in range(mnist.train.num_examples//batch_size):<br/>            batch = mnist.train.next_batch(batch_size)<br/>            <br/>            # Get images, reshape and rescale to pass to D<br/>            batch_images = batch[0].reshape((batch_size, 784))<br/>            batch_images = batch_images*2 - 1<br/>            <br/>            # Sample random noise for G<br/>            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))<br/>            <br/>            # Run optimizers<br/>            _ = sess.run(d_train_opt, feed_dict={input_real: batch_images, input_z: batch_z})<br/>            _ = sess.run(g_train_opt, feed_dict={input_z: batch_z})<br/>        <br/>        # At the end of each epoch, get the losses and print them out<br/>        train_loss_d = sess.run(d_loss, {input_z: batch_z, input_real: batch_images})<br/>        train_loss_g = g_loss.eval({input_z: batch_z})<br/>            <br/>        print("Epoch {}/{}...".format(e+1, epochs),<br/>              "Discriminator Loss: {:.4f}...".format(train_loss_d),<br/>              "Generator Loss: {:.4f}".format(train_loss_g)) <br/>        # Save losses to view after training<br/>        losses.append((train_loss_d, train_loss_g))<br/>        <br/>        # Sample from generator as we're training for viewing afterwards<br/>        sample_z = np.random.uniform(-1, 1, size=(16, z_size))<br/>        gen_samples = sess.run(<br/>                       generator(input_z, input_size, n_units=g_hidden_size, reuse=True, alpha=alpha),<br/>                       feed_dict={input_z: sample_z})<br/>        samples.append(gen_samples)<br/>        saver.save(sess, './checkpoints/generator.ckpt')<br/><br/># Save training generator samples<br/>with open('train_samples.pkl', 'wb') as f:<br/>    pkl.dump(samples, f)</pre>
<p>Once the model is trained and saved, you can visualize the generated digits (the code is not here, but it can be downloaded). </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Semi-supervised learning and GAN</h1>
                </header>
            
            <article>
                
<p>So for, we have seen how GAN can be used to generate realistic images. In this section, we will see how GAN can be used for classification tasks where we have less labeled data but still <span>want to improve the accuracy</span> of the classifier. Here we will also use the same <strong>Street View House Number</strong> or <strong>SVHN</strong> dataset to classify images. As previously, here we also have two networks, the generator G and discriminator D. In this case, the discriminator is trained to become a classifier. Another change is that the output of the discriminator goes to a softmax function instead of a <kbd>sigmoid</kbd> function, as seen earlier. The softmax function returns the probability distribution over labels:</p>
<div class="CDPAlignCenter CDPAlign"><img height="207" src="assets/8f617d78-0c02-4e22-9e9c-1e99923915dc.png" width="546"/></div>
<p>Now we model the network as:</p>
<p class="CDPAlignCenter CDPAlign"><em>total cost = cost of labeled data + cost of unlabeled data</em></p>
<p>To get the cost of labeled data, we can use the <kbd>cross_entropy</kbd> function:</p>
<pre><span>cost of labeled data  = cross_entropy ( logits, labels)<br/></span><span>cost of unlabeled data = </span>  <span>cross_entropy ( logits, real)</span></pre>
<p>Then we can <span>calculate the sum </span>of all classes:</p>
<pre>real prob = sum (softmax(real_classes))</pre>
<p>Normal classifiers work on labeled data. However, semi-supervised GAN-based classifiers work on labeled data, real unlabeled data, and fake images. This works very well, that is, there are less classification errors even though we have less labeled data in the training process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature matching</h1>
                </header>
            
            <article>
                
<p>The idea of feature matching is to add an extra variable to the cost function of the generator in order to penalize the difference between absolute errors in the test data and training data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Semi-supervised classification using a GAN example</h1>
                </header>
            
            <article>
                
<p>In this section, we explain how to use GAN to build a classifier with the semi-supervised learning approach.</p>
<p>In supervised learning, we have a training set of inputs<span> <kbd>X</kbd></span><span> </span>and class labels<span> </span><kbd><span class="MathJax"><span class="MJX_Assistive_MathML">y</span></span></kbd>. We train a model that takes<span> <kbd>X</kbd></span> as input and gives<span> </span><kbd><span class="MathJax"><span class="MJX_Assistive_MathML">y</span></span></kbd><span> </span>as output.</p>
<p>In semi-supervised learning, our goal is still to train a model that takes<span> <kbd>X</kbd> </span>as input and generates<span> </span><kbd><span class="MathJax"><span class="MJX_Assistive_MathML">y</span></span></kbd><span> </span>as output. However, not all of our training examples have a label<span> </span><kbd><span class="MathJax"><span class="math"><span><span class="mrow"><span class="mi">y</span></span></span></span></span></kbd>. </p>
<p>We use the SVHN dataset. We'll turn the GAN discriminator into an 11 class <span>discriminator</span> (0 to 9 and one label for the fake image). It will recognize the 10 different classes of real SVHN digits, as well as an eleventh class of fake images that come from the generator. The discriminator will get to train on real labeled images, real unlabeled images, and fake images. By drawing on three sources of data instead of just one, it will generalize to the test set much better than a traditional classifier trained on only one source of data:</p>
<pre>def model_inputs(real_dim, z_dim):<br/>    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='input_real')<br/>    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')<br/>    y = tf.placeholder(tf.int32, (None), name='y')<br/>    label_mask = tf.placeholder(tf.int32, (None), name='label_mask')<br/>    <br/>    return inputs_real, inputs_z, y, label_mask</pre>
<p>Add the generator:</p>
<pre>def generator(z, output_dim, reuse=False, alpha=0.2, training=True, size_mult=128):<br/>    with tf.variable_scope('generator', reuse=reuse):<br/>        # First fully connected layer<br/>        x1 = tf.layers.dense(z, 4 * 4 * size_mult * 4)<br/>        # Reshape it to start the convolutional stack<br/>        x1 = tf.reshape(x1, (-1, 4, 4, size_mult * 4))<br/>        x1 = tf.layers.batch_normalization(x1, training=training)<br/>        x1 = tf.maximum(alpha * x1, x1)<br/>        <br/>        x2 = tf.layers.conv2d_transpose(x1, size_mult * 2, 5, strides=2, padding='same')<br/>        x2 = tf.layers.batch_normalization(x2, training=training)<br/>        x2 = tf.maximum(alpha * x2, x2)<br/>        <br/>        x3 = tf.layers.conv2d_transpose(x2, size_mult, 5, strides=2, padding='same')<br/>        x3 = tf.layers.batch_normalization(x3, training=training)<br/>        x3 = tf.maximum(alpha * x3, x3)<br/>        <br/>        # Output layer<br/>        logits = tf.layers.conv2d_transpose(x3, output_dim, 5, strides=2, padding='same')<br/>        <br/>        out = tf.tanh(logits)<br/>        <br/>        return out</pre>
<p>Add the discriminator:</p>
<pre>def discriminator(x, reuse=False, alpha=0.2, drop_rate=0., num_classes=10, size_mult=64):<br/>    with tf.variable_scope('discriminator', reuse=reuse):<br/>        x = tf.layers.dropout(x, rate=drop_rate/2.5)<br/>        <br/>        # Input layer is 32x32x3<br/>        x1 = tf.layers.conv2d(x, size_mult, 3, strides=2, padding='same')<br/>        relu1 = tf.maximum(alpha * x1, x1)<br/>        relu1 = tf.layers.dropout(relu1, rate=drop_rate)<br/>        <br/>        x2 = tf.layers.conv2d(relu1, size_mult, 3, strides=2, padding='same')<br/>        bn2 = tf.layers.batch_normalization(x2, training=True)<br/>        relu2 = tf.maximum(alpha * x2, x2)<br/>        <br/>        <br/>        x3 = tf.layers.conv2d(relu2, size_mult, 3, strides=2, padding='same')<br/>        bn3 = tf.layers.batch_normalization(x3, training=True)<br/>        relu3 = tf.maximum(alpha * bn3, bn3)<br/>        relu3 = tf.layers.dropout(relu3, rate=drop_rate)<br/>        <br/>        x4 = tf.layers.conv2d(relu3, 2 * size_mult, 3, strides=1, padding='same')<br/>        bn4 = tf.layers.batch_normalization(x4, training=True)<br/>        relu4 = tf.maximum(alpha * bn4, bn4)<br/>        <br/>        x5 = tf.layers.conv2d(relu4, 2 * size_mult, 3, strides=1, padding='same')<br/>        bn5 = tf.layers.batch_normalization(x5, training=True)<br/>        relu5 = tf.maximum(alpha * bn5, bn5)<br/>        <br/>        x6 = tf.layers.conv2d(relu5, 2 * size_mult, 3, strides=2, padding='same')<br/>        bn6 = tf.layers.batch_normalization(x6, training=True)<br/>        relu6 = tf.maximum(alpha * bn6, bn6)<br/>        relu6 = tf.layers.dropout(relu6, rate=drop_rate)<br/>        <br/>        x7 = tf.layers.conv2d(relu5, 2 * size_mult, 3, strides=1, padding='valid')<br/>        # Don't use bn on this layer, because bn would set the mean of each feature<br/>        # to the bn mu parameter.<br/>        # This layer is used for the feature matching loss, which only works if<br/>        # the means can be different when the discriminator is run on the data than<br/>        # when the discriminator is run on the generator samples.<br/>        relu7 = tf.maximum(alpha * x7, x7)<br/>        <br/>        # Flatten it by global average pooling<br/>        features = raise NotImplementedError()<br/>        <br/>        # Set class_logits to be the inputs to a softmax distribution over the different classes<br/>        raise NotImplementedError()<br/>        <br/>        <br/>        # Set gan_logits such that P(input is real | input) = sigmoid(gan_logits).<br/>        # Keep in mind that class_logits gives you the probability distribution over all the real<br/>        # classes and the fake class. You need to work out how to transform this multiclass softmax<br/>        # distribution into a binary real-vs-fake decision that can be described with a sigmoid.<br/>        # Numerical stability is very important.<br/>        # You'll probably need to use this numerical stability trick:<br/>        # log sum_i exp a_i = m + log sum_i exp(a_i - m).<br/>        # This is numerically stable when m = max_i a_i.<br/>        # (It helps to think about what goes wrong when...<br/>        # 1. One value of a_i is very large<br/>        # 2. All the values of a_i are very negative<br/>        # This trick and this value of m fix both those cases, but the naive implementation and<br/>        # other values of m encounter various problems)<br/>        raise NotImplementedError()<br/>        <br/>        return out, class_logits, gan_logits, features</pre>
<p><span>Calculate the loss:</span></p>
<pre>def model_loss(input_real, input_z, output_dim, y, num_classes, label_mask, alpha=0.2, drop_rate=0.):<br/>    """<br/>    Get the loss for the discriminator and generator<br/>    :param input_real: Images from the real dataset<br/>    :param input_z: Z input<br/>    :param output_dim: The number of channels in the output image<br/>    :param y: Integer class labels<br/>    :param num_classes: The number of classes<br/>    :param alpha: The slope of the left half of leaky ReLU activation<br/>    :param drop_rate: The probability of dropping a hidden unit<br/>    :return: A tuple of (discriminator loss, generator loss)<br/>    """<br/>    <br/>    <br/>    # These numbers multiply the size of each layer of the generator and the discriminator,<br/>    # respectively. You can reduce them to run your code faster for debugging purposes.<br/>    g_size_mult = 32<br/>    d_size_mult = 64<br/>    <br/>    # Here we run the generator and the discriminator<br/>    g_model = generator(input_z, output_dim, alpha=alpha, size_mult=g_size_mult)<br/>    d_on_data = discriminator(input_real, alpha=alpha, drop_rate=drop_rate, size_mult=d_size_mult)<br/>    d_model_real, class_logits_on_data, gan_logits_on_data, data_features = d_on_data<br/>    d_on_samples = discriminator(g_model, reuse=True, alpha=alpha, drop_rate=drop_rate, size_mult=d_size_mult)<br/>    d_model_fake, class_logits_on_samples, gan_logits_on_samples, sample_features = d_on_samples<br/>    <br/>    <br/>    # Here we compute `d_loss`, the loss for the discriminator.<br/>    # This should combine two different losses:<br/>    # 1. The loss for the GAN problem, where we minimize the cross-entropy for the binary<br/>    # real-vs-fake classification problem.<br/>    # 2. The loss for the SVHN digit classification problem, where we minimize the cross-entropy<br/>    # for the multi-class softmax. For this one we use the labels. Don't forget to ignore<br/>    # use `label_mask` to ignore the examples that we are pretending are unlabeled for the<br/>    # semi-supervised learning problem.<br/>    raise NotImplementedError()<br/>    <br/>    # Here we set `g_loss` to the "feature matching" loss invented by Tim Salimans at OpenAI.<br/>    # This loss consists of minimizing the absolute difference between the expected features<br/>    # on the data and the expected features on the generated samples.<br/>    # This loss works better for semi-supervised learning than the tradition GAN losses.<br/>    raise NotImplementedError()<br/><br/>    pred_class = tf.cast(tf.argmax(class_logits_on_data, 1), tf.int32)<br/>    eq = tf.equal(tf.squeeze(y), pred_class)<br/>    correct = tf.reduce_sum(tf.to_float(eq))<br/>    masked_correct = tf.reduce_sum(label_mask * tf.to_float(eq))<br/>    <br/>    return d_loss, g_loss, correct, masked_correct, g_model</pre>
<p class="mce-root">Add the optimizers:</p>
<pre>def model_opt(d_loss, g_loss, learning_rate, beta1):<br/>    """<br/>    Get optimization operations<br/>    :param d_loss: Discriminator loss Tensor<br/>    :param g_loss: Generator loss Tensor<br/>    :param learning_rate: Learning Rate Placeholder<br/>    :param beta1: The exponential decay rate for the 1st moment in the optimizer<br/>    :return: A tuple of (discriminator training operation, generator training operation)<br/>    """<br/>    # Get weights and biases to update. Get them separately for the discriminator and the generator<br/>    raise NotImplementedError()<br/><br/>    # Minimize both players' costs simultaneously<br/>    raise NotImplementedError()<br/>    shrink_lr = tf.assign(learning_rate, learning_rate * 0.9)<br/>    <br/>    return d_train_opt, g_train_opt, shrink_lr</pre>
<p class="mce-root"><span>Build the network model:</span></p>
<pre>class GAN:<br/>    """<br/>    A GAN model.<br/>    :param real_size: The shape of the real data.<br/>    :param z_size: The number of entries in the z code vector.<br/>    :param learnin_rate: The learning rate to use for Adam.<br/>    :param num_classes: The number of classes to recognize.<br/>    :param alpha: The slope of the left half of the leaky ReLU activation<br/>    :param beta1: The beta1 parameter for Adam.<br/>    """<br/>    def __init__(self, real_size, z_size, learning_rate, num_classes=10, alpha=0.2, beta1=0.5):<br/>        tf.reset_default_graph()<br/>        <br/>        self.learning_rate = tf.Variable(learning_rate, trainable=False)<br/>        inputs = model_inputs(real_size, z_size)<br/>        self.input_real, self.input_z, self.y, self.label_mask = inputs<br/>        self.drop_rate = tf.placeholder_with_default(.5, (), "drop_rate")<br/>        <br/>        loss_results = model_loss(self.input_real, self.input_z,<br/>                                  real_size[2], self.y, num_classes,<br/>                                  label_mask=self.label_mask,<br/>                                  alpha=0.2,<br/>                                  drop_rate=self.drop_rate)<br/>        self.d_loss, self.g_loss, self.correct, self.masked_correct, self.samples = loss_results<br/>        <br/>        self.d_opt, self.g_opt, self.shrink_lr = model_opt(self.d_loss, self.g_loss, self.learning_rate, beta1)</pre>
<p><span>Train and persist the model:</span></p>
<pre>def train(net, dataset, epochs, batch_size, figsize=(5,5)):<br/>    <br/>    saver = tf.train.Saver()<br/>    sample_z = np.random.normal(0, 1, size=(50, z_size))<br/><br/>    samples, train_accuracies, test_accuracies = [], [], []<br/>    steps = 0<br/><br/>    with tf.Session() as sess:<br/>        sess.run(tf.global_variables_initializer())<br/>        for e in range(epochs):<br/>            print("Epoch",e)<br/>            <br/>            t1e = time.time()<br/>            num_examples = 0<br/>            num_correct = 0<br/>            for x, y, label_mask in dataset.batches(batch_size):<br/>                assert 'int' in str(y.dtype)<br/>                steps += 1<br/>                num_examples += label_mask.sum()<br/><br/>                # Sample random noise for G<br/>                batch_z = np.random.normal(0, 1, size=(batch_size, z_size))<br/><br/>                # Run optimizers<br/>                t1 = time.time()<br/>                _, _, correct = sess.run([net.d_opt, net.g_opt, net.masked_correct],<br/>                                         feed_dict={net.input_real: x, net.input_z: batch_z,<br/>                                                    net.y : y, net.label_mask : label_mask})<br/>                t2 = time.time()<br/>                num_correct += correct<br/><br/>            sess.run([net.shrink_lr])<br/>            <br/>            <br/>            train_accuracy = num_correct / float(num_examples)<br/>            <br/>            print("\t\tClassifier train accuracy: ", train_accuracy)<br/>            <br/>            num_examples = 0<br/>            num_correct = 0<br/>            for x, y in dataset.batches(batch_size, which_set="test"):<br/>                assert 'int' in str(y.dtype)<br/>                num_examples += x.shape[0]<br/><br/>                correct, = sess.run([net.correct], feed_dict={net.input_real: x,<br/>                                                   net.y : y,<br/>                                                   net.drop_rate: 0.})<br/>                num_correct += correct<br/>            <br/>            test_accuracy = num_correct / float(num_examples)<br/>            print("\t\tClassifier test accuracy", test_accuracy)<br/>            print("\t\tStep time: ", t2 - t1)<br/>            t2e = time.time()<br/>            print("\t\tEpoch time: ", t2e - t1e)<br/>            <br/>            <br/>            gen_samples = sess.run(<br/>                                   net.samples,<br/>                                   feed_dict={net.input_z: sample_z})<br/>            samples.append(gen_samples)<br/>            _ = view_samples(-1, samples, 5, 10, figsize=figsize)<br/>            plt.show()<br/>            <br/>            <br/>            # Save history of accuracies to view after training<br/>            train_accuracies.append(train_accuracy)<br/>            test_accuracies.append(test_accuracy)<br/>            <br/><br/>        saver.save(sess, './checkpoints/generator.ckpt')<br/><br/>    with open('samples.pkl', 'wb') as f:<br/>        pkl.dump(samples, f)<br/>    <br/>    return train_accuracies, test_accuracies, samples</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep convolutional GAN</h1>
                </header>
            
            <article>
                
<p><strong>Deep convolutional GAN</strong>, also called <strong>DCGAN</strong>, is used to generate color images. Here we use a convolutional layer<span> in the generator and discriminator. We'll also need to use batch normalization to get the GAN to train appropriately. We will discuss batch normalization in detail in the performance improvement of deep neural networks chapter. We'll be training GAN on the SVHN dataset; a small example is shown in the following figure. After training, the generator will be able to create images that are nearly identical to these images. You can download the code for this example:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ca8ff0f4-19fc-465c-9b3e-aab5e4b9f1d7.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Google Street View house numbers view</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Batch normalization</h1>
                </header>
            
            <article>
                
<p><span>Batch normalization is a technique for improving the performance and stability of neural networks. The idea is to normalize the layer inputs so that they have a mean of zero and variance of 1. Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper, <em>Batch Normalization is Necessary to Make DCGANs Work</em>. The idea is that instead of just normalizing the inputs to the network, we normalize the inputs to layers within the network. It's called <strong>batch</strong> <strong>normalization</strong> because during training, we normalize each layer's input by using the mean and variance of the values in the current mini-batch.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen how the GAN model truly displays the power of CNN. We learned how to train our own generative model and saw a practical example of GAN that <span>can generate photos from paintings and turn horses into zebras</span>.</p>
<p>We understood how GAN differs from other discriminative models and learned why generative models are preferred.</p>
<p><span>In the next chapter, we will learn about deep learning software comparison from scratch.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>