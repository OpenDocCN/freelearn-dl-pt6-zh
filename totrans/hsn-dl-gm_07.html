<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introducing DRL</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Deep reinforcement learning</strong> (<strong>DRL</strong>) is currently taking the world by storm and is seen as the "it" of machine learning technologies, the it goal of reaching some form of general AI. Perhaps it is because DRL approaches the cusp of general AI or what we perceive as general intelligence. It is also likely to be one of the main reasons you are reading this book. Fortunately, this chapter, and the majority of the rest of the book, focuses deeply on <strong>reinforcement learning</strong> (<strong>RL</strong>) and its many variations. In this chapter, we start learning the basics of RL and how it can be adapted to <strong>deep learning</strong> (<strong>DL</strong>). We will explore the <strong>OpenAI Gym</strong> environment, a great RL playground, and see how to use it with some simple DRL techniques.</p>
<div class="mce-root packt_tip">Keep in mind, this is a hands-on book, so we will be keeping technical theory to a minimum, and instead we will explore plenty of working examples. Some readers may feel lost without the theoretical background and feel the need to explore the more theoretical side of RL on their own. <br/>
<br/>
For other readers not familiar with the theoretical background of RL, we will cover several core concepts, but this is the abridged version, so it is recommended you seek theoretical knowledge from other sources when you are ready.</div>
<p>In this chapter, we will start learning about DRL, a topic that will carry through to many chapters. We will start with the basics and then look to explore some working examples adapted to DL. Here is what we will cover in this chapter:</p>
<ul>
<li>Reinforcement learning</li>
<li>The Q-learning model</li>
<li>Running the OpenAI gym</li>
<li>The first DRL with Deep Q-Network</li>
<li>RL experiments</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For those of you who like to jump around books: yes, it is OK to start this book from this chapter. However, you may need to go back to previous chapters in order to complete some exercises. We will also assume that your Python environment is configured with TensorFlow and Keras, but if you are unsure, check out the <kbd>requirements.txt</kbd> file in the project folder.</p>
<div class="packt_tip">All the projects in this book are built with Visual Studio 2017 (Python), and it is the recommended editor for the examples in this book. If you use VS 2017 with Python, you can easily manage the samples by opening the chapter solution file. Of course, there are plenty of other excellent Python editors and tools, so use what you are comfortable with.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning</h1>
                </header>
            
            <article>
                
<p>RL currently leads the pack in advances compared to other machine learning methodologies. Note the use of the word <em>methodology</em> and not <em>technology</em>. RL is a methodology or algorithm that applies a principle we can use with neural networks, whereas, neural networks are a machine learning technology that can be applied to several methodologies. Previously, we looked at other methodologies that blended with DL, but we focused more on the actual implementation. However, RL introduces a new methodology that requires us to understand more of the inner and outer workings before we understand how to apply it. </p>
<div class="packt_infobox">RL was popularized by Richard Sutton, a Canadian, and current professor at the University of Alberta. Sutton has also assisted in the development of RL at Google's DeepMind, and is quite often regarded as the father of RL. </div>
<p>At the heart of any machine learning system is the need for training. Often, the AI agent/brain knows nothing, and then we feed it data through some automated process for it to learn. As we have seen, the most common way of doing this is called <strong>supervised training</strong>. This is when we first label our training data. We have also looked at <strong>unsupervised training</strong>, where our <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>) were trained by competing against each other. However, neither system replicated the type of learning or training we see in <strong>Biology</strong>, and that is often referred to as <strong>rewards</strong> or RL: the type of learning that lets you teach your dog to bark for a treat, fetch the paper, and use the outdoors for nature's calling, a type of learning that lets an agent explore its own environment and learn for itself. This is not unlike the type of learning a general AI would be expected to use; after all, RL is likely similar to the system we use, or so we believe.</p>
<p class="mce-root"/>
<div class="packt_infobox">David Silver, a former student of Prof Sutton's and now head of DeepMind, has an excellent video series on the theoretical background of RL. The first five videos are quite interesting and recommended viewing, but the later content gets quite deep and may not be for everyone. Here's the link for the videos: <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0">https://www.youtube.com/watch?v=2pWv7GOvuf0</a></div>
<p>RL defines its own type of training called by the same name. This form of reward-based training is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/9b176d0d-9b93-423d-9d0c-cb1998759379.png" style="width:40.08em;height:20.17em;"/><br/>
<br/>
Reinforcement learning </div>
<p class="mce-root"/>
<p>The diagram shows an agent in an environment. That agent reads the state of the environment and then decides and performs an action. This action may, or may not, give a reward, and that reward could be good or bad. After each action and possible reward, the agent collects the state of the environment again. The process repeats itself until the agent reaches a terminal or end state. That is, until it reaches the goal; perhaps it dies or just gets tired. It is important to note a couple of subtle things about the preceding diagram. First, the agent doesn't always receive a reward, meaning rewards could be delayed, until some future goal is reached. This is quite different from the other forms of learning we explored earlier, which provided immediate feedback to our training networks. Rewards can be good or bad, and it is often just as effective to negatively train agents this way, but less so for humans. </p>
<p>Now, as you might expect with any powerful learning model, the mathematics can be quite complex and certainly daunting to the newcomer. We won't go too far into the theoretical details other than to describe some of the foundations of RL in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The multi-armed bandit</h1>
                </header>
            
            <article>
                
<p>The diagram we saw earlier describes the full RL problem as we will use for most of the rest of this book. However, we often teach a simpler one-step variation of this problem called the <strong>multi-armed bandit</strong>. The armed bandit is in reference to the Vegas slot machine and nothing more nefarious. We use these simpler scenarios in order to explain the basics of RL in the form of a one-step or one-state problem. </p>
<p>In the case of the multi-armed bandit, picture a fictional multi-armed Vegas slot machine that awards different prizes based on which arm is pulled, but the prize for each arm is always the same. The agent's goal in this scenario would be to figure out the correct arm to pull every time. We could further model this in an equation such as the one shown here:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cce0e6d0-2ea0-4ea0-b7e2-6e13a93eb5fb.png" style="width:12.00em;height:1.25em;"/></div>
<div><br/>
Consider the following equation:</div>
<ul>
<li><img class="fm-editor-equation" src="assets/cfabb87d-9a06-4b96-8661-72ef51f3bb0a.png" style="color: #333333;font-size: 1em;width:1.92em;height:1.08em;"/><span> = vector of values (1,2,3,4)</span></li>
<li><img class="fm-editor-equation" src="assets/77c8a845-a682-4c89-b796-9055f0bd6b1f.png" style="color: #333333;font-size: 1em;width:0.67em;height:0.75em;"/><span> = action</span></li>
<li><img class="fm-editor-equation" src="assets/f7eea114-134d-4145-b713-94d4fb0ba75e.png" style="color: #333333;font-size: 1em;width:0.75em;height:0.67em;"/><span> = alpha = learning rate</span></li>
<li><img class="fm-editor-equation" src="assets/728ee055-bd4e-4791-be8b-c26fb766e47a.png" style="color: #333333;font-size: 1em;width:0.58em;height:0.92em;"/><span> = reward</span></li>
</ul>
<p>This equation calculates the value (<em>V</em>), a vector, for each action the agent takes. Then, it feeds back these values into itself, subtracted from the reward and multiplied by a learning rate. This calculated value can be used to determine which arm to pull, but first the agent needs to pull each arm at least once. Let's quickly model this in code, so as game/simulation programmers, we can see how this works. Open the <kbd>Chapter_5_1.py</kbd> code and follow these steps:</p>
<ol>
<li class="mce-root"><span>The code for this exercise is as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">alpha = .9<br/>arms = [['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]]<br/>v = [0,0,0,0]<br/><br/>for i in range(10):<br/>    for a in range(len(arms)):<br/>        print('pulling arm '+ arms[a][0])<br/>        v[a] = v[a] + alpha * (arms[a][1]-v[a])<br/><br/>print(v)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>This code creates the required setup variables, the <kbd>arms</kbd> (<kbd>gold</kbd>, <kbd>silver</kbd>, and <kbd>bronze</kbd>), and the value vector <kbd>v</kbd> (all zeros). Then, the code loops through a number of iterations (<kbd>10</kbd>) where each arm is pulled and the value, <kbd>v</kbd>, is calculated and updated based on the equation. Note that the reward value is replaced by the value of the arm pull, which is the term <kbd>arms[a][1]</kbd>.</li>
<li>Run the example, and you will see the output generated showing the value for each action, or in this case an arm pull.</li>
</ol>
<p>As we saw, with a simple equation, we were able to model the multi-armed bandit problem and arrive at a solution that will allow an agent to consistently pull the correct arm. This sets the foundation for RL, and in the next section, we take the next step and look at <strong>contextual bandits</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Contextual bandits</h1>
                </header>
            
            <article>
                
<p>We can now elevate the single multi-armed bandit problem into a problem with multiple multi-armed bandits, each with its own set of arms. Now our problem introduces context or state into the equation. With each bandit defining its own context/state, now we evaluate our equation in terms of quality and action. Our modified equation is shown here:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c461e128-42f5-4362-9277-a839f0c1589d.png" style="width:19.67em;height:1.58em;"/></div>
<div>Consider the following equation:</div>
<ul>
<li><img class="fm-editor-equation" src="assets/259fdfe3-f4e2-4625-8dc9-c073c9051569.png" style="color: #333333;font-size: 1em;width:2.92em;height:1.25em;"/><span> = table/matrix of values</span></li>
</ul>
<div style="padding-left: 60px">                                                                 [1,2,3,4</div>
<div>                                                                                 2,3,4,5</div>
<div>                                                                                 4,2,1,4]</div>
<ul>
<li><img src="assets/276ce14a-f047-46d2-afcb-e45b7a3caa43.png" style="width:0.75em;height:1.00em;"/> = state</li>
<li><img src="assets/dea129da-958b-4492-8437-591af763c9b2.png" style="width:0.83em;height:0.92em;"/> = action</li>
<li><img src="assets/ca094865-f756-450b-b7c2-9ebc383af35f.png" style="width:1.00em;height:0.92em;"/> = alpha = learning rate</li>
<li><img src="assets/3083521e-5c5f-4684-bc1c-2453b810c4c3.png" style="width:0.67em;height:1.00em;"/> = reward</li>
</ul>
<p class="mce-root"/>
<p>Let's open up <kbd>Chapter_5_2.py</kbd> and observe the following steps:</p>
<ol>
<li class="mce-root"><span>Open the code up, as follows, and follow the changes made from the previous sample:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">import random<br/><br/>alpha = .9<br/>bandits = [[['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]],<br/>           [['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]],<br/>           [['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]],<br/>           [['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]]]<br/>q = [[0,0,0,0],<br/>     [0,0,0,0],<br/>     [0,0,0,0],<br/>     [0,0,0,0]]<br/><br/>for i in range(10): <br/>    for b in range(len(bandits)):<br/>        arm = random.randint(0,3)<br/>        print('pulling arm {0} on bandit {1}'.format(arm,b))<br/>        q[b][arm] = q[b][arm] + alpha * (bandits[b][arm][1]-q[b][arm])<br/><br/>print(q)</pre>
<ol start="2">
<li>This code sets up a number of multi-armed bandits, each with its own set of arms. It then iterates through a number of iterations, but this time as it loops, it also loops through each bandit. During each loop, it picks a random arm to pull and evaluates the quality.</li>
<li>Run the sample and look at the output of <kbd>q</kbd>. Note how, even after selecting random arms, the equation again consistently selected the gold arm, the arm with the highest reward, to pull.</li>
</ol>
<p>Feel free to play around with this sample some more and look to the exercises for additional inspiration. We will expand on the complexity of our RL problems when we discuss Q-Learning. However, before we get to that section, we will take a quick diversion and look at setting up the OpenAI Gym in order to conduct more RL experiments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RL with the OpenAI Gym</h1>
                </header>
            
            <article>
                
<p>RL has become so popular that there is now a race to just build tools that help build RL algorithms. The two major competitors in this area right now are <strong>OpenAI Gym</strong> and <strong>Unity</strong>. Unity has quickly become the RL racing machine we will explore extensively later. For now, we will put our training wheels on and run OpenAI Gym to explore the fundamentals of RL further.</p>
<p>We need to install the OpenAI Gym toolkit before we can continue, and installation may vary greatly depending on your operating system. As such, we will focus on the Windows installation instructions here, as it is likely other OS users will have less difficulty. Follow the next steps to install OpenAI Gym on Windows:</p>
<ol>
<li>Install a C++ compiler; if you have Visual Studio 2017 installed, you may already have a recommended one. You can find other supported compilers here: <a href="https://wiki.python.org/moin/WindowsCompilers">https://wiki.python.org/moin/WindowsCompilers</a>.</li>
<li class="mce-root"><span>Be sure to have Anaconda installed, and open an Anaconda command prompt and run the following commands:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>conda create -n gym</strong><br/><strong>conda activate gym</strong><br/><strong>conda install python=3.5  # reverts Python, for use with TensorFlow later</strong><br/><strong>pip install tensorflow</strong><br/><strong>pip install keras<br/></strong><strong>pip install gym</strong></pre>
<ol start="3"/>
<ol start="3">
<li>For our purposes, in the short term, we don't need to install any other Gym modules. Gym has plenty of example environments, Atari games and MuJoCo (robotics simulator) being some of the most fun to work with. We will take a look at the Atari games module later in this chapter.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p>That should install the Gym environment for your system. Most of what we need will work with minimal setup. If you decide to do more with Gym, then you will likely want to install other modules; there are several. In the next section, we are going to test this new environment as we learn about Q-Learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A Q-Learning model</h1>
                </header>
            
            <article>
                
<p>RL is deeply entwined with several mathematical and dynamic programming concepts that could fill a textbook, and indeed there are several. For our purposes, however, we just need to understand the key concepts in order to build our DRL agents. Therefore, we will choose not to get too burdened with the math, but there are a few key concepts that you will need to understand to be successful. If you covered the math in the <a href="108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml" target="_blank">Chapter 1</a>, <em>Deep Learning for Games</em>, this section will be a breeze. For those that didn't, just take your time, but you can't miss this one.</p>
<p>In order to understand the Q-Learning model, which is a form of RL, we need to go back to the basics. In the next section, we talk about the importance of the <strong>Markov decision process</strong> and the <strong>Bellman</strong> e<strong>quation</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Markov decision process and the Bellman equation</h1>
                </header>
            
            <article>
                
<p>At the heart of RL is the <strong>Markov decision process</strong> (<strong>MDP</strong>). An MDP is often described as a discrete time stochastic control process. In simpler terms, this just means it is a control program that functions by time steps to determine the probability of actions, provided each action leads to a reward. This process is already used for most automation control of robotics, drones, networking, and of course RL. The classic way we picture this process is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/434ef2cd-a9d0-4c70-9cd8-e0cddd217734.png" style="width:34.25em;height:25.25em;"/><br/>
<br/>
The Markov decision process</div>
<p>Where represent an MDP as a tuple or vector <img class="fm-editor-equation" src="assets/2f668c37-4a3c-4fb3-bea6-89ee1da65240.png" style="width:7.17em;height:1.33em;"/>, using the following variables:</p>
<ul>
<li><span><img class="fm-editor-equation" src="assets/db442408-6913-47ba-8e98-f0dc1ac4d37e.png" style="width:0.83em;height:1.17em;"/> - </span>being a finite set of states,</li>
<li><span class="mwe-math-element"><img class="fm-editor-equation" src="assets/fd0b1ca8-560b-4a27-baa6-603d0ce3229d.png" style="width:0.92em;height:1.08em;"/> - being</span> a finite set of actions,</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y"><img class="fm-editor-equation" src="assets/02393b19-30c2-43a5-b8d6-8e27fee233c9.png" style="width:1.42em;height:1.17em;"/> -</span></span> the probability that action <span><img class="fm-editor-equation" src="assets/bf5cc02b-c6f0-43ba-a6d4-bfd93a7b3364.png" style="width:0.92em;height:1.08em;"/></span><span> </span>in state <span><img class="fm-editor-equation" src="assets/612e1b85-51ac-4943-bf9f-d3043356022d.png" style="width:0.75em;height:1.08em;"/></span><span> </span>at time <span><img class="fm-editor-equation" src="assets/2e0c7122-44b2-490a-9541-7005d2b217bd.png" style="width:0.50em;height:1.08em;"/></span><span> </span>will lead to state <span><img class="fm-editor-equation" src="assets/db1fa5b6-7a59-4f45-904c-928e1e782c1f.png" style="width:2.17em;height:1.25em;"/></span><span> </span>at time <span><img class="fm-editor-equation" src="assets/ed7e16b3-966f-43cd-8e6b-2cbccf8ab57a.png" style="width:2.83em;height:1.25em;"/></span>,</li>
<li><span class="mwe-math-element"><img class="fm-editor-equation" src="assets/a964622c-26ef-4841-865b-40ccc8197365.png" style="width:1.00em;height:1.17em;"/> - </span>is the immediate reward</li>
<li><span class="mwe-math-element"><img class="fm-editor-equation" src="assets/aaa15109-a497-4e52-a731-a66acb40c59d.png" style="width:0.83em;height:1.25em;"/> - gamma is a d</span>iscount factor we apply in order to discount the significance or provide significance to future rewards</li>
</ul>
<p>The diagram works by picturing yourself as an agent in one of the states. You then determine actions based on the probability, always taking a random action. As you move to the next state, the action gives you a reward and you update the probability based on the reward. Again, David Silver covers this piece very well in his lectures.</p>
<p class="mce-root"/>
<p>Now, the preceding process works, but another variation came along that provided for better future reward evaluation, and that was done by introducing the <strong>Bellman Equation</strong> and the concept of a policy/value iteration. Whereas before we had a value, <img class="fm-editor-equation" src="assets/d62b7e72-e9f9-470c-aafa-95bf8254b13f.png" style="width:0.92em;height:1.08em;"/>, we now have a policy (<img class="fm-editor-equation" src="assets/8aedee8a-b54e-4b76-96ec-ebd39c12acc0.png" style="width:0.92em;height:0.92em;"/>) for a value called <img class="fm-editor-equation" src="assets/6ae870a0-2615-43ca-a224-1b08d456a58f.png" style="width:1.83em;height:1.25em;"/>, and this yields us a new equation, shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/16cb44c4-34b7-4914-b933-ce07125fabf0.png" style="width:25.25em;height:3.75em;"/></p>
<p>We won't cover much more about this equation other than to say to keep the concept of quality iteration in mind. In the next section, we will see how we can reduce this equation back to a quality indicator of each action and use that for Q-Learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Q-learning</h1>
                </header>
            
            <article>
                
<p>With the introduction of quality iteration methods, the derivation of a finite state method called <strong>Q-learning</strong> or <strong>quality learning</strong> was derived. Q uses the technique of quality iteration for a given finite state problem to determine the best course of action for an agent. The equation we saw in the previous section can now be represented as the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/649eaf34-9453-4328-9183-5f8437af651f.png" style="width:30.92em;height:1.50em;"/></p>
<p>Consider the following equation:</p>
<ul>
<li> <img class="fm-editor-equation" src="assets/74182c8a-795c-46a2-bd88-8679475b3ddd.png" style="font-size: 1em;color: #333333;width:2.33em;height:1.00em;"/><span> current state</span></li>
<li>  <img class="fm-editor-equation" src="assets/a02d73e5-10d7-4937-b5bf-3b0d34f2807c.png" style="font-size: 1em;color: #333333;width:2.58em;height:1.00em;"/><span> current action</span></li>
<li><img class="fm-editor-equation" src="assets/8fe87e76-4098-487c-8033-e01ad9688d4b.png" style="font-size: 1em;color: #333333;width:2.17em;height:0.83em;"/><span> next action</span></li>
<li><img class="fm-editor-equation" src="assets/1d73936e-4c41-408e-8249-949594662494.png" style="font-size: 1em;color: #333333;width:2.17em;height:0.92em;"/><span> current reward</span></li>
<li><img class="fm-editor-equation" src="assets/6035c5ee-320c-4ab2-865c-5294f55549c2.png" style="font-size: 1em;color: #333333;width:2.42em;height:0.83em;"/><span> learning rate (alpha)</span></li>
<li><img class="fm-editor-equation" src="assets/57f56e14-a30f-4531-9da1-e5991077f44b.png" style="font-size: 1em;color: #333333;width:2.00em;height:1.00em;"/><span> reward discount factor (gamma)</span></li>
</ul>
<p>The Q value is now updated alliteratively, as the agent roams through its environment. Nothing demonstrates these concepts better than an example. Open up <kbd>Chapter_5_3.py</kbd> and follow these steps:</p>
<ol>
<li class="mce-root"><span>We start with the various imports and set them up as shown in the following code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">from collections import deque<br/>import numpy as np<br/>import os<br/>clear = lambda: os.system('cls') #linux/mac use 'clear'<br/>import time<br/>import gym<br/>from gym import wrappers, logger</pre>
<ol start="2">
<li>These imports just load the basic libraries we need for this example. Remember, you will need to install <kbd>Gym</kbd> to run this sample.</li>
<li class="mce-root"><span>Next, we set up a new environment; in this example, we use the basic</span> <kbd>FrozenLake-v0</kbd> sample, a perfect example to test on Q-learning:</li>
</ol>
<pre style="color: black;padding-left: 60px">environment = 'FrozenLake-v0'<br/>env = gym.make(environment)</pre>
<ol start="4">
<li>Then we set up the AI environment (<kbd>env</kbd>) and a number of other parameters:</li>
</ol>
<pre style="color: black;padding-left: 60px">outdir = os.path.join('monitor','q-learning-{0}'.format(environment))<br/>env = wrappers.Monitor(env, directory=outdir, force=True)<br/>env.seed(0)<br/>env.is_slippery = False<br/>q_table = np.zeros([env.observation_space.n, env.action_space.n])<br/><br/>#parameters<br/>wins = 0<br/>episodes = 40000<br/>delay = 1<br/><br/>epsilon = .8<br/>epsilon_min = .1<br/>epsilon_decay = .001<br/>gamma = .9<br/>learning_rate = .1</pre>
<ol start="5">
<li>In this section of the code, we set up a number of variables that we will get to shortly. For this sample, we are using a wrapper tool to monitor the environment, and this is useful for determining any potential training issues. The other thing to note is the setup of the <kbd>q_table</kbd> array, defined by the environment <kbd>observation_space</kbd> (state) and <kbd>action_space</kbd> (action); spaces define arrays and not just vectors. In this particular example, the <kbd>action_space</kbd> is a vector, but it could be a multi-dimensional array or tensor.</li>
</ol>
<ol start="6">
<li class="mce-root">Pass over the next section of functions and skip to the end, where the training iteration occurs and is shown in the following code:</li>
</ol>
<pre style="color: black;padding-left: 60px">for episode in range(episodes): <br/>    state = env.reset()<br/>    done = False<br/>    while not done:<br/>        <strong>action = act(env.action_space,state)</strong><br/>        next_state, reward, done, _ = env.step(action)<br/>        clear()<br/>        env.render()<br/>        <strong>learn(state, action, reward, next_state)</strong><br/>        if done:<br/>            if reward &gt; 0:<br/>                wins += 1<br/>            time.sleep(3*delay)<br/>        else:<br/>            time.sleep(delay)<br/><br/>print("Goals/Holes: %d/%d" % (wins, episodes - wins))<br/>env.close() </pre>
<ol start="7">
<li>Most of the preceding code is relatively straightforward and should be easy to follow. Look at how the <kbd>env</kbd> (environment) is using the <kbd>action</kbd> generated from the <kbd>act</kbd> function; this is used to step or conduct an action on the agent. The output of the <kbd>step</kbd> function is <kbd>next_state</kbd>, <kbd>reward</kbd>, and <kbd>done</kbd>, which we use to determine the optimum Q policy by using the <kbd>learn</kbd> function.</li>
<li>Before we get into the action and learning functions, run the sample and watch how the agent trains. It may take a while to train, so feel free to return to the book.</li>
</ol>
<p>The following is an example of the OpenAI Gym FrozenLake environment running our Q-learning model:</p>
<div class="CDPAlignCenter packt_figref CDPAlign"><img src="assets/c2cbf3e7-ae6a-4e0c-a5c8-3961aa6119c6.png" style="width:20.42em;height:9.08em;"/><br/>
<br/>
FrozenLake Gym environment</div>
<p>As the sample runs, you will see a simple text output showing the environment. <kbd>S</kbd> represents the start, <kbd>G</kbd> the goal, <kbd>F</kbd> a frozen section, and <kbd>H</kbd> a hole. The goal for the agent is to find its way through the environment, without falling in a hole, and reach the goal. Pay special attention to how the agent moves and finds it way around the environment. In the next section, we unravel the <kbd>learn</kbd> and <kbd>act</kbd> functions and understand the importance of exploration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Q-learning and exploration</h1>
                </header>
            
            <article>
                
<p class="mce-root">One problem we face with the policy iterative<span> models such as</span><span> Q-learning is the problem of exploration versus exploitation. The Q-model equation assumes the use of maximum quality to determine an action and we refer to this as exploitation (exploiting the model). The problem with this is that it can often corner an agent into a solution that only looks for the best short-term benefits. Instead, we need to allow the agent some flexibility to explore the environment and learn on its own. We do this by introducing a dissolving exploration factor into the training. Let's see how this looks by again opening up the <kbd>Chapter_5_3.py</kbd> example:</span></p>
<ol>
<li class="mce-root"><span>Scroll down to the</span> <span><kbd>act</kbd></span> <span>and</span> <span><kbd>is_explore</kbd></span> <span>functions as shown:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">def is_explore():<br/>    global epsilon, epsilon_decay, epsilon_min<br/>    epsilon = max(epsilon-epsilon_decay,epsilon_min)<br/>    if np.random.rand() &lt; epsilon:<br/>        return True<br/>    else:<br/>        return False<br/><br/>def act(action_space, state):<br/>    # 0 - left, 1 - Down, 2 - Right, 3 - Up<br/>    global q_table<br/>    if is_explore():<br/>        return action_space.sample()<br/>    else:<br/>        return np.argmax(q_table[state])</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Note that in the <kbd>act</kbd> function, it first tests whether the agent wants to or needs to explore with <kbd>is_explore()</kbd>. In the <kbd>is_explore</kbd> function, we can see that the global <kbd>epsilon</kbd> value is decayed over each iteration with <kbd>epsilon_decay</kbd> to a global minimum value, <kbd>epsilon_min</kbd>. When the agent starts an episode, their exploration <kbd>epsilon</kbd> is high, making them more probable to explore. Over time, as the episode progresses, the <kbd>epsilon</kbd> decreases. We do in with the assumption that over time the agent will need to explore less and less. This trade-off between exploration and exploitation is quite important and something to understand with respect to the size of the environment state. We will see this trade-off explored more throughout this book.<br/>
Note that the agent uses an exploration function and just selects a random action.</li>
<li class="mce-root"><span>Finally, we get to the</span> <kbd>learn</kbd> <span>function. This function is where the <kbd>Q</kbd> value is calculated, as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">def learn(state, action, reward, next_state):<br/>    # Q(s, a) += alpha * (reward + gamma * max_a' Q(s', a') - Q(s, a))<br/>    global q_table<br/>    q_value = gamma * np.amax(q_table[next_state])<br/>    q_value += reward<br/>    q_value -= q_table[state, action]<br/>    q_value *= learning_rate<br/>    q_value += q_table[state, action]<br/>    q_table[state, action] = q_value</pre>
<ol start="4">
<li>Here, the equation is broken up and simplified, but this is the step that calculates the value the agent will use when exploiting.</li>
</ol>
<p>Keep the agent running until it finishes. We just completed the first full reinforcement learning problem, albeit the one that had a finite state. In the next section, we greatly expand our horizons and look at deep learning combined with reinforcement learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">First DRL with Deep Q-learning</h1>
                </header>
            
            <article>
                
<p>Now that we understand the reinforcement learning process in detail, we can look to adapt our Q-learning model to work with deep learning. This, as you could likely guess, is the culmination of our efforts and where the true power of RL shines. As we learned through earlier chapters, deep learning is essentially a complex system of equations that can map inputs through a non-linear function to generate a trained output.</p>
<p class="mce-root"/>
<p>A neural network is just another, simpler method of solving a non-linear equation. We will look at how to use DNN to solve other equations later, but for now we will focus on using it to solve the Q-learning equation we saw in the previous section.</p>
<p>We will use the <strong>CartPole</strong> training environment from the OpenAI Gym toolkit. This environment is pretty much the standard used to learn <strong>Deep Q-learning</strong> (<strong>DQN</strong>).</p>
<p>Open up <kbd>Chapter_5_4.py</kbd> and follow the next steps to see how we convert our solver to use deep learning:</p>
<ol>
<li class="mce-root"><span>As usual, we look at the imports and some initial starting parameters, as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">import random<br/>import gym<br/>import numpy as np<br/>from collections import deque<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.optimizers import Adam<br/><br/>EPISODES = 1000</pre>
<ol start="2">
<li>Next, we are going to create a class this time to contain the functionality of the DQN agent. The <kbd>__init__</kbd> function is as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">class DQNAgent:<br/>    def __init__(self, state_size, action_size):<br/>        self.state_size = state_size<br/>        self.action_size = action_size<br/>        self.memory = deque(maxlen=2000)<br/>        self.gamma = 0.95 # discount rate<br/>        self.epsilon = 1.0 # exploration rate<br/>        self.epsilon_min = 0.01<br/>        self.epsilon_decay = 0.995<br/>        self.learning_rate = 0.001<br/>        self.model = self._build_model()</pre>
<ol start="3">
<li>Most of the parameters have already been covered, but note a new one called <kbd>memory</kbd>, which is a <strong>deque</strong> collection that holds that last 2,000 steps. This allows us to batch train our neural network in a sort of replay mode.</li>
</ol>
<ol start="4">
<li><span>Next, we look at how the neural network model is built with the</span> <kbd>_build_model</kbd> <span>function, as fo</span><span>llows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">def _build_model(self):<br/>    # Neural Net for Deep-Q learning Model<br/>    model = Sequential()<br/>    model.add(Dense(24, input_dim=self.state_size, activation='relu'))<br/>    model.add(Dense(24, activation='relu'))<br/>    model.add(Dense(self.action_size, activation='linear'))<br/>    model.compile(loss='mse',<br/>                      optimizer=Adam(lr=self.learning_rate))<br/>    return model</pre>
<ol start="5">
<li>This builds a fairly simple model, compared to others we have already seen, with three <strong>dense</strong> layers outputting a value for each action. The input into this network is the state.</li>
<li class="mce-root"><span>Jump down to the bottom of the file and look at the training iteration loop, shown as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">if __name__ == "__main__":<br/>    env = gym.make('CartPole-v1')<br/>    state_size = env.observation_space.shape[0]<br/>    action_size = env.action_space.n<br/>    agent = DQNAgent(state_size, action_size)<br/>    # agent.load("./save/cartpole-dqn.h5")<br/>    done = False<br/>    batch_size = 32<br/><br/>    for e in range(EPISODES):<br/>        state = env.reset()<br/>        state = np.reshape(state, [1, state_size]) <br/>        for time in range(500):<br/>            # env.render()<br/>            action = agent.act(state)<br/>            env.render()<br/>            next_state, reward, done, _ = env.step(action)<br/>            reward = reward if not done else -10<br/>            <strong>next_state = np.reshape(next_state, [1, state_size])</strong><br/><strong>            agent.remember(state, action, reward, next_state, done)</strong><br/>            state = next_state<br/>            if done:<br/>                print("episode: {}/{}, score: {}, e: {:.2}"<br/>                      .format(e, EPISODES, time, agent.epsilon))<br/>                break<br/>            if len(agent.memory) &gt; batch_size:<br/>                <strong>agent.replay(batch_size)</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="7">
<li>
<p>In this sample, our training takes place in a real-time <kbd>render</kbd> loop. The important sections of the code are highlighted, showing the reshaping of the state and calling the <kbd>agent.remember</kbd> function. The <kbd>agent.replay</kbd> function at the end is where the network trains. The <kbd>remember</kbd> function is as follows:</p>
</li>
</ol>
<pre style="color: black;padding-left: 60px">def remember(self, state, action, reward, next_state, done):<br/>    self.memory.append((state, action, reward, next_state, done))</pre>
<ol start="8">
<li>This function just stores the <kbd>state</kbd>, <kbd>action</kbd>, <kbd>reward</kbd>, <kbd>next_state</kbd>,  and <kbd>done</kbd> parameters for the replay training. Scroll down more to the <kbd>replay</kbd> function, as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px">def replay(self, batch_size):<br/>    minibatch = random.sample(self.memory, batch_size)<br/>    for state, action, reward, next_state, done in minibatch:<br/>        target = reward<br/>        if not done:<br/>            target = (reward+self.gamma*<br/>                      np.amax(s<strong>elf.model.predict</strong>(next_state)[0]))<br/>            target_f = s<strong>elf.model.predict</strong>(state)<br/>            target_f[0][action] = target<br/>            self.model.fit(state, target_f, epochs=1, verbose=0)<br/>        if self.epsilon &gt; self.epsilon_min:<br/>            self.epsilon *= self.epsilon_decay</pre>
<ol start="9">
<li>The <kbd>replay</kbd> function is where the network training occurs. We first define a <kbd>minibatch</kbd>, which is defined from a random sampling of previous experiences grouped by <kbd>batch_size</kbd>. Then, we loop through the batches setting <kbd>reward</kbd> to the <kbd>target</kbd> and if not <kbd>done</kbd> calculating a new target based on the model prediction on the <kbd>next_state</kbd>. After that, we use the <kbd>model.predict</kbd> function on the <kbd>state</kbd> to determine the final target. Finally, we use the <kbd>model.fit</kbd> function to backpropagate the trained target back into the network.<br/>
As this section is important, let's reiterate. Note the line where the variable <kbd>target</kbd> is calculated and set. These lines of code may look familiar, as they match the Q value equation we saw earlier. This <kbd>target</kbd> value is the value that should be predicted for the current action. This is the value that is backpropagated back for the current action and set by the returned <kbd>reward</kbd>.</li>
</ol>
<ol start="10">
<li>Run the sample and watch the agent train to balance the pole on the cart. The following shows the environment as it is being trained:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/48db2129-1bae-40e8-8dd2-1c61deb1dbc1.png" style="width:24.67em;height:16.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">CartPole OpenAI Gym environment</div>
<p>The example environment uses the typical first environment, CartPole, we use to learn to build our first DRL model. In the next section, we will look at how to use the DQNAgent in other scenarios and other models supplied through the Keras-RL API.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RL experiments</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning is quickly advancing, and the DQN model we just looked at has quickly become outpaced by more advanced algorithms. There are several variations and advancements in RL algorithms that could fill several chapters, but most of that material would be considered academic. As such, we will instead look at some more practical examples of the various RL models the Keras RL API provides.</p>
<p>The first simple example we can work with is changing our previous<span> example to work with a new</span> <kbd>gym</kbd> <span>environment. Open up</span> <kbd>Chapter_5_5.py</kbd> <span>and follow the next exercise:</span></p>
<ol>
<li class="mce-root"><span>Change the environment name in the following code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">if __name__ == "__main__":<br/>    env = gym.make('<strong>MountainCar-v0</strong>')</pre>
<ol start="2">
<li>In this case, we are going to use the <kbd>MountainCar</kbd> environment, as shown:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/e4ed1377-b31b-4cc7-adf3-e86c57db2247.png" style="width:23.58em;height:15.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example of MountainCar environment</div>
<ol start="3">
<li>Run the code as you normally would and see how the DQNAgent solves the hill-climbing problem.</li>
</ol>
<p>You can see how quickly we were able to switch environments and test the DQNAgent in another environment. In the next section, we look at training Atari games with the various RL algorithms that the Keras-RL API provides.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keras RL</h1>
                </header>
            
            <article>
                
<p>Keras provides a very useful RL API that wraps several variations such as DQN, DDQN, SARSA, and so on. We won't get into the details of those various RL variations right now, but we will cover the important parts later, as we get into more complex models. For now, though, we are going to look at how you can quickly build a DRL model to play Atari games. Open up <kbd>Chapter_5_6.py</kbd> and follow these steps:</p>
<ol>
<li class="mce-root"><span>We first need to install several dependencies with <kbd>pip</kbd>; open a command shell or Anaconda window, and enter the following commands:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>pip install Pillow</strong><br/><strong>pip install keras-rl</strong><br/><br/><strong>pip install gym[atari]</strong> # on Linux or Mac<br/><strong>pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py</strong>  # on Windows thanks to Nikita Kniazev</pre>
<p class="mce-root"/>
<ol start="2">
<li>This will install the Keras RL API, <kbd>Pillow</kbd>, an image framework, and the Atari environment for <kbd>gym</kbd>.</li>
<li class="mce-root"><span>Run the example code as you normally would. This sample does take script arguments, but we don't need to use them here. An example of the rendered Atari Breakout environment follows:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/3f3e5a5c-4ad3-419f-8f5b-4a57934c63c3.png"/><br/>
<span><br/></span><span>Atari Breakout environment</span></div>
<p>Unfortunately, you cannot see the game run as the agent plays, because all the action takes place in the background, but let the agent run until it completes and saves the model. Here's how we would run the sample:</p>
<ol>
<li>You can rerun the sample using <kbd>--mode test</kbd> as an argument to let the agent run over 10 episodes and see the results.</li>
<li class="mce-root"><span>As the sample runs, look through the code and pay special attention to the model, as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">model = Sequential()<br/>if K.image_dim_ordering() == 'tf':<br/>    # (width, height, channels)<br/>    model.add(Permute((2, 3, 1), input_shape=input_shape))<br/>elif K.image_dim_ordering() == 'th':<br/>    # (channels, width, height)<br/>    model.add(Permute((1, 2, 3), input_shape=input_shape))<br/>else:<br/>    raise RuntimeError('Unknown image_dim_ordering.')<br/>model.add(Convolution2D(32, (8, 8), strides=(4, 4)))<br/>model.add(Activation('relu'))<br/>model.add(Convolution2D(64, (4, 4), strides=(2, 2)))<br/>model.add(Activation('relu'))<br/>model.add(Convolution2D(64, (3, 3), strides=(1, 1)))<br/>model.add(Activation('relu'))<br/>model.add(Flatten())<br/>model.add(Dense(512))<br/>model.add(Activation('relu'))<br/>model.add(Dense(nb_actions))<br/>model.add(Activation('linear'))<br/>print(model.summary())</pre>
<ol start="3">
<li>Note how our model is using <kbd>Convolution</kbd>, with pooling. This is because this example reads each screen/frame of the game as input (state) and responds accordingly. In this case, the model state is massive, and this demonstrates the real power of DRL. In this case, we are still training to a state model, but in future chapters, we will look at training a policy, rather than a model.</li>
</ol>
<p>This was a simple introduction to RL, and we have omitted several details that can get lost on newcomers. As we plan to cover several more chapters on RL, and in particular the <strong>Proximal Policy Optimization</strong> (<strong><span>PPO</span></strong><span>)</span><strong><span> </span></strong>in more detail in <a href="1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml">Chapter 8</a>, <em>Understanding PPO</em>, don't fret too much about differences such as policy and model-based RL.</p>
<div class="packt_tip">There is an excellent example of this same DQN in TensorFlow at this GitHub link: <a href="https://github.com/floodsung/DQN-Atari-Tensorflow" target="_blank">https://github.com/floodsung/DQN-Atari-Tensorflow</a>. The code may be a bit dated, but it is a simple and excellent example that is worth taking a look at.</div>
<p>We won't look any further at the code, but the reader is certainly invited to. Now let's try some exercises.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>As always, use the exercises in this section to get a better understanding of the material you learn. Try to work through at least two or three exercises in this section:</p>
<ol>
<li>Return to the example <kbd>Chapter_5_1.py</kbd> and change the <strong>alpha</strong> (<kbd>learning_rate</kbd>) variable and see what effect this has on the values calculated.</li>
<li>Return to the example <kbd>Chapter_5_2.py</kbd> and alter the arm positions on the various bandits.</li>
<li>Change the learning rate on the example <kbd>Chapter_5_2.py</kbd> and see what effect this has on the Q results output.</li>
</ol>
<p class="mce-root"/>
<ol start="4">
<li>Alter the gamma reward discount factor in the <kbd>Chapter_5_3.py</kbd> example, and see what effect this has on agent training.</li>
<li>Change the exploration epsilon in the <kbd>Chapter_5_3.py</kbd> to different values and rerun the sample. See what effect altering the various exploration parameters has on training the agent.</li>
<li>Alter the various parameters (<strong>exploration</strong>, <strong>alpha</strong>, and <strong>gamma</strong>) in the <kbd>Chapter_5_4.py</kbd> example and see what effect this has on training.</li>
<li>Alter the size of the memory in the <kbd>Chapter_5_4.py</kbd> example, either higher or lower, and see what effect this has on training.</li>
<li>Try to use different Gym environments in the DQNAgent example from <kbd>Chapter_5_5.py</kbd>. You can do a quick Google search to see the other possible environments you can choose from.</li>
<li>The <kbd>Chapter_5_6.py</kbd> example currently uses a form-exploration policy called <kbd>LinearAnnealedPolicy</kbd>; change the policy to use the <kbd>BoltzmannQPolicy</kbd> policy as mentioned in the code comments.</li>
<li>Be sure to download and run other Keras-RL examples from <a href="https://github.com/keras-rl/keras-rl">https://github.com/keras-rl/keras-rl</a>. Again, you may have to install other Gym environments to get them working.</li>
</ol>
<p>There are plenty of other examples, videos, and other materials to study with respect to RL. Learn as much as you can, as this material is extensive and complex and not something you will pick up overnight.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>RL is the machine learning technology currently dominating the interest of many researchers. It is typically appealing to us, because it fits well with games and simulations. In this chapter, we covered some of the foundations of RL by starting with the fundamental introductory problems of the multi-armed and contextual bandits. Then, we quickly looked at installing the OpenAI Gym RL toolkit. We then looked at Q-learning and how to implement that in code and train it on an OpenAI Gym environment. Finally, we looked at how we could conduct various other experiments with Gym by loading a couple of other environments, including the Atari games simulator.</p>
<p>In the next chapter, we look at the quickly evolving a cutting-edge RL platform that Unity is currently developing.</p>


            </article>

            
        </section>
    </body></html>