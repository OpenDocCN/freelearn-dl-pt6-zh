["```py\n<dependency>\n    <groupId>org.deeplearning4j</groupId>\n    <artifactId>deeplearning4j-core</artifactId>\n    <version>0.4-rc3.9</version>\n</dependency>\n<dependency>\n    <artifactId>canova-api</artifactId>\n    <groupId>org.nd4j</groupId>\n    <version>0.4-rc3.9</version>\n</dependency>\n<dependency>\n    <groupId>org.nd4j</groupId>\n    <artifactId>nd4j-native</artifactId>\n    <version>0.4-rc3.9</version>\n</dependency>\n<dependency>\n    <groupId>org.nd4j</groupId>\n    <artifactId>canova-api</artifactId>\n    <version>0.0.0.17</version>\n</dependency>\n```", "```py\n****************************************************************\nWARNING: COULD NOT LOAD NATIVE SYSTEM BLAS\nND4J performance WILL be reduced\n****************************************************************\n```", "```py\nval layer_0 = new ConvolutionLayer.Builder(5, 5)\n    .nIn(nChannels)\n    .stride(1, 1)\n    .nOut(20)\n    .activation(\"relu\")\n    .build()\n```", "```py\nval layer_1 = new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n    .kernelSize(2, 2)\n    .stride(2, 2)\n    .build()\n```", "```py\n\nval layer_2 = new ConvolutionLayer.Builder(5, 5)\n    .nIn(nChannels)\n    .stride(1, 1)\n    .nOut(50)\n    .activation(\"relu\")\n    .build()\n```", "```py\nval layer_3 = new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n    .kernelSize(2, 2)\n    .stride(2, 2)\n    .build()\n```", "```py\nval layer_4 = new DenseLayer.Builder()\n    .activation(\"relu\")\n    .nOut(500)\n    .build()\n```", "```py\nval layer_5 = new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n    .nOut(outputNum)\n    .activation(\"softmax\")\n    .build()\n```", "```py\nval builder: MultiLayerConfiguration.Builder = new NeuralNetConfiguration.Builder()\n    .seed(seed)\n    .iterations(iterations)\n    .regularization(true).l2(0.0005)\n    .learningRate(0.01)\n    .weightInit(WeightInit.XAVIER)\n   .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n    .updater(Updater.NESTEROVS).momentum(0.9)\n    .list()\n        .layer(0, layer_0)\n        .layer(1, layer_1)\n        .layer(2, layer_2)\n        .layer(3, layer_3)\n        .layer(4, layer_4)\n        .layer(5, layer_5)\n    .backprop(true).pretrain(false) // feedforward and supervised so no pretraining\n```", "```py\nnew ConvolutionLayerSetup(builder, 28, 28, 1) //image size is 28*28\nval conf: MultiLayerConfiguration = builder.build()\nval model: MultiLayerNetwork = new MultiLayerNetwork(conf)\nmodel.init()\n```", "```py\nval nChannels = 1 // for grayscale image\nval outputNum = 10 // number of class\nval nEpochs = 10 // number of epoch\nval iterations = 1 // number of iteration\nval seed = 12345 // Random seed for reproducibility\nval batchSize = 64 // number of batches to be sent\nlog.info(\"Load data....\")\nval mnistTrain: DataSetIterator = new MnistDataSetIterator(batchSize, true, 12345)\nval mnistTest: DataSetIterator = new MnistDataSetIterator(batchSize, false, 12345)\n```", "```py\nlog.info(\"Model training started...\")\nmodel.setListeners(new ScoreIterationListener(1))\nvar i = 0\nwhile (i <= nEpochs) {\n    model.fit(mnistTrain);\n    log.info(\"*** Completed epoch {} ***\", i)\n    i = i + 1\n    }\nvar ds: DataSet = null var output: INDArray = null\n```", "```py\nlog.info(\"Model evaluation....\")\nval eval: Evaluation = new Evaluation(outputNum)\nwhile (mnistTest.hasNext()) {\n    ds = mnistTest.next()\n    output = model.output(ds.getFeatureMatrix(), false)\n    }\neval.eval(ds.getLabels(), output)\n```", "```py\nprintln(\"Accuracy: \" + eval.accuracy())\nprintln(\"F1 measure: \" + eval.f1())\nprintln(\"Precision: \" + eval.precision())\nprintln(\"Recall: \" + eval.recall())\nprintln(\"Confusion matrix: \" + \"n\" + eval.confusionToString())\nlog.info(eval.stats())\nmnistTest.reset()\n>>>\n==========================Scores=======================================\n Accuracy: 1\n Precision: 1\n Recall: 1\n F1 Score: 1\n=======================================================================\n```", "```py\npackage com.example.CIFAR\n\nimport org.canova.api.records.reader.RecordReader\nimport org.canova.api.split.FileSplit\nimport org.canova.image.loader.BaseImageLoader\nimport org.canova.image.loader.NativeImageLoader\nimport org.canova.image.recordreader.ImageRecordReader\nimport org.deeplearning4j.datasets.iterator.DataSetIterator\nimport org.canova.image.recordreader.ImageRecordReader\nimport org.deeplearning4j.datasets.canova.RecordReaderDataSetIterator\nimport org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator\nimport org.deeplearning4j.eval.Evaluation\nimport org.deeplearning4j.nn.api.OptimizationAlgorithm\nimport org.deeplearning4j.nn.conf.MultiLayerConfiguration\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration\nimport org.deeplearning4j.nn.conf.Updater\nimport org.deeplearning4j.nn.conf.layers.ConvolutionLayer\nimport org.deeplearning4j.nn.conf.layers.DenseLayer\nimport org.deeplearning4j.nn.conf.layers.OutputLayer\nimport org.deeplearning4j.nn.conf.layers.SubsamplingLayer\nimport org.deeplearning4j.nn.conf.layers.setup.ConvolutionLayerSetup\nimport org.deeplearning4j.nn.multilayer.MultiLayerNetwork\nimport org.deeplearning4j.nn.weights.WeightInit\nimport org.deeplearning4j.optimize.listeners.ScoreIterationListener\nimport org.nd4j.linalg.api.ndarray.INDArray\nimport org.nd4j.linalg.api.rng.Random\nimport org.nd4j.linalg.dataset.DataSet\nimport org.nd4j.linalg.dataset.SplitTestAndTrain\nimport org.nd4j.linalg.lossfunctions.LossFunctions\nimport org.slf4j.Logger\nimport org.slf4j.LoggerFactory\nimport java.io.File\nimport java.util.ArrayList\nimport java.util.List\n\nobject MNIST {\n val log: Logger = LoggerFactory.getLogger(MNIST.getClass)\n def main(args: Array[String]): Unit = {\n val nChannels = 1 // for grayscale image\n val outputNum = 10 // number of class\n val nEpochs = 1 // number of epoch\n val iterations = 1 // number of iteration\n val seed = 12345 // Random seed for reproducibility\n val batchSize = 64 // number of batches to be sent\n\n    log.info(\"Load data....\")\n val mnistTrain: DataSetIterator = new MnistDataSetIterator(batchSize, true, 12345)\n val mnistTest: DataSetIterator = new MnistDataSetIterator(batchSize, false, 12345)\n\n    log.info(\"Network layer construction started...\")\n    //First convolution layer with ReLU as activation function\n val layer_0 = new ConvolutionLayer.Builder(5, 5)\n        .nIn(nChannels)\n        .stride(1, 1)\n        .nOut(20)\n        .activation(\"relu\")\n        .build()\n\n    //First subsampling layer\n val layer_1 = new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n        .kernelSize(2, 2)\n        .stride(2, 2)\n        .build()\n\n    //Second convolution layer with ReLU as activation function\n val layer_2 = new ConvolutionLayer.Builder(5, 5)\n        .nIn(nChannels)\n        .stride(1, 1)\n        .nOut(50)\n        .activation(\"relu\")\n        .build()\n\n    //Second subsampling layer\n val layer_3 = new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n        .kernelSize(2, 2)\n        .stride(2, 2)\n        .build()\n\n    //Dense layer\n val layer_4 = new DenseLayer.Builder()\n        .activation(\"relu\")\n        .nOut(500)\n        .build()\n\n    // Final and fully connected layer with Softmax as activation function\n val layer_5 = new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n        .nOut(outputNum)\n        .activation(\"softmax\")\n        .build()\n\n    log.info(\"Model building started...\")\n val builder: MultiLayerConfiguration.Builder = new NeuralNetConfiguration.Builder()\n        .seed(seed)\n        .iterations(iterations)\n        .regularization(true).l2(0.0005)\n        .learningRate(0.01)\n        .weightInit(WeightInit.XAVIER)\n        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n        .updater(Updater.NESTEROVS).momentum(0.9)\n        .list()\n            .layer(0, layer_0)\n            .layer(1, layer_1)\n            .layer(2, layer_2)\n            .layer(3, layer_3)\n            .layer(4, layer_4)\n            .layer(5, layer_5)\n    .backprop(true).pretrain(false) // feedforward so no backprop\n\n// Setting up all the convlutional layers and initialize the network\nnew ConvolutionLayerSetup(builder, 28, 28, 1) //image size is 28*28\nval conf: MultiLayerConfiguration = builder.build()\nval model: MultiLayerNetwork = new MultiLayerNetwork(conf)\nmodel.init()\n\nlog.info(\"Model training started...\")\nmodel.setListeners(new ScoreIterationListener(1))\n var i = 0\n while (i <= nEpochs) {\n        model.fit(mnistTrain);\n        log.info(\"*** Completed epoch {} ***\", i)\n        i = i + 1\n var ds: DataSet = null\n var output: INDArray = null\n        log.info(\"Model evaluation....\")\n val eval: Evaluation = new Evaluation(outputNum)\n\n while (mnistTest.hasNext()) {\n            ds = mnistTest.next()\n            output = model.output(ds.getFeatureMatrix(), false)\n                }\n        eval.eval(ds.getLabels(), output)\n        println(\"Accuracy: \" + eval.accuracy())\n        println(\"F1 measure: \" + eval.f1())\n        println(\"Precision: \" + eval.precision())\n        println(\"Recall: \" + eval.recall())\n        println(\"Confusion matrix: \" + \"n\" + eval.confusionToString())\n        log.info(eval.stats())\n        mnistTest.reset()\n                }\n    log.info(\"****************Example finished********************\")\n            }\n    }\n```", "```py\nval labelMap = readBusinessLabels(\"data/labels/train.csv\")\nval businessMap = readBusinessToImageLabels(\"data/labels/train_photo_to_biz_ids.csv\")\nval imgs = getImageIds(\"data/images/train/\", businessMap, businessMap.map(_._2).toSet.toList).slice(0,100) // 20000 images\n\nprintln(\"Image ID retreival done!\")\nval dataMap = processImages(imgs, resizeImgDim = 128)\nprintln(\"Image processing done!\")\nval alignedData = new featureAndDataAligner(dataMap, businessMap, Option(labelMap))()\n\nprintln(\"Feature extraction done!\")\nval cnn0 = trainModelEpochs(alignedData, businessClass = 0, saveNN = \"models/model0\")\nval cnn1 = trainModelEpochs(alignedData, businessClass = 1, saveNN = \"models/model1\")\nval cnn2 = trainModelEpochs(alignedData, businessClass = 2, saveNN = \"models/model2\")\nval cnn3 = trainModelEpochs(alignedData, businessClass = 3, saveNN = \"models/model3\")\nval cnn4 = trainModelEpochs(alignedData, businessClass = 4, saveNN = \"models/model4\")\nval cnn5 = trainModelEpochs(alignedData, businessClass = 5, saveNN = \"models/model5\")\nval cnn6 = trainModelEpochs(alignedData, businessClass = 6, saveNN = \"models/model6\")\nval cnn7 = trainModelEpochs(alignedData, businessClass = 7, saveNN = \"models/model7\")\nval cnn8 = trainModelEpochs(alignedData, businessClass = 8, saveNN = \"models/model8\")\n\nval businessMapTE = readBusinessToImageLabels(\"data/labels/test_photo_to_biz.csv\")\n\nval imgsTE = getImageIds(\"data/images/test//\", businessMapTE, businessMapTE.map(_._2).toSet.toList)\n\nval dataMapTE = processImages(imgsTE, resizeImgDim = 128) // make them 128*128\n\nval alignedDataTE = new featureAndDataAligner(dataMapTE, businessMapTE, None)()\nval Results = SubmitObj(alignedDataTE, \"results/ModelsV0/\")\nval SubmitResults = writeSubmissionFile(\"kaggleSubmitFile.csv\", Results, thresh = 0.9)\n```", "```py\ndef makeSquare(img: java.awt.image.BufferedImage): java.awt.image.BufferedImage = {\n val w = img.getWidth\n val h = img.getHeight\n val dim = List(w, h).min\n    img match {\n case x \n if w == h => img // do nothing and returns the original one\n case x \n if w > h => Scalr.crop(img, (w - h) / 2, 0, dim, dim)\n case x \n if w < h => Scalr.crop(img, 0, (h - w) / 2, dim, dim)\n        }\n    }\n```", "```py\ndef resizeImg(img: java.awt.image.BufferedImage, width: Int, height: Int) = {\n    Scalr.resize(img, Scalr.Method.BALANCED, width, height) \n}\n```", "```py\nimport org.imgscalr._\nimport java.io.File\nimport javax.imageio.ImageIO\n```", "```py\n<dependency>\n    <groupId>org.imgscalr</groupId>\n    <artifactId>imgscalr-lib</artifactId>\n    <version>4.2</version>\n</dependency>\n<dependency>\n    <groupId>org.datavec</groupId>\n    <artifactId>datavec-data-image</artifactId>\n    <version>0.9.1</version>\n</dependency>\n<dependency>\n    <groupId>com.sksamuel.scrimage</groupId>\n    <artifactId>scrimage-core_2.10</artifactId>\n    <version>2.1.0</version>\n</dependency>\n```", "```py\ndef pixels2Gray(R: Int, G: Int, B: Int): Int = (R + G + B) / 3\ndef makeGray(testImage: java.awt.image.BufferedImage): java.awt.image.BufferedImage = {\n val w = testImage.getWidth\n val h = testImage.getHeight\n for { \n        w1 <- (0 until w).toVector\n        h1 <- (0 until h).toVector\n        } \n yield \n    {\n val col = testImage.getRGB(w1, h1)\n val R = (col & 0xff0000) / 65536\n val G = (col & 0xff00) / 256\n val B = (col & 0xff)\n val graycol = pixels2Gray(R, G, B)\ntestImage.setRGB(w1, h1, new Color(graycol, graycol, graycol).getRGB)\n    }\ntestImage\n}\n```", "```py\nval demoImage = ImageIO.read(new File(x))\n    .makeSquare\n    .resizeImg(resizeImgDim, resizeImgDim) // (128, 128)\n    .image2gray\n```", "```py\nimport scala.Vector\nimport org.imgscalr._\n\nobject imageUtils {\n implicitclass imageProcessingPipeline(img: java.awt.image.BufferedImage) {\n    // image 2 vector processing\n def pixels2gray(R: Int, G:Int, B: Int): Int = (R + G + B) / 3\n def pixels2color(R: Int, G:Int, B: Int): Vector[Int] = Vector(R, G, B)\n private def image2vec[A](f: (Int, Int, Int) => A ): Vector[A] = {\n val w = img.getWidth\n val h = img.getHeight\n for {\n            w1 <- (0 until w).toVector\n            h1 <- (0 until h).toVector\n            } \n yield {\n val col = img.getRGB(w1, h1)\n val R = (col & 0xff0000) / 65536\n val G = (col & 0xff00) / 256\n val B = (col & 0xff)\n        f(R, G, B)\n                }\n            }\n\n def image2gray: Vector[Int] = image2vec(pixels2gray)\n def image2color: Vector[Int] = image2vec(pixels2color).flatten\n\n    // make image square\n def makeSquare = {\n val w = img.getWidth\n val h = img.getHeight\n val dim = List(w, h).min\n        img match {\n case x     \n if w == h => img\n case x \n if w > h => Scalr.crop(img, (w-h)/2, 0, dim, dim)\n case x \n if w < h => Scalr.crop(img, 0, (h-w)/2, dim, dim)\n              }\n            }\n\n    // resize pixels\n def resizeImg(width: Int, height: Int) = {\n        Scalr.resize(img, Scalr.Method.BALANCED, width, height)\n            }\n        }\n    }\n```", "```py\ndef readMetadata(csv: String, rows: List[Int]=List(-1)): List[List[String]] = {\n val src = Source.fromFile(csv)\n\n def reading(csv: String): List[List[String]]= {\n        src.getLines.map(x => x.split(\",\").toList)\n            .toList\n            }\n try {\n if(rows==List(-1)) reading(csv)\n else rows.map(reading(csv))\n            } \n finally {\n            src.close\n            }\n        }\n```", "```py\ndef readBusinessLabels(csv: String, rows: List[Int]=List(-1)): Map[String, Set[Int]] = {\n val reader = readMetadata(csv)\n    reader.drop(1)\n        .map(x => x match {\n case x :: Nil => (x(0).toString, Set[Int]())\n case _ => (x(0).toString, x(1).split(\" \").map(y => y.toInt).toSet)\n        }).toMap\n}\n```", "```py\ndef readBusinessToImageLabels(csv: String, rows: List[Int] = List(-1)): Map[Int, String] = {\n val reader = readMetadata(csv)\n    reader.drop(1)\n        .map(x => x match {\n case x :: Nil => (x(0).toInt, \"-1\")\n case _ => (x(0).toInt, x(1).split(\" \").head)\n        }).toMap\n}\n```", "```py\nval patt_get_jpg_name = new Regex(\"[0-9]\")\n```", "```py\ndef getImgIdsFromBusinessId(bizMap: Map[Int, String], businessIds: List[String]): List[Int] = {\n    bizMap.filter(x => businessIds.exists(y => y == x._2)).map(_._1).toList \n    }\n```", "```py\ndef getImageIds(photoDir: String, businessMap: Map[Int, String] = Map(-1 -> \"-1\"), businessIds:         \n    List[String] = List(\"-1\")): List[String] = {\n val d = new File(photoDir)\n val imgsPath = d.listFiles().map(x => x.toString).toList\n if (businessMap == Map(-1 -> \"-1\") || businessIds == List(-1)) {\n        imgsPath\n    } \n else {\n val imgsMap = imgsPath.map(x => patt_get_jpg_name.findAllIn(x).mkString.toInt -> x).toMap\n val imgsPathSub = getImgIdsFromBusinessId(businessMap, businessIds)\n        imgsPathSub.filter(x => imgsMap.contains(x)).map(x => imgsMap(x))\n        } \n    }\n```", "```py\ndef processImages(imgs: List[String], resizeImgDim: Int = 128, nPixels: Int = -1): Map[Int,Vector[Int]]= {\n    imgs.map(x => patt_get_jpg_name.findAllIn(x).mkString.toInt -> {\n val img0 = ImageIO.read(new File(x))\n        .makeSquare\n        .resizeImg(resizeImgDim, resizeImgDim) // (128, 128)\n        .image2gray\n if(nPixels != -1) img0.slice(0, nPixels)\n else img0\n        }\n    ).filter( x => x._2 != ())\n    .toMap\n    }\n```", "```py\nList[(Int, String, Vector[Int], Set[Int])]\n```", "```py\nval alignedData = new featureAndDataAligner(dataMap, businessMap, Option(labelMap))()\n```", "```py\nclass featureAndDataAligner(dataMap: Map[Int, Vector[Int]], bizMap: Map[Int, String], labMap: Option[Map[String, Set[Int]]])(rowindices: List[Int] = dataMap.keySet.toList) {\n def this(dataMap: Map[Int, Vector[Int]], bizMap: Map[Int, String])(rowindices: List[Int]) =         this(dataMap, bizMap, None)(rowindices)\n\n def alignBusinessImgageIds(dataMap: Map[Int, Vector[Int]], bizMap: Map[Int, String])\n        (rowindices: List[Int] = dataMap.keySet.toList): List[(Int, String, Vector[Int])] = {\n for { \n            pid <- rowindices\n val imgHasBiz = bizMap.get(pid) \n            // returns None if img doe not have a bizID\n val bid = if(imgHasBiz != None) imgHasBiz.get \n else \"-1\"\n if (dataMap.keys.toSet.contains(pid) && imgHasBiz != None)\n            } \n yield {\n        (pid, bid, dataMap(pid))\n           }\n        }\ndef alignLabels(dataMap: Map[Int, Vector[Int]], bizMap: Map[Int, String], labMap: Option[Map[String,     Set[Int]]])(rowindices: List[Int] = dataMap.keySet.toList): List[(Int, String, Vector[Int], Set[Int])] = {\n def flatten1[A, B, C, D](t: ((A, B, C), D)): (A, B, C, D) = (t._1._1, t._1._2, t._1._3, t._2)\n val al = alignBusinessImgageIds(dataMap, bizMap)(rowindices)\n for { p <- al\n        } \n yield {\n val bid = p._2\n val labs = labMap match {\n case None => Set[Int]()\n case x => (if(x.get.keySet.contains(bid)) x.get(bid) \n        else Set[Int]())\n            }\n            flatten1(p, labs)\n        }\n    }\n lazy val data = alignLabels(dataMap, bizMap, labMap)(rowindices)\n   // getter functions\n def getImgIds = data.map(_._1)\n def getBusinessIds = data.map(_._2)\n def getImgVectors = data.map(_._3)\n def getBusinessLabels = data.map(_._4)\n def getImgCntsPerBusiness = getBusinessIds.groupBy(identity).mapValues(x => x.size) \n}\n```", "```py\ndef makeDataSet(alignedData: featureAndDataAligner, bizClass: Int): DataSet = {\n val alignedXData = alignedData.getImgVectors.toNDArray\n val alignedLabs = alignedData.getBusinessLabels.map(x => \n if (x.contains(bizClass)) Vector(1, 0) \n    else Vector(0, 1)).toNDArray\n new DataSet(alignedXData, alignedLabs)\n    }\n```", "```py\ndef makeDataSetTE(alignedData: featureAndDataAligner): INDArray = {\n    alignedData.getImgVectors.toNDArray\n    }\n```", "```py\ndef trainModelEpochs(alignedData: featureAndDataAligner, businessClass: Int = 1, saveNN: String = \"\") = {\n val ds = makeDataSet(alignedData, businessClass)\n val nfeatures = ds.getFeatures.getRow(0).length // Hyperparameter\n val numRows = Math.sqrt(nfeatures).toInt //numRows*numColumns == data*channels\n val numColumns = Math.sqrt(nfeatures).toInt //numRows*numColumns == data*channels\n val nChannels = 1 // would be 3 if color image w R,G,B\n val outputNum = 9 // # of classes (# of columns in output)\n val iterations = 1\n val splitTrainNum = math.ceil(ds.numExamples * 0.8).toInt // 80/20 training/test split\n val seed = 12345\n val listenerFreq = 1\n val nepochs = 20\n val nbatch = 128 // recommended between 16 and 128\n\n    ds.normalizeZeroMeanZeroUnitVariance()\n    Nd4j.shuffle(ds.getFeatureMatrix, new Random(seed), 1) // shuffles rows in the ds.\n    Nd4j.shuffle(ds.getLabels, new Random(seed), 1) // shuffles labels accordingly\n\n val trainTest: SplitTestAndTrain = ds.splitTestAndTrain(splitTrainNum, new Random(seed))\n\n    // creating epoch dataset iterator\n val dsiterTr = new ListDataSetIterator(trainTest.getTrain.asList(), nbatch)\n val dsiterTe = new ListDataSetIterator(trainTest.getTest.asList(), nbatch)\n val epochitTr: MultipleEpochsIterator = new MultipleEpochsIterator(nepochs, dsiterTr)\n\n val epochitTe: MultipleEpochsIterator = new MultipleEpochsIterator(nepochs, dsiterTe)\n    //First convolution layer with ReLU as activation function\n val layer_0 = new ConvolutionLayer.Builder(6, 6)\n        .nIn(nChannels)\n        .stride(2, 2) // default stride(2,2)\n        .nOut(20) // # of feature maps\n        .dropOut(0.5)\n        .activation(\"relu\") // rectified linear units\n        .weightInit(WeightInit.RELU)\n        .build()\n\n    //First subsampling layer\n val layer_1 = new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n        .kernelSize(2, 2)\n        .stride(2, 2)\n        .build()\n\n    //Second convolution layer with ReLU as activation function\n val layer_2 = new ConvolutionLayer.Builder(6, 6)\n        .nIn(nChannels)\n        .stride(2, 2)\n        .nOut(50)\n        .activation(\"relu\")\n        .build()\n\n    //Second subsampling layer\n val layer_3 = new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n        .kernelSize(2, 2)\n        .stride(2, 2)\n        .build()\n\n    //Dense layer\n val layer_4 = new DenseLayer.Builder()\n        .activation(\"relu\")\n        .nOut(500)\n        .build()\n\n    // Final and fully connected layer with Softmax as activation function\n val layer_5 = new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)\n        .nOut(outputNum)\n        .weightInit(WeightInit.XAVIER)\n        .activation(\"softmax\")\n        .build()\n val builder: MultiLayerConfiguration.Builder = new NeuralNetConfiguration.Builder()\n        .seed(seed)\n        .iterations(iterations)\n        .miniBatch(true)\n        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n        .regularization(true).l2(0.0005)\n        .learningRate(0.01)\n        .list(6)\n            .layer(0, layer_0)\n            .layer(1, layer_1)\n            .layer(2, layer_2)\n            .layer(3, layer_3)\n            .layer(4, layer_4)\n            .layer(5, layer_5)\n    .backprop(true).pretrain(false)\n\n new ConvolutionLayerSetup(builder, numRows, numColumns, nChannels)\n val conf: MultiLayerConfiguration = builder.build()\n val model: MultiLayerNetwork = new MultiLayerNetwork(conf)\n\n    model.init()\n    model.setListeners(Seq[IterationListener](new ScoreIterationListener(listenerFreq)).asJava)\n    model.fit(epochitTr)\n\n val eval = new Evaluation(outputNum)\n while (epochitTe.hasNext) {\n val testDS = epochitTe.next(nbatch)\n val output: INDArray = model.output(testDS.getFeatureMatrix)\n        eval.eval(testDS.getLabels(), output)\n        }\n if (!saveNN.isEmpty) {\n        // model config\n        FileUtils.write(new File(saveNN + \".json\"), model.getLayerWiseConfigurations().toJson())\n        // model parameters\n val dos: DataOutputStream = new DataOutputStream(Files.newOutputStream(Paths.get(saveNN + \".bin\")))\n        Nd4j.write(model.params(), dos)\n        }\n    }\n```", "```py\ndef saveNN(model: MultiLayerNetwork, NNconfig: String, NNparams: String) = {\n    // save neural network config\n    FileUtils.write(new File(NNconfig), model.getLayerWiseConfigurations().toJson())\n    // save neural network parms\n val dos: DataOutputStream = new DataOutputStream(Files.newOutputStream(Paths.get(NNparams)))\n    Nd4j.write(model.params(), dos)\n}\n```", "```py\ndef loadNN(NNconfig: String, NNparams: String) = {\n    // get neural network config\n val confFromJson: MultiLayerConfiguration =                     \n    MultiLayerConfiguration.fromJson(FileUtils.readFileToString(new File(NNconfig)))\n\n    // get neural network parameters\n val dis: DataInputStream = new DataInputStream(new FileInputStream(NNparams))\n val newParams = Nd4j.read(dis)\n\n    // creating network object\n val savedNetwork: MultiLayerNetwork = new MultiLayerNetwork(confFromJson)\n    savedNetwork.init()\n    savedNetwork.setParameters(newParams)\n    savedNetwork \n    }\n```", "```py\ndef scoreModel(model: MultiLayerNetwork, ds: INDArray) = {\n    model.output(ds)\n}\n```", "```py\n\ndef aggImgScores2Business(scores: INDArray, alignedData: featureAndDataAligner ) = {\n    assert(scores.size(0) == alignedData.data.length, \"alignedData and scores length are different. They     must be equal\")\n\ndef getRowIndices4Business(mylist: List[String], mybiz: String): List[Int] = mylist.zipWithIndex.filter(x     => x._1 == mybiz).map(_._2)\n\ndef mean(xs: List[Double]) = xs.sum / xs.size\n    alignedData.getBusinessIds.distinct.map(x => (x, {\n val irows = getRowIndices4Business(alignedData.getBusinessIds, x)\n val ret = \n for(row <- irows) \n yield scores.getRow(row).getColumn(1).toString.toDouble\n        mean(ret)\n        }))\n    }\n```", "```py\npackage Yelp.Classifier\nimport Yelp.Preprocessor.CSVImageMetadataReader._\nimport Yelp.Preprocessor.featureAndDataAligner\nimport Yelp.Preprocessor.imageFeatureExtractor._\nimport Yelp.Evaluator.ResultFileGenerator._\nimport Yelp.Preprocessor.makeND4jDataSets._\nimport Yelp.Evaluator.ModelEvaluation._\nimport Yelp.Trainer.CNNEpochs._\nimport Yelp.Trainer.NeuralNetwork._\n\nobject YelpImageClassifier {\n def main(args: Array[String]): Unit = {\n        // image processing on training data\n val labelMap = readBusinessLabels(\"data/labels/train.csv\")\n val businessMap = readBusinessToImageLabels(\"data/labels/train_photo_to_biz_ids.csv\")\n val imgs = getImageIds(\"data/images/train/\", businessMap, \n        businessMap.map(_._2).toSet.toList).slice(0,20000) // 20000 images\n\n        println(\"Image ID retreival done!\")\n val dataMap = processImages(imgs, resizeImgDim = 256)\n        println(\"Image processing done!\")\n\n val alignedData = \n new featureAndDataAligner(dataMap, businessMap, Option(labelMap))()\n        println(\"Feature extraction done!\")\n\n        // training one model for one class at a time. Many hyperparamters hardcoded within\n val cnn0 = trainModelEpochs(alignedData, businessClass = 0, saveNN = \"models/model0\")\n val cnn1 = trainModelEpochs(alignedData, businessClass = 1, saveNN = \"models/model1\")\n val cnn2 = trainModelEpochs(alignedData, businessClass = 2, saveNN = \"models/model2\")\n val cnn3 = trainModelEpochs(alignedData, businessClass = 3, saveNN = \"models/model3\")\n val cnn4 = trainModelEpochs(alignedData, businessClass = 4, saveNN = \"models/model4\")\n val cnn5 = trainModelEpochs(alignedData, businessClass = 5, saveNN = \"models/model5\")\n val cnn6 = trainModelEpochs(alignedData, businessClass = 6, saveNN = \"models/model6\")\n val cnn7 = trainModelEpochs(alignedData, businessClass = 7, saveNN = \"models/model7\")\n val cnn8 = trainModelEpochs(alignedData, businessClass = 8, saveNN = \"models/model8\")\n\n    // processing test data for scoring\n val businessMapTE = readBusinessToImageLabels(\"data/labels/test_photo_to_biz.csv\")\n val imgsTE = getImageIds(\"data/images/test//\", businessMapTE,     \n        businessMapTE.map(_._2).toSet.toList)\n\n val dataMapTE = processImages(imgsTE, resizeImgDim = 128) // make them 256x256\n val alignedDataTE = new featureAndDataAligner(dataMapTE, businessMapTE, None)()\n\n        // creating csv file to submit to kaggle (scores all models)\n val Results = SubmitObj(alignedDataTE, \"results/ModelsV0/\")\n val SubmitResults = writeSubmissionFile(\"kaggleSubmitFile.csv\", Results, thresh = 0.9)\n        }\n    }\n>>>\n==========================Scores======================================\n Accuracy: 0.6833\n Precision: 0.53\n Recall: 0.5222\n F1 Score: 0.5261\n======================================================================\n```"]