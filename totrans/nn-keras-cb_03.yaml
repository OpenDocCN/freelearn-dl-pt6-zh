- en: Applications of Deep Feedforward Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting credit default
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting house prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorizing news articles into topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying common audio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting stock prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned about building a neural network and the
    various parameters that need to be tweaked to ensure that the model built generalizes
    well. Additionally, we learned about how neural networks can be leveraged to perform
    image analysis using MNIST data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn how neural networks can be used for prediction
    on top of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Structured dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical output prediction
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous output prediction
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, we will also be learning about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a custom loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning higher weights for certain classes of output over others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning higher weights for certain rows of a dataset over others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging a functional API to integrate multiple sources of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will learn about all the preceding by going through the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting a credit default
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting house prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorizing news articles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting stock prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying common audio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, you should note that these applications are provided only for you to
    understand how neural networks can be leveraged to analyze a variety of input
    data. Advanced ways of analyzing text, audio, and time-series data will be provided
    in later chapters about the  Convolutional Neural Network and the Recurrent Neural
    Network.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting credit default
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the financial services industry, one of the major sources of losing out on
    revenues is the default of certain customers. However, a very small percentage
    of the total customers default. Hence, this becomes a problem of classification
    and, more importantly, identifying rare events.
  prefs: []
  type: TYPE_NORMAL
- en: In this case study, we will analyze a dataset that tracks certain key attributes
    of a customer at a given point in time and tries to predict whether the customer
    is likely to default.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider the way in which you might operationalize the predictions from
    the model we build. Businesses might want to have a special focus on the customers
    who are more likely to default—potentially giving them alternative payment options
    or  a way to reduce the credit limit, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy that we''ll adopt to predict default of a customer is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Objective**: Assign a high probability to the customers who are more likely
    to default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mea****surement** **criterion**: Maximize the number of customers who have
    actually defaulted when we consider only the top 10% of members by decreasing
    the default probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The strategy we will be adopting to assign a probability of default for each
    member will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the historic data of all members.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Understand the variables that can help us to identify a customer who is likely
    to default:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Income-to-debt ratio is a very good indicator of whether a member is likely
    to default.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be extracting a few other variables similar to that.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the previous step, we created the input variables; now, let''s go ahead
    and create the dependent variable:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will extract the members who have actually defaulted in the next 2 years
    by first going back in history and then looking at whether members defaulted in
    the next 2 years
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to have a time lag, as it might not give us any levers to change
    the outcome if we do not have a time gap between when a member is likely to default
    and the date of prediction.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that the  outcome is binary, we will minimize the binary cross-entropy
    loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model shall have a hidden layer that connects the input layer and the output
    layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We shall calculate the number of the top 10% probability members who have actually
    defaulted, in the test dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that we assume that test data is representative here, as we are not in
    a position to assess the performance of a model on unseen dataset without productionalizing
    the model. We shall assume that the model's performance on an unseen dataset is
    a good indicator of how well the model will perform on future data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll code up the strategy as follows (Please refer to the `Credit default
    prediction.ipynb` file in GitHub while implementing the code):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages and the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first three rows of the dataset we downloaded are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2c32cdc-9ecb-4a70-89f9-1460501a5bc3.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot is a subset of variables in the original dataset. The
    variable named `Defaultin2yrs` is the output variable that we need to predict,
    based on the rest of the variables present in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summarize the dataset to understand the variables better:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you look at the output you will notice the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Certain variables have a small range (`age`), while others have a much bigger
    range (`Income`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certain variables have missing values (`Income`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certain variables have outlier values (`Debt_income_ratio`). In the next steps,
    we will go ahead and correct all the issues flagged previously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Impute missing values in a variable with the variable''s median value:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we excluded the first variable, as it is the variable
    that we are trying to predict, and then we imputed the missing values in the rest
    of the variables (provided the variable does have a missing value).
  prefs: []
  type: TYPE_NORMAL
- en: 'Cap each variable to its corresponding 95^(th) percentile value so that we
    do not have outliers in our input variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have identified the 95^(th) percentile value of each
    variable, created a new variable that has a value of one if the row contains an
    outlier in the given variable, and zero otherwise. Additionally, we have capped
    the variable values to the 95^(th) percentile value of the original value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we summarize the modified data, we notice that except for the `Debt_income_ratio`
    variable every other variable does not seem to have outliers anymore. Hence, let''s
    constrain `Debt_income_ratio` further to have a limited range of output, by capping
    it at the 80^(th) percentile value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize all variables to the same scale for a value between zero and one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are limiting all the variables to a similar range
    of output, which is between zero and one, by dividing each input variable value
    with the input variable column's maximum value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the input and the output dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the datasets into train and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding step, we use the `train_test_split` method to split the input
    and output arrays into train and test datasets where the test dataset has 30%
    of the total number of data points in the input and the corresponding output arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the datasets are created, let''s define the neural network model,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/faf052a6-457c-4e04-9b97-131f47334ab3.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous architecture, we connect the input variables to a hidden layer
    that has 1,000 hidden units.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile the model. We shall employ binary cross entropy, as the output variable
    has only two classes. Additionally, we will specify that `optimizer` is an `adam`
    optimization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The variation of training and test loss, accuracy over increasing epochs is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6314265-8680-40d4-a66a-75d2da8f9876.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Make predictions on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Check for the number of actual defaulters that are captured in the top 10%
    of the test dataset when ranked in order of decreasing probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we concatenated the predicted values with actual values
    and then sorted the dataset by probability. We checked the actual number of defaulters
    that are captured in the top 10% of the test dataset (which is the first 4,500
    rows).
  prefs: []
  type: TYPE_NORMAL
- en: We should note that there are 1,580 actual defaulters that we have captured
    by going through the 4,500 high-probability customers. This is a good prediction,
    as on average only 6% of the total customers default. Hence, in this case, ~35%
    of customers who have a high probability of default actually defaulted.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we have learned about the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Imputin****g** **m****issing** **values**: We have learned that one of the
    ways to impute the missing values of a variable is by replacing the missing values
    with the median of the corresponding variable. Other ways to deal with the missing
    values is by replacing them with the mean value, and also by replacing the missing
    value with the mean of the variable''s value in the rows that are most similar
    to the row that contains a missing value (this technique is called i**dentifying
    the K-Nearest Neighbours**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Capping the outlier values**: We have also learned that one way to cap the
    outliers is by replacing values that are above the 95^(th) percentile value with
    the 95^(th) percentile value. The reason we performed this exercise is to ensure
    that the input variable does not have all the values clustered around a small
    value (when the variable is scaled by the maximum value, which is an outlier).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling dataset**: Finally, we scaled the dataset so that it can then be
    passed to a neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning weights for classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we assign equal weightage to the rows that belong to a defaulter and the
    rows that belong to a non-defaulter, potentially the model can fine-tune for the
    non-defaulters. In this section, we will look into ways of assigning a higher
    weightage so that our model classifies defaulters better.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we assigned the same weightage for each class; that
    is, the categorical cross entropy loss is the same if the magnitude of difference
    between actual and predicted is the same, irrespective of whether it is for the
    prediction of a default or not a default.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the scenario further, let''s consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Scenario** | **Probability of default** | **Actual value of default** |
    **Cross entropy loss** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | *0.2* | *1* | *1*log(0.2)* |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | *0.8* | *0* | *(1-0)*log(1-0.8)* |'
  prefs: []
  type: TYPE_TB
- en: In the preceding scenario, the cross-entropy loss value is just the same, irrespective
    of the actual value of default.
  prefs: []
  type: TYPE_NORMAL
- en: However, we know that our objective is to capture as many actual defaulters
    as possible in the top 10% of predictions when ranked by probability.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, let's go ahead and assign a higher weight of loss (a weight of *100*)
    when the actual value of default is *1* and a lower weightage (a weight of *1*)
    when the actual value of default is *0*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous scenario now changes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Scenario** | **Probability of default** | **Actual value of default** |
    **Cross entropy loss** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | *0.2* | *1* | *100*1*log(0.2)* |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | *0.8* | *0* | *1*(1-0)*log(1-0.8)* |'
  prefs: []
  type: TYPE_TB
- en: Now, if we notice the cross entropy loss, it is much higher when the predictions
    are wrong when the actual value of default is *1* compared to the predictions
    when the actual value of default is *0*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood the intuition of assigning weightages to classes,
    let's go ahead and assign weights to output classes in the credit default dataset.
  prefs: []
  type: TYPE_NORMAL
- en: All the steps performed to build the dataset and model remain the same as in
    the previous section, except for the model-fitting process.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The model fitting process is done by following these steps (Please refer to
    `Credit default prediction.ipynb` file in GitHub while implementing the code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in the preceding code snippet, we created a dictionary with the weights
    that correspond to the distinct classes in output that is then passed as an input
    to the `class_weight` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding step ensures that we assign a weightage of `100` to calculating
    the loss value when the actual outcome is `1` and a weightage of `1` when calculating
    the loss value when the actual outcome is `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variation of accuracy and loss values over increasing epochs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/319b3dc9-49bd-46b1-b089-16157ec61b44.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the accuracy values are much lower in this iteration, as we are predicting
    more number of data points to have a value 1 than in the scenario of equal weightage
    to both classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is fitted, let''s proceed and check for the number of actual
    defaulters that are captured in the top 10% of predictions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You notice that compared to the previous scenario of 1,580 customers being captured
    in the the top 10%, we have 1,640 customers captured in the top 10% in this scenario,
    and thus a better outcome for the objective we have set where we have captured
    36% of all defaulters in top 10% of high probable customers in this scenario,
    when compared to 35% in the previous scenario.
  prefs: []
  type: TYPE_NORMAL
- en: It is not always necessary that accuracy improves as we increase class weights.
    Assigning class weights is a mechanism to give higher weightage to the prediction
    of our interest.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting house prices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous case study, we had an output that was categorical. In this case
    study, we shall look into an output that is continuous in nature, by trying to
    predict the price of a house where 13 variables that are likely to impact the
    house price are provided as input.
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to minimize the error by which we predict the price of a house.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given that the objective is to minimize error, let's define the error that we
    shall be minimizing—we should ensure that a positive error and a negative error
    do not cancel out each other. Hence, we shall minimize the absolute error. An
    alternative of this is to minimize the squared error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have fine-tuned our objective, let''s define our strategy of solving
    this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Normalize the input dataset so that all variables range between zero to one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the given data to train and test datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize the hidden layer that connects the input of 13 variables to the output
    of one variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compile the model with the Adam optimizer, and define the loss function to minimize
    as the mean absolute error value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a prediction on the test dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the error in the prediction on the test dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have defined our approach, let's go ahead and perform it in code
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import the relevant dataset (Please refer to the `Predicting house price.ipynb`
    file in GitHub while implementing the code and for the recommended dataset):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the input and output dataset so that all variables have a range from
    zero to one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have normalized the test dataset with the maximum value in the
    train dataset itself, as we should not be using any of the values from the test
    dataset in the model-building process. Additionally, note that we have normalized
    both the input and the output values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the input and output datasets are prepared, let''s proceed and define
    the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/096001a6-4571-48cb-9e30-93b7c2c7ef5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we performed  `L1`  regularization in the model-building process so
    that the model does not overfit on the training data (as the number of data points
    in the training data is small).
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile the model to minimize the mean absolute error value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the mean absolute error on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We should note that the mean absolute error is *~6.7* units.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will vary the loss function and add custom weights to
    see whether we can improve upon the mean absolute error values.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the custom loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we used the predefined mean absolute error `loss` function
    to perform the optimization. In this section, we will learn about defining a custom
    loss function to perform optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The custom loss function that we shall build is a modified mean squared error
    value, where the error is the difference between the square root of the actual
    value and the square root of the predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The custom loss function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have defined the `loss` function, we will be reusing the same input
    and output datasets that we prepared in previous section, and we will also be
    using the same model that we defined earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s compile the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, note that we defined the `loss` value as the custom loss
    function that we defined earlier—`loss_function`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Once we fit the model, we will note that the mean absolute error is *~6.5* units,
    which is slightly less than the previous iteration where we used the `mean_absolute_error` loss
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Categorizing news articles into topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous case studies, we analyzed datasets that were structured, that
    is, contained variables and their corresponding values. In this case study, we
    will be working on a dataset that has text as input, and the expected output is
    one of the 46 possible topics that the text is related to.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the intuition of performing text analysis, let's consider the
    Reuters dataset, where each news article is classified into one of the 46 possible
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will adopt the following strategy to perform our analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Given that a dataset could contain thousands of unique words, we will shortlist
    the words that we shall consider.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this specific exercise, we shall consider the top 10,000 most frequent words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An alternative approach would be to consider the words that cumulatively constitute
    80% of all words within a dataset. This ensures that all the rare words are excluded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the words are shortlisted, we shall one-hot-encode the article based on
    the constituent frequent words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, we shall one-hot-encode the output label.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each input now is a 10,000-dimensional vector, and the output is a 46-dimensional
    vector:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will divide the dataset into train and test datasets. However, in code, you
    will notice that we will be using the in-built dataset of `reuters` in Keras that
    has built-in function to identify the top `n` frequent words and split the dataset
    into train and test datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Map the input and output with a hidden layer in between.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will perform softmax at the output layer to obtain the probability of the
    input belonging to one of the 46 classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that we have multiple possible outputs, we shall employ a categorical
    cross entropy loss function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We shall compile and fit the model to measure its accuracy on a test dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll code up the strategy defined previously as follows (please refer to
    the `Categorizing news articles into topics.ipynb` file in GitHub while implementing
    the code):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the dataset :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we loaded data from the `reuters` dataset that
    is available  in Keras. Additionally, we consider only the `10000` most frequent
    words in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspect the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'A sample of the loaded training dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40f57912-d7fe-41a3-8840-03871083ff15.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the numbers in the preceding output represent the index of words that
    are present in the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can extract the index of values as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Vectorize the input. We will convert the text into a vector in the following
    way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One-hot-encode the input words—resulting in a total of `10000` columns in the
    input dataset.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If a word is present in the given text, the column corresponding to the word
    index shall have a value of one and every other column shall have a value of zero.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Repeat the preceding step for all the unique words in a text. If a text has
    two unique words, there will be a total of two columns that have a value of one,
    and every other column will have a value of zero:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding function, we initialized a variable that is a zero matrix and
    imputed it with a value of one, based on the index values present in the input
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In the following code, we are converting the words into IDs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'One-hot-encode the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code converts each output label into a vector that is `46` in
    length, where one of the `46` values is one and the rest are zero, depending on
    the label's index value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model and compile it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/849f6764-c6d9-4993-8ffd-7319ebba6bf7.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that while compiling, we defined `loss` as `categorical_crossentropy` as
    the output in this case is categorical (multiple classes in output).
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in a model that has 80% accuracy in classifying
    the input text into the right topic, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ef23cd1-8063-4ac8-8614-e647abd39c7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Classifying common audio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we have understood the strategy to perform modeling
    on a structured dataset and also on unstructured text data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will be learning about performing a classification exercise
    where the input is raw audio.
  prefs: []
  type: TYPE_NORMAL
- en: The strategy we will be adopting is that we will be extracting features from
    the input audio, where each audio signal is represented as a vector of a fixed
    number of features.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways of extracting features from an audio—however, for this
    exercise, we will be extracting the **Mel Frequency Cepstral Coefficients** (**MFCC**)
    corresponding to the audio file.
  prefs: []
  type: TYPE_NORMAL
- en: Once we extract the features, we shall perform the classification exercise in
    a way that is very similar to how we built a model for MNIST dataset classification—where
    we had hidden layers connecting the input and output layers.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will be performing classification on top of an
    audio dataset where there are ten possible classes of output.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy that we defined previously is coded as follows (Please refer to
    the `Audio classification.ipynb` file in GitHub while implementing the code):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract features for each audio input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we defined a function that takes `file_name` as input,
    extracts the `40` MFCC corresponding to the audio file, and returns the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the input and the output dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we loop through one audio file at a time, extracting
    its features and storing it in the input list. Similarly, we will be storing the
    output class in the output list. Additionally, we will convert the output list
    into a categorical value that is one-hot-encoded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `pd.get_dummies` method works very similar to the `to_categorical` method
    we used earlier; however, `to_categorical` does not work on text classes (it works
    on numeric values only, which get converted to one-hot-encoded values).
  prefs: []
  type: TYPE_NORMAL
- en: 'Build the model and compile it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The summary of the preceding model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23de187e-b380-4973-9db6-ef8af7130d91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Create the train and test datasets and then fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Once the model is fitted, you will notice that the model has 91% accuracy in
    classifying audio in the right class.
  prefs: []
  type: TYPE_NORMAL
- en: Stock price prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we learned about performing audio, text, and structured
    data analysis using neural networks. In this section, we will learn about performing
    a time-series analysis using a case study of predicting a stock price.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To predict a stock price, we will perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Order the dataset from the oldest to the newest date.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the first five stock prices as input and the sixth stock price as output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Slide it across so that in the next data point the second to the sixth data
    points are input and the seventh data point is the output, and so on, till we
    reach the final data point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given that it is a continuous number that we are predicting, the `loss` function
    this time shall be the mean squared error value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, we will also try out the scenario where we integrate the text
    data into the historic numeric data to predict the next day's stock price.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The above strategy is coded as follows (please refer to `Chapter 3  - stock
    price prediction.ipynb` file in GitHub while implementing the code and for the
    recommended dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages and the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare the dataset where the input is the previous five days'' stock price
    value and the output is the stock price value on the sixth day:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare the train and test datasets, build the model, compile it, and fit it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the model and compile it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code results in a summary of model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19ad486e-b866-4549-8fc7-a842143634a8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Once we fit the model, we should note that the mean squared error value *~$360*
    in predicting the stock price or ~$18 in predicting the stock price.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there is a pitfall in predicting a stock price this way. However,
    that will be dealt with in the chapter on RNN applications.
  prefs: []
  type: TYPE_NORMAL
- en: For now, we will focus on learning how neural networks can be useful in a variety
    of different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will understand the ways in which we can integrate the
    numeric data with the text data of news headlines in a single model.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging a functional API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will continue to improve the accuracy of the stock price
    prediction by integrating historical price points data with the most-recent headlines
    of the company for which we are predicting the stock price.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy that we will adopt to integrate data from multiple sources—structured
    (historical price) data and unstructured (headline) data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We will convert the unstructured text into a structured format in a manner that
    is similar to the way we categorized news articles into topics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will pass the structured format of text through a neural network and extract
    the hidden layer output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we pass the hidden layer output to the output layer, where the output
    layer has one node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a similar manner, we pass the input historical price data through the neural
    network to extract the hidden layer values, which then get passed to the output
    layer that has one unit in output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We multiply the output of each of the individual neural network operations to
    extract the final output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The squared error value of the final output shall now be minimized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous strategy is coded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s fetch the headline data from the API provided by the Guardian, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Once `titles` and `dates` are extracted, we shall preprocess the data to convert
    the `date` values to a `date` format, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the most recent headline for every date on which we are trying
    to predict the stock price, we will integrate the two data sources, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the datasets are merged, we will go ahead and normalize the text data
    so that we remove the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert all words in a text into lowercase so that the words like `Text` and
    `text` are treated the same.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove punctuation so that words such as `text.` and `text` are treated the
    same.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Remove stop words such as `a`, `and`, `the`, which do not add much context
    to the text:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace all the null values in the `title` column with a hyphen `-`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have preprocessed the text data, let''s assign an ID to each word.
    Once we have finished this assignment, we can perform text analysis in a way that
    is very similar to what we did in the *Categorizing news articles into topics*
    section, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that we have encoded all the words, let''s replace them with their corresponding
    text in the original text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have encoded the texts, we understand the way in which we will integrate
    the two data sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we shall prepare the training and test datasets, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Typically, we would use a functional API when there are multiple inputs or multiple
    outputs expected. In this case, given that there are multiple inputs, we will
    be leveraging a functional API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, a functional API takes out the sequential process of building
    the model and is performed as follows. Take the input of the vectorized documents
    and extract the output from it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, note that we have not used the sequential modeling process
    but defined the various connections using the `Dense` layer.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the input has a shape of `2406`, as there are `2406` unique words
    that remain after the filtering process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the input of the previous `5` stock prices and build the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We will multiply the output of the two inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have defined the output, we will build the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that, in the preceding step, we used the `Model` layer to define the input
    (passed as a list) and the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67dff3a9-1e83-4591-9096-e379635afd33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A visualization of the preceding output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e66fdee9-4b95-4101-b450-29712e649413.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Compile and fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code results in a mean squared error of *~5000* and clearly shows
    that the model overfits, as the training dataset loss is much lower than the test
    dataset loss.
  prefs: []
  type: TYPE_NORMAL
- en: Potentially, the overfitting is a result of a very high number of dimensions
    in the vectorized text data. We will look at how we can improve upon this in [Chapter
    11](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml), *Building a Recurrent Neural
    Network*.
  prefs: []
  type: TYPE_NORMAL
- en: Defining weights for rows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Predicting house prices* recipe, we learned about defining a custom
    loss function. However, we are not in a position yet to assign a higher weightage
    for certain rows over others. (We did a similar exercise for  a credit default
    prediction case study where we assigned higher weightage to one class over the
    other; however, that was a classification problem, and the current problem that
    we are solving is a continuous variable-prediction problem.)
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will define weights for each row and then pass them to the
    `custom_loss` function that we will define.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue working on the same dataset that we analyzed in the *Stock
    price prediction* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform specifying weightages at a row level, we will modify our train and
    test datasets in such a way that the first `2100` data points after ordering the
    dataset are in the train dataset and the rest are in the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'A row in input shall have a higher weight if it occurred more recently and
    less weightage otherwise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block assigns lower weightage to initial data points and
    a higher weightage to data points that occurred more recently.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have defined the weights for each row, we will include them in the
    custom loss function. Note that in this case our custom loss function shall include
    both the predicted and actual values of output as well as the weight that needs
    to be assigned to each row.
  prefs: []
  type: TYPE_NORMAL
- en: 'The partial method enables us to pass more variables than just the actual and
    predicted values to the custom loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'To pass `weights` to the `custom_loss` function, we shall be using the partial
    function to pass both `custom_loss` and `weights` as a parameter in step 7\. In
    the code that follows, we are defining the  `custom_loss`  function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that the model we are building has two inputs, input variables and weights
    corresponding to each row, we will first define the `shape` input of the two as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have defined the inputs, let''s initialize `model` that accepts
    the two inputs as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have initialized `model`, we will define the optimization function
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding scenario, we specify that we need to minimize the `custom_loss_4`
    function and also that we provide an additional variable (`weights_tensor`) to
    the custom loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, before fitting the model, we will also provide `weights` for each
    row corresponding to the test dataset. Given that we are predicting these values,
    it is of no use to provide a low weightage to certain rows over others, as the
    test dataset is not provided to model. However, we will only specify this to make
    a prediction using the model we defined (which accepts two inputs):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we specify the `weights`  of test data, we will go ahead and fit the model
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The preceding results in a test dataset loss that is very different to what
    we saw in the previous section. We will look at the reason for this in more detail
    in the [Chapter 11](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml), *Building a Recurrent
    Neural Network* chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You need to be extremely careful while implementing the preceding model, as
    it has a few pitfalls. However, in general, it is advised to implement models
    to predict stock price movements only after sufficient due diligence.
  prefs: []
  type: TYPE_NORMAL
