- en: Applying Autoencoder Neural Networks Using Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Autoencoder networks belong to the unsupervised learning category of methods,
    where labeled target values are not available. However, since autoencoders often
    use targets that are some form of input data, they can also be called self-supervised
    learning methods. In this chapter, we will learn how to apply autoencoder neural
    networks using Keras. We will cover three applications of autoencoders: dimension
    reduction, image denoising, and image correction. The examples in this chapter
    will use images of fashion items, images of numbers, and pictures containing people.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Types of autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimension reduction autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denoising autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image correction autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Autoencoder neural networks consist of two main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The first part is called the encoder, which reduces the dimensions of the input
    data. Generally, this is an image. When data from an input image is passed through
    a network that leads to a lower dimension, the network is forced to extract only
    the most important features of the input data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second part of the autoencoder is called the decoder and it tries to reconstruct
    the original data from whatever is available from the output of the encoder. The
    autoencoder network is trained by specifying what output this network should try
    to match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s consider some examples where we will use image data. If the output that''s
    specified is the same image that was given as input, then after training, the
    autoencoder network is expected to provide an image with a lower resolution that
    retains the key features of the input image but misses some finer details that
    were part of the original input image. This type of autoencoder can be used for
    dimension reduction applications. Since autoencoders are based on neural networks
    that are able to capture non-linearity in data, they have superior performance
    compared to methods that only use linear functions. The following diagram shows
    the encoder and decoder parts of autoencoder networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b47cb56-2078-4095-8607-d50b35bbfe3c.png)'
  prefs: []
  type: TYPE_IMG
- en: If we train autoencoders so that the input image contains some noise or non-clarity
    and the output as the same image but without any noise, then we can create denoising
    autoencoders. Similarly, if we train autoencoders with such input/output images
    where we have images with and without glasses, or with and without a mustache,
    and so on, we can create networks that help with image correction/modification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will look at three separate examples of how to use an autoencoder:
    using dimension reduction, image denoising, and image correction. We will start
    by using autoencoders for dimension reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: Dimension reduction autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use fashion-MNIST data, specify the autoencoder model
    architecture, compile the model, fit the model, and then reconstruct the images.
    Note that fashion-MNIST is part of the Keras library.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST fashion data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will continue to use the Keras and EBImage libraries. The code for reading
    the fashion-MNIST data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, the training data has 60,000 images and the test data has 10,000 images
    of fashion items. Since we will be using an unsupervised learning approach for
    this example, we will not use the labels that are available for the train and
    test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We store the training image data in `trainx` and test image data in `testx`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The first 64 images of fashion items can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a0f5048-5008-4987-95a7-7adc47b687b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will reshape the image data into a suitable format, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have also divided `trainx` and `testx` by 255 to change the range of
    values that are between 0-255 to a range between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To specify the encoder model architecture, we will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, for the input to the encoder, we specify the input layer so that it's
    28 x 28 x 1 in size. Two convolutional layers, one with 8 filters and another
    with 4 filters, are used. Activation functions for both of these layers use **rectified
    linear units** (**relus**). The convolutional layer includes `padding = 'same'`,
    which retains the height and width of the input at the time of the output. For
    example, after the first convolution layer, the output has 28 x 28 as its height
    and width. Each convolution layer is followed by pooling layers. After the first
    pooling layer, the height and width change to 14 x 14, and, after the second pooling
    layer, it changes to 7 x 7\. The output of the encoder network in this example
    is 7 x 7 x 4.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To specify the decoder model architecture, we will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, the encoder model has become the input for the decoder model. For the
    decoder network, we use a similar structure, with the first convolutional layer
    having 4 filters and the second convolutional layer having 8 filters. In addition,
    instead of pooling layers, we now use up-sampling layers. The first upsampling
    layer changes the height and width to 14 x 14 and the second upsampling layer
    restores it to the original height and width of 28 x 28\. In the last layer, we
    make use of the sigmoid activation function, which ensures that the output values
    remain between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The autoencoder model and the summary of the model showing the output shape
    and the number of parameters for each layer is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, the autoencoder model has five convolutional layers, two maximum pooling
    layers, and two upsampling layers, apart from the input layer. Here, the total
    number of parameters in this autoencoder model is 889.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling and fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will compile and fit the model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, we compile the model using mean squared error as the loss function and
    specify `adam` as the optimizer. For training the model, we will make use of `trainx `as
    the input and output. We'll use `textx` for validation. We fit the model with
    a batch size of 32 and use 20 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the plot of the loss values for the train and validation
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76f4d73c-4630-49ef-b704-94b3edda787c.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding plot shows good convergence and doesn't show any signs of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstructed images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To obtain reconstructed images, we use `predict_on_batch` to predict the output
    using the autoencoder model. We do this with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The first five fashion images from the training data (first row) and the corresponding
    reconstructed images (second row) are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8c31f06-520c-4258-a0b7-9bdb92d08ee3.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, as expected, the reconstructed images are seen to capture key features
    of the training images. However, it ignores certain finer details. For example,
    the logos that are more clearly visible in the original training images are blurred
    in the reconstructed images.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also take a look at the plot of the original and reconstructed images
    using images from the test data. For this, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image shows the original images (first row) and reconstructed
    images (second row) using the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2fc374d-bcf1-4380-b27f-c35a2ded9a8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the reconstructed images behave as they did previously for the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we have used MNIST fashion data to build an autoencoder network
    that helps reduce the dimensions of the images by keeping the main features and
    removing the features that involve finer details. Next, we will look at another
    variant of the autoencoder model that helps remove noise from images.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In situations where input images contain unwanted noise, autoencoder networks
    can be trained to remove such noise. This is achieved by providing images with
    noise as input and providing a clean version of the same image as output. The
    autoencoder network is trained so that the output of the autoencoder is as close
    to the output image as possible.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will make use of MNIST data that''s available in the Keras package to illustrate
    the steps that are involved in creating a denoising autoencoder network. MNIST
    data can be read using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The structure of the MNIST data indicates that it contains train and test data,
    along with the respective labels. The training data has 60,000 images of digits
    from 0 to 9\. Similarly, the test data has 10,000 images of digits from 0 to 9\.
    Although each image has a corresponding label identifying the image, in this example,
    the data for labels isn't required and so we will ignore this information.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be storing training images in `trainx` and the test images in `testx`.
    To do this, we will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image shows a plot of 64 images in 8 rows and 8 columns based
    on images of digits between 0 and 9 from MNIST:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a795fbe1-fb3c-48ff-8367-9667b7b0a8fb.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding plot shows handwritten digits in various writing styles. We will
    reshape this image data in the required format and add random noise to it.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will reshape images in the required format using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, we've reshaped the training data so that it's 60,000 x 28 x 28 x 1 in
    size and reshaped the test data so that it's 10,000 x 28 x 28 x 1 in size. We
    also divided the pixel values that are between 0 and 255 by 255 to obtain a new
    range that is between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Adding noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To add noise to the training images, we need to obtain 60,000 × 28 × 28 random
    numbers between 0 and 1 using uniform distribution using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we're reshaping the random numbers that were generated using uniform distribution
    to match the dimensions of the matrix that we have for the training images. The
    results are plotted in the form of images that show resulting images containing
    noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows the images containing noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4569d41-9dba-4c13-a702-617696522fcc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The images depicting noise are added to the images that are stored in `trainx`.
    We need to divide this by 2 to keep the resulting `trainn` values between 0 and
    1\. We can use the following code to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The first 64 training images, along with their noise, are shown in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/385dc121-daff-4c4f-82b2-230468ef3b50.png)'
  prefs: []
  type: TYPE_IMG
- en: Although noise is added to the original handwritten digits, the digits are still
    readable. The main objective of using a denoising autoencoder is to train a network
    that retains the handwritten digits and removes noise from the images.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will repeat the same steps for the test data using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have added noise to test images and stored them in `testn`. Now, we
    can specify the encoder architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code that''s used for the encoder network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, the input layer is specified to be 28 x 28 x 1 in size. We use two convolution
    layers with 32 filters each and a rectifier linear unit as the activation function.
    Each convolution layer is followed by pooling layers. After the first pooling
    layer, the height and width change to 14 x 14, and after the second pooling layer,
    this changes to 7 x 7\. The output of the encoder network in this example has
    7 x 7 x 32 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the decoder network, we keep the same structure, except that, instead of
    pooling layers, we use upsampling layers. We can use the following code to do
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the first upsampling layer changes the height and width
    to 14 x 14 and the second upsampling layer restores it to the original height
    and width of 28 x 28\. In the last layer, we use a sigmoid activation function,
    which ensures that the output values remain between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we can specify the autoencoder network. The autoencoder''s model and summary
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding summary of the autoencoder network, we can see that there
    are 28,353 parameters in total. Next, we will compile this model using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: For denoising autoencoders, the`bianary_crossentropy `loss function performs
    better than other options.
  prefs: []
  type: TYPE_NORMAL
- en: When compiling the autoencoder model, we will use `binary_crossentropy` for
    the loss function since the input values are between 0 and 1\. For the optimizer,
    we will use `adam`. After compiling the model, we are ready to fit it.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train the model, we use images with noise stored in `trainn` as input and
    images without noise stored in `trainx` as output. The code that''s used to fit
    the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we also use `testn` and `testx` to monitor validation errors. We will
    run 100 epochs with a batch size of 128\. After network training is completed,
    we obtain the loss values for the train and test data using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The loss for the training and test data is 0.0743 and 0.0739, respectively.
    The closeness of the two numbers indicates the lack of an overfitting problem.
  prefs: []
  type: TYPE_NORMAL
- en: Image reconstruction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After fitting the model, we can reconstruct images using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we have used `ae_model` to reconstruct the images by
    providing images with noise contained in `trainn`. As shown in the following image,
    we have plotted the first 64 reconstructed images to see if the noisy images become
    clearer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/639dcb99-aa19-45ac-b4c9-0c984f5ae639.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can observe that the autoencoder network has successfully
    removed noise. We can also reconstruct the images for the test data with the help
    of `ae_model` using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting images for the first 64 handwritten digits in the test data are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cdb8d104-dd40-4bbf-90f1-f12906fd341a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can observe that the denoising autoencoder does a decent job of removing
    noise from the images of 0 to 9 digits. To look more closely at the model''s performance,
    we can plot the first image in the test data, the corresponding image with noise,
    and the reconstructed image after noise removal, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8631120a-b2aa-4b7e-bf32-bc466e01fb34.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, the first image is the original image, while the
    second image is the one that's obtained after adding noise. The autoencoder was
    provided the second image as input and the results that were obtained from the
    model (third image) were made to match the first image. Here, we can see that
    the denoising autoencoder network helps remove noise. Note that the third image
    is unable to retain some of the finer details of the original image that we can
    see in the first image. For example, in the original image, seven appears to be
    slightly thicker at the beginning and toward the lower part compared to the third
    image. However, it does successfully extract the overall pattern of seven from
    the image containing digit seven with noise.
  prefs: []
  type: TYPE_NORMAL
- en: Image correction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this third application, we will go over an example where we''ll develop
    an autoencoder model to remove certain artificially created marks on various pictures.
    We will use 25 images containing a black line across the picture. The code for
    reading the image files and carrying out the related processing is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we read images with `.jpeg` extensions from the `peoplex`
    folder and resize these images so that they have a height and width of 128 x 128\.
    We also update the dimensions to 128 x 128 x 3 since all the images are color
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Images that need correction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the following code to combine the 25 images and then plot them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we save the data involving all 25 images after combining them into `trainx`.
    Looking at the structure of `tranix`, we can see that, after combining the image
    data, the dimensions now become 128 x 128 x 3 x 16\. In order to change this to
    the required format of 16 x 128 x 128 x 3, we use the `aperm` function. Then,
    we plot all 25 images. Note that if the images are plotted with rotation, they
    can be adjusted to the correct orientation very easily on any computer. The following
    are the 25 pictures, with a black line across all the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89a138f3-655a-4d24-8728-7160f9b4e5d4.png)'
  prefs: []
  type: TYPE_IMG
- en: The autoencoder model in this application will use these images with a black
    line as input and will be trained so that the black lines are removed.
  prefs: []
  type: TYPE_NORMAL
- en: Clean images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will also read the same 25 images without the black line and save them in
    `trainy`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, after resizing and changing dimensions, we are combining the images,
    just like we did previously. We also need to make some adjustments to the dimensions
    to obtain the required format. Next, we will plot all 25 clean images, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50fd289c-9b2f-4f3c-92d4-3571b323aa59.png)'
  prefs: []
  type: TYPE_IMG
- en: At the time of training the autoencoder network, we will use these clean images
    as output. Next, we will specify the encoder model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the encoder model, we will use three convolutional layers with 512, 512,
    and 256 filters, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Here, the encoder network is 16 x 16 x 256 in size. We will keep the other features
    similar to the encoder models that we used in the previous two examples. Now,
    we will specify the decoder architecture of the autoencoder network.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the decoder model, the first three convolutional layers have 256, 512,
    and 512, filters, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used upsampling layers. In the last convolutional layer, we made use
    of a sigmoid activation function. In the last convolutional layer, we used three
    filters since we are making use of color images. Finally, the output of the decoder
    model has 128 x 128 x 3 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling and fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we can compile and fit the model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we compile the autoencoder model using mean squared error
    as the loss function and specify `adam` as the optimizer. We use `trainx`, which
    contains images with a black line across them, as input to the model and `trainy`,
    which contains clean images, as output that the model tries to match. We specify
    the number of epochs as 100 and use a batch size of 128\. Using a validation split
    of 0.2 or 20%, we will use 20 images out of 25 for training and 5 images out of
    25 for computing validation errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows the mean square error for 100 epochs for the training
    and validation images for `model_three`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82f0b487-14ef-4626-bac6-cde615926fd0.png)'
  prefs: []
  type: TYPE_IMG
- en: The plot for the mean square error shows that there is an improvement in model
    performance based on the training and validation data as the model training proceeds.
    We can also see that, between about 80 and 100 epochs, the model's performance
    becomes approximately flat. In addition to this, it's suggested that increasing
    the number of epochs isn't likely to improve model performance any further.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstructing images from training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we can reconstruct images from the training data using the model that
    we have obtained. To do this, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we''re using `predict_on_batch` to reconstruct the images
    after feeding `trainx`, which contains images with black lines across them. All
    25 reconstructed images can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05fe2408-d189-4377-a1be-5debedec0717.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding plot, it can be seen that the autoencoder model has learned
    to remove the black lines from the input images. The pictures are somewhat blurred
    since the autoencoder model tries to output only the main features from the images
    and misses certain details.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstructing images from new data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test the autoencoder model with new and unseen data, we will make use of
    25 new images that have black lines across them. To do this, we will use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, we read the new image data and then formatted
    all images, like we did previously. All 25 new pictures with a black line across
    them are shown in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/498d8de0-0595-43d8-b0df-f46cd3507b1f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, all 25 images have a black line across them. We will use the data from
    these new images and reconstruct the images using the autoencoder model that we''ve
    developed to remove the black lines. The code that''s used for reconstructing
    and plotting the images is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the reconstructed images after using the autoencoder
    model based on the 25 new images that had a black line across them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4986e67b-57f0-4576-a6fe-3322e07f6c0b.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot once again shows that the autoencoder model successfully
    removes the black lines from all the images. However, as we observed earlier,
    the image quality is low. This example provides promising results. If the results
    that were obtained also had a higher quality output for the images, then we could
    use this in several different situations. For example, we could reconstruct an
    image with glasses as an image without glasses or vice versa, or we may be able
    to reconstruct an image of a person without a smile to an image of them with a
    smile. There are several variants of such approaches that have the potential to
    have significant business value.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went over three application examples of autoencoder networks.
    The first type of autoencoder involved a dimension reduction application. Here,
    we used an autoencoder network architecture that only allowed us to learn about
    the key features of the input image. The second type of autoencoder was illustrated
    using MNIST data containing images of numbers. We artificially added noise to
    the images of numbers and trained the network in such a way that it learned to
    remove noise from the input image. The third type of autoencoder network involved
    image correction application. The autoencoder network in this application was
    trained to remove a black line from input images.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go over another class of deep networks, called
    **transfer learning**, and use them for image classification.
  prefs: []
  type: TYPE_NORMAL
