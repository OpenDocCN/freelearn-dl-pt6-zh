["```py\n<properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <java.version>1.8</java.version>\n        <jdk.version>1.8</jdk.version>\n        <spark.version>2.3.0</spark.version>\n</properties>\n```", "```py\n<dependencies>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-core_2.11</artifactId>\n            <version>${spark.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-sql_2.11</artifactId>\n            <version>${spark.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-mllib_2.11</artifactId>\n            <version>${spark.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-graphx_2.11</artifactId>\n            <version>${spark.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-yarn_2.11</artifactId>\n            <version>${spark.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-network-shuffle_2.11</artifactId>\n            <version>${spark.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-streaming-flume_2.11</artifactId>\n            <version>${spark.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>com.databricks</groupId>\n            <artifactId>spark-csv_2.11</artifactId>\n            <version>1.3.0</version>\n        </dependency>\n</dependencies>\n```", "```py\nSparkSession spark = SparkSession\n                     .*builder*()\n                     .master(\"local[*]\")\n                     .config(\"spark.sql.warehouse.dir\", \"/tmp/spark\")\n                     .appName(\"SurvivalPredictionMLP\")\n                     .getOrCreate();\n```", "```py\nDataset<Row> df = spark.sqlContext()\n                .read()\n                .format(\"com.databricks.spark.csv\")\n                .option(\"header\", \"true\")\n                .option(\"inferSchema\", \"true\")\n                .load(\"data/train.csv\");\ndf.show();\n```", "```py\nprivate static UDF1<String,Option<Integer>> *normEmbarked*=(String d) -> {\n        if (null == d)\n            return Option.*apply*(null);\n        else {\n            if (d.equals(\"S\"))\n                return Some.apply(0);\n            else if (d.equals(\"C\"))\n                return Some.apply(1);\n            else\n                return Some.apply(2);\n        }\n    };\n```", "```py\nprivate static UDF1<String, Option<Integer>> normSex = (String d) -> {\n      if (null == d)\n          return Option.apply(null);\n      else {\n        if (d.equals(\"male\"))\n            return Some.apply(0);\n        else\n            return Some.apply(1);\n      }\n    };\n```", "```py\nDataset<Row> projection = df.select(\n                col(\"Survived\"),\n                col(\"Fare\"),\n                callUDF(\"normSex\", col(\"Sex\")).alias(\"Sex\"),\n                col(\"Age\"),\n                col(\"Pclass\"),\n                col(\"Parch\"),\n                col(\"SibSp\"),\n                 callUDF(\"normEmbarked\",\n                col(\"Embarked\")).alias(\"Embarked\"));\nprojectin.show();\n```", "```py\nJavaRDD<Vector> statsDf =projection.rdd().toJavaRDD().map(row -> Vectors.*dense*( row.<Double>getAs(\"Fare\"),\n               row.isNullAt(3) ? 0d : row.Double>getAs(\"Age\")\n                  ));\n```", "```py\nMultivariateStatisticalSummary summary = Statistics.*colStats*(statsRDD.rdd());\ndouble meanFare = summary.mean().apply(0);\ndouble meanAge = summary.mean().apply(1); \n```", "```py\nUDF1<String, Option<Double>> normFare = (String d) -> {\n            if (null == d) {\n                return Some.apply(meanFare);\n            }\n            else\n                return Some.apply(Double.parseDouble(d));\n        };\n```", "```py\nUDF1<String, Option<Double>> normAge = (String d) -> {\n          if (null == d)\n              return Some.apply(meanAge);\n          else\n              return Some.apply(Double.parseDouble(d));\n        };\n```", "```py\nspark.sqlContext().udf().register(\"normFare\", normFare, DataTypes.DoubleType);\nspark.sqlContext().udf().register(\"normAge\", normAge, DataTypes.DoubleType);\n```", "```py\nDataset<Row> finalDF = projection.select(\n                *col*(\"Survived\"),\n                *callUDF*(\"normFare\",\n                *col*(\"Fare\").cast(\"string\")).alias(\"Fare\"),\n                *col*(\"Sex\"),\n                *callUDF*(\"normAge\",\n                *col*(\"Age\").cast(\"string\")).alias(\"Age\"),\n                *col*(\"Pclass\"),\n                *col*(\"Parch\"),\n                *col*(\"SibSp\"),\n                *col*(\"Embarked\"));\nfinalDF.show();\n```", "```py\nVector stddev = Vectors.dense(Math.sqrt(summary.variance().apply(0)), Math.sqrt(summary.variance().apply(1)));\n\nVector mean = Vectors.dense(summary.mean().apply(0), summary.mean().apply(1));\nStandardScalerModel scaler = new StandardScalerModel(stddev, mean);\n```", "```py\nEncoder<Integer> integerEncoder = Encoders.INT();\nEncoder<Double> doubleEncoder = Encoders.DOUBLE();\nEncoders.BINARY();\n\nEncoder<Vector> vectorEncoder = Encoders.kryo(Vector.class);\nEncoders.tuple(integerEncoder, vectorEncoder);\nEncoders.tuple(doubleEncoder, vectorEncoder);\n```", "```py\nJavaRDD<VectorPair> scaledRDD = trainingDF.toJavaRDD().map(row -> {\n                VectorPair vectorPair = new VectorPair();\n                vectorPair.setLable(new\n                Double(row.<Integer> getAs(\"Survived\")));\n\n                vectorPair.setFeatures(Util.*getScaledVector*(\n                                row.<Double>getAs(\"Fare\"),\n                                row.<Double>getAs(\"Age\"),\n                                row.<Integer>getAs(\"Pclass\"),\n                                row.<Integer>getAs(\"Sex\"),\n                                row.isNullAt(7) ? 0d :\n                                row.<Integer>getAs(\"Embarked\"),\n                                scaler));\n                return vectorPair;\n        });\n```", "```py\npublic static org.apache.spark.mllib.linalg.Vector getScaledVector(double fare, \n double age, double pclass,  double sex, double embarked, StandardScalerModel scaler) {\n        org.apache.spark.mllib.linalg.Vector scaledContinous = scaler.transform(Vectors.dense(fare, age));\n        Tuple3<Double, Double, Double> pclassFlat = flattenPclass(pclass);\n        Tuple3<Double, Double, Double> embarkedFlat = flattenEmbarked(embarked);\n        Tuple2<Double, Double> sexFlat = flattenSex(sex);\n\n        return Vectors.dense(\n                scaledContinous.apply(0),\n                scaledContinous.apply(1),\n                sexFlat._1(),\n                sexFlat._2(),\n                pclassFlat._1(),\n                pclassFlat._2(),\n                pclassFlat._3(),\n                embarkedFlat._1(),\n                embarkedFlat._2(),\n                embarkedFlat._3());\n    }\n```", "```py\nDataset<Row> scaledDF = spark.createDataFrame(scaledRDD, VectorPair.class);\n```", "```py\nscaledDF.show();\n```", "```py\nDataset<Row> scaledData2 = MLUtils.convertVectorColumnsToML(scaledDF);\n```", "```py\nDataset<Row> data = scaledData2.toDF(\"features\", \"label\");\nDataset<Row>[] datasets = data.randomSplit(new double[]{0.80, 0.20}, 12345L);\n\nDataset<Row> trainingData = datasets[0];\nDataset<Row> validationData = datasets[1];\n```", "```py\nint[] layers = new int[] {10, 8, 16, 2};\n```", "```py\nMultilayerPerceptronClassifier mlp = new MultilayerPerceptronClassifier()\n                                          .setLayers(layers)\n                                          .setBlockSize(128)\n                                          .setSeed(1234L)\n                                          .setTol(1E-8)\n                                          .setMaxIter(1000);\n```", "```py\nMultilayerPerceptronClassificationModel model = mlp.fit(trainingData);\n```", "```py\nDataset<Row> predictions = model.transform(validationData);\n```", "```py\npredictions.show();\n```", "```py\nMulticlassClassificationEvaluator evaluator = new MulticlassClassificationEvaluator()\n                                              .setLabelCol(\"label\")\n                                              .setPredictionCol(\"prediction\");\n\nMulticlassClassificationEvaluator evaluator1 = evaluator.setMetricName(\"accuracy\");\nMulticlassClassificationEvaluator evaluator2 = evaluator.setMetricName(\"weightedPrecision\");\nMulticlassClassificationEvaluator evaluator3 = evaluator.setMetricName(\"weightedRecall\");\nMulticlassClassificationEvaluator evaluator4 = evaluator.setMetricName(\"f1\");\n```", "```py\ndouble accuracy = evaluator1.evaluate(predictions);\ndouble precision = evaluator2.evaluate(predictions);\ndouble recall = evaluator3.evaluate(predictions);\ndouble f1 = evaluator4.evaluate(predictions);\n\n// Print the performance metrics\nSystem.*out*.println(\"Accuracy = \" + accuracy);\nSystem.*out*.println(\"Precision = \" + precision);\nSystem.*out*.println(\"Recall = \" + recall);\nSystem.*out*.println(\"F1 = \" + f1);\n\nSystem.*out*.println(\"Test Error = \" + (1 - accuracy));\n```", "```py\n<q>>>></q> Accuracy = 0.7796476846282568\n Precision = 0.7796476846282568\n Recall = 0.7796476846282568\n F1 = 0.7796476846282568\n Test Error = 0.22035231537174316\n```", "```py\nDataset<Row> testDF = Util.getTestDF();\n```", "```py\nMap<String, Object> m = new HashMap<String, Object>();\nm.put(\"Age\", meanAge);\nm.put(\"Fare\", meanFare);\n\nDataset<Row> testDF2 = testDF.na().fill(m);\n```", "```py\nJavaRDD<VectorPair> testRDD = testDF2.javaRDD().map(row -> {\n            VectorPair vectorPair = new VectorPair();\n            vectorPair.setLable(row.<Integer>getAs(\"PassengerId\"));\n            vectorPair.setFeatures(Util.*getScaledVector*(\n                    row.<Double>getAs(\"Fare\"),\n                    row.<Double>getAs(\"Age\"),\n                    row.<Integer>getAs(\"Pclass\"),\n                    row.<Integer>getAs(\"Sex\"),\n                    row.<Integer>getAs(\"Embarked\"),\n                    scaler));\n            return vectorPair;\n        });\n```", "```py\nDataset<Row> scaledTestDF = spark.createDataFrame(testRDD, VectorPair.class);\n```", "```py\nDataset<Row> finalTestDF = MLUtils.convertVectorColumnsToML(scaledTestDF).toDF(\"features\", \"PassengerId\");\n```", "```py\nDataset<Row> resultDF = model.transform(finalTestDF).select(\"PassengerId\", \"prediction\"); \nresultDF.show();\n```", "```py\nresultDF.write().format(\"com.databricks.spark.csv\").option(\"header\", true).save(\"result/result.csv\");\n```", "```py\nprivate static final UDF1<String, Option<String>> getTitle = (String name) ->      {\n    if(name.contains(\"Mr.\")) { // If it has Mr.\n        return Some.apply(\"Mr.\");\n    } else if(name.contains(\"Mrs.\")) { // Or if has Mrs.\n        return Some.apply(\"Mrs.\");\n    } else if(name.contains(\"Miss.\")) { // Or if has Miss.\n        return Some.apply(\"Miss.\");\n    } else if(name.contains(\"Master.\")) { // Or if has Master.\n        return Some.apply(\"Master.\");\n    } else{ // Not any.\n        return Some.apply(\"Untitled\");\n    }\n};\n```", "```py\nspark.sqlContext().udf().register(\"getTitle\", getTitle, DataTypes.StringType);\n\nDataset<Row> categoricalDF = df.select(callUDF(\"getTitle\", col(\"Name\")).alias(\"Name\"), col(\"Sex\"), \n                                       col(\"Ticket\"), col(\"Cabin\"), col(\"Embarked\"));\ncategoricalDF.show();\n```", "```py\nint[] layers = new int[] {10, 16, 16, 2};\n```", "```py\nMultilayerPerceptronClassifier mlp = new MultilayerPerceptronClassifier()\n                     .setLayers(layers)\n                     .setSeed(1234L);\n```", "```py\nParamMap[] paramGrid = new ParamGridBuilder() \n                    .addGrid(mlp.blockSize(), new int[] {32, 64, 128})\n                    .addGrid(mlp.maxIter(), new int[] {10, 50})\n                    .addGrid(mlp.tol(), new double[] {1E-2, 1E-4, 1E-6})\n                    .build();\nMulticlassClassificationEvaluator evaluator = new MulticlassClassificationEvaluator()\n          .setLabelCol(\"label\")\n          .setPredictionCol(\"prediction\");\n```", "```py\nint numFolds = 10;\nCrossValidator crossval = new CrossValidator()\n          .setEstimator(mlp)\n          .setEvaluator(evaluator)\n          .setEstimatorParamMaps(paramGrid)\n          .setNumFolds(numFolds);\n```", "```py\nCrossValidatorModel cvModel = crossval.fit(trainingData);\n```", "```py\nDataset<Row> predictions = cvModel.transform(validationData);\n```", "```py\ndouble accuracy = evaluator1.evaluate(predictions);\ndouble precision = evaluator2.evaluate(predictions);\ndouble recall = evaluator3.evaluate(predictions);\ndouble f1 = evaluator4.evaluate(predictions);\n\n// Print the performance metrics\nSystem.out.println(\"Accuracy = \" + accuracy);\nSystem.out.println(\"Precision = \" + precision);\nSystem.out.println(\"Recall = \" + recall);\nSystem.out.println(\"F1 = \" + f1);\nSystem.out.println(\"Test Error = \" + (1 - accuracy));\n>>>Accuracy = 0.7810132575757576\n Precision = 0.7810132575757576\n Recall = 0.7810132575757576\n F1 = 0.7810132575757576\n Test Error = 0.21898674242424243\n```"]