- en: '*Chapter 3*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Describe Deep Learning and its applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiate between Deep Learning and machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore neural networks and their applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the training and functioning of a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Keras to create neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter aims to introduce you to neural networks, their applications in
    Deep Learning, and their general drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous two chapters, you learned about the basics of natural language
    processing, its importance, the steps required to prepare text for processing,
    and two algorithms that aid a machine in understanding and executing tasks based
    on natural language. However, to cater to higher, more complicated natural language
    processing problems, such as creating a personal voice assistant like *Siri* and
    *Alexa*, additional techniques are required. Deep learning systems, such as neural
    networks, are often used in natural language processing, and so we're going to
    cover them in this chapter. In the following chapters, you learn how to use neural
    networks for the purpose of natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter begins with an explanation on deep learning and how it is different
    from machine learning. Then, it discusses neural networks, which make up a large
    part of deep learning techniques, and their basic functioning along with real-world
    applications. Additionally, it introduces **Keras**, a Python deep learning library.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Artificial Intelligence is the idea of agents possessing the natural intelligence
    of humans. This natural intelligence includes the ability to plan, understand
    human language, learn, make decisions, solve problems, and recognize words, images
    and objects. When building these agents, this intelligence is known as artificial
    intelligence, since it is human-made. These agents do not refer to physical objects.
    They are, in fact, a reference to software that demonstrates artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of artificial intelligence—narrow and generalized. Narrow
    artificial intelligence is the kind of artificial intelligence that we are currently
    surrounded by; it is any single agent possessing one of the several capabilities
    of natural intelligence. The application areas of natural language processing
    that you learned about in the first chapter of this book are examples of narrow
    Artificial Intelligence, because they are agents capable of carrying out a single
    task, such as, a machine being able to automatically summarize an article. There
    do exist Technologies do exist that are capable of more than one task, such as
    self-driving cars, but these are still considered a combination of several narrow
    AIs.
  prefs: []
  type: TYPE_NORMAL
- en: Generalized artificial intelligence is the possession of all human capabilities
    and more, in a single agent, rather than one or two capabilities in a single agent.
    AI experts claim that once AI has surpassed this goal of generalized AI and it
    is smarter and more adept than humans themselves in all fields, it will become
    super artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the previous chapters, natural language processing is an approach
    to achieving artificial intelligence, by enabling machines to understand and communicate
    with humans in the natural language of humans. Natural language processing prepares
    textual data and transforms it into a form that machines are able to process—a
    numerical form. This is where deep learning comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Like natural language processing and machine learning, deep learning is also
    a category of techniques and algorithms. It is a subfield of machine learning
    because both these approaches share the same primary principle—both machine learning
    and deep learning algorithms take input and use it to predict output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 3.1: Deep Learning as a subfield of Machine Learning'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig 3.1: Deep learning as a subfield of machine learning'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When trained on a training dataset, both types of algorithms (machine learning
    and deep learning) aim to minimize the difference between the actual outcomes
    and their predicted outcomes. This aids them in forming an association between
    the input and the output, thus resulting in higher accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Machine Learning and Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While both these approaches are based on the same principle—predicting output
    from input—they achieve this in different ways, which is why deep learning has
    been categorized as a separate approach. Additionally, one of the main reasons
    for deep learning coming about was the increased accuracy these models provide
    in their prediction process.
  prefs: []
  type: TYPE_NORMAL
- en: While machine learning models are quite self-sufficient, they still need human
    intervention to determine that a prediction is incorrect, and thus they need to
    get better at performing that particular task. Deep learning models, on the other
    hand, are capable of determining whether a prediction is incorrect or not by themselves.
    Thus, deep learning models are self-sufficient; they can make decisions and improve
    their efficiency without human interventions.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand this, let's take the example of an air conditioner whose
    temperature settings can be controlled by voice commands. Let's say that when
    the air conditioner hears the word "hot," it decreases the temperature, and when
    it hears the word "cold," it increases the temperature. If this were a machine
    learning model, then the air conditioner would learn to recognize these two words
    in different sentences over time. However, if this were a deep learning model,
    it could learn to alter the temperature based on words and sentences similar to
    the words "hot" and "cold," such as "It's a little warm" or "I'm freezing!" and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: This is an example that directly relates to natural language processing since
    the model understands the natural language of humans and acts on what it has understood.
    In this book we will be sticking to using deep learning models for the purpose
    of natural language processing, though in reality they can be used in almost every
    field. They are currently involved in automating the task of driving, by enabling
    a vehicle to recognize stop signs, read traffic signals, and halt for pedestrians.
    The medical field is also utilizing deep learning methods to detect diseases at
    early stages – cancer cells. But since our focus in this book is on enabling machines
    to understand the natural language of humans, let's get back to that.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning techniques are most often used in the supervised learning way,
    that is, they are provided with labelled data to learn from. However, the key
    difference between machine learning methods and deep learning methods is that
    the latter require insanely large amounts of data which didn't exist before. Thus,
    deep learning has only recently become advantageous. It also requires quite a
    bit of computing power since it needs to be trained on such large amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference, however, is in a algorithms themselves. If you've studied
    machine learning before, then you're aware of the variety of algorithms that exist
    to solve classification and regression problems, as well as unsupervised learning
    ones. Deep learning systems differ from these algorithms because they use Artificial
    Neural Networks.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often neural networks and deep learning are terms that are used interchangeably.
    They do not mean the same thing, however, so let's learn the difference.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, deep learning is an approach that follows the same principle
    as machine learning, but does so with more accuracy and efficiency. Deep learning
    systems make use of artificial neural networks, which are computing models on
    their own. So, basically, neural networks are a part of the deep learning approach
    but are not the deep learning approach on their own. They are frameworks that
    are incorporated by deep learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 3.2: Neural Networks as a part of the Deep Learning Approach'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig 3.2: Neural Networks as a part of the deep learning Approach'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Artificial neural networks are based on a framework inspired by the biological
    neural networks found in the human brain. These neural networks are made of nodes
    that enable the networks to learn from images, text, real-life objects, and other
    things, to be able to execute tasks and predict things accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks consist of layers, which we will take a look at in the following
    section. The number of layers that a network has can be anywhere from three to
    hundreds. Neural networks that are made of only three or four layers are called
    shallow neural networks, whereas networks that have many more layers than that
    are referred to as deep neural networks. Thus, the neural networks used by the
    deep learning approach are deep neural networks and they possess several layers.
    Due to this, deep learning models are very well suited to complex tasks such as
    facial recognition translating text, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: These layers break down the input into several levels of abstraction. As a result,
    the deep learning model is better able to learn from and understand the input,
    be it images or text or another form of input, which aids it in making decisions
    and predicting things the way our human mind does.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through an example to understand these layers. Imagine that you're
    in your bedroom doing some work and you notice you're sweating. That's your input
    data—the fact that you're feeling hot and so in your head a little voice goes
    "I'm feeling hot!" Next, you might wonder why you're feeling so hot—"Why am I
    feeling so hot?" This is a thought. You'll then try to come up with a solution
    to this problem, maybe by taking a shower—"Let me take a quick shower." This is
    a decision that you've made. But then you remember that you've got to leave for
    work soon—"But, I need to leave the house soon." This is a memory. You might try
    to convince yourself by thinking "Isn't there enough time to squeeze in a quick
    shower, though?" This is the process of a reasoning. Lastly, you'll probably act
    on your thoughts by either thinking "I'm going to take a shower," or, "there's
    no time for a shower, never mind." This is decision making and in the event you
    do take a shower, it is an action.
  prefs: []
  type: TYPE_NORMAL
- en: The multiple layers in a deep neural network allow the model to go through these
    different levels of processing just like the mind does, thus building upon the
    principles of biological neural networks. These layers are how and why deep learning
    models are able to perform tasks and predict outputs with such high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Neural network architecture refers to the elements that are the building blocks
    of a neural network. While there are several different types of neural networks,
    the basic architecture and foundation remains constant. The architecture includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Layers**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nodes**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edges**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Biases**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation functions**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned before, neural networks are made up of layers. While the number
    of these layers varies from model to model and is dependent on the task at hand,
    there are only three types of layers. Each layer is made up of individual nodes
    and the number of these nodes depends on the requirement of the layer and the
    neural network as a whole. A node can be thought of as a neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'The layers present in a neural network are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The input layer*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the name suggests, this is the layer that consists of the input data entering
    the neural network. It is a mandatory layer as every neural network requires input
    data to learn from and perform operations on to be able to generate an output.
    This layer can only occur once in a neural network. Each input node is connected
    to each node present in the proceeding layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The variables or characteristics of input data are known as features. The target
    output is dependent on these features. For example, take the iris dataset. (The
    Iris dataset is one of the most popular datasets for machine learning beginners.
    It consists of data of three different types of flowers. Each instance has four
    features and one target class.) The classification label of a flower is dependent
    on the four features—petal length and width, and sepal length and width. The features,
    and thus the input layer, is denoted by **X**, and each individual featured is
    denoted by **X1**, **X2**, ... , **Xn**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*The hidden layer*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the layer where the actual computation is done. It comes after the input
    layer, since it acts on the input provided by the input layer, and before the
    output layer, since it generates the output that is provided by the output layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A hidden layer is made up of nodes known as "activation nodes." Each node possesses
    an activation function, which is a mathematical function that is performed on
    the inputs received by an activation node to generate an output. Activation functions
    will be discussed later on in this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is the only type of layer that can occur multiple times, and thus in deep
    neural networks, there can be up to hundreds of hidden layers present. The number
    of hidden layers depends on the task at hand.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The output generated by the nodes of one hidden layer are fed into the proceeding
    hidden layer as input. The output generated by each activation node of a hidden
    layer is sent to each activation node of the next layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*The output layer*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the last layer of the neural network and it consists of nodes that provide
    the final outcome of all the processing and computing. This is also a mandatory
    layer since a neural network must produce an output based on input data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the case of the iris dataset, the output for a particular instance of a flower
    would be the category of that flower—Iris setosa, Iris virginica, or Iris versicolor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The output, often known as the target, is denoted as **y**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Fig 3.3: A Neural Network with 2 Hidden Layers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig 3.3: A Neural Network with 2 Hidden Layers'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Each activation node or neuron possess the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: An activation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the current state of the node—whether it is active or not.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A threshold value (optional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If present, this determines whether a neuron is activated or not, depending
    on whether the weighted sum is above or below this threshold value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is what computes a new activation for the activation node based on the
    inputs and the weighted sum.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An output function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This generates the output for the particular activation node based on the activation
    function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Input neurons have no such components as they don't perform computation, nor
    do they have any preceding neurons. Similarly, output neurons don't have these
    components, since they don't perform computation, nor do they have proceeding
    neurons.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Edges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![ Fig 3.4: The Weighted Connections of a Neural Network'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig 3.4: The Weighted Connections of a Neural Network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each of the arrows in the preceding diagram represents a connection between
    two nodes from two different layers. A connection is known as an edge. Each edge
    that leads to an activation node has its own weight, which can be considered as
    a sort of impact that one node has on the other node. Weights can be either positive
    or negative.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the earlier diagram. Before the values reach the activation function,
    their values are multiplied by the weights assigned to their respective connections.
    These multiplied values are then added together to obtain a weighted sum. This
    weighted sum is basically a measure of how much impact that node has on the output.
    Thus if the value is low, that means that it doesn't really affect the output
    that much and so it's not that important. If the value is high, then it shares
    a strong correlation with the target output and thus plays a role in determining
    what the output is.
  prefs: []
  type: TYPE_NORMAL
- en: Biases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A bias is a node, and each layer of a neural network has its own bias node,
    except for the output layer. Thus, each layer has its own bias node. The bias
    node holds a value, known as the bias. This value is incorporated in the process
    of calculating the weighted sum and so also plays a role in determining the output
    generated by a node.
  prefs: []
  type: TYPE_NORMAL
- en: Bias is an important aspect of neural networks because it allows the activation
    function to shift either to the right or to the left. This helps the model to
    better fit the data and thus produce accurate outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Activation functions are functions that are part of the activation nodes found
    in the hidden layers of neural networks. They serve the purpose of introducing
    non-linearity into neural networks, which is really important, as without them
    neural networks would just have linear functions, leaving no difference between
    them and linear regression models. This defeats the purpose of neural networks,
    because then they wouldn't be able to learn complex functional relationships that
    exist within data. Activation functions also need to be differentiable for backpropagation
    to occur. This will be discussed in future sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, an activation node calculates the weighted sum of the inputs it receives,
    adds the bias, and then applies an activation function to this value. This generates
    an output for that particular activation node which is then used as input by the
    proceeding layer. This output is known as an activation value. Therefore, the
    proceeding activation node in the next layer will receive multiple activation
    values from preceding activation nodes and calculate a new weighted sum. It will
    apply its activation function to this value to generate its own activation value.
    This is how data flows through a neural network. Thus, an activation function
    helps convert an input signal into an output signal.
  prefs: []
  type: TYPE_NORMAL
- en: This process of calculating the weighted sum, applying an activation function,
    and producing an activation value is known as feedforward.
  prefs: []
  type: TYPE_NORMAL
- en: There are several activation functions (Logistic, TanH, ReLU, and so on). The
    Sigmoid function is one of the most popular and simple activation functions out
    there. When represented mathematically, this function looks like
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5: Expression for sigmoid function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.5: Expression for sigmoid function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, this function is non-linear.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we know that once an input is provided to a neural network, it enters
    the input layer which is an interface that exists to pass on the input to the
    next layer. If a hidden layer is present, then the inputs are sent to the activation
    nodes of the hidden layer via weighted connections. The weighted sum of all the
    inputs received by the activations nodes is calculated by multiplying the inputs
    with their respective weights and adding these values up along with the bias.
    The activation function generates an activation value from the weighted sum and
    this is passed on to the nodes in the next layer. If the next layer is another
    hidden layer, then it uses the activation values from the previous hidden layer
    as inputs and repeats the activation process. However, if the proceeding layer
    is the output layer, then the output is provided by the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: From all of this information, we can conclusively say that there are three parts
    of the deep learning model that have an impact on the output generated by the
    model—the inputs, the connection weights and biases, and the activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6: Aspects of a deep learning model that impact the output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.6: Aspects of a deep learning model that impact the output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While the inputs are taken from the dataset, the former two are not. Thus, the
    following two questions arise—who or what decides what the weight is for a connection?
    How do we know which activation functions to use? Let's tackle these questions
    one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating Weights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Weights play a very important role in multilayer neural networks, since altering
    the weight of a single connection can completely alter the weights assigned to
    further connections and thus the outputs generated by the proceeding layers. Thus,
    having the optimal weights is necessary to create an accurate deep learning model.
    This sounds like a lot of pressure, but lucky for us, deep learning models are
    capable of finding the optimal weights all on their own. To understand this better,
    let's take the example of linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear regression is a supervised machine learning algorithm that, as suggested
    by the name itself, is suitable to solve regression problems (datasets whose output
    is in the form of continuous numerical values, such as the selling prices of houses).
    This algorithm assumes there exists a linear relationship between the input (the
    features) and the output (the target). Basically, it believes that there exists
    a line of best fit that accurately describes the relationship between the input
    and output variables. It uses this to predict future numerical values. In a scenario
    where there is only one input feature, the equation for this line is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: Expression for linear regression'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.7: Expression for linear regression'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Where,
  prefs: []
  type: TYPE_NORMAL
- en: '**y** is the target output'
  prefs: []
  type: TYPE_NORMAL
- en: '**c** is the y-intercept'
  prefs: []
  type: TYPE_NORMAL
- en: '**m** is the model coefficient'
  prefs: []
  type: TYPE_NORMAL
- en: '**x** is the input feature'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the connections in neural networks, the input features have values
    attached to them too—they're called model coefficients. In a way, these model
    coefficients determine the importance a feature has in determining the output,
    which is similar to what the weights in neural networks do. It is important to
    ensure these model coefficients are of the correct value so as to get correct
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that we want to predict the selling price of a house based on how
    many bedrooms it has. So, the price of the house is our target output and the
    number of bedrooms it has is our input feature. Since this is a supervised learning
    method, our model will be fed a dataset that contains instances of our input feature
    matched with the correct target output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 3.8: Sample Dataset for Linear Regression'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig 3.8: Sample Dataset for Linear Regression'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, our linear regression model needs to find a model coefficient that describes
    the impact of the number of bedrooms on the selling price of the house. It does
    this by making use of two algorithms—the loss function and the gradient descent
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The Loss Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The loss function is also sometimes known as the cost function.
  prefs: []
  type: TYPE_NORMAL
- en: For classification problems, the loss function calculates the difference between
    the predicted probability of a particular category and the category itself. For
    example, let's say you have a binary classification problem that needs to predict
    whether a house will be sold or not. There are only two outputs—"yes" and "no."
    A classification model when fitted on this data will predict the probability of
    an instance of data falling in either the "yes" category or the "no" category.
    Let's say the "yes" category has a value of 1, and "no" has a value of 0\. Thus,
    if the output probability is closer to 1 it will fall in the "yes" category. The
    loss function for this model will measure this difference.
  prefs: []
  type: TYPE_NORMAL
- en: For regression problems, the loss function calculates the error between actual
    values and predicted values. The house price example from the previous section
    is a regression problem and so the loss function is calculating the error between
    the actual price of a house, and the price that our model predicted. Thus, in
    a way, the loss function helps the model self-evaluate its performance. Obviously,
    the model's aim is to predict the price that is exactly, if not closest to, the
    actual price. To do this, it needs to minimize the loss function as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The only factor that is directly affecting the price predicted by the model
    is the model coefficient. To arrive at the model coefficient that is best suited
    for the problem at hand, the model needs to keep improving the values for the
    model coefficient. Let's call each different value an update of the model coefficient.
    So, with each update of the model coefficient, the model must calculate the error
    between the actual price and the price that the model has predicted using that
    update of the model coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Once the function has reached its minimum value, the model coefficient at this
    minimum point is chosen as the final model coefficient. This value is stored and
    used in the linear equation described above by the linear regression algorithm.
    From that point onwards, whenever the model is fed input data in the form of how
    many bedrooms a house has without target outputs, it uses the linear equation
    with the apt model coefficient to calculate and predict the price that that house
    will be sold at.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different kinds of loss functions—such as MSE (for regression
    problems) and Log Loss (for classification problems). Let's take a look at how
    they work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Mean Squared Error function calculates the difference between the actual
    values and the predicted values, squares this difference, and then averages it
    out across the entire dataset. The function, when expressed mathematically, looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9: Expression for mean squared error function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.9: Expression for mean squared error function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Where,
  prefs: []
  type: TYPE_NORMAL
- en: '**n** is the total number of data points'
  prefs: []
  type: TYPE_NORMAL
- en: '**yi** is the ith actual value'
  prefs: []
  type: TYPE_NORMAL
- en: '**xi** is the input'
  prefs: []
  type: TYPE_NORMAL
- en: '**f()** is the function being carried out on the input to generate the output,
    therefore'
  prefs: []
  type: TYPE_NORMAL
- en: '**f(xi)** is the predicted value'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log loss is used for classification models whose output is a probability value
    in the range of 0 and 1\. The higher the difference between the predicted probability
    and the actual category, the higher the log loss. The mathematical representation
    of the log loss functions is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10: Expression for log loss function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.10: Expression for log loss function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Where,
  prefs: []
  type: TYPE_NORMAL
- en: '**N** is the total number of data points'
  prefs: []
  type: TYPE_NORMAL
- en: '**yi** is the ith actual label'
  prefs: []
  type: TYPE_NORMAL
- en: '**p** is the predicted probability'
  prefs: []
  type: TYPE_NORMAL
- en: The Gradient Descent Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The process of evaluating the model's performance via the loss function is one
    that the model carries out independently, as is the process for updating and ultimately
    choosing the model coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you're on a mountain and you want to climb back down and reach
    the absolute bottom. It's cloudy and there are quite a few peaks so you can't
    exactly see where the bottom is, or which direction it is in, you just know that
    you need to get there. You start your journey at 5000 meters above sea level,
    and you decide to take large steps. You take a step and then you check your phone
    to see how many meters above sea level you are. Your phone says you are 5003 meters
    above sea level, which means you've gone in the wrong direction. Now, you take
    a large step in another direction and your phone says you are 4998 meters above
    sea level. This means you're getting closer to the bottom, but how do you know
    that this step was the one with the steepest descent? What if you took a step
    in another direction that brought you down to 4996 meters above sea level? Thus,
    you check your position after taking a step in each possible direction, and whichever
    takes you closest the bottom, is the one you choose.
  prefs: []
  type: TYPE_NORMAL
- en: You keep repeating this process, and then you reach a point where your phone
    says you are 100 meters above sea level. When you take another step, your phone's
    reading remains the same—100 meters above sea level. Finally, you have reached
    what seems to be the bottom since a step in any direction from this point results
    in you still being 100 meters above sea level.
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 3.11: Updating Parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig 3.11: Updating Parameters'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is how the gradient descent algorithm works. The algorithm descends a plot
    of the loss function against possible values for the model coefficient and the
    y-intercept, like you descended the mountain. It starts off with an assigned value
    for the model coefficient—this is you standing at a point 5000 meters above sea
    level. It calculates the gradient of the plot at this point. This gradient tells
    the model which direction it should move in to update the coefficient in order
    to get closer to the global minimum, which is the end goal. So, it takes a step
    and arrives at a new point with a new model coefficient. It repeats the process
    of calculating the gradient, obtaining a direction to move in, updating the coefficient,
    and taking another step. It checks to see that this step is the one that provides
    it with the steepest descent. With each step that it takes, it arrives at a new
    model coefficient and calculates the gradient at that point. This process is repeated
    until the value of the gradient doesn't change for a number of trials. This means
    that the algorithm has reached the global minimum and has converged. The model
    coefficient at this point is used as the final model coefficient in the linear
    equations.
  prefs: []
  type: TYPE_NORMAL
- en: In neural networks, the gradient descent algorithm and loss function work together
    to find values to be assigned to connections as weights and to biases. These values
    are updated by minimizing the loss function using the gradient descent algorithm,
    as is the case in linear regression models. Additionally, with the case of linear
    regression, there is always only one minimum, due to the fact that the loss function
    is bowl shaped. This makes it easy for the gradient descent algorithm to find
    it and be sure that this is the lowest point. In the case of neural networks,
    however, it is not that simple. The activation functions used by neural networks
    serve the purpose of introducing non-linearity to the situation.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the plot of the loss function of a neural network is not a bowl-shaped
    curve, and this does not have just one minimum point. Instead, it has several
    minimums, only one of which is the global minima. The rest are known as local
    minima. This sounds like a major issue, but it is, in fact, alright for the gradient
    descent algorithm to reach a local minima and choose the weight values at that
    point, due to the fact that most local minima are usually quite close to the global
    minimum. There are modified versions of the gradient descent algorithm that are
    also used when designing neural networks. Stochastic and batch-sized gradient
    descent are two of them.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say our loss function is MSE, and we need the gradient descent algorithm
    to update one weight (w) and one bias (b).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12: Expression for gradient of loss function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.12: Expression for gradient of loss function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The gradient is the partial derivative of the loss function, with respect to
    the weight and the bias. The mathematical representation of this is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13: Expression of gradient with partial derivaive of loss function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.13: Expression of gradient with partial derivaive of loss function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The result of this is the gradient of the loss function at the current position.
    This also tells us which direction we should move in to continue updating the
    weight and the bias.
  prefs: []
  type: TYPE_NORMAL
- en: The size of the step taken is adjusted by a parameter called the learning rate
    and is a very sensitive parameter in the gradient descent algorithm. It is called
    alpha and is denoted by α. If the learning rate is too small, then the algorithm
    will take too many tiny steps and thus take too long to reach the minimum. However,
    if the learning rate is too large then the algorithm might miss the minimum altogether.
    Thus, it is important to tweak and test out the algorithm using different learning
    rates to ensure the right one is chosen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning rate is multiplied with the gradient calculated at each step in
    order to modify the size of the step, thus the step size of each step is not always
    the same. Mathematically, this looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14: Expression for learning rate multipled with gradient'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.14: Expression for learning rate multiplied with gradient'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: And,
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15: Expression for learning rate multipled with gradient at each
    step'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.15: Expression for learning rate multiplied with gradient at each
    step'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The values are subtracted from the previous values of the weight and bias because
    the partial derivatives point in the direction of the steepest ascent, but our
    aim is to descend.
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 3.16: Learning Rate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig 3.16: Learning Rate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear regression is basically a neural network, but without a hidden layer
    and with an identity activation function (which is a linear function, therefore
    linearity). Hence, the learning process remains the same as the one described
    in the previous sections—the loss function aims to minimize the error by having
    the gradient descent algorithm constantly update the weights till the global minimum
    is reached.
  prefs: []
  type: TYPE_NORMAL
- en: However, when dealing with larger, more complicated neural networks that are
    not linear in nature, the loss calculated is sent back through the network to
    each layer, which then begins the process of weight updating again. The loss is
    propagated backwards, therefore this is known as backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation is performed using the partial derivatives of the loss function.
    It involves calculating the loss of every node in every layer by propagating backwards
    in the neural network. Knowing the loss of every node allows the network to understand
    which weights are having a drastic negative impact on the output and the loss.
    Thus, the gradient descent algorithm is able to reduce the weights of these connections
    that have high error rates, consequently reducing the impact that that node has
    on the network's output.
  prefs: []
  type: TYPE_NORMAL
- en: 'When dealing with many layers in a neural network, there are many activation
    functions working on the inputs. This can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17: Expression for backpropagation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.17: Expression for backpropagation function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here **X**, **Y**, and **Z** are activation functions. As we can see, **f(x)**
    is a composite function, thus, backpropagation can be seen as an application of
    the chain rule. The chain rule is the formula used to calculate the partial derivatives
    of a composite function, which is what we're doing through backpropagation. Therefore,
    by applying the chain rule to the preceding function (known as the forward propagation
    function since values are moving in the forward direction to generate an output)
    and calculating the partial derivatives with respect to each weight, we will be
    able to determine exactly how much of an impact each node has on the final output.
  prefs: []
  type: TYPE_NORMAL
- en: The loss of the final node present in the output layer is the total loss of
    the entire neural network, because it is in the output layer and so the loss of
    all the previous nodes gets accumulated. The input nodes present in the input
    layer do not have a loss because they don't have an impact on the neural network.
    The input layer is merely an interface that sends the input to the activation
    nodes present in the hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the process of backpropagation is the process of updating the weights
    using the gradient descent algorithm and the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For more information on the math of backpropagation, click here: https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html'
  prefs: []
  type: TYPE_NORMAL
- en: Designing a Neural Network and Its Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Common machine learning techniques are used when training and designing a neural
    network. Neural networks can be classified as:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These are like the example used in the previous section (predicting the price
    of the house based on how many rooms it has). Supervised neural networks are trained
    on datasets consisting of sample inputs with their corresponding outputs. These
    are suitable for noise classification and making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of supervised learning methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is for problems that have discrete categories or classes as target outputs,
    for example the Iris dataset. The neural network learns from sample inputs and
    outputs how to correctly classify new data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is for problems that have a range of continuous numerical values as target
    outputs, like the price of a house example. The neural network describes the causal
    relationship between the inputs and their outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unsupervised neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These neural networks are trained on data without any target output, and thus
    are able to recognize and draw out patterns and inferences from the data. This
    makes them well-suited for tasks such as identifying category relationships and
    discovering natural distributions in data.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cluster analysis is the grouping together of similar inputs. These neural
    networks can be used for gene sequence analysis and object recognition, amongst
    other things.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks that are capable of pattern recognition can be trained both
    by supervised or unsupervised methods. They play a key role in text classification
    and speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 17: Creating a neural network'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we're going to implement a simple, classic neural network,
    by following the workflow outlined earlier, to predict whether a review is positive
    or negative.
  prefs: []
  type: TYPE_NORMAL
- en: This is a natural language processing problem, since the neural network is going
    to be fed rows of sentences that are actually reviews. Each review has a label
    in the training set—either 0 for negative or 1 for positive. This label is dependent
    on the words present in the review and so, our neural network needs to understand
    the meaning of the review and accordingly label it. Ultimately, our neural network
    needs to be able to predict whether a review is positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Download the dataset from the link:'
  prefs: []
  type: TYPE_NORMAL
- en: https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2003
  prefs: []
  type: TYPE_NORMAL
- en: The following steps will help you with the solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up a new Jupyter notebook by typing the following command in the directory
    you''d like to code in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, import `pandas` so that you can store the data in a dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Import the regular expressions package
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function to preprocess the reviews by removing the `HTML` tags, escaped
    quotes and normal quotes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply this function to the reviews currently stored in your dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `train_test_split` from `scikit-learn` to divide this data into a training
    set and a validation set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `nltk` and `stopwords` from `nltk` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now machine learning and deep learning models require numerical data as input,
    and currently our data is in the form of text. Thus, we're going to use an algorithm
    called CountVectorizer to convert the words present in the reviews into word count
    vectors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our data is clean and prepped now!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We're going to create a two-layer neural network. When defining a neural network,
    the number of layers does not include the input layer since it's a given that
    an input layer exists and because the input layer isn't a part of the computation
    process. So, a two-layer neural network includes an input layer, one hidden layer
    and an output layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the model and the layers from Keras:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initiate the neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the hidden layer. Specify the number of nodes the layer will have the activation
    function the nodes possess and what the input for the layer is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Add the output layer. Once again, specify the number of nodes and the activation
    function. We're going to use the `sigmoid` function here because this is a binary
    classification problem (predicting whether a review is positive or negative).
    We're going to have only one output node since the output is just one value—either
    1 or 0\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''re going to compile the neural network now, and decide which loss function,
    optimization algorithm and performance metric we want to use. Since the problem
    is a binary classification one, we''re going to use `binary_crossentropy` as our
    `loss` function. The optimization algorithm is basically the gradient descent
    algorithm. Different versions and modifications of gradient descent exist. In
    this case, we''re going to use the `Adam` algorithm, which is an extension of
    stochastic gradient descent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s summarize our model and see what''s going on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output you''ll get will look something like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.18: Model summary ](img/C13783_03_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.18: Model summary'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, it''s time to train the model. Fit the neural network on the `X_train`
    and `y_train` data we had divided earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's it! Our neural network is now ready for testing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Transform the input validation data into word count vectors and evaluate the
    neural network. Print the accuracy score to see how your network is doing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Your score might be a little different, but it should be close to 0.875\.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which is a pretty good score. So, there you have it. You just created your first
    ever neural network, trained it, and validated it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.19: Expected accuracy score'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_03_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.19: Expected accuracy score'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Save your model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Fundamentals of Deploying a Model as a Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The purpose of deploying a model as a service is for other people to view and
    access it with ease, and in other ways besides just looking at your code on GitHub.
    There are different types of model deployments, depending on why you've created
    the model in the first place. You could say there are three types—a streaming
    model (one that constantly learns as it is constantly fed data and then makes
    predictions), an analytics as a service model (AaaS—one that is open for anyone
    to interact with) and an on-line model (one which is only accessible by people
    working within the same company).
  prefs: []
  type: TYPE_NORMAL
- en: The most common way of showcasing your work is through a web application. There
    are multiple deployment platforms that aid and allow you to deploy your models
    through them, such as Deep Cognition, MLflow, and others.
  prefs: []
  type: TYPE_NORMAL
- en: Flask is the easiest micro web framework to use to deploy your own model without
    using an existing platform. It is written in Python. Using this framework, you
    can build a Python API for your model that will easily generate predictions and
    display them for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'The flow is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a directory for the API
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy your pre-trained neural network model to this directory
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a program that loads this model, preprocess the input so that it matches
    the training input of your model, use the model to make predictions and prepare,
    send, display this prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To test and run the API, you simply need to type the applications name along
    with **.run()**.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the neural network we created in, we would save that model and
    load it into a new Jupyter notebook. We would convert input data (the cleaned
    reviews) into word count vectors so that the input data for our API would be the
    same as the training data. Then, we would use our models to generate predictions
    and display them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4: Sentiment Analysis of Reviews'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the activity, we are going to review comments from a dataset and categorize
    them as positive or negat**ive.** The following steps will help you with the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You will find the dataset at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2004
  prefs: []
  type: TYPE_NORMAL
- en: Open a new `Jupyter` notebook. Import the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the necessary Python packages and necessary classes. Load the dataset
    in a dataframe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the necessary libraries to clean and prepare the data. Create an array
    for your cleaned text to be stored in. Using a `for` loop, iterate through every
    instance (every review).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import CountVectorizer and convert the words into word count vectors. Create
    an array to store each unique word as its own column, hence making them independent
    variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import necessary label encoding entities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the dataset into training and testing sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the neural network model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model and validate it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the neural network and print the accuracy scores to see how it's doing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.20: Accuracy score'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_03_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.20: Accuracy score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for the activity can be found on page 302.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to a subset of machine learning—deep learning.
    You learned about the differences and similarities between the two categories
    of techniques and understood the requirement for deep learning and its applications.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are artificial representations of the biological neural networks
    that are present in the human brain. Artificial neural networks are frameworks
    that are incorporated by deep learning models and have proven to be increasingly
    efficient and accurate. They are used in several fields, from training self-driving
    cars to detecting cancer cells in very early stages.
  prefs: []
  type: TYPE_NORMAL
- en: We studied the different components of a neural network and learned a network
    trains and corrects itself, with the help of the loss function, the gradient descent
    algorithm and backpropagation. You also learned how to perform sentiment analysis
    on text inputs! Furthermore, you learned the basics of deploying a model as a
    service.
  prefs: []
  type: TYPE_NORMAL
- en: In the coming chapters, you will learn more about neural networks and their
    different types, along with which neural network to use in what situations.
  prefs: []
  type: TYPE_NORMAL
