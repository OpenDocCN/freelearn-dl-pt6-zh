<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">In <a href="a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml" target="_blank">Chapter 2</a>, <em>Deep Learning Basics</em>, we learned about a very high level overview of <strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>). In this chapter, we are going to understand more details about this type of CNN, the possible implementations of their layers, and we will start hands-on implementing CNNs through the DeepLearning4j framework. The chapter ends with examples involving Apache Spark too. Training and evaluation strategies for CNNs will be covered in <a href="3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml" target="_blank"/><a href="3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml" target="_blank">Chapter 7</a>, <em>Training Neural Networks with Spark</em>, <a href="b30120ea-bd42-4cb7-95d9-5ecaa2b7c181.xhtml" target="_blank">Chapter 8</a>, <em>Monitoring and Debugging Neural Network Training</em>, and <a href="869a9495-e759-4810-8623-d8b76ba61398.xhtml" target="_blank">Chapter 9</a>, <em>Interpreting Neural Network Output</em>. In the description of the different layers, I have tried to reduce the usage of math concepts and formulas as much as possible in order to make the reading and comprehension easier for developers and data analysts who might have no math or data science background. Therefore, you have to expect more focus on the code implementation in Scala. </p>
<p class="mce-root">The chapter covers the following topics:</p>
<ul>
<li>Convolutional layers</li>
<li>Pooling layers</li>
<li>Fully connected layers</li>
<li>Weights</li>
<li>GoogleNet Inception V3 model</li>
<li>Hands-on CNN with Spark</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional layers</h1>
                </header>
            
            <article>
                
<p>Since the CNN section was covered in <a href="a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml" target="_blank">Chapter 2</a>, <em>Deep Learning Basics</em>, you should know in which context CNNs are commonly used. In that section, we have mentioned that each layer of the same CNN can have a different implementation. The first three sections of this chapter describe <span>possible layer implementations </span>in detail, starting from the convolutional layers. But first, let's recap the process by which CNN perceive images. They perceive images as volumes (3D objects) and not as bi-dimensional canvases (having width and height only). The reason is the following: digital color images have a <strong>Red</strong>-<strong>Blue</strong>-<strong>Green</strong> (<strong>RGB</strong>) encoding and it is the mixing of these colors that produces the spectrum that can be perceived by human eyes. This also means that CNNs ingest images as three separate layers of color, one on top of the other. This translates into receiving a color image in the form of a rectangular box where width and height can be measured in pixels and having a three layers (referred as <strong>channels</strong>) depth, one for each RGB color. Cutting a long story short, an input image is seen by a CNN as a multi-dimensional array. Let's give a practical example. If we consider a 480 x 480 image, it is perceived by the network as a 480 x 480 x 3 array, for which each of its elements can have a value of between 0 and 255. These values describe the pixel intensity at a given point. Here's the main difference between the human eyes and a machine: these array values are the only inputs available to it. The output of a computer receiving those numbers as input will be other numbers describing the probability of the image being a certain class. The first layer of a CNN is always <strong>convolutional</strong>. Suppose having an input that is a 32 x 32 x 3 array of pixel values, let's try to imagine a concrete visualization that clearly and simply explains what a convolutional layer is. Let's try to visualize a torch that shines over the top-left part of the image.</p>
<p> </p>
<p>This following diagram shows the torch shines, covering a 5 x 5 area:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1112 image-border" src="assets/82c754f7-f72d-46e3-859a-e98da3dba6e1.png" style="width:52.25em;height:26.75em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 5.1: 5 x 5 filter</div>
<p>Then the imaginary torch starts sliding over all the other areas of the image. The proper term to call it is <strong>filter</strong> (or <strong>neuron</strong> or <strong>kernel</strong>) and the image region that lights up is called the <strong>receptive field</strong>. In math terms, a filter is an array of numbers (called <strong>weights</strong> or <strong>parameters</strong>). The depth of a filter has to match the depth of the input. Referring to this section example, we have a filter that's dimensions are 5 x 5 x 3. The first position the filter covers (as shown in the diagram in preceding diagram) is the top left corner of the input image. While the filter slides around the image, or convolves (from the Latin verb <em>convolvere</em>, which means to wrap up), it multiplies its values with the image original pixel values. The multiplications are then all summed up (in our example, in total we have 75 multiplications). The outcome is a single number, which represents when the filter is only at the top left of the input image. This process is then repeated for every location on the input image. As with the first one, every unique location produces a single number. Once the filter has completed its sliding process over all the locations of the image, the result is a 28 x 28 x 1 (given a 32 x 32 input image, a 5 x 5 filter can fit 784 different locations) numeric array called <strong>activation map</strong> (or <strong>feature map</strong>).</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pooling layers</h1>
                </header>
            
            <article>
                
<p>It is common practice (as you will see next through the code examples of this chapter and from <a href="3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml" target="_blank">Chapter 7</a>, <em>Training Neural Networks with Spark</em>, onward) to periodically insert a pooling layer between successive convolution layers in a CNN model. This kind of layers scope is to progressively reduce the number of parameters for the network (which translates into a significant lowering of the computation costs). In fact, spatial pooling (which is also found in literature as downsampling or subsampling) is a technique that reduces the dimensionality of each feature map, while at the same time retaining the most important part of the information. Different types of spatial pooling exist. The most used are max, average, sum, and L2-norm.<br/>
<br/>
Let's take as an example, max pooling. This technique requires defining a spatial neighborhood (typically a 2 × 2 window); the largest element within that window is then taken from the rectified feature map. The average pooling strategy requires taking the average or the sum of all elements in the given window. Several papers and use cases provide evidence that max pooling has been shown to produce better results than other spatial pooling techniques.</p>
<p>The following diagram shows an example of max pooling operation (a 2 × 2 window is used here):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1546b290-0a65-447d-9bc8-13e8341f8c6a.png" style="width:52.08em;height:24.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 5.2: Max pooling operation using a 2 × 2 window</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fully connected layers</h1>
                </header>
            
            <article>
                
<p class="mce-root">A fully connected layer is the last layer of a CNN. Fully connected layers, given an input volume, return as output a multi-dimensional vector. The dimension of the output vector matches the number of classes for the particular problem to solve.</p>
<p class="mce-root">This chapter and others in this book present some examples of CNN implementation and training for digit classification purposes. In those cases, the dimension of the output vector would be 10 (the possible digits are 0 to 9). Each number in the 10-dimensional output vector represents the probability of a certain class (digit). The following is an output vector for a digit classification inference:</p>
<p><kbd>[0 0 0 .1 .75 .1 .05 0 0 0]</kbd></p>
<p class="mce-root">How do we interpret those values? The network is telling us that it believes that the input image is a four with a 75% probability (which is the highest in this case), with a 10% probability that the image is a three, another 10% probability that the image is a five, and a 5% probability that the image is a six. A fully connected layer looks at the output of the previous layer in the same network and determines which features most correlate to a particular class.</p>
<p class="mce-root">The same happens not only in digit classification. An example of a general use case of image classification is that, if a model that has been trained using images of animals predicts that the input image is, for example, a horse, it will have high values in the activation maps that represent specific high-level features, like four legs or a tail, just to mention a couple. Similarly, if the same model predicts that an image is a different animal, let's say a fish, it will have high values in the activation maps that represent specific high-level features, like fins or a gill. We can then say that a fully connected layer looks at those high-level features that most strongly correlate to a particular class and has particular weights: this ensures that the correct probabilities for each different class are obtained after the products between weights and the previous layer have been calculated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weights</h1>
                </header>
            
            <article>
                
<p class="mce-root">CNNs share weights in convolutional layers. This means that the same filter is used for each receptive field in a layer and that these replicated units share the same parameterization (weight vector and bias) and form a feature map.</p>
<p class="mce-root"/>
<p class="mce-root">The following diagram shows three hidden units of a network belonging to the same feature map:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1113 image-border" src="assets/c1de62be-e726-41dd-999d-490fa29ca2ed.png" style="width:48.00em;height:20.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 5.3: Hidden units </div>
<p class="mce-root">The weights <span>in the darker gray color in the preceding diagram are shared and identical. This replication allows features detection regardless of the position they have in the visual field. Another outcome of this weight sharing is the following: the efficiency of the learning process increases by drastically reducing the number of free parameters to be learned.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">GoogleNet Inception V3 model</h1>
                </header>
            
            <article>
                
<p>As a concrete implementation of a CNN, in this section, I am going to present the GoogleNet (<a href="https://ai.google/research/pubs/pub43022">https://ai.google/research/pubs/pub43022</a>) architecture by Google (<a href="https://www.google.com/">https://www.google.com/</a>) and its inception layers. It has been presented at the <em>ImageNet Large Scale Visual Recognition Challenge 2014</em> (<em>ILSVRC2014</em>, <a href="http://www.image-net.org/challenges/LSVRC/2014/">http://www.image-net.org/challenges/LSVRC/2014/</a>). Needless to say, it won that competition. The distinct characteristic of this implementation is the following: increased depth and width and, at the same time, a constant computational budget. Improved computing resources utilization is part of the network design.</p>
<p>This chart summarizes all of the layers for this network implementation presented in the context:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0b9d5d53-12f0-43de-a557-fa66e2b1ba98.png" style="width:53.00em;height:34.25em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 5.4: GoogleNet layers</div>
<p>There are 22 layers with parameters (excluding the pooling layers; the total is 27 if they are included) and almost 12 times fewer parameters than the winning architecture of the past editions of the same context. This network has been designed keeping in mind computational efficiency and practicality, so that inference can be run also on individual devices having limited resources, in particular those with a low memory footprint. All the convolution layers use <strong>Rectified Linear Unit</strong> (<span><strong>ReLU</strong>) </span>activation. The of the receptive field is 224 × 224 in the RGB color space (with zero means). Looking at the table in the preceding diagram, the <strong>#3 × 3</strong> and <strong>#5 × 5</strong> reduces are the number of 1 × 1 filters in the reduction layer preceding the 3 × 3 and 5 × 5 convolution layers. The activation function for those reduction layers is ReLU as well.</p>
<p>The diagram at <a href="https://user-images.githubusercontent.com/32988039/33234276-86fa05fc-d1e9-11e7-941e-b3e62771716f.png">https://user-images.githubusercontent.com/32988039/33234276-86fa05fc-d1e9-11e7-941e-b3e62771716f.png</a> shows a schematic view of the network.</p>
<p>In this architecture, each unit from an earlier layer corresponds to a region of the input image—these units are grouped into filter banks. In the layers that are closer to the input, correlated units concentrate in local regions. This results in a lot of clusters concentrated in a single region, so they can be covered by a 1 × 1 convolution in the following layer. However, there could be a smaller number of more spatially split clusters covered by convolutions over larger chunks, and there would be a decreasing number of chunks over larger regions. To prevent those patch-alignment issues, the inception architecture implementations are restricted to use 1 × 1, 3 × 3 and 5 × 5 filters. The suggested architecture is then a combination of layers which output filter banks are aggregated in a single output vector, which represents the input of the next stage. Additionally, adding an alternative pooling path in parallel to each stage could have a further beneficial effect:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-834 image-border" src="assets/0dda3eb9-ac5d-4ba5-be6c-cf620bfe7afc.png" style="width:50.83em;height:28.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 5.5: Naive version of the inception module</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Looking at the preceding diagram, you can understand that, in terms of computational cost, for a layer with a large number of filters, it could be too expensive to have 5 × 5 convolutions (even if there aren't many). And, of course, this becomes a bigger problem when adding more pooling units, because the number of output filters is equal to the number of filters in the previous stage. Definitely merging the output of a pooling layer with outputs of a convolutional layer could inevitably lead to more and more outputs moving from stage to stage. For this reason, a second and more computational idea of the inception architecture has been proposed. The new idea is to reduce dimension where the computational requirements could increase too much. But there's a caveat: low dimensional embeddings could contain lots of information about a large image chunk, but they represent information in a compressed form, making its processing hard. A good compromise is then to keep the representation mostly sparse and at the same time compress the signals only where there is a real need to heavily aggregate them. For this reason, in order to compute reductions, <strong>1 × 1 convolutions</strong> are used before any expensive <strong>3 × 3</strong> and <strong>5 × 5 convolutions</strong>.</p>
<p>The following diagram shows the new module following the preceding consideration:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-835 image-border" src="assets/641e6658-04b0-41a6-bd8a-34a319764999.png" style="width:50.00em;height:29.33em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 5.6: Inception module with dimension reductions</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hands-on CNN with Spark</h1>
                </header>
            
            <article>
                
<p>In the previous sections of this chapter, we went through the theory of CNNs and the GoogleNet architecture. If this is the first time you're reading about these concepts, probably you are wondering about the complexity of the Scala code to implement CNN's models, train, and evaluate them. Adopting a high-level framework like DL4J, you are going to discover how many facilities come out-of-the-box with it and that the implementation process is easier than expected.</p>
<p>In this section, we are going to explore a real example of CNN configuration and training using the DL4J and Spark frameworks. The training data used comes from the <kbd>MNIST</kbd> database (<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>). It contains images of handwritten digits, with each image labeled by an integer. It is used to benchmark the performance of ML and DL algorithms. It contains a training set of 60,000 examples and a test set of 10,000 examples. The training set is used to teach the algorithm to predict the correct label, the integer, while the test set is used to check how accurate the trained network can make guesses.</p>
<p>For our example, we download, and extract somewhere locally, the <kbd>MNIST</kbd> data. A directory named <kbd>mnist_png</kbd> is created. It has two subdirectories: <kbd>training</kbd>, containing the training data, and <kbd>testing</kbd>, containing the evaluation data.</p>
<p>Let's start using DL4J only first (we would add Spark to the stack later). The first thing we need to do is to vectorize the training data. We use <kbd>ImageRecordReader</kbd> (<a href="https://deeplearning4j.org/datavecdoc/org/datavec/image/recordreader/ImageRecordReader.html">https://deeplearning4j.org/datavecdoc/org/datavec/image/recordreader/ImageRecordReader.html</a>) as reader, because the training data are images, and a <kbd>RecordReaderDataSetIterator</kbd> (<a href="http://javadox.com/org.deeplearning4j/deeplearning4j-core/0.4-rc3.6/org/deeplearning4j/datasets/canova/RecordReaderDataSetIterator.html">http://javadox.com/org.deeplearning4j/deeplearning4j-core/0.4-rc3.6/org/deeplearning4j/datasets/canova/RecordReaderDataSetIterator.html</a>) to iterate through the dataset, as follows:</p>
<pre>val trainData = new ClassPathResource("/mnist_png/training").getFile<br/>val trainSplit = new FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS, randNumGen)<br/>val labelMaker = new ParentPathLabelGenerator(); // parent path as the image label<br/>val trainRR = new ImageRecordReader(height, width, channels, labelMaker)<br/>trainRR.initialize(trainSplit)<br/>val trainIter = new RecordReaderDataSetIterator(trainRR, batchSize, 1, outputNum)</pre>
<p class="mce-root"/>
<p>Let's do a min-max scaling of the pixel values from 0-255 to 0-1<span>, as follows</span>:</p>
<pre>val scaler = new ImagePreProcessingScaler(0, 1)<br/>scaler.fit(trainIter)<br/>trainIter.setPreProcessor(scaler)</pre>
<p>The same vectorization needs to be done for the testing data as well.</p>
<p>Let's configure the network<span>, as follows</span>:</p>
<pre>val channels = 1<br/>val outputNum = 10<br/>      <br/>val conf = new NeuralNetConfiguration.Builder()<br/>      .seed(seed)<br/>      .iterations(iterations)<br/>      .regularization(true)<br/>      .l2(0.0005)<br/>      .learningRate(.01)<br/>      .weightInit(WeightInit.XAVIER)<br/>      .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)<br/>      .updater(Updater.NESTEROVS)<br/>      .momentum(0.9)<br/>      .list<br/>      .layer(0, new ConvolutionLayer.Builder(5, 5)<br/>        .nIn(channels)<br/>        .stride(1, 1)<br/>        .nOut(20)<br/>        .activation(Activation.IDENTITY)<br/>        .build)<br/>      .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>        .kernelSize(2, 2)<br/>        .stride(2, 2)<br/>        .build)<br/>      .layer(2, new ConvolutionLayer.Builder(5, 5)<br/>        .stride(1, 1)<br/>        .nOut(50)<br/>        .activation(Activation.IDENTITY)<br/>        .build)<br/>      .layer(3, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>        .kernelSize(2, 2)<br/>        .stride(2, 2)<br/>        .build)<br/>      .layer(4, new DenseLayer.Builder()<br/>        .activation(Activation.RELU)<br/>        .nOut(500)<br/>        .build)<br/>      .layer(5, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)<br/>        .nOut(outputNum)<br/>        .activation(Activation.SOFTMAX).build)<br/>      .setInputType(InputType.convolutionalFlat(28, 28, 1))<br/>      .backprop(true).pretrain(false).build<kbd><br/></kbd></pre>
<p>The <kbd>MultiLayerConfiguration</kbd> object <span>produced</span> (<a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/MultiLayerConfiguration.html">https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/MultiLayerConfiguration.html</a>) can then be used to initialize the model (<a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html">https://deeplearning4j.org/doc/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html</a>)<span>, as follows</span>:</p>
<pre>val model: MultiLayerNetwork = new MultiLayerNetwork(conf)<br/>model.init()</pre>
<p>We can now train (and evaluate) the model,<span> as follows</span>:</p>
<pre>model.setListeners(new ScoreIterationListener(1))<br/>for (i &lt;- 0 until nEpochs) {<br/>    model.fit(trainIter)<br/>    println("*** Completed epoch {} ***", i)<br/>    ...<br/>}</pre>
<p>Let's now put Apache Spark into the game. Through Spark, it is possible to parallelize the training and evaluation in memory across multiple nodes of a cluster.</p>
<p>As usual, create a Spark context first<span>, as follows</span>:</p>
<pre>val sparkConf = new SparkConf<br/>sparkConf.setMaster(master)<br/>        .setAppName("DL4J Spark MNIST Example")<br/>val sc = new JavaSparkContext(sparkConf)</pre>
<p>Then, after vectorizing the training data, parallelize them through the Spark context<span>, as follows</span>:</p>
<pre>val trainDataList = mutable.ArrayBuffer.empty[DataSet]<br/>while (trainIter.hasNext) {<br/>    trainDataList += trainIter.next<br/>}<br/> <br/>val paralleltrainData = sc.parallelize(trainDataList)</pre>
<p>The same needs to be done for the testing data as well.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>After configuring and initializing the model, you can configure Spark for training<span>, as follows</span>:</p>
<pre>var batchSizePerWorker: Int = 16<br/>val tm = new ParameterAveragingTrainingMaster.Builder(batchSizePerWorker)<br/>    .averagingFrequency(5)<br/>    .workerPrefetchNumBatches(2)      <br/>    .batchSizePerWorker(batchSizePerWorker)<br/>    .build</pre>
<p>Create the Spark network<span>, as follows</span>:</p>
<pre>val sparkNet = new SparkDl4jMultiLayer(sc, conf, tm)</pre>
<p>Finally, replace the previous training code with the following:</p>
<pre>var numEpochs: Int = 15<br/>var i: Int = 0<br/>for (i &lt;- 0 until numEpochs) {<br/>    sparkNet.fit(paralleltrainData)<br/>    println("Completed Epoch {}", i)<br/>}</pre>
<p>When done, don't forget to delete the temporary training files<span>, as follows</span>:</p>
<pre>tm.deleteTempFiles(sc)</pre>
<p>The full example is part of the source code shipped with the book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we first went deeper into the CNN main concepts and explored one of the most popular and performing examples of the CNN architecture provided by Google. We started then to implement some code using DL4J and Spark.</p>
<p>In the next chapter, we will follow a similar trail to go deeper into RNNs.</p>


            </article>

            
        </section>
    </body></html>