<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Cancer Types Prediction Using Recurrent Type Networks</h1>
                </header>
            
            <article>
                
<p><span class="Heading2Char">Large-scale cancer genomics data often comes in multiplatform and heterogeneous forms. These datasets impose great challenges in terms of the bioinformatics approach and computational algorithms. Numerous researchers have proposed to utilize this data to overcome several challenges, using classical machine learning algorithms as either the primary subject or a supporting element for cancer diagnosis and prognosis.</span></p>
<p><span class="Heading2Char">In this chapter, we will use some deep learning architectures for cancer type classification from a very-high-dimensional dataset curated from The Cancer Genome Atlas (TCGA).</span> First, we will describe the dataset and perform some preprocessing such that the dataset can be fed to our networks. We will then see how to prepare our programming environment, before moving on to coding with an open source, deep learning library called <strong>Deeplearning4j</strong> (<strong>DL4J</strong>). First, we will revisit the Titanic survival prediction problem again using a <strong>Multilayer Perceptron</strong> (<strong>MLP</strong>) implementation from DL4J.</p>
<p>Then we will use an improved architecture of <strong>Recurrent Neural Networks</strong> (<strong>RNN</strong>) called <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) for cancer type prediction. Finally, we will see some frequent questions related to this project and DL4J hyperparameters/nets tuning.</p>
<p>In a nutshell, we will be learning the following topics in the chapter:</p>
<ul>
<li>Deep learning in cancer genomics</li>
<li>Cancer genomics dataset description</li>
<li>Getting started with Deeplearning4j</li>
<li>Developing a cancer type predictive model using LSTM-RNN</li>
<li>Frequently asked questions</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning in cancer genomics</h1>
                </header>
            
            <article>
                
<p>Biomedical informatics includes all techniques regarding the development of data analytics, mathematical modeling, and computational simulation for the study of biological systems. In recent years, we've witnessed huge leaps in biological computing that has resulted in large, information-rich resources being at our disposal. These cover domains such as anatomy, modeling (3D printers), genomics, and pharmacology, among others.</p>
<p>One of the most famous success stories of biomedical informatics is from the <span>domain </span>of genomics. The <strong>Human Genome Project</strong> (<strong>HGP</strong>) was an international research project with the objective of determining the full sequence of human DNA. This project has been one of the most important landmarks in computational biology and has been used as a base for other projects, including the Human Brain Project, which is determined to sequence the human brain. The data that was used in this thesis is also the indirect result of the HGP.</p>
<p>The era of big data starts from the last decade or so, which was marked by an overflow of digital information in comparison to its analog counterpart. Just in the year 2016, 16.1 zettabytes of digital data were generated, and it is predicted to reach 163 ZB/year by 2025. As good a piece of news as this is, there are some problems lingering, especially of data storage and analysis. For the latter, simple machine learning methods that were used in normal-size data analysis won't be effective anymore and should be substituted by deep neural network learning methods. Deep learning is generally known to deal very well with these types of large and complex datasets.</p>
<p>Along with other crucial areas, the biomedical area has also been exposed to these big data phenomena. One of the main largest data sources is omics data such as genomics, metabolomics, and proteomics. Innovations in biomedical techniques and equipment, such as DNA sequencing and mass spectrometry, have led to a massive accumulation of -omics data.</p>
<p>Typically -omics data is full of veracity, variability and high dimensionality. These datasets are sourced from multiple, and even sometimes incompatible, data platforms. These properties make these types of data suitable for applying DL approaches. Deep learning analysis of -omics data is one of the main tasks in the biomedical sector as it has a chance to be the leader in personalized medicine. By acquiring information about a person's omics data, diseases can be dealt with better and treatment can be focused on preventive measures.</p>
<p>Cancer is generally known to be one of the deadliest diseases in the world, which is mostly due to its complexity of diagnosis and treatment. It is a genetic disease that involves multiple gene mutations. As the importance of genetic knowledge in cancer treatment is increasingly addressed, several projects to document the genetic data of cancer patients has emerged recently. One of the most well known is <strong>The Cancer Genome Atlas</strong> (<strong>TCGA</strong>) project, which is available on the TCGA research network: <a href="http://cancergenome.nih.gov/">http://cancergenome.nih.gov/</a>.</p>
<p>As mentioned before, there have been a number of deep learning implementations in the biomedical sector, including cancer research. For cancer research, most researchers usually use -omics or medical imaging data as inputs. Several research works have focused on cancer analysis. Some of them use either a histopathology image or a PET image as a source. Most of that research focuses on classification based on that image data with <strong>convolutional neural networks</strong> (<strong>CNNs</strong>).</p>
<p>However, many of them use -omics data as their source. Fakoor et al. classified the various types of cancer using patients' gene expression data. Due to the different dimensionality of each data from each cancer type, they used <strong>principal component analysis</strong> (<strong>PCA</strong>) first to reduce the dimensionality of microarray gene expression data.</p>
<div class="packt_infobox">PCA is a statistical technique used to emphasize variation and extract the most significant patterns from a dataset; principal components are the simplest of the true eigenvector-based multivariate analyses. PCA is frequently used for making data exploration easy to visualize. Consequently, PCA is one of the most used algorithms in exploratory data analysis and for making predictive models.</div>
<p>Then they applied sparse and stacked autoencoders to classify various cancers, including acute myeloid leukemia, breast cancer, and ovarian cancer.</p>
<div class="packt_infobox">For detailed information, refer to the following publication, entitled <em>Using deep learning to enhance cancer diagnosis and classification </em>by R. Fakoor et al. in proceedings of the International Conference on Machine Learning, 2013.</div>
<p>Ibrahim et al. , on the other hand, used miRNA expression data from six types of cancer genes/miRNA feature selection. They proposed a novel multilevel feature selection approach named <strong>MLFS</strong> (short for <strong>Multilevel gene</strong>/<strong>miRNA feature selection</strong>), which was based on <strong>Deep Belief Networks (DBN)</strong> and unsupervised active learning.</p>
<div class="packt_infobox">You can read more in the publication titled <em>Multilevel gene/miRNA feature selection using deep belief nets and active learning</em> (R. Ibrahim, et al.) in Proceedings 36th annual International Conference Eng. Med. Biol. Soc. (EMBC), pp. 3957-3960, IEEE, 2014.</div>
<p>Finally, Liang et al. clustered ovarian and breast cancer patients using multiplatform genomics and clinical data. The ovarian cancer dataset contained gene expression, DNA methylation, and miRNA expression data across 385 patients, which were downloaded from <strong>The Cancer Genome Atlas (TCGA)</strong>.</p>
<div class="packt_infobox"><span>You can read more </span>more in the following publication entitled <em>Integrative data analysis of multi-platform cancer data with a multimodal deep learning approach</em> (by M. Liang et al.) in Molecular Pharmaceutics, vol. 12, pp. 928{937, IEEE/ACM Transaction Computational Biology and Bioinformatics, 2015.</div>
<p>The breast cancer dataset included GE data and corresponding clinical information, such as survival time and time to recurrence data, which was collected by the Netherlands Cancer Institute. To deal with this multiplatform data, they used <strong>multimodal Deep Belief Networks</strong> (<strong>mDBN</strong>).</p>
<p>First, they implemented a DBN for each of those data to get their latent features. Then, another DBN used to perform the clustering is implemented using those latent features as the input. Apart from these researchers, much research work is going on to give cancer genomics, identification, and treatment a significant boost.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cancer genomics dataset description</h1>
                </header>
            
            <article>
                
<p>Genomics data covers all data related to DNA on living things. Although in this thesis we will also use other types of data like transcriptomic data (RNA and miRNA), for convenience purposes, all data will be termed as genomics data. Research on human genetics found a huge breakthrough in recent years due to the success of the HGP (1984-2000) on sequencing the full sequence of human DNA.</p>
<p>One of the areas that have been helped a lot due to this is the research of all diseases related to genetics, including cancer. Due to various biomedical analyses done on DNA, there exist various types of -omics or genomics data. Here are some types of -omics data that were crucial to cancer analysis:</p>
<ul>
<li><strong>Raw sequencing data:</strong> This corresponds to the DNA coding of whole chromosomes. In general, every human has 24 types of chromosomes in each cell of their body, and each chromosome consists of 4.6-247 million base pairs. Each base pair can be coded in four different types, which are <strong>adenine</strong> (<strong>A</strong>), <strong>cytosine</strong> (<strong>C</strong>), <strong>guanine</strong> (<strong>G</strong>), and <strong>thymine</strong> (<strong>T</strong>). Therefore, raw sequencing data consists of billions of base pair data, with each coded in one of these four different types.</li>
<li><strong>Single-Nucleotide Polymorphism</strong> (<strong>SNP</strong>) data: Each human has a different raw sequence, which causes genetic mutation. Genetic mutation can cause an actual disease, or just a difference in physical appearance (such as hair color), or nothing at all. When this mutation happens only on a single base pair instead of a sequence of base pairs, it is called <strong>Single-Nucleotide Polymorphism</strong> (<strong>SNP</strong>).</li>
<li><strong>Copy Number Variation</strong> (<strong>CNV</strong>) data: This corresponds to a genetic mutation that happens in a sequence of base pairs. Several types of mutation can happen, including deletion of a sequence of base pairs, multiplication of a sequence of base pairs, and relocation of a sequence of base pairs into other parts of the chromosome.</li>
<li><strong>DNA methylation data</strong>: Which corresponds to the amount of methylation (methyl group connected to base pair) that happens to areas in the chromosome. A large amount of methylation in promoter regions of a gene can cause gene repression. DNA methylation is the reason each of our organs acts differently even though all of them have the same DNA sequence. In cancer, this DNA methylation is disrupted.</li>
<li><strong>Gene expression data</strong>: This corresponds to the number of proteins that were expressed from a gene at a given time. Cancer happens either because of high expression of an oncogene (that is, a gene that causes a tumor), low expression of a tumor suppressor gene (a gene that prevents a tumor), or both. Therefore, the analysis of gene expression data can help discover protein biomarkers in cancer. We will use this in this project.</li>
<li><strong>miRNA expression data</strong>: Corresponds to the amount of microRNA that was expressed at a given time. miRNA plays a role in protein silencing at the mRNA stage. Therefore, an analysis of gene expression data can help discover miRNA biomarkers in cancer.</li>
</ul>
<p>There are several databases of genomics datasets, where the aforementioned data can be found. Some of them focus on the genomics data of cancer patients. These databases include:</p>
<ul>
<li><strong>The Cancer Genome Atlas</strong> (<strong>TCGA</strong>): <strong><a href="https://cancergenome.nih.gov/">https://cancergenome.nih.gov/</a></strong></li>
<li><strong>International Cancer Genome Consortium</strong> (<strong>ICGC</strong>): <strong><a href="https://icgc.org/">https://icgc.org/</a></strong></li>
<li><strong>Catalog of Somatic Mutations in Cancer</strong> (<strong>COSMIC</strong>): <strong><a href="https://cancer.sanger.ac.uk/cosmic">https://cancer.sanger.ac.uk/cosmic</a></strong></li>
</ul>
<p>This genomics data is usually accompanied by clinical data of the patient. This clinical data can comprise general clinical information (for example, age or gender) and their cancer status (for example, cancer location or cancer stage). All of this genomics data itself has a characteristic of high dimensions. For example, the gene expression data for each patient is structured based on the gene ID, which reaches around 60,000 types.</p>
<p>Moreover, some of the data itself comes from more than one format. For example, 70% of the DNA methylation data is collected from breast cancer patients and the remaining 30% are curated from different platforms. Therefore, there are two different structures on in this dataset. Therefore, to analyze genomics data by dealing with the <span>heterogeneity</span>, researchers have often used powerful machine learning techniques or even deep neural networks.</p>
<p>Now let's see what a real-life dataset looks like that can be used for our purpose. We will be using the <span class="heading0">gene expression cancer RNA-Seq dataset downloaded from the UCI machine learning repository (see</span> <a href="https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq">https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq#</a> <span class="heading0">for more information).</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/918516cc-0429-4221-a5e6-b4b6a07981f9.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">The data collection pipeline for the pan-cancer analysis project (source: "Weinstein, John N., et al. 'The cancer genome atlas pan-cancer analysis project.' Nature Genetics 45.10 (2013): 1113-1120")</div>
<p>This dataset is a random subset of another dataset reported in the following paper: Weinstein, John N., et al. <em>The cancer genome atlas pan-cancer analysis project</em>. <em>Nature Genetics 45.10 (2013): 1113-1120</em>. The preceding diagram shows the data collection pipeline for the pan-cancer analysis project.</p>
<p>The name of the project is The Pan-Cancer analysis project. It assembled data from thousands of patients with primary tumors occurring in different sites of the body. It covered 12 tumor types (see the upper-left panel in the preceding figure) including:</p>
<ul>
<li><strong>Glioblastoma Multiform</strong> (<strong>GBM</strong>)</li>
<li><strong>Lymphoblastic acute myeloid leukemia</strong> (<strong>AML</strong>)</li>
<li><strong>Head and Neck Squamous Carcinoma</strong> (<strong>HNSC</strong>)</li>
<li><strong>Lung Adenocarcinoma</strong> (<strong>LUAD</strong>)</li>
<li><strong>lung Squamous Carcinoma</strong> (<strong>LUSC</strong>)</li>
<li><strong>Breast Carcinoma</strong> (<strong>BRCA</strong>)</li>
<li><strong>kidney Renal Clear Cell Carcinoma</strong> (<strong>KIRC</strong>)</li>
<li><strong>ovarian Carcinoma</strong> (<strong>OV</strong>)</li>
<li><strong>Bladder Carcinoma</strong> (<strong>BLCA</strong>)</li>
<li><strong>Colon Adenocarcinoma</strong> (<strong>COAD</strong>)</li>
<li><strong>Uterine Cervical and Endometrial Carcinoma</strong> (<strong>UCEC</strong>)</li>
<li><strong>Rectal Adenocarcinoma</strong> (<strong>READ</strong>)</li>
</ul>
<p>This collection of data is part of the RNA-Seq (HiSeq) PANCAN dataset. It is a random extraction of gene expressions of patients having different types of tumors: BRCA, KIRC, COAD, LUAD, and PRAD.</p>
<p>This dataset is a random collection of cancer patients from 801 patients, each having 20,531 attributes. Samples (instances) are stored row-wise. Variables (attributes) of each sample are RNA-Seq gene expression levels measured by the illumina HiSeq platform. A dummy name (<kbd>gene_XX</kbd>) is given to each attribute. The attributes are ordered consistently with the original submission. For example, <kbd>gene_1</kbd> on <kbd>sample_0</kbd> is significantly and differentially expressed with a a value of <kbd>2.01720929003</kbd>.</p>
<p>When you download the dataset, you will see there are two CSV files:</p>
<ul>
<li><kbd>data.csv</kbd><strong>:</strong> Contains the gene expression data of each sample</li>
<li><kbd>labels.csv</kbd><strong>:</strong> The labels associated with each sample</li>
</ul>
<p>Let's take a look at the processed dataset. Note we will see only a few selected features considering the high dimensionality in the following screenshot, where the first column represents sample IDs (that is, anonymous patient IDs). The rest of the columns represent how a certain gene expression occurs in the tumor samples of the patients:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-777 image-border" src="assets/52a7de3c-2b87-45db-992b-18d3fa4cf1bc.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Sample gene expression dataset</div>
<p>Now look at the labels in <em>Figure 3</em>. Here, <kbd>id</kbd> contains the sample ids and <kbd>Class</kbd> represents the cancer labels:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/daf22379-e09a-4d38-bfde-7ef397fa9d83.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Samples are classified into different cancer types</div>
<p>Now you can imagine why I have chosen this dataset. Well, although we will not have so many samples, the dataset is <span>still </span>very high dimensional. In addition, this type of high-dimensional dataset is very suitable for applying a deep learning algorithm.</p>
<p>Alright. Therefore, if the features and labels are given, can we classify these samples based on features and the ground truth. Why not? We will try to solve the problem with the DL4J library. First, we have to configure our programming environment so that we can start writing our codes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing programming environment</h1>
                </header>
            
            <article>
                
<p>In this section, we will discuss how to configure DL4J, ND4s, Spark, and ND4J before getting started with the coding. The following are prerequisites when working with DL4J:</p>
<ul>
<li>Java 1.8+ (64-bit only)</li>
<li>Apache Maven for automated build and dependency manager</li>
<li>IntelliJ IDEA or Eclipse IDE</li>
<li>Git for version control and CI/CD</li>
</ul>
<p>The following libraries can be integrated with DJ4J to enhance your JVM experience while developing your ML applications:</p>
<ul>
<li><strong>DL4J</strong>: The core neural network framework, which comes up with many DL architectures and underlying functionalities.</li>
<li><strong>ND4J</strong>: Can be considered as the NumPy of the JVM. It comes with some basic operations of linear algebra. Examples are matrix creation, addition, and multiplication.</li>
<li><strong>DataVec</strong>: This library enables ETL operation while performing feature engineering.</li>
<li><strong>JavaCPP</strong>: This library acts as the bridge between Java and Native C++.</li>
<li><strong>Arbiter</strong>: This library provides basic evaluation functionalities for the DL algorithms.</li>
<li><strong>RL4J</strong>: Deep reinforcement learning for the JVM.</li>
<li><strong>ND4S</strong>: This is a scientific computing library, and it also supports n-dimensional arrays for JVM-based languages.</li>
</ul>
<p>If you are using Maven on your preferred IDE, let's define the project properties to mention the versions in the <kbd>pom.xml</kbd> file:</p>
<pre><strong>&lt;properties&gt;</strong><br/>        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;<br/>        &lt;java.version&gt;1.8&lt;/java.version&gt;<br/>        &lt;nd4j.version&gt;1.0.0-alpha&lt;/nd4j.version&gt;<br/>        &lt;dl4j.version&gt;1.0.0-alpha&lt;/dl4j.version&gt;<br/>        &lt;datavec.version&gt;1.0.0-alpha&lt;/datavec.version&gt;<br/>        &lt;arbiter.version&gt;1.0.0-alpha&lt;/arbiter.version&gt;<br/>        &lt;logback.version&gt;1.2.3&lt;/logback.version&gt;<br/>        &lt;dl4j.spark.version&gt;1.0.0-alpha_spark_2&lt;/dl4j.spark.version&gt;<br/><strong>&lt;/properties&gt;</strong></pre>
<p>Then use the following dependencies required for DL4J, ND4S, ND4J, and so on:</p>
<pre><strong>&lt;dependencies&gt;</strong><br/>    &lt;dependency&gt;<br/>        &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>        &lt;artifactId&gt;nd4j-native&lt;/artifactId&gt;<br/>        &lt;version&gt;${nd4j.version}&lt;/version&gt;<br/>    &lt;/dependency&gt;<br/>    &lt;dependency&gt;<br/>        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>        &lt;artifactId&gt;dl4j-spark_2.11&lt;/artifactId&gt;<br/>        &lt;version&gt;1.0.0-alpha_spark_2&lt;/version&gt;<br/>    &lt;/dependency&gt;<br/>    &lt;dependency&gt;<br/>        &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>        &lt;artifactId&gt;nd4j-native&lt;/artifactId&gt;<br/>        &lt;version&gt;1.0.0-alpha&lt;/version&gt;<br/>        &lt;type&gt;pom&lt;/type&gt;<br/>    &lt;/dependency&gt;<br/>    &lt;dependency&gt;<br/>        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>        &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;<br/>        &lt;version&gt;${dl4j.version}&lt;/version&gt;<br/>    &lt;/dependency&gt;<br/>    &lt;dependency&gt;<br/>        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>        &lt;artifactId&gt;deeplearning4j-nlp&lt;/artifactId&gt;<br/>        &lt;version&gt;${dl4j.version}&lt;/version&gt;<br/>    &lt;/dependency&gt;<br/>    &lt;dependency&gt;<br/>        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>        &lt;artifactId&gt;deeplearning4j-zoo&lt;/artifactId&gt;<br/>        &lt;version&gt;${dl4j.version}&lt;/version&gt;<br/>    &lt;/dependency&gt;<br/>    &lt;dependency&gt;<br/>        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>        &lt;artifactId&gt;arbiter-deeplearning4j&lt;/artifactId&gt;<br/>        &lt;version&gt;${arbiter.version}&lt;/version&gt;<br/>    &lt;/dependency&gt;<br/>    &lt;dependency&gt;<br/>        &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>        &lt;artifactId&gt;arbiter-ui_2.11&lt;/artifactId&gt;<br/>        &lt;version&gt;${arbiter.version}&lt;/version&gt;<br/>    &lt;/dependency&gt;<br/>    &lt;dependency&gt;<br/>        &lt;artifactId&gt;datavec-data-codec&lt;/artifactId&gt;<br/>        &lt;groupId&gt;org.datavec&lt;/groupId&gt;<br/>        &lt;version&gt;${datavec.version}&lt;/version&gt;<br/>    &lt;/dependency&gt;<br/>    &lt;dependency&gt;<br/>        &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;<br/>        &lt;artifactId&gt;httpclient&lt;/artifactId&gt;<br/>        &lt;version&gt;4.3.5&lt;/version&gt;<br/>    &lt;/dependency&gt;<br/>    &lt;dependency&gt;<br/>        &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;<br/>        &lt;artifactId&gt;logback-classic&lt;/artifactId&gt;<br/>        &lt;version&gt;${logback.version}&lt;/version&gt;<br/>        &lt;/dependency&gt;<br/><strong>&lt;/dependencies&gt;</strong></pre>
<p>By the way, DL4J comes with Spark 2.1.0. Additionally, if a native system BLAS is not configured on your machine, ND4J's performance will be reduced. You will experience the following warning once you execute simple code written in Scala:</p>
<pre><span class="packt_screen">****************************************************************<br/> WARNING: COULD NOT LOAD NATIVE SYSTEM BLAS<br/> ND4J performance WILL be reduced<br/> ****************************************************************</span></pre>
<p>However, installing and configuring BLAS such as <kbd>OpenBLAS</kbd> or <kbd>IntelMKL</kbd> is not that difficult; you can invest some time and do it. Refer to the following URL for further details: <a href="http://nd4j.org/getstarted.html#open">http://nd4j.org/getstarted.html#open</a>.</p>
<p>Well done! Our programming environment is ready for simple deep learning application development. Now it's time to get your hands dirty with some sample code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Titanic survival revisited with DL4J</h1>
                </header>
            
            <article>
                
<p>In the preceding chapter, we solved the Titanic survival prediction problem using Spark-based MLP. We also saw that by using Spark-based MLP, the user has very little transparency of using the layering structure. Moreover, it was not explicit to define hyperparameters and so on.</p>
<p>Therefore, what I have done is used the training dataset and then performed some preprocessing and feature engineering. Then I randomly split the pre-processed dataset into training and testing (to be precise, 70% for training and 30% for testing). First, we create the Spark session as follows:</p>
<pre><strong>SparkSession</strong> spark = <strong>SparkSession</strong>.builder()<br/>                  .master("local[*]")<br/>                  .config("spark.sql.warehouse.dir", "temp/")// change accordingly<br/>                  .appName("TitanicSurvivalPrediction")<br/>                  .getOrCreate();</pre>
<p>In this chapter, we have seen that there are two CSV files. However,  <kbd>test.csv</kbd> one does not provide any ground truth. Therefore, I decided to use only the <kbd>training.csv</kbd> one, so that we can compare the model's performance. So let's read the training dataset using the spark <kbd>read()</kbd> API:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> df = spark.sqlContext()<br/>                .read()<br/>                .format("com.databricks.spark.csv")<br/>                .option("header", "true") // Use first line of all files as header<br/>                .option("inferSchema", "true") // Automatically infer data types<br/>                .load("data/train.csv");</pre>
<p>We have seen in <a href="fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml" target="_blank">Chapter 1</a>, <span class="item-title"><em>Getting Started with Deep Learning</em> </span>that the <kbd>Age</kbd> and <kbd>Fare</kbd> columns have many null values. So, instead of writing <kbd>UDF</kbd> for each column, here I just replace the missing values of the age and fare columns by their mean:</p>
<pre><strong>Map&lt;String, Object&gt;</strong> m = new <strong>HashMap</strong>&lt;String, Object&gt;();<br/>m.put("Age", 30);<br/>m.put("Fare", 32.2);<br/><strong>Dataset&lt;Row&gt;</strong> trainingDF1 = df2.na().fill(m);  </pre>
<div class="packt_tip">To get more det<span>a</span>iled insights into handling missing/null values and machine learning, interested readers can take a look at Boyan Angelov's blog at <a href="https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce">https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce</a>.</div>
<p>For simplicity, we can drop a few more columns too, such as <kbd>"PassengerId"</kbd>, <kbd>"Name"</kbd>, <kbd>"Ticket"</kbd>, and <kbd>"Cabin"</kbd>:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> trainingDF2 = trainingDF1.drop("PassengerId", "Name", "Ticket", "Cabin");</pre>
<p>Now, here comes the tricky part. Similar to Spark ML-based estimators, DL4J-based networks also need training data in numeric form. Therefore, we now have to convert the categorical features into numerics. For that, we can use a <kbd>StringIndexer()</kbd> transformer. What we will do is we will create two that is, <kbd>StringIndexer</kbd> for the <kbd>"Sex"</kbd> and <kbd>"Embarked"</kbd> columns:</p>
<pre><strong>StringIndexer</strong> sexIndexer = new <strong>StringIndexer</strong>()<br/>                                    .setInputCol("Sex")<br/>                                    .setOutputCol("sexIndex")<br/>                                    .setHandleInvalid("skip");//// we skip column having nulls<br/>        <br/><strong>StringIndexer</strong> embarkedIndexer = new <strong>StringIndexer</strong>()<br/>                                    .setInputCol("Embarked")<br/>                                    .setOutputCol("embarkedIndex")<br/>                                    .setHandleInvalid("skip");//// we skip column having nulls</pre>
<p>Then we will chain them into a single pipeline. Next, we will perform the transformation operation:</p>
<pre><strong>Pipeline</strong> pipeline = <strong>new</strong> <strong>Pipeline</strong>().setStages(<strong>new</strong> <strong>PipelineStage</strong>[] {sexIndexer, embarkedIndexer});</pre>
<p>Then we will fit the pipeline, transform, and drop both the <kbd>"Sex"</kbd> and <kbd>"Embarked"</kbd> columns to get the transformed dataset:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> trainingDF3 = pipeline.fit(trainingDF2).transform(trainingDF2).drop("Sex", "Embarked");</pre>
<p>Then our final pre-processed dataset will have only the numerical features. Note that DL4J considers the last column as the label column. That means DL4J will consider <kbd>"Pclass"</kbd>, <kbd>"Age"</kbd>, <kbd>"SibSp"</kbd>, <kbd>"Parch"</kbd>, <kbd>"Fare"</kbd>, <kbd>"sexIndex"</kbd>, and <kbd>"embarkedIndex"</kbd> as features. Therefore, I placed the <kbd>"Survived"</kbd> column as the last column:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> finalDF = trainingDF3.select("Pclass", "Age", "SibSp","Parch", "Fare",                                                                   <br/>                                           "sexIndex","embarkedIndex", "Survived");<br/>finalDF.show();</pre>
<p>Then we randomly split the dataset into training and testing as 70% and 30%, respectively. That is, we used 70% for training and the rest to evaluate the model:</p>
<pre><strong>Dataset&lt;Row&gt;</strong>[] splits = finalDF.randomSplit(<strong>new </strong><strong>double</strong>[] {0.7, 0.3}); <br/><strong>Dataset&lt;Row&gt;</strong> trainingData = splits[0]; <br/><strong>Dataset&lt;Row&gt;</strong> testData = splits[1];</pre>
<p>Finally, we have both the DataFrames as separate CSV files to be used by DL4J:</p>
<pre>trainingData<br/>      .coalesce(1)// coalesce(1) writes DF in a single CSV<br/>      .write() <br/>      .format("com.databricks.spark.csv")<br/>      .option("header", "false") // don't write the header<br/>      .option("delimiter", ",") // comma separated<br/>      .save("data/Titanic_Train.csv"); // save location<br/><br/>testData<br/>      .coalesce(1)// coalesce(1) writes DF in a single CSV<br/>      .write() <br/>      .format("com.databricks.spark.csv")<br/>      .option("header", "false") // don't write the header<br/>      .option("delimiter", ",") // comma separated<br/>      .save("data/Titanic_Test.csv"); // save location</pre>
<p>Additionally, DL4J does not support the header info in the training set, so I intentionally skipped writing the header.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multilayer perceptron network construction</h1>
                </header>
            
            <article>
                
<p>As I informed you in the preceding chapter, DL4J-based neural networks are made of multiple layers. Everything starts with a <kbd>MultiLayerConfiguration</kbd>, which organizes those layers and their hyperparameters.</p>
<div class="packt_infobox">Hyperparameters are a set of variables that determine how a neural network would learn. There are many parameters, for example, how many times and how often to update the weights of the model (called an <strong>epoch</strong>), how to initialize network weights, which activation function to be used, which updater and optimization algorithms to be used, the learning rate (that is, how fast the model should learn), how many hidden layers are there, how many neurons are there in each layer, and so on.</div>
<p>We now create the network. First, let us create the layers. Similar to the MLP we created in <a href="fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml" target="_blank">Chapter 1</a><span>, </span><span class="item-title"><em>Getting Started with Deep Learning</em></span>, our MLP will have four layers:</p>
<ul>
<li><strong>Layer 0</strong>: Input layer</li>
<li><strong>Lauer 1</strong>: Hidden layer 1</li>
<li><strong>Layer 2</strong>: Hidden layer 2</li>
<li><strong>Layer 3</strong>: Output layer</li>
</ul>
<p>More technically, the first layer is the input layer, and then two layers are placed as hidden layers. For the first three layers, we initialized the weights using Xavier and the activation function is ReLU. Finally, the output layer is placed. This setting is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-751 image-border" src="assets/afcd3570-cc02-41ad-b098-30f8d1fb93f5.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Multilayer perceptron for Titanic survival prediction input layer</div>
<p>We have specified the neurons (that is, nodes), which are an equal number of inputs, and an arbitrary number of neurons as output. We set a smaller value considering very few inputs and features:</p>
<pre><strong>DenseLayer</strong> input_layer = new <strong>DenseLayer</strong>.Builder()<br/>                .weightInit(WeightInit.XAVIER)<br/>                .activation(Activation.RELU)<br/>                .nIn(numInputs)<br/>                .nOut(16)<br/>                .build();</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hidden layer 1</h1>
                </header>
            
            <article>
                
<p>The number of input neurons is equal to the output of the input layer. Then the number of outputs is an arbitrary value. We set a smaller value considering very few inputs and features:</p>
<pre><strong>DenseLayer</strong> hidden_layer_1 = new <strong>DenseLayer</strong>.Builder()<br/>                .weightInit(WeightInit.XAVIER)<br/>                .activation(Activation.RELU)<br/>                .nIn(16).nOut(32)<br/>                .build();</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hidden layer 2</h1>
                </header>
            
            <article>
                
<p>The number of input neurons is equal to the output of hidden layer 1. Then the number of outputs is an arbitrary value. Again we set a smaller value considering very few inputs and features:</p>
<pre> <strong>DenseLayer</strong> hidden_layer_2 = new <strong>DenseLayer</strong>.Builder()<br/>                .weightInit(WeightInit.XAVIER)<br/>                .activation(Activation.RELU)<br/>                .nIn(32).nOut(16)<br/>                .build();</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Output layer</h1>
                </header>
            
            <article>
                
<p>The number of input neurons is equal to the output of the hidden layer 1. Then the number of outputs is equal to the number of predicted labels. We set a smaller value yet again, considering a very few inputs and features.</p>
<p>Here we used the Softmax activation function, which gives us a probability distribution over classes (the outputs sum to 1.0), and the losses function as cross-entropy for binary classification (XNET) since we want to convert the output (probability) to a discrete class, that is, zero or one:</p>
<pre><strong>OutputLayer</strong> output_layer = new OutputLayer.Builder(LossFunction.XENT) // XENT for Binary Classification<br/>                .weightInit(WeightInit.XAVIER)<br/>                .activation(Activation.SOFTMAX)<br/>                .nIn(16).nOut(numOutputs)<br/>                .build();</pre>
<div class="packt_infobox">XNET is used for binary classification with logistic regression. Check out more about this in <kbd>LossFunctions.java</kbd> class in DL4J.</div>
<p>Now we create a <kbd>MultiLayerConfiguration</kbd> by specifying <kbd>NeuralNetConfiguration</kbd> before conducting the training. With DL4J, we can add a layer by calling <kbd>layer</kbd> on the <kbd>NeuralNetConfiguration.Builder()</kbd>, specifying its place in the order of layers (the zero-indexed layer in the following code is the input layer):</p>
<pre><strong>MultiLayerConfiguration</strong> MLPconf = new <strong>NeuralNetConfiguration</strong>.Builder().seed(seed)<br/>                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)<br/>                .weightInit(WeightInit.XAVIER)<br/>                .updater(new Adam(0.0001))<br/>                .list()<br/>                    .layer(0, input_layer)<br/>                    .layer(1, hidden_layer_1)<br/>                    .layer(2, hidden_layer_2)<br/>                    .layer(3, output_layer)<br/>                .pretrain(false).backprop(true).build();// no pre-traning required    </pre>
<p>Apart from these, we also specify how to set the network's weights. For example, as discussed, we use Xavier as the weight initialization and <strong>Stochastic Gradient Descent</strong> (<strong>SGD</strong>) optimization algorithm with Adam as the updater. Finally, we also specify that we do not need to do any pre-training (which is typically needed in DBN or stacked autoencoders). Nevertheless, since MLP is a feedforward network, we set backpropagation as true.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network training</h1>
                </header>
            
            <article>
                
<p>First, we create a <kbd>MultiLayerNetwork </kbd> using the preceding <kbd>MultiLayerConfiguration</kbd>. Then we initialize the network and start the training on the training set:</p>
<pre><strong>MultiLayerNetwork</strong> model = new <strong>MultiLayerNetwork</strong>(MLPconf);<br/>model.init();<br/>log.info("Train model....");<br/>for( int i=0; i&lt;numEpochs; i++ ){<br/>    model.fit(trainingDataIt);<br/>        }</pre>
<p>In the preceding code block, we start training the model by invoking the <kbd>model.fit()</kbd> on the  training set (<kbd>trainingDataIt</kbd> in our case). Now we will discuss how we prepared the training and test set. Well, for reading the training set or test set that are in an inappropriate format (features are numeric and labels are integers), I have created a method called <kbd>readCSVDataset()</kbd>:</p>
<pre><strong>private static DataSetIterator</strong> readCSVDataset(String csvFileClasspath, int batchSize, <br/>               int labelIndex, int numClasses) throws IOException, InterruptedException {<br/>        <strong>RecordReader</strong> rr = new CSVRecordReader();<br/>        <strong>File</strong> input = new File(csvFileClasspath);<br/>        rr.initialize(new FileSplit(input));<br/>        <strong>DataSetIterator</strong> iterator = new <strong>RecordReaderDataSetIterator</strong>(rr, batchSize, labelIndex, numClasses);<br/>        <strong>return</strong> iterator;<br/>    }</pre>
<p>If you see the previous code block, you can realize that it is basically a wrapper that reads the data in CSV format, and then the <kbd>RecordReaderDataSetIterator()</kbd> method converts the record reader as a dataset iterator. Technically, <kbd>RecordReaderDataSetIterator()</kbd> is the main constructor for classification. It takes the following parameters:</p>
<ul>
<li><kbd>RecordReader</kbd>: This is the <kbd>RecordReader</kbd> that provides the source of the data</li>
<li><kbd>batchSize</kbd>: Batch size (that is, number of examples) for the output <kbd>DataSet</kbd> objects</li>
<li><kbd>labelIndex</kbd>: The index of the label writable (usually an <kbd>IntWritable</kbd>) as obtained by <kbd>recordReader.next()</kbd></li>
<li><kbd>numPossibleLabels</kbd>: The number of classes (possible labels) for classification</li>
</ul>
<p>This will then convert the input class index (at position <kbd>labelIndex</kbd>, with integer values <kbd>0</kbd> to <kbd>numPossibleLabels-1</kbd>, inclusive) to the appropriate one-hot output/labels representation. So let's see how to proceed. First, we show the path of training and test sets:</p>
<pre><strong>String</strong> trainPath = "data/Titanic_Train.csv";<br/><strong>String</strong> testPath = "data/Titanic_Test.csv";<br/><br/><strong>int</strong> labelIndex = 7; // First 7 features are followed by the labels in integer <br/><strong>int</strong> numClasses = 2; // number of classes to be predicted -i.e survived or not-survived<br/><strong>int</strong> numEpochs = 1000; // Number of training eopich<br/>    <br/><strong>int</strong> seed = 123; // Randome seed for reproducibilty<br/><strong>int</strong> numInputs = labelIndex; // Number of inputs in input layer<br/><strong>int</strong> numOutputs = numClasses; // Number of classes to be predicted by the network <br/>        <br/><strong>int</strong> batchSizeTraining = 128;         </pre>
<p>Now let's prepare the data we want to use for training:</p>
<pre><strong>DataSetIterator</strong> trainingDataIt = <em>readCSVDataset</em>(trainPath, batchSizeTraining, labelIndex, numClasses);</pre>
<p>Next, let's prepare the data we want to classify:</p>
<pre><strong>int</strong> batchSizeTest = 128;<br/><strong>DataSetIterator</strong> testDataIt = <em>readCSVDataset</em>(testPath, batchSizeTest, labelIndex, numClasses);</pre>
<p>Fantastic! We have managed to prepare the training and test <kbd>DataSetIterator</kbd>. Remember, we will be following nearly the same approach to prepare the training and test sets for other problems too.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model</h1>
                </header>
            
            <article>
                
<p>Once the training has been completed, the next task would be evaluating the model. We will evaluate the model's performance on the test set. For the evaluation, we will be using <kbd>Evaluation()</kbd>; it creates an evaluation object with two possible classes (survived or not survived). More technically, the Evaluation class computes the evaluation metrics such as precision, recall, F1, accuracy, and Matthews' correlation coefficient. The last one is used to evaluate a binary classifier. Now let's take a brief overview on these metrics:</p>
<p><strong>Accuracy</strong> is the ratio of correctly predicted samples to total samples:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/76057702-d43e-4861-ac12-6a036a553a83.png" style="width:13.92em;height:2.42em;"/></div>
<p><strong>Precision</strong> is the ratio of correctly predicted positive samples to the total predicted positive samples:</p>
<div class="CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/1ebcd216-45df-45d1-a389-681ce465fa30.png" style="width:11.58em;height:2.58em;"/></div>
<p><strong>Recall</strong> is the ratio of correctly predicted positive samples to all samples in the actual class—yes:</p>
<div class="CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/557e92fb-f546-40eb-8e6f-f24718421d69.png" style="width:9.58em;height:2.50em;"/></div>
<p><strong>F1 score</strong> is the weighted average (harmonic mean) of Precision and Recall::</p>
<div class="mce-root CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/a931ec3c-90b7-41a3-9800-5114b46a9dc2.png" style="width:16.42em;height:2.50em;"/></div>
<p><strong>Matthews Correlation Coefficient</strong> (<strong>MCC</strong>) is a measure of the quality of binary (two-class) classifications. MCC can be calculated directly from the confusion matrix as follows (given that TP, FP, TN, and FN are already available):</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/69ca053d-0119-46ca-b39e-722ed60ced3c.png" style="width:26.00em;height:2.75em;"/></div>
<p>Unlike the Apache Spark-based classification evaluator, when solving a binary classification problem using the DL4J-based evaluator, special care should be taken for binary classification metrics such as F1, precision, recall, and so on.</p>
<p>Well, we will see these later on. First, let's iterate the evaluation over every test sample and get the network's prediction from the trained model. Finally, the <kbd>eval()</kbd> method checks the prediction against the true classes:</p>
<pre><strong><em>log</em></strong>.info("Evaluate model...."); <br/><strong>Evaluation</strong> eval = <strong>new</strong> <strong>Evaluation</strong>(2) // for class 1 <br/><br/><strong>while</strong>(testDataIt.hasNext()){<br/><strong>DataSet</strong> next = testDataIt.next(); <br/><strong>INDArray</strong> output = model.output(next.getFeatureMatrix());<br/>eval.eval(next.getLabels(), output);<br/>}<br/><strong><em>log</em></strong>.info(eval.stats());<br/><strong><em>log</em></strong>.info("****************Example finished********************");</pre>
<pre><strong>&gt;&gt;&gt;</strong><br/> <strong>==========================Scores========================================</strong><br/> <strong># of classes: 2</strong><br/> <strong>Accuracy: 0.6496</strong><br/> <strong>Precision: 0.6155</strong><br/> <strong>Recall: 0.5803</strong><br/> <strong>F1 Score: 0.3946</strong><br/> <strong>Precision, recall &amp; F1: reported for positive class (class 1 - "1") only</strong><br/> <strong>=======================================================================</strong></pre>
<p>Oops! Unfortunately, we have not managed to achieve very high classification accuracy for class 1 (that is, 65%). Now, we compute another metric called MCC for this binary classification problem.</p>
<pre>// Compute Matthews correlation coefficient <br/><strong>EvaluationAveraging</strong> averaging = <strong>EvaluationAveraging</strong>.<strong><em>Macro</em></strong>; <br/><strong>double</strong> MCC = eval.matthewsCorrelation(averaging); <br/>System.<strong><em>out</em></strong>.println("Matthews correlation coefficient: "+ MCC);</pre>
<pre><strong>&gt;&gt;&gt;</strong><br/> <span class="packt_screen">Matthews's correlation coefficient: 0.22308172619187497</span></pre>
<p>Now let's try to interpret this result based on the Matthews paper (see more at <a href="http://www.sciencedirect.com/science/article/pii/0005279575901099">www.sciencedirect.com/science/article/pii/0005279575901099</a>), which describes the following properties: A correlation of C = 1 indicates perfect agreement, C = 0 is expected for a prediction no better than random, and C = -1 indicates total disagreement between prediction and observation.</p>
<p>Following this, our result shows a weak positive relationship. Alright! Although we have not achieved good accuracy, you guys can still try by tuning hyperparameters or even by changing other networks such as LSTM, which we are going to discuss in the next section. But we'll do so for solving our cancer prediction problem, which is the main goal of this chapter. So stay with me!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cancer type prediction using an LSTM network</h1>
                </header>
            
            <article>
                
<p>In the previous section, we have seen what our data (that is, features and labels) looks like. Now in this section, we try to classify those samples according to their labels. However, as we have seen, DL4J needs the data in a well-defined format so that it can be used to train the model. So let us perform the necessary preprocessing and feature engineering.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset preparation for training</h1>
                </header>
            
            <article>
                
<p>Since we do not have any unlabeled data, I would like to select some samples <span>randomly </span>for test. Well, one more thing is that features and labels come in two separate files. Therefore, we can perform the necessary preprocessing and then join them together so that our pre-processed data will have features and labels together.</p>
<p>Then the rest will be used for training. Finally, we'll save the training and testing set in a separate CSV file to be used later on. First, let's load the samples and see the statistics. By the way, we use the <kbd>read()</kbd> method of Spark but specify the necessary options and format too:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> data = spark.read()<br/>                .option("maxColumns", 25000)<br/>                .format("com.databricks.spark.csv")<br/>                .option("header", "true") // Use first line of all files as header<br/>                .option("inferSchema", "true") // Automatically infer data types<br/>                .load("TCGA-PANCAN-HiSeq-801x20531/data.csv");// set your path accordingly</pre>
<p>Then we see some related statistics such as number of features and number of samples:</p>
<pre><strong>int</strong> numFeatures = data.columns().length;<br/><strong>long</strong> numSamples = data.count();<br/>System.<strong><em>out</em></strong>.println("Number of features: " + numFeatures);<br/>System.<strong><em>out</em></strong>.println("Number of samples: " + numSamples);</pre>
<pre><strong>&gt;&gt;&gt;</strong><br/> <span class="packt_screen">Number of features: 20532<br/> Number of samples: 801</span></pre>
<p>Therefore, there are <kbd>801</kbd> samples from <kbd>801</kbd> distinct patients and the dataset is too high in dimensions, having <kbd>20532</kbd> features. In addition, in <em>Figure 2</em>, we have seen that the <kbd>id</kbd> column represents only the patient's anonymous ID, so we can simply drop it:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> numericDF = data.drop("id"); // now 20531 features left</pre>
<p>Then we load the labels using the <kbd>read()</kbd> method of Spark and also specify the necessary options and format:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> labels = spark.read()<br/>                .format("com.databricks.spark.csv")<br/>                .option("header", "true") // Use first line of all files as header<br/>                .option("inferSchema", "true") // Automatically infer data types<br/>                .load("TCGA-PANCAN-HiSeq-801x20531/labels.csv");<br/>labels.show(10);</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9d270be1-d0f0-4c68-b1c2-36f293e6f48b.png" style=""/></div>
<p>We have already seen how the labels dataframe looks. We will skip the <kbd>id</kbd>. However, the <kbd>Class</kbd> column is categorical. Now, as I said, DL4J does not support categorical labels to be predicted. Therefore, we have to convert it to numeric (integer, to be more specific); for that I would use <kbd>StringIndexer()</kbd> from Spark.</p>
<p>First, create a <kbd>StringIndexer()</kbd>; we apply the index operation to the <kbd>Class</kbd> column and rename it as <kbd>label</kbd>. Additionally, we skip null entries:</p>
<pre><strong>StringIndexer</strong> indexer = new StringIndexer()<br/>                        .setInputCol("Class")<br/>                        .setOutputCol("label")<br/>                        .setHandleInvalid("skip");// skip null/invalid values</pre>
<p>Then we perform the indexing operation by calling the <kbd>fit()</kbd> and <kbd>transform()</kbd> operations as follows:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> indexedDF = indexer.fit(labels)<br/>                         .transform(labels)<br/>                         .select(col("label")<br/>                         .cast(DataTypes.IntegerType));// casting data types to integer</pre>
<p>Now let's take a look at the indexed DataFrame:</p>
<pre>indexedDF.show();</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3af96ada-3e47-48e5-a11b-9d22419e50a8.png" style=""/></div>
<p>Fantastic! Now all of our columns (including features and labels) are numeric. Thus, we can join both features and labels into a single DataFrame. For that, we can use the <kbd>join()</kbd> method from Spark as follows:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> combinedDF = numericDF.join(indexedDF);</pre>
<p>Now we can generate both the training and test sets by randomly splitting the <kbd>combindedDF</kbd>, as follows:</p>
<pre><strong>Dataset&lt;Row&gt;</strong>[] splits = combinedDF.randomSplit(<strong>new</strong><strong>double</strong>[] {0.7, 0.3});//70% for training, 30% for testing<br/><strong>Dataset&lt;Row&gt;</strong> trainingData = splits[0];<br/><strong>Dataset&lt;Row&gt;</strong> testData = splits[1];</pre>
<p>Now let's see the count of samples in each set:</p>
<pre>System.<strong>out</strong>.println(trainingData.count());// number of samples in training set<br/>System.<strong>out</strong>.println(testData.count());// number of samples in test set</pre>
<pre><strong>&gt;&gt;&gt;</strong><br/> <strong>561</strong><br/> <strong>240</strong></pre>
<p>Thus, our training set has <kbd>561</kbd> samples and the test set has <kbd>240</kbd> samples. Finally, save these two sets as separate CSV files to be used later on:</p>
<pre>trainingData.coalesce(1).write()<br/>                .format("com.databricks.spark.csv")<br/>                .option("header", "false")<br/>                .option("delimiter", ",")<br/>                .save("data/TCGA_train.csv");<br/>                <br/>testData.coalesce(1).write()<br/>                .format("com.databricks.spark.csv")<br/>                .option("header", "false")<br/>                .option("delimiter", ",")<br/>                .save("data/TCGA_test.csv");</pre>
<p>Now that we have the training and test sets, we can now train the network with the training set and evaluate the model with the test set. Considering the high dimensionality, I would rather try a better network such as LSTM, which is an improved variant of RNN. At this point, some contextual information about LSTM would be helpful to grasp the idea.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent and LSTM networks</h1>
                </header>
            
            <article>
                
<p>As discussed in <a href="fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml" target="_blank">Chapter 1</a><span>, </span><span class="item-title"><em>Getting Started with Deep Learning</em></span>, RNNs make use of information from the past; they can make predictions in data with high temporal dependencies. A more explicit architecture can be found in following diagram where the temporally shared weights <strong>w2</strong> (for the hidden layer) must be learned in addition to <strong>w1</strong> (for the input layer) and <strong>w3</strong> (for the output layer). From a computational point of view, an RNN takes many input vectors to process and generate output vectors. Imagine that each rectangle in the following diagram has a vectorial depth and other special hidden quirks:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-752 image-border" src="assets/2bc80381-93a3-4b60-90ce-4b00ea30e9a5.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">An RNN architecture where all weights in all layers have to be learned with time</div>
<p>However, we often need to look at <span>only</span><span> </span><span>recent information to perform the present task, rather than stored information or information that arrived a long time ago. This happens frequently in NLP for language modeling. Let's see a common example:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-753 image-border" src="assets/cbde902a-d013-4124-b453-f9ba290d0684.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">If the gap between the relevant information is small, RNNs can learn to use past information</div>
<p>Suppose we want to develop a DL-based NLP model to predict the next word based on the previous words. As a human being, if we try to predict the last word in <em>Berlin is the capital of...,</em> without further context, the next word is <span>most likely </span><em>Germany</em>. In such cases, the gap between the relevant information and the position is small. Thus, RNNs can learn to use past information easily.</p>
<p>However, consider another example that is a bit longer: <em>Reza grew up in Bangladesh. He studied in Korea. He speaks fluent...</em> Now to predict the last word, we would need a little bit more context. In this sentence, the most recent information advises  the network that the next word would probably be the name of a language. However, if we narrow down to language level, the context of Bangladesh (from the previous words) would be needed.</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-754 image-border" src="assets/de957c74-8ffe-4f84-a18a-14e11aa94883.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">If the gap between the relevant information and the place that is needed is bigger, RNNs can't learn to use past information</div>
<p>Here, the gap is larger than in the previous example, so an RNN is unable to learn to map the information. <span class="uiqtextrenderedqtext">Nevertheless,</span> <span class="uiqtextrenderedqtext">g</span><span class="uiqtextrenderedqtext">radients</span> <span class="uiqtextrenderedqtext">for deeper layers are calculated by multiplication (that is, the product) of many gradients coming from activation functions in the multilayer network. If those gradients are very small or close to zero, gradients will easily vanish. On the other hand, when they are bigger than 1, it will possibly explode. So, it becomes very hard to calculate and update.</span> Let's explain them in more detail.</p>
<p><span class="NormalPACKTCarattere">These two issues of RNN are jointly called a</span> <strong>vanishing-exploding gradient</strong> <span class="NormalPACKTCarattere">problem, which directly affects performance. In fact, the backpropagation time rolls out the RNN, creating</span> <em>a very deep</em> <span class="NormalPACKTCarattere">feedforward neural network. The impossibility of getting a long-term context from the RNN is <span>precisely </span>due to this phenomenon; if the gradient vanishes or explodes within a few layers, the network will not be able to learn high-temporal-distance relationships between the data.</span></p>
<p>Therefore, the inability of handling long-term dependency, gradient exploding and vanishing problems is a serious drawback of RNNs. Here comes LSTM as the savior.</p>
<p>As the name signifies, short-term patterns are not forgotten in the long term. An LSTM network is composed of cells (LSTM blocks) linked to each other. Each LSTM block contains three types of gates: an input gate, an output gate, and a forget gate. They implement the functions of writing, reading, and reset on the cell memory, respectively. These gates are not binary but analog (generally managed by a sigmoidal activation function mapped in the range <em>[0, 1]</em>, where zero indicates total inhibition and one shows total activation).</p>
<p>We can consider an LSTM cell very much like a basic cell, but still the training will converge more quickly and it will detect long-term dependencies in the data. Now the question would be: how does an LSTM cell work? The architecture of a basic LSTM cell is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-755 image-border" src="assets/1fc7d7b2-006a-4aff-99c1-89e49f6facb1.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Block diagram of an LSTM cell</div>
<p>Now, let's see the mathematical notation behind this architecture. If we don't look at what's inside the LSTM box, the LSTM cell itself looks exactly like a regular memory cell, except that its state is split into two vectors, <em>h(t)</em> and <em>c(t)</em>:</p>
<ul>
<li><em>c</em> is a cell</li>
<li><em>h(t)</em> is the short-term state</li>
<li><em>c(t)</em> is the long-term state</li>
</ul>
<p>Now, let's open the box! The key idea is that the network can learn the following:</p>
<ul>
<li>What to store in the long-term state</li>
<li>What to throw away</li>
<li>What to read</li>
</ul>
<p>In more simplified words, in an STM, all hidden units of the original RNN are replaced by memory blocks, where each memory block contains a memory cell to store input history information and three gates to define how to update the information. These gates are an input gate, a forget gate, and an output gate.</p>
<p>The presence of these gates allows LSTM cells to remember information for an indefinite period. In fact, if the input gate is below the activation threshold, the cell will retain the previous state, and if the current state is enabled, it will be combined with the input value. As the name suggests, the forget gate resets the current state of the cell (when its value is cleared to zero), and the output gate decides whether the value of the cell must be carried out or not.</p>
<p>Although the long-term state is copied and passed through the tanh function, internally in an LSTM cell, incorporation between two activation functions is needed. For example, in the following diagram, tanh <span class="uiqtextrenderedqtext">decides which values to add to the state, with the help of the sigmoid gate:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/21bcb1c3-96e4-41ff-b57f-6281f0e45715.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The internal organization of the LSTM cell structure</div>
<p>Now, since this book is not meant to teach theory, I would like to stop the discussion here, but interested readers can find more details on the DL4J website at <a href="https://deeplearning4j.org/lstm.html">https://deeplearning4j.org/lstm.html</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset preparation</h1>
                </header>
            
            <article>
                
<p>In the previous section, we prepared the training and test sets. However, we need to put some extra efforts into making them consumable by DL4J. To be more specific, DL4J expects the training data as numeric and the last column to be the label column, and the remaining are features.</p>
<p>We will now try to prepare our training and test sets like that. First, we show the files where we saved the training and test sets:</p>
<pre><strong>String</strong> trainPath = "data/TCGA_train.csv"; // training set<br/><strong>String</strong> testPath = "data/TCGA_test.csv"; // test set</pre>
<p>Then, we define the required parameters, such as the number of features, number of classes, and batch size. Here, I use <kbd>128</kbd> as the <kbd>batchSize</kbd> but adjust it accordingly:</p>
<pre><strong>int</strong> labelIndex = 20531;// number of features<br/><strong>int</strong> numClasses = 5; // number of classes to be predicted<br/><strong>int</strong> batchSize = 128; // batch size (feel free to adjust)</pre>
<p>This dataset is used for training:</p>
<pre><strong>DataSetIterator</strong> trainingDataIt = readCSVDataset(trainPath, batchSize, labelIndex, numClasses);</pre>
<p>This is the data we want to classify:</p>
<pre><strong>DataSetIterator</strong> testDataIt = <em>readCSVDataset</em>(testPath, batchSize, labelIndex, numClasses);</pre>
<p>If you see the preceding two lines, you can realize that <kbd>readCSVDataset()</kbd> is basically a wrapper that reads the data in CSV format, and then the <kbd>RecordReaderDataSetIterator()</kbd> method converts the record reader as a dataset iterator. For more details, refer to the <em>Titanic survival revisited with DL4J </em>section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LSTM network construction</h1>
                </header>
            
            <article>
                
<p>As discussed in the Titanic survival prediction section, again everything starts with <kbd>MultiLayerConfiguration</kbd>, which organizes those layers and their hyperparameters. Our LSTM network consists of five layers. The input layer is followed by three LSTM layers. Then the last layer is an RNN layer, which is also the output layer.</p>
<p>More technically, the first layer is the input layer, and then three layers are placed as LSTM layers. For the LSTM layers, we initialized the weights using Xavier. We use SGD as the optimization algorithm with Adam updater and the activation function is tanh.</p>
<p>Finally, the RNN output layer has a softmax activation function, <span>which</span> gives us a probability distribution over classes (that is, outputs sum to <em>1.0</em>), and MCXENT, which is the Multiclass cross-entropy loss function. This setting is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2a036774-db9c-4e74-bbec-5e29b818ca01.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Multilayer perceptron for Titanic survival prediction. It takes 20,531 features and fixed bias (that is, 1) and generates multi-class outputs.</div>
<p><span class="uiqtextrenderedqtext">For creating LSTM layers, DL4J provides both LSTM and GravesLSTM classes. The latter is</span> an LSTM recurrent net, based on <em>Supervised Sequence Labelling with Recurrent Neural Networks</em> (see more at <a href="http://www.cs.toronto.edu/~graves/phd.pdf">http://www.cs.toronto.edu/~graves/phd.pdf</a>).</p>
<div class="packt_tip">GravesLSTM is not compatible with CUDA. Thus, using LSTM is recommended while performing the training on GPU. Otherwise, GravesLSTM is faster than LSTM.</div>
<p><span class="uiqtextrenderedqtext">Now before, we start creating the network, let's define required hyperparameters such as the number of input/hidden/output nodes (neurons):</span></p>
<pre>// Network hyperparameters<br/><strong>int</strong> numInputs = labelIndex; // number of input features<br/><strong>int</strong> numOutputs = numClasses; // number of classes to be predicted<br/><strong>int</strong> numHiddenNodes = 5000; // too many features, so 5000 sounds good</pre>
<p>We now create a network configuration and conduct network training. With DL4J, you add a layer by calling <kbd>layer</kbd> on the <kbd>NeuralNetConfiguration.Builder()</kbd>, specifying its place in the order of layers (the zero-indexed layer in the following code is the input layer):</p>
<pre>// Create network configuration and conduct network training<br/><strong>MultiLayerConfiguration</strong> LSTMconf = new <strong>NeuralNetConfiguration</strong>.Builder()<br/>            .seed(seed)    //Random number generator seed for improved repeatability. Optional.<br/>            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)<br/>            .weightInit(WeightInit.XAVIER)<br/>            .updater(new Adam(0.001))<br/>            .list()<br/>            .layer(0, new LSTM.Builder()<br/>                        .nIn(numInputs)<br/>                        .nOut(numHiddenNodes)<br/>                        .activation(Activation.RELU)<br/>                        .build())<br/>            .layer(1, new LSTM.Builder()<br/>                        .nIn(numHiddenNodes)<br/>                        .nOut(numHiddenNodes)<br/>                        .activation(Activation.RELU)<br/>                        .build())<br/>            .layer(2, new LSTM.Builder()<br/>                        .nIn(numHiddenNodes)<br/>                        .nOut(numHiddenNodes)<br/>                        .activation(Activation.RELU)<br/>                        .build())<br/>            .layer(3, new RnnOutputLayer.Builder()<br/>                        .activation(Activation.SOFTMAX)<br/>                        .lossFunction(LossFunction.MCXENT)<br/>                        .nIn(numHiddenNodes)<br/>                        .nOut(numOutputs)<br/>                        .build())<br/>            .pretrain(false).backprop(true).build();</pre>
<p>Finally, we also specify that we do not need to do any pre-training (which is typically needed in DBN or stacked autoencoders).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network training</h1>
                </header>
            
            <article>
                
<p>First, we create a <kbd>MultiLayerNetwork </kbd> using the preceding <kbd>MultiLayerConfiguration</kbd>. Then we initialize the network and start the training on the training set:</p>
<pre><strong>MultiLayerNetwork</strong> model = <strong>new</strong> MultiLayerNetwork(LSTMconf);<br/>model.init();<br/><br/>log.info("Train model....");<br/>for(int i=0; i&lt;numEpochs; i++ ){<br/>    model.fit(trainingDataIt);<br/> }</pre>
<p>Typically, this type of network has so many hyperparameters. Let's print the number of parameters in the network (and for each layer):</p>
<pre><strong>Layer</strong>[] layers = model.getLayers();<br/><strong>int</strong> totalNumParams = 0;<br/><strong>for</strong>( <strong>int</strong> i=0; i&lt;layers.length; i++ ){<br/>         <strong>int</strong> nParams = layers[i].numParams();<br/>        System.<strong><em>out</em></strong>.println("Number of parameters in layer " + i + ": " + nParams);<br/>       totalNumParams += nParams;<br/>}<br/>System.<strong><em>out</em></strong>.println("Total number of network parameters: " + totalNumParams);</pre>
<pre><strong>&gt;&gt;&gt;</strong><br/> <strong>Number of parameters in layer 0: 510655000</strong><br/> <strong>Number of parameters in layer 1: 200035000</strong><br/> <strong>Number of parameters in layer 2: 200035000</strong><br/> <strong>Number of parameters in layer 3: 25005</strong><br/> <strong>Total number of network parameters: 910750005</strong></pre>
<p>As I said, our network has 910 million parameters, which is huge. This also poses a great challenge while tuning hyperparameters. However, we will see some tricks in the FAQs section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model</h1>
                </header>
            
            <article>
                
<p>Once the training has been completed, the next task would be evaluating the model. We will evaluate the model's performance on the test set. For the evaluation, we will be using <kbd>Evaluation(). This method </kbd> creates an evaluation object with five possible classes. First, let's iterate the evaluation over every test sample and get the network's prediction from the trained model. Finally, the <kbd>eval()</kbd> method checks the prediction against the true class:</p>
<pre><strong><em>log</em></strong>.info("Evaluate model....");<br/><strong>Evaluation</strong> eval = <strong>new</strong> <strong>Evaluation</strong>(5) // for 5 classes<br/><strong>while</strong>(testDataIt.hasNext()){<br/>        <strong>DataSet</strong> next = testDataIt.next();<br/>        <strong>INDArray</strong> output = model.output(next.getFeatureMatrix());<br/>        eval.eval(next.getLabels(), output);<br/>}<br/><strong><em>log</em></strong>.info(eval.stats());<br/><strong><em>log</em></strong>.info("****************Example finished********************");</pre>
<pre><strong>&gt;&gt;&gt;</strong><br/> <span class="packt_screen">==========================Scores========================================<br/>  # of classes:    5<br/>  Accuracy:        0.9950<br/>  Precision:       0.9944<br/>  Recall:          0.9889<br/>  F1 Score:        0.9915<br/> Precision, recall &amp; F1: macro-averaged (equally weighted avg. of 5 classes)<br/> ========================================================================</span><br/> <strong>****************Example finished********************</strong></pre>
<p>Wow! Unbelievable! Our LSTM network has accurately classified the samples so accurately. Finally, let's see how the classifier predicts across each class:</p>
<pre><span class="packt_screen">Predictions labeled as 0 classified by model as 0: 82 times<br/> Predictions labeled as 1 classified by model as 1: 17 times<br/> Predictions labeled as 1 classified by model as 2: 1 times<br/> Predictions labeled as 2 classified by model as 2: 35 times<br/> Predictions labeled as 3 classified by model as 3: 31 times<br/> Predictions labeled as 4 classified by model as 4: 35 times</span></pre>
<p>The predictive accuracy for cancer type prediction using LSTM is suspiciously higher. Did our network underfit? Is there any way to observe how the training went? In other words, the question would be why our LSTM neural net shows 100% accuracy. We will try to answer these questions in the next section. So stay with me!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Frequently asked questions (FAQs)</h1>
                </header>
            
            <article>
                
<p>Now that we have solved the Titanic survival prediction problem with an acceptable level of accuracy, there are other practical aspects of this problem and overall deep learning phenomena that need to be considered too. In this section, we will see some frequently asked questions that might be already in your mind. Answers to these questions can be found in <em>Appendix A</em>.</p>
<ol>
<li> Can't we use MLP to solve the cancer type prediction by handling this too high-dimensional data?</li>
<li>Which activation and loss function can be used with RNN type nets?</li>
<li>What is the best way of recurrent net weight initialization?</li>
<li>Which updater and optimization algorithm should be used?</li>
<li>In the Titanic survival prediction problem, we did not experience good accuracy. What could be possible reasons and how can we improve the accuracy?</li>
<li>The predictive accuracy for cancer type prediction using LSTM is suspiciously higher. Did our network underfit? Is there any way to observe how the training went?</li>
<li>Which type RNN variants should I use, that is, LSTM or GravesLSTM?</li>
<li>Why is my neural net throwing nan score values?</li>
<li>How to configure/change the DL4J UI port?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, <span class="Heading2Char">we saw how to classify cancer patients on the basis of tumor types from a very-high-dimensional gene expression dataset curated from</span> TCGA<span class="Heading2Char">.</span> Our LSTM architecture managed to achieve 100% accuracy, which is outstanding. Nevertheless, we discussed many aspects of DL4J, which will be helpful in upcoming chapters. Finally, we saw answers to some frequent questions related to this project, LSTM network, and DL4J hyperparameters/nets tuning.</p>
<p>In the next chapter,  we will see how to develop an end-to-end project for handling a multilabel (each entity can belong to multiple classes) image classification problem using CNN based on Scala and the DL4J framework on real Yelp image datasets. We will also discuss some theoretical aspects of CNNs before getting started. Nevertheless, we will discuss how to tune hyperparameters for better classification results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Answers to questions</h1>
                </header>
            
            <article>
                
<p><strong>Answer</strong> <strong>to question 1</strong>: The answer is yes, but not very comfortably. That means a very deep feedforward network such as deep MLP or DBN can classify them with too many iterations.</p>
<p>However, also to speak frankly, MLP is the weakest deep architecture and is not ideal for very high dimensions like this. Moreover, DL4J has deprecated DBN since the DL4J 1.0.0-alpha release. Finally, I would still like to show an MLP network config just in case you want to try it:</p>
<pre>// Create network configuration and conduct network training<br/><strong>MultiLayerConfiguration</strong> MLPconf = new <strong>NeuralNetConfiguration</strong>.Builder().seed(seed)<br/>                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)<br/>                .updater(new Adam(0.001)).weightInit(WeightInit.XAVIER).list()<br/>                .layer(0,new DenseLayer.Builder().nIn(numInputs).nOut(32)<br/>                        .weightInit(WeightInit.XAVIER)<br/>                        .activation(Activation.RELU).build())<br/>                .layer(1,new DenseLayer.Builder().nIn(32).nOut(64).weightInit(WeightInit.XAVIER)<br/>                        .activation(Activation.RELU).build())<br/>                .layer(2,new DenseLayer.Builder().nIn(64).nOut(128).weightInit(WeightInit.XAVIER)<br/>                        .activation(Activation.RELU).build())<br/>                .layer(3, new OutputLayer.Builder(LossFunction.XENT).weightInit(WeightInit.XAVIER)<br/>                        .activation(Activation.SOFTMAX).weightInit(WeightInit.XAVIER).nIn(128)<br/>                        .nOut(numOutputs).build())<br/>                .pretrain(false).backprop(true).build();    </pre>
<p>Then, just change the line from <kbd>MultiLayerNetwork model = new MultiLayerNetwork(LSTMconf);</kbd> to <kbd><strong>MultiLayerNetwork</strong> model = <strong>new</strong> <strong>MultiLayerNetwork</strong>(MLPconf);</kbd>. Readers can see the full source in the <kbd>CancerPreddictionMLP.java</kbd> file.</p>
<p><strong>Answer</strong> <strong>to question 2</strong>: There are two aspects to be aware of with regard to the choice of activation function.</p>
<p><strong>Activation</strong> <strong>function for hidden layers:</strong> Usually, ReLU or leakyrelu activations are good choices. Some other activation functions (tanh, sigmoid, <span>and so on</span>) are more prone to vanishing gradient problems. However, for LSTM layers, the tanh activation function is still commonly used.</p>
<p><span class="uiqtextrenderedqtext">A note here: The reason some people do not want to use Rectified Linear Unit (ReLU) is that it seems not to perform very well relative to smoother nonlinearity, such as sigmoid in the case of RNNs (see more at <a href="https://arxiv.org/pdf/1312.4569.pdf">https://arxiv.org/pdf/1312.4569.pdf</a>). Even tanh works much better with LSTM. Therefore, I used tanh as the activation function in the LSTM layers.</span></p>
<p><strong>The activation</strong> <strong>function for the output layer:</strong> For classification problems, using the Softmax activation function combined with the negative log-likelihood / MCXENT is recommended. However, for a regression problem, the "IDENTITY" activation function is a good choice, with MSE as the loss function. In short, the choice is really application-specific.</p>
<p><strong>Answer</strong> <strong>to question 3:</strong> Well, we need to make sure that the network weights are neither too big nor too small. I will not recommend using random or zero; rather, Xavier weight initialization is usually a good choice for this.</p>
<p><strong>Answer</strong> <strong>to question 4:</strong> Unless SGD converges well, momentum/rmsprop/adagrad optimizers are a good choice. However, I've often used Adam as the updater and observed good performance too.</p>
<p><strong>Answer</strong> <strong>to question 5:</strong> Well, there is no concrete answer to this question. In fact, there could be several reasons. For example, probably we have not chosen the appropriate hyperparameters. Secondly, we may not have enough data. Thirdly, we could be using another network such as LSTM. Fourthly, we did not normalize our data.</p>
<p>Well, for the third one, you can of course try using an LSTM network similarly; I did it for cancer type prediction. For the fourth one, of course normalized data always gives better classification accuracy. Now the question would be: what is the distribution of your data? Are you scaling it properly? Well, continuous values have to be in the range of -1 to 1, 0 to 1, or distributed normally with mean 0 and standard deviation 1.</p>
<p>Finally, I would like to give you a concrete example of data normalization in the Titanic example. For that, we can use DL4J's <kbd>NormalizerMinMaxScaler()</kbd>. Once we created the training dataset iterator, we can instantiate a <kbd>NormalizerMinMaxScaler()</kbd> object and then normalize the data by invoking the <kbd>fit()</kbd> method. Finally, we perform the transformation using the <kbd>setPreProcessor()</kbd> method, as follows:</p>
<pre><strong>NormalizerMinMaxScaler</strong> preProcessor = <strong>new</strong> NormalizerMinMaxScaler();<br/>preProcessor.fit(trainingDataIt);<br/>trainingDataIt.setPreProcessor(preProcessor);</pre>
<p>Now, for the test dataset iterator, we apply the same normalization for better results but without invoking the <kbd>fit()</kbd> method:</p>
<pre>testDataIt.setPreProcessor(preProcessor);</pre>
<p>More elaborately, <kbd>NormalizerMinMaxScaler ()</kbd> acts as the pre-processor for datasets that normalizes feature values (and optionally label values) to lie between a minimum and maximum value (by default, between 0 and 1). Readers can see the full source in the <kbd>CancerPreddictionMLP.java</kbd> file. After this normalization, I experienced slightly better result for class 1, as follows (you could try the same for class 0 too):</p>
<pre><span class="packt_screen">==========================Scores========================================<br/> # of classes: 2<br/> Accuracy: 0.6654<br/> Precision: 0.7848<br/> Recall: 0.5548<br/> F1 Score: 0.2056<br/> Precision, recall &amp; F1: reported for positive class (class 1 - "1") only<br/> ========================================================================</span></pre>
<p><strong>Answer to question 6:</strong> In the real world, it's a rare case that a neural net would achieve 100% accuracy. However, if the data is linearly separable, then yes, it's possible! Take a look at the following scatter plots, which show that the black line clearly separates the red points and the dark blue points:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/184293ff-a744-4941-b1c5-8084d6786233.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Very clearly and linearly separable data points</div>
<p>More technically, since a neuron's output (before it passes through an activation function) is a linear combination of its inputs, a network consisting of a single neuron can learn this pattern. That means if our neural net got the line right, it is possible to achieve 100% accuracy.</p>
<p>Now, to answer the second part: probably no. To prove this, we can observe the training loss, score, <span>and so on</span> on the DL4J UI, which is the interface used to visualize the current network status and progress of training in real time on your browser.</p>
<p>The UI is typically used to help with tuning neural networks, that is, the selection of hyperparameters to obtain good performance for a network. These are already in the <kbd>CancerPreddictionLSTM.java</kbd> file, so do not worry but just keep going.</p>
<p><strong>Step 1: Adding the dependency for DL4J to your project</strong></p>
<p>In the following dependency tag, <kbd>_2.11 suffix</kbd> is used to specify which Scala version should be used for the Scala play framework. You should setting accordingly:</p>
<pre><strong>&lt;dependency&gt;</strong><br/>    &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>    &lt;artifactId&gt;deeplearning4j-ui_2.11&lt;/artifactId&gt;<br/>    &lt;version&gt;${dl4j.version}&lt;/version&gt;<br/><strong>&lt;/dependency&gt;</strong></pre>
<p><strong>Step 2: Enabling UI in your project</strong></p>
<p>This is relatively straightforward. First, you have to initialize the user interface backend as follows:</p>
<pre><strong>UIServer</strong> uiServer = <strong>UIServer</strong>.<em>getInstance</em>();</pre>
<p>You then configure where the network information is to be stored. Then the StatsListener can be added to collect this information:</p>
<pre><strong>StatsStorage</strong> statsStorage = <strong>new</strong> <strong>InMemoryStatsStorage</strong>();</pre>
<p>Finally, we attach the StatsStorage instance to the UI:</p>
<pre>uiServer.attach(statsStorage); <br/><strong>int</strong> listenerFrequency = 1;<br/>model.setListeners(<strong>new</strong> StatsListener(statsStorage, listenerFrequency));</pre>
<p><strong>Step 3: Start collecting information by invoking the fit() method<br/></strong>Information will then be collected and routed to the UI when you call the <kbd>fit</kbd> method on your network.</p>
<p><strong>Step 4: Accessing the UI<br/></strong>Once it is configured, the UI can be accessed at <a href="http://localhost:9000/train">http://localhost:9000/train</a>. Now to answer "Did our network under fit? Is there any way to observe how the training went?" We can observe the <strong>Model Score versus Iteration Chart</strong> on the overview page<strong>.</strong> As suggested in the model tuning section at <a href="https://deeplearning4j.org/visualization">https://deeplearning4j.org/visualization</a><strong>,</strong> we have the following observation<strong>:</strong></p>
<ul>
<li>The overall score versus iteration should go down over time</li>
<li>The score does not increase consistently but decreases drastically when  the iteration moves on</li>
</ul>
<p>The issue might be that there is no noise in the line chart, which is ideally expected (that is, the line will go up and down within a small range).</p>
<p>Now to deal with this, again we can normalize the data and perform the training again to see how the performance differs. Well, I would like to leave this up to you, folks. One more clue would be following the same data normalization that we discussed in question 5.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2cbc83a2-105b-4027-a354-c9bddef68089.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">LSTM model score over iterations</div>
<p>Now, another observation would be worth mentioning too. For example, the gradients did not vanish until the end, which becomes clearer from this figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/49b98b00-61ba-44dc-bb37-9622cb4545c3.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">The LSTM network's gradients across different iterations</div>
<p>Finally, the activation functions performed their role consistently, which becomes clearer from the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5c7ef990-ef40-45d1-948f-343bfa31c34f.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">The LSTM network's activation functions performed their role consistently across different layers</div>
<p>The thing is that there are many more factors to be considered too. However, in reality, tuning a neural network is often more an art than a science, and we have not considered many aspects as I said. Yet, do not worry; we will see them in upcoming projects. So hang on and let's move on to the next question.</p>
<p><strong>Answer</strong> <strong>to question 7:</strong> LSTM allows both GPU/CUDA support, but GravesLSTM supports only CUDA, hence no support for CuDNN yet. Nonetheless, if you want faster training and convergence, using LSTM type is recommended.</p>
<p><strong>Answer</strong> <strong>to question 8:</strong> While training a neural network, the backpropagation involves multiplications across very small gradients. This happens due to limited precision when representing real numbers; values very close to zero cannot be represented.</p>
<p>It introduces an arithmetic underflow issue, which often happens in a deeper network such as DBN, MLP, or CNN. Moreover, if your network throws NaN, then you'll need to retune your network to avoid very small gradients.</p>
<p><strong>Answer</strong> <strong>to question 9:</strong> You can set the port by using the org.deeplearning4j.ui.port system property. To be more specific, to use port <kbd>9001</kbd> for example, pass the following to the JVM on launch:</p>
<p><kbd>-Dorg.deeplearning4j.ui.port=9001</kbd></p>


            </article>

            
        </section>
    </body></html>