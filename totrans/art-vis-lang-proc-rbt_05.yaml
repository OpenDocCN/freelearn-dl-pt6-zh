- en: '*Chapter 5*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional Neural Networks for Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain how convolutional neural networks work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a convolutional neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve the constructed model by using data augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use state-of-the-art models by implementing transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to use probability distributions as a form
    of unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about how a neural network can be trained
    to predict values and how a **recurrent neural network (RNN)**, based on its architecture,
    can prove to be useful in many scenarios. In this chapter, we will discuss and
    observe how **convolutional neural networks (CNNs)** work in a similar way to
    dense neural networks (also called fully-connected neural networks, as mentioned
    in *Chapter 2*, *Introduction to Computer Vision*).
  prefs: []
  type: TYPE_NORMAL
- en: CNNs have neurons with weights and biases that are updated during training time.
    CNNs are mainly used for image processing. Images are interpreted as pixels and
    the network outputs the class it thinks the image belongs to, along with loss
    functions that state the errors with every classification and every output.
  prefs: []
  type: TYPE_NORMAL
- en: These types of networks make an assumption that the input is an image or works
    like an image, allowing them to work more efficiently (CNNs are faster and better
    than deep neural networks). In the following sections, you will learn more about
    CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentals of CNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this topic, we will see how CNNs work and explain the process of convolving
    an image.
  prefs: []
  type: TYPE_NORMAL
- en: We know that images are made up of pixels, and if the image is in RGB, for example,
    it will have three channels where each letter/color (Red-Green-Blue) has its own
    channel with a set of pixels of the same size. Fully-connected neural networks
    do not represent this depth in an image in every layer. Instead, they have a single
    dimension to represent this depth, which is not enough. Furthermore, they connect
    every single neuron of one layer to every single neuron of the next layer, and
    so on. This in turn results in lower performance, meaning you would have to train
    a network for longer and would still not get good results.
  prefs: []
  type: TYPE_NORMAL
- en: '**CNNs** are a category of neural networks that has ended up being very effective
    for tasks such as classification and image recognition. Although, they also work
    very well for sound and text data. CNNs consist of an input, hidden layers, and
    an output layer, just like normal neural networks. The input and hidden layers
    are commonly formed by **convolutional layers**, **pooling layers** (layers that
    reduce the spatial size of the input), and **fully-connected layers** (fully-connected
    layers are explained in *Chapter 2*, *Introduction to Computer Vision*). Convolutional
    layers and pooling layers will be explained later on in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs give depth to every layer, starting from the original depth of the image
    to deeper hidden layers as well. The following figure shows how a CNN works and
    what one looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: Representation of a CNN](img/C13550_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Representation of a CNN'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding figure, the CNN takes an input image of 224 x 224 x 3, which
    by convolutional processes is transformed into the next layer, which compresses
    the size but has more depth to it (we will explain how these processes work later
    on). These operations continue over and over until the graphical representation
    is flattened and these dense layers are used to end up with the corresponding
    classes of the dataset as output.
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional Layers:** Convolutional layers consist of a set of **filters**
    of fixed size (typically a small size), which are matrices with certain values/weights,
    that are applied all over the input (an image, for example), by computing the
    scalar product between the filters and the input, which is called convolution.
    Each of these filters produces a two-dimensional activation map, which is stacked
    along the depth of the input. These activation maps look for features in the input
    and will determine how well the network learns. The more filters you have, the
    deeper the layer is, thus, the more your network learns, but the slower it gets
    at training time. For instance, in a particular image say, you would like to have
    3 filters in the first layer, 96 filters in the next layer, 256 in the next, and
    so on. Note that, at the beginning of the network, there are usually fewer filters
    than at the end or in the middle of the network. This is because the middle and
    the end of the network have more potential features to extract, thus we need more
    filters, of a smaller size, toward the end of the network. This is because the
    deeper we advance into the network, the more we look at little details within
    an image, therefore we want to extract more features from those details to get
    a good understanding of the image.'
  prefs: []
  type: TYPE_NORMAL
- en: The sizes of the filters of convolutional layers often go from 2x2 to 7x7, for
    example, depending on whether you are at the beginning of the network (higher
    sizes) or toward the end (smaller sizes).
  prefs: []
  type: TYPE_NORMAL
- en: In Figure 5.1, we can see convolution being applied using filters (in light
    blue) and the output would be a single value that goes to the next step/layer.
  prefs: []
  type: TYPE_NORMAL
- en: After performing convolution, and before another convolution is applied, a max
    pooling (**pooling layer**) layer is normally applied in order to reduce the size
    of the input so that the network can get a deeper understanding of the image.
    Nevertheless, lately, there is a tendency to avoid max pooling and instead encourage
    strides, which are naturally applied when performing convolution, so we are going
    to explain image reduction by naturally applying convolution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Strides:** This is the length, defined in pixels, for the steps of the filter
    being applied over the entire image. If a stride of one is selected, the filter
    will be applied, but one pixel at a time. Similarly, if a stride of two is selected,
    then the filter will be applied two pixels at a time, the output size is smaller
    than the input, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example. Firstly, Figure 5.2 will be used as the filter to
    convolve the image, which is a 2x2 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2: Convolution filter](img/C13550_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Convolution filter'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'And the following could be the image (matrix) we are convolving:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3: Image to convolve](img/C13550_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Image to convolve'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Of course, this is not a real image, but for the sake of simplicity, we are
    taking a matrix of 4x4 with random values to demonstrate how convolution works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we want to apply convolution with stride equal to 1, this would be
    the process, graphically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4: Convolution process Stride=1](img/C13550_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Convolution process Stride=1'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The preceding Figure shows a 2x2 filter being applied to the input image, pixel
    by pixel. The process goes from left to right and from top to bottom.
  prefs: []
  type: TYPE_NORMAL
- en: The filter multiplies every value of every position in its matrix to every value
    of every position of the zone (matrix) where it's being applied. For instance,
    in the first part of the process, the filter is being applied to the first 2x2
    part of the image [1 2; 5 6] and the filter we have is [2 1; -1 2], so it would
    be 1*2 + 2*1 + 5*(-1) + 6*2 = 11.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting image, after applying the filter matrix, would be as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5: Convolution result Stride=1](img/C13550_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Convolution result Stride=1'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the resulting image is now one dimension smaller. This is because
    there is another parameter, called **padding**, which is set to "valid" by default,
    which means that the convolution will be applied normally; that is, applying the
    convolution makes the image one pixel thinner by nature. If it is set to "same,"
    the image will be surrounded by one line of pixels with a value equal to zero,
    thus the output matrix will have the same size as the input matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are going to apply a stride of 2, to reduce the size by 2 (just like
    a max pooling layer of 2x2 would do). Remember that we are using a padding equal
    to "valid."
  prefs: []
  type: TYPE_NORMAL
- en: 'The process would have fewer steps, just like in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6: Convolution process Stride=2](img/C13550_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Convolution process Stride=2'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'And the output image/matrix would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7:  Convolution result Stride=2](img/C13550_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Convolution result Stride=2'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The resulting image would be an image of 2x2 pixels. This is due to the natural
    process of convolution with stride equal to 2.
  prefs: []
  type: TYPE_NORMAL
- en: These filters, which are applied on every convolutional layer, have weights
    that the neural network adjusts so that the outputs of those filters help the
    neural network learn valuable features. These weights, as explained, are updated
    by the process of backpropagation. As a reminder, backpropagation is the process
    where the network's loss (or the amount of errors) of the predictions made versus
    the expected results in a training step of the network is calculated, updating
    all the weights of the neurons of the network that have contributed to that error
    so that they do not make the same mistake again.
  prefs: []
  type: TYPE_NORMAL
- en: Building Your First CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For this chapter, we are going to still use Keras on top of TensorFlow as the
    backend, as mentioned in *Chapter 2, Introduction to Computer Vision* of this
    book. Also, we will still use Google Colab to train our network.
  prefs: []
  type: TYPE_NORMAL
- en: Keras is a very good library for implementing convolutional layers, as it abstracts
    the user so that layers do not have to be implemented by hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapter 2*, *Introduction to Computer Vision,* we imported the Dense, Dropout,
    and BatchNormalization layers by using the `keras.layers` package, and to declare
    convolutional layers of two dimensions, we are going to use the same package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Conv2D` module is just like the other modules: you have to declare a sequential
    model, which was explained in *Chapter 2, Introduction to Computer Vision* of
    this book, and we also add `Conv2D`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For the first layer, the input shape has to be specified, but after that, it
    is no longer needed.
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter that must be specified is the **number of filters** that
    the network is going to learn in that layer. As mentioned before, in the earlier
    layers, we will filter few layers which will be learned, rather than the layers
    deeper in the network.
  prefs: []
  type: TYPE_NORMAL
- en: The second parameter that must be specified is the **kernel size**, which is
    the size of the filter applied to the input data. Usually, a kernel of size 3x3
    is set, or even 2x2, but sometimes when the image is large, a bigger kernel size
    is set.
  prefs: []
  type: TYPE_NORMAL
- en: The third parameter is **padding**, which is set to "valid" by default, but
    it needs to be set to "same," as we want to preserve the size of the input in
    order to understand the behavior of down-sampling the input.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth parameter is **strides**, which, by default, is set to (1, 1). We
    will be setting it to (2, 2), since there are two numbers here and it has to be
    set for both the x and y axes.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the first layer, we will apply the same methodology as was mentioned
    in *Chapter 2*, *Introduction to Computer Vision*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As a reminder, the **BatchNormalization** layer is used to normalize the inputs
    of each layer, which helps the network converge faster and may give better results
    overall.
  prefs: []
  type: TYPE_NORMAL
- en: The `activation` function is a function that takes the input and calculates
    a weighted sum of it, adding a bias and deciding whether it should be activated
    or not (outputting 1 and 0, respectively).
  prefs: []
  type: TYPE_NORMAL
- en: The **Dropout** layer helps the network avoid overfitting, which is when the
    accuracy of the training set is much higher than the accuracy of the validation
    set, by switching off a percentage of neurons.
  prefs: []
  type: TYPE_NORMAL
- en: We could apply more sets of layers like this, varying the parameters, depending
    on the size of the problem to solve.
  prefs: []
  type: TYPE_NORMAL
- en: The last layers remain the same as those of dense neural networks, depending
    on the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 17: Building a CNN'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This exercise uses the same packages and libraries as *Chapter 2, Introduction
    to Computer Vision*. These libraries are Keras, Numpy, OpenCV, and Matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we are going to take the same problem as *Chapter 2*, *Activity
    2*, *Classify 10 Types of Clothes of the Fashion-MNIST Database*.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, in that activity, the neural network that was built was not capable
    of generalizing well enough to classify the unseen data that we passed to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, this problem is a classification problem, where the model has
    to classify 10 types of clothes correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: Open up your Google Colab interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder for the book and download the `Datasets` folder from GitHub
    and upload it in the folder in your drive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import drive and mount it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Every time you use a new collaborator, mount the drive to the desired folder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once you have mounted your drive for the first time, you will have to enter
    the authorization code mentioned by clicking on the URL given by Google and press
    the **Enter** key on your keyboard:![Figure 5.8: Mounting on Google Collab](img/C13550_05_08.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.8: Mounting on Google Collab'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now that you have mounted the drive, you need to set the path of the directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The path mentioned in step 5 may change as per your folder setup on Google Drive.
    The path will always begin with `cd /content/drive/My Drive/`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, let''s import the data from Keras and initialize the random seed to
    42 for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We import NumPy in order to pre-process the data and Keras utils to one-hot
    encode the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We declare the `Sequential` function to make a sequential model in Keras, the
    callbacks, and, of course, the layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: We have imported a callback called **EarlyStopping**. What this callback does
    is stop the training after a number of epochs, where the metric that you choose
    (for example, validation accuracy) has dropped. You can set that number with the
    number of epochs that you want.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we are going to build our first CNN. First, let''s declare the model as
    `Sequential` and add the first `Conv2D`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We add 32 filters as is the first layer, and a filter size of 3x3\. Padding
    is set to "`same`" and the strides are set to 2 to naturally reduce the dimensionality
    of the `Conv2D` module.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We follow this layer by adding `Activation` and `BatchNormalization` layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are going to add another three layers with the same characteristics as before,
    applying dropout and jumping to another block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we apply dropout of 20%, which turns off 20% of the neurons in the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are going to do the same procedure one more time but with 64 filters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the end of the network, we apply the `Flatten` layer to make the output
    of the last layer one-dimensional. We apply a `Dense` layer with 512 neurons.
    Where the logistics of the network occur, we apply the `Activation` layer and
    the `BatchNormalization` layer, before applying a `Dropout` of 50%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And, finally, we declare the last layer as a `dense` layer with 10 neurons,
    which is the number of classes of the dataset, and a `Softmax` activation function,
    which establishes which class the image is more likely to be, and we return the
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s declare the model along with the callbacks and compile it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For compiling, we are using the same optimizer. For declaring the checkpoint,
    we are using the same parameters. For declaring `EarlyStopping`, we are using
    the validation loss as the main metric and we set a patience of five epochs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let the training begin!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We set the batch size to 128 because there are enough images and because this
    way, it will take less time to train. The number of epochs is set to 100, as `EarlyStopping`
    will take care of stopping the training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The accuracy obtained is better than in the exercise in *Chapter 2*, *Introduction
    to Computer Vision* – we have obtained an accuracy of **92.72%**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Take a look at the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13550_05_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.9: val_acc shown as 0.9240, which is 92.72%'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The entire code for this exercise is available on GitHub: [https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise17/Exercise17.ipynb](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise17/Exercise17.ipynb).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s try with the same examples that we tried in *Activity 2*, *Classify
    10 Types of Clothes of the Fashion-MNIST Database* of *Chapter 2*, which is located
    in `Dataset/testing/`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10: Prediction of clothes using CNNs](img/C13550_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Prediction of clothes using CNNs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As a reminder, here is the table with the number of corresponding clothes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11: The table with the number of corresponding clothes      ](img/C13550_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: The table with the number of corresponding clothes'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that the model has predicted all the pictures well, so we can state
    that the model is far better than one with only dense layers.
  prefs: []
  type: TYPE_NORMAL
- en: Improving Your Model - Data Augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are situations, at times, where you would not be able to improve the accuracy
    of your model by building a better model. Sometimes, the problem is not the model
    but the data. One of the most important things to consider when working with machine
    learning is that the data you work with has to be good enough for a potential
    model to generalize that data.
  prefs: []
  type: TYPE_NORMAL
- en: Data can represent real-life things, but it can also include incorrect data
    that may perform badly. This can happen when you have incomplete data or data
    that does not represent the classes well. For those cases, data augmentation has
    become one of the most popular approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data augmentation actually increases the number of samples of the original
    dataset. For computer vision, this could mean increasing the number of images
    in a dataset. There are several data augmentation techniques, and you may want
    to use a specific technique, depending on the dataset. Some of these techniques
    are mentioned here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rotation**: The user sets the degree of rotation for images in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flip**: To flip the images horizontally or vertically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Crop**: Crop a section from the images randomly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Change color**: Change or vary the color of the images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Add Noise**: To add noise to images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying these or other techniques, you end up generating new images that vary
    from the original ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to implement this in code, Keras has a module called `ImageDataGenerator`,
    where you declare transformations that you want to apply to your dataset. You
    can import that module using this line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to declare the variable that is going to apply all those changes to
    your dataset, you have to declare it as in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can see what attributes you can pass to `ImageDataGenerator` by looking
    at this documentation from Keras: [https://keras.io/preprocessing/image/](https://keras.io/preprocessing/image/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After declaring `datagen`, you have to compute some calculations for feature-wise
    normalization by using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, `x_train` is your training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to train the model using data augmentation, the following code should
    be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`Datagen.flow()` is used so that data augmentation can be applied. As Keras
    does not know when to stop applying data augmentation in the given data, `Steps_per_epoch`
    is the parameter that sets that limit, which should be the length of the training
    set divided by the batch size.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we are going to jump right into the second exercise of this chapter to observe
    the output. Data augmentation promises better results and better accuracy. Let's
    find out whether that is true or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 18: Improving Models Using Data Augmentation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we are going to use the The Oxford - III Pet dataset, which
    is RGB images, of varying sizes and several classes, of different cat/dog breeds.
    In this case, we will separate the dataset into two classes: cats and dogs, for
    simplicity. There are 1,000 images for each class, which is not much, but it will
    increment the effect of data augmentation. This dataset is stored in the `Dataset/dogs-cats/`
    folder, added on GitHub.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will build a CNN and train it with and without data augmentation, and we
    will compare the results:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For this exercise, we are going to open another Google Colab notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire code for this exercise can be found on GitHub: [https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise18/Exercise18.ipynb](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise18/Exercise18.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Open up your Google Colab interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder for the book and download the `Datasets` folder from GitHub
    and upload it in the folder in your drive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import drive and mount it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Every time you use a new collaborator, mount the drive to the desired folder.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once you have mounted your drive for the first time, you have to enter the authorization
    code mentioned by clicking on the URL given by Google.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that you have mounted the drive, you need to set the path of the directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The path mentioned in step 5 may change as per your folder setup on Google Drive.
    The path will always begin with `cd /content/drive/My Drive/`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, let''s use these two methods, which we have already used before, to
    load the data from disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The size of the image is specified as 128x128\. This size is larger than the
    sizes used before, because we need more detail in these images, as the classes
    are more difficult to differentiate and the subjects are presented in varying
    positions, which makes the work even more difficult.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We load the corresponding images of dogs and cats as `X` for the images and
    `y` for the labels, and we print the shape of those:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 5.12: Dogs-cats data shape](img/C13550_05_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.12: Dogs-cats data shape'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we will import `random`, set the seed, and show some samples of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 5.13: Image samples of the Oxford Pet dataset](img/C13550_05_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.13: Image samples of the Oxford Pet dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To pre-process the data, we are going to use the same procedure as in *Exercise 17:
    Building a CNN*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we separate `X` and `y` into `x_train` and `y_train` for the training
    set, and `x_test` and `y_test` for the testing set, and we print the shapes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 5.14: Training and testing set shapes](img/C13550_05_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.14: Training and testing set shapes'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We import the corresponding data to build, compile, and train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s build the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model goes from 16 filters in the very first layer to 128 filters at the
    end, doubling the size in every 2 layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Because this problem is harder (we have bigger images with 3 channels and 128x128
    images), we have made the model deeper, adding another couple of layers with 16
    filters at the beginning (the first layer having a kernel size of 5x5, which is
    better in the very first stages) and another couple of layers with 128 filters
    at the end of the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have set the patience to 15 epochs for the EarlyStopping callback because
    it takes more epochs for the model to converge to the sweet spot, and the validation
    loss can vary a lot until then.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The batch size is also low as we do not have much data, but it could be increased
    to 16 easily.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, evaluate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13550_05_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.15: Output showing the accuracy of the model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see from the preceding figure, the accuracy achieved in this dataset
    with this model is **67.25%**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We are going to apply data augmentation to this process. We have to import
    ImageDataGenerator from Keras and declare it with transformations that we are
    going to make:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following transformations have been applied:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have set a rotation range of 15 degrees because dogs and cats within images
    can be positioned in slightly different ways (feel free to tweak this parameter).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have set the width shift range and height shift range to 0.2 to shift the
    image horizontally and vertically, as an animal could be anywhere within the image
    (also tweakable).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have set the horizontal flip property to `True` because these animals can
    be flipped in the dataset (horizontally; with vertical flipping, it is much more
    difficult to find an animal).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, we set zoom range to 0.3 to make random zooms on the images as the
    dogs and cats may be farther in the image or closer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We fit the `datagen` instance declared with the training data in order to compute
    quantities for feature-wise normalization and declare and compile the model again
    to make sure we are not using the previous one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we train the model with the `fit_generator` method of the model and
    the `flow()` method of the `datagen` instance generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We set the `steps_per_epoch` parameter equal to the length of the training set
    divided by the batch size (8).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We also set the number of workers to 4 to take advantage of the 4 cores of
    the processor:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.16: Output showing the accuracy of the model](img/C13550_05_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.16: Output showing the accuracy of the model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see from the preceding figure, with data augmentation, we achieve
    an accuracy of **81%**, which is far better.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If we want to load the model that we just trained (dogs versus cats), the following
    code achieves that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s try the model with unseen data. The data can be found in the `Dataset/testing`
    folder and the code from *Exercise 17*, *Building a CNN* will be used (but with
    different names for the samples):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In these lines of code, we are loading an image, resizing it to the expected
    size (128 x 128), normalizing the image – as we did with the training set – and
    reshaping it to (1, 128, 128, 3) to fit as input in the neural network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We continue the for loop:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/C13550_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.17: Prediction of the Oxford Pet dataset with unseen data using CNNs
    and data augmentation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that the model has made all the predictions well. Note that not all
    the breeds are stored in the dataset, so not all the cats and dogs will be predicted
    properly. Adding more types of breeds would be necessary in order to achieve that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 5: Making Use of Data Augmentation to Classify correctly Images of
    Flowers'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, you are going to put into practice what you have learned.
    We are going to use a different dataset, where the images are bigger (150x150).
    There are 5 classes in this dataset: daisy, dandelion, rose, sunflower, and tulip.
    There are, in total, 4,323 images, which is fewer when compared to the previous
    exercises we performed. The classes do not have the same number of images either,
    but do not worry about that. The images are RGB, so there will be three channels.
    We have stored them in NumPy arrays of each class, so we will provide a way to
    load them properly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will guide you through this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the dataset by using this code, as the data is stored in NumPy format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Show some samples from the dataset by importing `random` and `matplotlib`, using
    a random index to access the `X` set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The NumPy arrays were stored in BGR format (OpenCV format), so in order to
    show the images properly, you will need to use the following code to change the
    format to RGB (only to show the image): `image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will need to import `cv2`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Normalize the `X` set and set the labels to categorical (the `y` set).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the sets into a training and testing set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a CNN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: As we have bigger images, you should consider adding more layers, thus reducing
    the image size, and the first layer should contain a bigger kernel (the kernel
    should be an odd number when it is bigger than 3).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Declare ImageDataGenerator from Keras with the changes that you think will suit
    the variance of the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model. You can either choose an EarlyStopping policy or set a high
    number of epochs and wait or stop it whenever you want. If you declare the Checkpoint
    callback, it will always save only the best validation loss model (if that is
    the metric you are using).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Evaluate the model using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: This will print the accuracy of the model. Note that batch_size is the batch
    size you have set for your training sets and for `x_test` and `y_test`, which
    are your testing sets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can use this code in order to evaluate any model, but first you need to
    load the model. If you want to load the entire model from a `.h5` file, you will
    have to use this code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`from keras.models import load_model`     `model = load_model(''model.h5'')`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Try the model with unseen data. In the `Dataset/testing/` folder, you will
    find five images of flowers that you can load to try it out. Remember that the
    classes are in this order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: classes=['daisy','dandelion','rose','sunflower','tulip']
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'So, the result should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.18: Prediction of roses using CNNs](img/C13550_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.18: Prediction of roses using CNNs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 313.
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-Art Models - Transfer Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Humans do not learn each and every task that they want to achieve from scratch;
    they usually take previous knowledge as a base in order to learn tasks much faster.
  prefs: []
  type: TYPE_NORMAL
- en: When training neural networks, there are some tasks that are extremely expensive
    to train for every individual, such as having hundreds of thousands of images
    for training and having to distinguish between two or more similar objects, ending
    up having a cost of days to achieve good performance, for example. These neural
    networks are trained to achieve this expensive task, and because neural networks
    are capable of saving that knowledge, then other models can take advantage of
    those weights to retrain specific models for similar tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer learning** does just that – it transfers the knowledge of a pretrained
    model to your model, so you can take advantage of that knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: So, for example, if you want to make a classifier that is capable of identifying
    five objects but that task seems too expensive to train (it takes knowledge and
    time), you can take advantage of a pretrained model (usually trained on the famous
    **ImageNet** dataset) and retrain the model adapted to your problem. The ImageNet
    dataset is a large visual database designed for use in visual object recognition
    research and has more than 14 million images with more than 20,000 categories,
    which is very expensive for an individual to train.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, you load the model with the weights of the dataset where it was
    trained, and if you want to achieve a different problem, you only have to change
    the last layer of the model. If the model is trained on ImageNet, it could have,
    1000 classes but you only have 5 classes, so you would change the last layer to
    a dense layer with only 5 neurons. You could add more layers before the last one,
    though.
  prefs: []
  type: TYPE_NORMAL
- en: 'The layers of the model that you have imported (the base model) can be frozen
    so their weights do not reflect on the training time. Depending on this, there
    are two types of transfer learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Traditional**: Freeze all the layers of the base model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning**: Freeze only a part of the base model, typically the first
    layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Keras, we can import famous pretrained models such as Resnet50 and VGG16\.
    You can import a pretrained model with or without weights (in Keras, there are
    only weights for ImageNet), which includes the top of the model or not. The input
    shape can only be only specified if the top is not included and with a minimum
    size of 32.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the following lines of code, you would import the Resnet50 model without
    the top, with the `imagenet` weights and with a shape of 150x150x3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have included the top of the model because you want to use the last
    dense layers of the model (let''s say your problem is similar to ImageNet but
    with different classes), then you should write this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This code gets rid of the classification layer (the last dense layer) and prepares
    the model so that you can add your own last layer. Of course, you could add more
    layers at the end before adding your classification layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have not added the top of the model, then you should add your own top
    with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Here, `GlobalAveragePooling2D` is like a type of max pooling.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these kinds of models, you should preprocess the data just as you did
    with the data that trained those models (if you are using the weights). Keras
    has a `preprocess_input` method that does that for every model. For example, for
    ResNet50, it would be like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: You pass your array of images to that function and then you will have your data
    ready for training.
  prefs: []
  type: TYPE_NORMAL
- en: The **learning rate** in a model is how fast it should convert the model to
    a local minimum. Usually, you do not have to worry about this but if you are retraining
    a neural network, this is a parameter that you have to tweak. When you are retraining
    a neural network, you should decrease the value of this parameter so that the
    neural network does not unlearn what it has already learned. This parameter is
    tweaked when declaring the optimizer. You can avoid tweaking this parameter, although
    the model may end up not ever converging or overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: With this kind of approach, you could train your network with very little data
    and get good results overall, because you take advantage of the weights of the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: You can combine transfer learning with data augmentation as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 19: Classifying €5 and €20 Bills Using Transfer Learning with Very
    Little Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This problem is about differentiating €5 bills from €20 bills with very little
    data. We have 30 images for every class, which is much less than we have had in
    previous exercises. We are going to load the data, declare the pretrained model,
    then declare the changes on the data with data augmentation and train the model.
    After that, we will check how well the model performs with unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: Open up your Google Colab interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You would need to mount the `Dataset` folder on your drive and set the path
    accordingly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Declare functions to load the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the data is resized to 224x224.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The data is stored in `Dataset/money/`, where you have both classes in subfolders.
    In order to load the data, you have to write the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The label for the €20 bill is 0 and it's 1 for the €5 bill.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s show the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 5.21: Samples of bills](img/C13550_05_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.19: Samples of bills'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we are going to declare the pretrained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this case, we are loading the MobileNet model with the weights of imagenet.
    We are not including the top so we should build our own top. The input shape is
    224x224x3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have built the top of the model by taking the output of the last layer of
    MobileNet (which is not the classification layer) and start building on top of
    that. We have added `GlobalAveragePooling2D` for image reduction, a dense layer
    that we can train for our specific problem, a `Dropout` layer to avoid overfitting,
    and the classifier layer at the end.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The dense layer at the end has two neurons, as we have only two classes, and
    it has the `Softmax` activation function. For binary classification, the Sigmoid
    function can also be used, but it changes the entire process as you should not
    make the labels categorical and the predictions look different.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Afterward, we create the model that we are going to train with the input of
    MobileNet as input and the classification dense layer as output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We are going to do fine-tuning. In order to do that, we have to freeze some
    of the input layers and keep the rest of the trainable data, unchanged:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s compile the model with the `Adadelta` optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will use the `preprocess_input` method that we imported previously to
    preprocess the `X` set for MobileNet, and then we convert label `y` to one-hot
    encoding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the `train_test_split` method to split the data into a training set
    and testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are going to apply data augmentation to our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As a bill can be at different angles, we choose to make a rotation range of
    90º. The other parameters seem reasonable for this task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s declare a checkpoint to save the model when the validation loss decreases
    and train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have set the batch size to 4 because we have only a few samples of data and
    we do not want to pass all the samples to the neural network at once, but in batches.
    We are not using the EarlyStopping callback because the loss goes up and down
    due to the lack of data and the use of Adadelta with a high learning rate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check the results:![Figure 5.22: Showing the desired output](img/C13550_05_20.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.20: Showing the desired output'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding figure, we can see that, in the 7th epoch, we already achieve
    100% accuracy with low loss. This is due to the lack of data on the validation
    set, because with only 12 samples you cannot tell whether the model is performing
    well against unseen data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s run the code to calculate the accuracy of this model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.23: Accuracy achieved of 100%](img/C13550_05_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.21: Accuracy achieved of 100%'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s try this model with new data. There are test images in the `Dataset/testing`
    folder. We have added four examples of bills to check whether the model predicts
    them well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this code, we have loaded the unseen examples as well, and we have clubbed
    the output image, which looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.24: Prediction of bills](img/C13550_05_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.22: Prediction of bills'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The model has predicted all the images precisely!
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! Now you are able to train a model with your own dataset when
    you have little data, thanks to transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The complete code for this exercise is uploaded on GitHub: https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/blob/master/Lesson05/Exercise19/Exercise19.ipynb.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CNNs have shown much better performance than fully-connected neural networks
    when dealing with images. In addition, CNNs are also capable of accomplishing
    good results with text and sound data.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs have been explained in depth, as have how convolutions work and all the
    parameters that come along with them. Afterward, all this theory was put into
    practice with an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation is a technique for overcoming a lack of data or a lack of
    variation in a dataset by applying simple transformations to the original data
    in order to generate new images. This technique has been explained and also put
    into practice with an exercise and an activity, where you were able to experiment
    with the knowledge you acquired.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is a technique used when there is a lack of data or the problem
    is so complex that it would take too long to train on a normal neural network.
    Also, this technique does not need much of an understanding of neural networks
    at all, as the model is already implemented. It can also be used with data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning was also covered and put into practice with an exercise where
    the amount of data was very small.
  prefs: []
  type: TYPE_NORMAL
- en: Learning how to build CNNs is very useful for recognizing objects or environments
    in computer vision. When a robot is using its vision sensors to recognize an environment,
    normally, CNNs are employed and data augmentation is used to improve the CNNs
    performance. In *Chapter 8*, *Object Recognition to Guide the Robot Using CNNs,*
    the CNN concepts you have learned about will be applied to a real-world application,
    and you will be able to recognize an environment using deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Before applying these techniques to recognize the environment, first you need
    to learn how to manage a robot that will be able to recognize an environment.
    In *Chapter 6, Robot Operating System (ROS)*, you will learn how to manage a robot
    using a simulator by taking advantage of software called ROS.
  prefs: []
  type: TYPE_NORMAL
