["```py\nval spark = SparkSession\n      .builder\n       .appName(\"StructuredNetworkWordCount\")\n       .master(master)\n       .getOrCreate()\n```", "```py\nval lines = spark.readStream\n       .format(\"socket\")\n       .option(\"host\", host)\n       .option(\"port\", port)\n       .load()\n```", "```py\nval words = lines.as[String].flatMap(_.split(\" \"))\n```", "```py\nval wordCounts = words.groupBy(\"value\").count()\n```", "```py\nval query = wordCounts.writeStream\n       .outputMode(\"complete\")\n       .format(\"console\")\n       .start()\n```", "```py\nquery.awaitTermination()\n```", "```py\nnc -lk 9999 \n```", "```py\nlocalhost 9999\n```", "```py\nhello spark\n a stream\n hands on spark\n```", "```py\n-------------------------------------------\n Batch: 0\n -------------------------------------------\n +------+-----+\n | value|count|\n +------+-----+\n | hello|    1|\n | spark|    1|\n +------+-----+\n\n -------------------------------------------\n Batch: 1\n -------------------------------------------\n +------+-----+\n | value|count|\n +------+-----+\n | hello|    1|\n | spark|    1|\n |     a|    1|\n |stream|    1|\n +------+-----+\n\n -------------------------------------------\n Batch: 2\n -------------------------------------------\n +------+-----+\n | value|count|\n +------+-----+\n | hello|    1|\n | spark|    2|\n |     a|    1|\n |stream|    1|\n | hands|    1|\n |    on|    1|\n +------+-----+\n```", "```py\ngroupId = org.apache.spark\n artifactId = spark-core_2.11\n version = 2.2.1\n\n groupId = org.apache.spark\n artifactId = spark-streaming_2.11\n version = 2.2.1\n\n groupId = org.apache.spark\n artifactId = spark-streaming-kafka-0-10_2.11\n version = 2.2.1\n```", "```py\nval Array(brokers, topics) = args\n```", "```py\nval sparkConf = new SparkConf().setAppName(\"DirectKafkaWordCount\").setMaster(master)\n val ssc = new StreamingContext(sparkConf, Seconds(5))\n```", "```py\nval topicsSet = topics.split(\",\").toSet\n val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n       ssc, kafkaParams, topicsSet)\n```", "```py\nval lines = messages.map(_._2)\n val words = lines.flatMap(_.split(\" \"))\n val wordCounts = words.map(x => (x, 1L)).reduceByKey(_ + _)\n wordCounts.print()\n```", "```py\nssc.start()\n ssc.awaitTermination()\n```", "```py\n$KAFKA_HOME/bin/zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties\n```", "```py\n$KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties\n```", "```py\n$KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic packttopic\n```", "```py\n$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper localhost:2181\n```", "```py\n$KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic packttopic\n```", "```py\nFirst message\n Second message\n Third message\n Yet another message for the message consumer\n```", "```py\n-------------------------------------------\n Time: 1527457655000 ms\n -------------------------------------------\n (consumer,1)\n (Yet,1)\n (another,1)\n (message,2)\n (for,1)\n (the,1)\n```", "```py\nval inputDataSchema = new Schema.Builder()\n     .addColumnsDouble(\"Sepal length\", \"Sepal width\", \"Petal length\", \"Petal width\")\n     .addColumnInteger(\"Species\")\n     .build\n```", "```py\nval tp = new TransformProcess.Builder(inputDataSchema)\n     .removeColumns(\"Petal length\", \"Petal width\")\n     .build\n```", "```py\nval outputSchema = tp.getFinalSchema\n```", "```py\nval sparkConf = new SparkConf().setAppName(\"DirectKafkaDataVec\").setMaster(master)\n val ssc = new StreamingContext(sparkConf, Seconds(5))\n\n val topicsSet = topics.split(\",\").toSet\n val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n     ssc, kafkaParams, topicsSet)\n```", "```py\nval lines = messages.map(_._2)\n```", "```py\nlines.foreachRDD { rdd =>\n   val javaRdd = rdd.toJavaRDD()\n   val rr = new CSVRecordReader\n   val parsedInputData = javaRdd.map(new StringToWritablesFunction(rr))\n\n   if(!parsedInputData.isEmpty()) {\n     val processedData = SparkTransformExecutor.execute(parsedInputData, tp)\n\n     val processedAsString = processedData.map(new WritablesToStringFunction(\",\"))\n     val processedCollected = processedAsString.collect\n     val inputDataCollected = javaRdd.collect\n\n     println(\"\\n\\n---- Original Data ----\")\n     for (s <- inputDataCollected.asScala) println(s)\n\n     println(\"\\n\\n---- Processed Data ----\")\n     for (s <- processedCollected.asScala) println(s)\n   }\n }\n```", "```py\nssc.start()\n ssc.awaitTermination()\n```", "```py\n$KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic csvtopic\n```", "```py\n5.1,3.5,1.4,0.2,0\n 4.9,3.0,1.4,0.2,0\n 4.7,3.2,1.3,0.2,0\n 4.6,3.1,1.5,0.2,0\n```", "```py\n4.6,3.1,1.5,0.2,0\n\n ---- Processed Data ----\n 4.6,3.1,0\n```"]