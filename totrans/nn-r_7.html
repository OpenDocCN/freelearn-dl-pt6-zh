<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Use Cases of Neural Networks – Advanced Topics</h1>
                
            
            <article>
                
<p class="calibre2">With <strong class="calibre1">Artificial Neural Networks</strong> (<strong class="calibre1">ANN</strong>), let's try to simulate typical brain activities such as image perception, pattern recognition, language understanding, sense-motor coordination, and so on. ANN models are composed of a system of nodes, equivalent to neurons of a human brain, which are interconnected by weighted links, equivalent to synapses between neurons. The output of the network is modified iteratively from link weights to convergence.</p>
<p class="calibre2">This final chapter presents ANN applications from different use cases and how neural networks can be used in the AI world. We will see some use cases and their implementation in R. You can adapt the same set of programs for other real work scenarios.</p>
<p class="calibre2">The following topics will be covered:</p>
<ul class="calibre16">
<li class="calibre17">TensorFlow integration with R</li>
<li class="calibre17">Keras integration with R</li>
<li class="calibre17">Handwritten digit recognition using <kbd class="calibre13">MNIST</kbd> dataset with <kbd class="calibre13">H2O</kbd></li>
<li class="calibre17">Building LSTM with <span><span>mxnet</span></span></li>
<li class="calibre17">Clustering data using auto encoders with <kbd class="calibre13">H2O</kbd></li>
<li class="calibre17"><strong class="calibre1">Principal Component Analysis</strong> (<strong class="calibre1">PCA</strong>) using <kbd class="calibre13">H2O</kbd></li>
<li class="calibre17">Breast cancer detection using the <kbd class="calibre13">darch</kbd> package</li>
</ul>
<p class="calibre2"><span>By</span> <span>the end of this chapter, you will have understood the advanced concepts of the learning process and their implementation in the R environment. We will apply different types of algorithms to implement a neural network. We will review how to train, test, and deploy a model. We will look again at how to perform a correct valuation procedure.</span> We will also cover more of deep learning in our use cases as deep learning is the latest thing that is based on advanced neural networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">TensorFlow integration with R</h1>
                
            
            <article>
                
<p class="calibre2">TensorFlow is an open source numerical computing library provided by Google for machine intelligence. It hides all of the programming required to build deep learning models and gives the developers a black box interface to program. The Keras API for TensorFlow provides a high-level interface for neural networks.</p>
<p class="calibre2">Python is the <strong class="calibre1">de facto</strong> programming language for deep learning, but R is catching up. Deep learning libraries are now available with R and a developer can easily download TensorFlow or Keras similar to other R libraries and use them.</p>
<p class="calibre2">In TensorFlow, nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. TensorFlow was originally developed by the Google Brain Team within Google's machine intelligence research for machine learning and deep neural networks research, but it is now available in the public domain. TensorFlow exploits GPU processing when configured appropriately.</p>
<p class="calibre2">The generic use cases for TensorFlow are as follows:</p>
<ul class="calibre16">
<li class="calibre17">Image recognition</li>
<li class="calibre17">Computer vision</li>
<li class="calibre17">Voice/sound recognition</li>
<li class="calibre17">Time series analysis</li>
<li class="calibre17">Language detection</li>
<li class="calibre17">Language translation</li>
<li class="calibre17">Text-based processing</li>
<li class="calibre17"><strong class="calibre1">Handwriting Recognition</strong> (<strong class="calibre1">HWR</strong>)</li>
<li class="calibre17">Many others</li>
</ul>
<p class="calibre2">In this section, we will see how we can bring TensorFlow libraries into R. This will open up a huge number of possibilities with deep learning using TensorFlow in R. In order to use TensorFlow, we must first install Python. If you don't have a Python installation on your machine, it's time to get it.</p>
<p class="calibre2">Python is a dynamic <strong class="calibre1">Object-Oriented Programming</strong> (<strong class="calibre1">OOP</strong>) language that can be used for many types of software development. It offers strong support for integration with other languages and programs, is provided with a large standard library, and can be learned within a few days. Many Python programmers can confirm a substantial increase in productivity and feel that it encourages the development of higher quality code and maintainability. Python runs on Windows, Linux/Unix, <span>macOS X</span>, OS/2, Amiga, Palm Handhelds, and Nokia phones. It also works on Java and .NET virtual machines. Python is licensed under the OSI-approved open source license; its use is free, including for commercial products.</p>
<p class="calibre2"><span>Python was created in the early 1990s by Guido van Rossum at Stichting Mathematisch Centrum in the Netherlands as a successor of a language called <strong class="calibre1">ABC</strong>. Guido remains Python's principal author, although it includes many contributions from others.</span></p>
<div class="packt_infobox">If you do not know which version to use, there is an (English) document that could help you choose. In principle, if you have to start from scratch, we recommend choosing Python 3, and if you need to use third-party software packages that may not be compatible with Python 3, we recommend using Python 2.7. All information about the available versions and how to install Python is given at <a href="https://www.python.org/" class="calibre68">https://www.python.org/</a>.</div>
<p class="calibre2">After properly installing the Python version of our machine, we have to worry about installing TensorFlow. We can retrieve all library information and available versions of the operating system from the following link: <a href="https://www.tensorflow.org/" class="calibre4">https://www.tensorflow.org/</a>.</p>
<p class="calibre2">Also, in the install section, we can find a series of guides that explain how to install a version of TensorFlow that allows us to write applications in Python. Guides are available for the following operating systems:</p>
<ul class="calibre16">
<li class="calibre17">Installing TensorFlow on Ubuntu</li>
<li class="calibre17">Installing TensorFlow on macOS X</li>
<li class="calibre17">Installing TensorFlow on Windows</li>
<li class="calibre17">Installing TensorFlow from sources</li>
</ul>
<p class="calibre2">For example, to install Tensorflow on Windows, we must choose one of the following types:</p>
<ul class="calibre16">
<li class="calibre17">TensorFlow with CPU support only</li>
<li class="calibre17">TensorFlow with GPU support</li>
</ul>
<p class="calibre2">To install TensorFlow, start a terminal with privileges as administrator. Then issue the appropriate <kbd class="calibre13">pip3</kbd> install command in that terminal. To install the CPU-only version, enter the following command:</p>
<pre class="calibre24"><strong class="calibre1">C:\&gt; pip3 install --upgrade tensorflow</strong></pre>
<p class="calibre2">A series of code lines will be displayed on the video to keep us informed of the execution of the installation procedure, as shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00137.gif"/></div>
<p class="calibre2">At this point, we can return to our favorite environment; I am referring to the R development environment. We will need to install the interface to TensorFlow. <span>The R interface to TensorFlow lets you work productively using the high-level Keras and Estimator APIs, and when you need more control, it provides full access to the core TensorFlow API. To install the R interface to TensorFlow, we will use the following procedure.</span></p>
<p class="calibre2">First, install the <kbd class="calibre13">tensorflow</kbd> R package from CRAN as follows:</p>
<pre class="calibre24"><strong class="calibre1">install.packages("tensorflow")</strong></pre>
<p class="calibre2">Then, use the <strong class="calibre1">install_tensorflow()</strong> function to install TensorFlow (for a proper installation procedure, you must have administrator privileges):</p>
<pre class="calibre24"><strong class="calibre1">library(tensorflow)</strong><br class="title-page-name"/><strong class="calibre1">install_tensorflow()</strong></pre>
<p class="calibre2">We can confirm that the installation succeeded:</p>
<pre class="calibre24"><strong class="calibre1">sess = tf$Session()</strong><br class="title-page-name"/><strong class="calibre1">hello &lt;- tf$constant('Hello, TensorFlow!')</strong><br class="title-page-name"/><strong class="calibre1">sess$run(hello)</strong></pre>
<p class="calibre2">This will provide you with a default installation of TensorFlow suitable for use with the <kbd class="calibre13">tensorflow</kbd> R package. Read on if you want to learn about additional installation options, including installing a version of TensorFlow that takes advantage of NVIDIA GPUs if you have the correct CUDA libraries installed. In the following code, we can check the success of the installation:</p>
<pre class="calibre24"><strong class="calibre1">&gt; library(tensorflow)</strong><br class="title-page-name"/><strong class="calibre1">&gt; sess = tf$Session()</strong><br class="title-page-name"/><strong class="calibre1">&gt; hello &lt;- tf$constant('Hello, TensorFlow!')</strong><br class="title-page-name"/><strong class="calibre1">&gt; sess$run(hello)</strong><br class="title-page-name"/><strong class="calibre1">b'Hello, TensorFlow!'</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Keras integration with R</h1>
                
            
            <article>
                
<p class="calibre2">Keras is a set of open source neural network libraries coded in Python. It is capable of running on top of MxNet, TensorFlow, or Theano. The steps to install Keras in RStudio are very simple. The following code snippet gives the steps for installation and we can check whether Keras is working by checking the load of the <kbd class="calibre13">MNIST</kbd> dataset.</p>
<p class="calibre2">By default, RStudio loads the CPU version of TensorFlow. Once Keras is loaded, we have a powerful set of deep learning libraries that can be utilized by R programmers to execute neural networks and deep learning. To install Keras for R, use this code:</p>
<pre class="calibre24"><strong class="calibre1">install.packages("devtools")</strong><br class="title-page-name"/><strong class="calibre1">devtools::install_github("rstudio/keras")</strong></pre>
<p class="calibre2">At this point, we load the <kbd class="calibre13">keras</kbd> library:</p>
<pre class="calibre24"><strong class="calibre1">library(keras)</strong></pre>
<p class="calibre2">Finally, we check whether keras is installed correctly by loading the <kbd class="calibre13">MNIST</kbd> dataset:</p>
<pre class="calibre24"><strong class="calibre1">&gt; data=dataset_mnist()</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">MNIST HWR using R</h1>
                
            
            <article>
                
<p class="calibre2">Handwriting Recognition (HWR) is a very commonly used procedure in modern technology. The image of the written text can be detected offline from a piece of paper by optical scanning (<strong class="calibre1">optical character recognition</strong> (<strong class="calibre1">OCR</strong>)) or intelligent word recognition. Alternatively, pen tip movements can be detected online (for example, from a pen-computer surface, a task that is generally easier since there are more clues available). Technically, recognition of handwriting is the ability of a computer to receive and interpret a handwritten intelligible input from sources such as paper documents, photos, touchscreens, and other devices.</p>
<p class="calibre2">HWR is performed through various techniques that generally require OCR. However, a complete script recognition system also manages formatting, carries out correct character segmentation, and finds the most plausible words.</p>
<p class="calibre2"><strong class="calibre1">Modified National Institute of Standards and Technology</strong> (<strong class="calibre1">MNIST</strong>) is a large database of handwritten digits. It has a set of 70,000 examples of data. It is a subset of NIST's larger dataset. The digits are of 28x28 pixel resolution and are stored in a matrix of 70,000 rows and 785 columns; 784 columns form each pixel value from the 28x28 matrix and one value is the actual digit. <span>The digits have been size-normalized and centered in a fixed-size image.</span></p>
<div class="packt_infobox"><span class="calibre62">The digit images in the MNIST set were originally selected and experimented with by Chris Burges and Corinna Cortes using bounding-box normalization and centering. Yann LeCun's version uses centering by center of mass within in a larger window.</span> The data is available on Yann LeCun's website at <a href="http://yann.lecun.com/exdb/mnist/" class="calibre68">http://yann.lecun.com/exdb/mnist/</a>.</div>
<p class="calibre2">Each image is created as <span>28x28</span>. Here is a sample of images of <em class="calibre14">0-8</em> from the MNIST dataset:</p>
<div class="cdpaligncenter"><img class="image-border33" src="../images/00138.jpeg"/></div>
<p class="calibre2">MNIST has a sample of several handwritten digits. This dataset can be fed for our training to an R program and our code can recognize any new handwritten digit that is presented as data for prediction. This is a case where the neural network architecture functions as a computer vision system for an AI application.</p>
<p class="calibre2">The following table shows the distribution of the <kbd class="calibre13">MNIST</kbd> dataset available on LeCun's website:</p>
<table class="calibre84">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Digit</strong></td>
<td class="calibre8"><strong class="calibre1">Count</strong></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">0</em></td>
<td class="calibre8"><em class="calibre14">5923</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">1</em></td>
<td class="calibre8"><em class="calibre14">6742</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">2</em></td>
<td class="calibre8"><em class="calibre14">5958</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">3</em></td>
<td class="calibre8"><em class="calibre14">6131</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">4</em></td>
<td class="calibre8"><em class="calibre14">5842</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">5</em></td>
<td class="calibre8"><em class="calibre14">5421</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">6</em></td>
<td class="calibre8"><em class="calibre14">5918</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">7</em></td>
<td class="calibre8"><em class="calibre14">6265</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">8</em></td>
<td class="calibre8"><em class="calibre14">5851</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">9</em></td>
<td class="calibre8"><em class="calibre14">5949</em></td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">We will not use the <kbd class="calibre13">h2o</kbd> package for deep learning to train and test the <kbd class="calibre13">MNIST</kbd> dataset. We will split the dataset of 70,000 rows into 60,000 training rows and 10,000 test rows. Then, we'll find the accuracy of the model. The model can then be used to predict any incoming dataset of 28x28 pixel handwritten digits containing numbers between zero and nine. Finally, we will reduce the file size to 100 rows for demo training processing on two datasets in <kbd class="calibre13">.csv</kbd> format, named <kbd class="calibre13">mnist_train_100.csv</kbd> and <kbd class="calibre13">mnist_test_10.csv</kbd>.</p>
<p class="calibre2">For our sample R code, we use a 100-row training dataset and a 10-row test dataset. The R code is presented here:</p>
<pre class="calibre24"><strong class="calibre1">#################################################################</strong><br class="title-page-name"/><strong class="calibre1">### Chapter 7 - Neural Networks with R - Use cases      #########</strong><br class="title-page-name"/><strong class="calibre1">### Handwritten digit recognition through MNIST dataset #########</strong><br class="title-page-name"/><strong class="calibre1">#################################################################</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">library("h2o")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">h2o.init(nthreads=-1,max_mem_size="3G")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">setwd ("c://R")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">train_mnist=read.csv("mnist_train_100.csv", header=FALSE)</strong><br class="title-page-name"/><strong class="calibre1">attach(train_mnist)</strong><br class="title-page-name"/><strong class="calibre1">names(train_mnist)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">test_mnist=read.csv("mnist_test_10.csv", header=FALSE)</strong><br class="title-page-name"/><strong class="calibre1">attach(test_mnist)</strong><br class="title-page-name"/><strong class="calibre1">names(test_mnist)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">m = matrix(unlist(train_mnist[10,-1]), </strong><br class="title-page-name"/><strong class="calibre1">           nrow = 28, </strong><br class="title-page-name"/><strong class="calibre1">           byrow = TRUE)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">image(m,col=grey.colors(255))</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">rotate = function(x) t(apply(x, 2, rev)) </strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">image(rotate(m),col=grey.colors(255))</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">par(mfrow=c(2,3))</strong><br class="title-page-name"/><strong class="calibre1">lapply(1:6, </strong><br class="title-page-name"/><strong class="calibre1">       function(x) image(</strong><br class="title-page-name"/><strong class="calibre1">         rotate(matrix(unlist(train_mnist[x,-1]),</strong><br class="title-page-name"/><strong class="calibre1">                       nrow = 28, </strong><br class="title-page-name"/><strong class="calibre1">                       byrow = TRUE)),</strong><br class="title-page-name"/><strong class="calibre1">         col=grey.colors(255),</strong><br class="title-page-name"/><strong class="calibre1">         xlab=train_mnist[x,1]</strong><br class="title-page-name"/><strong class="calibre1">       )</strong><br class="title-page-name"/><strong class="calibre1">)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">par(mfrow=c(1,1))</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">str(train_mnist)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">x=2:785</strong><br class="title-page-name"/><strong class="calibre1">y=1</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">table(train_mnist[,y])</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">model=h2o.deeplearning(x,</strong><br class="title-page-name"/><strong class="calibre1">                       y,</strong><br class="title-page-name"/><strong class="calibre1">                       as.h2o(train_mnist),</strong><br class="title-page-name"/><strong class="calibre1">                       model_id="MNIST_deeplearning",</strong><br class="title-page-name"/><strong class="calibre1">                       seed=405,</strong><br class="title-page-name"/><strong class="calibre1">                       activation="RectifierWithDropout",</strong><br class="title-page-name"/><strong class="calibre1">                       l1=0.00001,</strong><br class="title-page-name"/><strong class="calibre1">                       input_dropout_ratio=0.2,</strong><br class="title-page-name"/><strong class="calibre1">                       classification_stop = -1,</strong><br class="title-page-name"/><strong class="calibre1">                       epochs=2000</strong><br class="title-page-name"/><strong class="calibre1">                       )</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">summary(model)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">h2o.scoreHistory(model)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">preds=h2o.performance(model, </strong><br class="title-page-name"/><strong class="calibre1">                      as.h2o(test_mnist))</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">newdata = h2o.predict(model, </strong><br class="title-page-name"/><strong class="calibre1">                   as.h2o(test_mnist))</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">predictions = cbind(as.data.frame(seq(1,10)),</strong><br class="title-page-name"/><strong class="calibre1">                    test_mnist[,1],</strong><br class="title-page-name"/><strong class="calibre1">                    as.data.frame(newdata[,1]))</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">names(predictions) = c("Number","Actual","Predicted")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">as.matrix(predictions)</strong><br class="title-page-name"/><strong class="calibre1">#################################################################</strong></pre>
<p class="calibre2"><span>Now, let's go through the code to learn how to apply the</span> <kbd class="calibre13">h2o</kbd> <span>package to solve a HWR problem. We've already properly introduced the <kbd class="calibre13">h2o</kbd> package in <a href="part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 3</a>, <em class="calibre14">Deep Learning Using Multilayer Neural Networks</em>. The <kbd class="calibre13">h2o</kbd></span> is included and initiated through the following code:</p>
<pre class="calibre24"><strong class="calibre1">library("h2o")</strong><br class="title-page-name"/><strong class="calibre1">h2o.init(nthreads=-1,max_mem_size="3G")</strong></pre>
<p class="calibre2">The following results are displayed in the R prompt:</p>
<pre class="calibre24"><strong class="calibre1">&gt; h2o.init(nthreads=-1,max_mem_size="3G")</strong><br class="title-page-name"/><strong class="calibre1">H2O is not running yet, starting it now...</strong><br class="title-page-name"/><strong class="calibre1">Note: In case of errors look at the following log files:</strong><br class="title-page-name"/><strong class="calibre1"> C:\Users\lavoro\AppData\Local\Temp\Rtmpiit6zE/h2o_lavoro_started_from_r.out</strong><br class="title-page-name"/><strong class="calibre1"> C:\Users\lavoro\AppData\Local\Temp\Rtmpiit6zE/h2o_lavoro_started_from_r.err</strong><br class="title-page-name"/><strong class="calibre1">java version "1.7.0_40"</strong><br class="title-page-name"/><strong class="calibre1">Java(TM) SE Runtime Environment (build 1.7.0_40-b43)</strong><br class="title-page-name"/><strong class="calibre1">Java HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)</strong><br class="title-page-name"/><strong class="calibre1">Starting H2O JVM and connecting: ..... Connection successful!</strong><br class="title-page-name"/><strong class="calibre1">R is connected to the H2O cluster: </strong><br class="title-page-name"/><strong class="calibre1"> H2O cluster uptime: 15 seconds 229 milliseconds </strong><br class="title-page-name"/><strong class="calibre1"> H2O cluster version: 3.10.5.3 </strong><br class="title-page-name"/><strong class="calibre1"> H2O cluster version age: 2 months and 18 days </strong><br class="title-page-name"/><strong class="calibre1"> H2O cluster name: H2O_started_from_R_lavoro_huu267 </strong><br class="title-page-name"/><strong class="calibre1"> H2O cluster total nodes: 1 </strong><br class="title-page-name"/><strong class="calibre1"> H2O cluster total memory: 2.67 GB </strong><br class="title-page-name"/><strong class="calibre1"> H2O cluster total cores: 4 </strong><br class="title-page-name"/><strong class="calibre1"> H2O cluster allowed cores: 4 </strong><br class="title-page-name"/><strong class="calibre1"> H2O cluster healthy: TRUE </strong><br class="title-page-name"/><strong class="calibre1"> H2O Connection ip: localhost </strong><br class="title-page-name"/><strong class="calibre1"> H2O Connection port: 54321 </strong><br class="title-page-name"/><strong class="calibre1"> H2O Connection proxy: NA </strong><br class="title-page-name"/><strong class="calibre1"> H2O Internal Security: FALSE </strong><br class="title-page-name"/><strong class="calibre1"> R Version: R version 3.4.1 (2017-06-30)</strong></pre>
<p class="calibre2">The training file is opened using a handle. It is set to have 100 rows to simplify the demo work. The complete dataset can be downloaded from the URL suggested before.</p>
<pre class="calibre24"><strong class="calibre1">setwd("C://R")</strong></pre>
<p class="calibre2"><span>This command sets the working directory where we will have inserted the dataset for the next reading.</span></p>
<pre class="calibre24"><strong class="calibre1">train_mnist=read.csv("mnist_train_100.csv", header=FALSE)</strong><br class="title-page-name"/><strong class="calibre1">attach(train_mnist)</strong><br class="title-page-name"/><strong class="calibre1">names(train_mnist)</strong></pre>
<p class="calibre2">This piece of code first loads the training dataset of <kbd class="calibre13">MNIST</kbd>, reducing the file size to 100 rows for demo training processing. Then we use the <kbd class="calibre13">attach()</kbd> function to attach the database to the R search path. This means that the database is searched by R when evaluating a variable, so objects in the database can be accessed by simply giving their names. Finally, we use the <kbd class="calibre13">names()</kbd> function to set the names of the dataset. The same thing we will do for the dataset to be used in the testing phase:</p>
<pre class="calibre24"><strong class="calibre1">test_mnist=read.csv("mnist_test_10.csv", header=FALSE)</strong><br class="title-page-name"/><strong class="calibre1">attach(test_mnist)</strong><br class="title-page-name"/><strong class="calibre1">names(test_mnist)</strong></pre>
<p class="calibre2">At this point, we create a 28x28 matrix with pixel color values by taking the tenth row of the dataset, which contains the number zero:</p>
<pre class="calibre24"><strong class="calibre1">m = matrix(unlist(train_mnist[10,-1]),</strong><br class="title-page-name"/><strong class="calibre1">            + nrow = 28,</strong><br class="title-page-name"/><strong class="calibre1">            + byrow = TRUE)</strong></pre>
<p class="calibre2">Let's see what we've got by plotting an object <kbd class="calibre13">image</kbd>:</p>
<pre class="calibre24"><strong class="calibre1">image(m,col=grey.colors(255))</strong></pre>
<p class="calibre2">In the following is shown the image of the <span>handwritten digit:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00139.jpeg"/></div>
<p class="calibre2">Now let's create a mirror image of the handwritten digit:</p>
<pre class="calibre24"><strong class="calibre1">&gt; rotate = function(x) t(apply(x, 2, rev))</strong></pre>
<p class="calibre2">Then, view the image to verify the operation just made:</p>
<pre class="calibre24"><strong class="calibre1">&gt; image(rotate(m),col=grey.colors(255))</strong></pre>
<p class="calibre2"><span>In the following is shown the mirror image</span><span>:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00140.jpeg"/></div>
<p class="calibre2">Now, let's do the same for the first six rows in the dataset:</p>
<pre class="calibre24"><strong class="calibre1">par(mfrow=c(2,3))</strong><br class="title-page-name"/><strong class="calibre1">lapply(1:6,</strong><br class="title-page-name"/><strong class="calibre1">       function(x) image(</strong><br class="title-page-name"/><strong class="calibre1">         rotate(matrix(unlist(train_mnist[x,-1]),</strong><br class="title-page-name"/><strong class="calibre1">                       nrow = 28,</strong><br class="title-page-name"/><strong class="calibre1">                       byrow = TRUE)),</strong><br class="title-page-name"/><strong class="calibre1">         col=grey.colors(255),</strong><br class="title-page-name"/><strong class="calibre1">         xlab=train_mnist[x,1]</strong><br class="title-page-name"/><strong class="calibre1">       )</strong><br class="title-page-name"/><strong class="calibre1">)</strong></pre>
<p class="calibre2"><span>These are the images of the</span> <span>handwritten digits contained in the first six rows of the dataset:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00141.jpeg"/></div>
<p class="mce-root">Reset the plot options back to default:</p>
<pre class="calibre24"><strong class="calibre1"> par(mfrow=c(1,1))</strong> </pre>
<p class="calibre2">The next command lets us do some explanatory analysis of the training data:</p>
<pre class="calibre24"><strong class="calibre1">str(train_mnist)</strong><br class="title-page-name"/><strong class="calibre1">x=2:785</strong><br class="title-page-name"/><strong class="calibre1">y=1</strong> </pre>
<p class="calibre2">This command finds the count of each number in the training matrix:</p>
<pre class="calibre24"><strong class="calibre1">table(train_mnist[,y])</strong> </pre>
<p class="calibre2">The results are shown here:</p>
<pre class="calibre24"><strong class="calibre1">&gt; table(train_mnist[,y])</strong><br class="title-page-name"/><strong class="calibre1"> 0  1  2  3  4  5  6  7  8  9</strong><br class="title-page-name"/><strong class="calibre1">13 14  6 11 11  5 11 10  8 11</strong></pre>
<p class="calibre2">Above are displayed the number of occurrences of each digit in the dataset. It's time to build and train the model:</p>
<pre class="calibre24"><strong class="calibre1">model=h2o.deeplearning(x,</strong><br class="title-page-name"/><strong class="calibre1">                       y,</strong><br class="title-page-name"/><strong class="calibre1">                       as.h2o(train_mnist),</strong><br class="title-page-name"/><strong class="calibre1">                       model_id="MNIST_deeplearning",</strong><br class="title-page-name"/><strong class="calibre1">                       seed=405,</strong><br class="title-page-name"/><strong class="calibre1">                       activation="RectifierWithDropout",</strong><br class="title-page-name"/><strong class="calibre1">                       l1=0.00001,</strong><br class="title-page-name"/><strong class="calibre1">                       input_dropout_ratio=0.2,</strong><br class="title-page-name"/><strong class="calibre1">                       classification_stop = -1,</strong><br class="title-page-name"/><strong class="calibre1">                       epochs=2000</strong><br class="title-page-name"/><strong class="calibre1">)</strong></pre>
<p class="calibre2">Now, to produce the summaries of the results of the <kbd class="calibre13">model</kbd> fitting function, we will use the <kbd class="calibre13">summary()</kbd> function:</p>
<pre class="calibre24"><strong class="calibre1">summary(model)</strong></pre>
<p class="calibre2">The following figure shows some of the results obtained:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00142.jpeg"/></div>
<p class="calibre2">We can understand the evolution of the algorithm used, by checking the performance of the training model:</p>
<pre class="calibre24"><strong class="calibre1">preds=h2o.performance(model,</strong><br class="title-page-name"/><strong class="calibre1">                      as.h2o(test_mnist))</strong></pre>
<p class="calibre2">At this point, we have a properly trained <kbd class="calibre13">model</kbd>, so we can use it to make predictions. In our case, we will use it to recognize handwritten digits:</p>
<pre class="calibre24"><strong class="calibre1">newdata = h2o.predict(model,</strong><br class="title-page-name"/><strong class="calibre1">                      as.h2o(test_mnist))</strong></pre>
<p class="calibre2">Now that we have used <kbd class="calibre13">model</kbd>, we need to format the actual and expected matrices to verify the accuracy:</p>
<pre class="calibre24"><strong class="calibre1">predictions = cbind(as.data.frame(seq(1,10)),</strong><br class="title-page-name"/><strong class="calibre1">                    test_mnist[,1],</strong><br class="title-page-name"/><strong class="calibre1">                    as.data.frame(newdata[,1]))</strong></pre>
<p class="calibre2">Enter the names of the variables inserted into the matrix:</p>
<pre class="calibre24"><strong class="calibre1">names(predictions) = c("Number","Actual","Predicted")</strong></pre>
<p class="calibre2">Finally, check the output:</p>
<pre class="calibre24"><strong class="calibre1">as.matrix(predictions)</strong></pre>
<p class="calibre2">The results are shown here:</p>
<pre class="calibre24"><strong class="calibre1">&gt; as.matrix(predictions)</strong><br class="title-page-name"/><strong class="calibre1">      Number Actual   Predicted</strong><br class="title-page-name"/><strong class="calibre1"> [1,]      1      7  6.90180840</strong><br class="title-page-name"/><strong class="calibre1"> [2,]      2      3  3.62368445</strong><br class="title-page-name"/><strong class="calibre1"> [3,]      3      1  0.53782891</strong><br class="title-page-name"/><strong class="calibre1"> [4,]      4      0 -0.03092147</strong><br class="title-page-name"/><strong class="calibre1"> [5,]      5      6  5.21024129</strong><br class="title-page-name"/><strong class="calibre1"> [6,]      6      1  0.30850593</strong><br class="title-page-name"/><strong class="calibre1"> [7,]      7      6  6.44916207</strong><br class="title-page-name"/><strong class="calibre1"> [8,]      8      9  3.59962551</strong><br class="title-page-name"/><strong class="calibre1"> [9,]      9      5  3.17590073</strong><br class="title-page-name"/><strong class="calibre1">[10,]     10      9  7.35213625</strong></pre>
<p class="calibre2">As can be seen from the analysis of the table just proposed, for the test data, the model has predicted 60 percent (six out of ten) correctly. This accuracy is only for the small training dataset. The model can be improved further by:</p>
<ul class="calibre16">
<li class="calibre17">Increasing the training dataset count</li>
<li class="calibre17">Tweaking the parameters of the <kbd class="calibre13">h20.deeplearning</kbd> function</li>
<li class="calibre17">Allocating more memory to the <kbd class="calibre13">h2o</kbd> JVM</li>
<li class="calibre17">Expanding the test dataset</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">LSTM using the iris dataset</h1>
                
            
            <article>
                
<p class="calibre2">Continuing with the LSTM architecture for RNN introduced in <a href="part0111.html#39REE0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 6</a>, <em class="calibre14">Recurrent and Convolutional Neural Networks</em>, we present the <kbd class="calibre13">iris</kbd> dataset processing using the <kbd class="calibre13">mxnet</kbd> LSTM function. The function expects all inputs and outputs as numeric. It is particularly useful for processing text sequences, but here we will train an LSTM model on the <kbd class="calibre13">iris</kbd> dataset. The input values are <kbd class="calibre13">petal.length</kbd>, <kbd class="calibre13">petal.width</kbd>, <kbd class="calibre13">sepal.length</kbd>, and <kbd class="calibre13">sepal.width</kbd>. The output variable is <kbd class="calibre13">Species</kbd>, which is converted to a numeric value between one and three. The <kbd class="calibre13">iris</kbd> dataset has been detailed in <a href="part0088.html#2JTHG0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 4</a>, <em class="calibre14">Perceptron Neural Network Modeling – Basic Models</em>:</p>
<pre class="calibre24"><strong class="calibre1">#################################################################</strong><br class="title-page-name"/><strong class="calibre1">### Chapter 7 - Neural Networks with R - Use cases      #########</strong><br class="title-page-name"/><strong class="calibre1">### Prediction using LSTM on IRIS dataset               #########</strong><br class="title-page-name"/><strong class="calibre1">#################################################################</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">##Required one time</strong><br class="title-page-name"/><strong class="calibre1">library("mxnet")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">data(iris)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">x = iris[1:5!=5,-5]</strong><br class="title-page-name"/><strong class="calibre1">y = as.integer(iris$Species)[1:5!=5]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">train.x = data.matrix(x)</strong><br class="title-page-name"/><strong class="calibre1">train.y = y</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">test.x = data.matrix(iris[1:5==5,-5])</strong><br class="title-page-name"/><strong class="calibre1">test.y = as.integer(iris$Species)[1:5==5]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">model &lt;- mx.mlp(train.x, train.y, hidden_node=10, out_node=3, out_activation="softmax",</strong><br class="title-page-name"/><strong class="calibre1">                num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,</strong><br class="title-page-name"/><strong class="calibre1">                eval.metric=mx.metric.accuracy)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">preds = predict(model, test.x)</strong><br class="title-page-name"/><strong class="calibre1">pred.label = max.col(t(preds))</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">test.y</strong><br class="title-page-name"/><strong class="calibre1">pred.label</strong><br class="title-page-name"/><strong class="calibre1">#################################################################</strong></pre>
<p class="calibre2">The program requires <kbd class="calibre13">mxnet</kbd>, which needs to be installed. <kbd class="calibre13">mxnet</kbd> for R is available for both CPUs and GPUs and for the following OSes: Linux, macOS, and Windows.</p>
<div class="packt_infobox">We will only indicate the installation procedures for Windows machines and CPU versions. Refer to the following URL for information on installation procedures for other architectures: <a href="https://mxnet.incubator.apache.org/get_started/install.html" class="calibre68">https://mxnet.incubator.apache.org/get_started/install.html</a>.</div>
<p class="calibre2">To install <kbd class="calibre13">mxnet</kbd> on a computer with a CPU processor, we use the prebuilt binary package. We can install the package directly on the R console through the following code:</p>
<pre class="calibre24"><strong class="calibre1">cran &lt;- getOption("repos")</strong><br class="title-page-name"/><strong class="calibre1">cran["dmlc"] &lt;- "https://s3-us-west-2.amazonaws.com/apache-mxnet/R/CRAN/"</strong><br class="title-page-name"/><strong class="calibre1">options(repos = cran)</strong><br class="title-page-name"/><strong class="calibre1">install.packages("mxnet")</strong></pre>
<p class="calibre2">The following packages are installed:</p>
<pre class="calibre24"><strong class="calibre1">package ‘bindr’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘brew’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘assertthat’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘bindrcpp’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘glue’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘pkgconfig’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘BH’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘plogr’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘yaml’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘irlba’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘hms’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘XML’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘Rook’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘tidyselect’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘gridExtra’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘dplyr’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘downloader’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘htmltools’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘htmlwidgets’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘igraph’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘influenceR’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘purrr’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘readr’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘rstudioapi’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘rgexf’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘tidyr’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘viridis’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘DiagrammeR’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘visNetwork’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘data.table’ successfully unpacked and MD5 sums checked</strong><br class="title-page-name"/><strong class="calibre1">package ‘mxnet’ successfully unpacked and MD5 sums checked</strong></pre>
<p class="calibre2">As you can see the installation of the <kbd class="calibre13">mxnet</kbd> package, install in addition to several packages. So, we already have everything we need to proceed. This <kbd class="calibre13">mxnet</kbd> library contains the <kbd class="calibre13">mx.lstm</kbd> function we are going to use:</p>
<pre class="calibre24"><strong class="calibre1">library("mxnet")</strong></pre>
<p class="calibre2">In the following code, the internal dataset <kbd class="calibre13">iris</kbd> is loaded and the <kbd class="calibre13">x</kbd> and <kbd class="calibre13">y</kbd> variables are set with independent and target variables, respectively. The Species variable is converted to a number between one and three:</p>
<pre class="calibre24"><strong class="calibre1">data(iris)</strong><br class="title-page-name"/><strong class="calibre1">x = iris[1:5!=5,-5]</strong><br class="title-page-name"/><strong class="calibre1">y = as.integer(iris$Species)[1:5!=5]</strong></pre>
<p class="calibre2">Just an explanation, with the following code:</p>
<pre class="calibre24"><strong class="calibre1">x = iris[1:5!=5,-5]</strong></pre>
<p class="calibre2">We asked R to select from the <kbd class="calibre13">iris</kbd> dataset, which consists of 150 lines and five columns, only lines one to four, leaving out the fifth. This procedure will also be performed for multiples of five, so in the end, we will omit every multiple row of five from our selection. We will also omit the fifth column. At the end, we will get 120 rows and four columns.</p>
<p class="calibre2">We now set the input and output:</p>
<pre class="calibre24"><strong class="calibre1">train.x = data.matrix(x)</strong><br class="title-page-name"/><strong class="calibre1">train.y = y</strong></pre>
<p class="calibre2">Then we set the dataframe we will use for the test, by selecting only the lines we had previously omitted:</p>
<pre class="calibre24"><strong class="calibre1">test.x = data.matrix(iris[1:5==5,-5])</strong><br class="title-page-name"/><strong class="calibre1">test.y = as.integer(iris$Species)[1:5==5]</strong></pre>
<p class="calibre2">The <kbd class="calibre13">mx.lstm</kbd> function is called with the input and output values so that the model is trained with the LSTM on the RNN with the dataset:</p>
<pre class="calibre24"><strong class="calibre1">model &lt;- mx.mlp(train.x, train.y, hidden_node=10, out_node=3, out_activation="softmax",</strong><br class="title-page-name"/><strong class="calibre1">                num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,</strong><br class="title-page-name"/><strong class="calibre1">                eval.metric=mx.metric.accuracy)</strong></pre>
<p class="calibre2">Now we can make predictions:</p>
<pre class="calibre24"><strong class="calibre1">preds = predict(model, test.x)</strong><br class="title-page-name"/><strong class="calibre1">pred.label = max.col(t(preds))</strong></pre>
<p class="calibre2">Finally, we print the results to compare the model performance:</p>
<pre class="calibre24"><strong class="calibre1">test.y</strong><br class="title-page-name"/><strong class="calibre1">pred.label</strong></pre>
<p class="calibre2">Here are the results:</p>
<pre class="calibre24"><strong class="calibre1">&gt; test.y</strong><br class="title-page-name"/><strong class="calibre1"> [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3</strong><br class="title-page-name"/><strong class="calibre1">&gt; pred.label</strong><br class="title-page-name"/><strong class="calibre1"> [1] 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3</strong></pre>
<p class="calibre2">From the comparison between the test data and those obtained from the forecast it can be noticed that the best results were obtained for the versicolor species. From the results obtained, it is clear that the model needs to be improved because the forecasts it is able to perform are not at the level of those obtained in the models we obtained in the previous examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Working with autoencoders</h1>
                
            
            <article>
                
<p class="calibre2">We have seen autoencoders in the deep learning chapter for unsupervised learning. Autoencoders utilize neural networks to perform non-linear dimensionality reduction. They represent data in a better way, by finding latent features in it using universal function approximators. Autoencoders try to combine or compress input data in a different way.</p>
<p class="calibre2">A sample representation using MLP is shown here:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00143.jpeg"/></div>
<p class="calibre2"> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">PCA using H2O</h1>
                
            
            <article>
                
<p class="calibre2">One of the greatest difficulties encountered in multivariate statistical analysis is the problem of displaying a dataset with many variables. Fortunately, in datasets with many variables, some pieces of data are often closely related to each other. This is because they actually contain the same information, as they measure the same quantity that governs the behavior of the system. These are therefore redundant variables that add <span>nothing</span> to the model we want to build. We can then simplify the problem by replacing a group of variables with a new variable that encloses the information content.</p>
<p class="calibre2">PCA generates a new set of variables, among them uncorrelated, called principal components; each main component is a linear combination of the original variables. All principal components are orthogonal to each other, so there is no redundant information. The principal components as a whole constitute an orthogonal basis for the data space. The goal of PCA is to explain the maximum amount of variance with the fewest number of principal components. It is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space that retains the maximum amount of information about the variables. A principal component is therefore a combination of the original variables after a linear transformation.</p>
<p class="calibre2">In the following example, we use <kbd class="calibre13">h2o</kbd> to achieve PCA. The <kbd class="calibre13">prcomp()</kbd> function is used find the principal components of a set of input features. This is unsupervised learning:</p>
<pre class="calibre24"><strong class="calibre1">library(h2o)</strong><br class="title-page-name"/><strong class="calibre1">h2o.init()</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">ausPath = system.file("extdata", "australia.csv", package="h2o")</strong><br class="title-page-name"/><strong class="calibre1">australia.hex = h2o.uploadFile(path = ausPath)</strong><br class="title-page-name"/><strong class="calibre1">summary(australia.hex)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">pca_model=h2o.prcomp(training_frame = australia.hex, </strong><br class="title-page-name"/><strong class="calibre1">                     k = 8, </strong><br class="title-page-name"/><strong class="calibre1">                     transform = "STANDARDIZE")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">summary(pca_model)</strong><br class="title-page-name"/><strong class="calibre1">barplot(as.numeric(pca_model@model$importance[2,]),</strong><br class="title-page-name"/><strong class="calibre1">        main="Pca model", </strong><br class="title-page-name"/><strong class="calibre1">        xlab="Pca component",</strong><br class="title-page-name"/><strong class="calibre1">        ylab="Proportion of Variance")</strong></pre>
<p class="calibre2"><span>Now, let's go through the code to understand how to apply the</span> <kbd class="calibre13">h2o</kbd> <span>package to apply PCA.</span></p>
<p class="calibre2">We can proceed with loading the library:</p>
<pre class="calibre24"><strong class="calibre1">library(h2o)</strong></pre>
<p class="calibre2">This command loads the library into the R environment. <span>The following f</span><span>unction initiates the</span> <kbd class="calibre13">h2o</kbd> <span>engine with a maximum memory size of</span> <kbd class="calibre13">2</kbd> <span>GB and two parallel cores:</span></p>
<pre class="calibre24"><strong class="calibre1">h2o.init()</strong></pre>
<p class="calibre2">The following messages are returned:</p>
<pre class="calibre24"><strong class="calibre1">&gt; h2o.init()</strong><br class="title-page-name"/><strong class="calibre1"> Connection successful!</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">R is connected to the H2O cluster: </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster uptime: 5 hours 40 minutes </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster version: 3.10.5.3 </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster version age: 2 months and 18 days </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster name: H2O_started_from_R_lavoro_huu267 </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster total nodes: 1 </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster total memory: 2.63 GB </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster total cores: 4 </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster allowed cores: 4 </strong><br class="title-page-name"/><strong class="calibre1">    H2O cluster healthy: TRUE </strong><br class="title-page-name"/><strong class="calibre1">    H2O Connection ip: localhost </strong><br class="title-page-name"/><strong class="calibre1">    H2O Connection port: 54321 </strong><br class="title-page-name"/><strong class="calibre1">    H2O Connection proxy: NA </strong><br class="title-page-name"/><strong class="calibre1">    H2O Internal Security: FALSE </strong><br class="title-page-name"/><strong class="calibre1">    R Version: R version 3.4.1 (2017-06-30)</strong> </pre>
<p class="calibre2">We follow the directions on the R prompt:</p>
<pre class="calibre24"><strong class="calibre1">c1=h2o.init(max_mem_size = "2G", </strong><br class="title-page-name"/><strong class="calibre1">      nthreads = 2, </strong><br class="title-page-name"/><strong class="calibre1">      ip = "localhost", </strong><br class="title-page-name"/><strong class="calibre1">      port = 54321)</strong></pre>
<p class="calibre2">The <kbd class="calibre13">h20.init</kbd> function initiates the <kbd class="calibre13">h2o</kbd> engine with a maximum memory size of <kbd class="calibre13">2</kbd> GB and two parallel cores. <span>The following commands load the data into the R environment:</span></p>
<pre class="calibre24"><strong class="calibre1">ausPath = system.file("extdata", "australia.csv", package="h2o")</strong><br class="title-page-name"/><strong class="calibre1">australia.hex = h2o.uploadFile(path = ausPath)</strong></pre>
<p class="calibre2">The first instruction generates the path that contains the file to upload. To upload a file in a directory local to your <kbd class="calibre13">h2o</kbd> instance, use <kbd class="calibre13">h2o.uploadFile()</kbd>, which can also upload data local to your <kbd class="calibre13">h2o</kbd> instance in addition to your R session. In the parentheses, specify the <kbd class="calibre13">h2o</kbd> reference object in R and the complete URL or normalized file path for the file. Let's see now that it's inside:</p>
<pre class="calibre24"><strong class="calibre1">summary(australia.hex)</strong></pre>
<p class="calibre2">Now let's print a brief summary of the dataset:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00144.jpeg"/></div>
<p class="calibre2">To p<span>erform PCA on the given dataset, we will use the <kbd class="calibre13">prcomp()</kbd> function:</span></p>
<pre class="calibre24"><strong class="calibre1">pca_model=h2o.prcomp(training_frame = australia.hex, </strong><br class="title-page-name"/><strong class="calibre1">                     k = 8, </strong><br class="title-page-name"/><strong class="calibre1">                     transform = "STANDARDIZE")</strong></pre>
<p class="calibre2"><span>Now let's print a brief <kbd class="calibre13">summary</kbd> of the model:</span></p>
<pre class="calibre24"><strong class="calibre1">summary(pca_model)</strong></pre>
<p class="calibre2">In the following figure, we see a summary of the PCA <span>model:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00145.jpeg"/></div>
<p class="calibre2">To better understand the results, we can make a scree plot of the percent variability explained by each principal component. The percent variability explained is contained in the model importance variables from the PCA model.</p>
<p class="calibre2">The following figure shows a scree plot of the percent variability explained by each principal component:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00146.gif"/></div>
<p class="calibre2">The bar plot shows the proportion of variance for each principal component; as you can see, the first two components have about 70 percent of the variance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Autoencoders using H2O</h1>
                
            
            <article>
                
<p class="calibre2">An autoencoder is an ANN used for learning without efficient coding control. The purpose of an autoencoder is to learn coding for a set of data, typically to reduce dimensionality.<br class="title-page-name"/>
Architecturally, the simplest form of autoencoder is an advanced and non-recurring neural network very similar to the MLP, with an input level, an output layer, and one or more hidden layers that connect them, but with the layer outputs having the same number of input level nodes for rebuilding their inputs.</p>
<p class="calibre2">In the following is proposed an example of autoencoder using <kbd class="calibre13">h2o</kbd> on a <kbd class="calibre13">movie</kbd> dataset.</p>
<div class="packt_infobox">The dataset used in this example is a set of movies and genre taken from <a href="https://grouplens.org/datasets/movielens" class="calibre68">https://grouplens.org/datasets/movielens</a>.</div>
<p class="calibre2">We use the movies.csv file, which has three columns:</p>
<ul class="calibre16">
<li class="calibre17"><kbd class="calibre13"><span>movieId</span></kbd></li>
<li class="calibre17"><kbd class="calibre13"><span>title</span></kbd></li>
<li class="calibre17"><kbd class="calibre13"><span>genres</span></kbd></li>
</ul>
<p class="calibre2">There are 164,979 rows of data for clustering. We will use <kbd class="calibre13">h2o.deeplearning</kbd> to have the <kbd class="calibre13">autoencoder</kbd> parameter fix the clusters. The objective of the exercise is to cluster the movies based on genre, which can then be used to recommend similar movies or same-genre movies to the users. The program uses <kbd class="calibre13">h20.deeplearning</kbd>, with the <kbd class="calibre13">autoencoder</kbd> parameter set to <kbd class="calibre13">T</kbd>:</p>
<pre class="calibre24"><strong class="calibre1">#################################################################</strong><br class="title-page-name"/><strong class="calibre1">### Chapter 7 - Neural Networks with R - Use cases      #########</strong><br class="title-page-name"/><strong class="calibre1">### Autoencoder using H2O on a movie dataset            #########</strong><br class="title-page-name"/><strong class="calibre1">#################################################################</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">library("h2o")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">setwd ("c://R")</strong><br class="title-page-name"/><strong class="calibre1">#Load the training dataset of movies</strong><br class="title-page-name"/><strong class="calibre1">movies=read.csv ( "movies.csv", header=TRUE)</strong><br class="title-page-name"/><strong class="calibre1">head(movies)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">model=h2o.deeplearning(2:3, </strong><br class="title-page-name"/><strong class="calibre1">                       training_frame=as.h2o(movies),</strong><br class="title-page-name"/><strong class="calibre1">                       hidden=c(2), </strong><br class="title-page-name"/><strong class="calibre1">                       autoencoder = T, </strong><br class="title-page-name"/><strong class="calibre1">                       activation="Tanh")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">summary(model)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">features=h2o.deepfeatures(model,</strong><br class="title-page-name"/><strong class="calibre1">                         as.h2o(movies),</strong><br class="title-page-name"/><strong class="calibre1">                         layer=1)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">d=as.matrix(features[1:10,])</strong><br class="title-page-name"/><strong class="calibre1">labels=as.vector(movies[1:10,2])</strong><br class="title-page-name"/><strong class="calibre1">plot(d,pch=17)</strong><br class="title-page-name"/><strong class="calibre1">text(d,labels,pos=3)</strong></pre>
<p class="calibre2"><span>Now, let's go through the code:</span></p>
<pre class="calibre24"><strong class="calibre1">library("h2o")</strong><br class="title-page-name"/><strong class="calibre1">setwd ("c://R")</strong></pre>
<p class="calibre2"><span>These commands load the library in the R environment and set the working directory where we will have inserted the dataset for the next reading. Then we load the data:</span></p>
<pre class="calibre24"><strong class="calibre1">movies=read.csv( "movies.csv", header=TRUE)</strong></pre>
<p class="calibre2"><span>To visualize the type of data contained in the dataset, we analyze a preview of one of these variables:</span></p>
<pre class="calibre24"><strong class="calibre1">head(movies)</strong></pre>
<p class="calibre2">The following figure shows the first <kbd class="calibre13">20</kbd> rows of the <kbd class="calibre13">movie</kbd> dataset:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00147.jpeg"/></div>
<p class="calibre2">Now we build and train <kbd class="calibre13">model</kbd>:</p>
<pre class="calibre24"><strong class="calibre1">model=h2o.deeplearning(2:3, </strong><br class="title-page-name"/><strong class="calibre1">                       training_frame=as.h2o(movies),</strong><br class="title-page-name"/><strong class="calibre1">                       hidden=c(2), </strong><br class="title-page-name"/><strong class="calibre1">                       autoencoder = T, </strong><br class="title-page-name"/><strong class="calibre1">                       activation="Tanh")</strong></pre>
<p class="calibre2">Let's analyze some of the information contained in <kbd class="calibre13">model</kbd>:</p>
<pre class="calibre24"><strong class="calibre1">summary(model)</strong></pre>
<p class="calibre2"><span>This is an extract from the results of the <kbd class="calibre13">summary()</kbd> function:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00148.jpeg"/></div>
<p class="calibre2">In the next command, we use the <kbd class="calibre13">h2o.deepfeatures()</kbd> function to extract the non-linear feature from an <kbd class="calibre13">h2o</kbd> dataset using an H2O deep learning model:</p>
<pre class="calibre24"><strong class="calibre1">features=h2o.deepfeatures(model,</strong><br class="title-page-name"/><strong class="calibre1">                         as.h2o(movies),</strong><br class="title-page-name"/><strong class="calibre1">                         layer=1)</strong></pre>
<p class="calibre2">In the following code, the first six rows of the features extracted from the model are shown:</p>
<pre class="calibre24"><strong class="calibre1">&gt; features</strong><br class="title-page-name"/><strong class="calibre1"> DF.L1.C1 DF.L1.C2</strong><br class="title-page-name"/><strong class="calibre1">1 0.2569208 -0.2837829</strong><br class="title-page-name"/><strong class="calibre1">2 0.3437048 -0.2670669</strong><br class="title-page-name"/><strong class="calibre1">3 0.2969089 -0.4235294</strong><br class="title-page-name"/><strong class="calibre1">4 0.3214868 -0.3093819</strong><br class="title-page-name"/><strong class="calibre1">5 0.5586608 0.5829145</strong><br class="title-page-name"/><strong class="calibre1">6 0.2479671 -0.2757966</strong><br class="title-page-name"/><strong class="calibre1">[9125 rows x 2 columns]</strong></pre>
<p class="calibre2">Finally, we plot a diagram where we want to see how the model grouped the movies through the results obtained from the analysis:</p>
<pre class="calibre24"><strong class="calibre1">d=as.matrix(features[1:10,])</strong><br class="title-page-name"/><strong class="calibre1">labels=as.vector(movies[1:10,2])</strong><br class="title-page-name"/><strong class="calibre1">plot(d,pch=17)</strong><br class="title-page-name"/><strong class="calibre1">text(d,labels,pos=3)</strong></pre>
<p class="mce-root">The plot of the movies, once clustering is done, is shown next. We have plotted only 100 movie titles due to space issues. We can see some movies being closely placed, meaning they are of the same genre. The titles are clustered based on distances between them, based on genre.</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00149.jpeg"/></div>
<p class="calibre2">Given the large number of titles, the movie names cannot be distinguished, but what appears to be clear is that the model has grouped the movies into three distinct groups.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Breast cancer detection using darch</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will use the <kbd class="calibre13">darch</kbd> package, which is used for deep architectures and <strong class="calibre1">Restricted Boltzmann Machines</strong> (<strong class="calibre1">RBM</strong>). The <kbd class="calibre13">darch</kbd> package is built on the basis of the code from G. E. Hinton and R. R. Salakhutdinov (available under MATLAB code for <strong class="calibre1">Deep Belief Nets</strong> (<strong class="calibre1">DBN</strong>)). This package is for generating neural networks with many layers (deep architectures) and training them with the method introduced by the authors.<br class="title-page-name"/>
This method includes a pre-training with the contrastive divergence method and fine-tuning with commonly known training algorithms such as backpropagation or conjugate gradients. Additionally, supervised fine-tuning can be enhanced with maxout and dropout, two recently developed techniques used to improve fine-tuning for deep learning.</p>
<p class="calibre2">The basis of the example is classification based on a set of inputs. <span>To do this, we will use the data contained in the dataset named BreastCancer.csv that we just used in <a href="part0096.html#2RHM00-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 5</a>, <em class="calibre14">Training and Visualizing a Neural Network in R</em>. This data has been taken from the <span>UCI Repository Of Machine Learning.</span> The dataset is periodically updated as soon as Dr. Wolberg reports his clinical cases.</span> The data is of breast cancer patients with a classification of benign or malignant tumor based on a set of ten independent variables.</p>
<div class="packt_infobox"><span class="calibre62">To get the data, we draw on the large collection of data available in the <span class="calibre62">UCI Machine Learning Repository</span> at</span> <span class="calibre62"><a href="http://archive.ics.uci.edu/ml" class="calibre68">http://archive.ics.uci.edu/ml</a>.</span></div>
<p class="calibre2">Details of the data are as follows:</p>
<ul class="calibre16">
<li class="calibre17"><strong class="calibre1">Number of instances</strong>: 699 (as of 15 July 1992)</li>
<li class="calibre17"><strong class="calibre1">Number of attributes</strong>: 10 plus the class attribute</li>
<li class="calibre17"><strong class="calibre1">Attribute information</strong>: The class attribute has been moved to the last column</li>
</ul>
<p class="calibre2">The description of the attributes is shown here:</p>
<pre class="calibre24"><strong class="calibre1">   #  Attribute                     Domain
   -- -----------------------------------------
   1. Sample code number            id number
   2. Clump Thickness               1 - 10
   3. Uniformity of Cell Size       1 - 10
   4. Uniformity of Cell Shape      1 - 10
   5. Marginal Adhesion             1 - 10
   6. Single Epithelial Cell Size   1 - 10
   7. Bare Nuclei                   1 - 10
   8. Bland Chromatin               1 - 10
   9. Normal Nucleoli               1 - 10
  10. Mitoses                       1 - 10
  11. Class:                        (2 for benign, 4 for malignant)</strong><br class="title-page-name"/><br class="title-page-name"/></pre>
<p class="calibre2">To understand the <kbd class="calibre13">darch</kbd> function, we first set up an XOR gate and then use it for training and verification. T<span>he</span> <kbd class="calibre13">darch</kbd> <span>function</span> uses output data and input attributes to build the model, which can be tested internally by <kbd class="calibre13">darch</kbd> itself. In this case, we achieve 0 percent error and 100 percent accuracy.</p>
<p class="calibre2">Next, we use the breast cancer data to build the <kbd class="calibre13">darch</kbd> model and then check the accuracy:</p>
<pre class="calibre24"><strong class="calibre1">#####################################################################</strong><br class="title-page-name"/><strong class="calibre1">####Chapter 7 - Neural Networks with R #########</strong><br class="title-page-name"/><strong class="calibre1">####Breast Cancer Detection using darch package #########</strong><br class="title-page-name"/><strong class="calibre1">#####################################################################</strong><br class="title-page-name"/><strong class="calibre1">library("mlbench")</strong><br class="title-page-name"/><strong class="calibre1">library("darch")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">data(BreastCancer)</strong><br class="title-page-name"/><strong class="calibre1">summary(BreastCancer)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">data_cleaned &lt;- na.omit(BreastCancer) </strong><br class="title-page-name"/><strong class="calibre1">summary(data_cleaned)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">model &lt;- darch(Class ~ ., data_cleaned,layers = c(10, 10, 1),</strong><br class="title-page-name"/><strong class="calibre1">        darch.numEpochs = 50, darch.stopClassErr = 0, retainData = T)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">plot(model)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">predictions &lt;- predict(model, newdata = data_cleaned, type = "class")</strong><br class="title-page-name"/><strong class="calibre1">cat(paste("Incorrect classifications:", sum(predictions != data_cleaned[,11])))</strong><br class="title-page-name"/><strong class="calibre1">table(predictions,data_cleaned[,11])</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">library(gmodels)</strong><br class="title-page-name"/><strong class="calibre1">CrossTable(x = data_cleaned$Class, y = predictions,</strong><br class="title-page-name"/><strong class="calibre1">           prop.chisq=FALSE)</strong></pre>
<p class="calibre2">We begin analyzing the code line by line, explaining in detail all the features applied to capture the results:</p>
<pre class="calibre24"><strong class="calibre1">library("mlbench")</strong><br class="title-page-name"/><strong class="calibre1">library("darch")</strong></pre>
<p class="calibre2">The first two lines of the initial code are used to load the libraries needed to run the analysis.</p>
<div class="packt_tip">Remember that, to install a library that is not present in the initial distribution of R, you must use the <kbd class="calibre61">install.package</kbd> function. <span class="calibre62">This is the main function to install packages. It takes a vector of names and a destination library, downloads the packages from the repositories and installs them. T</span>his function should be used only once and not every time you run the code.</div>
<p class="calibre2">The <kbd class="calibre13">mlbench</kbd> library <span>contains a collection of artificial and real-world machine learning benchmark problems, including, for example, several datasets from the UCI repository.</span></p>
<p class="calibre2"><span>The <kbd class="calibre13">darch</kbd> library is a p</span>ackage for deep architectures and RBM:</p>
<pre class="calibre24"><strong class="calibre1">data(BreastCancer)</strong></pre>
<p class="calibre2"><span>With this command, we upload the dataset named <kbd class="calibre13">BreastCancer</kbd>, as mentioned, in the <kbd class="calibre13">mlbench</kbd> library. Let's see now that it's inside:</span></p>
<pre class="calibre24"><strong class="calibre1">summary(BreastCancer)</strong></pre>
<p class="calibre2"><span>With this command,</span> we see a brief summary by using the <kbd class="calibre13">summary()</kbd> function.</p>
<div class="packt_tip">Remember, the <kbd class="calibre61">summary()</kbd> function is a generic function used to produce result summaries of the results of various model fitting functions. The function invokes particular methods that depend on the class of the first argument.</div>
<p class="calibre2">In this case, the function has been applied to a dataframe and the results are listed in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00150.jpeg"/></div>
<p class="calibre2">The <kbd class="calibre13">summary()</kbd> function returns a set of statistics for each variable. In particular, it is useful to highlight the result provided for the <kbd class="calibre13">Class</kbd> variable, which contains the diagnosis of the cancer mass. In this case, <kbd class="calibre13">458</kbd> cases of <kbd class="calibre13">benign</kbd> class and <kbd class="calibre13">241</kbd> cases of <kbd class="calibre13">malignant</kbd> class were detected. Another feature to highlight is the <kbd class="calibre13">Bare.nuclei</kbd> variable. For this variable, <kbd class="calibre13">16</kbd> cases of missing values were detected.</p>
<p class="calibre2"><span>To remove missing values, we can use the <kbd class="calibre13">na.omit()</kbd> function:</span></p>
<pre class="calibre24"><strong class="calibre1">data_cleaned &lt;- na.omit(BreastCancer)</strong> </pre>
<p class="calibre2"><span>Now we build and train the model:</span></p>
<pre class="calibre24"><strong class="calibre1">model &lt;- darch(Class ~ ., data_cleaned,layers = c(10, 10, 1),</strong><br class="title-page-name"/><strong class="calibre1">        darch.numEpochs = 50, darch.stopClassErr = 0, retainData = T)</strong></pre>
<p class="calibre2">To evaluate the <kbd class="calibre13">model</kbd> performance, we can plot the raw network error:</p>
<pre class="calibre24"><strong class="calibre1">plot(model)</strong></pre>
<p class="calibre2">The plot of error versus epoch is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00151.gif"/></div>
<p class="calibre2">We get the minimum error at 34 epochs.</p>
<p class="calibre2">We finally have the network trained and ready for use; now we can use it to make our predictions:</p>
<pre class="calibre24"><strong class="calibre1">predictions &lt;- predict(model, newdata = data_cleaned, type = "class")</strong></pre>
<p class="calibre2">We used the entire set of data at our disposal to make our forecast using the model. All we have to do is compare the results obtained with the model predictions and the data available in the dataset:</p>
<pre class="calibre24"><strong class="calibre1">cat(paste("Incorrect classifications:", sum(predictions != data_cleaned[,11])))</strong></pre>
<p class="calibre2">The results are shown as follows:</p>
<pre class="calibre24"><strong class="calibre1">&gt; cat(paste("Incorrect classifications:", sum(predictions != data_cleaned[,11])))</strong><br class="title-page-name"/><strong class="calibre1">Incorrect classifications: 2</strong></pre>
<p class="calibre2">The results are really good! Only two wrong classifications! I would say that we can be content with the fact that they started from <kbd class="calibre13">683</kbd> observations. To better understand what the errors were, we build a confusion matrix:</p>
<pre class="calibre24"><strong class="calibre1">table(predictions,data_cleaned[,11])</strong></pre>
<p class="calibre2"><span>The results are shown here:</span></p>
<pre class="calibre24"><strong class="calibre1">&gt; table(predictions,data_cleaned[,11])</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">predictions benign malignant</strong><br class="title-page-name"/><strong class="calibre1">  benign       443         1</strong><br class="title-page-name"/><strong class="calibre1">  malignant      1       238</strong></pre>
<p class="calibre2"><span>Although in a simple way, the matrix tells us that we only made two errors equally distributed between the two values of the class. For more information on the confusion matrix, we can use the</span> <kbd class="calibre13">CrossTable()</kbd> <span>function contained in the</span> <kbd class="calibre13">gmodels</kbd> <span>package. As always, before loading the book, you need to install it:</span></p>
<pre class="calibre24"><strong class="calibre1">library(gmodels)</strong><br class="title-page-name"/><strong class="calibre1">CrossTable(x = data_cleaned$Class, y = predictions,</strong><br class="title-page-name"/><strong class="calibre1">           prop.chisq=FALSE)</strong></pre>
<p class="calibre2"><span>The confusion matrix obtained by using the</span> <kbd class="calibre13">CrossTable()</kbd> <span>function is shown in the following figure:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00152.jpeg"/></div>
<p class="calibre2">As we had anticipated in the classification, our model has only two errors: <em class="calibre14">FP</em> and <em class="calibre14">FN</em>. Then calculate the accuracy; as indicated in <a href="part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 2</a>, <em class="calibre14">Learning Processes in Neural Networks</em>, it is given by the following formula:</p>
<div class="calibre30"><img src="../images/00153.jpeg" class="calibre85"/></div>
<p class="calibre2">Let's calculate the accuracy in R environment:</p>
<pre class="calibre24"><strong class="calibre1">&gt; Accuracy = (443+238)/683</strong><br class="title-page-name"/><strong class="calibre1">&gt; Accuracy</strong><br class="title-page-name"/><strong class="calibre1">[1] 0.9970717</strong></pre>
<p class="calibre2">As mentioned before, the classifier has achieved excellent results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this final chapter, we saw some use cases with neural networks and deep learning. This should form the basis of your future work on neural networks. The usage is common in most cases, with changes in the dataset involved for the model during training and testing.</p>
<p class="calibre2">We saw the following examples in this chapter:</p>
<ul class="calibre16">
<li class="calibre17">Integrating TensorFlow and Keras with R, which opens up vast set of use cases to be built using R</li>
<li class="calibre17">Building a digit recognizer through classification using H2O</li>
<li class="calibre17">Understanding the LSTM function with MxNet</li>
<li class="calibre17">PCA using H2O</li>
<li class="calibre17">Building an autoencoder using H2O</li>
<li class="calibre17">Usage of <kbd class="calibre13">darch</kbd> for classification problems</li>
</ul>
<p class="calibre2">R is a very flexible and a major statistical programming language for data scientists across the world. A grasp of neural networks with R will help the community evolve further and increase the usage of R for deep learning and newer use cases.</p>


            </article>

            
        </section>
    </body></html>