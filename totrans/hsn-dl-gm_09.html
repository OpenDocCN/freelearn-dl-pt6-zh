<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Agent and the Environment</h1>
                </header>
            
            <article>
                
<p class="mce-root">Playing with and exploring experimental reinforcement learning environments is all well and good, but, at the end of the day, most game developers want to develop their own learning environment. To do that, we need to understand a lot more about training deep reinforcement learning environments, and, in particular, how an agent receives and processes input. Therefore, in this chapter, we will take a very close look at training one of the more difficult sample environments in Unity. This will help us understand many of the intricate details of how important input and state is to training an agent, and the many features in the Unity ML-Agents toolkit that make it easy for us to explore multiple options. This will be a critical chapter for anyone wanting to build their own environments and use the ML-Agents in their game. So, if you need to work through this chapter a couple of times to understand the details, please do so.</p>
<p>In this chapter, we are going to cover many details related to how agents process input/state, and how you can adapt this to fit your agent training. Here is a summary of what we will cover in this chapter:</p>
<ul>
<li>Exploring the training environment</li>
<li>Understanding state</li>
<li>Understanding visual state</li>
<li>Convolution and visual state</li>
<li>Recurrent networks</li>
</ul>
<p>Ensure that you have read, understood, and ran some of the sample exercises from the last chapter, <a href="b422aff5-b743-4696-ba80-e0a222ea5b4d.xhtml">Chapter 6</a>, <em>Unity ML-Agents</em>. It is essential that you have Unity and the ML-Agents toolkit configured and running correctly before continuing.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the training environment</h1>
                </header>
            
            <article>
                
<p class="mce-root">One of the things that often pushes us to success, or pushes us to learn, is failure. As humans, when we fail, one of two things happens: we try harder or we quit. Interestingly, this is not unlike a negative reward in reinforcement learning. In RL, an agent that gets a negative reward may quit exploring a path if it sees no future value, or that<span> it predicts will not give enough benefit. However, if the agent feels like more exploration is needed, or it hasn't exhausted the path fully, it will push on and, often, this leads it to the right path. Again, this is certainly not unlike us humans. Therefore, in this section, we are going to train one of the more difficult example agents to push ourselves to learn how to fail and fix training failures. </span></p>
<div class="packt_infobox">Unity is currently in the process of building a multi-level bench marking tower environment that features multiple levels of difficulty. This will allow DRL enthusiasts, practitioners, and researchers to test their skills/models on baseline environments. The author has been told, on reasonably good authority, that this environment should be completed by early/mid 2019.</div>
<p>We will need to use many of the advanced features of the Unity ML-Agents toolkit ultimately get this example working. This will require you to have a good understanding of the first five chapters of this book. If you skipped those chapters to get here, please go back and review them as needed. In many places in this chapter, helpful links have been provided to previous relevant chapters.</p>
<p>The training sample environment we will focus on is the <span class="packt_screen">VisualHallway</span>, not to be confused with the standard <span class="packt_screen">Hallway</span> example. The <span class="packt_screen">VisualHallway</span> differs in that it uses the camera as the complete input state into the model, while the other Unity examples we previously looked at used some form of multi-aware sensor input, often allowing the agent to see 90 to 360 degrees at all times, and be given other useful information. This is fine for most games, and, in fact, many games still allow  such cheats or intuition for NPC or computer opponents as part of their AI. Putting these cheats in for a game's AI has been an accepted practice for many years, but perhaps that will soon change.</p>
<p>After all, good games are fun to play, and make sense to the player. Games of the not so distant past could get away with giving the AI cheats. However, now, players are expecting more, they want their AI to play by the same rules as them. The previous perception that computer AI was hindered by technological limitations is gone, and now a game AI must play by the same rules as the player, which makes our focus on getting the <span class="packt_screen">VisualHallway</span> sample working/training more compelling.</p>
<p class="mce-root"/>
<p>There is, of course, another added benefit to teaching an AI to play/learn like a player, and that is the ability to transfer that capability to play in other environments using a concept called transfer learning. We will explore transfer learning in <a href="1525f2f4-b9e1-4b7f-ac40-33e801c668ed.xhtml">Chapter 10</a>, <em>Imitation and Transfer Learning</em>, where we will learn how to adapt pretrained models/parameters and apply them to other environments.</p>
<p>The <span class="packt_screen">VisualHallway</span>/<span class="packt_screen">Hallway</span> samples start by dropping the agent into a long room or hallway at random. In the center of this space is a colored block, and at one end of the hallway in each corner is a colored square covering the floor. The block is either red or gold (orange/yellow) and is used to inform the agent of the target square that is the same color. The goal is for the agent to move to the correct colored square. In the standard Hallway example, the agent is given 360 degree sensor awareness. In the Visual Hallway example, the agent is only shown a camera view of the room, exactly as the player version of the game would see. This puts our agent on equal footing with a player.</p>
<p>Before we get to training, let's open up the example and play it as a player would, and see how we do. Follow this exercise to open the <span class="packt_screen">VisualHallway</span> sample:</p>
<ol>
<li>Ensure you have a working installation of ML-Agents and can train a brain externally in Python before continuing. Consult the previous chapter if you need help.</li>
<li>Open the <span class="packt_screen">VisualHallway</span> scene from the <span class="packt_screen">Assets</span> | <span class="packt_screen">ML-Agents</span> | <span class="packt_screen">Examples</span> | <span class="packt_screen">Hallway</span> | <span class="packt_screen">Scenes</span> folder in the <span class="packt_screen">Project</span> window.</li>
</ol>
<p> </p>
<ol start="3">
<li>Make sure that <span class="packt_screen">Agent </span>| <span class="packt_screen">Hallway Agent </span>| <span class="packt_screen">Brain</span> is set to <span class="packt_screen">VisualHallwayPlayer</span>, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/259082a8-06ae-4583-9189-fa851ca24130.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Hallway Agent | Brain set to player</span></span></div>
<ol start="4">
<li>Press <span class="packt_screen">Play</span> in the editor to run the scene, and use the <em>W</em>, <em>A</em>, <em>S</em>, and <em>D</em> keys to control the agent. Remember, the goal is to move to the square that is the same color as the center square.</li>
<li>Play the game and move to both color squares to see what happens when a reward is given, either negative or positive. The game screen will flash with green or red when a reward square is entered.</li>
</ol>
<p>This game environment is typical of a first person shooter, and perfect for training an agent to play in first person as well. Training an agent to play as a human would be the goal of many an AI practitioner, and one you may or may not strive to incorporate in your game. As we will see, depending on the complexity of your game, this type of learning/training may not even be a viable option. At this point, we should look at how to set up and train the agent visually.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the agent visually</h1>
                </header>
            
            <article>
                
<p>Fortunately, setting up the agent to train it visually is quite straightforward, especially if you worked through the exercises in the last chapter. Open the Unity editor to the <span class="packt_screen">VisualHallway</span> scene, have a Python command or Anaconda window ready, and let's begin:</p>
<ol>
<li>In Unity, change <span class="packt_screen">Agent </span>| <span class="packt_screen">Hallway Agent </span>| <span class="packt_screen">Brain</span> to <span class="packt_screen">VisualHallwayLearning</span>, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3f960c28-6e36-45f1-b6c7-aeff1738ac2a.png" style="width:24.75em;height:17.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Changing that the Brain to learning</span></span></div>
<ol start="2">
<li>Click on the <span class="packt_screen">VisualHallwayLearning</span> brain to locate it in the <span class="packt_screen">Project</span> window.</li>
<li>Click on the <span class="packt_screen">VisualHallwayLearning</span> brain to view its properties in the <span class="packt_screen">Inspector</span> window, and as shown in the following screen excerpt:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c8bbaee9-105f-4b1f-8982-64535af4acf8.png" style="width:33.50em;height:26.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Confirming the properties are set correctly on the learning brain</span></div>
<ol start="4">
<li>Make sure that the <span class="packt_screen">Brain</span> parameters are set to accept a single <span class="packt_screen">Visual Observation</span> at a resolution of <kbd>84</kbd> x <kbd>84</kbd> pixels, and are not using <span class="packt_screen">Gray</span> scale. Gray is simply the removal of the color channels, which makes the input one channel instead of three. Recall our discussion of CNN layers in <a href="391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml">Chapter 2</a>, <em>Convolutional and Recurrent Networks</em>. Also, be sure that the <span class="packt_screen">Vector Observation</span> | <span class="packt_screen">Space Size</span> is <span class="packt_screen">0</span>, as shown in the preceding screenshot.</li>
<li>From the <span class="packt_screen">Menu</span>, select <span class="packt_screen">File</span> | <span class="packt_screen">Save</span> and <span class="packt_screen">File</span> | <span class="packt_screen">Save Project</span> to save all your changes.</li>
<li class="mce-root"><span>Switch to your</span> Python <span>window or</span> Anaconda <span>prompt, make sure you are in the</span> <kbd>ML-Agents/ml-agents</kbd> <span>directory, and run the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=visualhallway --train</strong></pre>
<p class="mce-root"/>
<ol start="7">
<li>After the command runs, wait for the prompt to start the editor. Then, run the editor when prompted and let the sample run to completion, or however long you have the patience for.</li>
<li>After you run the sample to completion, you should see something like the following:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7856918a-91b1-4722-9212-9e1b1f86e07e.png" style="width:55.83em;height:42.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Full training run to completion</span></div>
<p>Assuming you trained your agent to the end of the run that is, for 500 K iterations, then you can confirm that the agent does, in fact, learn nothing. So, why would Unity put an example like that in their samples? Well, you could argue that it was an intentional challenge, or perhaps just an oversight on their part. Either way, we will take it as a challenge to better understand reinforcement learning.</p>
<p>Before we tackle this challenge, let's take a step back and reaffirm our understanding of this environment by looking at the easier to train Hallway example in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reverting to the basics</h1>
                </header>
            
            <article>
                
<p>Often, when you get stuck on a problem, it helps to go back to the beginning and reaffirm that your understanding of everything works as expected. Now, to be fair, we have yet to explore the internals of ML-Agents and really understand DRL, so we never actually started at the beginning, but, for the purposes of this example, we will take a step back and look at the <span class="packt_screen">Hallway</span> example in more detail. Jump back into the editor and follow this exercise:</p>
<ol>
<li>Open the <span class="packt_screen">Hallway</span> sample scene in the editor. Remember, the scene is located in the <span class="packt_screen">Assets</span> | <span class="packt_screen">ML-Agents</span> | <span class="packt_screen">Examples</span> | <span class="packt_screen">Hallway</span> | <span class="packt_screen">Scenes</span> folder.</li>
<li>This example is configured to use several concurrent training environments. We are able to train multiple concurrent training environments with the same brain, because <strong>Proximal Policy Optimization</strong> (<strong>PPO</strong>), the RL algorithm powering this agent, trains to a policy and not a model. We will cover the fundamentals of policy and model-based learning when we get to the internals of PPO in <a href="1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml">Chapter 8</a>, <em>Understanding PPO,</em> for RL. For our purposes and for simplicity, we will disable these additional environments for now.</li>
<li>Press <em>Shift</em> and then select all the numbered <span class="packt_screen">HallwayArea</span> (1-15) objects in the <span class="packt_screen">Hierarchy</span>. </li>
<li>With all the extra <span class="packt_screen">HallwayArea</span> objects selected, disable them all by clicking the <span class="packt_screen">Active</span> checkbox, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bc31838a-bed8-4a26-aff8-f888099a6171.png" style="width:35.17em;height:11.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Disabling all the extra training hallways</span></span></div>
<ol start="5">
<li>Open the remaining active <span class="packt_screen">HallwayArea</span> in the <span class="packt_screen">Hierarchy</span> window and select the <span class="packt_screen">Agent</span>.</li>
<li>Set the <span class="packt_screen">Brain</span> agents to use the <span class="packt_screen">HallwayLearning</span> brain. It may be set to use the player brain by default.</li>
<li>Select the <span class="packt_screen">Academy</span> object back in the <span class="packt_screen">Hierarchy</span> window, and make sure the <span class="packt_screen">Hallway Academy</span> component has its brain set to <span class="packt_screen">Learning</span> and that the <span class="packt_screen">Control</span> checkbox is enabled.</li>
<li class="mce-root"><span>Open a Python or Anaconda window to the</span> <kbd>ML-Agents/ml-agents</kbd> <span>folder. Make sure your ML-Agents virtual environment is active and run the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=hallway --train</strong></pre>
<ol start="9">
<li>Let the trainer start up and prompt you to click <span class="packt_screen">Play</span> in the editor. Watch the agent run and compare its performance to the <span class="packt_screen">VisualHallway</span> example.</li>
</ol>
<p>Generally, you will notice some amount of training activity from the agent before 50,000 iterations, but this may vary. By training activity, we mean the agent is responding with a Mean Reward greater than -1.0 and a Standard Reward not equal to zero. Even if you let the example run to completion, that is, 500,000 iterations again, it is unlikely that the sample will train to a positive Mean Reward. We generally want our rewards to range from -1.0 to +1.0, with some amount of variation to show learning activity. If you recall from the VisualHallway example, the agent showed no learning activity for the duration of the training. We could have extended the training iterations, but it is unlikely we would have seen any stable training emerge. The reason for this has to do with the increased state space and handling of rewards. We will expand our understanding of state and how it pertains to RL in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding state</h1>
                </header>
            
            <article>
                
<p>The <span class="packt_screen">Hallway</span> and <span class="packt_screen">VisualHallway</span> examples are essentially the same game problem, but provide a different perspective, or what we may refer to in reinforcement learning as environment or game state. In the <span class="packt_screen">Hallway</span> example, the agent learns by sensor input, which is something we will look at shortly, while in the <span class="packt_screen">VisualHallway</span> example, the agent learns by a camera or player view. What will be helpful at this point is to understand how each example handles state, and how we can modify it.</p>
<p class="mce-root"/>
<p>In the following exercise, we will modify the <span class="packt_screen">Hallway</span> input state and see the results:</p>
<ol>
<li>Jump back into the <span class="packt_screen">Hallway</span> scene with learning enabled as we left it at the end of the last exercise. </li>
<li>We will need to modify a few lines of C# code, nothing very difficult, but it may be useful to install Visual Studio (Community or another version) as this will be our preferred editor. You can, of course, use any code editor you like as long as it works with Unity.</li>
<li>Locate the <span class="packt_screen">Agent</span> object in the <span class="packt_screen">Hierarchy</span> window, and then, in the <span class="packt_screen">Inspector</span> window, click the Gear icon over the <span class="packt_screen">Hallway Agent</span> component, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/62b836fb-b5d0-405a-aaef-059a8205417c.png" style="width:30.58em;height:23.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Opening the HallwayAgent.cs script </span></div>
<ol start="4">
<li>From the context menu, select the <span class="packt_screen">Edit Script</span> option, as shown in the previous screenshot. This will open the script in your code editor of choice.</li>
</ol>
<p> </p>
<ol start="5">
<li><span>Lo</span><span>cate the following section of</span> C# <span>code in your editor:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">public override void CollectObservations()<br/>{<br/>  if (useVectorObs)<br/>  {<br/>    float rayDistance = 12f;<br/>    <strong>float[] rayAngles = { 20f, 60f, 90f, 120f, 160f };</strong><br/>    string[] detectableObjects = { "orangeGoal", "redGoal", "orangeBlock", "redBlock", "wall" };<br/>    AddVectorObs(GetStepCount() / (float)agentParameters.maxStep);<br/>    AddVectorObs(rayPer.Perceive(rayDistance, rayAngles, detectableObjects, 0f, 0f));<br/>  }<br/>}</pre>
<ol start="6">
<li>The <kbd>CollectObservations</kbd> method is where the agent collects its observations or inputs its state. In the Hallway example, the agent has <kbd>useVectorObs</kbd> set to <kbd>true</kbd>, meaning that it detects state by using the block of code that's internal to the <kbd>if</kbd> statement. All this code does is cast a ray or line from the agent in angles of <kbd>20f</kbd>, <kbd>60f</kbd>, <kbd>120f</kbd>, and <kbd>160f</kbd> degrees at a distance defined by <kbd>rayDistance</kbd> and detect objects defined in <kbd>detectableObjects</kbd>. The ray perception is done with a helper component called <kbd>rayPer</kbd> of the <kbd>RayPerception</kbd> type, and it executes <kbd>rayPer.Percieve</kbd> to collect the environment state it perceives. This, along with the ratio of steps, is added to the vector observations or state the agent will input. At this point, the state is 36 vectors in length. As of this version, this needs to be constructed in code, but this will likely change in the future.</li>
<li class="mce-root"><span>Alter the</span> <kbd>rayAngles</kbd> <span>line of code so that it matches the following:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>float[] rayAngles = { 20f, 60f };</strong></pre>
<ol start="8">
<li> This has the effect of narrowing the agent's vision or perception dramatically from 180 to 60 degrees. Another way to think of it is reducing the input state.</li>
<li><span>After you finish the edit, save the file and return to Unity. Unity will recompile the code when you return to the editor. </span></li>
</ol>
<p> </p>
<ol start="10">
<li>Locate the <span class="packt_screen">HallwayLearning</span> brain in the <span class="packt_screen">Assets</span> | <span class="packt_screen">ML-Agents</span> | <span class="packt_screen">Examples</span> | <span class="packt_screen">Hallway</span> | <span class="packt_screen">Brains</span> folder and change the <span class="packt_screen">Vector Observation</span> | <span class="packt_screen">Space Size</span> to <kbd>15</kbd>, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign" style="color: black;font-size: 1em"><img src="assets/39bc0790-79d7-4128-ac3d-1fedb99babb0.png" style="width:31.83em;height:20.50em;"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><span>Setting the Vector Observation Space Size</span></div>
<ol start="11">
<li>The reason we reduce this to 15 is that the input now consists of two angle inputs, plus one steps input. Each angle input consists of five detectable objects, plus two boundaries for seven total perceptions or inputs. Thus, two angles times seven perceptions plus one for steps, equals 15. Previously, we had five angles times seven perceptions plus one step, which equals 35.</li>
<li>Make sure that you save the project after modifying the <span class="packt_screen">Brain</span> scriptable objects.</li>
<li>Run the example again in training and watch how the agent trains. Take some time and pay attention to the actions the agent takes and how it learns. Be sure to let this example run as long as you let the other Hallway sample run for, hopefully to completion.</li>
</ol>
<p>Were you surprised by the results? Yes, our agent with a smaller view of the world actually trained quicker. This may seem completely counter-intuitive, but think about this in terms of mathematics. A smaller input space or state means the agent has less paths to explore, and so should train quicker. This is indeed what we saw in this example when we reduced the input space by more than half. At this point, we definitely need to see what happens when we reduce the visual state space in the VisualHallway example.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding visual state</h1>
                </header>
            
            <article>
                
<p>RL is a very powerful algorithm, but can become very computationally complex when we start to look at massive state inputs. To account for massive states, many powerful RL algorithms use the concept of model-free or policy-based learning, something we will cover in a later chapter. As we already know, Unity uses a policy-based algorithm that allows it to learn any size of state space by generalizing to a policy. This allows us to easily input a state space of 15 vectors in the example we just ran to something more massive, as in the VisualHallway example.</p>
<p>Let's open up Unity to the VisualHallway example scene and look at how to reduce the visual input space in the following exercise:</p>
<ol>
<li>With the <span class="packt_screen">VisualHallway</span> scene open, locate the <span class="packt_screen">HallwayLearningBrain</span> in the <span class="packt_screen">Assets</span> | <span class="packt_screen">ML-Agents</span> | <span class="packt_screen">Examples</span> | <span class="packt_screen">Hallway</span> | <span class="packt_screen">Brains</span> folder and select it.</li>
<li>Modify the <span class="packt_screen">Brain Parameters</span> <strong><span class="packt_screen">|</span></strong> <span class="packt_screen">Visual Observation</span> first camera observable to an input of <kbd>32</kbd> x <kbd>32</kbd> <span class="packt_screen">Gray</span> scale. An example of this is shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6655a1f7-7595-454c-863c-60b2c4107101.png" style="width:32.50em;height:23.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Setting up the visual observation space for the agent</span></span></div>
<ol start="3">
<li>When <span class="packt_screen">Visual Observations</span> are set on a brain, then every frame is captured from the camera at the resolution selected. Previously, the captured image was 84 x 84 pixels large, by no means as large as the game screen in player mode, but still significantly larger than 35 vector inputs. By reducing our image size and making it <span class="packt_screen">gray</span>, scale we reduced one input frame from 84 x 84 x 3 = 20,172 inputs to 32 x 32 x 1 =1,024. In turn, this greatly reduces our required model input space and the complexity of the network that's needed to learn.</li>
<li>Save the project and the scene.</li>
<li class="mce-root"><span>Run the</span> VisualHallway <span>in learning mode again using the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=vh_reduced --train</strong></pre>
<ol start="6">
<li>Notice how we are changing the <kbd>--run-id</kbd> parameter with every run. Recall that, if we want to use TensorBoard, then each of our runs needs a unique name, otherwise it just writes over previous runs.</li>
<li>Let the sample train for as long as you ran the earlier VisualHallway exercise, as this will give you a good comparison of the change we made in state.</li>
</ol>
<p>Are the results what you expected? Yeah, the agent still doesn't learn, even after reducing the state. The reason for this is because the smaller visual state actually works against the agent in this particular case. Not unlike the results, we would expect us humans to have when trying to solve a task by looking through a pinhole. However, there is another way to reduce visual state into feature sets using convolution. As you may recall, we covered convolution and CNN in <a href="391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml">Chapter 2</a>, <em>Convolutional and Recurrent Networks</em>, at some length. In the next section, we will look at how we can reduce the visual state of our example by adding convolutional layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolution and visual state</h1>
                </header>
            
            <article>
                
<p>The visual state an agent uses in the ML-Agents toolkit is defined by a process that takes a screenshot at a specific resolution and then feeds that into a convolutional network to train some form of embedded state. In the following exercise, we will open up the ML-Agents training code and enhance the convolution code for better input state:</p>
<ol>
<li>Use a file browser to open the ML-Agents <kbd>trainers</kbd> folder located at <kbd>ml-agents.6\ml-agents\mlagents\trainers</kbd><strong>. </strong>Inside this folder, you will find several Python files that are used to train the agents. The file we are interested in is called <kbd>models.py</kbd>.</li>
</ol>
<p> </p>
<ol start="2">
<li>Open the <kbd>models.py</kbd> file in your Python editor of choice. Visual Studio with the Python data extensions is an excellent platform, and also provides the ability to interactively debug code.</li>
<li class="mce-root"><span>Scroll down in the file to locate the</span> <kbd>create_visual_observation_encoder</kbd> <span>function, which looks as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">def create_visual_observation_encoder(self, image_input, h_size, activation, num_layers, scope,reuse):<br/>  #comments removed        <br/>  with tf.variable_scope(scope):<br/>    <strong>conv1 = tf.layers.conv2d(image_input, 16, kernel_size=[8, 8], strides=[4, 4],activation=tf.nn.elu, reuse=reuse, name="conv_1")</strong><br/><strong>    conv2 = tf.layers.conv2d(conv1, 32, kernel_size=[4, 4], strides=[2, 2],activation=tf.nn.elu, reuse=reuse, name="conv_2")</strong><br/>    hidden = c_layers.flatten(conv2)<br/><br/>    with tf.variable_scope(scope + '/' + 'flat_encoding'):<br/>      hidden_flat = self.create_vector_observation_encoder(hidden, h_size, activation, num_layers, scope, reuse)<br/> return hidden_flat</pre>
<ol start="4">
<li>The code is Python using TensorFlow, but you should be able to identify the <kbd>conv1</kbd> and <kbd>conv2</kbd> <span>convolution layers. </span>Notice how the kernel and stride is defined for layers and the missing pooling layers as well. Unity does not use pooling in order to avoid loss of spatial relationships in data. However, as we discussed earlier, this is not always so cut-and-dry, and really varies by the type of visual features you are trying to identify.</li>
<li class="mce-root"><span>Add the following lines of code after the two convolution layers and modify the</span> <kbd>hidden</kbd> <span>layer setup, as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">conv1 = tf.layers.conv2d(image_input, 16, kernel_size=[8, 8], strides=[4, 4], activation=tf.nn.elu, reuse=reuse, name="conv_1")<br/>conv2 = tf.layers.conv2d(conv1, 32, kernel_size=[4, 4], strides=[2, 2], activation=tf.nn.elu, reuse=reuse, name="conv_2")<br/><strong>conv3 = tf.layers.conv2d(image_input, 64, kernel_size=[2, 2], strides=[2, 2], activation=tf.nn.elu, reuse=reuse, name="conv_3")</strong><br/><br/>hidden = c_layers.flatten(<strong>conv3</strong>)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="6">
<li>This will have the effect of adding another layer of convolution to extract finer details in the agents game view. As we saw in <a href="391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml">Chapter 2</a>, <em>Convolutional and Recurrent Networks</em>, adding extra layers of convolution will increase training time, but does increase training performance – at least on image classifiers, anyway.</li>
<li class="mce-root"><span>Jump back to your command or Anaconda window and run the sample in learning mode with the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=vh_conv1 --train</strong></pre>
<ol start="8">
<li>Observe the training and watch how the agent performs—be sure to watch the agent's movements in the <span class="packt_screen">Game</span> window as the sample runs. Is the agent doing what you expected? Compare your results with the previous runs and notice the differences.</li>
</ol>
<p>One thing you will certainly notice is the agent becoming slightly more graceful and being able to perform finer movements. While the training may take much longer overall, this agent will be able to observe finer changes in the environment, and so will make finer movements. You could, of course, swap the entire CNN architecture of ML-Agents to use more well-defined architectures. However, be aware that most image classification networks ignore spatial relevance that, as we will see in the next section, is very relevant to game agents.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">To pool or not to pool</h1>
                </header>
            
            <article>
                
<p>As we discussed in <a href="391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml">Chapter 2</a>, <em>Convolutional and Recurrent Networks</em>, ML-Agents does not use any pooling in order to avoid any loss of spatial relationships in data. However, as we saw in our self-driving vehicle example, a single pooling layer or two up at the higher feature level extraction (convolutional layers) can in fact help. Although our example was tested on a much more complex network, it will be helpful to see how this applies to a more complex ML-Agents CNN embedding. Let's try this out, and apply a layer of pooling to the last example by completing the following exercise:</p>
<ol>
<li>Open the<span> </span><kbd>models.py</kbd><span> </span>file in your<span> </span>Python<span> </span>editor of choice. Visual Studio<span> </span>with the<span> </span>Python<span> </span>data extensions is an excellent platform, and also provides the ability to interactively debug code.</li>
<li class="mce-root"><span> Locate the following block of code, which is as we last left it in the previous exercise:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">conv1 = tf.layers.conv2d(image_input, 16, kernel_size=[8, 8], strides=[4, 4], activation=tf.nn.elu, reuse=reuse, name="conv_1")<br/>conv2 = tf.layers.conv2d(conv1, 32, kernel_size=[4, 4], strides=[2, 2], activation=tf.nn.elu, reuse=reuse, name="conv_2")<br/>conv3 = tf.layers.conv2d(image_input, 64, kernel_size=[2, 2], strides=[2, 2], activation=tf.nn.elu, reuse=reuse, name="conv_3")<br/><br/>hidden = c_layers.flatten(<strong>conv3</strong>)</pre>
<ol start="3">
<li>We will now inject a layer of pooling by modifying the block of code, like so:</li>
</ol>
<pre style="color: black;padding-left: 60px">conv1 = tf.layers.conv2d(image_input, 16, kernel_size=[8, 8], strides=[4, 4], activation=tf.nn.elu, reuse=reuse, name="conv_1")<br/>#################### ADD POOLING<br/>conv2 = tf.layers.conv2d(conv1, 32, kernel_size=[4, 4], strides=[2, 2], activation=tf.nn.elu, reuse=reuse, name="conv_2")<br/><strong>conv3 = tf.layers.conv2d(image_input, 64, kernel_size=[2, 2], strides=[2, 2], activation=tf.nn.elu, reuse=reuse, name="conv_3")</strong><br/><br/>hidden = c_layers.flatten(<strong>conv3</strong>)</pre>
<ol start="4">
<li>This now sets up our previous sample to use a single layer of pooling. You can think of this as extracting all the upper features, such as the sky, wall, or floor, and pooling the results together. When you think about it, how much spatial information does the agent need to know regarding one sky patch versus another? All the agent really needs to know is that the sky is always up.</li>
<li class="mce-root"><span>Open your command shell or Anaconda window and train the sample by running the following code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=vh_conv_wpool1 --train</strong></pre>
<ol start="6">
<li>As always, watch the performance of the agent and notice how the agent moves as it trains. Watch the training until completion, or as much as you observed others.</li>
</ol>
<p>Now, depending on your machine or environment you may have noticed a substantial improvement in training time, but actual performance suffered slightly. This means that each training iteration executed much quicker, two to three times or more, but the agent needs slightly more interactions. In this case, the agent will train quicker time-wise, but in other environments, pooling at higher levels maybe more <span>detrimental</span><span>. When it comes down to it, it will depend on the visuals of your environment, how well you want your agent to perform, and, ultimately, your patience.</span></p>
<p> </p>
<p>In the next section, we will look at another characteristic of state – memory, or sequencing. We will look at how recurrent networks are used to capture the importance of remembering sequences or event series.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent networks for remembering series</h1>
                </header>
            
            <article>
                
<p>The sample environments we have been running in this chapter use a form of recurrent memory by default to remember past sequences of events. This recurrent memory is constructed of <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) layers that allow the agent to remember beneficial sequences that may encourage some amount of future reward. Remember that we extensively covered LSTM networks in <a href="391f7c79-537a-4c1d-bb92-e517097cd4d8.xhtml">Chapter 2</a>, <em>Convolutional and Recurrent Networks</em>. For example, an agent may see the same sequence of frames repeatedly, perhaps moving toward the target goal, and then associate that sequence of states with an increased reward. A diagram showing the original form of this network, taken from the paper <em>Training an Agent for FPS Doom Game using Visual Reinforcement Learning and VizDoom</em> by <em>Khan Aduil et a</em><em>l.,</em> is as follows:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/42b80820-40ef-424c-8c27-8651f58f377d.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><br/>
<span>DQRN Architecture</span></div>
<p>The authors referred to the network architecture as DQRN, which stands for deep Q recurrent network. It is perhaps strange they did not call it DQCRN, since the diagram clearly shows the addition of convolution. While the ML-Agents implementation is slightly different, the concept is very much the same. Either way, the addition of LSTM layers can be a huge benefit to agent training, but, at this stage, we have yet to see the affect of not being used in training.</p>
<p class="mce-root"/>
<p>Therefore, in the following exercise, we will learn how to disable recurrent networks and see what effect this has on training:</p>
<ol>
<li>Open the standard Hallway example scene, the one without visual learning, from the <kbd>Assets/ML-Agents/Examples/Hallway/Scenes</kbd> folder.</li>
<li>Open a command shell or Anaconda window and make sure your ML-Agent's virtual Python environment is active.</li>
<li>Locate and open the <kbd>trainer_config.xml</kbd> file located in the <kbd>ML-Agents/ml-agents/config</kbd> folder in a text or XML editor of your choice.</li>
<li class="mce-root"><span>Locate the configuration block, as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">HallwayLearning:<br/>    <strong>use_recurrent: true</strong><br/>    sequence_length: 64<br/>    num_layers: 2<br/>    hidden_units: 128<br/>    memory_size: 256<br/>    beta: 1.0e-2<br/>    gamma: 0.99<br/>    num_epoch: 3<br/>    buffer_size: 1024<br/>    batch_size: 128<br/>    max_steps: 5.0e5<br/>    summary_freq: 1000<br/>    time_horizon: 64</pre>
<ol start="5">
<li>The named configuration block, called <kbd>HallwayLearning</kbd>, matches the name of the brain we set up in the <span class="packt_screen">Academy</span> within the scene. If you need to confirm this, go ahead.</li>
<li>We generally refer to all these configuration parameters as hyperparameters, and they can have a considerable effect on training, especially if set incorrectly. If you scroll to the top of the file, you will notice a set of default parameters, followed by exceptions for each of the named brains. Each section of brain parameters for each brain override the default settings.</li>
<li class="mce-root"><span>Disable the <kbd>use_recurrent</kbd> networks by modifying the code, as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">HallwayLearning:<br/>    <strong>use_recurrent: false</strong></pre>
<ol start="8">
<li>Setting <kbd>use_recurrent</kbd> to <kbd>false</kbd> disables the use of recurrent encoding. We can now see what effect this has on training.</li>
</ol>
<p> </p>
<ol start="9">
<li>Save the configuration file.</li>
<li>Run the sample on learning as you normally would. You should be able to run a training sample in your sleep by now.</li>
<li>As always, watch how the agent performs and be sure to pay attention to the agent's movements as well.</li>
</ol>
<p>As you can see, the agent performs considerably worse in this example, and it is obvious that the use of recurrent networks to capture sequences of important moves made a big difference. In fact, in most repetitive game environments, such as the Hallway and VisualHallway, the addition of recurrent state works quite well. However, there will be other environments that may not benefit, or may indeed suffer, from the use of state sequencing. Environments that feature extensive exploration or new content may, in fact, suffer. Since the agent may prefer shorter action sequences, this is limited by the amount of memory that is configured for the agent. Try to keep that in mind when you develop a new environment.</p>
<p>Now that we have a comparison for how our samples run without recurrent or LSTM layers, we can test the sample again by tweaking some of the relevant recurrent hyperparameters in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tuning recurrent hyperparameters</h1>
                </header>
            
            <article>
                
<p>As we learned in our discussion of recurrent networks, LSTM layers may receive variable input, but we still need to define a maximum sequence length that we want the network to remember. There are two critical hyperparameters we need to play with when using recurrent networks. A description of these parameters, at the time of writing, and as listed in the ML-Agents docs, is as follows:</p>
<ul>
<li> <kbd>sequence_length</kbd>: <em>C</em>orresponds to the length of the sequences of experience that are passed through the network during training. This should be long enough to capture whatever information your agent might need to remember over time. For example, if your agent needs to remember the velocity of objects, then this can be a small value. If your agent needs to remember a piece of information that's given only once at the beginning of an episode, then this should be a larger value:
<ul>
<li>Typical Range: 4 – 128</li>
</ul>
</li>
<li>
<p class="mce-root"><kbd>memory_size</kbd>: Corresponds to the size of the array of floating point numbers that are used to store the hidden state of the recurrent neural network. This value must be a multiple of four, and should scale with the amount of information you expect the agent will need to remember to successfully complete the task:</p>
<ul>
<li>
<p class="mce-root">Typical Range: 64 – 512</p>
</li>
</ul>
</li>
</ul>
<div class="packt_tip">The description of the recurrent <kbd>sequence_length</kbd> and <kbd>memory_size</kbd> <span>hyperparameters was </span>extracted directly from the Unity ML-Agents documentation.</div>
<p>If we look at our VisualHallway example configuration in the <kbd>trainer_config.yaml</kbd> file, we can see that the parameters are defined as follows:</p>
<pre>VisualHallwayLearning:<br/>    use_recurrent: true<br/>    <strong>sequence_length: 64</strong><br/>    num_layers: 1<br/>    hidden_units: 128<br/>    <strong>memory_size: 256</strong><br/>    beta: 1.0e-2<br/>    gamma: 0.99<br/>    num_epoch: 3<br/>    buffer_size: 1024<br/>    batch_size: 64<br/>    max_steps: 5.0e5<br/>    summary_freq: 1000<br/>    time_horizon: 64</pre>
<p>This effectively means that our agent will remember 64 frames or states of input using a memory size of 256. The documentation is unclear as to how much memory a single input takes, so we can only assume that the default visual convolutional encoding network, the original two layer model, requires four per frame. We can assume that, by increasing our convolutional encoding in the previous examples, the agent may have not been able to remember every frame of state. Therefore, let's modify the configuration in the VisualHallway example to account for that increase in memory, and see the effect it has in the following exercise:</p>
<ol>
<li>Open up the VisualHallway example to where we last left it in the previous exercises, with or without pooling enabled. Just be sure to remember if you are or are not using pooling, as this will make a difference to the required memory.</li>
</ol>
<p> </p>
<ol start="2">
<li>Open the <kbd>trainer_config.yaml</kbd> file located in the <kbd>ML-Agents/ml-agents/config</kbd> folder.</li>
<li class="mce-root"><span>Modify the</span> <kbd>VisualHallwayLearning</kbd> <span>config section, as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">VisualHallwayLearning:<br/>    use_recurrent: true<br/>    <strong>sequence_length: 128</strong><br/>    num_layers: 1<br/>    hidden_units: 128<br/>    <strong>memory_size: 2048 without pooling, 1024 with pooling</strong><br/>    beta: 1.0e-2<br/>    gamma: 0.99<br/>    num_epoch: 3<br/>    buffer_size: 1024<br/>    batch_size: 64<br/>    max_steps: 5.0e5<br/>    summary_freq: 1000<br/>    time_horizon: 64</pre>
<ol start="4">
<li>We are increasing the agent's memory from 64 to 128 sequences, thus doubling its memory. Then, we are increasing the memory to 2,048 when not using pooling, and 1,024 when using pooling. Remember that pooling collects features and reduces the number of feature maps that are produced at every step of convolution.</li>
<li>Save the file after you finish editing it.</li>
<li class="mce-root"><span>Open your command or Anaconda window and start training with the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=vh_recurrent --train</strong></pre>
<ol start="7">
<li>When prompted, start the training session in the editor by pressing <span class="packt_screen">Play</span> and watch the action unfold.</li>
<li>Wait for the agent to train, like you did for the other examples we ran. You should notice another increase in training performance, as well as the choice of actions the agent makes, which should look better coordinated.</li>
</ol>
<p>As we can see, a slight tweaking of hyperparameters allowed us to improve the performance of the agent. Understanding the use of the many parameters that are used in training will be critical to your success in building remarkable agents. In the next section, we will look at further exercises you can use to improve your understanding and skill.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>As always, try and complete a minimum of two to three of these exercises on your own, and for your own benefit. While this is a hands-on book, it always helps to spend a little more time applying your knowledge to new problems.</p>
<p>Complete the following exercises on your own:</p>
<ol>
<li>Go through and explore the <span class="packt_screen">VisualPushBlock</span> example. This example is quite similar to the <span class="packt_screen">Hallway</span>, and is a good analog to play with.</li>
<li>Modify the Hallway example's <span class="packt_screen">HallwayAgent</span> script to use more scanning angles, and thus more vector observations.</li>
<li>Modify the Hallway example to use a combined sensor sweep and visual observation input. This will require you to modify the learning brain configuration by adding a camera, and possibly updating some hyperparameters.</li>
<li>Modify other visual observation environments to use some form of vector observation. A good example to try this on is the <span class="packt_screen">VisualPushBlock</span> example.</li>
<li>Modify the visual observation camera space to be larger or smaller than 84 x 84 pixels, and to use, or not use, gray scaling. This is a good exercise to play with when testing more complex or simpler CNN network architectures.</li>
<li>Modify the <kbd>create_visual_observation_encoder</kbd> <span>convolutional encoding function so that it can</span> use different CNN architectures. These architectures may be as simple or complex as you want.</li>
<li>Modify the <kbd>create_visual_observation_encoder</kbd><span> convolutional encoding function </span>to use different levels and amounts of pooling layers. Try and use pooling after every convolutional layer to explore its effect on training.</li>
<li>Disable and enable recurrent networks on one or two of the other example's environments and explore the effect this has.</li>
<li>Play with the <kbd>sequence_length</kbd> and <kbd>memory_size</kbd> parameters with recurrent enabled to see the effect that different sequence lengths have on agent performance. Be sure to increase the <kbd>memory_size</kbd> parameter if you increase the <kbd>sequence_length</kbd>.</li>
<li>Consider adding additional vector or visual observations to the agent. After all, an agent doesn't have to have only a single form of sensory input. An agent could always detect the direction it is in, or perhaps it may have other forms of sensory input, such as being able to listen. We will give an agent the ability to listen in a later chapter, but try and implement this yourself.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Remember, these exercises are provided for your benefit and enjoyment, so be sure to try at least a couple. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we took a very close look at how the agents in ML-Agents perceive their environment and process input. An agent's perception of the environment is completely in control by the developer, and it is often a fine balance of how much or how little input/state you want to give an agent. We played with many examples in this chapter and started by taking an in-depth look at the Hallway sample and how an agent uses rays to perceive objects in the environment. Then, we looked at how an agent can use visual observations, not unlike us humans, as input or state that it may learn from. From this, we delved into the CNN architecture that ML-Agents uses to encode the visual observations it provides to the agent. We then learned how to modify this architecture by adding or removing convolution or pooling layers. Finally, we looked at the role of memory, or how recurrent sequencing of input state can be used to help with agent training. Recurrent networks allow an agent to add more value to action sequences that provide a reward.</p>
<p>In the next chapter, we will take a closer look at RL and how agents use the PPO algorithm. We will learn more about the foundations of RL along the way, as well as learn about the importance of the many hyperparameters used in training.</p>


            </article>

            
        </section>
    </body></html>