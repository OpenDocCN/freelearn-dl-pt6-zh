- en: Create and Train Machine Translation Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of this project is to train an **artificial intelligence** (**AI**)
    model to be able to translate between two languages. Specifically, we will see
    an automatic translator which reads German and produces English sentences; although,
    the model and the code developed in this chapter is generic enough for any language
    pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'The project explored in this chapter has four important sections, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A walkthrough of the architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing the corpora
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the machine translator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing and translating
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of them will describe one key component of the project, and, at the end,
    you'll have a clear picture of what's going on.
  prefs: []
  type: TYPE_NORMAL
- en: A walkthrough of the architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A machine translation system receives as input an arbitrary string in one language
    and produces, as output, a string with the same meaning but in another language.
    Google Translate is one example (but also many other main IT companies have their
    own). There, users are able to translate to and from more than 100 languages.
    Using the webpage is easy: on the left just put the sentence you want to translate
    (for example, Hello World), select its language (in the example, it''s English),
    and select the language you want it to be translated to.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example where we translate the sentence Hello World to French:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccea21d1-123d-41a1-b60d-7cae15f0e6cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Is it easy? At a glance, we may think it's a simple dictionary substitution.
    Words are chunked, the translation is looked up on the specific English-to-French
    dictionary, and each word is substituted with its translation. Unfortunately,
    that's not the case. In the example, the English sentence has two words, while
    the French one has three. More generically, think about phrasal verbs (turn up,
    turn off, turn on, turn down), Saxon genitive, grammatical gender, tenses, conditional
    sentences... they don't always have a direct translation, and the correct one
    should follow the context of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s why, for doing machine translation, we need some artificial intelligence tools.
    Specifically, as for many other **natural language processing** (**NLP**) tasks,
    we''ll be using **recurrent neural networks** (**RNNs**). We introduced RNNs in
    the previous chapter, and the main feature they have is that they work on sequences:
    given an input sequence, they produce an output sequence. The objective of this
    chapter is to create the correct training pipeline for having a sentence as the
    input sequence, and its translation as the output one. Remember also the *no free
    lunch theorem*: this process isn''t easy, and more solutions can be created with
    the same result. Here, in this chapter, we will propose a simple but powerful
    one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we start with the corpora: it''s maybe the hardest thing to find,
    since it should contain a high fidelity translation of many sentences from a language
    to another one. Fortunately, NLTK, a well-known package of Python for NLP, contains
    the corpora Comtrans. **Comtrans** is the acronym of **combination approach to
    machine translation**, and contains an aligned corpora for three languages: German,
    French, and English.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this project, we will use these corpora for a few reasons, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It's easy to download and import in Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No preprocessing is needed to read it from disk / from the internet. NLTK already
    handles that part.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It's small enough to be used on many laptops (a few dozen thousands sentences).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It's freely available on the internet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For more information about the Comtrans project, go to [http://www.fask.uni-mainz.de/user/rapp/comtrans/](http://www.fask.uni-mainz.de/user/rapp/comtrans/).
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, we will try to create a machine translation system to translate
    German to English. We picked these two languages at random among the ones available
    in the Comtrans corpora: feel free to flip them, or use the French corpora instead.
    The pipeline of our project is generic enough to handle any combination.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now investigate how the corpora is organized by typing some commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The pairs of sentences are available using the function `aligned_sents`. The
    filename contains the from and to language. In this case, as for the following
    part of the project, we will translate German (*de*) to English (*en*). The returned
    object is an instance of the class `nltk.translate.api.AlignedSent`. By looking
    at the documentation, the first language is accessible with the attribute `words`,
    while the second language is accessible with the attribute `mots`. So, to extract
    the German sentence and its English translation separately, we should run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How nice! The sentences are already tokenized, and they look as sequences. In
    fact, they will be the input and (hopefully) the output of the RNN which will
    provide the service of machine translation from German to English for our project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, if you want to understand the dynamics of the language, Comtrans
    makes available the alignment of the words in the translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first word in German is translated to the first word in English *(Wiederaufnahme*
    to *Resumption),* the second to the second *(der* to both *of* and *the),* and
    the third (at index 1) is translated with the fourth *(Sitzungsperiode* to *session).*
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing of the corpora
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step is to retrieve the corpora. We've already seen how to do this,
    but let's now formalize it in a function. To make it generic enough, let's enclose
    these functions in a file named `corpora_tools.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do some imports that we will use later on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create the function to retrieve the corpora:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This function has one argument; the file containing the aligned sentences from
    the NLTK Comtrans corpora. It returns two lists of sentences (actually, they're
    a list of tokens), one for the source language (in our case, German), the other
    in the destination language (in our case, English).
  prefs: []
  type: TYPE_NORMAL
- en: 'On a separate Python REPL, we can test this function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code creates the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We also printed the number of sentences in each corpora (33,000) and asserted
    that the number of sentences in the source and the destination languages is the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following step, we want to clean up the tokens. Specifically, we want
    to tokenize punctuation and lowercase the tokens. To do so, we can create a new
    function in `corpora_tools.py`. We will use the `regex` module to perform the
    further splitting tokenization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, in the REPL, let''s test the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code outputs the same sentence as before, but chunked and cleaned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Nice!
  prefs: []
  type: TYPE_NORMAL
- en: The next step for this project is filtering the sentences that are too long
    to be processed. Since our goal is to perform the processing on a local machine,
    we should limit ourselves to sentences up to *N* tokens. In this case, we set
    *N*=20, in order to be able to train the learner within 24 hours. If you have
    a powerful machine, feel free to increase that limit. To make the function generic
    enough, there's also a lower bound with a default value set to 0, such as an empty
    token set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logic of the function is very easy: if the number of tokens for a sentence
    or its translation is greater than *N*, then the sentence (in both languages)
    is removed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, let''s see in the REPL how many sentences survived this filter. Remember,
    we started with more than 33,000:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code prints the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Almost 15,000 sentences survived, that is, half of the corpora.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we finally move from text to numbers (which AI mainly uses). To do so,
    we shall create a dictionary of the words for each language. The dictionary should
    be big enough to contain most of the words, though we can discard some if the
    language has words with low occourrence. This is a common practice even in the
    tf-idf (term frequency within a document, multiplied by the inverse of the document
    frequency, i.e. in how many documents that token appears), where very rare words
    are discarded to speed up the computation, and make the solution more scalable
    and generic. We need here four special symbols in both dictionaries:'
  prefs: []
  type: TYPE_NORMAL
- en: One symbol for padding (we'll see later why we need it)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One symbol for dividing the two sentences
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One symbol to indicate where the sentence stops
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One symbol to indicate unknown words (like the very rare ones)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For doing so, let''s create a new file named `data_utils.py` containing the
    following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, back to the `corpora_tools.py` file, let''s add the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes as arguments the number of entries in the dictionary and
    the path of where to store the dictionary. Remember, the dictionary is created
    while training the algorithms: during the testing phase it''s loaded, and the
    association token/symbol should be the same one as used in the training. If the
    number of unique tokens is greater than the value set, only the most popular ones
    are selected. At the end, the dictionary contains the association between a token
    and its ID for each language.'
  prefs: []
  type: TYPE_NORMAL
- en: After building the dictionary, we should look up the tokens and substitute them
    with their token ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'For that, we need another function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This step is very simple; the token is substituted with its ID. If the token
    is not in the dictionary, the ID of the unknown token is used. Let''s see in the
    REPL how our sentences look after these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This code prints the token and its ID for both the sentences. What''s used
    in the RNN will be just the second element of each tuple, that is, the integer
    ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Please also note how frequent tokens, such as *the* and *of* in English, and
    *der* in German, have a low ID. That's because the IDs are sorted by popularity
    (see the body of the function `create_indexed_dictionary`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though we did the filtering to limit the maximum size of the sentences,
    we should create a function to extract the maximum size. For the lucky owners
    of very powerful machines, which didn''t do any filtering, that''s the moment
    to see how long the longest sentence in the RNN will be. That''s simply the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s apply the following to our sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The final preprocessing step is padding. We need all the sequences to be the
    same length, therefore we should pad the shorter ones. Also, we need to insert
    the correct tokens to instruct the RNN where the string begins and ends.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, this step should:'
  prefs: []
  type: TYPE_NORMAL
- en: Pad the input sequences, for all being 20 symbols long
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pad the output sequence, to be 20 symbols long
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insert an `_GO` at the beginning of the output sequence and an `_EOS` at the
    end to position the start and the end of the translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is done by this function (insert it in the `corpora_tools.py`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To test it, let''s prepare the dataset and print the first sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the input and the output are padded with zeros to have a constant
    length (in the dictionary, they correspond to `_PAD`, see `data_utils.py`), and
    the output contains the markers 1 and 2 just before the start and the end of the
    sentence. As proven effective in the literature, we're going to pad the input
    sentences at the start and the output sentences at the end. After this operation,
    all the input sentences are `20` items long, and the output sentences `22`.
  prefs: []
  type: TYPE_NORMAL
- en: Training the machine translator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've seen the steps to preprocess the corpora, but not the model used.
    The model is actually already available on the TensorFlow Models repository, freely
    downloadable from [https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py](https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py.).
  prefs: []
  type: TYPE_NORMAL
- en: 'The piece of code is licensed with Apache 2.0\. We really thank the authors
    for having open sourced such a great model. Copyright 2015 The TensorFlow Authors.
    All Rights Reserved. Licensed under the Apache License, Version 2.0 (the License);
    You may not use this file except in compliance with the License. You may obtain
    a copy of the License at: [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)
    Unless required by applicable law or agreed to in writing, software. Distributed
    under the License is distributed on an AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS
    OF ANY KIND, either express or implied. See the License for the specific language
    governing permissions and limitations under the License.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will see the usage of the model throughout this section. First, let''s create
    a new file named `train_translator.py` and put in some imports and some constants.
    We will save the dictionary in the `/tmp/` directory, as well as the model and
    its checkpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use all the tools created in the previous section within a function
    that, given a Boolean flag, returns the corpora. More specifically, if the argument
    is `False`, it builds the dictionary from scratch (and saves it); otherwise, it
    uses the dictionary available in the path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This function returns the cleaned sentences, the dataset, the maximum length
    of the sentences, and the lengths of the dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we need to have a function to clean up the model. Every time we run the
    training routine we need to clean up the model directory, as we haven''t provided
    any garbage information. We can do this with a very simple function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s create the model in a reusable fashion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This function calls the constructor of the model, passing the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The source vocabulary size (German, in our example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target vocabulary size (English, in our example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The buckets (in our example is just one, since we padded all the sequences to
    a single size)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **long short-term memory** (**LSTM**) internal units size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of stacked LSTM layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum norm of the gradient (for gradient clipping)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mini-batch size (that is, how many observations for each training step)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate decay factor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The direction of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The type of data (in our example, we will use flat16, that is, float using 2
    bytes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make the training faster and obtain a model with good performance, we have
    already set the values in the code; feel free to change them and see how it performs.
  prefs: []
  type: TYPE_NORMAL
- en: The final if/else in the function retrieves the model, from its checkpoint,
    if the model already exists. In fact, this function will be used in the decoder
    too to retrieve and model on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have reached the function to train the machine translator. Here
    it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The function starts by creating the model. Also, it sets some constants on
    the steps per checkpoints and the maximum number of steps. Specifically, in the
    code, we will save a model every 100 steps and we will perform no more than 20,000
    steps. If it still takes too long, feel free to kill the program: every checkpoint
    contains a trained model, and the decoder will use the most updated one.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we enter the while loop. For each step, we ask the model to get
    a minibatch of data (of size 64, as set previously). The method `get_batch` returns
    the inputs (that is, the source sequence), the outputs (that is, the destination
    sequence), and the weights of the model. With the method `step`, we run one step
    of the training. One piece of information returned is the loss for the current
    minibatch of data. That's all the training!
  prefs: []
  type: TYPE_NORMAL
- en: 'To report the performance and store the model every 100 steps, we print the
    average perplexity of the model (the lower, the better) on the 100 previous steps,
    and we save the checkpoint. The perplexity is a metric connected to the uncertainty
    of the predictions: the more confident we''re about the tokens, the lower will
    be the perplexity of the output sentence. Also, we reset the counters and we extract
    the same metric from a single minibatch of the test set (in this case, it''s a
    random minibatch of the dataset), and performances of it are printed too. Then,
    the training process restarts again.'
  prefs: []
  type: TYPE_NORMAL
- en: As an improvement, every 100 steps we also reduce the learning rate by a factor.
    In this case, we multiply it by 0.99\. This helps the convergence and the stability
    of the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have to connect all the functions together. In order to create a script
    that can be called by the command line but is also used by other scripts to import
    functions, we can create a `main`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In the console, you can now train your machine translator system with a very
    simple command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'On an average laptop, without an NVIDIA GPU, it takes more than a day to reach
    a perplexity below 10 (12+ hours). This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Test and translate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for the translation is in the file `test_translator.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with some imports and the location of the pre-trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's create a function to decode the output sequence generated by the
    RNN. Mind that the sequence is multidimensional, and each dimension corresponds
    to the probability of that word, therefore we will pick the most likely one. With
    the help of the reverse dictionary, we can then figure out what was the actual
    word. Finally, we will trim the markings (padding, start, end of string) and print
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will decode the first five sentences in the training set,
    starting from the raw corpora. Feel free to insert new strings or use different
    corpora:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, again, we need a `main` to work with the command line, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the output is mainly correct, although there are still some
    problematic tokens. To mitigate the problem, we'd need a more complex RNN, a longer
    corpora or a more diverse one.
  prefs: []
  type: TYPE_NORMAL
- en: Home assignments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This model is trained and tested on the same dataset; that''s not ideal for
    data science, but it was needed to have a working project. Try to find a longer
    corpora and split it into two pieces, one for training and one for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the settings of the model: how does that impact the performance and
    the training time?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the code in `seq2seq_model.py`. How can you insert the plot of the loss
    in TensorBoard?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLTK also contains the French corpora; can you create a system to translate
    them both together?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter we''ve seen how to create a machine translation system based
    on an RNN. We''ve seen how to organize the corpus, how to train it and how to
    test it. In the next chapter, we''ll see another application where RNN can be
    used: chatbots.'
  prefs: []
  type: TYPE_NORMAL
