- en: Image Analysis Applications in Self-Driving Cars
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned about object classification and also object
    localization. In this chapter, we will go through multiple case studies that are
    relevant to self-driving cars.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be learning about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Traffic sign identification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the angle within which a car needs to be turned
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying cars on the road using the U-net architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic segmentation of objects on the road
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic sign identification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this case study, we will understand the way in which we can classify a signal
    into one of the 43 possible classes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this exercise, we will adopt the following strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the dataset that contains all possible traffic signs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Perform histogram normalization on top of input images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Certain images are taken in broad day light, while others might be taken in
    twilight
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Different lighting conditions result in a variation in pixel values, depending
    on the lighting condition at which the picture is taken
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Histogram normalization performs normalization on pixel values so that they
    all have a similar distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale the input images
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build, compile, and fit a model to reduce the categorical cross entropy loss
    value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Download the dataset, as follows (the code file is available as `Traffic_signal_detection.ipynb`
    in GitHub). The dataset is available through the paper: J. Stallkamp, M. Schlipsing,
    J. Salmen, C. Igel, Man vs. computer: Benchmarking machine learning algorithms
    for traffic sign recognition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the image paths into a list, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A sample of the images looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12bf31d5-f7a6-4255-afeb-78f1abe0d861.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that certain images have a smaller shape when compared to others and also
    that certain images have more lighting when compared to others. Thus, we'll have
    to preprocess the images so that all images are normalized per exposure to lighting
    as well as shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform histogram normalization on top of the input dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are first converting an image that is in RGB format
    into a **Hue Saturation Value (HSV)** format. By transforming the image from RGB
    to HSV format, we are essentially converting the combined RGB values into an array
    that can then be transformed into an array of single dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Post that, we are normalizing the values obtained in HSV format so that they
    belong to the same scale by using the `equalize_hist` method.
  prefs: []
  type: TYPE_NORMAL
- en: Once the images are normalized in the last channel of the HSV format, we convert
    them back in to RGB format.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we resize the images to a standard size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the image prior to passing it through histogram normalization and contrast
    that with post histogram normalization (post passing the image through the `preprocess_img`
    function), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c4e7a40e-ffcd-451d-9a24-7547c009ff94.png)![](img/548e4913-54ac-4117-814e-c8b5207d0392.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding pictures, we can see that there is a considerable change
    in the visibility of the image (the image on the left) post histogram normalization
    (the image on the right).
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the input and output arrays, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the training and test datasets, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Build and compile the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23d3c55c-1505-4cf1-a1b8-10214e8d9207.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fit the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code, results in a model that has an accuracy of ~99%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe909540-e0d5-466a-bd5c-bfc9597710c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Additionally, if you perform the exact same analysis like we did, but without
    histogram normalization (correcting for exposure), the accuracy of the model is
    ~97%.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the angle within which a car needs to be turned
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this case study, we will understand the angle within which a car needs to
    be turned based on the image provided.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy we adopt to build a steering angle prediction is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Gather a dataset that has the images of the road and the corresponding angle
    within which the steering needs to be turned
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the image
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the image through the VGG16 model to extract features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a neural network that performs regression to predict the steering angle,
    which is a continuous value to be predicted
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Download the following dataset. This dataset is available from the following
    link: [https://github.com/SullyChen/driving-datasets](https://github.com/SullyChen/driving-datasets): (the
    code file is available as `Car_steering_angle_detection.ipynb` in GitHub):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the relevant packages, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the images and their corresponding angles in radians into separate lists,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the train and test datasets, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the output label values in the train and test datasets, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/40a861cd-fd88-473f-a389-2d38d2e18b25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Remove the pixels in the first 100 rows, as they do not correspond to the image
    of a road, and then pass the resulting image through the VGG16 model. Additionally,
    for this exercise, we will work on only the first 10,000 images in the dataset
    so that we are able to build a model faster. Remove the pixels in the first 100
    rows, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Build and compile the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the output layer has linear activation as the output is a continuous
    value that ranges from -9 to +9\. A summary of the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edb25ad9-1735-49e8-8eac-039a1b89c67e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we''ll compile the model we''ve defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/971589a3-0d22-4c0f-8fa9-30f7742401d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Test loss is the line that has the lower loss in the preceding diagram.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have divided the input dataset by 11 so that we can scale it to
    have a values between 0 to 1\. Now, we should be in a position to simulate the
    movement of the car based on the angle that it is predicted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steering angle predictions obtained by the model for a sample of images
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d08b8a4-ce50-464a-9b5d-46afa3198668.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/342b5540-0861-4d72-b58c-908420c84c6f.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that you should be very careful while taking a model like the preceding
    one and implementing it. It should be first tested on multiple daylight conditions
    before finally going to production.
  prefs: []
  type: TYPE_NORMAL
- en: Instance segmentation using the U-net architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in the previous two chapters, we have learned about detecting objects
    and also about identifying the bounding boxes within which the objects within
    an image are located. In this section, we will learn about performing instance
    segmentation, where all the pixels belonging to a certain object are highlighted
    while every other pixel isn't (this is similar to masking all the other pixels
    that do not belong to an object with zeros and masking the pixels that belong
    to the object with pixel values of one).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform instance segmentation, we will perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Work on a dataset that has the input image and the corresponding masked image
    of the pixels where the object is located in the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The image and its masked image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll pass the image through the pre-trained VGG16 model to extract features
    out of each convolution layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll gradually up sample the convolution layers so that we get an output image
    that is of 224 x 224 x 3 in shape
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll freeze the layers where VGG16 weights are used
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Concatenate the up sampled convolution layers with the down sampled convolution
    layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This forms the U-shaped connection
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The U-shaped connection helps in model having the context in a way similar to
    ResNet (previously down sampled layer provides context in addition to the up sampled
    layer)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reconstructing an image is much easier if we take the first layer's output,
    as much of the image is intact in the first layer (earlier layers learn the contours).
    If we try to reconstruct an image from the last few layers by up sampling them,
    there is a good chance that the majority of the information about the image is
    lost
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fit a model that maps the input image to masked image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the masked image is binary in nature—where the black values correspond
    to a pixel value of 0 and the white pixels have a value of 1
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimize the binary cross entropy loss function across all the 224 x 224 x 1
    pixels
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The reason this model is called a **U-net architecture** is because the visualization
    of the model looks as follows—a rotated U-like structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4e97c5a-c390-4983-92f7-c9da19ef8318.png)'
  prefs: []
  type: TYPE_IMG
- en: The U-like structure of the model is due to the early layers connecting to up
    sampled versions of the down sampled layers.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following code, we will perform instance segmentation to detect a car
    within an image:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download and import files from [https://github.com/divamgupta/image-segmentation-keras](https://github.com/divamgupta/image-segmentation-keras),
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the images and their corresponding masks into arrays, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding step, we have created the input and output arrays and also
    normalized the input array. Finally, we separated the mask of a car from everything
    else, as this dataset has 12 unique classes of which cars are masked with a pixel
    value of 8.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample of input and masked images are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c1e655a-3c45-45ce-bfda-8863dbc5dbf8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Furthermore, we create input and output arrays where we scale the input array
    and reshape the output array (so that it can be passed to network), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the model where the image is first passed through the VGG16 model layers
    and the convolution features are extracted, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code, we are importing the pre-trained VGG16 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, the features of various convolution layers when passed
    through the VGG16 model are extracted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are up scaling the features using the `UpSampling` method
    and then concatenating with the down scaled VGG16 convolution features at each
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are defining the input and output to the model, where
    the input is passed to the `base_pretrained_model` first and the output is `conv10` (which
    has the shape of 224 x 224 x 1—the intended shape of our output):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Freeze the convolution layers obtained from the multiplication of the VGG16
    model from training, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile and fit the model for the first 1,000 images in our dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3027213c-790c-4e63-8d37-c02db987f53a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Test the preceding model on a test image (the last 2 images of our dataset—they
    are test images that have `validtion_split = 0.1`), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/35f3e4be-1c52-4657-a157-aaf44df7d756.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the generated mask is realistic for the given input of road
    and also in a way that's better than what we were doing prior as the noisy dots
    are not present in the predicted mask image.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation of objects in an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned about performing segmentation on top of
    an image where the image contained only one object. In this segmentation, we will
    learn about performing segmentation so that we are able to distinguish between
    multiple objects that are present in an image of a road.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy that we''ll adopt to perform semantic segmentation on top of images
    of a road is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gather a dataset that has the annotation of where the multiple objects within
    an image are located:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A sample of the semantic image looks as follows:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9bee9bc6-479d-41cb-a8f7-25a68aa90b73.png)'
  prefs: []
  type: TYPE_IMG
- en: Convert the output mask into a multi dimensional array where there are as many
    columns as the number of all possible unique objects
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If there are 12 possible unique values (12 unique objects), convert the output
    image into  an image that is 224 x 224 x 12 in shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A value of a channel represents that the object corresponding to that channel
    is present in that location of image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Leverage the model architecture that we have seen in previous sections to train
    a model that has 12 possible output values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reshape the prediction into three channels by assigning all three channels
    to have the same output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output is the argmax of prediction of the probabilities of the 12 possible
    classes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Semantic segmentation in code is performed as follows (The code file is available
    as `Semantic_segmentation.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the images and their corresponding labels into separate lists, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function that converts the three channel output images into 12 channels
    where there are 12 unique values of output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract the number of unique values (objects) that are present in the output,
    as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the masked image into a one-hot encoded version with as many channels
    as the number of objects in the total dataset, as follows:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass the images through the pre-trained VGG16 model, as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the VGG16 features of the image, as follows:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass the convolution features through up sampling layers and concatenate them
    to form a U-net architecture in a sim, as follows:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Freeze the VGG16 layers, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile and fit the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cae3a4fb-af64-4933-b45b-f0ace18dc9c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Predict on a test image, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in an image where the predicted and actual semantic
    images are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a872688-dd90-47ec-8d9c-675650960d84.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding images, we can see that we are able to accurately identify
    the semantic structures within an image with a high degree of accuracy (~90% for
    the model we trained).
  prefs: []
  type: TYPE_NORMAL
