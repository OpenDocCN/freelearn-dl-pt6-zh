["```py\n# BUILDING THE BRAIN\nclass Brain(object):\n\n    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD\n\n    def __init__(self, learning_rate = 0.001, number_actions = 5):\n        self.learning_rate = learning_rate\n\n        # BUILDING THE INPUT LAYER COMPOSED OF THE INPUT STATE\n        states = Input(shape = (3,))\n\n        # BUILDING THE FULLY CONNECTED HIDDEN LAYERS\n        x = Dense(units = 64, activation = 'sigmoid')(states)\n        y = Dense(units = 32, activation = 'sigmoid')(x)\n\n        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER\n        q_values = Dense(units = number_actions, activation = 'softmax')(y)\n\n        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT\n        self.model = Model(inputs = states, outputs = q_values)\n\n        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER\n        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate)) \n```", "```py\n# BUILDING THE ENVIRONMENT IN A CLASS\nclass Environment(object):\n\n    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE ENVIRONMENT\n\n    def __init__(self, optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 10, initial_rate_data = 60):\n        self.monthly_atmospheric_temperatures = [1.0, 5.0, 7.0, 10.0, 11.0, 20.0, 23.0, 24.0, 22.0, 10.0, 5.0, 1.0]\n        self.initial_month = initial_month\n        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[initial_month]\n        self.optimal_temperature = optimal_temperature\n        self.min_temperature = -20\n        self.max_temperature = 80\n        self.min_number_users = 10\n        self.max_number_users = 100\n        self.max_update_users = 5\n        self.min_rate_data = 20\n        self.max_rate_data = 300\n        self.max_update_data = 10\n        self.initial_number_users = initial_number_users\n        self.current_number_users = initial_number_users\n        self.initial_rate_data = initial_rate_data\n        self.current_rate_data = initial_rate_data\n        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n        self.temperature_ai = self.intrinsic_temperature\n        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n        self.total_energy_ai = 0.0\n        self.total_energy_noai = 0.0\n        self.reward = 0.0\n        self.game_over = 0\n        self.train = 1 \n```", "```py\n # MAKING A METHOD THAT UPDATES THE ENVIRONMENT RIGHT AFTER THE AI PLAYS AN ACTION\n\n    def update_env(self, direction, energy_ai, month):\n\n        # GETTING THE REWARD\n\n        # Computing the energy spent by the server's cooling system when there is no AI\n        energy_noai = 0\n        if (self.temperature_noai < self.optimal_temperature[0]):\n            energy_noai = self.optimal_temperature[0] - self.temperature_noai\n            self.temperature_noai = self.optimal_temperature[0]\n        elif (self.temperature_noai > self.optimal_temperature[1]):\n            energy_noai = self.temperature_noai - self.optimal_temperature[1]\n            self.temperature_noai = self.optimal_temperature[1]\n        # Computing the Reward\n        self.reward = energy_noai - energy_ai\n        # Scaling the Reward\n        self.reward = 1e-3 * self.reward \n```", "```py\n # GETTING THE NEXT STATE\n\n        # Updating the atmospheric temperature\n        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[month]\n        # Updating the number of users\n        self.current_number_users += np.random.randint(-self.max_update_users, self.max_update_users)\n        if (self.current_number_users > self.max_number_users):\n            self.current_number_users = self.max_number_users\n        elif (self.current_number_users < self.min_number_users):\n            self.current_number_users = self.min_number_users\n        # Updating the rate of data\n        self.current_rate_data += np.random.randint(-self.max_update_data, self.max_update_data)\n        if (self.current_rate_data > self.max_rate_data):\n            self.current_rate_data = self.max_rate_data\n        elif (self.current_rate_data < self.min_rate_data):\n            self.current_rate_data = self.min_rate_data\n        # Computing the Delta of Intrinsic Temperature\n        past_intrinsic_temperature = self.intrinsic_temperature\n        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n        delta_intrinsic_temperature = self.intrinsic_temperature - past_intrinsic_temperature\n        # Computing the Delta of Temperature caused by the AI\n        if (direction == -1):\n            delta_temperature_ai = -energy_ai\n        elif (direction == 1):\n            delta_temperature_ai = energy_ai\n        # Updating the new Server's Temperature when there is the AI\n        self.temperature_ai += delta_intrinsic_temperature + delta_temperature_ai\n        # Updating the new Server's Temperature when there is no AI\n        self.temperature_noai += delta_intrinsic_temperature \n```", "```py\n # GETTING GAME OVER\n\n        if (self.temperature_ai < self.min_temperature):\n            if (self.train == 1):\n                self.game_over = 1\n            else:\n                self.total_energy_ai += self.optimal_temperature[0] - self.temperature_ai\n                self.temperature_ai = self.optimal_temperature[0]\n        elif (self.temperature_ai > self.max_temperature):\n            if (self.train == 1):\n                self.game_over = 1\n            else:\n                self.total_energy_ai += self.temperature_ai - self.optimal_temperature[1]\n                self.temperature_ai = self.optimal_temperature[1] \n```", "```py\n # UPDATING THE SCORES\n\n        # Updating the Total Energy spent by the AI\n        self.total_energy_ai += energy_ai\n        # Updating the Total Energy spent by the server's cooling system when there is no AI\n        self.total_energy_noai += energy_noai \n```", "```py\n # SCALING THE NEXT STATE\n\n        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (self.max_temperature - self.min_temperature)\n        scaled_number_users = (self.current_number_users - self.min_number_users) / (self.max_number_users - self.min_number_users)\n        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (self.max_rate_data - self.min_rate_data)\n        next_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data]) \n```", "```py\n # RETURNING THE NEXT STATE, THE REWARD, AND GAME OVER\n\n        return next_state, self.reward, self.game_over \n```", "```py\n # MAKING A METHOD THAT RESETS THE ENVIRONMENT\n\n    def reset(self, new_month):\n        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[new_month]\n        self.initial_month = new_month\n        self.current_number_users = self.initial_number_users\n        self.current_rate_data = self.initial_rate_data\n        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n        self.temperature_ai = self.intrinsic_temperature\n        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n        self.total_energy_ai = 0.0\n        self.total_energy_noai = 0.0\n        self.reward = 0.0\n        self.game_over = 0\n        self.train = 1 \n```", "```py\n # MAKING A METHOD THAT GIVES US AT ANY TIME THE CURRENT STATE, THE LAST REWARD AND WHETHER THE GAME IS OVER\n\n    def observe(self):\n        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (self.max_temperature - self.min_temperature)\n        scaled_number_users = (self.current_number_users - self.min_number_users) / (self.max_number_users - self.min_number_users)\n        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (self.max_rate_data - self.min_rate_data)\n        current_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n        return current_state, self.reward, self.game_over \n```", "```py\n# AI for Business - Minimize cost with Deep Q-Learning   #1\n# Building the Brain without Dropout   #2\n#3\n# Importing the libraries   #4\nfrom keras.layers import Input, Dense   #5\nfrom keras.models import Model   #6\nfrom keras.optimizers import Adam   #7\n   #8\n# BUILDING THE BRAIN   #9\n   #10\nclass Brain(object):   #11\n    #12\n    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD   #13\n    #14\n    def __init__(self, learning_rate = 0.001, number_actions = 5):   #15\n        self.learning_rate = learning_rate   #16\n        #17\n        # BUILDING THE INPUT LAYER COMPOSED OF THE INPUT STATE   #18\n        states = Input(shape = (3,))   #19\n        #20\n        # BUILDING THE FULLY CONNECTED HIDDEN LAYERS   #21\n        x = Dense(units = 64, activation = 'sigmoid')(states)   #22\n        y = Dense(units = 32, activation = 'sigmoid')(x)   #23\n        #24\n        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER   #25\n        q_values = Dense(units = number_actions, activation = 'softmax')(y)   #26\n        #27\n        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT   #28\n        self.model = Model(inputs = states, outputs = q_values)   #29\n        #30\n        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER   #31\n        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))   #32 \n```", "```py\n# AI for Business - Minimize cost with Deep Q-Learning\n# Building the Brain with Dropout\n# Importing the libraries\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n# BUILDING THE BRAIN\nclass Brain(object):\n\n    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD\n\n    def __init__(self, learning_rate = 0.001, number_actions = 5):\n        self.learning_rate = learning_rate\n\n        # BUILDING THE INPUT LAYER COMPOSED OF THE INPUT STATE\n        states = Input(shape = (3,))\n\n        # BUILDING THE FIRST FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\n        x = Dense(units = 64, activation = 'sigmoid')(states)\n        x = Dropout(rate = 0.1)(x)\n\n        # BUILDING THE SECOND FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\n        y = Dense(units = 32, activation = 'sigmoid')(x)\n        y = Dropout(rate = 0.1)(y)\n\n        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER\n        q_values = Dense(units = number_actions, activation = 'softmax')(y)\n\n        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT\n        self.model = Model(inputs = states, outputs = q_values)\n\n        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER\n        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate)) \n```", "```py\n# AI for Business - Minimize cost with Deep Q-Learning   #1\n# Implementing Deep Q-Learning with Experience Replay   #2\n#3\n# Importing the libraries   #4\nimport numpy as np   #5\n#6\n# IMPLEMENTING DEEP Q-LEARNING WITH EXPERIENCE REPLAY   #7\n#8\nclass DQN(object):   #9\n    #10\n    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE DQN   #11\n    def __init__(self, max_memory = 100, discount = 0.9):   #12\n        self.memory = list()   #13\n        self.max_memory = max_memory   #14\n        self.discount = discount   #15\n#16\n    # MAKING A METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY   #17\n    def remember(self, transition, game_over):   #18\n        self.memory.append([transition, game_over])   #19\n        if len(self.memory) > self.max_memory:   #20\n            del self.memory[0]   #21\n#22\n    # MAKING A METHOD THAT BUILDS TWO BATCHES OF INPUTS AND TARGETS BY EXTRACTING TRANSITIONS FROM THE MEMORY   #23\n    def get_batch(self, model, batch_size = 10):   #24\n        len_memory = len(self.memory)   #25\n        num_inputs = self.memory[0][0][0].shape[1]   #26\n        num_outputs = model.output_shape[-1]   #27\n        inputs = np.zeros((min(len_memory, batch_size), num_inputs))   #28\n        targets = np.zeros((min(len_memory, batch_size), num_outputs))   #29\n        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):   #30\n            current_state, action, reward, next_state = self.memory[idx][0]   #31\n            game_over = self.memory[idx][1]   #32\n            inputs[i] = current_state   #33\n            targets[i] = model.predict(current_state)[0]   #34\n            Q_sa = np.max(model.predict(next_state)[0])   #35\n            if game_over:   #36\n                targets[i, action] = reward   #37\n            else:   #38\n                targets[i, action] = reward + self.discount * Q_sa   #39\n        return inputs, targets   #40 \n```", "```py\n# AI for Business - Minimize cost with Deep Q-Learning   #1\n# Training the AI without Early Stopping   #2\n#3\n# Importing the libraries and the other python files   #4\nimport os   #5\nimport numpy as np   #6\nimport random as rn   #7\nimport environment   #8\nimport brain_nodropout   #9\nimport dqn   #10 \n```", "```py\n# Setting seeds for reproducibility   #12\nos.environ['PYTHONHASHSEED'] = '0'   #13\nnp.random.seed(42)   #14\nrn.seed(12345)   #15\n#16\n# SETTING THE PARAMETERS   #17\nepsilon = .3   #18\nnumber_actions = 5   #19\ndirection_boundary = (number_actions - 1) / 2   #20\nnumber_epochs = 100   #21\nmax_memory = 3000   #22\nbatch_size = 512   #23\ntemperature_step = 1.5   #24\n#25\n# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS   #26\nenv = environment.Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)   #27\n#28\n# BUILDING THE BRAIN BY SIMPLY CREATING AN OBJECT OF THE BRAIN CLASS   #29\nbrain = brain_nodropout.Brain(learning_rate = 0.00001, number_actions = number_actions)   #30\n#31\n# BUILDING THE DQN MODEL BY SIMPLY CREATING AN OBJECT OF THE DQN CLASS   #32\ndqn = dqn.DQN(max_memory = max_memory, discount = 0.9)   #33\n#34\n# CHOOSING THE MODE   #35\ntrain = True   #36 \n```", "```py\noptimal_temperature = (18.0, 24.0),\ninitial_month = 0,\ninitial_number_users = 20,\ninitial_rate_data = 30 \n```", "```py\nlearning_rate = 0.00001,\nnumber_actions = number_actions \n```", "```py\nmax_memory = max_memory,\ndiscount = 0.9 \n```", "```py\n# TRAINING THE AI   #38\nenv.train = train   #39\nmodel = brain.model   #40\nif (env.train):   #41\n    # STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)   #42\n    for epoch in range(1, number_epochs):   #43\n        # INITIALIAZING ALL THE VARIABLES OF BOTH THE ENVIRONMENT AND THE TRAINING LOOP   #44\n        total_reward = 0   #45\n        loss = 0\\.   #46\n        new_month = np.random.randint(0, 12)   #47\n        env.reset(new_month = new_month)   #48\n        game_over = False   #49\n        current_state, _, _ = env.observe()   #50\n        timestep = 0   #51\n        # STARTING THE LOOP OVER ALL THE TIMESTEPS (1 Timestep = 1 Minute) IN ONE EPOCH   #52\n        while ((not game_over) and timestep <= 5 * 30 * 24 * 60):   #53\n            # PLAYING THE NEXT ACTION BY EXPLORATION   #54\n            if np.random.rand() <= epsilon:   #55\n                action = np.random.randint(0, number_actions)   #56\n                if (action - direction_boundary < 0):   #57\n                    direction = -1   #58\n                else:   #59\n                    direction = 1   #60\n                energy_ai = abs(action - direction_boundary) * temperature_step   #61\n            # PLAYING THE NEXT ACTION BY INFERENCE   #62\n            else:   #63\n                q_values = model.predict(current_state)   #64\n                action = np.argmax(q_values[0])   #65\n                if (action - direction_boundary < 0):   #66\n                    direction = -1   #67\n                else:   #68\n                    direction = 1   #69\n                energy_ai = abs(action - direction_boundary) * temperature_step   #70\n            # UPDATING THE ENVIRONMENT AND REACHING THE NEXT STATE   #71\n            next_state, reward, game_over = env.update_env(direction, energy_ai, ( new_month + int(timestep/(30*24*60)) ) % 12)   #72\n            total_reward += reward   #73\n            # STORING THIS NEW TRANSITION INTO THE MEMORY   #74\n            dqn.remember([current_state, action, reward, next_state], game_over)   #75\n            # GATHERING IN TWO SEPARATE BATCHES THE INPUTS AND THE TARGETS   #76\n            inputs, targets = dqn.get_batch(model, batch_size = batch_size)   #77\n            # COMPUTING THE LOSS OVER THE TWO WHOLE BATCHES OF INPUTS AND TARGETS   #78\n            loss += model.train_on_batch(inputs, targets)   #79\n            timestep += 1   #80\n            current_state = next_state   #81\n        # PRINTING THE TRAINING RESULTS FOR EACH EPOCH   #82\n        print(\"\\n\")   #83\n        print(\"Epoch: {:03d}/{:03d}\".format(epoch, number_epochs))   #84\n        print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))   #85\n        print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))   #86\n        # SAVING THE MODEL   #87\n        model.save(\"model.h5\")   #88 \n```", "```py\n# TRAINING THE AI   #38\nenv.train = train   #39\nmodel = brain.model   #40\nearly_stopping = True   #41\npatience = 10   #42\nbest_total_reward = -np.inf   #43\npatience_count = 0   #44\nif (env.train):   #45\n    # STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)   #46\n    for epoch in range(1, number_epochs):   #47 \n```", "```py\n # EARLY STOPPING   #91\n        if (early_stopping):   #32\n            if (total_reward <= best_total_reward):   #93\n                patience_count += 1   #94\n            elif (total_reward > best_total_reward):   #95\n                best_total_reward = total_reward   #96\n                patience_count = 0   #97\n            if (patience_count >= patience):   #98\n                print(\"Early Stopping\")   #99\n                break   #100\n        # SAVING THE MODEL   #101\n        model.save(\"model.h5\")   #102 \n```", "```py\n# AI for Business - Minimize cost with Deep Q-Learning\n# Testing the AI\n\n# Installing Keras\n# conda install -c conda-forge keras\n\n# Importing the libraries and the other python files\nimport os\nimport numpy as np\nimport random as rn\nfrom keras.models import load_model\nimport environment\n\n# Setting seeds for reproducibility\nos.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(42)\nrn.seed(12345)\n\n# SETTING THE PARAMETERS\nnumber_actions = 5\ndirection_boundary = (number_actions - 1) / 2\ntemperature_step = 1.5\n\n# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\nenv = environment.Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n\n# LOADING A PRE-TRAINED BRAIN\nmodel = load_model(\"model.h5\")\n\n# CHOOSING THE MODE\ntrain = False\n\n# RUNNING A 1 YEAR SIMULATION IN INFERENCE MODE\nenv.train = train\ncurrent_state, _, _ = env.observe()\nfor timestep in range(0, 12 * 30 * 24 * 60):\n    q_values = model.predict(current_state)\n    action = np.argmax(q_values[0])\n    if (action - direction_boundary < 0):\n        direction = -1\n    else:\n        direction = 1\n    energy_ai = abs(action - direction_boundary) * temperature_step\n    next_state, reward, game_over = env.update_env(direction, energy_ai, int(timestep / (30 * 24 * 60)))\n    current_state = next_state\n\n# PRINTING THE TRAINING RESULTS FOR EACH EPOCH\nprint(\"\\n\")\nprint(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\nprint(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\nprint(\"ENERGY SAVED: {:.0f} %\".format((env.total_energy_noai - env.total_energy_ai) / env.total_energy_noai * 100)) \n```", "```py\nUsing TensorFlow backend. \n```", "```py\nTotal Energy spent with an AI: 261985\nTotal Energy spent with no AI: 1978293\nENERGY SAVED: 87% \n```"]