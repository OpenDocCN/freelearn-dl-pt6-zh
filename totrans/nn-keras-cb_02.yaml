- en: Building a Deep Feedforward Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Training a vanilla neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling the input dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impact of training when the majority of inputs are greater than zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impact of batch size on model accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a deep neural network to improve network accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varying the learning rate to improve network accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varying the loss optimizer to improve network accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the scenario of overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speeding up the training process using batch normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at the basics of the function of a neural
    network. We also learned that there are various hyperparameters that impact the
    accuracy of a neural network. In this chapter, we will get into the details of
    the functions of the various hyperparameters within a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: All the codes for this chapter are available at https://github.com/kishore-ayyadevara/Neural-Networks-with-Keras-Cookbook/blob/master/Neural_network_hyper_parameters.ipynb
  prefs: []
  type: TYPE_NORMAL
- en: Training a vanilla neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how to train a vanilla neural network, we will go through the
    task of predicting the label of a digit in the MNIST dataset, which is a popular
    dataset of images of digits (one digit per image) and the corresponding label
    of the digit that is contained in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training a neural network is done in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the relevant packages and datasets
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Preprocess the targets (convert them into one-hot encoded vectors) so that
    we can perform optimization on top of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We shall be minimizing categorical cross entropy loss
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create train and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have the train dataset so that we create a model based on it
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The test dataset is not seen by the model:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, the accuracy on the test dataset is an indicator of how well the model
    is likely to work on data when the model is productionalized, as data in the production
    scenario (which might occur a few days/weeks after building the model) cannot
    be seen by the model
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize a model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the model architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the number of units in a hidden layer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the activation function that is to be performed in a hidden layer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the number of hidden layers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the loss function that we want to minimize
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide the optimizer that will minimize the loss function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mention the batch size to update weights
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mention the total number of epochs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Test the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mention the validation data, otherwise, mention the validation split, which
    will consider the last x% of total data as test data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the accuracy and loss values on top of the test dataset
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Check for anything interesting in the way in which loss value and accuracy values
    changed over an increasing number of epochs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using this strategy, let's go ahead and build a neural network model in Keras,
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import the relevant packages and dataset, and visualize the input dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are importing the relevant Keras files and are also
    importing the MNIST dataset (which is provided as a built-in dataset in Keras).
  prefs: []
  type: TYPE_NORMAL
- en: 'The MNIST dataset contains images of digits where the images are of 28 x 28
    in shape. Let''s plot a few images to see what they will look like in the code
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output of the previous code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97a5b714-f5d0-493b-a367-5565bf21d112.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Flatten the 28 x 28 images so that the input is all the 784 pixel values. Additionally,
    one-hot encode the outputs. This step is key in the dataset preparation process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding step, we are reshaping the input dataset using the reshape
    method that converts an array of a given shape into a different shape. In this
    specific case, we are converting an array that has an `X_train.shape[0]` number
    of data points (images) where there are `X_train.shape[1]` rows and `X_train.shape[2]`
    columns in each image, into an array of an `X_train.shape[0]` number of data points
    (images) and `X_train.shape[1] * X_train.shape[2]` values per image. Similarly,
    we perform the same exercise on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try to understand how one-hot encoding works. If the unique possible
    labels are *{0, 1, 2, 3}*, they will be one-hot encoded, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Label** | **0** | **1** | **2** | **3** |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Essentially, each label will occupy a unique column in the dataset, and if the
    label is present, the column value will be one, and every other column value will
    be zero.
  prefs: []
  type: TYPE_NORMAL
- en: In Keras, the one-hot encoding approach on top of labels is performed using
    the `to_categorical` method, which figures out the number of unique labels in
    the target data, and then converts them into a one-hot encoded vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a neural network with a hidden layer with 1,000 units:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding step, we mention that the input has 784 values that are connected
    to 1,000 values in a hidden layer. Additionally, we are also specifying that the
    activation, which is to be performed in the hidden layer after the matrix multiplication
    of the input and the weights connecting the input and hidden layer, is the ReLu
    activation.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the hidden layer is connected to an output that has 10 values (as there
    are 10 columns in the vector created by the `to_categorical` method), and we perform
    softmax on top of the output so that we obtain the probability of an image belonging
    to a certain class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding model architecture can be visualized as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02ffb258-590f-499a-a9b7-6d7f2bd3b7ee.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding architecture, the number of parameters in the first layer is
    785,000, as the 784 input units are connected to 1,000 hidden units, resulting
    in 784 * 1,000 weight values, and 1,000 bias values, for the 1,000 hidden units,
    resulting in a total of 785,000 parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the output layer has 10 outputs, which are connected to each of the
    1,000 hidden layers, resulting in 1,000 * 10 parameters and 10 biases—a total
    of 10,010 parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The output layer has 10 units as there are 10 possible labels in the output.
    The output layer now gives us a probability value for each class for a given input
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that because the target variable is a one-hot encoded vector with multiple
    classes in it, the loss function will be a categorical cross-entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we are using the Adam optimizer to minimize the cost function
    (more on different optimizers in the *Varying the loss optimizer to improve network
    accuracy* recipe).
  prefs: []
  type: TYPE_NORMAL
- en: We are also noting that we will need to look at the accuracy metric while the
    model is getting trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have specified the input (`X_train`) and the output
    (`y_train`) that the model will fit. Additionally, we also specify the input and
    output of the test dataset, which the model will not use to train weights; however
    it, will give us an idea of how different the loss value and accuracy values are
    between the training and the test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the training and test loss and accuracy metrics over different epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: While fitting a model, the history variable will have stored the accuracy and
    loss values corresponding to the model in each epoch for both the training and
    the test datasets. In the preceding steps, we are storing those values in a list
    so that we can plot the variation of accuracy and loss in both the training and
    test datasets over an increasing number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualize the training and test loss and the accuracy over a different number
    of epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following diagram, where the first plot shows
    the training and test loss values over increasing epochs, and the second plot
    shows the training and test accuracy over increasing epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d56bf42-6d83-4844-a9b7-06ea6ad79d71.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the previous network resulted in an accuracy of 97%. Also, note that
    loss values (and thereby, accuracy) have a step change over a different number
    of epochs. We will contrast this change in loss with the scenario when the input
    dataset is scaled in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s calculate the accuracy of the model manually:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding step, we are using the `predict` method to calculate the expected
    output values for a given input (`X_test` in this case) to the model. Note that
    we are specifying it as `model.predict`, as we have initialized a sequential model
    named `model`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are looping all of the test predictions one at a time.
    For each test prediction, we are perming `argmax` to obtain the index that has
    the highest probability value.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we perform the same exercise for the actual values of the test dataset.
    The prediction of the index of the highest value is the same in both the prediction
    and the actual values of the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the number of correct predictions over the total number of data points
    in the test dataset is the accuracy of the model on the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key steps that we have performed in the preceding code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We flattened the input dataset so that each pixel is considered a variable using
    the `reshape` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We performed one-hot encoding on the output values so that we can distinguish
    between different labels using the `to_categorical` method in the `np_utils` package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We built a neural network with a hidden layer using the sequential addition
    of layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compiled the neural network to minimize the categorical cross entropy loss
    (as the output has 10 different categories) using the `model.compile` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We fitted the model with training data using the `model.fit` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We extracted the training and test loss accuracies across all the epochs that
    were stored in the history
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We predicted the probability of each class in the test dataset using the `model.predict`
    method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We looped through all the images in the test dataset and identified the class
    that has the highest probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we calculated the accuracy (the number of instances in which a predicted
    class matches the actual class of the image out of the total number of instances)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will look at the reasons for the step change in the
    loss and accuracy values, and move toward making the change more smooth.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the input dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling a dataset is a process where we limit the variables within a dataset
    to ensure they do not have a very wide range of different values. One way to achieve
    this is to divide each variable in the dataset by the maximum value of the variable.
    Typically, neural networks perform well when we scale the input datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, let's understand the reason neural networks perform better
    when the dataset is scaled.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the impact of the scaling input on the output, let's contrast
    the scenario where we check the output when the input dataset is not scaled, with
    the output when the input dataset is scaled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input data is not scaled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c549749-a8f8-45e8-8f9c-1ab0f2077506.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding table, note that the output (sigmoid) did not vary a lot,
    even though the weight value varied from 0.01 to 0.9\. The sigmoid function is
    calculated as the sigmoid value of the multiplication of the input with the weight,
    and then adding a bias to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Where `w` is the weight, `x` is the input, and `b` is the bias value.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for no change in the sigmoid output is due to the fact that the multiplication
    of  `w*x` is a large number (as x is a large number) resulting in the sigmoid
    value always falling in the saturated portion of the sigmoid curve (saturated
    value on the top-right or bottom-left of the sigmoid curve).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this scenario, let''s multiply different weight values by a small input
    number, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7de39d87-5f55-4a9c-ad30-771958d1d0ce.png)'
  prefs: []
  type: TYPE_IMG
- en: The sigmoid output in the preceding table varies, as the input and weight values
    are small, resulting in a smaller value when the input and the weight are multiplied,
    further resulting in the sigmoid value having variation in output.
  prefs: []
  type: TYPE_NORMAL
- en: From this exercise, we learned about the importance of scaling the input dataset
    so that it results in a smaller value when the weights (provided the weights do
    not have a high range) are multiplied by the input values. This phenomenon results
    in the weight value not getting updated quickly enough.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to achieve the optimal weight value, we should scale our input dataset
    while initializing the weights to not have a huge range (typically, weights have
    a random value between -1 and +1 during initialization).
  prefs: []
  type: TYPE_NORMAL
- en: These issues hold true when the weight value is also a very big number. Hence,
    we are better off initializing the weight values as a small value that is closer
    to zero.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go through the set up of scaling the dataset that we have used in the
    previous section, and compare the results with and without scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages and datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: There are multiple ways to scale a dataset. One way is to convert all the data
    points to a value between zero and one (by dividing each data point with the maximum
    value in the total dataset, which is what we are doing in the following code).
    Another popular method, among the multiple other ways, is to normalize the dataset
    so that the values are between -1 and +1 by subtracting each data point with the
    overall dataset mean, and then dividing each resulting data point by the standard
    deviation of values in the original dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now we will be flattening the input dataset and scaling it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding step, we have scaled the training and test inputs to a value
    between zero and one by dividing each value by the maximum possible value in the
    dataset, which is 255\. Additionally, we convert the output dataset into a one-hot
    encoded format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the model and compile it using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that the preceding model is exactly the same as the one we built in the
    previous section. However, the only difference is that it will be executed on
    the training dataset that is scaled, whereas the previous one was not scaled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: You will notice that the accuracy of the preceding model is ~98.25%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the training and test accuracy and the loss values over different epochs
    (the code to generate the following plots remains the same as the one we used
    in step 8 of the *Training a vanilla neural network* recipe):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4affc74a-14b2-4085-9407-5da31fb07f4b.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding diagram, you should notice that training and test losses
    decreased smoothly over increasing epochs when compared to the non-scaled dataset
    that we saw in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: While the preceding network gave us good results in terms of a smoothly decreasing
    loss value, we noticed that there is a gap between the training and test accuracy/loss
    values, indicating that there is potential overfitting on top of the training
    dataset. **Overfitting** is the phenomenon where the model specializes on the
    training data that it might not work as well on the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key steps that we have performed in the preceding code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We flattened the input dataset so that each pixel is considered a variable using
    the reshape method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, we scaled the dataset so that each variable now has a value between
    zero and one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We achieved the preceding by dividing the values of a variable with the maximum
    value of that variable
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We performed one-hot encoding on the output values so that we can distinguish
    between different labels using the `to_categorical` method in the `np_utils` package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We built a neural network with a hidden layer using the sequential addition
    of layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compiled the neural network to minimize categorical cross entropy loss (as
    the output has 10 different categories) using the `model.compile` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We fitted the model with training data using the `model.fit` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We extracted the training and test losses accuracies across all the epochs that
    were stored in the history
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also identified a scenario that we consider overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to scaling a variable''s values by dividing the values by the maximum
    among the values in a variable, the other commonly used scaling methods are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Min-max normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information about these scaling methods can be found on Wikipedia here: [https://en.wikipedia.org/wiki/Feature_scaling](https://en.wikipedia.org/wiki/Feature_scaling).
  prefs: []
  type: TYPE_NORMAL
- en: Impact on training when the majority of inputs are greater than zero
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in the dataset that we have considered, we have not looked at the distribution
    of values in the input dataset. Certain values of the input result in faster training.
    In this section, we will understand a scenario where weights are trained faster
    when the training time depends on the input values.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will follow the model-building process in exactly the same
    way as we did in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we will adopt a small change to our strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: We will invert the background color, and also the foreground color. Essentially,
    the background will be colored white in this scenario, and the label will be written
    in black.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intuition for this change impacting the model accuracy is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: The pixels in the corner of images do not contribute toward predicting the label
    of an image. Given that a black pixel (original scenario) has a pixel value of
    zero, it is automatically taken care of, as when this input is multiplied by any
    weight value, the output is zero. This will result in the network learning that
    any change in the weight value connecting this corner pixel to a hidden layer
    will not have an impact on changing the loss value.
  prefs: []
  type: TYPE_NORMAL
- en: However, if we have a white pixel in the corner (where we already know that
    the corner pixels do not contribute toward predicting the label of an image),
    it will contribute toward certain hidden unit values, and thus the weights need
    to be fine-tuned until the impact of the corner pixels on the predicted label
    is minimal.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load and scale the input dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the distribution of the input values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code flattens all the inputs into a single list, and hence, is
    of the shape (47,040,000), which is the same as the `28 x 28 x X_train.shape[0]`.
    Let''s plot the distribution of all the input values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/aeb37d82-2cee-490f-b933-4cfe855c3975.png)'
  prefs: []
  type: TYPE_IMG
- en: We notice that the majority of the inputs are zero (you should note that all
    the input images have a background that is black hence, a majority of the values
    are zero, which is the pixel value of the color black).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, let''s explore a scenario where we invert the colors, in which
    the background is white and the letters are written in black, using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'They will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41d3b675-5acf-4a2a-830c-d911857cb189.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The histogram of the resulting images now looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8f2c774-2d0e-47dd-9324-7e199534c934.png)'
  prefs: []
  type: TYPE_IMG
- en: You should notice that the majority of the input values now have a value of
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and build our model using the same model architecture that
    we built in the S*caling input dataset* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the training and test accuracy and loss values over different epochs (the
    code to generate the following plots remains the same as the one we used in step
    8 of the *Training a vanilla neural network* recipe):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bf81c069-33e3-44f5-905f-76c8bcb34578.png)'
  prefs: []
  type: TYPE_IMG
- en: We should note that model accuracy has now fallen to ~97%, compared to ~98%
    when using the same model for the same number of epochs and batch size, but on
    a dataset that has a majority of zeros (and not a majority of ones). Additionally,
    the model achieved an accuracy of 97%, considerably more slowly than in the scenario
    where the majority of the input pixels are zero.
  prefs: []
  type: TYPE_NORMAL
- en: The intuition for the decrease in accuracy, when the majority of the data points
    are non-zero is that, when the majority of pixels are zero, the model's task was
    easier (less weights had to be fine-tuned), as it had to make predictions based
    on a few pixel values (the minority that had a pixel value greater than zero).
    However, a higher number of weights need to be fine-tuned to make predictions
    when a majority of the data points are non-zero.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of batch size on model accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, for all the models that we have built, we considered
    a batch size of 32\. In this section, we will try to understand the impact of
    varying the batch size on accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand the reason batch size has an impact on model accuracy, let''s
    contrast two scenarios where the total dataset size is 60,000:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch size is 30,000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch size is 32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the batch size is large, the number of times of weight update per epoch
    is small, when compared to the scenario when the batch size is small.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for a high number of weight updates per epoch when the batch size
    is small is that less data points are considered to calculate the loss value.
    This results in more batches per epoch, as, loosely, in an epoch, you would have
    to go through all the training data points in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the lower the batch size, the better the accuracy for the same number
    of epochs. However, while deciding the number of data points to be considered
    for a batch size, you should also ensure that the batch size is not too small
    so that it might overfit on top of a small batch of data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous recipe, we built a model with a batch size of 32\. In this
    recipe, we will go ahead and implement the model to contrast the scenario between
    a low batch size and a high batch size for the same number of epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Preprocess the dataset and fit the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note that the only change in code is the `batch_size` parameter in the model
    fit process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the training and test accuracy and loss values over different epochs (the
    code to generate the following plots remains the same as the code we used in step
    8 of the *Training a vanilla neural network* recipe):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8045caed-04a4-41b0-a58d-5aaafba99a70.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding scenario, you should notice that the model accuracy reached
    ~98% at a much later epoch, when compared to the model accuracy it reached when
    the batch size was smaller.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should notice that the accuracy is much lower initially and that it catches
    up only after a considerable number of epochs are run. The reason for a low accuracy
    during initial epochs is that the number of times of weight update is much lower
    in this scenario when compared to the previous scenario (where the batch size
    was smaller).
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, when the batch size is 30,000, and the total dataset size
    is 60,000, when we run the model for 500 epochs, the weight updates happens at
    epochs * (dataset size/ batch size) = 500 * (60,000/30,000) = 1,000 times.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous scenario, the weight updates happens at 500 * (60,000/32) =
    937,500 times.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the lower the batch size, the more times the weights get updated and,
    generally, the better the accuracy is for the same number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, you should be careful not to have too few examples in the
    batch size, which might result in not only having a very long training time, but
    also a potential overfitting scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Building a deep neural network to improve network accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we have looked at model architectures where the neural network has
    only one hidden layer between the input and the output layers. In this section,
    we will look at the neural network where there are multiple hidden layers (and
    hence a deep neural network), while reusing the same MNIST training and test dataset
    that were scaled.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A deep neural network means that there are multiple hidden layers connecting
    the input to the output layer. Multiple hidden layers ensure that the neural network
    learns a complex non-linear relation between the input and output, which a simple
    neural network cannot learn (due to a limited number of hidden layers).
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical deep feedforward neural network looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b116810-f567-4e06-9f94-efdbe6f013e0.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A deep neural network architecture is built by adding multiple hidden layers
    between input and output layers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the dataset and scale it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a model with multiple hidden layers connecting the input and output layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding model architecture results in a model summary, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f36020ac-67c3-494d-a646-f863b71527dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the preceding model results in a higher number of parameters, as a
    result of deep architectures (as there are multiple hidden layers in the model).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the model is set up, let''s compile and fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding results in a model with an accuracy of 98.6%, which is slightly
    better than the accuracies we observed with the model architectures that we saw
    earlier. The training and test loss and accuracy are as follows (the code to generate
    the plots in the following diagram remains the same as the code we used in step
    8 of the *Training a vanilla neural network* recipe):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f11ab003-caee-4343-8b8b-eadce1391e26.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in this scenario, there is a considerable gap between training and
    test loss, indicating that the deep feedforward neural network specialized on
    training data. Again, in the sections on overfitting, we will learn about ways
    to avoid overfitting on training data.
  prefs: []
  type: TYPE_NORMAL
- en: Varying the learning rate to improve network accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in the previous recipes, we used the default learning rate of the Adam
    optimizer, which is 0.0001.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will manually set the learning rate to a higher number and
    see the impact of changing the learning rate on model accuracy, while reusing
    the same MNIST training and test dataset that were scaled in the previous recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter on building feedforward neural networks, we learned
    that the learning rate is used in updating weights and the change in weight is
    proportional to the amount of loss reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, a change in a weight's value is equal to the decrease in loss
    multiplied by the learning rate. Hence, the lower the learning rate, the lower
    the change in the weight value, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: You can essentially think of the weight values as a continuous spectrum where
    the weights are initialized randomly. When the change in the weight values is
    great, there is a good possibility that the various weight values in the spectrum
    are not considered. However, when the change in the weight value is slight, the
    weights might achieve a global minima, as more possible weight values could be
    considered.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this further, let''s consider the toy example of fitting the
    *y = 2x* line where the initial weight value is 1.477 and the initial bias value
    is zero. The feedforward and back propagation functions will remain the same as
    we saw in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the only change from the backward propagation function that we saw
    in the previous chapter is that we are passing the learning rate as a parameter
    in the preceding function. The value of weight when the learning rate is 0.01
    over a different number of epochs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot of the change in weight over different epochs can be obtained using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff23c179-fb57-4c71-905c-512353516f30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a similar manner, the value of the weight over a different number of epochs
    when the learning rate is 0.1 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/226c0088-34ef-46bc-a399-46ce411fd70e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This screenshot shows the value of the weight over a different number of epochs when
    the learning rate is 0.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6883c3e-8044-4ba1-97cc-34943ece045f.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in the preceding scenario, there was a drastic change in the weight
    values initially, and the 0.1 learning rate converged, while the 0.5 learning
    rate did not converge to an optimal solution, and thus became stuck in a local
    minima.
  prefs: []
  type: TYPE_NORMAL
- en: In the case when the learning rate was 0.5, given the weight value was stuck
    in a local minima, it could not reach the optimal value of two.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand how learning rate influences the output values, let's
    see the impact of the learning rate in action on the MNIST dataset we saw earlier,
    where we keep the same model architecture but will only be changing the learning
    rate parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we will be using the same data-preprocessing steps as those of step
    1 and step 2 in the *Scaling input dataset* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the dataset preprocessed, we vary the learning rate of the model
    by specifying the optimizer in the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We change the learning rate as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: With the preceding code, we have initialized the Adam optimizer with a specified
    learning rate of 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: 'We build, compile, and fit the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The accuracy of the preceding network is ~90% at the end of 500 epochs. Let''s
    have a look at how loss function and accuracy vary over a different number of
    epochs (the code to generate the plots in the following diagram remains the same
    as the code we used in step 8 of the *Training a vanilla neural network* recipe):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b584542-f5c3-48b0-9af0-1855dea2f4c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that when the learning rate was high (0.01 in the current scenario) compared
    to 0.0001 (in the scenario considered in the *Scaling input dataset* recipe),
    the loss decreased less smoothly when compared to the low-learning-rate model.
  prefs: []
  type: TYPE_NORMAL
- en: The low-learning-rate model updates the weights slowly, thereby resulting in
    a smoothly reducing loss function, as well as a high accuracy, which was achieved
    slowly over a higher number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, the step changes in loss values when the learning rate is higher
    are due to the loss values getting stuck in a local minima until the weight values
    change to optimal values. A lower learning rate gives a better possibility of
    arriving at the optimal weight values faster, as the weights are changed slowly,
    but steadily, in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a similar manner, let''s explore the network accuracy when the learning
    rate is as high as 0.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'It is to be noted that the loss values could not decrease much further, as
    the learning rate was high; that is, potentially the weights got stuck in a local
    minima:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c8659c9-e4e3-4524-9dfd-5f6661fd3b96.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, it is, in general, a good idea to set the learning rate to a low value
    and let the network learn over a high number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Varying the loss optimizer to improve network accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in the previous recipes, we considered the loss optimizer to be the
    Adam optimizer. However, there are multiple other variants of optimizers, and
    a change in the optimizer is likely to impact the speed with which the model learns
    to fit the input and the output.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will understand the impact of changing the optimizer on model
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand the impact of varying the optimizer on network accuracy, let''s
    contrast the scenario laid out in previous sections (which was the Adam optimizer)
    with using a **stochastic gradient descent optimizer** in this section, while reusing
    the same MNIST training and test datasets that were scaled (the same data-preprocessing
    steps as those of step 1 and step 2 in the *Scaling the dataset* recipe):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that when we used the stochastic gradient descent optimizer in the preceding
    code, the final accuracy after 100 epochs is ~98% (the code to generate the plots
    in the following diagram remains the same as the code we used in step 8 of the *Training
    a vanilla neural network* recipe):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38cdae47-275b-46d5-b662-fc29d5f85df3.png)'
  prefs: []
  type: TYPE_IMG
- en: However, we should also note that the model achieved the high accuracy levels
    much more slowly when compared to the model that used Adam optimization.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some of the other loss optimizers available are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: RMSprop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adagrad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adadelta
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adamax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nadam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can learn more about the various optimizers here: [https://keras.io/optimizers/](https://keras.io/optimizers/).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you can find the source code of each optimizer here: [https://github.com/keras-team/keras/blob/master/keras/optimizers.py](https://github.com/keras-team/keras/blob/master/keras/optimizers.py).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the scenario of overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some of the previous recipes, we have noticed that the training accuracy
    is ~100%, while test accuracy is ~98%, which is a case of overfitting on top of
    a training dataset. Let's gain an intuition of the delta between the training
    and the test accuracies.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the phenomenon resulting in overfitting, let''s contrast two
    scenarios where we compare the training and test accuracies along with a histogram
    of the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: Model is run for five epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model is run for 100 epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The comparison-of-accuracy metric between training and test datasets between
    the two scenarios is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Scenario** | **Training dataset** | **Test dataset** |'
  prefs: []
  type: TYPE_TB
- en: '| 5 epochs | 97.59% | 97.1% |'
  prefs: []
  type: TYPE_TB
- en: '| 100 epochs | 100% | 98.28% |'
  prefs: []
  type: TYPE_TB
- en: 'Once we plot the histogram of weights that are connecting the hidden layer
    to the output layer, we will notice that the 100-epochs scenario has a higher
    spread of weights when compared to the five-epochs scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6f6b04b-293f-4fb2-a76f-211d40b81f0c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/7c5f76b7-f483-4666-9fc5-ea98573fc588.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding pictures, you should note that the 100 epochs scenario had
    a higher dispersion of weight values when compared to the five-epochs scenario.
    This is because of the higher amount of opportunity that the model had to overfit
    on top of the training dataset when the model is run for 100-epochs, when compared
    to when the model is run for five epochs, as the number of weight updates in the
    100-epochs scenario is higher than the number of weight updates in the five-epochs
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: A high value of weight (along with a difference in the training and test dataset)
    is a good indication of a potential over-fitting of the model and/or a potential
    opportunity to scale input/weights to increase the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, also note that a neural network can have hundreds of thousands
    of weights (and millions in certain architectures) that need to be adjusted, and
    thus, there is always a chance that one or the other weight can get updated to
    a very high number to fine-tune for one outlier row of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming over-fitting using regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we established that a high weight magnitude is one
    of the reasons for over-fitting. In this section, we will look into ways to get
    around the problem of over-fitting, such as penalizing for high weight magnitude
    values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Regularization** gives a penalty for having a high magnitude of weights in
    model. L1 and L2 regularizations are among the most commonly used regularization
    techniques and work as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'L2 regularization minimizes the weighted sum of squares of weights at the specified
    layer of the neural network, in addition to minimizing the loss function (which
    is the sum of squared loss in the following formula):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64a98b14-388a-4cbb-b604-30c56e739681.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/f9fef716-fe17-45b4-8abf-6a669c5d3cc6.png) is the weightage associated
    with the regularization term and is a hyperparameter that needs to be tuned, *y*
    is the predicted value of ![](img/28abf306-b171-4bd7-b8de-1bdffe82639c.png), and ![](img/f552cdd0-2311-4b72-8ec6-29e8c99fcf69.png) is
    the weight values across all the layers of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'L1 regularization minimizes the weighted sum of absolute values of weights
    at the specified layer of the neural network in addition to minimizing the loss
    function (which is the sum of the squared loss in the following formula):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e52d2e2-aca1-4e06-a3af-007deacb2468.png).'
  prefs: []
  type: TYPE_NORMAL
- en: This way, we ensure that weights do not get customized for extreme cases in
    the training dataset only (and thus, not generalizing on the test data).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'L1/L2 regularization is implemented in Keras, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Note that the preceding involves invoking an additional hyperparameter—`kernel_regularizer`—and
    then specifying whether it is an L1/L2 regularization. Furthermore, we also specify
    the lambda value that gives the weight to regularization.
  prefs: []
  type: TYPE_NORMAL
- en: We notice that, post regularization, the training dataset accuracy does not
    happen to be at ~100%, while the test data accuracy is at 98%. The histogram of
    weights post-L2 regularization is visualized in the next graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights of connecting the hidden layer to the output layer are extracted
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the weights are extracted, they are plotted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e672e37d-2059-4520-9564-12ee59698144.png)'
  prefs: []
  type: TYPE_IMG
- en: We notice that the majority of weights are now much closer to zero when compared
    to the previous scenario, thus presenting a case to avoid the overfitting issue.
    We would see a similar trend in the case of L1 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the weight values when regularization exists are much lower when
    compared to the weight values when regularization is performed.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the L1 and L2 regularizations help us to avoid the overfitting issue on
    top of the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming overfitting using dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section of overcoming overfitting using regularization, we used
    L1/ L2 regularization as a means to avoid overfitting. In this section, we will
    use another tool that is helpful to achieve the same—**dropout**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dropout can be considered a way in which only a certain percentage of weights
    get updated, while the others do not get updated in a given iteration of weight
    updates. This way, we are in a position where not all weights get updated in a
    weight update process, thus avoiding certain weights to achieve a very high magnitude
    when compared to others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have given a dropout of 0.75; that is, randomly, 75%
    of weights do not get updated in a certain weight update iteration.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding would result in the gap between the training and test accuracy
    being not as high as it is when the model was built without dropout in the previous
    scenario, where the spread of weights was higher.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the histogram of weights of the first layer now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/64d666de-bbb5-4c96-94d6-79946b4e26a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that in the preceding scenario, the frequency count of weights that are
    beyond 0.2 or -0.2 is less when compared to the 100-epochs scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Speeding up the training process using batch normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section on the scaling dataset, we learned that optimization
    is slow when the input data is not scaled (that is, it is not between zero and
    one).
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden layer value could be high in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Input data values are high
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight values are high
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The multiplication of weight and input are high
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any of these scenarios can result in a large output value on the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the hidden layer is the input layer to output layer. Hence, the phenomenon
    of high input values resulting in a slow optimization holds true when hidden layer
    values are large as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch normalization** comes to the rescue in this scenario. We have already
    learned that, when input values are high, we perform scaling to reduce the input
    values. Additionally, we have learned that scaling can also be performed using
    a different method, which is to subtract the mean of the input and divide it by
    the standard deviation of the input. Batch normalization performs this method
    of scaling.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, all values are scaled using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc09c4fd-85ae-46d7-97ac-2dc608a9c27f.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/e4844bdd-acf2-4e1d-b5f7-f8288413842a.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/110c9b11-033b-4bfc-9b4f-445cc803a93e.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/aa3c6535-7868-42c7-bd2f-4056453df216.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that *γ* and *β* are learned during training, along with the original
    parameters of the network.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In code, batch normalization is applied as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we will be using the same data-preprocessing steps as those we used
    in step 1 and step 2 in the *Scaling the input dataset* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `BatchNormalization` method as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate a model and build the same architecture as we built when using
    the regularization technique. The only addition is that we perform batch normalization
    in a hidden layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Build, compile, and fit the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding results in training that is much faster than when there is no
    batch normalization, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f5f1e2c-8a0d-45df-8c1a-d1a7918628b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous graphs show the training and test loss and accuracy when there
    is no batch normalization, but only regularization. The following graphs show
    the training and test loss and accuracy with both regularization and batch normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32d51718-2639-441a-8155-bd78778bed39.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in the preceding two scenarios, we see much faster training when
    we perform batch normalization (test dataset accuracy of ~97%) than compared to
    when we don't (test dataset accuracy of ~91%).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, batch normalization results in much quicker training.
  prefs: []
  type: TYPE_NORMAL
