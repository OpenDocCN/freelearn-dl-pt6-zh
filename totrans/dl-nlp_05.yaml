- en: '*Chapter 5*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Describe classical feedforward networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiate between feedforward neural networks and recurrent neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the application of backpropagation through time for recurrent neural
    networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the drawbacks of recurrent neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use recurrent neural networks with keras to solve the author attribution problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter aims to introduce you to recurrent neural networks and their applications,
    as well as their drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We encounter different kinds of data in our day-to-day lives, and some of this
    data has temporal dependencies (dependencies over time) while some does not. For
    example, an image by itself contains the information it wants to convey. However,
    data forms such as audio and video have dependencies over time. They cannot convey
    information if a fixed point in time is taken into consideration. Based on the
    problem statement, the input that's needed in order to solve the problem can differ.
    If we have a model to detect a particular person in a frame, a single image can
    be used as input. However, if we need to detect their actions, we need a stream
    of images, contiguous in time, as the input. We can understand the person's actions
    by analyzing these images together, but not independently.
  prefs: []
  type: TYPE_NORMAL
- en: While watching a movie, a particular scene makes sense because its context is
    known, and we remember all the information gathered before in the movie to understand
    the current scene. This is very important, and we, as humans, can do this because
    our brains can store memory, analyze past data, and retrieve useful information
    to understand the current scene.
  prefs: []
  type: TYPE_NORMAL
- en: Networks such as multi-layered perceptron and convolutional neural networks
    lack this capability. Every input given to these networks is treated independently,
    and they don't store any information from past inputs to analyze the current inputs
    because they lack memory in their architecture. That being the case, maybe there
    is a way we can enable neural networks to have memory. We can try and make them
    store useful information from the past and make them retrieve information from
    the past that helps them to analyze the current input. This is indeed possible,
    and the architecture for it is called the **Recurrent** **Neural** **Network**
    (**RNN**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we delve deep into the theory of RNNs, let''s take a look at their applications.
    Currently, RNNs are widely used. Some of the applications are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Speech recognition*: Whether it''s Amazon''s Alexa, Apple''s Siri, Google''s
    voice assistant, or Microsoft''s Cortana, all their speech recognition systems
    use RNNs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Time series predictions*: Any application with time series data, such as stock
    market data, website traffic, call center traffic, movie recommendations, Google
    Maps routes, and so on, uses RNNs to predict future data, the optimal path, optimal
    resource allocations, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Natural language processing*: Applications such as machine translation (for
    Google Translate, for instance), chatbots (such as those for Slack and Google),
    and question answering all use RNNs to model dependencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Previous Versions of Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Around 40 years ago, it became clear that **Feed** **Forward** **Neural** **Networks**
    (**FFNNs**) could not capture time-variable dependencies, which are essential
    for capturing the time-variable properties of a signal. Modeling time-variable
    dependencies is very important in many applications involving real-world data,
    such as speech and video, in which data has time-variable properties. Also, human
    biological neural networks have a recurrent relationship, so it is the most obvious
    direction to take. How could this recurrent relationship be added to existing
    feedforward networks?
  prefs: []
  type: TYPE_NORMAL
- en: One of the first attempts to achieve this was done by adding delay elements,
    and the network was called the **Time-Delay** **Neural** **Network**, or **TDNN**
    for short.
  prefs: []
  type: TYPE_NORMAL
- en: In this network, as the following figure shows, the delay elements are added
    to the network and the past inputs are given to the network along with the current
    timestep as the input to the network. This definitely has an advantage over the
    traditional feed forward networks but has the disadvantage of having only so many
    inputs from the past as the window allows. If the window is too large, the network
    grows with increasing parameters and computational complexities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: TDNN structure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.1: TDNN structure'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Then came Elman networks, or simple RNNs. Elman networks are very similar to
    feedforward networks, except that the hidden layer of output is stored and used
    for the next input. This way, information from the previous timesteps can be captured
    in these hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: One way of looking at Elman networks is that at each input, we append the previous
    hidden layers' outputs along with the inputs and send them all as the inputs to
    the network. So, if the input size is **m** and the hidden layer size is **n**,
    the effective input layer size becomes **m+n**.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure shows a simple three-layer network, where the previous
    state is fed back to the network to store the context, and therefore it is called
    **SimpleRNN**. There are other variations to this architecture, such as Jordan
    networks, which we will not study in this chapter. For those are interested in
    the early history of RNNs, reading more on Elman networks and Jordan networks
    might be the best place to start.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2: SimpleRNN structure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.2: SimpleRNN structure'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: And then came the **RNN**, which is the topic of this chapter. We will look
    into RNNs in detail in the coming sections It is important to note that in recurrent
    networks, since there are memory units and weights associated to these units,
    they need to be learned during backpropagation. Since these gradients are also
    backpropagated through time, we call it **Back** **Propagation** **Through** **Time**,
    or **BPTT**. We will discuss BPTT in detail in the upcoming sections. However,
    TDNN, Elman networks, and RNNs have a major drawback due to BPTT, and it is called
    vanishing gradients. Vanishing gradients is a problem where gradients get smaller
    and smaller as they backpropagate, and in these networks, as timesteps increase,
    back-propagated gradients get smaller and smaller, resulting in vanishing gradients.
    It's almost impossible to capture time dependencies greater than 20 timesteps.
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, an architecture called the **Long** **Short-Term** **Memory**
    (**LSTM**) architecture was introduced. The key idea here is to hold some cell
    states constant and introduce them as needed in future timesteps. These decisions
    are made by gates, including forget gates and output gates. Another commonly used
    variant of the LSTM is called the **Gated** **Recurrent** **Unit**, or **GRU**
    for short. Don't worry much if you didn't understand this completely. There are
    two chapters following that are dedicated to making these concepts clear.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recurrent often means occurring repeatedly. The recurrent part of RNNs simply
    means that the same task is done over all the inputs in the input sequence (for
    RNNs, we give a sequence of timesteps as the input sequence). One main difference
    between feed forward networks and RNNs is that RNNs have memory elements called
    states that capture the information from the previous inputs. So, in this architecture,
    the current output not only depends on the current input, but also on the current
    state, which takes into account past inputs.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs are trained by sequences of inputs rather than a single input; similarly,
    we can consider each input to an RNN as a sequence of timesteps. The state elements
    in RNNs contain information about past inputs to process the current input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3: RNN structure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.3: RNN structure'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For each input in the input sequence, the RNN gets a state, calculates its output,
    and sends its state to the next input in the sequence. The same set of tasks is
    repeated for all the elements in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: It's easy to understand RNNs and their operations by comparing them to feedforward
    networks. Let's do that now.
  prefs: []
  type: TYPE_NORMAL
- en: By now, it's very clear that the inputs are independent of each other in feedforward
    neural networks, so we train the network by randomly drawing pairs of inputs and
    outputs. There is no significance to the sequence. At any given time, the output
    is a function of input and weights.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4: Expression for the output of an RNN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_05_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.4: Expression for the output of an RNN'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In RNNs, our output at time **t** depends not only on the current input and
    the weight, but also on previous inputs. In this case, the output at time **t**
    will be defined as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5: Expression for the output of an RNN at time t'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_05_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.5: Expression for the output of an RNN at time t'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let's look at a simple structure of an RNN that is called a folded model. In
    the following figure, the **S****t** state vector is fed back into the network
    from the previous timestep. One important takeaway from this representation is
    that RNNs share the same weight matrices across timesteps. By increasing the timesteps,
    we are not learning more parameters, but we are looking at a bigger sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6: Folded model of an RNN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.6: Folded model of an RNN'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is a folded model of an RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Xt** : Current input vector in the input sequence'
  prefs: []
  type: TYPE_NORMAL
- en: '**Yt**: Current output vector in the output sequence'
  prefs: []
  type: TYPE_NORMAL
- en: '**St**: Current state vector'
  prefs: []
  type: TYPE_NORMAL
- en: '**Wx**: Weight matrix connecting the input vector to the state vector'
  prefs: []
  type: TYPE_NORMAL
- en: '**Wy**: Weight matrix connecting the state vector to the output vector'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ws**: Weight matrix connecting the state vector of previous timestep to the
    next one'
  prefs: []
  type: TYPE_NORMAL
- en: Since the input, **x** is a sequence of timesteps and we perform the same task
    for elements in this sequence, we can unfold this model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7: Unfolding of an RNN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.7: Unfolding of an RNN'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For example, the output at time **t+1**,**y****t+1** depends on input at time
    **t+1**, weight matrices, and all the inputs before it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8: Unfolded RNN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.8: Unfolded RNN'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since RNNs are extensions of FFNNs, it's best to understand the differences
    between these architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9: Differences between FFNNs and RNNs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.9: Differences between FFNNs and RNNs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output expressions for FFNNs and RNNs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10: Output expressions for FFNNs and RNNs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.10: Output expressions for FFNNs and RNNs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the previous figure and equations, it is very evident that there are a
    lot of similarities between these two architectures. In fact, they are the same
    if **Ws=0**. This is obviously the case since **Ws** is the weight associated
    with the state that is fed back to the network. Without **Ws**, there is no feedback,
    which is the basis of the RNN.
  prefs: []
  type: TYPE_NORMAL
- en: In FFNNs, the output at **t** depends on the input at **t** and weight matrices.
    In RNNs, the output at **t** depends on input at **t**, **t-1**, **t-2**, and
    so on, as well as the weight matrices. This is explained with the further calculation
    of hidden vector **h** in the case of an FFNN and **s** in the case of an RNN.
    At first glance, it might look like the state at **t** depends on the input at
    **t**, the state at **t-1**, and the weight matrices; and the state at **t-1**
    depends on the input at **t-1**, the state at **t-2**, and so on; creating a chain
    that goes back all the way to the first timestep considered. The output calculations
    of both FFNNs and RNNs are same, though.
  prefs: []
  type: TYPE_NORMAL
- en: RNN Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RNNs can come in many forms, and the appropriate architecture needs to be chosen
    depending on the problem we are solving.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 Different architectures of RNNs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_05_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11 Different architectures of RNNs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*One to many*: In this architecture, a single input is given, and the output
    is a sequence. An example of this is image captioning, where the input is a single
    image, and the output is a sequence of words explaining the image.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Many to one*: In this architecture, a sequence of inputs is given, but a single
    output is expected. An example is any time series prediction where the next timestep
    in the sequence needs to be predicted, given the previous timesteps.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Many to many*: In this architecture, an input sequence is given to the network,
    and the network outputs a sequence. In this case, the sequence can be either synced
    or not synced. For example, in machine translation, the whole sentence needs to
    be fed in before the networks starts to translate it. Sometimes, the input and
    output are not in sync; for example, in the case of speech enhancement, where
    an audio frame is given as input and a cleaner version of the input frame is the
    output expected. In such cases, the input and output are in sync.'
  prefs: []
  type: TYPE_NORMAL
- en: RNNs can also be stacked on top of each other. It is important to note that
    each RNN in the stack has its own weight matrices. So, the weight matrices are
    shared on the horizontal axis (the time axis) and not on the vertical axis (the
    number of RNNs).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12: Stacked RNNs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.12: Stacked RNNs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: BPTT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RNNs can deal with varying sequence lengths, can be used in different forms,
    and can be stacked on top of each other. Previously, you have come across the
    back propagation technique to backpropagate loss values to adjust weights. In
    the case of RNNs, something similar can be done, with a bit of a twist, which
    is a gate loss through time. It's called **BPTT**.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the basic theory of back propagation, we know the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13: Expression for weight update'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_05_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.13: Expression for weight update'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The update value is calculated through gradient calculations using the chain
    rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 Partial derivative of error with regards to weight'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_05_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.14 Partial derivative of error with regards to weight
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, **α** is the learning rate. The partial derivative of **Error** (**loss**)
    with respect to the weight matrix is the main calculation. Once this new matrix
    is obtained, adjusting the weight matrices is simply adding this new matrix, scaled
    by a learning factor, to itself.
  prefs: []
  type: TYPE_NORMAL
- en: When calculating the update values for RNNs, we will use BPTT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example to understand this better. Consider a loss function,
    such as the mean squared error (which is commonly used for regression problems):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15: Loss function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.15: Loss function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'At timestep **t = 3**, the loss calculated is as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 Loss at time t=3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_05_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.16 Loss at time t=3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This loss needs to be backpropagated, and the **Wy**, **Wx**, and **Ws** weights
    need to be updated.
  prefs: []
  type: TYPE_NORMAL
- en: As seen previously, we need to calculate the update value to adjust these weights,
    and this update value can be calculated using partial derivatives and the chain
    rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three parts to doing this:'
  prefs: []
  type: TYPE_NORMAL
- en: Update Weight **Wy** by calculating the partial derivative of the error with
    respect to **Wy**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update Weight **Ws** by calculating the partial derivative of the error with
    respect to **Ws**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update Weight **Wx** by calculating the partial derivative of the error with
    respect to **Wx**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we look at these updates, let's unroll the model and keep the part of
    the network that's actually relevant for our calculations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 Unfolded RNN with loss at time t=3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.17 Unfolded RNN with loss at time t=3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since we are looking at how loss at **t=3** affects the weight matrices, the
    loss values at and previous to **t=2** are not relevant. Now, we need to understand
    how to backpropagate this loss through the network.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at each of these updates and show the gradient flow for each of the
    updates shown in the preceding figure.
  prefs: []
  type: TYPE_NORMAL
- en: Updates and Gradient Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The updates can be listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting weight matrix **Wy**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusting weight matrix **Ws**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For updating **Wx**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusting Weight Matrix **Wy**
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The model can be visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18: Back propagation of loss through weight matrix Wy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.18: Back propagation of loss through weight matrix Wy'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For Wy, the update is very simple since there are no additional paths or variables
    between Wy and the error. The matrix can be realized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19: Expression for weight matrix Wy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_05_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.19: Expression for weight matrix Wy'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Adjusting Weight Matrix **Ws**
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure 5.20: Back propagation of loss through weight matrix Ws with respect
    to S3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.20: Back propagation of loss through weight matrix Ws with respect
    to S3'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can calculate the partial derivate of error with respect to **Ws** using
    the chain rule, as shown in the previous figure. It looks like that is what is
    needed, but it''s important to remember that **S****t**is dependent on **S****t-1**,
    and therefore **S****3** is dependent on **S****2**, so we need to consider **S****2**
    also, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21: Back propagation of loss through weight matrix Ws with respect
    to S2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.21: Back propagation of loss through weight matrix Ws with respect
    to S2'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Again, **S****2** in turn depends on **S****1**, and therefore **S****1** needs
    to be considered, too, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22: Back propagation of loss through weight matrix Ws with respect
    to S1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.22: Back propagation of loss through weight matrix Ws with respect
    to S1'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'At **t=3**, we must consider the contribution of state **S****3** to the error,
    the contribution of state **S****2** to the error, and the contribution of state
    **S****1** to the error, **E****3**. The final value looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23: Sum of all derivatives of error with respect to Ws at t=3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.23: Sum of all derivatives of error with respect to Ws at t=3'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In general, for timestep **N**, all the contributions of the previous timesteps
    need to be considered. So, the general formula looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24: General expression for the derivative of error with respect
    to Ws'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.24: General expression for the derivative of error with respect to
    Ws'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For Updating **Wx**
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can calculate the partial derivate of error with respect to **Wx** using
    the chain rule, as shown in the next few figures. With the same reasoning that
    **S****t** is dependent on **S****t-1**, the calculation of partial derivative
    of error with respect to **Wx** can be divided into three stages at **t=3**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25: Back propagation of loss through weight matrix Wx with respect
    to S2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.25: Back propagation of loss through weight matrix Wx with respect
    to S2'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Back propagation of loss through weight matrix Wx with respect to S2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26: Back propagation of loss through weight matrix Wx with respect
    to S2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.26: Back propagation of loss through weight matrix Wx with respect
    to S2'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Back propagation of loss through weight matrix Wx with respect to S1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.27: Back propagation of loss through weight matrix Wx with respect
    to S1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.27: Back propagation of loss through weight matrix Wx with respect
    to S1'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Similar to the previous discussion, at **t=3**, we must consider the contribution
    of state **S****3** to the error, the contribution of state **S****2** to the
    error, and the contribution of state **S****1** to the error, **E****3**. The
    final value looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.28: Sum of all derivatives of error with respect to Wx at t=3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.28: Sum of all derivatives of error with respect to Wx at t=3'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In general, for timestep N, all the contributions of the previous timesteps
    need to be considered. So, the general formula looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.29: General expression of derivative of error with respect to Wx'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.29: General expression of derivative of error with respect to Wx'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since the chain of derivatives already has 5 multiplicative terms at **t=3**,
    this number grows to 22 multiplicative terms for timestep 20\. It's possible that
    each of these derivatives could be either greater than 0 or less than 0\. Due
    to consecutive multiplications with longer timesteps, the total derivative gets
    smaller or larger. This problem is either vanishing gradients or exploding gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Gradients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two types of gradients that have been identified are:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploding gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanishing gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploding Gradients
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name indicates, this happens when gradients explode to much bigger values.
    This could be one of the problems that RNN architectures could encounter with
    larger timesteps. This could happen when each of the partial derivatives is larger
    than **1**, and multiplication of these partial derivatives leads to an even larger
    value. These larger gradient values cause a dramatic shift in the weight values
    each time they are adjusted using back propagation, leading to a network that
    doesn't learn well.
  prefs: []
  type: TYPE_NORMAL
- en: There are some techniques used to mitigate this issue, such as gradient clipping,
    wherein the gradient is normalized once it exceeds a set threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing Gradients
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whether it is RNNs or CNNs, vanishing gradients could be a problem if calculated
    loss has to travel back a lot. In CNNs, this problem could occur when there are
    a lot of layers with activations such as sigmoid or tanh. The loss has to travel
    all the way back to the initial layers, and these activations generally dilute
    them by the time they reach the initial layers, which means there are almost no
    weight updates for the initial layers, resulting in underfitting. This is even
    common in RNNs, since even if a network has one RNN layer but a large number of
    timesteps, the loss has to travel all the way through the timesteps due to backpropagation
    through time. Since the gradients are multiplicative, as seen in the generalized
    derivative expressions earlier, these values tend to become low, and weights are
    not updated after a certain timestep. This means that even if more timesteps are
    shown to a network, the network can't benefit because the gradients cannot travel
    all the way back. This limitation in RNNs is due to vanishing gradients.
  prefs: []
  type: TYPE_NORMAL
- en: As the name indicates, this happens when the gradients become too small. This
    could happen when each of partial derivatives is smaller than 1 and multiplication
    of these partial derivatives leads to a much smaller value. With this geometric
    decay of information, the network cannot learn properly. There are almost no changes
    in the weight values, which leads to underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: There must be a better mechanism to use to know what parts of the previous timesteps
    to remember, what to forget, and so on. To address this issue, architectures such
    as LSTM networks and GRUs were created.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs with Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have discussed the theory behind RNNs, but there are a lot of frameworks
    available that can abstract away the implementation details. As long as we know
    how to use these frameworks, we can successfully get our projects working. **TensorFlow**,
    **Theano**, **Keras**, **PyTorch**, and **CNTK** are some of these frameworks.
    In this chapter, let's take a closer look at the most commonly used framework,
    called **Keras**. It uses either Tensorflow or Theano as the backend, indicating
    that it creates an even higher level of abstraction than other frameworks. It
    is a tool best suited for beginners. Once comfortable with Keras, tools such as
    TensorFlow give much more power in implementing custom functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many variants of RNNs that you will study in the next few chapters,
    but all of them use the same base class, called RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In this chapter, we have discussed the simple form of the RNN, which is called
    **SimpleRNN** in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the arguments here, there are two kinds: one for regular
    kernels, used to compute the outputs of a layer, and the other for recurrent kernels
    used to compute states. Don''t worry too much about constraints, regularizers,
    initializers, and dropout. You can find more about them at https://keras.io/layers/recurrent/.
    They are mostly used to avoid overfitting. The role of activation here is the
    same as the role of activation with any other layer.'
  prefs: []
  type: TYPE_NORMAL
- en: The units are the number of recurrent units in a particular layer. The greater
    the number of units, the more parameters there are that need to be learned.
  prefs: []
  type: TYPE_NORMAL
- en: '`return_sequences` is the argument that specifies whether the RNN layer should
    return the whole sequence or just the last timestep. If `return_sequences` is
    false, the output of the RNN layer is just the last timestep, so we cannot stack
    this with another RNN layer. In other words, if an RNN layer needs to be stacked
    by another RNN layer, `return_sequences` need to be true. If an RNN layer is connected
    to the Dense layer, this can argument can be either true or false, depending on
    the application.'
  prefs: []
  type: TYPE_NORMAL
- en: The `return_state` argument specifies whether the last state of the RNN needs
    to be returned along with the output. This can be set to either True or False,
    depending on the application.
  prefs: []
  type: TYPE_NORMAL
- en: '`go_backwards` can be used if, for any reason, the input sequence needs to
    be processed backward. Keep a note that if this is set to True, even the returned
    sequence is reversed.'
  prefs: []
  type: TYPE_NORMAL
- en: '`stateful` is an argument that can be set to true if a state needs to be passed
    between batches. If this argument is set to true, the data needs to be handled
    carefully; we have a topic covering this in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: '`unroll` is an argument that leads to the network being unrolled if set to
    true, which can speed up operations but can be very memory extensive depending
    on the timesteps. Generally, this argument is set to true for short sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of timesteps is not an argument for a particular layer since it
    stays the same for the whole network, which is represented in the input shape.
    This brings us to the important point of the shape of the network when using RNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you start building a network with an RNN layer, `input_shape` must be specified.
  prefs: []
  type: TYPE_NORMAL
- en: After a model is built, `model.summary()` can be used to see the shapes of each
    layer and the total number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 23: Building an RNN Model to Show the Stability of Parameters over
    Time'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's build a simple RNN model to show that the parameters do not change with
    timesteps. Note that while mentioning the `input_shape` argument, `batch_size`
    need not be mentioned unless needed. It is needed for a stateful network, which
    we will discuss next. `batch_size` is mentioned while training the model with
    the fit() or `fit_generator()` functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you with the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary Python packages. We will be using Sequential, SimpleRNN,
    and Dense.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define the model and its layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can check the summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`model.summary()` gives the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13783_05_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.30: Model summary for model layers'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: In this case, `batch_size` parameter, which will be provided by the `fit()`
    function. The output of the RNN layer is **(None, 64)** since it is not returning
    the sequence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s look at the model that returns sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The summary of the model that returns sequence looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13783_05_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.31: Model summary of sequence-returning model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Now the RNN layer is returning a sequence, and therefore its output shape is
    3D instead of 2D, as seen earlier. Also, note that the **Dense** layer is automatically
    adjusted to this change in its input. The **Dense** layer with the current Keras
    version has the capability of adjusting to time_steps from a previous RNN layer.
    In the previous versions of Keras, **TimeDistributed**(**Dense**) was used to
    achieve this.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We have previously discussed how the RNN shares its parameters over timesteps.
    Let''s see that in action and change the timesteps of the previous model from
    10 to 1,000:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 5.32: Model summary for timesteps'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_05_32.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.32: Model summary for timesteps'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Clearly, the output shapes of the network changed to this new time_steps. However,
    there is no change in the parameters between the two models.
  prefs: []
  type: TYPE_NORMAL
- en: This indicates that the parameters are shared over time and are not impacted
    by changing the number of timesteps. Note that the same is applicable to the **Dense**
    layer when operating on more than one timestep.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful versus Stateless
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two modes of operation available with RNNs considering the states:
    the stateless and stateful modes. If the **argument stateful=True**, you are working
    with stateful mode, and **False** signifies stateless mode.'
  prefs: []
  type: TYPE_NORMAL
- en: Stateless mode is basically saying that one example in a batch is not related
    to any example in the next batch; that is, every example is independent in the
    given case. The state is reset after every example. Each example has a certain
    number of timesteps depending on the model architecture. For example, the last
    model we saw had 1,000 timesteps, and between these 1000 timesteps, the state
    vector was calculated and passed from one timestep to the next. However, at the
    end of the example or the beginning of the next example, there was no state passed.
    Each example was independent and therefore there was no consideration needed regarding
    the way the data was shuffled.
  prefs: []
  type: TYPE_NORMAL
- en: In stateful mode, the state from example **i** of **batch 1** is passed to the
    **i+1** example of **batch 2**. This means that the state is passed from one example
    to the next among batches. For this reason, the examples must be contiguous across
    batches and cannot be random. The following figure explains this situation. The
    examples **i**, **i+1**, **i+2**, and so on are contiguous, and so are **j**,
    **j+1**, **j+2**, and so on, and **k**, **k+1**, **k+2**, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.33 Batch formations for stateful RNN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_5_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.33 Batch formations for stateful RNN
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 24: Turning a Stateless Network into a Stateful Network by Only Changing
    Arguments'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to turn a network from stateless to stateful by changing the arguments,
    the following steps should be taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we would need to import the required Python packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, build the model using `Sequential` and define the layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the optimizer to `Adam`, set `categorical` `crosstropy` as the loss parameter,
    and set the metrics to fit the model. Compile the model and fit the model over
    100 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assume that `X` and `Y` are training data as contiguous examples. Turn this
    model into a stateful one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the optimizer to `Adam`, set `categorical` `crossentropy` as the loss parameter,
    and set the metrics to fit the model. Compile the model and fit the model over
    100 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can use a box and whisker plot to visualize the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.34: Box and whisker plot for stateful vs stateless'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_05_34.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.34: Box and whisker plot for stateful vs stateless'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The output may vary depending on the data used.
  prefs: []
  type: TYPE_NORMAL
- en: From the concept of stateful models, we understand that the data fed in batches
    need to be contiguous, so turn randomization **OFF**. However, even with **batch_size
    >1**, the data across batches will not be contiguous, so make **batch_size=1**.
    By turning the network to **stateful=True** and fitting it with the mentioned
    parameters, we are essentially training the model correctly in a stateful manner.
  prefs: []
  type: TYPE_NORMAL
- en: However, we are not using the concept of mini batch gradient descent, and nor
    are we shuffling the data. So, a generator needs to be implemented that can carefully
    train a stateful network, which is outside the scope of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '`model.compile` is a function where an optimizer and a loss function are assigned
    to the network, along with the metrics that we care about.'
  prefs: []
  type: TYPE_NORMAL
- en: '`model.fit()` is a function that is used to train a model by specifying its
    training data, validation data, the number of epochs, the batch size, the mode
    of shuffling, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6: Solving a Problem with an RNN – Author Attribution'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Author attribution is a classic text classification problem that comes under
    the umbrella of natural language processing (NLP). Authorship attribution is a
    well-studied problem that led to the field of **stylometry**.
  prefs: []
  type: TYPE_NORMAL
- en: In this problem, we are given a set of documents from certain authors. We need
    to train a model to understand the authors' styles and use the model to identify
    the authors of the unknown documents. As with many other NLP problems, it has
    benefited greatly from the increase in available computer power, data, and advanced
    machine learning techniques. This makes authorship attribution a natural candidate
    for the use of **deep learning (DL**). In particular, we can benefit from DL's
    ability to automatically extract the relevant features for a specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this activity, we will focus on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting character-level features from the text of each author (to get each
    author's style)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using those features to build a classification model for authorship attribution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying the model for identifying the author of a set of unknown documents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find the required data for the activity at https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2005.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The following steps will help you with the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary Python packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the text document to be used. Then, pre-process the text file by converting
    all text into lowercase, converting all newlines and multiple whitespaces into
    single whitespaces, and removing any mention of the authors' names, otherwise
    we risk data leakage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To break the long texts into smaller sequences, we use the `Tokenizer` class
    from the Keras framework.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Proceed to create the training and validation sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We construct the model graph and perform the training procedure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the model to the unknown papers. Do this for all the papers in the **Unknown**
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.35: Output for author attribution'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_05_35.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.35: Output for author attribution'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for the activity can be found on page 309.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to RNNs and covered the major differences
    between the architectures of RNNs and FFNNs. We looked at BPTT and how weight
    matrices are updated. We learned how to use RNNs using Keras and solved a problem
    of author attribution using RNNs in Keras. We looked at the shortcomings of RNNs
    by looking at vanishing gradients and exploding gradients. In the next chapters,
    we will look into architectures that will address these issues.
  prefs: []
  type: TYPE_NORMAL
