- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Going Pro with Artificial Brains – Deep Q-Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This next AI model is fantastic, because it is the first AI model that is really
    inspired by human intelligence. I hope you're ready to go pro on the next exciting
    step in your AI journey; this book is not only a crash course on AI, but also
    an introduction to deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Today, some of the top AI models integrate deep learning. They form a new branch
    of AI called deep Reinforcement Learning. The model we'll cover in this chapter
    belongs to that branch, and is called deep Q-learning. You already know what Q-learning
    is all about, but you might not know anything about deep learning and **Artificial
    Neural Networks** (**ANNs**); we'll start with them. Of course, if you are an
    expert in deep learning, you can skip the first sections of this chapter, but
    consider that a little refresher never hurt anyone.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start going through the theory, you'll begin with real, working code
    written in Python. You'll create some AI first, and then I'll help you understand
    it afterwards. Right now, we're going to build an ANN to predict house prices.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting house prices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What we want to do is predict how much a certain house might cost, based on
    some variables. In order to do so you need to follow these four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Get some historical data on house sales; for this example, you'll use a dataset
    of about 20,000 houses in Seattle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import this data to your code while applying some scaling to your variables
    (I'll explain scaling to you as we go).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build an Artificial Neural Network using any library—you'll use Keras, as it
    is simple and reliable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train your ANN and get the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you know the structure of your future code, you can start writing it.
    Since all the libraries that you'll use are available in Google Colab, you can
    easily use it to perform this task.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Start by creating a new Google Colab notebook. Once we have created your new
    notebook, before you start coding anything, you have to upload your dataset. You
    can find this dataset, called `kc_house_data.csv`, on the GitHub repository in
    the `Chapter 09` folder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: GitHub – Chapter 09'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have done that, you can upload it to Colab by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Click this little arrow here:![](img/B14110_09_02.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2: Google Colab – Uploading files (1/3)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the window that pops up, go to **Files**. You should get something like this:![](img/B14110_09_03.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 3: Google Colab – Uploading files (2/3)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on **UPLOAD** and then select the file location where you saved the `kc_house_data`
    dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After you have done that, you should get a new folder with our dataset, like
    this:![](img/B14110_09_04.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 4: Google Colab – Uploading files (3/3)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Great! Now you can start coding.
  prefs: []
  type: TYPE_NORMAL
- en: Importing libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Every time you start coding something you ought to begin by importing the necessary
    libraries. Therefore, we start our code with these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In lines 4 and 5, after the comment, you import the `pandas` and `numpy` libraries.
    Pandas will help you read the dataset and NumPy is very useful when you're dealing
    with arrays or lists; you'll use it to drop some unnecessary columns from your
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the two subsequent lines you import two useful tools from the Scikit-Learn
    library. The first one is a tool that will help split the dataset into a training
    set and a test set (you should always have both of them; the AI model is trained
    on the training set and then tested on the test set) and the second one is a scaler
    that will help you later when scaling values.
  prefs: []
  type: TYPE_NORMAL
- en: Lines 9, 10, and 11 are responsible for importing the `keras` library, which
    you'll use in order to build a neural network. Each of these tools is used later
    in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have imported your libraries you can read the dataset. Do it by
    using the Pandas library you imported before, with this one line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since you used `pd` as an abbreviation for the Pandas library when you imported
    it, you can use it to shorten your code. After you call the Pandas library with
    `pd`, you can use one of its functions, `read_csv`, which, as the name suggests,
    reads csv files. Then in the brackets you input the file name, which in your case
    is `kc_house_data.csv`. No other arguments are needed.
  prefs: []
  type: TYPE_NORMAL
- en: Now I have a little exercise for you! Have a look at the dataset and try to
    judge which of the variables will matter for our price prediction. Believe me,
    not all of them are relevant. I strongly suggest that you try to do it alone even
    though we'll discuss them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Excluding variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Were you able to discern which variables are necessary and which are not? Don't
    worry if not; we'll explain them and their relevance right now.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table explains every column in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Variable** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Id | Unique ID for each household |'
  prefs: []
  type: TYPE_TB
- en: '| Date | Date when the house was sold |'
  prefs: []
  type: TYPE_TB
- en: '| Price | How much the house cost when sold |'
  prefs: []
  type: TYPE_TB
- en: '| Bedrooms | Number of bedrooms |'
  prefs: []
  type: TYPE_TB
- en: '| Bathrooms | Number of bathrooms; 0.5 represents room with a toilet but no
    shower |'
  prefs: []
  type: TYPE_TB
- en: '| Sqft_living | Square footage of the apartment''s interior living space |'
  prefs: []
  type: TYPE_TB
- en: '| Sqft_lot | Square footage of the land space |'
  prefs: []
  type: TYPE_TB
- en: '| Floors | Number of floors |'
  prefs: []
  type: TYPE_TB
- en: '| Waterfront | 0 if the apartment doesn''t overlooking the waterfront, 1 if
    it does |'
  prefs: []
  type: TYPE_TB
- en: '| View | Value in the range 0-4 depending on how good the view of the property
    is |'
  prefs: []
  type: TYPE_TB
- en: '| Condition | Value from 1-5 defining the condition of the property |'
  prefs: []
  type: TYPE_TB
- en: '| Grade | Value from 1-13 indicating the design and construction of the building
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sqft_above | The square footage of the interior housing space that is above
    ground level |'
  prefs: []
  type: TYPE_TB
- en: '| Sqft_basement | The square footage of the basement |'
  prefs: []
  type: TYPE_TB
- en: '| Yr_built | Year when the house was built |'
  prefs: []
  type: TYPE_TB
- en: '| Yr_renovated | Year when the house was renovated (0 if wasn''t) |'
  prefs: []
  type: TYPE_TB
- en: '| Zipcode | Zip code of the area house is located in |'
  prefs: []
  type: TYPE_TB
- en: '| Lat | Latitude |'
  prefs: []
  type: TYPE_TB
- en: '| Long | Longitude |'
  prefs: []
  type: TYPE_TB
- en: '| Sqft_living15 | The square footage of the interior housing living space for
    the nearest 15 neighbors |'
  prefs: []
  type: TYPE_TB
- en: '| Sqft_lot15 | Square footage of the land lots of the nearest 15 neighbors
    |'
  prefs: []
  type: TYPE_TB
- en: It turns out that from those 21 variables, only 18 count. That is because unique,
    category-like values do not have any impact on your prediction. That includes
    Id, Date, and Zipcode. Price is the target of your prediction, and therefore you
    should get rid of that from your variables as well. After all that, you have 17
    independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explained all the variables and decided which are relevant
    and which are not, you can go back to your code. You're going to exclude these
    unnecessary variables and split the dataset into the features and the target (in
    our case the target is price).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: On line 17, you take all rows and all columns starting with the fourth one (since
    you're excluding Id, Date, Price) from your dataset and call this new set `X`.
    You use `.iloc` to slice the dataset, and then take `.values` to change it to
    a NumPy object. These will be your features.
  prefs: []
  type: TYPE_NORMAL
- en: Next you need to exclude Zipcode, which quite unfortunately is in the middle
    of the features set. That's why you have to use a NumPy function (`np.r_`) that
    separates `X`, excludes the columns you choose (in this case it is column 14\.
    13 is the index of this column, since indexes in Python start with zero; it's
    also worth mentioning that upper bounds are excluded in Python notation, which
    is why we write `0:13`), and then connects them once again to form a new array.
    In the next line, you get the target of your prediction and call it `y`. This
    corresponds to the third column in your dataset, that is, Price.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you''ve separated your important features and target, you can split
    your `X` and `y` into training and test sets. We do that with the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is very important when doing any kind of machine learning. You always have
    to have a training set on which you train your model, and a test set on which
    you test it. You perform that operation using the `train_test_split` function
    you imported before. After doing that, you get `X_train`, which is of equal size
    to `y_train`, and each of them are exactly 80% of our previous `X` and `y` set.
    `X_test` and `y_test` are made up of the remaining 20% of `X` and `y`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have both a training set and a test set, what do you think the
    next step is? Well, you have to scale your data.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now you might be wondering why on earth you have to perform such an operation.
    You already have the data, so why not build and train the neural network already?
  prefs: []
  type: TYPE_NORMAL
- en: There's a problem with that; if we leave the data as it is, you'll notice that
    your ANN does not learn. The reason for that is because different variables will
    impact your prediction more or less depending on their values.
  prefs: []
  type: TYPE_NORMAL
- en: Take this graph illustrating what I mean, based on a property that has 3 bedrooms
    and 1,350 square feet of living area.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Example for 3 bedrooms and 1350 square feet of living area'
  prefs: []
  type: TYPE_NORMAL
- en: You can clearly see that the number of bedrooms won't affect the prediction
    as much as Sqft_living will. Even we humans cannot see any difference between
    zero bedrooms and three bedrooms on this graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of many solutions to this problem is to scale all variables to be in a
    range between 0 and 1\. We achieve this by calculating this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x* – the value we are scaling in our case every value in a column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[min] – minimum value across all in a column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[max] – maximum value across all in a column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[scaled] – *x* after performing scaling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After performing this scaling, our previous graph now looks something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Same graph after scaling'
  prefs: []
  type: TYPE_NORMAL
- en: Now we can undoubtedly say that the number of bedrooms will have a similar impact
    to Sqft_living. We can clearly see the difference between zero bedrooms and three
    bedrooms.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we implement that in code? Since you know the equation, I recommend
    that you try to do it yourself. Don't worry if you fail; I'll show you a very
    simple way to do it in the next paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you were able to scale the data on your own, then congratulations! If not,
    follow along through this next section to see the answer. You might have noticed
    that you imported a class of Scikit-learn library called `MinMaxScaler`. You can
    use that class to scale the variables with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This code creates two scalers, one to scale the features and one to scale the
    targets. Call them `xscaler` and `yscaler`. The `feature_range` argument is the
    range to which you want your data to be scaled (from 0 to 1 in your case).
  prefs: []
  type: TYPE_NORMAL
- en: Then you use the `fit_transform` method, which scales `X_train` and `y_train`
    and adjusts the scalers based on these sets (`fit` part of this method sets *x*[min]
    and *x*[max]). After that you use the `transform` method to scale `X_test` and
    `y_test` without adjusting `yscaler` and `xscaler`.
  prefs: []
  type: TYPE_NORMAL
- en: When scaling the `y` variables, you have to reshape them by using `.reshape(-1,1)`
    in order to create a fake second dimension (so the code can treat this one-dimensional
    array as a two-dimensional array with one column). We need this fake second dimension
    to avoid a format error.
  prefs: []
  type: TYPE_NORMAL
- en: If you still do not understand why we have to use scaling, please read this
    section once again. It'll also get clearer once we go through the theory.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can proceed to building a neural network! Keep in mind that all
    the theory behind it will be covered later in the chapter, so don't be scared
    if you have trouble understanding something.
  prefs: []
  type: TYPE_NORMAL
- en: Building the neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To build the neural network, you can use a highly reliable and easy to use
    library called Keras. Let''s get straight into coding it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In line 35 of the code block you instantiate your model by using the `Sequential`
    class from the Keras library.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you add a line that adds a new layer with 64 neurons to your neural network.
    `kernel_initializer` is an argument that defines the way the initial weights are
    created in the layer, `activation` is the activation function of this layer and
    `input_dim` is the size of the input; in your case, these are the 17 features
    that define how much a house costs.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you add two more layers, one with 16 neurons and one with 1 neuron that
    will be the output of the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you don't know what I'm talking about right now, what activations, losses,
    and optimizers are, you don't have to worry. You'll understand them soon, when
    we get to the theory later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Training the neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you've built your model, you can finally train it!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This simple one-liner is responsible for learning.
  prefs: []
  type: TYPE_NORMAL
- en: As the first two arguments of this fit method, you input `X_train` and `y_train`
    which are the sets your model will be trained on. Then you have an argument called
    `batch_size`; this defines after how many records in your dataset you update your
    weights (loss is summed up and back-propagated after `batch_size` inputs). Next
    you have `epochs`, and this value defines how many times you teach your model
    on the entire `X_train` and `y_train` set. The final argument is `validation_data`,
    and there, as you can see, you put `X_test` and `y_test`. This means that after
    every epoch, your model will be tested on this set, but it won't learn from it.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You're nearly there; you have just one last non-obligatory step to take. You
    calculate the absolute error on the test set and see its real, unscaled predictions
    (actual prices, not in the range (0,1)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You rescale back your `y_test` on line 45\. Then, you make a prediction on your
    test set of features and rescale it back too, since the predictions are also scaled
    down.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last two lines you calculate the absolute error using the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Since both prediction and `y_test` are NumPy arrays, you can divide them by
    simply using the `/` symbol. In the last line, you calculate the mean error using
    a NumPy function.
  prefs: []
  type: TYPE_NORMAL
- en: Superb! Now that you have it all finished, you can finally run this code and
    see the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Results'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the last line, your result is shown. In my case the average
    error was 13.5%. That is a really good result!
  prefs: []
  type: TYPE_NORMAL
- en: Now we can get into the theory behind deep learning, and find out how a neural
    network really works.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is our plan of attack to go pro and tackle deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: The neuron
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The activation function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do neural networks work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do neural networks learn?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forward-propagation and back-propagation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient descent, including Batch, Stochastic, and Mini-Batch methods
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I hope you're excited about this section—deep learning is an awesome and powerful
    field to study.
  prefs: []
  type: TYPE_NORMAL
- en: The neuron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The neuron is the basic building block of Artificial Neural Networks, and they
    are based on the neuron cells found the brain.
  prefs: []
  type: TYPE_NORMAL
- en: Biological neurons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the following images are real-life neurons that have been smeared onto a
    slide, colored a little bit, and observed through a microscope:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The neuron'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, they have the structure of a central body with lots of different
    branches coming out of it. The question is: How can we recreate that in a machine?
    We really want to recreate it in a machine, since the whole purpose of deep learning
    is to mimic how the human brain works in the hope that by doing so we create something
    amazing: a powerful infrastructure for learning machines.'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we hope for that? Because the human brain just happens to be one of the
    most powerful learning tools on the planet. We hope that if we recreate it, then
    we'll have something just as awesome as that.
  prefs: []
  type: TYPE_NORMAL
- en: Our challenge right now, our very first step in creating artificial neural networks,
    is to recreate a neuron. So how do we do it? Well, first of all let's take a closer
    look at what a neuron actually is.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 1899, the neuroscientist Santiago Ramón y Cajal dyed neurons in actual brain
    tissue, and looked at them under a microscope. While he was looking at them, he
    drew what he saw, which was something very much like the slides we looked at before.
    Today, technology has advanced quite a lot, allowing us to see neurons much more
    closely and in more detail. That means that we can draw what they look like diagrammatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The neuron''s structure'
  prefs: []
  type: TYPE_NORMAL
- en: This neuron exchanges signals between its neighbor neurons. The dendrites are
    the receivers of the signal and the axon is the transmitter of the signal.
  prefs: []
  type: TYPE_NORMAL
- en: The dendrites of the neuron are connected to the axons of other neurons above
    it. When the neuron fires, the signal travels down its axon and passes on to the
    dendrites of the next neuron. That is how they are connected, and how a neuron
    works. Now we can move from neuroscience to technology.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neurons
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here''s how a neuron is represented inside an Artificial Neural Network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: An Artificial Neural Network with a single neuron'
  prefs: []
  type: TYPE_NORMAL
- en: Just like a human neuron, it gets some input signals and it has an output signal.
    The blue arrow connecting the input signals to the neuron, and the neuron to the
    output signal, are like the synapses in the human neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here in the artificial neuron, what exactly are the input and output signals
    going to be? The input signals are the scaled independent variables composing
    the states of the environment. For example, in the server cooling practical example
    we''ll code later in this book (*Chapter 11*, *AI for Business – Minimize Costs
    with Deep Q-Learning*), these are the temperature of the server, the number of
    users, and the rate of data transmission. The output signal is the output values,
    which in a deep Q-learning model are always the Q-Values. Knowing all that, we
    can make a general representation of a neuron for machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Neuron – The output values'
  prefs: []
  type: TYPE_NORMAL
- en: 'To finish describing the neuron, we need to add the last element missing from
    this representation, which is also the most important one: the weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each synapse (blue arrow) is attributed a weight. The larger the weight, the
    stronger the signal is through the synapse. What is fundamental to understand
    is that these weights are what the machine updates over time to improve its predictions.
    Let''s add them to the previous graphic, to make sure you can visualize them well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Neuron – The weights'
  prefs: []
  type: TYPE_NORMAL
- en: That's the neuron. The next thing to understand is the activation function;
    the way the neuron decides what output to produce given a set of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The activation function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The activation function is the function *![](img/B14110_09_050.png)*, operating
    inside the neuron, that takes as inputs the linear sum of the input values multiplied
    by their associated weights, and that returns the output value as shown in the
    following graphic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The activation function'
  prefs: []
  type: TYPE_NORMAL
- en: 'such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Your next question is probably: what exactly is the function *![](img/B14110_09_050.png)*?'
  prefs: []
  type: TYPE_NORMAL
- en: 'There can be many of them, but here we''ll describe the three most used ones,
    including the one you''ll use in the practical activity:'
  prefs: []
  type: TYPE_NORMAL
- en: The threshold activation function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The sigmoid activation function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rectifier activation function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's push your expertise further by having a look at them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: The threshold activation function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The threshold activation function is simply defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and can be represented by the following curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The threshold activation function'
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the signal passing through the neuron is discontinuous, and
    will only be activated if:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_0011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s have a look at the next activation function: the sigmoid activation
    function. The sigmoid activation function is the most effective and widely used
    one in Artificial Neural Networks, but mostly in the last hidden layer that leads
    to the output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid activation function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The sigmoid activation function is defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and can be represented by the following curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: The sigmoid activation function'
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the signal passing through the neuron is continuous and will
    always be activated. And the higher the value of:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_008.png)'
  prefs: []
  type: TYPE_IMG
- en: the stronger the signal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s have a look at another widely used activation function: the rectifier
    activation function. You''ll find it in most of the deep neural networks, but
    mostly inside the early hidden layers, as opposed to the sigmoid function, which
    is rather used for the last hidden layer leading to the output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: The rectifier activation function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The rectifier activation function is simply defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and is therefore represented by the following curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: The rectifier activation function'
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the signal passing through the neuron is continuous, and will
    only be activated if:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_0012.png)'
  prefs: []
  type: TYPE_IMG
- en: The higher the weighted sum of inputs, the stronger the signal.
  prefs: []
  type: TYPE_NORMAL
- en: 'That raises the question: which activation function should you choose, or,
    as it''s more frequently asked, how do you know which one to choose?'
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that the answer is simple. It actually depends on what gets
    returned as the dependent variable. If it's a binary outcome, 0 or 1, then a good
    choice would be the threshold activation function. If what you want returned is
    the probability that the dependent variable is 1, then the sigmoid activation
    function is an excellent choice, since its sigmoid curve is a perfect fit to model
    probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, here''s the small blueprint highlighted in this figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Activation function blueprint'
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the rectifier activation function should be used within the hidden
    layers of a deep neural network with more than one hidden layer, and the sigmoid
    activation function should be used in the last hidden layer leading to the output
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s highlight this in the following figure so that you can visualize it
    and remember it better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Different activation functions in different layers'
  prefs: []
  type: TYPE_NORMAL
- en: We're progressing fast! You already know quite a lot about deep learning. It's
    not over yet though—let's move on to the next section to explain how neural networks
    actually work.
  prefs: []
  type: TYPE_NORMAL
- en: How do neural networks work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To explain this, let''s go back to the problem of predicting real estate prices.
    We had some independent variables which we were using to predict the price of
    houses and apartments. For simplicity''s sake, and to be able to represent everything
    in a graph, let''s say that our only independent variables (our predictors) are
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Area (square feet)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Number of bedrooms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distance to city (miles)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Age
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our dependent variable is the apartment price that we're predicting. Here's
    how the magic works in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: A weight is attributed to each of the independent, scaled variables in such
    a way that the higher the weight is, the more of an effect the independent variable
    will have on the dependent variable; that is, the stronger a predictor it will
    be of the dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: As soon as new inputs enter the neural network, the signals are forward-propagated
    from each of the inputs, reaching the neurons of the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Inside each neuron of the hidden layer, the activation function is applied,
    so that the lower the weight of the input, the more the activation function blocks
    the signal coming from that input, and the higher the weight of that input, the
    more the activation function lets that signal go through.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, all the signals coming from the hidden neurons, more or less blocked
    by the activation functions, are forward propagated to the output layer, to return
    the final outcome: the price prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a visualization of how that neural network works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: How Neural Networks work – Example in real estate price prediction'
  prefs: []
  type: TYPE_NORMAL
- en: That covers half of the story. Now we know how a neural network works, we need
    to find out how it learns.
  prefs: []
  type: TYPE_NORMAL
- en: How do neural networks learn?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Neural networks learn by updating, over many iterations, the weights of all
    the inputs and hidden neurons (when having several hidden layers), always towards
    the same goal: to reduce the loss error between the predictions and the actual
    values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for neural networks to learn, we need the actual values, which are
    also called the targets. In our preceding example about real estate pricing, the
    actual values are the real prices of the houses and apartments taken from our
    dataset. These real prices depend on the independent variables listed previously
    (area, number of bedrooms, distance to city, and age), and the neural network
    learns to make better predictions of these prices, by running the following process:'
  prefs: []
  type: TYPE_NORMAL
- en: The neural network forward propagates the signals coming from the inputs; independent
    variables ![](img/B14110_09_011.png), ![](img/B14110_09_012.png), ![](img/B14110_09_013.png)
    and ![](img/B14110_09_014.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then it gets the predicted price ![](img/B14110_09_015.png) in the output layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then it computes the loss error, *C*, between the predicted price ![](img/B14110_09_016.png)
    (prediction) and the actual price *y* (target):![](img/B14110_09_017.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then this loss error is back-propagated inside the neural network, from right
    to left in our representation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, on each of the neurons, the neural network runs a technique called gradient
    descent (which we will discuss in the next section) to update the weights in the
    direction of loss reduction, that is, into new weights which reduce the loss error
    *C*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then this whole process is repeated many times, with each time new inputs and
    new targets, until we get the desired performance (early stopping) or the last
    iteration (the number of iterations chosen in the implementation).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's show the two main phases, forward-propagation and back-propagation, of
    this whole process in two separate graphics in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Forward-propagation and back-propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Phase 1: Forward-propagation**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how the signal is forward-propagated throughout the artificial neural
    network, from the inputs to the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: Forward-propagation'
  prefs: []
  type: TYPE_NORMAL
- en: Once the signal's been propagated through the entire network, the loss error
    *C* is calculated so that it can be back-propagated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Phase 2: Back-propagation**:'
  prefs: []
  type: TYPE_NORMAL
- en: And after forward-propagation comes back-propagation, during which the loss
    error *C* is propagated back into the neural network from the output to the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Back-propagation'
  prefs: []
  type: TYPE_NORMAL
- en: During back-propagation, the weights are updated to reduce the loss error *C*
    between the predictions (output value) and the targets (actual value). How are
    they updated? This is where gradient descent comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Gradient descent is an optimization technique that helps us find the minimum
    of a cost function, like the preceding loss error *C* we had:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s visualize it in the most intuitive way, like the following ball in a
    bowl (with a little math sprinkled on top):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: Gradient Descent (1/4)'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine this is a cross section of a bowl, into which we drop a small red ball
    and let it find its way down to the bottom of the bowl. After some time, it will
    stop rolling, when it finds the sweet spot at the bottom of the bowl.
  prefs: []
  type: TYPE_NORMAL
- en: You can think about gradient descent in the same way. It starts somewhere in
    the bowl (initial values of parameters) and tries to find the bottom of the bowl,
    or in other words, the minimum of a cost function.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through the example that is shown in the preceding image. The initial
    values of the parameters have set our ball at the position shown. Based on that
    we get some predictions, which we compare to our target values. The difference
    between these two sets is our loss for the current set of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Then we calculate the first derivative of the cost function, with respect to
    the parameters. This is where the name **gradient** comes from. Here, this first
    derivative gives us the slope of the tangent to the curve where the ball is. If
    the gradient of the slope is negative, like on the preceding image, we take the
    next step to the right side. If the gradient of the slope is positive, we take
    the next step to the left side.
  prefs: []
  type: TYPE_NORMAL
- en: 'The name **descent** thus comes from the fact that we always take the next
    step that points downhill, as represented in the following graphic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: Gradient Descent (2/4)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next position our ball rests on a positive slope, so we have to take
    the next step to the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: Gradient Descent (3/4)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Eventually, by repeating the same steps, the ball will end up at the bottom
    of the bowl:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: Gradient Descent (4/4)'
  prefs: []
  type: TYPE_NORMAL
- en: 'And that''s it! That''s how gradient descent operates in one dimension (one
    parameter). Now you might ask: "Great, but how does this scale?" We saw an example
    of one-dimensional optimization, but what about two or even three dimensions?'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s an excellent question. gradient descent guarantees that this approach
    scales on as many dimensions as needed, provided the cost function is convex.
    In fact, if the cost function is convex, gradient descent will find the absolute
    minimum of the cost function. Following is an example in two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 26: Gradient Descent – Convergence guaranteed for convex cost functions'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if the cost function is not convex, gradient descent will only find
    a local minimum. Here is an example in three dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: Example of non-convergence (right) for a non-convex function (left)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand what gradient descent is all about, we can study the
    most advanced and most effective versions of it:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch gradient descent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stochastic gradient descent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mini-batch gradient descent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"Gradient descent", "batch gradient descent", "mini batch gradient descent",
    "stochastic gradient descent," there are so many terms and someone like you who''s
    just starting may find themselves very confused. Don''t worry—I''ve got your back.'
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between all of these versions of gradient descent is just
    the way we feed our data to a model, and how often we update our parameters (weights)
    to move our small red ball. Let's start by explaining batch gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Batch gradient descent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Batch gradient descent is when we have a batch of inputs (as opposed to a single
    input) feeding the neural network, forward-propagating them to obtain in the end
    a batch of predictions, which themselves are compared to a batch of targets. The
    global loss error between the predictions and the targets of the two batches is
    then computed as the sum of the loss errors between each prediction and its associated
    target.
  prefs: []
  type: TYPE_NORMAL
- en: That global loss is back-propagated into the neural network, where gradient
    descent or stochastic gradient descent is performed to update all the weights,
    according to how much they were responsible for that global loss error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of batch gradient descent. The problem to solve is about
    predicting the score (from 0 to 100 %) students get in an exam, based on the time
    spent studying (Study Hrs) and the time spent sleeping (Sleep Hrs):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 28: Batch Gradient Descent'
  prefs: []
  type: TYPE_NORMAL
- en: An important thing to note on this preceding graphic is that these are not multiple
    neural networks, but a single one represented by separate weight updates. As we
    can see in this example of batch gradient descent, we feed all of our data into
    the model at once.
  prefs: []
  type: TYPE_NORMAL
- en: 'This produces collective updates of the weights and fast optimization of the
    network. However, there is a bad side to this as well. There is, once again, the
    possibility of getting stuck in a local minimum, as we can see in the following
    graphic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 29: Getting stuck in a local minimum'
  prefs: []
  type: TYPE_NORMAL
- en: 'We explained the reason why this happens a bit earlier: it is because the cost
    function in the preceding graphic is not convex, and this type of optimization
    (simple gradient descent) requires the cost function to be convex. If that is
    not the case, we can find ourselves stuck in a local minimum and never find the
    global minimum with the optimal parameters. On the other hand, here is an example
    of a convex cost function, the same one as we saw earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 30: An example of a convex function'
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, a function is convex if it has only one global minimum. And
    the graph of a convex function has the bowl shape. However, in most problems,
    including business problems, the cost function will not be convex (as in the following
    graphic example in 3D), and thus not allow simple gradient descent to perform
    well. This is where stochastic gradient descent comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 31: Example of non-convergence (right) for a non-convex function (left)'
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Stochastic Gradient Descent** (**SGD**) comes to save the day. It provides
    better results overall, preventing the algorithm from getting stuck in a local
    minimum. However, as its name suggests, it is stochastic, or in other words, random.'
  prefs: []
  type: TYPE_NORMAL
- en: Because of this property, no matter how many times you run the algorithm, the
    process will always be slightly different, regardless of the initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'SGD does not run on the whole dataset at once, but instead input by input.
    The process goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Input a single observation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forward propagate that input to get a single prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss error between the prediction (output) and the target (actual
    value).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Back-propagate the loss error into the neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weights with gradient descent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 to 5 through the whole dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s show the first three iterations on the first three single inputs for
    the example we looked at earlier, predicting the scores in an exam:'
  prefs: []
  type: TYPE_NORMAL
- en: '**First input row of observation**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 32: Stochastic Gradient Descent – First input row of observation'
  prefs: []
  type: TYPE_NORMAL
- en: '**Second input row of observation**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 33: Stochastic Gradient Descent – Second input row of observation'
  prefs: []
  type: TYPE_NORMAL
- en: '**Third input row of observation**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 34: Stochastic Gradient Descent – Third input row of observation'
  prefs: []
  type: TYPE_NORMAL
- en: Each of the preceding three graphics is an example of one weight's update run
    by SGD. As we can see, each time we only input a single row of observation from
    our dataset to the neural network, then we update the weights accordingly and
    proceed to the next input row of observation.
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, SGD seems slower, because we input each row separately. In
    reality, it's much faster, because we don't have to load the whole dataset in
    the memory, nor wait for the whole dataset to pass through the model updating
    the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'To finish this section, let''s recap the difference between batch gradient
    descent and SGD with the following graphic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 35: Batch Gradient Descent vs. Stochastic Gradient Descent'
  prefs: []
  type: TYPE_NORMAL
- en: Now we can consider a middle-ground approach; mini-batch gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Mini-batch gradient descent
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mini-batch gradient descent uses the best from both worlds, combining batch
    gradient descent with SGD. This is done by feeding the artificial neural network
    with small batches of data, instead of feeding single input rows of observations
    one by one or the whole dataset at once.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is faster than classic SGD, and still prevents you from getting
    stuck in a local minimum. Mini-batch gradient descent also helps if you don't
    have enough computing resources to load the whole dataset in the memory, or enough
    processing power to get the full benefit of SGD.
  prefs: []
  type: TYPE_NORMAL
- en: That's all for neural networks! Now you're ready to combine your knowledge of
    neural networks with your knowledge of Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You've toured the foundations of deep learning, and you already know Q-learning;
    since deep Q-learning consists of combining Q-learning and deep learning, you're
    ready to get an intuitive grasp of deep Q-learning and crush it.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start, try to guess some of how this is going to work. I would like
    you to take a moment and think about how you could integrate Q-learning into an
    ANN.
  prefs: []
  type: TYPE_NORMAL
- en: First things first, you might have guessed what the inputs and outputs of the
    neural network are going to be. The input of the artificial neural network is
    of course going to be the input state, which could be a 1-dimensional vector encoding
    what is happening in the environment, or an image (like the ones seen by a self-driving
    car). And the output is going to be the set of Q-values for each action, meaning
    it is going to be a 1-dimensional vector of several Q-values, one for each action
    that can be performed. Then, just like before, the AI takes the action that has
    the maximum Q-value, and performs it.
  prefs: []
  type: TYPE_NORMAL
- en: Very simply, that means that instead of predicting the Q-values through iterative
    updates with the Bellman equation (simple Q-learning), we'll predict them with
    an ANN that takes as inputs the input states, and returns as output the Q-values
    of the different actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'That raises the question: it''s good that we know what to predict, but what
    are going to be the targets (actual values) of these predictions when we are training
    the AI? As a reminder, the target is the actual value, or what you want your prediction
    to be ideally: the closer your prediction is to the target, the more it is correct.
    That''s why we compute the loss error *C* between the prediction and the target,
    in order to reduce it through back-propagation with stochastic or mini-batch gradient
    descent.'
  prefs: []
  type: TYPE_NORMAL
- en: When we were doing simple property price prediction, it was obvious what the
    targets were. They were simply the prices in the dataset that were available to
    us. But what about the targets of Q-values when you are training a self-driving
    car, for example? It's not that obvious, even though it is an explicit function
    of the Q-values and the reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is a fundamental formula in deep Q-learning. The target of an input
    state ![](img/B14110_09_019.png) is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_020.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B14110_09_021.png) is the last reward obtained and ![](img/B14110_09_022.png)
    is the discount factor, as seen previously.
  prefs: []
  type: TYPE_NORMAL
- en: Do you recognize the formula of the target? If you remember Q-learning, you
    should have no problem answering this question.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s in the temporal difference, of course! Remember, the temporal difference
    is defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_07_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So now it''s obvious. The target is simply the first element at the left of
    the temporal difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'so that we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_025.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that at the beginning, the Q-values are null, so the target is simply the
    reward.
  prefs: []
  type: TYPE_NORMAL
- en: There's one more piece to the puzzle before we can say that we really understand
    deep Q-learning; the Softmax method.
  prefs: []
  type: TYPE_NORMAL
- en: The Softmax method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the missing piece before we're ready to assemble everything for deep
    Q-learning. The Softmax method is the way we're going to select the action to
    perform after predicting the Q-values. In Q-learning, that was simple; the action
    performed was the one with the highest Q-value. That was the argmax method. In
    deep Q-learning, things are different. The problems are usually more complex,
    and so, in order to find an optimal solution, we must go through a process called
    **Exploration**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploration consists of the following: instead of performing the action that
    has the maximum Q-value (which is called Exploitation), we''re going to give each
    action a probability proportional to its Q-value, such that the higher the Q-value,
    the higher the probability. This creates, exactly, a distribution of the performable
    actions. Then finally, the action performed will be selected as a random draw
    from that distribution. Let me explain with an example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s imagine we are building a self-driving car (we actually will, in *Chapter
    10, AI for Autonomous Vehicles - Build a Self-Driving Car*). Let''s say that the
    possible actions to perform are simple: move forward, turn left or turn right.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, at a specific time, let''s say that our AI predicts the following Q-values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Move Forward | Turn Left | Turn Right |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | 38 | 11 |'
  prefs: []
  type: TYPE_TB
- en: 'The way we can create the distribution of probabilities we need is by dividing
    each Q-value by the sum of the three Q-values, which results each time in the
    probability of a particular action. Let''s perform those sums:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_09_026.png)Probability of Turning Left![](img/B14110_09_027.png)![](img/B14110_09_028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Perfect—the probabilities sum to 1 and they are proportional to the Q-values.
    That gives us a distribution of the actions. To perform an action, the Softmax
    method takes a random draw from this distribution, such that:'
  prefs: []
  type: TYPE_NORMAL
- en: The action of Moving Forward has a 33% chance of being selected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The action of Turning Left has a 52% chance of being selected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The action of Turning Right has a 15% chance of being selected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can you feel the difference between Softmax and argmax, and do you understand
    why it is called Exploration instead of Exploitation? With argmax, the action
    *Turn Left* would be the one performed with absolute certainty. That's Exploitation.
    But with Softmax, even though the action *Turn Left* is the one with the highest
    chance of being selected, there's still a chance that the other actions might
    be selected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, of course, the question is: why do we want to do that? It''s simply because
    we want to explore the other actions, in case they lead to transitions resulting
    in higher rewards than we would obtain with pure exploitation. That often happens
    with complex problems, which are the ones for which deep Q-learning is used to
    find a solution. deep Q-learning finds that solution thanks to its advanced model,
    but also through exploration of the actions. This is a technique in AI called
    Policy Exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: As before, the next step is a step back. We're going to recap how deep Q-learning
    works.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-learning recap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep Q-learning consists of combining Q-learning with an ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs are encoded vectors, each one defining a state of the environment. These
    inputs go into an ANN, where the output contains the predicted Q-values for each
    action.
  prefs: []
  type: TYPE_NORMAL
- en: More precisely, if there are *n* possible actions the AI could take, the output
    of the artificial neural network is a 1D vector comprised of *n* elements, each
    one corresponding to the Q-values of each action that could be performed in the
    current state. Then, the action performed is chosen via the Softmax method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, in each state ![](img/B14110_09_029.png):'
  prefs: []
  type: TYPE_NORMAL
- en: The prediction is the Q-value ![](img/B14110_09_030.png), where ![](img/B14110_09_031.png)
    is performed by the Softmax method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The target is ![](img/B14110_09_032.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss error between the prediction and the target is the square of the temporal
    difference:![](img/B14110_09_033.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This loss error is back-propagated into the neural network, and the weights
    are updated according to how much they contributed to the error, through stochastic
    or mini-batch gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Experience replay
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You might noticed that so far we have only considered transitions from one state
    ![](img/B14110_07_018.png) to the next state ![](img/B14110_09_035.png). The problem
    with this is that ![](img/B14110_09_036.png) is most of the time very correlated
    with ![](img/B14110_09_037.png); therefore, the neural network is not learning
    much.
  prefs: []
  type: TYPE_NORMAL
- en: This could be improved if, instead of only considering the last transition each
    time, we considered the last *m* transitions, where *m* is a large number. This
    set of the last *m* transitions is what is called the experience replay memory,
    or simply memory. From this memory we sample some random transitions into small
    batches. Then we train the neural network with these batches to then update the
    weights through mini-batch gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: The whole deep Q-learning algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's summarize the different steps of the whole deep Q-learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the memory of the experience replay to an empty list *M*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a maximum size for the memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At each time *t*, we repeat the following process, until the end of the epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: Predict the Q-values of the current state ![](img/B14110_09_038.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the action selected by the Softmax method:![](img/B14110_09_039.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the reward ![](img/B14110_09_040.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reach the next state ![](img/B14110_09_041.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Append the transition ![](img/B14110_09_042.png) to the memory *M*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take a random batch **![](img/B14110_09_043.png)** of transitions. For all
    the transitions ![](img/B14110_09_044.png) of the random batch ![](img/B14110_09_045.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the predictions: ![](img/B14110_09_046.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get the targets: ![](img/B14110_09_047.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the loss between the predictions and the targets, over the whole batch
    ![](img/B14110_09_048.png):![](img/B14110_09_049.png)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Back-propagate this loss error back into the neural network, and through stochastic
    gradient descent, update the weights according to how much they contributed to
    the loss error.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You''ve just unlocked the full deep Q-learning process! That means that you
    are now able to build powerful real-world AI applications in many fields. Here''s
    a tour of some of the applications where deep Q-learning can create significant
    added value:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Energy**: It was a deep Q-learning model that the DeepMind AI used to reduce
    Google''s Data Center cooling bill by 40%. Also, deep Q-learning can optimize
    the functioning of smart grids; in other words, it can make smart grids even smarter.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Transport**: Deep Q-learning can optimize traffic light control in order
    to reduce traffic.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Autonomous Vehicles**: Deep Q-learning can be used to build self-driving
    cars, which we will illustrate in the next chapter of this book.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Robotics**: Today, many advanced robots are built with deep Q-learning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**And much more**: Chemistry, recommender systems, advertising, and many more—even
    video games, as you''ll discover in *Chapter 13*, *AI for Games – Become the Master
    at Snake*, when you use deep convolutional Q-learning to train an AI to play Snake.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You learned a lot in this chapter; we first discussed ANNs. ANNs are built from
    neurons put in multiple layers. Each neuron from one layer is connected to every
    neuron from the previous layer, and every layer has its own activation function—a
    function that decides how much each output signal should be blocked.
  prefs: []
  type: TYPE_NORMAL
- en: 'The step in which an ANN works out the prediction is called forward-propagation
    and the step in which it learns is called back-propagation. There are three main
    types of back-propagation: batch gradient descent, stochastic gradient descent,
    and the best one, mini-batch gradient descent, which mixes the advantages of both
    previous methods.'
  prefs: []
  type: TYPE_NORMAL
- en: The last thing we talked about in this chapter was deep Q-learning. This method
    uses Neural Networks to predict the Q-Values of taking certain actions. We also
    mentioned the experience replay memory, which stores a huge chunk of experience
    for our AI.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you'll put all of this into practice by coding your very
    own self-driving car.
  prefs: []
  type: TYPE_NORMAL
