["```py\n$ sudo pip3 install termcolor\n\n```", "```py\nfrom collections import deque\nimport numpy as np\nimport argparse\nimport os\nimport time\nfrom termcolor import colored\n\nclass QWorld():\n    def __init__(self):\n        # 4 actions\n        # 0 - Left, 1 - Down, 2 - Right, 3 - Up\n        self.col = 4\n\n        # 6 states\n        self.row = 6\n\n        # setup the environment\n        self.q_table = np.zeros([self.row, self.col])\n        self.init_transition_table()\n        self.init_reward_table()\n\n        # discount factor\n        self.gamma = 0.9\n\n        # 90% exploration, 10% exploitation\n        self.epsilon = 0.9\n        # exploration decays by this factor every episode\n        self.epsilon_decay = 0.9\n        # in the long run, 10% exploration, 90% exploitation\n        self.epsilon_min = 0.1\n\n        # reset the environment\n        self.reset()\n        self.is_explore = True\n\n    # start of episode\n    def reset(self):\n        self.state = 0\n        return self.state\n\n    # agent wins when the goal is reached\n    def is_in_win_state(self):\n        return self.state == 2\n\n    def init_reward_table(self):\n        \"\"\"\n        0 - Left, 1 - Down, 2 - Right, 3 - Up\n        ----------------\n        | 0 | 0 | 100  |\n        ----------------\n        | 0 | 0 | -100 |\n        ----------------\n        \"\"\"\n        self.reward_table = np.zeros([self.row, self.col])\n        self.reward_table[1, 2] = 100.\n        self.reward_table[4, 2] = -100.\n\n    def init_transition_table(self):\n        \"\"\"\n        0 - Left, 1 - Down, 2 - Right, 3 - Up\n        -------------\n        | 0 | 1 | 2 |\n        -------------\n        | 3 | 4 | 5 |\n        -------------\n        \"\"\"\n        self.transition_table = np.zeros([self.row, self.col], dtype=int)\n\n        self.transition_table[0, 0] = 0\n        self.transition_table[0, 1] = 3\n        self.transition_table[0, 2] = 1\n        self.transition_table[0, 3] = 0\n\n        self.transition_table[1, 0] = 0\n        self.transition_table[1, 1] = 4\n        self.transition_table[1, 2] = 2\n        self.transition_table[1, 3] = 1\n\n        # terminal Goal state\n        self.transition_table[2, 0] = 2\n        self.transition_table[2, 1] = 2\n        self.transition_table[2, 2] = 2\n        self.transition_table[2, 3] = 2\n\n        self.transition_table[3, 0] = 3\n        self.transition_table[3, 1] = 3\n        self.transition_table[3, 2] = 4\n        self.transition_table[3, 3] = 0\n\n        self.transition_table[4, 0] = 3\n        self.transition_table[4, 1] = 4\n        self.transition_table[4, 2] = 5\n        self.transition_table[4, 3] = 1\n\n        # terminal Hole state\n        self.transition_table[5, 0] = 5\n        self.transition_table[5, 1] = 5\n        self.transition_table[5, 2] = 5\n        self.transition_table[5, 3] = 5\n\n    # execute the action on the environment\n    def step(self, action):\n        # determine the next_state given state and action\n        next_state = self.transition_table[self.state, action]\n        # done is True if next_state is Goal or Hole\n        done = next_state == 2 or next_state == 5\n        # reward given the state and action\n        reward = self.reward_table[self.state, action]\n        # the enviroment is now in new state\n        self.state = next_state\n        return next_state, reward, done\n\n    # determine the next action\n    def act(self):\n        # 0 - Left, 1 - Down, 2 - Right, 3 - Up\n        # action is from exploration\n        if np.random.rand() <= self.epsilon:\n            # explore - do random action\n            self.is_explore = True\n            return np.random.choice(4,1)[0]\n\n        # or action is from exploitation\n        # exploit - choose action with max Q-value\n        self.is_explore = False\n        return np.argmax(self.q_table[self.state])\n\n    # Q-Learning - update the Q Table using Q(s, a)\n    def update_q_table(self, state, action, reward, next_state):\n        # Q(s, a) = reward + gamma * max_a' Q(s', a')\n        q_value = self.gamma * np.amax(self.q_table[next_state])\n        q_value += reward\n        self.q_table[state, action] = q_value\n\n    # UI to dump Q Table contents\n    def print_q_table(self):\n        print(\"Q-Table (Epsilon: %0.2f)\" % self.epsilon)\n        print(self.q_table)\n\n    # update Exploration-Exploitation mix\n    def update_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n```", "```py\n# state, action, reward, next state iteration\nfor episode in range(episode_count):\n    state = q_world.reset()\n    done = False\n    print_episode(episode, delay=delay)\n    while not done:\n        action = q_world.act()\n        next_state, reward, done = q_world.step(action)\n        q_world.update_q_table(state, action, reward, next_state)\n        print_status(q_world, done, step, delay=delay)\n        state = next_state\n        # if episode is done, perform housekeeping\n        if done:\n            if q_world.is_in_win_state():\n                wins += 1\n                scores.append(step)\n                if wins > maxwins:\n                    print(scores)\n                    exit(0)\n            # Exploration-Exploitation is updated every episode\n            q_world.update_epsilon()\n            step = 1 \n        else:\n            step += 1\n\nprint(scores)\nq_world.print_q_table()\n```", "```py\n$ python3 q-learning-9.3.1.py\n\n```", "```py\n$ python3 q-learning-9.3.1.py --train\n\n```", "```py\n$ sudo pip3 install gym\n\n```", "```py\n$ python3 q-frozenlake-9.5.1.py -d -t=1\n\n```", "```py\npython3 q-frozenlake-9.5.1.py\n\n```", "```py\npython3 q-frozenlake-9.5.1.py -d\n\n```", "```py\npython3 q-frozenlake-9.5.1.py -e\n\n```", "```py\npython3 q-frozenlake-9.5.1.py -s\n\n```", "```py\npython3 q-frozenlake-9.5.1.py -s -d\n\n```", "```py\npython3 q-frozenlake-9.5.1.py -s -e\n\n```", "```py\nfrom collections import deque\nimport numpy as np\nimport argparse\nimport os\nimport time\nimport gym\nfrom gym import wrappers, logger\n\nclass QAgent():\n    def __init__(self,\n                 observation_space,\n                 action_space,\n                 demo=False,\n                 slippery=False,\n                 decay=0.99):\n\n        self.action_space = action_space\n        # number of columns is equal to number of actions\n        col = action_space.n\n        # number of rows is equal to number of states\n        row = observation_space.n\n        # build Q Table with row x col dims\n        self.q_table = np.zeros([row, col])\n\n        # discount factor\n        self.gamma = 0.9\n\n        # initially 90% exploration, 10% exploitation\n        self.epsilon = 0.9\n        # iteratively applying decay til 10% exploration/90% exploitation\n        self.epsilon_decay = decay\n        self.epsilon_min = 0.1\n\n        # learning rate of Q-Learning\n        self.learning_rate = 0.1\n\n        # file where Q Table is saved on/restored fr\n        if slippery:\n            self.filename = 'q-frozenlake-slippery.npy'\n        else:\n            self.filename = 'q-frozenlake.npy'\n\n        # demo or train mode \n        self.demo = demo\n        # if demo mode, no exploration\n        if demo:\n            self.epsilon = 0\n\n    # determine the next action\n    # if random, choose from random action space\n    # else use the Q Table\n    def act(self, state, is_explore=False):\n        # 0 - left, 1 - Down, 2 - Right, 3 - Up\n        if is_explore or np.random.rand() < self.epsilon:\n            # explore - do random action\n            return self.action_space.sample()\n\n        # exploit - choose action with max Q-value\n        return np.argmax(self.q_table[state])\n\n    # TD(0) learning (generalized Q-Learning) with learning rate\n    def update_q_table(self, state, action, reward, next_state):\n        # Q(s, a) += alpha * (reward + gamma * max_a' Q(s', a') - Q(s, a))\n        q_value = self.gamma * np.amax(self.q_table[next_state])\n        q_value += reward\n        q_value -= self.q_table[state, action]\n        q_value *= self.learning_rate\n        q_value += self.q_table[state, action]\n        self.q_table[state, action] = q_value\n\n    # dump Q Table\n    def print_q_table(self):\n        print(self.q_table)\n        print(\"Epsilon : \", self.epsilon)\n\n    # save trained Q Table\n    def save_q_table(self):\n        np.save(self.filename, self.q_table)\n\n    # load trained Q Table\n    def load_q_table(self):\n        self.q_table = np.load(self.filename)\n\n    # adjust epsilon\n    def update_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n```", "```py\n# loop for the specified number of episode\nfor episode in range(episodes):\n    state = env.reset()\n    done = False\n    while not done:\n        # determine the agent's action given state\n        action = agent.act(state, is_explore=args.explore)\n        # get observable data\n        next_state, reward, done, _ = env.step(action)\n        # clear the screen before rendering the environment\n        os.system('clear')\n        # render the environment for human debugging\n        env.render()\n        # training of Q Table\n        if done:\n            # update exploration-exploitation ratio\n            # reward > 0 only when Goal is reached\n            # otherwise, it is a Hole\n            if reward > 0:\n                wins += 1\n\n        if not args.demo:\n            agent.update_q_table(state, action, reward, next_state)\n            agent.update_epsilon()\n\n        state = next_state\n        percent_wins = 100.0 * wins / (episode + 1)\n        print(\"-------%0.2f%% Goals in %d Episodes---------\"\n              % (percent_wins, episode))\n        if done:\n            time.sleep(5 * delay)\n        else:\n            time.sleep(delay)\n```", "```py\n# policy prediction for a given state\nq_values = self.q_model.predict(state)\n\n# get Q_max\nq_value = self.get_target_q_value(next_state)\n\n# correction on the Q value for the action used\nq_values[0][action] = reward if done else q_value\n```", "```py\nfrom keras.layers import Dense, Input\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom collections import deque\nimport numpy as np\nimport random\nimport argparse\nimport gym\nfrom gym import wrappers, logger\n\nclass DQNAgent():\n    def __init__(self, state_space, action_space, args, episodes=1000):\n\n        self.action_space = action_space\n\n        # experience buffer\n        self.memory = []\n\n        # discount rate\n        self.gamma = 0.9\n\n        # initially 90% exploration, 10% exploitation\n        self.epsilon = 0.9\n        # iteratively applying decay til 10% exploration/90% exploitation\n        self.epsilon_min = 0.1\n        self.epsilon_decay = self.epsilon_min / self.epsilon\n        self.epsilon_decay = self.epsilon_decay ** (1\\. / float(episodes))\n\n        # Q Network weights filename\n        self.weights_file = 'dqn_cartpole.h5'\n        # Q Network for training\n        n_inputs = state_space.shape[0]\n        n_outputs = action_space.n\n        self.q_model = self.build_model(n_inputs, n_outputs)\n        self.q_model.compile(loss='mse', optimizer=Adam())\n        # target Q Network\n        self.target_q_model = self.build_model(n_inputs, n_outputs)\n        # copy Q Network params to target Q Network\n        self.update_weights()\n\n        self.replay_counter = 0\n        self.ddqn = True if args.ddqn else False\n        if self.ddqn:\n            print(\"----------Double DQN--------\")\n        else:\n            print(\"-------------DQN------------\")\n\n    # Q Network is 256-256-256 MLP\n    def build_model(self, n_inputs, n_outputs):\n        inputs = Input(shape=(n_inputs, ), name='state')\n        x = Dense(256, activation='relu')(inputs)\n        x = Dense(256, activation='relu')(x)\n\t   x = Dense(256, activation='relu')(x)\n        x = Dense(n_outputs, activation='linear', name='action')(x)\n        q_model = Model(inputs, x)\n        q_model.summary()\n        return q_model\n\n    # save Q Network params to a file\n    def save_weights(self):\n        self.q_model.save_weights(self.weights_file)\n\n    def update_weights(self):\n        self.target_q_model.set_weights(self.q_model.get_weights())\n\n    # eps-greedy policy\n    def act(self, state):\n        if np.random.rand() < self.epsilon:\n            # explore - do random action\n            return self.action_space.sample()\n\n        # exploit\n        q_values = self.q_model.predict(state)\n        # select the action with max Q-value\n        return np.argmax(q_values[0])\n\n    # store experiences in the replay buffer\n    def remember(self, state, action, reward, next_state, done):\n        item = (state, action, reward, next_state, done)\n        self.memory.append(item)\n\n    # compute Q_max\n    # use of target Q Network solves the non-stationarity problem\n    def get_target_q_value(self, next_state):\n        # max Q value among next state's actions\n        if self.ddqn:\n            # DDQN\n            # current Q Network selects the action\n            # a'_max = argmax_a' Q(s', a')\n            action = np.argmax(self.q_model.predict(next_state)[0])\n            # target Q Network evaluates the action\n            # Q_max = Q_target(s', a'_max)\n            q_value = self.target_q_model.predict(next_state)[0][action]\n        else:\n            # DQN chooses the max Q value among next actions\n            # selection and evaluation of action is on the \n\t\t  # target Q Network\n            # Q_max = max_a' Q_target(s', a')\n            q_value = np.amax(self.target_q_model.predict(next_state)[0])\n\n        # Q_max = reward + gamma * Q_max\n        q_value *= self.gamma\n        q_value += reward\n        return q_value\n\n    # experience replay addresses the correlation issue between samples\n    def replay(self, batch_size):\n        # sars = state, action, reward, state' (next_state)\n        sars_batch = random.sample(self.memory, batch_size)\n        state_batch, q_values_batch = [], []\n\n        # fixme: for speedup, this could be done on the tensor level\n        # but easier to understand using a loop\n        for state, action, reward, next_state, done in sars_batch:\n            # policy prediction for a given state\n            q_values = self.q_model.predict(state)\n\n            # get Q_max\n            q_value = self.get_target_q_value(next_state)\n\n            # correction on the Q value for the action used\n            q_values[0][action] = reward if done else q_value\n\n            # collect batch state-q_value mapping\n            state_batch.append(state[0])\n            q_values_batch.append(q_values[0])\n\n        # train the Q-network\n        self.q_model.fit(np.array(state_batch),\n                         np.array(q_values_batch),\n                         batch_size=batch_size,\n                         epochs=1,\n                         verbose=0)\n\n        # update exploration-exploitation probability\n        self.update_epsilon()\n        # copy new params on old target after every 10 training updates\n        if self.replay_counter % 10 == 0:\n            self.update_weights()\n\n        self.replay_counter += 1\n\n    # decrease the exploration, increase exploitation\n    def update_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n```", "```py\n# Q-Learning sampling and fitting\nfor episode in range(episode_count):\n    state = env.reset()\n    state = np.reshape(state, [1, state_size])\n    done = False\n    total_reward = 0 \n    while not done:\n        # in CartPole-v0, action=0 is left and action=1 is right\n        action = agent.act(state)\n        next_state, reward, done, _ = env.step(action)\n        # in CartPole-v0:\n        # state = [pos, vel, theta, angular speed]\n        next_state = np.reshape(next_state, [1, state_size])\n        # store every experience unit in replay buffer\n        agent.remember(state, action, reward, next_state, done)\n        state = next_state\n        total_reward += reward\n\n    # call experience relay\n    if len(agent.memory) >= batch_size:\n        agent.replay(batch_size)\n\n    scores.append(total_reward)\n    mean_score = np.mean(scores)\n    if mean_score >= win_reward[args.env_id] and episode >= win_trials:\n        print(\"Solved in episode %d: Mean survival = %0.2lf in %d episodes\"\n              % (episode, mean_score, win_trials))\n        print(\"Epsilon: \", agent.epsilon)\n        agent.save_weights()\n        break\n    if episode % win_trials == 0:\n        print(\"Episode %d: Mean survival = %0.2lf in %d episodes\" %\n              (episode, mean_score, win_trials))\n```", "```py\n# compute Q_max\n# use of target Q Network solves the non-stationarity problem\ndef get_target_q_value(self, next_state):\n    # max Q value among next state's actions\n if self.ddqn:\n # DDQN\n # current Q Network selects the action\n # a'_max = argmax_a' Q(s', a')\n action = np.argmax(self.q_model.predict(next_state)[0])\n # target Q Network evaluates the action\n # Q_max = Q_target(s', a'_max)\n q_value = self.target_q_model.predict(next_state)[0][action]\n    else:\n        # DQN chooses the max Q value among next actions\n        # selection and evaluation of action is on the target Q Network\n        # Q_max = max_a' Q_target(s', a')\n        q_value = np.amax(self.target_q_model.predict(next_state)[0])\n\n    # Q_max = reward + gamma * Q_max\n    q_value *= self.gamma\n    q_value += reward\n    return q_value\n```", "```py\n$ python3 dqn-cartpole-9.6.1.py -d\n\n```"]