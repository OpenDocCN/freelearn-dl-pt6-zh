- en: Playing GridWorld Game Using Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As human beings, we learn from experiences. We have not become so charming overnight
    or by accident. Years of compliments as well as criticism have all helped shape
    who we are today. We learn how to ride a bike by trying out different muscle movements
    until it just clicks. When you perform actions, you are sometimes rewarded immediately,
    and this is known as **reinforcement learning** **(****RL****).**
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is all about designing a machine learning system driven by criticisms
    and rewards. We will see how to develop a demo GridWorld game using **Deeplearning4j**
    (**DL4J**), **reinforcement learning 4j** (**RL4J**), and Neural Q-learning that
    acts as the Q function. We will start from reinforcement learning and its theoretical
    background so that the concept is easier to grasp. In summary, the following topics
    will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Notation, policy, and utility in reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Q-learning algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a GridWorld game using deep Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequently asked questions (FAQs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notation, policy, and utility for RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whereas supervised and unsupervised learning appear at opposite ends of the
    spectrum, RL exists somewhere in the middle. It is not supervised learning, because
    the training data comes from the algorithm deciding between exploration and exploitation.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, it is not unsupervised, because the algorithm receives feedback
    from the environment. As long as you are in a situation where performing an action
    in a state produces a reward, you can use reinforcement learning to discover a
    good sequence of actions to take the maximum expected rewards. The goal of an
    RL agent will be to maximize the total reward that it receives in the end. The
    third main sub-element is the value function.
  prefs: []
  type: TYPE_NORMAL
- en: While rewards determine an immediate desirability of states, values indicate
    the long-term desirability of states, taking into account the states that may
    follow and the available rewards in those states. The value function is specified
    with respect to the chosen policy. During the learning phase, an agent tries actions
    that determine the states with the highest value, because these actions will get
    the best amount of reward in the end.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning techniques are being used in many areas. A general idea
    that is being pursued right now is creating an algorithm that does not need anything
    apart from a description of its task. When this kind of performance is achieved,
    it will be applied virtually everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: Notations in reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may notice that reinforcement learning jargon involves incarnating the algorithm
    into taking actions in situations to receive rewards. In fact, the algorithm is
    often referred to as an agent that acts with the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can just think it is an intelligent hardware agent sensing with sensors
    and interact with the environment using its actuators. Therefore, it should not
    be a surprise that much of RL theory is applied in robotics. Now, to prolong our
    discussion further, we need to know a few terminologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Environment**: This is the system having multiple states and mechanisms to
    transition in between states. For example, for a GridWorld game playing agent''s
    the environment is the grid space itself that defines the states and the way the
    agent gets rewarded to reach the goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agent**: This is an autonomus system that interacts with the environment.
    For example, in our GridWorld game, an agent is the player.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State**: A state in an environment is a set of variables that fully describe
    the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Goal**: It is also a state, which provides a higher discounted cumulative
    reward than any other state. For our GridWorld game, the goal is the state where
    the player wants to reach ultimately, but by accumulating the highest possible
    rewards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action**: Actions define the transition between different states. Thus, upon
    execution of an action, an agent can be rewarded or punished from the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy**: This defines a set of rules based on actions to be performed and
    executed for a given state in the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward**: This is a positive or negative quantity (that is score) for good
    and bad action/move respectively. Ultimately, the learning goal is reaching the
    goal with maximum score (reward). This way, rewards are essentially the training
    set for an agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Episode (also known as trials)**: This is the number of steps necessary to
    reach the goal state from the initial state (that is, position of an agent).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will discuss more on policy and utility later in this section. The following
    diagram demonstrates the interplay between states, actions, and rewards. If you
    start at state *s[1]*, you can perform action *a[1]* to obtain a reward *r (s[1],
    a[1])*. Arrows represent actions, and states are represented by circles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ff64270-027c-4839-b06b-01795d189ff3.png)'
  prefs: []
  type: TYPE_IMG
- en: When an agent performs an action, the state produces a reward
  prefs: []
  type: TYPE_NORMAL
- en: A robot performs actions to change between different states. But how does it
    decide which action to take? Well, it is all about using a different or a concrete
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: Policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In reinforcement learning, a policy is a set of rules or a strategy. Therefore,
    one of the learning outcomes is to discover a good strategy that observes the
    long-term consequences of actions in each state. So, technically, a policy defines
    an action to be taken in a given state. The following diagram shows the optimal
    action given any state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2762d760-cfcf-4ac5-9d6b-fba6784b50fd.png)'
  prefs: []
  type: TYPE_IMG
- en: A policy defines an action to be taken in a given state
  prefs: []
  type: TYPE_NORMAL
- en: 'The short-term consequence is easy to calculate:It is just the reward. Although
    performing an action yields an immediate reward, it is not always a good idea
    to choose the action greedily with the best reward. There may be different types
    of policies dependeing upon your RL problem formulation, as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: When an agent always try to achieve the highest immediate reward by performing
    an action, we call this **greedy policy**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If an action is performed arbitrarily, the policy is called **random policy**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a neural network learns a policy for picking actions by updating weights
    through backpropopagation and explicit feedback from the environment, we call
    this **policy gradients**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we want to come up with a robust a policy to solve an RL problem, we have
    to find the optimal one that performs better than that of random and greedy policies.
    In this chapter, we will see why policy gradient is more direct and optimistic.
  prefs: []
  type: TYPE_NORMAL
- en: Utility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The long-term reward is the **utility**. To decide which action to take, an
    agent can the action that produces the highest utility in a greedy way. The utility
    of performing an action *a* at a state *s* is written as a function *Q(s, a)*,
    called the utility function. The utility function predicts the immediate and final
    rewards based on an optimal policy generated by the input consisting of state
    and action, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b01b6793-f371-4a12-96a4-7a9f74c661e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Using a utility function
  prefs: []
  type: TYPE_NORMAL
- en: Neural Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most reinforcement learning algorithms boil down to just three main steps:
    infer, do, and learn. During the first step, the algorithm selects the best action
    *a* in a given state *s* using the knowledge it has so far. Next, it performs
    an action to find the reward *r* as well as the next state *s''*.'
  prefs: []
  type: TYPE_NORMAL
- en: Then it improves its understanding of the world using the newly acquired knowledge
    *(s, r, a, s')*. These steps can be formulated even better using QLearning algorithms,
    which is more or less at the core of Deep Reinforcement Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to QLearning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Computing the acquired knowledge using *(s, r, a, s'')* is just a naive way
    to calculate the utility. So, we need to find a more robust way to compute it
    in such that we calculate the utility of a particular state-action pair *(s, a)*
    by recursively considering the utilities of future actions. The utility of your
    current action is influenced by not only the immediate reward but also the next
    best action, as shown in the following formula, called **Q-function**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54fe0db0-e6ab-426a-a24a-fbe697d49bcf.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous formula, *s'* denotes the next state, *a'* denotes the next
    action, and the reward of taking action *a* in state *s* is denoted by *r(s, a).*
    Whereas*,* γ is a hyperparameter called the **discount factor**. If *γ* is *0*,
    then the agent chooses a particular action that maximizes the immediate reward.
    Higher values of *γ* will make the agent put more importance on considering long-term
    consequences.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we have more such hyperparameters to be considered. For example,
    if a vacuum cleaner robot is expected to learn to solve tasks quickly but not
    necessarily optimally, we may want to set a faster learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, if a robot is allowed more time to explore and exploit, we might
    tune down the learning rate. Let us call the learning rate *α*, and change our
    utility function as follows (note that when *α = 1*, both the equations are identical):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52146aac-6bcb-412c-b6f9-b5d38208f74a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In summary, an RL problem can be solved if we know this *Q(s, a)* function.
    This motivates researchers to propose a more advanced **QLearning** algorithm
    called **neural QLearning**, which is a type of algorithm used to calculate state-action
    values. It falls under the class of **temporal difference** (**TD**) algorithms,
    which suggests that time differences between actions taken and rewards received
    are involved.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks as a Q-function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we know the state and the action to perform. However, the `QLearning` agent
    needs to know the search space of the form (states x actions). The next step consists
    of creating the graph or search space, which is the container responsible for
    any sequence of states. The `QLSpace` class defines the search space (states x
    actions) for the `QLearning` algorithm, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20fae0f1-19b4-47ff-a3f5-b8a228aec422.png)'
  prefs: []
  type: TYPE_IMG
- en: State transition matrix with QLData (Q-value, reward, probability)
  prefs: []
  type: TYPE_NORMAL
- en: 'The end user with a list of states and actions can provide the search space.
    Alternatively, it is automatically created by providing the number of states,
    by taking the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**States**: The sequence of all possible states defined in the Q-learning search
    space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Goals**: A list of identifiers of states that are goals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, classical notation of such a search space (or lookup table) is sometimes
    not efficient; as in most interesting problems, our state-action space is much
    too large to store in a table, for example, the *Pac-Man* game. Rather we need
    to generalize and pattern-match between states anyway. In other words, we need
    our Q-learning algorithm to say, *The value of this kind of state is X* instead
    of saying, *the value of this exact, super-specific state is X*.
  prefs: []
  type: TYPE_NORMAL
- en: Here neural-network-based Q-learning can be used instead of a lookup table as
    our *Q*(*s*, *a*) such that it accepts a state *s* and an action *a* and spits
    out the value of that state-action. However, as I alluded to earlier, an NN sometimes
    has millions of parameters associated with it. These are the weights. Therefore,
    our *Q* function actually looks like *Q*(*s*, *a*, *θ*), where *θ* is a vector
    of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of iteratively updating values in a table, we will iteratively update
    the *θ* parameters of our neural network so that it learns to provide us with
    better estimates of state-action values. By the way, we can use gradient descent
    (backpropagation) to train such a deep Q-learning network just like any other
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the state (search space) is represented by an image, a neural
    network can rank the possible actions made by the agent such that it can predict
    the possible reward. For example, running left returns five points, jumping up
    returns seven, and jumping down returns two points, but running left returns none.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2dd9f59b-8fc8-4f23-a414-c19e938a5bc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Using a neural network for reinforcement learning-based gaming
  prefs: []
  type: TYPE_NORMAL
- en: To make this happen, instead of having to run our network forward for every
    action, we can run it forward once we just need to get the *max Q*(*s*′,*a*′),
    that is, *max Q* values for every possible action in the new state *s'*.
  prefs: []
  type: TYPE_NORMAL
- en: We will see how to create a deep Q-learning network like this with `MultiLayerNetwork`
    and the `MultiLayerConfiguration` configuration of DL4J. Therefore, the neural
    network will serve as our Q*-*function. Now that we have minimal theoretical knowing
    about RL and Q-learning, it is time to get to coding.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a GridWorld game using a deep Q-network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now start diving into **Deep Q-Network** (**DQN**) to train an agent
    to play GridWorld, which is a simple text-based game. There is a 4 x 4 grid of
    tiles and four objects are placed. There is an agent (a player), a pit, a goal,
    and a wall.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/109cb766-7ba7-4c1d-b8b6-4d53282de1e5.png)'
  prefs: []
  type: TYPE_IMG
- en: GridWorld project structure
  prefs: []
  type: TYPE_NORMAL
- en: 'The project has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DeepQNetwork.java`: Provides the reference architecture for the DQN'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Replay.java`: Generates replay memory for the DQN to ensure that the gradients
    of the deep network are stable and do not diverge across episodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GridWorld.java`: The main class used for training the DQN and playing the
    game.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the way, we perform the training on GPU and cuDNN for faster convergence.
    However, feel free to use the CPU backend as well if your machine does not have
    a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the grid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be developing a simple game by initializing a grid in exactly the same
    way each time. The game starts with the agent (*A*), goal (*+*), pit (-), and
    wall (*W*). All elements are randomly placed on the grid in each game. This is
    such that the Q-learning just needs to learn how to move the agent from a known
    starting position to a known goal without hitting the pit, which gives negative
    rewards. Take a look at this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a59c6f73-11ff-4199-8fd7-25476b7e85b0.png)'
  prefs: []
  type: TYPE_IMG
- en: A grid for the GridWorld game showing the elements (that is, agent, goal, pit,
    and wall)
  prefs: []
  type: TYPE_NORMAL
- en: In short, the target of the game is to reach the goal, where the agent will
    receive a numerical reward. For simplicity, we will avoid a pit; if the agent
    lands on the pit, it gets penalized with a negative reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'The wall can block the agent''s path too, but it offers no reward or penalty,
    so we''re safe. Since this is a simple way of defining the state, the agent can
    make the following moves (that is, actions):'
  prefs: []
  type: TYPE_NORMAL
- en: Up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This way, an action *a* can be defined as follows: `a ∈ A {up, down, left,
    right}`. Now let''s see, based on the preceding assumption, how the grid would
    look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the grid is constructed, it can be printed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Calculating agent and goal positions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now the search space for the agent is ready. So let''s calculate the initial
    position of the agent and the goal. First, we compute the initial position of
    the agent in the grid, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we calculate the position of the goal, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the generated grid can be considered as four separate grid planes, where
    each plane represents the position of each element. In the following diagram,
    the agent''s current grid position is (3, 0), the wall is at (0, 0), the pit is
    at (0, 1), and the goal is at (1, 0), which also means that all other elements
    are 0s:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/826ed961-0272-49c2-9a9c-d8bd9a35bd10.png)'
  prefs: []
  type: TYPE_IMG
- en: A generated grid can be considered as four separate grid planes
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we developed the grid such that some of the objects contain a *1* at the
    same *x*, *y* position (but different *z* positions), which indicates they're
    at the same position on the grid.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the action mask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we set all the outputs to 0, except the one for the action we actually
    saw, such that the network multiplies its outputs by a mask corresponding to the
    one-hot encoded action. We can then pass 0 as the target for all unknown actions,
    and our neural network should thus perform fine. When we want to predict all actions,
    we can simply pass a mask of all 1s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Providing guidance action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now the agent''s action plan is known. The next task is providing some guidance
    to the agent moving from the current position towards the goal. For example, not
    all the action is accurate, that is, a bad move:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code block, we encoded the action as follows: `0` is up, `1`
    is down, `2` is left, and `3` is right. Otherwise, we treat the action as a bad
    move, and so the agent gets penalized.'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the reward
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that the agent is provided with some guidance—reinforcement—the next task
    is to calculate the reward for each action the agent makes. Take a look at this
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Flattening input for the input layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Then we need to convert the output of the network into a 1D-feature vector,
    to be used by the DQN. This flattening gets the output of the network; it flattens
    all its structure to create a single long-feature vector to be used by the dense
    layer. Take a look at this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Up to this point, we just created the logical skeleton for the `GridWorld`.
    Thus, we create the `DQN` before we start playing the game.
  prefs: []
  type: TYPE_NORMAL
- en: Network construction and training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As I stated, we will create a DQN network using `MultiLayerNetwork` and the
    `MultiLayerConfiguration` configuration of DL4J, which will serve as our Q-function.
    Therefore, the first step is to create a `MultiLayerNetwork` by defining `MultiLayerConfiguration`.
    Since the state has 64 elements—4 x 4 x 4—our network has to have an input layer
    of 64 units, two hidden layers of 164 and 150 units each, and an output layer
    of 4, for four possible actions (up, down, left, and right). This is outlined
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4134ff8-c90a-4cbb-947b-d7b4efa6c89d.png)'
  prefs: []
  type: TYPE_IMG
- en: The structure of the DQN network, showing an input layer, two hidden layers,
    and an output layer
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, we will be using experience replay memory for training our DQN,
    which will help us store the transitions observed by the agent. This will allow
    the DQN to reuse this data later. By sampling from it randomly, the transitions
    that build up a batch are de-correlated. It has been shown that this greatly stabilizes
    and improves the DQN training procedure. Following the preceding config, the following
    code can be used to create such a `MultiLayerConfiguration`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we use this configuration to create a DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We will discuss the parameters shortly, but, before that, we''ll look at how
    to create such a deep architecture. First, we define some parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we define the constructor to initialize these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the implementation for the main loop of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: We set up a `for` loop to the number of episodes while the game is in progress.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We run the Q-network forward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use an epsilon-greedy implementation, so at time *t* with probability *ϵ,*
    the *agent* chooses a random action. However, with probability 1−*ϵ,* the action
    associated with the highest Q-value from our neural network is performed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the agent takes an action *a*, which is determined in the previous step;
    we observe a new state *s*′ and reward *r[t]*[+1].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the Q-network forward pass is executed using *s*′, and the highest Q-value
    (maxQ) is stored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent's target value is then computed as reward + (gamma * maxQ) to train
    the network, where gamma is a parameter (0<=*γ*<=1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We aim to update the output associated with the action we just took for four
    possible outputs. Here, the agent's target output vector is the same as the output
    vector from the first execution, except the one output associated with an action
    to *reward + (gamma * maxQ)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The preceding steps are for one episode, and then the loop iterates for the
    defined episode by the user. In addition, the grid is first constructed, and then
    the next reward for each move is computed and saved. In short, the preceding steps
    can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, network computes the observed reward for each
    mini batch of flattened input data. Take a look at this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding reward is calculated to estimate the optimal future value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, the future reward is computed by taking the maximum
    value of the neural network output. Take a look at this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As stated before, the observed reward is computed once the network training
    starts. The combined input is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the network needs to compute the combined input for the next pass. Take
    a look at this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code blocks, the map at each time step is saved with the `addToBuffer()`
    method, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the DQNet takes the flattening input into batches for each episode, and
    the training starts. Then the current and target outputs by maximizing the reward
    are computed based on the current and target inputs. Take a look at this code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, future rewards are computed by maximizing the
    value of the neural network output, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As I stated earlier, this is very simple game, and if the agent takes action
    2 (that is, left), one step results in reaching the goal. Therefore, we just keep
    all other outputs the same as before and change the one for the action we took.
    So, implementing experience replay is a better idea, which gives us mini-batch
    updating in an online learning scheme.
  prefs: []
  type: TYPE_NORMAL
- en: 'It works such that we run the agent to collect enough transitions to fill up
    the replay memory, without training. For example, our memory may be of size 10,000\.
    Then, at every step, the agent will obtain a transition; we''ll add this to the
    end of the memory and pop off the earliest one. Take a look at this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, sample a mini batch of experiences from the memory randomly, and update
    our Q-function on that, similar to mini-batch gradient descent. Take a look at
    this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Playing the GridWorld game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this project, I haven''t used any visualization to demonstrate the states
    and actions. Rather it is a text-based game, as I alluded to earlier. Then you
    can run the `GridWorld.java` class (containing the main method) using following
    invocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In this invocation, here''s the parameter description outlined:'
  prefs: []
  type: TYPE_NORMAL
- en: '`conf`: This is the `MultiLayerConfiguration` used to create the DQN'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`100000`: This is the replay memory capacity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.99f`: The discount'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1d`: This is the epsilon'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1024`: The batch size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`500`: This is the update frequency; second 1,024 is the replay start size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InputLength`: This is the input length of size x size x 2 + 1= 33 (considering
    size=4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`4`: This is the number of possible actions that can be performed by the agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We initialize epsilon (*ϵ*-greedy action selection) to 1, which will decrease
    by a small amount on every episode. This way, it will eventually reach 0.1 and
    saturate. Based on the preceding setting, the training should be started, which
    will start generating a grid representing the map at each timestamp and the outputs
    of the DQN for the up/down/left/right order, followed by the index of the highest
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do not have any module for a graphical representation of the game. So in
    the previous result, 0, 1, -1, and so on,the grid represents the map at each timestamp
    for five episodes. The numbers in brackets are just the outputs of the DQN, followed
    by the index of the highest value. Take a look at this code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the agent has been able to make a total score of 10 (that is, positive).
  prefs: []
  type: TYPE_NORMAL
- en: Frequently asked questions (FAQs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have solved the GridWorld problem, there are other practical aspects
    in reinforcement learning and overall deep learning phenomena that need to be
    considered too. In this section, we will see some frequently asked questions that
    may be already on your mind. Answers to these questions can be found in Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: What is Q in Q-learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I understand that we performed the training on GPU and cuDNN for faster convergence.
    However, there is no GPU on my machine. What can I do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no visualization, so it is difficult to follow the moves made by the
    agent toward the target.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give a few more examples of reinforcement learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do I reconcile the results obtained for our mini-batch processing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would I reconcile the DQN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I would like to save the trained network. Can I do that?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I would like to restore the saved (that is, trained) network. Can I do that?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to develop a demo GridWorld game using DL4J, RL4J,
    and neural Q-learning, which acts as the Q-function. We also provided some basic
    theoretical background necessary for developing a deep QLearning network for playing
    the GridWorld game. However, we did not develop any module for visualizing the
    moves of the agent for the entire episodes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will develop a very common end-to-end movie recommendation
    system project, but with the neural **Factorization Machine** (**FM**) algorithm.
    The MovieLens 1 million dataset will be used for this project. We will be using
    RankSys and Java-based FM libraries for predicting both movie ratings and rankings
    from the users. Nevertheless, Spark ML will be used for exploratory analysis of
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Answers to questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Answer** **to question 1:** Do not confuse the Q in Q-learning with the Q-function
    we have discussed in the previous parts. The Q-function is always the name of
    the function that accepts states and actions and spits out the value of that state-action
    pair. RL methods involve a Q-function but are not necessarily Q-learning algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 2:** No worries as you can perform the training on
    a CPU backend too. In that case, just remove the entries for CUDA and cuDNN dependencies
    from the `pom.xml` file and replace them with the CPU ones. The properties would
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t use these two dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Use only one, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Then you are ready to get going with the CPU backend.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 3:** As stated earlier, the initial target was to
    develop a simple text-based game. However, with some effort, all the moves can
    be visualized too. I want to leave this up to the readers. Nevertheless, the visualization
    module will be added to the GitHub repository very soon.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 4:** Well, there are some basic examples of RL4J on
    the DL4J GitHub repository at [https://github.com/deeplearning4j/dl4j-examples/](https://github.com/deeplearning4j/dl4j-examples/tree/master/rl4j-examples/src/main/java/org/deeplearning4j/examples/rl4j).
    Feel free to try to extend them to meet your needs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 5:** Processing each mini-batch gives us the best
    weights/biases result for the input used in that mini-batch. This question evolves
    several subquestions related to this: i) How do we reconcile the results obtained
    for all mini-batches? ii) Do we take the average to come up with the final weights/biases
    for the trained network?'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, each mini-batch contains the average of the gradients of individual
    errors. If you had two mini-batches, you could take the average of the gradient
    updates of both mini-batches to tweak the weights, to reduce the error for those
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 6:** Refer to question 5 to get the theoretical understanding.
    However, in our example, use the `setParams()` method from DL4J, which helps you
    reconcile network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the question would be: Where do we use such reconciling? Well, the answer
    is while computing the reward (see the `observeReward()` method).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 7:** Saving the DQN is similar to saving another DL4J-based
    network. For this, I wrote a method called `saveNetwork()` that saves network
    parameters as a single ND4J object in JSON format. Take a look at this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**Answer** **to question 8:** Restoring the DQN is similar to saving another
    DL4J-based network. For this, I wrote a method called `restoreNetwork()` that
    reconciles the params and reloads the saved network as `MultiLayerNetwork`. Here
    it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
