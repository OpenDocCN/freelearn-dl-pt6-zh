- en: Convolutional and Recurrent Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The human brain is often the main inspiration and comparison we make when building
    AI and is something deep learning researchers often look to for inspiration or
    reassurance. By studying the brain and its parts in more detail, we often discover
    neural sub-processes. An example of a neural sub-process would be our visual cortex,
    the area or region of our brain responsible for vision. We now understand that
    this area of our brain is wired differently and responds differently to input.
    This just so happens to be analogous to analog what we have found in our previous
    attempts at using neural networks to classify images. Now, the human brain has
    many sub-processes all with specific mapped areas in the brain (sight, hearing,
    smell, speech, taste, touch, and memory/temporal), but in this chapter, we will
    look at how we model just sight and memory by using advanced forms of deep learning
    called **convolutional and recurrent networks**. The two-core sub-processes of
    sight and memory are used extensively by us for many tasks including gaming and
    form the focus of research of many deep learners.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers often look to the brain for inspiration, but the computer models
    they build often don't entirely resemble their biological counterpart. However,
    researchers have begun to identify almost perfect analogs to neural networks inside
    our brains. One example of this is the ReLU activation function. It was recently
    found that the excitement level in our brains' neurons, when plotted, perfectly
    matched a ReLU graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore, in some detail, convolutional and recurrent
    neural networks. We will look at how they solve the problem of replicating accurate
    vision and memory in deep learning. These two new network or layer types are a
    fairly recent discovery but have been responsible in part for many advances in
    deep learning. This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a self-driving CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory and recurrent networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing rock, paper, scissors with LSTMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be sure you understand the fundamentals outlined in the previous chapter reasonably
    well before proceeding. This includes running the code samples, which install
    this chapter's required dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sight is hands-down the most-used sub-process. You are using it right now! Of
    course, it was something researchers attempted to mimic with neural networks early
    on, except that nothing really worked well until the concept of convolution was
    applied and used to classify images. The concept of convolution is the idea behind
    detecting, sometimes grouping, and isolating common features in an image. For
    instance, if you cover up 3/4 of a picture of a familiar object and show it to
    someone, they will almost certainly recognize the image by recognizing just the
    partial features. Convolution works the same way, by blowing up an image and then
    isolating the features for later recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolution works by dissecting an image into its feature parts, which makes
    it easier to train a network. Let''s jump into a code sample that extends from
    where we left off in the previous chapter but that now introduces convolution.
    Open up the `Chapter_2_1.py` listing and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the first couple of lines doing the import:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we import new layer types: `Conv2D`, `MaxPooling2D`, and `UpSampling2D`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then we set the `Input` and build up the encoded and decoded network sections
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first thing to note is that we are now preserving the dimensions of the
    image, in this case, 28 x 28 pixels wide and 1 layer or channel. This example
    uses an image that is in grayscale, so there is only a single color channel. This
    is vastly different from before, when we just unraveled the image into a single
    784-dimension vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second thing to note is the use of the `Conv2D` layer or two-dimensional
    convolutional layer and the following `MaxPooling2D` or `UpSampling2D` layers.
    Pooling or sampling layers are used to gather or conversely unravel features.
    Note how we use pooling or down-sampling layers after convolution when the image
    is encoded and then up-sampling layers when decoding the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we build and train the model with the following block of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The training of the model in the preceding code mirrors what we did at the end
    of the previous chapter, but note the selection of training and testing sets now.
    We no longer squish the image but rather preserve its spatial properties as inputs
    into the convolutional layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we output the results with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Run the code, as you have before, and you'll immediately notice that it is about
    100 times slower to train. This may or may not require you to wait, depending
    on your machine; if it does, go get a beverage or three and perhaps a meal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training our simple sample now takes a large amount of time, which may be quite
    noticeable on older hardware. In the next section, we look at how we can start
    to monitor the training sessions, in great detail.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring training with TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorBoard is essentially a mathematical graph or calculation engine that performs
    very well at crunching numbers, hence our use of it in deep learning. The tool
    itself is still quite immature, but there are some very useful features for monitoring
    training exercises.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to start monitoring training on our sample:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can monitor the training session by entering the following command into
    a new **Anaconda** or command window from the same directory/folder that you are
    running the sample from:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will launch a TensorBoard server, and you can view the output by navigating
    your browser to the URL in italics, as shown in the window you are running `TensorBoard`
    from. It will typically look something like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note, the URL should use your machine name, but if that doesn't work, try the
    second form. Be sure to allow ports `6000`, and `6006` and/or the **TensorBoard**
    application through your firewall if prompted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When the sample is done running, you should see the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d666bad9-02b7-43c3-ae41-bee3a36635e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Auto-encoding digits using convolution
  prefs: []
  type: TYPE_NORMAL
- en: Go back and compare the results from this example and the last example from
    [Chapter 1](108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml),* Deep Learning for Games*.
    Note the improvement in performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your immediate thought may be, "*Is the increased training time we experienced
    worth the effort?*" After all, the decoded images look quite similar in the previous
    example, and it trained much faster, except, remember we are training the network
    weights slowly by adjusting each weight over each iteration, which we can then
    save as a model. That model or brain can then be used to perform the same task
    again later, without training. Works scarily enough! Keep this concept in mind
    as we work through this chapter. In [Chapter 3](cb51d15b-9855-47e2-8e45-f74a115ebfa8.xhtml),
    *GAN for Games*, we will start saving and moving our brain models around.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we take a more in-depth look at how convolution works.
    Convolution can be tricky to understand when you first encounter it, so take your
    time. It is important to understand how it works, as we will use it extensively
    later.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolution** is a way of extracting features from an image that may allow
    us to more easily classify it based on known features. Before we get into convolution,
    let''s first take a step back and understand why networks, and our vision for
    that matter, need to isolate features in an image. Take a look at the following;
    it''s a sample image of a dog, called Sadie, with various image filters applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb6c51c0-04d3-40ec-9f20-5a342cefff6f.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of an image with different filters applied
  prefs: []
  type: TYPE_NORMAL
- en: The preceding shows four different versions with no filter, edge detection,
    pixelate, and glowing edges filters applied. In all cases, though, you as a human
    can clearly recognize it is a picture of a dog, regardless of the filter applied,
    except note that in the edge detection case, we have eliminated the extra image
    data that is unnecessary to recognize a dog. By using a filter, we can extract
    just the required features our NN needs to recognize a dog. This is all a convolution
    filter does, and in some cases, one of those filters could be just a simple edge
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'A convolution filter is a matrix or kernel of numbers that defines a single
    math operation. The process starts by being multiplied by the upper-left corner
    pixel value, with the results of the matrix operation summed and set as the output.
    The kernel is slid across the image in a step size called a **stride**, and this
    operation is demonstrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/114cee40-0fd4-4978-b46d-c8d5f8a5882d.png)'
  prefs: []
  type: TYPE_IMG
- en: Applying a convolution filter
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, a stride of 1 is being used. The filter being applied
    in the convolution operation is essentially an edge detection filter. If you look
    at the result of the final operation, you can see the middle section is now filled
    with OS, greatly simplifying any classification task. The less information our
    networks need to learn, the quicker they will learn and with less data. Now, the
    interesting part of this is that the convolution learns the filter, the numbers,or
    the weights it needs to apply in order to extract the relevant features. This
    is not so obvious and may be confusing, so let''s go over it again. Go back to
    our previous example and look at how we define the first convolution layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In that line of code, we define the first convolution layer as having `16` output
    filters, meaning our output from this layer is actually 16 filters. We then set
    the kernel size to `(3,3)`, which represents a `3x3` matrix , just as in our example.
    Note how we don't specify the values of the various kernel filter weights, as
    that is after all what the network is training to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this looks when everything is put together in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a13bb31-cf32-4ec2-838a-e066ef86f047.png)'
  prefs: []
  type: TYPE_IMG
- en: Full convolution operation
  prefs: []
  type: TYPE_NORMAL
- en: The output from the first step in convolution is the feature map. One feature
    map represents a single convolution filter being applied and is generated by applying
    the learned filter/kernel. In our example, the first layer produces **16 kernels**,
    which in turn produce **16 feature maps**; remember that the value of `16` is
    for the number of filters.
  prefs: []
  type: TYPE_NORMAL
- en: 'After convolution, we then apply pooling or subsampling in order to collect
    or gather features into sets. This subsampling further creates new concentrated
    feature maps that highlight the image''s important features we are training for.
    Take a look back at how we defined the first pooling layer in our previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code, we are subsampling using a `pool_size` of `(2,2)`. The size indicates
    the factor by which to down-sample the image by width and height. So a 2 x 2 pool
    size will create four feature maps at half the size in width and height. This
    results in a total of 64 feature maps after our first layer of convolution and
    pooling. We get this by multiplying 16 (convolution feature maps) x 4 (pooling
    feature maps) = 64 feature maps. Consider how many total feature maps we build
    in our simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2061d9ed-16c1-475a-a519-bb47eff32601.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/d0f65917-9560-4f1b-8aae-c62ff29a9071.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/56c4dfa5-f217-4d4a-8f46-e0cd5d904258.png)'
  prefs: []
  type: TYPE_IMG
- en: That is 65,536 feature maps of 4 x 4 images. This means we now train our network
    on 65,536 smaller images; for each image, we attempt to encode or classify. This
    is obviously the cause for the increased training time, but also consider the
    amount of extra data we are now using to classify our images. Now our network
    is learning how to identify parts or features of our image, just as we humans
    identify objects.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you were just shown the nose of a dog, you could likely recognize
    that as a dog. Consequently, our sample network now is identifying parts of the
    handwritten digits, which as we know now, dramatically improves performance.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, convolution works well for identifying images, but the process
    of pooling can have disruptive consequences to preserving spatial relationships.
    Therefore, when it comes to games or learning requiring some form of spatial understanding,
    we prefer to limit pooling or eliminate altogether. Since it is important to understand
    when to use and not to use pooling, we will cover that in more detail in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Building a self-driving CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nvidia created a multi-layer CNN called **PilotNet**, in 2017, that was able
    to steer a vehicle by just showing it a series of images or video. This was a
    compelling demonstration of the power of neural networks, and in particular the
    power of convolution. A diagram showing the neural architecture of PilotNet is
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc9e1eee-fc4c-4305-9b0e-489e93ff06d6.png)'
  prefs: []
  type: TYPE_IMG
- en: PilotNet neural architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram shows the input of the network moving up from the bottom where
    the results of a single input image output to a single neuron represent the steering
    direction. Since this is such a great example, several individuals have posted
    blog posts showing an example of PilotNet, and some actually work. We will examine
    the code from one of these blog posts to see how a similar architecture is constructed
    with Keras. Next is an image from the original PilotNet blog, showing a few of
    the types of images our self-driving network will use to train:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cf9c76e-f157-44bb-a455-14ffb5240090.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of PilotNet training images
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of training in this example is to output the degree to which the steering
    wheel should be turned in order to keep the vehicle on the road. Open up the code
    listing in `Chapter_2_2.py` and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now switch to using Keras for a few samples. While the TensorFlow embedded
    version of Keras has served us well, there are a couple of features we need that
    are only found in the full version. To install Keras and other dependencies, open
    a shell or Anaconda window and run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'At the start of the code file (`Chapter_2_2.py`), we begin with some imports
    and load the sample data using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This code just does some imports and then downloads the sample driving frames
    from the author's source data. The original source of this blog was written in
    a notebook by **Roscoe's Notebooks** and can be found at [https://wroscoe.github.io/keras-lane-following-autopilot.html](https://wroscoe.github.io/keras-lane-following-autopilot.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pickle` is a decompression library that unpacks the data in datasets `X` and
    `Y` at the bottom of the previous listing.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then we shuffle the order of the frames around or essentially randomize the
    data. We often randomize data this way to make our training stronger. By randomizing
    the data order, the network needs to learn an absolute steering value for an image,
    rather than a possible relative or incremental value. The following code does
    this shuffle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: All this code does is use `numpy` to randomly shuffle the image frames. Then
    it prints out the length of the first shuffled set `shuffled_X` so we can confirm
    the training data is not getting lost.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we need to create a training and test set of data. The training set is
    used to train the network (weights), and the test, or validation, set is used
    to confirm the accuracy on new or raw data. As we have seen before, this is a
    common theme when using supervised training or labeled data. We often break the
    data into 80% training and 20% test. The following code is what does this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the training and test sets, we now want to augment or expand
    the training data. In this particular case, the author augmented the data just
    by flipping the original images and adding those to the dataset. There are many
    other ways of augmenting data that we will discover in later chapters, but this
    simple and effective method of flipping is something to add to your belt of machine
    learning tools. The code to do this flip is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the heavy lifting part. The data is prepped, and it is time to build
    the model as shown in the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The code to build the model at this point should be fairly self-explanatory.
    Take note of the variation in the architecture and how the code is written from
    our previous examples. Also note the two highlighted lines. The first one uses
    a new layer type called `Flatten`. All this layer type does is flatten the 2 x
    2 image into a vector that is then input into a standard `Dense` hidden fully
    connected layer. The second highlighted line introduces another new layer type
    called `Dropout`. This layer type needs a bit more explanation and will be covered
    in more detail at the end of this section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally comes the training part, which this code sets up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This last piece of code sets up a set of `callbacks` to update and control the
    training. We have already used callbacks to update the TensorBoard server with
    logs. In this case, we use the callbacks to resave the model after every checkpoint
    (epoch) and check for an early exit. Note the form in which we are saving the
    model – an `hdf5` file. This file format represents a hierarchical data structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the code as you have already been doing. This sample can take a while, so
    again be patient. When you are done, there will be no output, but pay special
    attention to the minimized loss value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point in your deep learning career, you may be realizing that you need
    much more patience or a better computer or perhaps a TensorFlow-supported GPU.
    If you want to try the latter, feel free to download and install the TensorFlow
    GPU library and the other required libraries for your OS, as this will vary. Plenty
    of documentation can be found online. After you have the GPU version of TensorFlow
    installed, Keras will automatically try to use that. If you have a supported GPU,
    you should notice a performance increase, and if not, then consider buying one.
  prefs: []
  type: TYPE_NORMAL
- en: While there is no output for this example, in order to keep it simple, try to
    appreciate what is happening. After all, this could just as easily be set up as
    a driving game, where the network drives the vehicle by just looking at screenshots.
    We have omitted the results from the author's original blog post, but if you want
    to see how this performs further, go back and check out the [source link](https://wroscoe.github.io/keras-lane-following-autopilot.html).
  prefs: []
  type: TYPE_NORMAL
- en: One thing the author did in his blog post was to use pooling layers, which,
    as we have seen, is quite standard when working with convolution. However, when
    and how to use pooling layers is a bit contentious right now and requires further
    detailed discussion, which is provided in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial convolution and pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Geoffrey Hinton and his team have recently strongly suggested that using pooling
    with convolution removes spatial relationships in the image. Hinton instead suggests
    the use of **CapsNet**, or **Capsule Networks**. Capsule Networks are a method
    of pooling that preserves the spatial integrity of the data. Now, this may not
    be a problem in all cases. For handwritten digits, spatial relationships don't
    matter that much. However, self-driving cars or networks tasked with spatial tasks,
    a prime example of which is games, often don't perform as well when using pooling.
    In fact, the team at Unity do not use pooling layers after convolution; let's
    understand why.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pooling or down-sampling is a way of augmenting data by collecting its common
    features together. The problem with this is that any relationship in the data
    often gets lost entirely. The following diagram demonstrates **MaxPooling(2,2)**
    over a convolution map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1fa08f01-0b68-450b-9f82-d8cca463a995.png)'
  prefs: []
  type: TYPE_IMG
- en: Max pooling at work
  prefs: []
  type: TYPE_NORMAL
- en: Even in the simple preceding diagram, you can quickly appreciate that pooling
    loses the spatial relationship of the corner (upper-left, bottom-left, lower-right
    and upper-right) the pooled value started in. Note that, after a couple layers
    of pooling, any sense of spatial relation will be completely gone.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test the effect of removing pooling layers from the model and test this
    again by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `Chapter_2_3.py` file and note how we commented out a couple of pooling
    layers, or you can just delete the lines as well, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note how we didn't comment out (or delete) all the pooling layers and left one
    in. In some cases, you may still want to leave a couple of pooling layers in,
    perhaps to identify features that are not spatially important. For example, when
    recognizing digits, space is less important with respect to the overall shape.
    However, if we consider recognizing a face, then the distance between a person's
    eyes, mouth, and so on, is what distinguishes a face from another face. However,
    if you just wanted to identify a face, with eyes, mouth, and so on, then just
    applying pooling could be quite acceptable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we also increase the dropout rate on our `Dropout` layer like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We will explore dropout in some detail in the next section. For now, though,
    just realize that this change will have a more positive effect on our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lastly, we bump up the number of epochs to `10` with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In our previous run, if you were watching the loss rate when training, you would
    realize the last example more or less started to converge at four epochs. Since
    dropping the pooling layers also reduces the training data, we need to also bump
    up the number of epochs. Remember, pooling or down-sampling increases the number
    of feature maps, and fewer maps means the network needs more training runs. If
    you are not training on a GPU, this model will take a while, so be patient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, run the example, again with those minor changes. One of the first things
    you will notice is that the training time shoots up dramatically. Remember, this
    is because our pooling layers do facilitate quicker training, but at a cost. This
    is one of the reasons we allow for a single pooling layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the sample is finished running, compare the results for the `Chapter_2_2.py`
    sample we ran earlier. Did it do what you expected it to?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We only focus on this particular blog post because it is extremely well presented
    and well written. The author obviously knew his stuff, but this example just shows
    how important it is to understand the fundamentals of these concepts in as much
    detail as you can handle. This is not such an easy task with the flood of information,
    but this also reinforces the fact that developing working deep learning models
    is not a trivial task, at least not yet.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the cost/penalty of pooling layers, we can move on to the
    next section, where we jump back to understanding `Dropout`. It is an excellent
    tool you will use over and over again.
  prefs: []
  type: TYPE_NORMAL
- en: The need for Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s go back to our much-needed discussion about `Dropout`. We use dropout
    in deep learning as a way of randomly cutting network connections between layers
    during each iteration. An example showing an iteration of dropout being applied
    to three network layers is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0588924a-6516-4bc0-8f6f-e217d96e8af1.png)'
  prefs: []
  type: TYPE_IMG
- en: Before and after dropout
  prefs: []
  type: TYPE_NORMAL
- en: The important thing to understand is that the same connections are not always
    cut. This is done to allow the network to become less specialized and more generalized.
    Generalizing a model is a common theme in deep learning, and we often do this
    so our models can learn a broader set of problems, more quickly. Of course, there
    may be times where generalizing a network limits a network's ability to learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we go back to the previous sample now and look at the code, we see a `Dropout`
    layer being used like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: That simple line of code tells the network to drop out or disconnect 50% of
    the connections randomly after every iteration. Dropout only works for fully connected
    layers (**Input** -> **Dense** -> **Dense**) but is very useful as a way of improving
    performance or accuracy. This may or may not account for some of the improved
    performance from the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at how deep learning mimics the memory sub-process
    or temporal scent.
  prefs: []
  type: TYPE_NORMAL
- en: Memory and recurrent networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Memory is often associated with **Recurrent Neural Network** (**RNN**), but
    that is not entirely an accurate association. An RNN is really only useful for
    storing a sequence of events or what you may refer to as a **temporal sense**,
    a sense of time if you will. RNNs do this by persisting state back onto itself
    in a recursive or recurrent loop. An example of how this looks is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e37e1983-cc7b-4826-818a-aa84968820fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Unfolded recurrent neural network
  prefs: []
  type: TYPE_NORMAL
- en: What the diagram shows is the internal representation of a recurrent neuron
    that is set to track a number of time steps or iterations where **x** represents
    the input at a time step and **h** denotes the state. The network weights of **W**,
    **U**, and **V** remain the same for all time steps and are trained using a technique
    called **Backpropagation Through Time** (**BPTT**). We won't go into the math
    of BPTT and leave that up the reader to discover on their own, but just realize
    that the network weights in a recurrent network use a cost gradient method to
    optimize them.
  prefs: []
  type: TYPE_NORMAL
- en: A recurrent network allows a neural network to identify sequences of elements
    and predict what elements typically come next. This has huge applications in predicting
    text, stocks, and of course games. Pretty much any activity that can benefit from
    some grasp of time or sequence of events will benefit from using RNN, except standard
    RNN, the type shown previously, which fails to predict longer sequences due to
    a problem with gradients. We will get further into this problem and the solution
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing and exploding gradients rescued by LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The problem the RNN suffers from is either vanishing or exploding gradients.
    This happens because, over time, the gradient we try to minimize or reduce becomes
    so small or big that any additional training has no effect. This limits the usefulness
    of the RNN, but fortunately this problem was corrected with **Long Short-Term
    Memory*** (***LSTM**) blocks, as shown in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2cfd7cf-6e48-4993-8751-2d580fc8901f.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of an LSTM block
  prefs: []
  type: TYPE_NORMAL
- en: LSTM blocks overcome the vanishing gradient problem using a few techniques.
    Internally, in the diagram where you see a **x** inside a circle, it denotes a
    gate controlled by an activation function. In the diagram, the activation functions
    are **σ** and **tanh**. These activation functions work much like a step or ReLU
    do, and we may use either function for activation in a regular network layer.
    For the most part, we will treat an LSTM as a black box, and all you need to remember
    is that LSTMs overcome the gradient problem of RNN and can remember long-term
    sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a working example to see how this comes together. Open
    up `Chapter_2_4.py` and follow the these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin as per usual by importing the various Keras pieces we need, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This example was pulled from [https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/).
    This is a site hosted by **Dr. Jason Brownlee**, who has plenty more excellent
    examples explaining the use of LSTM and recurrent networks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This time we are importing two new classes, `Sequential` and `LSTM`. Of course
    we know what `LSTM` is for, but what about `Sequential`? `Sequential` is a form
    of model that defines the layers in a sequence one after another. We were less
    worried about this detail before, since our previous models were all sequential.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we set the random seed to a known value. We do this so that our example
    can replicate itself. You may have noticed in previous examples that not all runs
    perform the same. In many cases, we want our training to be consistent, and hence
    we set a known seed value by using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: It is important to realize that this just sets the `numpy` random seed value.
    Other libraries may use different random number generators and require different
    seed settings. We will try to identify these inconsistencies in the future when
    possible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we need to identify a sequence we will train to; in this case, we will
    just use the `alphabet` as shown in this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code builds our sequence of characters as integers and builds
    a map of each character sequence. It builds a `seq_in` and `seq_out` showing the
    forward and reverse positions. Since the length of a sequence is defined by `seq_length
    = 1`, then we are only concerned about a letter of the alphabet and the character
    that comes after it. You could, of course, do longer sequences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With the sequence data built, it is time to shape the data and normalize it
    with this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The first line in the preceding code reshapes the data into a tensor with a
    size length of `dataX`, the number of steps or sequences, and the number of features
    to identify. We then normalize the data. Normalizing the data comes in many forms,
    but in this case we are normalizing values from 0 to 1\. Then we one hot encode
    the output for easier training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'One hot encoding is where we you set the value to 1 where you have data or
    a response, and to zero everywhere else. In the example, our model output is 26
    neurons, which could also be represented by 26 zeros, one zero for each neuron,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '**00000000000000000000000000**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where each zero represents the matching character position in the alphabet.
    If we wanted to denote a character **A**, we would output the one hot encoded
    value as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**10000000000000000000000000**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we construct the model, using a slightly different form of code than we
    have seen before and as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The critical piece to the preceding code is the highlighted line that shows
    the construction of the `LSTM` layer. We construct an `LSTM` layer by setting
    the number of units, in this case `32`, since our sequence is 26 characters long
    and we want our units disable by `2`. Then we set the `input_shape` to match the
    previous tensor, `X`, that we created to hold our training data. In this case,
    we are just setting the shape to match all the characters (26) and the sequence
    length, in this case `1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we output the model with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Run the code as you normally would and examine the output. You will notice that
    the accuracy is around 80%. See whether you can improve the accuracy of the model
    for predicting the next sequence in the alphabet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This simple example demonstrated the basic use of an LSTM block for recognizing
    a simple sequence. In the next section, we look at a more complex example: using
    LSTM to play Rock, Paper, Scissors.'
  prefs: []
  type: TYPE_NORMAL
- en: Playing Rock, Paper, Scissors with LSTMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remembering sequences of data have huge applications in many areas, not the
    least of which includes gaming. Of course, producing a simple, clean example is
    another matter. Fortunately, examples abound on the internet and `Chapter_2_5.py`
    shows an example of using an LSTM to play Rock, Paper, Scissors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up that sample file and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: This example was pulled from [https://github.com/hjpulkki/RPS](https://github.com/hjpulkki/RPS),
    but the code needed to be tweaked in several places to get it to work for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start as we normally do with the imports. For this sample, be sure to
    have Keras installed as we did for the last set of exercises:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we set some constants as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we build the model, this time with three LSTM layers, one for each element
    in our sequence (rock, paper and scissors), like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create a function to extract our data from the `data.txt` file. This
    file holds the sequences of training data using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are training each block of training through 100 epochs in
    the same order as they are in the file. A better method would be to train each
    training sequence in a random order.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then we create the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the data using a loop, with each iteration pulling a batch from the `data.txt`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we evaluate the results with a validation sequence as shown in this
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Run the sample as you normally would. Check the results at the end and note
    how accurate the model gets at predicting the sequence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Be sure to run through this simple example a few times and understand how the
    LSTM layers are set up. Pay special attention to the parameters and how they are
    set.
  prefs: []
  type: TYPE_NORMAL
- en: That concludes our quick look at understanding how to use recurrent aka LSTM
    blocks for recognizing and predicting sequences of data. We will of course use
    this versatile layer type many more times throughout the course of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In the final section of this chapter, we again showcase a number of exercises
    you are encouraged to undertake for your own benefit.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Complete the following exercises in your own time and to improve your own learning
    experience. Improving your understanding of the material will make you a more
    successful deep learner, and you will likely enjoy this book better as well:'
  prefs: []
  type: TYPE_NORMAL
- en: In the `Chapter_2_1.py` example, change the `Conv2D` layers to use a different
    filter size. Run the sample again, and see what effect this has on training performance
    and accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comment out or delete a couple of the `MaxPooling` layers and corresponding
    `UpSampling` layers in the `Chapter_2_1.py` example. Remember, if you remove a
    pooling layer between layers 2 and 3, you likewise need to remove the up-sampling
    to remain consistent. Run the sample again, and see what effect this has on training
    time, accuracy, and performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alter the **Conv2D** layers in the `Chapter_2_2.py` example using a different
    filter size. See what effect this has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alter the **Conv2D** layers in the `Chapter_2_2.py` example by using a stride
    value of **2**. You may need to consult the **Keras** docs in order to do this.
    See what effect this has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alter the **MaxPooling** layers in the `Chapter_2_2.py` example by altering
    the pooling dimensions. See what effect this has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove all or comment out different **MaxPooling** layers used in the `Chapter_2_3.py`
    example. What happens if all the pooling layers are commented out? Do you need
    to increase the training epochs now?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alter the use of **Dropout** in the various examples used throughout this chapter.
    This includes adding dropout. Test the effects of using different levels of dropout.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the sample in `Chapter_2_4.py` so that the model produces better accuracy.
    What do you need to do in order to improve training performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the sample in `Chapter_2_4.py` to predict more than one character in
    the sequence. If you need help, go back and review the original blog post for
    more information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens if you change the number of units that the three **LSTM** layers
    use in the `Chapter_2_5.py` example? What if you increase the value to 128, 32,
    or 16? Try these values to understand the effect they have.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel free to expand on these exercises on your own. Try to write a new example
    on your own as well, even if it is just a simple one. There really is no better
    way to learn to code than to write your own.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter and the last, we took a deep dive into the core elements of
    deep learning and neural networks. While our review in the last couple chapters
    was not extensive, it should give you a good base for continuing through the rest
    of the book. If you had troubles with any of the material in the first two chapters,
    turn back now and spend more time reviewing the previous material. It is important
    that you understand the basics of neural network architecture and the use of various
    specialized layers, as we covered in this chapter (CNN and RNN). Be sure you understand
    the basics of CNN and how to use it effectively in picking features and what the
    trade—offs are when using pooling or sub sampling. Also understand the concept
    of RNN and how and when to use LSTM blocks for predicting or detecting temporal
    events. Convolutional layers and LSTM blocks are now fundamental components of
    deep learning, and we will use them in several networks we build going forward.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we start to build out our sample game for this book and
    introduce GANs, or generative adversarial networks. We will explore GANs and how
    they can be used to generate game content.
  prefs: []
  type: TYPE_NORMAL
