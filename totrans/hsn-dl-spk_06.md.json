["```py\nval LEARNSTRING_CHARS: util.LinkedHashSet[Character] = new util.LinkedHashSet[Character]\nfor (c <- LEARNSTRING) {\n        LEARNSTRING_CHARS.add(c)\n}\nLEARNSTRING_CHARS_LIST.addAll(LEARNSTRING_CHARS)\n```", "```py\nval builder: NeuralNetConfiguration.Builder = new NeuralNetConfiguration.Builder\nbuilder.iterations(10)\nbuilder.learningRate(0.001)\nbuilder.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\nbuilder.seed(123)\nbuilder.biasInit(0)\nbuilder.miniBatch(false)\nbuilder.updater(Updater.RMSPROP)\nbuilder.weightInit(WeightInit.XAVIER)\n```", "```py\nval listBuilder = builder.list\nfor (i <- 0 until HIDDEN_LAYER_CONT) {\n  val hiddenLayerBuilder: GravesLSTM.Builder = new GravesLSTM.Builder\n  hiddenLayerBuilder.nIn(if (i == 0) LEARNSTRING_CHARS.size else HIDDEN_LAYER_WIDTH)\n  hiddenLayerBuilder.nOut(HIDDEN_LAYER_WIDTH)\n  hiddenLayerBuilder.activation(Activation.TANH)\n  listBuilder.layer(i, hiddenLayerBuilder.build)\n}\n```", "```py\nval outputLayerBuilder: RnnOutputLayer.Builder = new RnnOutputLayer.Builder(LossFunction.MCXENT)\noutputLayerBuilder.activation(Activation.SOFTMAX)\noutputLayerBuilder.nIn(HIDDEN_LAYER_WIDTH)\noutputLayerBuilder.nOut(LEARNSTRING_CHARS.size)\nlistBuilder.layer(HIDDEN_LAYER_CONT, outputLayerBuilder.build)\n```", "```py\nlistBuilder.pretrain(false)\nlistBuilder.backprop(true)\n```", "```py\nval conf = listBuilder.build\nval net = new MultiLayerNetwork(conf)\nnet.init()\nnet.setListeners(new ScoreIterationListener(1))\n```", "```py\nval input = Nd4j.zeros(1, LEARNSTRING_CHARS_LIST.size, LEARNSTRING.length)\nval labels = Nd4j.zeros(1, LEARNSTRING_CHARS_LIST.size, LEARNSTRING.length)\nvar samplePos = 0\nfor (currentChar <- LEARNSTRING) {\n  val nextChar = LEARNSTRING((samplePos + 1) % (LEARNSTRING.length))\n  input.putScalar(Array[Int](0, LEARNSTRING_CHARS_LIST.indexOf(currentChar), samplePos), 1)\n  labels.putScalar(Array[Int](0, LEARNSTRING_CHARS_LIST.indexOf(nextChar), samplePos), 1)\n  samplePos += 1\n}\nval trainingData: DataSet = new DataSet(input, labels)\n```", "```py\nval rng = new Random(12345)\nval lstmLayerSize: Int = 200\nval tbpttLength: Int = 50\nval nSamplesToGenerate: Int = 4\nval nCharactersToSample: Int = 300\nval generationInitialization: String = null\nval conf = new NeuralNetConfiguration.Builder()\n    .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n    .iterations(1)\n    .learningRate(0.1)\n    .rmsDecay(0.95)\n    .seed(12345)\n    .regularization(true)\n    .l2(0.001)\n    .weightInit(WeightInit.XAVIER)\n    .updater(Updater.RMSPROP)\n    .list\n    .layer(0, new GravesLSTM.Builder().nIn(SparkLSTMCharacterExample.CHAR_TO_INT.size).nOut(lstmLayerSize).activation(Activation.TANH).build())\n    .layer(1, new GravesLSTM.Builder().nIn(lstmLayerSize).nOut(lstmLayerSize).activation(Activation.TANH).build())\n    .layer(2, new RnnOutputLayer.Builder(LossFunction.MCXENT).activation(Activation.SOFTMAX)\n      .nIn(lstmLayerSize).nOut(SparkLSTMCharacterExample.nOut).build) //MCXENT + softmax for classification\n    .backpropType(BackpropType.TruncatedBPTT).tBPTTForwardLength(tbpttLength).tBPTTBackwardLength(tbpttLength)\n    .pretrain(false).backprop(true)\n    .build\n```", "```py\nval sparkConf = new SparkConf\nsparkConf.setMaster(master)\nsparkConf.setAppName(\"LSTM Character Example\")\nval sc = new JavaSparkContext(sparkConf)\n```", "```py\nval averagingFrequency: Int = 5\nval batchSizePerWorker: Int = 8\nval examplesPerDataSetObject = 1\nval tm = new ParameterAveragingTrainingMaster.Builder(examplesPerDataSetObject)\n    .workerPrefetchNumBatches(2)\n    .averagingFrequency(averagingFrequency)\n    .batchSizePerWorker(batchSizePerWorker)\n    .build\nval sparkNetwork: SparkDl4jMultiLayer = new SparkDl4jMultiLayer(sc, conf, tm)\nsparkNetwork.setListeners(Collections.singletonList[IterationListener](new ScoreIterationListener(1)))\n```", "```py\nval conf = new SparkConf\nconf.setMaster(master)\nconf.setAppName(\"DataVec S3 Example\")\nval sparkContext = new JavaSparkContext(conf)\n```", "```py\nval origData = sparkContext.binaryFiles(\"s3a://dl4j-bucket\")\n```", "```py\nval numHeaderLinesEachFile = 0\nval delimiter = \",\"\nval seqRR = new CSVSequenceRecordReader(numHeaderLinesEachFile, delimiter)\n```", "```py\nval sequencesRdd = origData.map(new SequenceRecordReaderFunction(seqRR))\n```", "```py\nval labelIndex = 1\nval numClasses = 4\nval dataSetRdd = sequencesRdd.map(new DataVecSequenceDataSetFunction(labelIndex, numClasses, false))\n```"]