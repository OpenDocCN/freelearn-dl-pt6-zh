<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Working with Other Frameworks</h1>
                </header>
            
            <article>
                
<p class="mce-root">In <a href="95863955-3504-48ab-a217-e95339a754d3.xhtml">Chapter 4</a>, <em>Working with Caffe</em>, we learnt about Caffe and its relationship with Caffe2. We examined the Caffe and Caffe2 model file formats and looked at the process of importing a pre-trained Caffe model into Caffe2 using AlexNet as an example. In this chapter, we will look at how to export from, and import to, Caffe2 from other popular DL frameworks. And we will also look at how to enable other DL frameworks to use a model trained with Caffe2.</p>
<p>The topics covered in this chapter are as follows:</p>
<ul>
<li>The ONNX model format</li>
<li>Support for ONNX in Caffe2</li>
<li>How to export a Caffe2 model to ONNX format</li>
<li>How to import an ONNX model into Caffe2</li>
<li>How to visualize ONNX models</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Open Neural Network Exchange</h1>
                </header>
            
            <article>
                
<p><strong>Open Neural Network Exchange</strong> (<strong>ONNX</strong>), typically pronounced as <em>on-niks</em>, is a format to represent a computation graph, with support for a wide variety of operators and data types. This format is general enough to support both neural networks and traditional ML models. Started by Facebook and Microsoft, this format has quickly gained a reputation as a popular format for the export and import of deep neural networks among most DL frameworks.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing ONNX</h1>
                </header>
            
            <article>
                
<p>The ONNX source code can be found online at: <a href="https://github.com/onnx/onnx">https://github.com/onnx/onnx</a> This includes definitions of the format and scripts to operate on ONNX files. Libraries and tools to convert from and to specific DL framework formats are usually provided by DL frameworks.</p>
<p>DL frameworks with built-in support for ONNX include Caffe2, PyTorch, MXNet, and Chainer. There are also converters to convert to and from other DL frameworks, such as TensorFlow. There are runtimes that can use ONNX models on specialized hardware accelerators. For example, TensorRT provides an inference runtime with ONNX support for use on NVIDIA GPUs, and OpenVINO does the same for use on Intel CPUs. (We will discuss TensorRT and OpenVINO in <a href="800ca2c2-fb20-4ad3-9268-12bb0aa83b8a.xhtml">Chapter 6</a>, <em>Deploying Models to Accelerators for Inference</em>.)</p>
<p>The Python library of ONNX can be installed easily, using the following command on Ubuntu:</p>
<pre><strong>$ sudo pip install onnx</strong></pre>
<p>You can check if the installation was successful by testing whether the following command at the shell executes successfully:</p>
<pre><strong>$ python -c "import onnx"</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ONNX format</h1>
                </header>
            
            <article>
                
<p>ONNX is an open source format and its specification and source code can be found online at <a href="https://github.com/onnx/onnx">https://github.com/onnx/onnx</a>. In <a href="95863955-3504-48ab-a217-e95339a754d3.xhtml">Chapter 4</a>, <em>Working with Caffe</em>, we  observed how both Caffe2 and Caffe use Google ProtoBuf for defining the data structure for serialization and deserialization of their network structures and weights. ONNX also uses Google ProtoBuf. It supports both ProtoBuf versions 2 and 3.</p>
<p>The definition of a graph, such as that of a neural network or generally any ML model, defines the various operators that the graph is composed of, the operators' parameters and the relationship between the operators. The syntax and semantics of this information are defined in ONNX as two distinct representations. The <strong>Intermediate Representation</strong> (<strong>IR</strong>) defines constructs, such as graph, node, and tensor. The operators define the various types of possible operators in the graph.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ONNX IR</h1>
                </header>
            
            <article>
                
<p>The ProtoBuf definition of the ONNX computation graph and its data types can be found defined in the <kbd>onnx/onnx.in.proto</kbd> file in the ONNX source code. These are also referred to as the IR of ONNX.</p>
<p>By examining the IR definition of ONNX in the preceding file we can see the following definitions:</p>
<ul>
<li><kbd>NodeProto</kbd>: Used to define each of the layers in a neural network or each of the nodes in other ML models.</li>
<li><kbd>ModelProto</kbd>: Used to define a model and its associated graph.</li>
<li><kbd>GraphProto</kbd>: Used to define the <strong>directed acyclic graph</strong> (<strong>DAG</strong>) structure of a neural network or the graph of other ML models.</li>
<li><kbd>TensorProto</kbd>: Used to define an N-dimensional tensor.</li>
<li><kbd>TypeProto</kbd>: Used to define the ONNX data types.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ONNX operators</h1>
                </header>
            
            <article>
                
<p>The definition of an operator in ONNX can be found in the <kbd>onnx/onnx-operators.in.proto</kbd> file in the ONNX source code. We can find the definitions of <kbd>OperatorProto</kbd>, <kbd>OperatorSetProto</kbd>, and <kbd>FunctionProto</kbd> in this file.</p>
<p>The actual definitions of all the operators supported in ONNX can be found in C++ source files named <kbd>defs.cc</kbd> in subdirectories under the <kbd>onnx/defs</kbd> directory in the ONNX source code. For example, many of the common neural network operators can be found defined in the <kbd>onnx/defs/math/defs.cc</kbd> and <kbd>onnx/defs/nn/defs.cc</kbd> files in the ONNX source code.</p>
<p>For another example, consider the ReLU operator that we introduced in <a href="3c2dd7d3-b762-49a3-a5d6-0b791eadadb2.xhtml">Chapter 3</a>, <em>Training Networks</em>. This operator has the name <kbd>Relu</kbd> (note the lower case <em>lu</em>) in ONNX and is defined in the <kbd>onnx/defs/math/defs.cc</kbd> file in the ONNX source code as follows:</p>
<pre>static const char* Relu_ver6_doc = R"DOC(<br/>    Relu takes one input data (Tensor&lt;T&gt;) and produces one output data<br/>    (Tensor&lt;T&gt;) where the rectified linear function, y = max(0, x), is <br/>    applied to the tensor elementwise.<br/>    )DOC";<br/><br/>ONNX_OPERATOR_SET_SCHEMA(<br/>    Relu,<br/>    6,<br/>    OpSchema()<br/>        .SetDoc(Relu_ver6_doc)<br/>        .Input(0, "X", "Input tensor", "T")<br/>        .Output(0, "Y", "Output tensor", "T")<br/>        .TypeConstraint(<br/>            "T",<br/>            {"tensor(float16)", "tensor(float)", "tensor(double)"},<br/>            "Constrain input and output types to float tensors.")<br/>        .TypeAndShapeInferenceFunction(propagateShapeAndTypeFromFirstInput));</pre>
<p>We can see that every operator is defined using the <kbd>ONNX_OPERATOR_SET_SCHEMA</kbd> macro. This macro is defined in the <kbd>onnx/defs/schema.h</kbd> source file as follows:</p>
<pre>#define ONNX_OPERATOR_SET_SCHEMA(name, ver, impl) \<br/>ONNX_OPERATOR_SET_SCHEMA_EX(name, Onnx, ONNX_DOMAIN, ver, true, impl)</pre>
<p>We can see that every operator definition has three components: name (<kbd>name</kbd>), version (<kbd>ver</kbd>) and implementation (<kbd>impl</kbd>).</p>
<p>Thus, for the example of the <kbd>Relu</kbd> operator we saw in the preceding definition, we can deduce the following characteristics:</p>
<ul>
<li><strong>Name</strong>: The name of the operator in ONNX. In this case, it is <kbd>Relu</kbd>. Note that individual DL frameworks might map this name to a distinct operator or layer name in their own DL framework. That is, the name in ONNX and the corresponding name in the DL framework may not always be the same.</li>
<li><strong>Version</strong>: The version of the definition of this operator. In this case, it is version 6.</li>
<li><strong>Implementation</strong>:</li>
<li style="padding-left: 30px">A documentation string explaining what the operator does. In this case, it is as follows:</li>
</ul>
<div class="packt_quote">"Relu takes one input data (Tensor&lt;T&gt;) and produces one output data (Tensor&lt;T&gt;) where the rectified linear function, y = max(0, x), is applied to the tensor elementwise."</div>
<ul>
<li style="padding-left: 30px">The input operands. In this case, a single tensor.</li>
<li style="padding-left: 30px">The output operands. In this case, a single tensor.</li>
<li style="padding-left: 30px">Constraints on the data type of the tensor values. In this case, ONNX is stating that it only supports data types of float (32-bit), double (64-bit) and float16 (16-bit, sometimes called half) for tensor values.</li>
<li style="padding-left: 30px">A function to infer the type and shape of the tensor operands. In this case, it states that the output tensor must have the same type and shape as the input tensor. It does this by using the function named <kbd>propagateShapeAndTypeFromFirstInput</kbd>.</li>
</ul>
<p class="mce-root"/>
<p>From the example of the preceding definition of the Relu operator, we can see that every operator definition has a lot of documentation embedded in it. All of this is used to auto-generate the complete ONNX operator documentation. This auto-generated documentation can be found as the <kbd>docs/Operators.md</kbd> files in the ONNX source code. This is a useful reference when we are searching for a suitable ONNX operator or trying to understand the details of a particular ONNX operator.</p>
<p>For example, the auto-generated documentation of the <kbd>Relu</kbd> operator that we considered previously appears as shown as follows in Figure 5.1:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-334 image-border" src="assets/56a9af30-3dbc-4a97-89d4-f8d536d158f7.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5.1: Auto-generated Relu operator documentation in ONNX</div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ONNX in Caffe2</h1>
                </header>
            
            <article>
                
<p>Caffe2 has built-in support for ONNX. This includes support for exporting Caffe2 models to ONNX format and importing ONNX models directly for inference in Caffe2. C++ source files related to Caffe2's support of ONNX can be found in the <kbd>onnx</kbd> directory in the Caffe2 source code. Python source files that provide the frontend and backend support for ONNX can be found in the <kbd>python/onnx</kbd> directory in the Caffe2 source code.</p>
<p>The <kbd>onnx/onnx_exporter.h</kbd> and <kbd>onnx/onnx_exporter.cc</kbd> contain the definitions necessary to export a Caffe2 model to ONNX format. Support for exporting from Caffe2 to ONNX includes details such as the mapping from Caffe2 to ONNX for operators, data types, and transformations of data.</p>
<p>For example, in <kbd>onnx/onnx_exporter.cc</kbd> we find the following mapping of some Caffe2 operators to ONNX operators:</p>
<pre>const std::unordered_map&lt;std::string, std::string&gt;&amp;<br/>OnnxExporter::get_renamed_operators() const {<br/>  const static std::unordered_map&lt;std::string, std::string&gt; kRenamedOperators{<br/>      {"SpatialBN", "BatchNormalization"},<br/>      {"Conv1D", "Conv"},<br/>      {"Conv2D", "Conv"},<br/>      {"Conv3D", "Conv"},<br/>      {"ConvTranspose1D", "ConvTranspose"},<br/>      {"ConvTranspose2D", "ConvTranspose"},<br/>      {"ConvTranspose3D", "ConvTranspose"},<br/>      {"MaxPool1D", "MaxPool"},<br/>      {"MaxPool2D", "MaxPool"},<br/>      {"MaxPool3D", "MaxPool"},<br/>      {"AveragePool1D", "AveragePool"},<br/>      {"AveragePool2D", "AveragePool"},<br/>      {"AveragePool3D", "AveragePool"}};<br/>  return kRenamedOperators;<br/>}</pre>
<p>Every DL framework that uses ONNX will have such a mapping. This is because every DL framework tends to have its own distinct operator or layer naming and a distinct jargon of defining the operator characteristics and relationships between operators. So, a clear and complete mapping is necessary for a DL framework to be able to digest an ONNX model definition into its own graph definition.</p>
<p>From the mapping between Caffe2 and ONNX we can see that the Caffe2 <kbd>SpatialBN</kbd> operator is renamed as the <kbd>BatchNormalization</kbd> operator in ONNX. Similarly, the Caffe2 <kbd>Conv2D</kbd> operator is renamed as the <kbd>Conv</kbd> operator in ONNX.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exporting the Caffe2 model to ONNX</h1>
                </header>
            
            <article>
                
<p>Caffe2 models can be easily exported to ONNX format using Python. This enables a vast number of other DL frameworks to use our Caffe2 models for training and inference. The <kbd>frontend</kbd> module provided by Caffe2-ONNX does all of the heavy lifting of the exporting. This module is located as the <kbd>python/onnx/frontend.py</kbd> file in the Caffe2 source code.</p>
<p>The <kbd>ch5/export_to_onnx.py</kbd> script provided along with this book's source code shows how to export an existing Caffe2 model to ONNX format. As an example, consider converting the Caffe2 model of AlexNet that we created in <a href="95863955-3504-48ab-a217-e95339a754d3.xhtml"/><a href="95863955-3504-48ab-a217-e95339a754d3.xhtml">Chapter 4</a>, <em>Working wi</em><em>th Caffe</em>. We exported the operators and the weights of this network in Caffe2 to the files <kbd>predict_net.pb</kbd> and <kbd>init_net.pb</kbd> files respectively.</p>
<p>We can invoke the ONNX conversion script, as follows, to convert this Caffe2 model to an ONNX file named <kbd>alexnet.onnx</kbd>:</p>
<pre><strong>./export_to_onnx.py predict_net.pb init_net.pb alexnet.onnx</strong></pre>
<p>Let's look at the pertinent sections of this script that help us to export from Caffe2 to ONNX.</p>
<p>First are the imports, which are seen in the following code:</p>
<pre><strong>import onnx import caffe2.python.onnx.frontend from caffe2.proto import caffe2_pb2</strong></pre>
<p>The <kbd>caffe2.proto.caffe2_pb2</kbd> module has the functionality needed to import the Caffe2 models stored in the <kbd>protobuf</kbd> format. The <kbd>onnx</kbd> and <kbd>caffe2.python.onnx.frontend</kbd> modules have the functionality that's necessary to export to ONNX format.</p>
<p>In the following script we also define the name and shape of the inputs to the model:</p>
<pre>INPUT_NAME = "data"
INPUT_SHAPE = (1, 3, 227, 227)</pre>
<p>In <a href="95863955-3504-48ab-a217-e95339a754d3.xhtml"/><a href="95863955-3504-48ab-a217-e95339a754d3.xhtml">Chapter 4</a>, <em>Working with Caffe</em>, you might have noticed that the input layer and parameters are annotated in the Caffe <kbd>protobuf</kbd> format. However, this information is not stored in both the Caffe2 <kbd>protobuf</kbd> format and the ONNX format. We would need to explicitly indicate the name and shape of the input whenever we use a Caffe2 and ONNX model.</p>
<p>We used an AlexNet model in this example, which has input named <kbd>data</kbd>, and the input shape is <kbd>(1, 3, 227, 227)</kbd>. Note that not all models have this input shape. For example, popular CNN models have inputs with the shape <kbd>(1, 3, 224, 224)</kbd>.</p>
<p>We are now ready to read in the Caffe2 model files using the <kbd>caffe2_pb2</kbd> methods, as shown in the following example:</p>
<pre># Read Caffe2 predict and init model files to protobuf

predict_net = caffe2_pb2.NetDef()
with open(predict_net_fpath, "rb") as f:
    predict_net.ParseFromString(f.read())

init_net = caffe2_pb2.NetDef()
with open(init_net_fpath, "rb") as f:
    init_net.ParseFromString(f.read())</pre>
<p>We need to read in both the <kbd>predict_net.pb</kbd> and <kbd>init_net.pb</kbd> Caffe2 model files, representing the network and its weights respectively. We do this by using the familiar <kbd>ParserFromString</kbd> method, which originates from the Google ProtoBuf Python library.</p>
<p>Next we should initialize the data type and tensor shape of the input and associate that information with the input name using a Python dictionary, as follows:</p>
<pre># Network input type, shape and name

data_type = onnx.TensorProto.FLOAT
value_info = {INPUT_NAME: (data_type, INPUT_SHAPE)}</pre>
<p>We can now convert the Caffe2 <kbd>protobuf</kbd> objects to an ONNX <kbd>protobuf</kbd> object using the <kbd>caffe2_net_to_onnx_model</kbd> method of the <kbd>frontend</kbd> module, as follows:</p>
<pre># Convert Caffe2 model protobufs to ONNX

onnx_model = caffe2.python.onnx.frontend.caffe2_net_to_onnx_model(
    predict_net,
    init_net,
    value_info,
)</pre>
<p>Note how this conversion method needs the input information, stored in <kbd>value_info</kbd>, for the conversion.</p>
<p>Finally, we can serialize the ONNX <kbd>protobuf</kbd> object to a byte buffer using the ProtoBuf <kbd>SerializeToString</kbd> method and then write that buffer to disk, as follows:</p>
<pre># Write ONNX protobuf to file

print("Writing ONNX model to: " + onnx_model_fpath)
with open(onnx_model_fpath, "wb") as f:
    f.write(onnx_model.SerializeToString())</pre>
<p>The full source code of the <kbd>ch5/export_to_onnx.py</kbd> script is listed as follows:</p>
<pre>#!/usr/bin/env python2

"""Script to convert Caffe2 model files to ONNX format.

Input is assumed to be named "data" and of dims (1, 3, 227, 227)
"""

# Std
import sys

# Ext
import onnx
import caffe2.python.onnx.frontend
from caffe2.proto import caffe2_pb2

INPUT_NAME = "data"
INPUT_SHAPE = (1, 3, 227, 227)

def main():

    # Check if user provided all required inputs
    if len(sys.argv) != 4:
        print(__doc__)
        print("Usage: " + sys.argv[0] + " &lt;path/to/caffe2/predict_net.pb&gt; &lt;path/to/caffe2/init_net.pb&gt; &lt;path/to/onnx_output.pb&gt;")
        return

    predict_net_fpath = sys.argv[1]
    init_net_fpath = sys.argv[2]
    onnx_model_fpath = sys.argv[3]

    # Read Caffe2 model files to protobuf

    predict_net = caffe2_pb2.NetDef()
    with open(predict_net_fpath, "rb") as f:
        predict_net.ParseFromString(f.read())

    init_net = caffe2_pb2.NetDef()
    with open(init_net_fpath, "rb") as f:
        init_net.ParseFromString(f.read())

    print("Input Caffe2 model name: " + predict_net.name)

    # Network input type, shape and name

    data_type = onnx.TensorProto.FLOAT
    value_info = {INPUT_NAME: (data_type, INPUT_SHAPE)}

    # Convert Caffe2 model protobufs to ONNX

    onnx_model = caffe2.python.onnx.frontend.caffe2_net_to_onnx_model(
        predict_net,
        init_net,
        value_info,
    )

    # Write ONNX protobuf to file

    print("Writing ONNX model to: " + onnx_model_fpath)
    with open(onnx_model_fpath, "wb") as f:
        f.write(onnx_model.SerializeToString())


if __name__ == "__main__":
    main()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the ONNX model in Caffe2</h1>
                </header>
            
            <article>
                
<p>In the previous section, we converted a Caffe2 model to ONNX format so that it could be used with other DL frameworks. In this section, we will learn how to use an ONNX model exported from other DL frameworks into Caffe2 for inference.</p>
<p>The <kbd>backend</kbd> module provided in the Caffe2 ONNX package enables this import of the ONNX model to Caffe2. This can be seen in the <kbd>backend.py</kbd> file in the <kbd>python/onnx</kbd> directory in the Caffe2 source code.</p>
<p>The <kbd>ch5/run_onnx_model.py</kbd> script provided along with this book's source code demonstrates how to load an ONNX model to Caffe2, and run an inference on an input image using that model.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The script first imports the Python modules necessary to work with the images (<kbd>PIL.Image</kbd>), Caffe2, and ONNX (<kbd>caffe2.python.onnx.backend</kbd>) as follows:</p>
<pre class="mce-root"># Std
import PIL.Image
import json
import sys

# Ext
import numpy as np
from caffe2.python import workspace
import onnx
import caffe2.python.onnx.backend</pre>
<p>The <kbd>prepare_input_image</kbd> method reads in an image from the input file path and prepares it to be passed as a blob to Caffe2, as shown in the following example:</p>
<pre class="mce-root">def prepare_input_image(img_fpath):
    """Read and prepare input image as AlexNet input."""

    # Read input image as 3-channel 8-bit values
    pil_img = PIL.Image.open(sys.argv[1])

    # Resize to AlexNet input size
    res_img = pil_img.resize((IMG_SIZE, IMG_SIZE), PIL.Image.LANCZOS)

    # Convert to NumPy array and float values
    img = np.array(res_img, dtype=np.float32)

    # Change HWC to CHW
    img = img.swapaxes(1, 2).swapaxes(0, 1)

    # Change RGB to BGR
    img = img[(2, 1, 0), :, :]

    # Mean subtraction
    img = img - MEAN

    # Change CHW to NCHW by adding batch dimension at front
    img = img[np.newaxis, :, :, :]

    return img</pre>
<p>In the preceding code, we first used the <kbd>PIL.Image</kbd> module to read in the image from the input file as a 3-channel byte values. We then resized the image to the size required by AlexNet and used NumPy, which made the rest of the image processing easier. PIL reads the image channels in the order <kbd>HWC</kbd> (height, width, channel) and the channels are in <kbd>RGB</kbd> order. But AlexNet expects the data to be laid out as <kbd>BGR</kbd> channels of <kbd>HW</kbd> size. So, we converted to that format. Finally, we subtracted the mean from the image values and then added in a batch dimension in front to reformat the data to <kbd>NCHW</kbd> format.</p>
<p>Loading the ONNX model from a file is easy if you use the <kbd>load</kbd> method from the <kbd>onnx</kbd> package, as follows:</p>
<pre>model = onnx.load("alexnet.onnx")</pre>
<p>Finally, we can use the loaded ONNX model for inference directly, using the <kbd>predict_img_class</kbd> method described as follows:</p>
<pre>def predict_img_class(onnx_model, img):
    """Get image class determined by network."""

    results = caffe2.python.onnx.backend.run_model(onnx_model, [img])
    class_index = np.argmax(results[0])
    class_prob = results[0][0, class_index]

    imgnet_classes = json.load(open("imagenet1000.json"))
    class_name = imgnet_classes[class_index]

    return class_index, class_name, class_prob</pre>
<p>We need to use the <kbd>run_model</kbd> method, provided by Caffe2 ONNX backend <kbd>caffe2.python.backend</kbd>, to pass the inputs and obtain the results after inference through this model. Because we used an ImageNet model, we should use a JSON file with the mapping from the ImageNet class index number to its class name. We should pick the class index with the highest probability value and find its ImageNet class name.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing the ONNX model</h1>
                </header>
            
            <article>
                
<p>When working with ONNX models, it can be useful to have a tool that can help in visualizing the network structure. ONNX ships with such a script called <kbd>net_drawer.py</kbd>. You can find this tool in the <kbd>onnx/onnx/tools</kbd> directory in the ONNX source repository. If you installed ONNX from its Python package, then you can find this script at <kbd>/usr/local/lib/python2.7/dist-packages/onnx/tools/net_drawer.py</kbd>.</p>
<p class="mce-root"/>
<p>This script can be applied to convert an ONNX file to a directed acyclic graph representation of the network in the GraphViz DOT format. For example, consider the ONNX file <kbd>alexnet.onnx</kbd> that we obtained in the earlier section on converting from the Caffe2 model to the ONNX model.</p>
<p>We can convert this AlexNet ONNX file to a DOT file using the following command:</p>
<pre><strong>$ python /usr/local/lib/python2.7/dist-packages/onnx/tools/net_drawer.py --input alexnet.onnx --output alexnet.dot</strong></pre>
<p>To convert the DOT file to a PNG image file for viewing, use the following command:</p>
<pre><strong>$ dot alexnet.dot -Tpng -o alexnet.png</strong></pre>
<p>The image thus produced, shows the visualization of AlexNet</p>
<p>Another excellent visualization tool for ONNX models is Netron. The usage of this tool is covered in <a href="91e4cdcf-24f6-4426-ac95-b6845c020d83.xhtml">Chapter 7</a>, <em>Caffe2 at the Edge and in the cloud</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced the details of the ONNX format, a popular representation for DL models. We examined how it depicts the intermediate representation and operators. We then looked at support for ONNX in Caffe2. Using AlexNet as the example, we looked at how to convert a Caffe2 model file to ONNX format. We also looked at the reverse process: importing an ONNX model file into Caffe2, and then using it for inference. Finally, we looked at a useful tool to visualize the graph representation of an ONNX file.</p>


            </article>

            
        </section>
    </body></html>