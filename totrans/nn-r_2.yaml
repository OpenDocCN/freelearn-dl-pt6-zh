- en: Learning Process in Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络中的学习过程
- en: Just as there are many different types of learning and approaches to human learning,
    so we can say about the machines as well. To ensure that a machine will be able
    to learn from experience, it is important to define the best available methodologies
    depending on the specific job requirements. This often means choosing techniques
    that work for the present case and evaluating them from time to time, to determine
    if we need to try something new.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如人类学习有许多不同的类型和方法一样，机器学习也可以有不同的方式。为了确保机器能够从经验中学习，定义最适合的学习方法非常重要，这取决于具体的工作要求。这通常意味着选择当前情况下有效的技术，并不时进行评估，以决定是否需要尝试新的方法。
- en: We have seen the basics of neural networks in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, and also two simple implementations
    using R. In this chapter, we will deal with the learning process, that is how
    to train, test, and deploy a neural network machine learning model. The training
    phase is used for learning, to fit the parameters of the neural networks. The
    testing phase is used to assess the performance of fully-trained neural networks.
    Finally, in the deployment phase, actual data is passed through the model to get
    the prediction.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第1章](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4)中学习了神经网络的基础知识，*神经网络与人工智能概念*，并通过R语言进行了两个简单的实现。在本章中，我们将探讨学习过程，也就是如何训练、测试和部署神经网络机器学习模型。训练阶段用于学习，以调整神经网络的参数。测试阶段用于评估完全训练好的神经网络的性能。最后，在部署阶段，实际数据会通过模型进行预测。
- en: 'The following is the list of concepts covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章涵盖的概念列表：
- en: Learning process
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习过程
- en: Supervised learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有监督学习
- en: Unsupervised learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Training, testing, and deploying a model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练、测试和部署模型
- en: Evaluation metrics-error measurement and fine tuning; measuring accuracy of
    a model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估指标——误差测量与微调；衡量模型的准确性
- en: Supervised learning model using neural networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络的有监督学习模型
- en: Unsupervised learning model using neural networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络的无监督学习模型
- en: Backpropagation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: By the end of the chapter, we will understand the basic concepts of the learning
    process and how to implement it in the R environment. We will discover different
    types of algorithms to implement a neural network. We will learn how to train,
    test, and deploy a model. We will know how to perform a correct valuation procedure.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将理解学习过程的基本概念，并学习如何在R环境中实现它。我们将发现实现神经网络的不同类型的算法。我们将学习如何训练、测试和部署模型。我们将知道如何执行正确的评估程序。
- en: What is machine learning?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习是什么？
- en: 'What do we mean by the term machine learning? The definition is quite difficult,
    to do so, we are asking large field of scientists to help. We can mention an artificial
    intelligence pioneer''s quote:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是什么意思？这个定义相当复杂，因此我们需要请广大科学家们帮助解答。我们可以提到一位人工智能先驱的名言：
- en: '*"Field of study that gives computers the ability to learn without being explicitly
    programmed."*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*“一个研究领域，赋予计算机无需明确编程即可学习的能力。”*'
- en: – Arthur Samuel
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: – 阿瑟·塞缪尔
- en: Machine learning is about training a model or an algorithm with data and then
    using the model to predict any new data. For example, a toddler is taught how
    to walk from his crawling phase. Initially, the toddler's parents hold the toddler's
    hand to help him up, and he is taught through the data that is given. On the basis
    of these procedures, if an obstacle presents itself in the toddler's path or if
    there is a turn somewhere, the toddler is able to navigate on his own after the
    training. The data used for training is the training data and the recipient continues
    to learn even after the formal training.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是指通过数据训练模型或算法，然后使用该模型预测新的数据。例如，一个幼儿从爬行阶段被教会如何走路。最初，幼儿的父母扶着孩子的手帮助他站起来，并通过给定的数据进行教学。基于这些过程，如果在幼儿的路上出现障碍，或者某个地方需要转弯，在训练之后，幼儿能够自己导航。用于训练的数据称为训练数据，接受者在正式训练后仍然继续学习。
- en: Machines too can be taught like toddlers to do a task based on training. First,
    we feed enough data to tell the machine what needs to be done on what circumstances.
    After the training, the machine can perform automatically and can also learn to
    fine-tune itself. This type of training the machine is called **machine learning**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器也可以像教小孩子一样，基于训练来完成任务。首先，我们输入足够的数据来告诉机器在什么情况下需要做什么。经过训练后，机器可以自动执行任务，还能学会自我调整。这种训练机器的方式叫做**机器学习**。
- en: The main difference between machine learning and programming is that there is
    no coding/programming involved in machine learning, while programming is about
    giving the machine a set of instructions to perform. In machine learning, the
    data is the only input provided and the model is based on the algorithm we have
    decided to use.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与编程的主要区别在于，机器学习不涉及编码/编程，而编程是给机器一组执行指令的过程。在机器学习中，数据是唯一的输入，而模型是基于我们决定使用的算法。
- en: 'The algorithm to be used is based on various factors of the data: the features
    (or independent variables), the type of dependent variable(s), the accuracy of
    the model, and the speed of training and prediction of the model.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的算法是基于数据的各种因素：特征（或自变量）、因变量的类型、模型的准确性以及模型训练和预测的速度。
- en: 'Based on the independent variable(s) of the machine learning data, there are
    three different ways to train a model:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基于机器学习数据的自变量，有三种不同的方式来训练模型：
- en: Supervised learning
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reinforcement learning
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'The following figure shows the different algorithms to train a machine learning
    model:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了用于训练机器学习模型的不同算法：
- en: '![](img/00035.gif)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00035.gif)'
- en: In the following sections, we will go through them on by one.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将逐一讲解这些内容。
- en: Supervised learning
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: '**Supervised learning** is a learning method where there is a part of the training
    data which acts as a teacher to the algorithm to determine the model. The machine
    is taught what to learn from the target data. The target data, or dependent or
    response variables, are the outcome of the collective action of the independent
    variables. The network training is done with the target data and its behavior
    with patterns of input data. The target labels are known in advance and the data
    is fed to the algorithm to derive the model.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**是一种学习方法，其中一部分训练数据作为算法的“老师”，帮助确定模型。机器被教会如何从目标数据中学习。目标数据或因变量是自变量的集体作用结果。网络训练是通过目标数据及其与输入数据模式的关系进行的。目标标签是事先已知的，数据被输入算法以推导模型。'
- en: Most of neural network usage is done using supervised learning. The weights
    and biases are adjusted based on the output values. The output can be categorical
    (like true/false or 0/1/2) or continuous (like 1,2,3, and so on). The model is
    dependent on the type of output variables, and in the case of neural networks,
    the output layer is built on the type of target variable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数神经网络的使用是基于监督学习的。权重和偏差根据输出值进行调整。输出可以是分类的（如真/假或0/1/2）或连续的（如1、2、3，依此类推）。模型依赖于输出变量的类型，而在神经网络中，输出层是根据目标变量的类型来构建的。
- en: For neural networks, all the independent and dependent variables need to be
    numeric, as a neural network is based on mathematical models. It is up to the
    data scientist to convert the data to numbers to be fed into the model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络，所有的自变量和因变量需要是数值型的，因为神经网络是基于数学模型的。数据科学家需要将数据转换为数值形式，以便输入模型。
- en: 'Supervised learning is depicted by the following diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习通过下图来表示：
- en: '![](img/00036.gif)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00036.gif)'
- en: Unsupervised learning
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In unsupervised learning (or self organization), the output layer is trained
    to organize the input data into another set of data without the need of a target
    variable. The input data is analyzed and patterns are found in it to derive the
    output, as shown in the following figure. Since there is no teacher (or target
    variable), this type of learning is called **unsupervised learning**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习（或自组织）中，输出层被训练以将输入数据组织成另一组数据，而无需目标变量。输入数据会被分析，并从中找到模式以推导输出，如下图所示。由于没有教师（或目标变量），这种学习方式称为**无监督学习**。
- en: '![](img/00037.gif)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00037.gif)'
- en: 'The different techniques available for unsupervised learning are as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习中可用的不同技术如下：
- en: Clustering (K-means, hierarchical)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类（K均值，层次聚类）
- en: Association techniques
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self Organizing Map **(**SOM**)/ Kohonen networks'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To summarize, the two main types of machine learning are depicted in the following
    figure:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00038.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: For neural networks, we have both the types available, using different ways
    available in R.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is a type of machine learning where there is constant
    feedback given to the model to adapt to the environment. There is a performance
    evaluation at each step to improve the model. For neural networks, there is a
    special type called **Q-learning**, combined with neuron to implement reinforcement
    learning in the backpropagation feedback mechanism. The details are out of scope
    of this book.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the three types of learnings we have covered so far:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00039.gif)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Training and testing the model
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training and testing the model forms the basis for further usage of the model
    for prediction in predictive analytics. Given a dataset of *100* rows of data,
    which includes the predictor and response variables, we split the dataset into
    a convenient ratio (say *70:30*) and allocate *70* rows for training and *30*
    rows for testing. The rows are selected in random to reduce bias.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Once the training data is available, the data is fed to the neural network to
    get the massive universal function in place. The training data determines the
    weights, biases, and activation functions to be used to get to output from input.
    Until recently, we could not say that a weight has a positive or a negative influence
    on the target variable. But now we've been able to shed some light inside the
    black box. For example, by plotting a trained neural network, we can discover
    trained synaptic weights and basic information about the training process.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Once the sufficient convergence is achieved, the model is stored in memory and
    the next step is testing the model. We pass the *30* rows of data to check if
    the actual output matches with the predicted output from the model. The evaluation
    is used to get various metrics which can validate the model. If the accuracy is
    too wary, the model has to be re-built with change in the training data and other
    parameters passed to the neural net function. We will cover more about the evaluation
    metrics later in this chapter.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: After training and testing, the model is said to be deployed, where actual data
    is passed through the model to get the prediction. For example, the use case may
    be determining a fraud transaction or a home loan eligibility check based on various
    input parameters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'The training, testing, and deployment is represented in the following figure:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00040.gif)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: So far, we have focused on the various algorithms available; it is now time
    to dedicate ourselves to the data that represents the essential element of each
    analysis.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: The data cycle
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data forms a key component for model building and the learning process.
    The data needs to be collected, cleaned, converted, and then fed to the model
    for learning. The overall data life cycle is shown as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是模型构建和学习过程中的关键组成部分。数据需要被收集、清洗、转换，然后输入模型进行学习。整体数据生命周期如下所示：
- en: '![](img/00041.gif)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00041.gif)'
- en: One of the critical requirements for modeling is having good and balanced data.
    This helps in higher accuracy models and better usage of the available algorithms.
    A data scientist's time is mostly spent on cleansing the data before building
    the model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 建模的一个关键要求是拥有良好且平衡的数据。这有助于提高模型的准确度，并更好地利用现有的算法。数据科学家的大部分时间都花费在数据清洗上，然后才能开始建模。
- en: We have seen the training and testing before deployment of the model. For testing,
    the results are captured as evaluation metrics, which helps us decide if we should
    use a particular model or change it instead.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到模型部署前的训练和测试过程。在测试时，结果作为评估指标进行记录，帮助我们决定是否使用某个模型，或是否需要改变模型。
- en: We will see the evaluation metrics next.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看到评估指标。
- en: Evaluation metrics
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标
- en: Evaluating a model involves checking if the predicted value is equal to the
    actual value during the testing phase. There are various metrics available to
    check the model, and they depend on the state of the target variable.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一个模型涉及在测试阶段检查预测值是否等于实际值。有多种评估指标可用，它们取决于目标变量的状态。
- en: 'For a binary classification problem, the predicted target variable and the
    actual target variable can be in any of the following four states:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类问题，预测目标变量和实际目标变量可以处于以下四种状态之一：
- en: '| **Predicted** | **Actual** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **预测** | **实际** |'
- en: '| *Predicted = TRUE* | *Actual = TRUE* |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| *预测 = 真* | *实际 = 真* |'
- en: '| *Predicted = TRUE* | *Actual = FALSE* |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| *预测 = 真* | *实际 = 假* |'
- en: '| *Predicted = FALSE* | *Actual = TRUE* |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| *预测 = 假* | *实际 = 真* |'
- en: '| *Predicted = FALSE* | *Actual = FALSE* |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| *预测 = 假* | *实际 = 假* |'
- en: When we have the predicted and actual values as same values, we are said to
    be accurate. If all predicted and actual values are same (either all *TRUE* or
    all *FALSE*), the model is *100* percent accurate. But, this is never the case.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当预测值与实际值相同，我们认为模型是准确的。如果所有预测值和实际值都相同（无论是全为*真*还是全为*假*），则模型准确度为*100*%。但实际情况中永远不会如此。
- en: Since neural networks are approximation models, there is always a bit of error
    possible. All the four states mentioned in the previous table are possible.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络是近似模型，总会有一些误差的可能性。前述表格中的四种状态都是可能的。
- en: 'We define the following terminology and metrics for a model:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为模型定义以下术语和指标：
- en: '**True Positives** (**TP**):All cases where the predicted and actual are both
    *TRUE* (good accuracy).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性** (**TP**)：所有预测值与实际值均为*真*的情况（准确度良好）。'
- en: '**True Negative** (**TN**): All cases when predicted is *FALSE* and the actual
    is also *FALSE* (good accuracy).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真负例** (**TN**)：所有预测为*假*且实际也是*假*的情况（准确度良好）。'
- en: '**False Positive **(**FP**):This is a case when we predict something as positive
    (*TRUE*), but it is actually negative. It is like a false alarm or an FP error.
    An example is when a male is predicted to be pregnant by a pregnancy test kit.
    All cases when predicted is *TRUE*, while the actual is *FALSE*. This is also
    called **type 1 error**.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性** (**FP**)：这是当我们预测某个值为阳性（*真*），但实际为阴性时的情况。就像是误报或FP错误。例如，当预测男性为孕妇时。所有预测为*真*，而实际为*假*的情况。这也叫做**第一类错误**。'
- en: '**False Negative** (**FN**):When we predict something as *FALSE*, but in actuality
    it is *TRUE*, then the case is called FN. For example, when a pregnant female
    is predicted as not being pregnant by a pregnancy test kit, it is an FN case.
    All cases when predicted is *FALSE* and actual *TRUE*. This is also called **type
    2 error**.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假负例** (**FN**)：当我们预测某个值为*假*，但实际却为*真*时，该情况称为FN。例如，当孕妇被孕检试剂预测为未怀孕时，就是一个FN案例。所有预测为*假*且实际为*真*的情况。这也叫做**第二类错误**。'
- en: Confusion matrix
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'When the values of the classification are plotted in a *nxn* matrix (*2x2*
    in case of binary classification), the matrix is called the **confusion matrix**.
    All the evaluation metrics can be derived from the confusion matrix itself:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当分类值被绘制成一个*nxn*矩阵（在二分类情况下为*2x2*矩阵）时，该矩阵称为**混淆矩阵**。所有评估指标都可以从混淆矩阵本身得出：
- en: '|  | **Predicted value** | **Predicted value** |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测值** | **预测值** |'
- en: '| *Actual values* | *TRUE* | *FALSE* |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| *实际值* | *真* | *假* |'
- en: '| *TRUE* | *TP* | *FN* |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| *真* | *TP* | *FN* |'
- en: '| *FALSE* | *FP* | *TN* |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| *假* | *FP* | *TN* |'
- en: Now, let's look at some evaluation metrics in detail.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们详细看看一些评估指标。
- en: True Positive Rate
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真阳性率
- en: '**True Positive Rate** (**TPR**) or sensitivity or recall or hit rate is a
    measure of how many true positives were identified out of all the positives identified:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**真阳性率** (**TPR**) 或敏感性或召回率或命中率，是指从所有识别出的正样本中，正确识别的真阳性比例：'
- en: '![](img/00042.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00042.jpeg)'
- en: Ideally, the model is better if we have this closer to one.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，如果模型的值接近1，那么模型的表现会更好。
- en: True Negative Rate
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真负率
- en: '**True Negative Rate** (**TNR**) or specificity is the ratio of true negatives
    and total number of negatives we have predicted:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**真负率** (**TNR**) 或特异度是正确预测的负样本与我们预测的所有负样本总数之比：'
- en: '![](img/00043.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00043.jpeg)'
- en: If this ratio is closer to zero, the model is more accurate.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个比率接近零，模型就更准确。
- en: Accuracy
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确率
- en: Accuracy is the measure of how good our model is. It is expected to be closer
    to 1, if our model is performing well.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是衡量我们模型好坏的标准。如果模型表现良好，它应该接近1。
- en: 'Accuracy is the ratio of correct predictions and all the total predictions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是正确预测与所有总预测的比率：
- en: '![](img/00044.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00044.jpeg)'
- en: Precision and recall
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确度和召回率
- en: Precision and recall are again ratios between the *TP* with (*TP+FP*) and *TP*
    with (*TP+FN*) respectively. These ratios determine how relevant our predictions
    are compared to the actual.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度和召回率分别是 *TP* 与 (*TP+FP*) 以及 *TP* 与 (*TP+FN*) 的比率。这些比率决定了我们的预测与实际的相关程度。
- en: Precision is defined as how many selected items are relevant. That is, how many
    of the predicted ones are actually correctly predicted.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度定义为选定项目中相关项目的比例。也就是说，预测的结果中有多少是实际正确的。
- en: 'The equation is:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式是：
- en: '![](img/00045.gif)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00045.gif)'
- en: If precision is closer to one, we are more accurate in our predictions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果精确度接近1，我们的预测就会更准确。
- en: 'Recall, on the other hand, tells how many relevant items we selected. Mathematically,
    it is:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率则告诉我们选择了多少相关项目。从数学上讲，它是：
- en: '![](img/00046.gif)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00046.gif)'
- en: 'The following diagram depicts clearly the discussion we have done so far:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表清晰地展示了我们迄今为止的讨论：
- en: '![](img/00047.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00047.jpeg)'
- en: F-score
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: F分数
- en: 'F-score, or F1-score, is another measure of accuracy. Technically, it is the
    harmonic mean of precision and recall:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: F分数，或F1分数，是衡量准确度的另一种方式。从技术上讲，它是精确度和召回率的调和均值：
- en: '![](img/00048.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00048.jpeg)'
- en: Receiver Operating Characteristic curve
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接收者操作特征曲线
- en: 'A **Receiver Operating Characteristic **(**ROC**) curve is a graphical visual
    that illustrates the predictive ability of a binary classifier system. The ROC
    curve is created by plotting a graph of the TPR against the **False Positive Rate**
    (**FPR**) at various threshold settings. This gives us **Sensitivity** versus
    (**1 - Specificity**). A ROC curve typically looks like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征** (**ROC**) 曲线是一种图形展示，说明二分类系统的预测能力。ROC曲线通过在各种阈值设置下绘制真阳性率（**TPR**）与**假阳性率**（**FPR**）的图表来创建。这给我们提供了**敏感性**与（**1
    - 特异度**）的关系。ROC曲线通常看起来是这样的：'
- en: '![](img/00049.gif)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00049.gif)'
- en: After acquiring the necessary skills, we are ready to analyze in detail the
    algorithms used for building the neural networks.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 获得必要技能后，我们已经准备好详细分析用于构建神经网络的算法。
- en: Learning in neural networks
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络中的学习
- en: As we saw in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, neural networks is a machine
    learning algorithm that has the ability to learn from data and give us predictions
    using the model built. It is a universal function approximation, that is, any
    input, output data can be approximated to a mathematical function.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4)《神经网络与人工智能概念》中看到的，神经网络是一种机器学习算法，具有从数据中学习并使用构建的模型进行预测的能力。它是一种通用的函数近似方法，也就是说，任何输入输出数据都可以逼近为数学函数。
- en: The forward propagation gives us an initial mathematical function to arrive
    at output(s) based on inputs by choosing random weights. The difference between
    the actual and predicted is called the error term. The learning process in a feed-forward
    neural network actually happens during the backpropagation stage. The model is
    fine tuned with the weights by reducing the error term in each iteration. Gradient
    descent is used in the backpropagation process.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正向传播通过选择随机权重为我们提供了一个初始的数学函数，用于根据输入得到输出。实际输出与预测输出之间的差异称为误差项。神经网络的学习过程实际上发生在反向传播阶段。通过减少每次迭代中的误差项，模型会对权重进行微调。在反向传播过程中使用梯度下降法。
- en: Let us cover the backpropagation in detail in this chapter, as it is an important
    machine learning aspect for neural networks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在本章详细讲解反向传播，因为它是神经网络中重要的机器学习方面。
- en: Back to backpropagation
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 返回到反向传播
- en: We have covered the forward propagation in detail in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, and a little about backpropagation
    using gradient descent. Backpropagation is one of the important concepts for understanding
    neural networks and it relies on calculus to update the weights and biases in
    each layer. Backpropagation of errors is similar to *learning from mistakes*.
    We correct ourselves in our mistakes (errors) in every iteration, until we reach
    a point called **convergence***. *The goal of backpropagation is to correct the
    weights in each layer and minimize the overall error at the output layer.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第1章](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4)中详细讨论了正向传播，*神经网络与人工智能概念*，以及使用梯度下降法的反向传播的一些内容。反向传播是理解神经网络的重要概念之一，它依赖于微积分来更新每一层的权重和偏置。误差的反向传播类似于*从错误中学习*。我们在每次迭代中纠正我们的错误（误差），直到达到一个称为**收敛**的点。*反向传播的目标是纠正每一层的权重，并最小化输出层的整体误差。
- en: 'Neural network learning heavily relies on backpropagation in feed-forward networks.
    The usual steps of forward propagation and error correction are explained as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的学习在前馈网络中极度依赖反向传播。正向传播和误差修正的常见步骤如下所示：
- en: Start the neural network forward propagation by assigning random weights and
    biases to each of the neurons in the hidden layer.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过为隐藏层中每个神经元分配随机的权重和偏置，开始神经网络的正向传播。
- en: Get the sum of *sum(weight*input) + bias* at each neuron.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个神经元上获取 *sum(weight*input) + bias* 的总和。
- en: Apply the activation function (*sigmoid*) at each neuron.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个神经元上应用激活函数（*sigmoid*）。
- en: Take this output and pass it onto the next layer neuron.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此输出传递给下一个层的神经元。
- en: If the layer is the output layer, apply the weights and get the sum of *sum(weight*input)
    + bias* at each output layer neuron.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果该层是输出层，应用权重并获取每个输出层神经元的 *sum(weight*input) + bias* 的总和。
- en: Again, apply the activation function at the output layer neuron.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，在输出层神经元应用激活函数。
- en: This forms the output of the neural network at the output layer for one forward
    pass.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就形成了神经网络在一次正向传播中的输出层输出。
- en: Now, with the training data, we can identify the error term at each output neuron,
    by subtracting the actual output and the activation function output value.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用训练数据，我们可以通过减去实际输出和激活函数输出值来识别每个输出神经元的误差项。
- en: 'The total of the errors is arrived at by using the following formula:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 误差的总和通过以下公式计算得出：
- en: '![](img/00050.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00050.jpeg)'
- en: A factor of *1/2* is used to cancel the exponent when the error function *E*
    is subsequently differentiated.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当误差函数 *E* 随后进行微分时，使用一个 *1/2* 的因子来消除指数。
- en: 'The gradient descent technique requires calculation of the partial derivative
    of the error term (*E*) with respect to the weights of the network. Calculating
    the partial derivative of the full error with respect to the weight *w[ij] *is
    done using the **chain rule**of differentiation:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度下降技术要求计算误差项（*E*）相对于网络权重的偏导数。使用**链式法则**来计算误差项相对于权重 *w[ij]* 的全误差的偏导数：
- en: '![](img/00051.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00051.jpeg)'
- en: The derivative is defined as the rate of change of a value, the gradient descent
    uses the derivative (or slope) to minimize the error term and arrive at a correct
    set of weights.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 导数定义为值变化的速率，梯度下降使用导数（或斜率）来最小化误差项并得到一组正确的权重。
- en: 'The first factor is partial derivative of the error term with respect to the
    output at that particular neuron *j* and *o[j]* is equal to *y*:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个因子是该特定神经元 *j* 对输出的误差项的偏导数，并且 *o[j]* 等于 *y*：
- en: '![](img/00052.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00052.jpeg)'
- en: 'The second factor in the chain rule is the partial derivative of the output
    of neuron *o[j ]* with respect to its input, and is the partial derivative of
    the activation function (the *sigmoid* function):'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 链式法则中的第二个因子是神经元 *o[j]* 对其输入的输出的偏导数，并且是激活函数（*sigmoid* 函数）的偏导数：
- en: '![](img/00053.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00053.jpeg)'
- en: Here *net[j ]*is the input to the neuron.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *net[j]* 是输入到神经元的值。
- en: The third term in the chain rule is simply *o[i]*.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 链式法则中的第三项只是 *o[i]*。
- en: '![](img/00054.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00054.jpeg)'
- en: 'Combining steps 11, 12, and 13, we get:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第 11 步、第 12 步和第 13 步结合，我们得到：
- en: '![](img/00055.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00055.jpeg)'
- en: The weight *w[ij]* at each neuron (any layer) is updated with this partial derivative,
    combined with the learning rate.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个神经元（任何层级）上的权重 *w[ij]* 会通过此偏导数与学习率结合进行更新。
- en: These steps are repeated until we have convergence of very low error term or
    a specified number of times.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤会重复进行，直到我们获得非常低的误差项的收敛或达到指定的次数。
- en: All the steps are taken care of internally in the R packages available. We can
    supply the learning rate along with various other parameters.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 所有步骤都在可用的 R 包中内部处理。我们可以提供学习率以及其他各种参数。
- en: 'The backpropagation is illustrated as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播过程如下所示：
- en: '![](img/00056.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00056.jpeg)'
- en: As with all things in life, even an algorithm has further improvement margins.
    In the next section, we'll see how to do it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 就像生活中的一切事物一样，即便是算法也有进一步改进的空间。在接下来的章节中，我们将看到如何实现这一点。
- en: Neural network learning algorithm optimization
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络学习算法优化
- en: The procedure used to carry out the learning process in a neural network is
    called the training algorithm. The learning algorithm is what the machine learning
    algorithm chooses as model with the best optimization. The aim is to minimize
    the loss function and provide more accuracy. Here we illustrate some of the optimization
    techniques, other than gradient descent.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中执行学习过程的程序称为训练算法。学习算法是机器学习算法选择的最佳优化模型。其目标是最小化损失函数并提高准确性。这里我们展示一些优化技术，除了梯度下降法。
- en: The **Particle Swarm Optimization** (**PSO**) method is inspired by observations
    of social and collective behavior on the movements of bird flocks in search of
    food or survival. It is similar to a fish school trying to move together. We know
    the position and velocity of the particles, and PSO aims at searching a solution
    set in a large space controlled by mathematical equations on position and velocity. It
    is bio-inspired from biological organism behavior for collective intelligence.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**粒子群优化**（**PSO**）方法受到鸟群在寻找食物或生存过程中的集体行为观察的启发。它类似于鱼群试图一起移动。我们知道粒子的位置和速度，PSO
    的目标是在由位置和速度控制的数学方程的指导下，在大空间中搜索解决方案集。它从生物体行为中汲取灵感，用于集体智能。'
- en: '**Simulated annealing** is a method that works on a probabilistic approach
    to approximate the global optimum for the cost function. The method searches for
    a solution in large space with simulation.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**模拟退火**是一种基于概率方法的技术，用来逼近代价函数的全局最优解。该方法在大空间中通过模拟搜索解决方案。'
- en: Evolutionary methods are derived from the evolutionary process in biology, and
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 进化方法源自生物学中的进化过程，且
- en: evolution can be in terms of reproduction, mutation, selection, and recombination.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 进化可以通过繁殖、变异、选择和重组来进行。
- en: A fitness function is used to determine the performance of a model, and based
    on this
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 适应度函数用于确定模型的性能，并基于此进行调整。
- en: function, we select our final model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 函数，我们选择最终模型。
- en: The **Expectation Maximization **(**EM**) methodis a statistical learning method
    that uses an iterative method to find maximum likelihood or maximum posterior
    estimate, thus minimizing the error.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**期望最大化**（**EM**）方法是一种统计学习方法，采用迭代方法找到最大似然或最大后验估计，从而最小化误差。'
- en: Supervised learning in neural networks
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络中的监督学习
- en: As previously mentioned, supervised learning is a learning method where there
    is a part of training data which acts as a teacher to the algorithm to determine
    the model. In the following section, an example of a regression predictive modeling
    problem is proposed to understand how to solve it with neural networks.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Boston dataset
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset describes 13 numerical properties of houses in Boston suburbs,
    and is concerned with modeling the price of houses in those suburbs in thousands
    of dollars. As such, this is a regression predictive modeling problem. Input attributes
    include things like crime rate, proportion of non-retail business acres, chemical
    concentrations, and more. In the following list are shown all the variables followed
    by a brief description:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of instances: *506*'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of attributes: *13* continuous attributes (including `class`'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: attribute `MEDV`), and one binary-valued attribute
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Each of the attributes is detailed as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '`crim` per capita crime rate by town.'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`zn` proportion of residential land zoned for lots over *25,000* square feet.'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`indus` proportion of non-retail business acres per town.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`chas` Charles River dummy variable (*= 1* if tract bounds river; *0* otherwise).'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`nox` nitric oxides concentration (parts per *10* million).'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`rm` average number of rooms per dwelling.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`age` proportion of owner-occupied units built prior to *1940*.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dis` weighted distances to five Boston employment centres'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`rad` index of accessibility to radial highways.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`tax` full-value property-tax rate per *$10,000*.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ptratio` pupil-teacher ratio by town.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`black` *1000(Bk - 0.63)^2* where *Bk* is the proportion of blacks by town.'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`lstat` percent lower status of the population.'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`medv` median value of owner-occupied homes in *$1000''s*.'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of these, `medv` is the response variable, while the other thirteen variables
    are possible predictors. The goal of this analysis is to fit a regression model
    that best explains the variation in `medv`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: There is a relation between the first thirteen columns and the `medv` response
    variable. We can predict the `medv` value based on the input thirteen columns.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is already provided with R libraries (`MASS`), as we will see later,
    so we do not have to worry about retrieving the data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Neural network regression with the Boston dataset
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will run a regression neural network for the `Boston` dataset.
    The `medv` value is predicted for the test data. The train to test split is *70:30*.
    The `neuralnet`function is used to model the data with a neural network:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Don't worry, now we will explain in detail the whole code, line by line.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first two lines of the code are simple, as they load the libraries we will
    use for later calculations. Specifically, the `neuralnet` library will help us
    to build and train the network, while the `MASS` library will serve us to load
    the `Boston` dataset that we have previously introduced in detail.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, for example, to install the `neuralnet` package, we should write:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, it should be emphasized that this function should be used only once
    and not every time you run the code. Instead, load the library through the following
    command and must be repeated every time you run the code:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The function `set.seed` sets the seed of R''s random number generator, which
    is useful for creating simulations or random objects that can be reproduced:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You have to use this function every time you want to get a reproducible random
    result. In this case, the random numbers are the same, and they would continue
    to be the same no matter how far out in the sequence we go.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command loads the `Boston` dataset, which, as we anticipated,
    is contained in the `MASS` library and saves it in a given frame:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Use the `str` function to view a compactly display the structure of an arbitrary
    R object. In our case, using `str(data)`, we will obtain the following results:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result obtained for the given object is shown in the following figure:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00057.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'Let''s go back to parse the code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We need this snippet of code to normalize the data.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Remember, it is good practice to normalize the data before training a neural
    network. With normalization, data units are eliminated, allowing you to easily
    compare data from different locations.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an extremely important procedure in building a neural network, as it
    avoids unnecessary results or very difficult training processes resulting in algorithm
    convergence problems. You can choose different methods for scaling the data (**z-normalization**,
    **min-max scale**, and so on). For this example, we will use the min-max method
    (usually called feature scaling) to get all the scaled data in the range *[0,1]*.
    The formula to achieve this is the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00058.gif)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: Before applying the method chosen for normalization, you must calculate the
    minimum and maximum values of each database column. To do this, we use the `apply`
    function. This function returns a vector or an array or a list of values obtained
    by applying a function to margins of an array or matrix. Let's understand the
    meaning of the arguments used.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first argument of the `apply` function specifies the dataset to apply the
    function, in our case, the dataset named `data`. The second argument must contain
    a vector giving the subscripts which the function will be applied over. In our
    case, one indicates rows and `2` indicates columns. The third argument must contain
    the function to be applied; in our case, the `max` function.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: To normalize the data, we use the `scale` function, which is a generic function
    whose default method centers and/or scales the columns of a numeric matrix.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the first line of the code just suggested, the dataset is split into *70:30*, with
    the intention of using *70* percent of the data at our disposal to train the network
    and the remaining *30* percent to test the network. In the second and third lines,
    the data of the dataframe named `data` is subdivided into two new dataframes,
    called `train_data` and `test_data.`
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Everything so far has only been used to prepare the data. It is now time to
    build the network. To do this, we first recover all the variable names using the
    `names` function. This function will get or set the name of an object.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we build `formula` that we will use to build the network, so we use the
    `neuralnet` function to build and train the network. In this case, we will create
    a network with only one hidden layer with `10` nodes. Finally, we plot the neural
    network,as shown in the following figure:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00059.jpeg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have the network, what do we do? Of course, we use it to make predictions.
    We had set aside *30* percent of the available data to do this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In our case, we applied the function to the `test_data` dataset, using only
    the first `13` columns representing the input variables of the network:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: But how do we figure out whether the forecasts the network is able to perform
    are accurate? We can use the **Mean Squared Error** (**MSE**) as a measure of
    how far away our predictions are from the real data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'In this regard, it is worth remembering that before we built the network we
    had normalized the data. Now, in order to be able to compare, we need to step
    back and return to the starting position. Once the values of the dataset are restored,
    we can calculate the *MSE* through the following equation:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00060.jpeg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'Well, we have calculated *MSE* now with what do we compare it to? To get an
    idea of the accuracy of the network prediction, we can build a linear regression
    model:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We build a linear regression model using the `lm` function. This function is
    used to fit linear models. It can be used to perform regression, single stratum
    analysis of variance, and analysis of covariance. To produce result summaries
    of the results of model fitting obtained, we have used the `summary` function,
    which returns the following results:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Also, for the regression model, we calculate the mean MSE. Finally, in order
    to assess the performance of the network, it is compared with a multiple linear
    regression model calculated with the same database as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The results are:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: From the analysis of the results, it is possible to note that the neural network
    has a lower `MSE` than the linear regression model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning in neural networks
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we present unsupervised learning models in neural network, named
    competitive learning and Kohonen SOM. Kohonen SOM was invented by a professor
    named Teuvo Kohonen and is a way to represent multidimensional data in much lower
    dimensions: *1D* or *2D*. It can classify data without supervision. Unsupervised
    learning aims at finding hidden patterns within the dataset and clustering them
    into different classes of data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: There are many unsupervised learning techniques, namely K-means clustering,
    dimensionality reduction, EM, and so on. The common feature is that there is no
    input-output mapping and we work only on the input values to create a group or
    set of outputs.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: For the case of neural networks, they can be used for unsupervised learning.
    They can group data into different buckets (clustering) or abstract original data
    into a different set of output data points (feature abstraction or dimensionality
    reduction). Unsupervised techniques require less processing power and memory than
    supervised technique.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'In unsupervised neural networks, there is no target variable and we cannot
    do backpropagation. Instead, we keep adjusting the weights without the error measure
    and try to group similar data together. There are two methods we will see for
    unsupervised neural networks:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Competitive learning
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kohonen SOMs
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Competitive learning
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, the neural network nodes compete with each other for the right to respond
    to a subset of the input data. The hidden layer is called the **competitive layer**.
    Every competitive neuron has its own weight and we calculate the similarity measure
    between the individual input vector and the neuron weight. For each input vector,
    the hidden neurons compete with each other to see which one is the *most *similar
    to the particular input vector:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00061.gif)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: The output neurons are said to be in competition for input patterns.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: During training, the output neuron that provides the highest activation to a
    given input pattern is declared the weights of the winner and is moved closer
    to the input pattern, whereas the rest of the neurons are left unchanged
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This strategy is also called **winner-takes-all**, since only the winning neuron
    is updated:'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00062.gif)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: 'Let us see a simple competitive learning algorithm example to find three neurons
    within the given input data:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: We will have three input neurons in the input layer. Each input to the neuron
    is a continuous variable and let the weight at each input neuron be a random number
    between *0.0* and *1.0*. The output of each node is the product of the three weights
    and its input.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each competitive layer neuron receives the sum of the product of weights and
    inputs.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The competitive layer node with the highest output is regarded as the winner.
    The input is then categorized as being within the cluster corresponding to that
    node.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The winner updates each of its weights, moving the weight from the connections
    that gave it weaker signals to the ones that gave it stronger signals.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus, as we receive more data, each node converges on the center of the cluster
    that it has come to represent. It activates more strongly for inputs belonging
    to this cluster and more weakly for inputs that belong to other clusters.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'There are basically two stopping conditions of competitive learning:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '**Predefined number of epochs**: Only *N *epochs are run and this prevents
    the algorithm from running for a relatively long time without convergence'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minimum value of weight update**: The algorithm is run until we have a minimum
    value of weight update'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kohonen SOM
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of competitive learning combined with neighborhood neurons gives
    us Kohonen SOMs. Every neuron in the output layer has two neighbors. The neuron
    that fires the greatest value updates its weights in competitive learning, but
    in SOM, the neighboring neurons also update their weights at a relatively slow
    rate. The number of neighborhood neurons that the network updates the weights
    is based on the dimension of the problem.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'For a *2D* problem, the SOM is represented as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00063.jpeg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: 'Diagrammatically, this is how the SOM maps different colors into different
    clusters:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.jpeg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: 'Let us understand the working of Kohonen SOM step-by-step:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: The number of inputs and the clusters that define the SOM structure and each
    node's weights are initialized.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A vector is chosen at random from the set of training data and is presented
    to the network.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Every node in the network is examined to calculate which one's weights are most
    similar to the input vector. The winning node is commonly known as the **Best
    Matching Unit** (**BMU**).
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The radius of the neighborhood of the BMU is calculated. This value starts large
    and is typically set to be the radius of the network, diminishing each time-step.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any neurons found within the radius of the BMU, calculated in step 4, are adjusted
    to make them more like the input vector. The closer a neuron is to the BMU, the
    more its weights are altered.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat from step 2 for *N* iterations.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The steps are repeated for a set of *N* epochs or until a minimum weight update
    is obtained.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: SOMs are used in the fields of clustering (grouping of data into different buckets),
    data abstraction (deriving output data from inputs), and dimensionality reduction
    (reducing the number of input features). SOMs handle the problem in a way similar
    to **Multi Dimensional Scaling** (**MDS**), but instead of minimizing the distances,
    they try regroup topology, or in other words, they try to keep the same neighbors.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Let us see an example of SOM implementation in R. The `kohonen` package is a
    package to be installed to use the functions offered in R for SOM.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'The following R program explains some functions from the `kohonen` package
    :'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The code uses a wine dataset, which contains a data frame with `177` rows and
    `13` columns; the object `vintages` contains the class labels. This data is obtained
    from the chemical analyses of wines grown in the same region in Italy (Piemonte)
    but derived from three different cultivars, namely, the `Nebbiolo`, `Barberas`,
    and `Grignolino` grapes. The wine from the `Nebbiolo` grape is called **Barolo**.
    The data consists of the amounts of several constituents found in each of the
    three types of wines, as well as some spectroscopic variables.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see the outputs at each section of the code.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The first line of the code is simple, as it loads the library we will use for
    later calculations. Specifically, the `kohonen` library will help us to train
    SOMs. Also, interrogation of the maps and prediction using trained maps are supported.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'These lines load the `wines` dataset, which, as we anticipated, is contained
    in the R distribution, and saves it in a dataframe named `data`. Then, we use
    the `str` function to view a compactly display the structure of the dataset. The
    function `head` is used to return the first or last parts of the dataframe. Finally,
    the `view` function is used to invoke a spreadsheet-style data viewer on the dataframe
    object, as shown in the following figure:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00065.jpeg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: 'We will continue to analyze the code:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After loading the wine data and setting `seed` for reproducibility, we call
    `som` to create a *5x5* matrix, in which the features have to be clustered. The
    function internally does the `kohonen` processing and the result can be seen by
    the clusters formed with the features. There are *25* clusters created, each of
    which has a combined set of features having common pattern, as shown in the following
    image:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.jpeg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: 'The next part of the code plots the mean distance to the closest unit versus
    the number of iterations done by `som`:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the following figure is shown mean distance to closest unit versus the number
    of iterations:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00067.jpeg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: 'Next, we create a `training` dataset with `150` rows and `test` dataset with `27`
    rows. We run the SOM and predict with the test data. The `supersom`function is
    used here. Here, the model is supervised SOM:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we invoke the `table` function that uses the cross-classifying factors
    to build a contingency table of the counts at each combination of factor levels,
    as shown next:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `kohonen` package features standard SOMs and two extensions: for classification
    and regression purposes, and for data mining. Also, it has extensive graphics
    capability for visualization.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists the functions available in the `kohonen` package:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '| **Function name** | **Description** |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| `som` | Standard SOM |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| `xyf`, `bdk` | Supervised SOM; two parallel maps |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| `supersom` | SOM with multiple parallel maps |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| `plot.kohonen` | Generic plotting function |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| `summary.kohonen` | Generic summary function |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| `map.kohonen` | Map data to the most similar neuron |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| `predict.kohonen` | Generic function to predict properties |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: Summary
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the machine learning field and we saw the learning
    process in a neural network. We learned to distinguish between supervised learning,
    unsupervised learning, and reinforcement learning. To understand in detail the
    necessary procedures, we also learned how to train and test the model.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Afterwards, we discovered the meaning of the data cycle and how the data must
    be collected, cleaned, converted, and then fed to the model for learning. So we
    went deeper into the evaluation model to see if the expected value is equal to
    the actual value during the test phase. We analyzed the different metrics available
    to control the model that depends on the status of the target variable.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Then we discovered one of the concepts important for understanding the neural
    networks, the backpropagation algorithm, that is based on computing to update
    weights and bias ions at each level.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we covered two practical programs in R for the learning process, by
    applying the `neuralnet` and the `kohonen` libraries. We can systematically use
    these basics for further building of complex networks.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discover the **Deep Neural Network** (**DNN**).
    We will see some basics of the `H2O` package. Overall, `H2O` is a highly user-friendly
    package that can be used to train feed-forward networks or deep auto-encoders.
    It supports distributed computations and provides a web interface. By including
    the `H2O` package, like any other package in R, we can do all kind of modeling
    and processing of DNN.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
