- en: Learning Process in Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as there are many different types of learning and approaches to human learning,
    so we can say about the machines as well. To ensure that a machine will be able
    to learn from experience, it is important to define the best available methodologies
    depending on the specific job requirements. This often means choosing techniques
    that work for the present case and evaluating them from time to time, to determine
    if we need to try something new.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen the basics of neural networks in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, and also two simple implementations
    using R. In this chapter, we will deal with the learning process, that is how
    to train, test, and deploy a neural network machine learning model. The training
    phase is used for learning, to fit the parameters of the neural networks. The
    testing phase is used to assess the performance of fully-trained neural networks.
    Finally, in the deployment phase, actual data is passed through the model to get
    the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the list of concepts covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training, testing, and deploying a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation metrics-error measurement and fine tuning; measuring accuracy of
    a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning model using neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning model using neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the chapter, we will understand the basic concepts of the learning
    process and how to implement it in the R environment. We will discover different
    types of algorithms to implement a neural network. We will learn how to train,
    test, and deploy a model. We will know how to perform a correct valuation procedure.
  prefs: []
  type: TYPE_NORMAL
- en: What is machine learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What do we mean by the term machine learning? The definition is quite difficult,
    to do so, we are asking large field of scientists to help. We can mention an artificial
    intelligence pioneer''s quote:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Field of study that gives computers the ability to learn without being explicitly
    programmed."*'
  prefs: []
  type: TYPE_NORMAL
- en: – Arthur Samuel
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is about training a model or an algorithm with data and then
    using the model to predict any new data. For example, a toddler is taught how
    to walk from his crawling phase. Initially, the toddler's parents hold the toddler's
    hand to help him up, and he is taught through the data that is given. On the basis
    of these procedures, if an obstacle presents itself in the toddler's path or if
    there is a turn somewhere, the toddler is able to navigate on his own after the
    training. The data used for training is the training data and the recipient continues
    to learn even after the formal training.
  prefs: []
  type: TYPE_NORMAL
- en: Machines too can be taught like toddlers to do a task based on training. First,
    we feed enough data to tell the machine what needs to be done on what circumstances.
    After the training, the machine can perform automatically and can also learn to
    fine-tune itself. This type of training the machine is called **machine learning**.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between machine learning and programming is that there is
    no coding/programming involved in machine learning, while programming is about
    giving the machine a set of instructions to perform. In machine learning, the
    data is the only input provided and the model is based on the algorithm we have
    decided to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm to be used is based on various factors of the data: the features
    (or independent variables), the type of dependent variable(s), the accuracy of
    the model, and the speed of training and prediction of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the independent variable(s) of the machine learning data, there are
    three different ways to train a model:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows the different algorithms to train a machine learning
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00035.gif)'
  prefs: []
  type: TYPE_IMG
- en: In the following sections, we will go through them on by one.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Supervised learning** is a learning method where there is a part of the training
    data which acts as a teacher to the algorithm to determine the model. The machine
    is taught what to learn from the target data. The target data, or dependent or
    response variables, are the outcome of the collective action of the independent
    variables. The network training is done with the target data and its behavior
    with patterns of input data. The target labels are known in advance and the data
    is fed to the algorithm to derive the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Most of neural network usage is done using supervised learning. The weights
    and biases are adjusted based on the output values. The output can be categorical
    (like true/false or 0/1/2) or continuous (like 1,2,3, and so on). The model is
    dependent on the type of output variables, and in the case of neural networks,
    the output layer is built on the type of target variable.
  prefs: []
  type: TYPE_NORMAL
- en: For neural networks, all the independent and dependent variables need to be
    numeric, as a neural network is based on mathematical models. It is up to the
    data scientist to convert the data to numbers to be fed into the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised learning is depicted by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00036.gif)'
  prefs: []
  type: TYPE_IMG
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In unsupervised learning (or self organization), the output layer is trained
    to organize the input data into another set of data without the need of a target
    variable. The input data is analyzed and patterns are found in it to derive the
    output, as shown in the following figure. Since there is no teacher (or target
    variable), this type of learning is called **unsupervised learning**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00037.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The different techniques available for unsupervised learning are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering (K-means, hierarchical)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self Organizing Map **(**SOM**)/ Kohonen networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To summarize, the two main types of machine learning are depicted in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For neural networks, we have both the types available, using different ways
    available in R.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is a type of machine learning where there is constant
    feedback given to the model to adapt to the environment. There is a performance
    evaluation at each step to improve the model. For neural networks, there is a
    special type called **Q-learning**, combined with neuron to implement reinforcement
    learning in the backpropagation feedback mechanism. The details are out of scope
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the three types of learnings we have covered so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00039.gif)'
  prefs: []
  type: TYPE_IMG
- en: Training and testing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training and testing the model forms the basis for further usage of the model
    for prediction in predictive analytics. Given a dataset of *100* rows of data,
    which includes the predictor and response variables, we split the dataset into
    a convenient ratio (say *70:30*) and allocate *70* rows for training and *30*
    rows for testing. The rows are selected in random to reduce bias.
  prefs: []
  type: TYPE_NORMAL
- en: Once the training data is available, the data is fed to the neural network to
    get the massive universal function in place. The training data determines the
    weights, biases, and activation functions to be used to get to output from input.
    Until recently, we could not say that a weight has a positive or a negative influence
    on the target variable. But now we've been able to shed some light inside the
    black box. For example, by plotting a trained neural network, we can discover
    trained synaptic weights and basic information about the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Once the sufficient convergence is achieved, the model is stored in memory and
    the next step is testing the model. We pass the *30* rows of data to check if
    the actual output matches with the predicted output from the model. The evaluation
    is used to get various metrics which can validate the model. If the accuracy is
    too wary, the model has to be re-built with change in the training data and other
    parameters passed to the neural net function. We will cover more about the evaluation
    metrics later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: After training and testing, the model is said to be deployed, where actual data
    is passed through the model to get the prediction. For example, the use case may
    be determining a fraud transaction or a home loan eligibility check based on various
    input parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training, testing, and deployment is represented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00040.gif)'
  prefs: []
  type: TYPE_IMG
- en: So far, we have focused on the various algorithms available; it is now time
    to dedicate ourselves to the data that represents the essential element of each
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The data cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data forms a key component for model building and the learning process.
    The data needs to be collected, cleaned, converted, and then fed to the model
    for learning. The overall data life cycle is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00041.gif)'
  prefs: []
  type: TYPE_IMG
- en: One of the critical requirements for modeling is having good and balanced data.
    This helps in higher accuracy models and better usage of the available algorithms.
    A data scientist's time is mostly spent on cleansing the data before building
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen the training and testing before deployment of the model. For testing,
    the results are captured as evaluation metrics, which helps us decide if we should
    use a particular model or change it instead.
  prefs: []
  type: TYPE_NORMAL
- en: We will see the evaluation metrics next.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating a model involves checking if the predicted value is equal to the
    actual value during the testing phase. There are various metrics available to
    check the model, and they depend on the state of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a binary classification problem, the predicted target variable and the
    actual target variable can be in any of the following four states:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Predicted** | **Actual** |'
  prefs: []
  type: TYPE_TB
- en: '| *Predicted = TRUE* | *Actual = TRUE* |'
  prefs: []
  type: TYPE_TB
- en: '| *Predicted = TRUE* | *Actual = FALSE* |'
  prefs: []
  type: TYPE_TB
- en: '| *Predicted = FALSE* | *Actual = TRUE* |'
  prefs: []
  type: TYPE_TB
- en: '| *Predicted = FALSE* | *Actual = FALSE* |'
  prefs: []
  type: TYPE_TB
- en: When we have the predicted and actual values as same values, we are said to
    be accurate. If all predicted and actual values are same (either all *TRUE* or
    all *FALSE*), the model is *100* percent accurate. But, this is never the case.
  prefs: []
  type: TYPE_NORMAL
- en: Since neural networks are approximation models, there is always a bit of error
    possible. All the four states mentioned in the previous table are possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the following terminology and metrics for a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positives** (**TP**):All cases where the predicted and actual are both
    *TRUE* (good accuracy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative** (**TN**): All cases when predicted is *FALSE* and the actual
    is also *FALSE* (good accuracy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive **(**FP**):This is a case when we predict something as positive
    (*TRUE*), but it is actually negative. It is like a false alarm or an FP error.
    An example is when a male is predicted to be pregnant by a pregnancy test kit.
    All cases when predicted is *TRUE*, while the actual is *FALSE*. This is also
    called **type 1 error**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative** (**FN**):When we predict something as *FALSE*, but in actuality
    it is *TRUE*, then the case is called FN. For example, when a pregnant female
    is predicted as not being pregnant by a pregnancy test kit, it is an FN case.
    All cases when predicted is *FALSE* and actual *TRUE*. This is also called **type
    2 error**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When the values of the classification are plotted in a *nxn* matrix (*2x2*
    in case of binary classification), the matrix is called the **confusion matrix**.
    All the evaluation metrics can be derived from the confusion matrix itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predicted value** | **Predicted value** |'
  prefs: []
  type: TYPE_TB
- en: '| *Actual values* | *TRUE* | *FALSE* |'
  prefs: []
  type: TYPE_TB
- en: '| *TRUE* | *TP* | *FN* |'
  prefs: []
  type: TYPE_TB
- en: '| *FALSE* | *FP* | *TN* |'
  prefs: []
  type: TYPE_TB
- en: Now, let's look at some evaluation metrics in detail.
  prefs: []
  type: TYPE_NORMAL
- en: True Positive Rate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**True Positive Rate** (**TPR**) or sensitivity or recall or hit rate is a
    measure of how many true positives were identified out of all the positives identified:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Ideally, the model is better if we have this closer to one.
  prefs: []
  type: TYPE_NORMAL
- en: True Negative Rate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**True Negative Rate** (**TNR**) or specificity is the ratio of true negatives
    and total number of negatives we have predicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If this ratio is closer to zero, the model is more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accuracy is the measure of how good our model is. It is expected to be closer
    to 1, if our model is performing well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy is the ratio of correct predictions and all the total predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Precision and recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Precision and recall are again ratios between the *TP* with (*TP+FP*) and *TP*
    with (*TP+FN*) respectively. These ratios determine how relevant our predictions
    are compared to the actual.
  prefs: []
  type: TYPE_NORMAL
- en: Precision is defined as how many selected items are relevant. That is, how many
    of the predicted ones are actually correctly predicted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00045.gif)'
  prefs: []
  type: TYPE_IMG
- en: If precision is closer to one, we are more accurate in our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall, on the other hand, tells how many relevant items we selected. Mathematically,
    it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00046.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram depicts clearly the discussion we have done so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: F-score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'F-score, or F1-score, is another measure of accuracy. Technically, it is the
    harmonic mean of precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Receiver Operating Characteristic curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **Receiver Operating Characteristic **(**ROC**) curve is a graphical visual
    that illustrates the predictive ability of a binary classifier system. The ROC
    curve is created by plotting a graph of the TPR against the **False Positive Rate**
    (**FPR**) at various threshold settings. This gives us **Sensitivity** versus
    (**1 - Specificity**). A ROC curve typically looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00049.gif)'
  prefs: []
  type: TYPE_IMG
- en: After acquiring the necessary skills, we are ready to analyze in detail the
    algorithms used for building the neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Learning in neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, neural networks is a machine
    learning algorithm that has the ability to learn from data and give us predictions
    using the model built. It is a universal function approximation, that is, any
    input, output data can be approximated to a mathematical function.
  prefs: []
  type: TYPE_NORMAL
- en: The forward propagation gives us an initial mathematical function to arrive
    at output(s) based on inputs by choosing random weights. The difference between
    the actual and predicted is called the error term. The learning process in a feed-forward
    neural network actually happens during the backpropagation stage. The model is
    fine tuned with the weights by reducing the error term in each iteration. Gradient
    descent is used in the backpropagation process.
  prefs: []
  type: TYPE_NORMAL
- en: Let us cover the backpropagation in detail in this chapter, as it is an important
    machine learning aspect for neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Back to backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered the forward propagation in detail in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, and a little about backpropagation
    using gradient descent. Backpropagation is one of the important concepts for understanding
    neural networks and it relies on calculus to update the weights and biases in
    each layer. Backpropagation of errors is similar to *learning from mistakes*.
    We correct ourselves in our mistakes (errors) in every iteration, until we reach
    a point called **convergence***. *The goal of backpropagation is to correct the
    weights in each layer and minimize the overall error at the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural network learning heavily relies on backpropagation in feed-forward networks.
    The usual steps of forward propagation and error correction are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Start the neural network forward propagation by assigning random weights and
    biases to each of the neurons in the hidden layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the sum of *sum(weight*input) + bias* at each neuron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the activation function (*sigmoid*) at each neuron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take this output and pass it onto the next layer neuron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the layer is the output layer, apply the weights and get the sum of *sum(weight*input)
    + bias* at each output layer neuron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, apply the activation function at the output layer neuron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This forms the output of the neural network at the output layer for one forward
    pass.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, with the training data, we can identify the error term at each output neuron,
    by subtracting the actual output and the activation function output value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The total of the errors is arrived at by using the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A factor of *1/2* is used to cancel the exponent when the error function *E*
    is subsequently differentiated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient descent technique requires calculation of the partial derivative
    of the error term (*E*) with respect to the weights of the network. Calculating
    the partial derivative of the full error with respect to the weight *w[ij] *is
    done using the **chain rule**of differentiation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The derivative is defined as the rate of change of a value, the gradient descent
    uses the derivative (or slope) to minimize the error term and arrive at a correct
    set of weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first factor is partial derivative of the error term with respect to the
    output at that particular neuron *j* and *o[j]* is equal to *y*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The second factor in the chain rule is the partial derivative of the output
    of neuron *o[j ]* with respect to its input, and is the partial derivative of
    the activation function (the *sigmoid* function):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here *net[j ]*is the input to the neuron.
  prefs: []
  type: TYPE_NORMAL
- en: The third term in the chain rule is simply *o[i]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Combining steps 11, 12, and 13, we get:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The weight *w[ij]* at each neuron (any layer) is updated with this partial derivative,
    combined with the learning rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps are repeated until we have convergence of very low error term or
    a specified number of times.
  prefs: []
  type: TYPE_NORMAL
- en: All the steps are taken care of internally in the R packages available. We can
    supply the learning rate along with various other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The backpropagation is illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As with all things in life, even an algorithm has further improvement margins.
    In the next section, we'll see how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network learning algorithm optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The procedure used to carry out the learning process in a neural network is
    called the training algorithm. The learning algorithm is what the machine learning
    algorithm chooses as model with the best optimization. The aim is to minimize
    the loss function and provide more accuracy. Here we illustrate some of the optimization
    techniques, other than gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: The **Particle Swarm Optimization** (**PSO**) method is inspired by observations
    of social and collective behavior on the movements of bird flocks in search of
    food or survival. It is similar to a fish school trying to move together. We know
    the position and velocity of the particles, and PSO aims at searching a solution
    set in a large space controlled by mathematical equations on position and velocity. It
    is bio-inspired from biological organism behavior for collective intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: '**Simulated annealing** is a method that works on a probabilistic approach
    to approximate the global optimum for the cost function. The method searches for
    a solution in large space with simulation.'
  prefs: []
  type: TYPE_NORMAL
- en: Evolutionary methods are derived from the evolutionary process in biology, and
  prefs: []
  type: TYPE_NORMAL
- en: evolution can be in terms of reproduction, mutation, selection, and recombination.
  prefs: []
  type: TYPE_NORMAL
- en: A fitness function is used to determine the performance of a model, and based
    on this
  prefs: []
  type: TYPE_NORMAL
- en: function, we select our final model.
  prefs: []
  type: TYPE_NORMAL
- en: The **Expectation Maximization **(**EM**) methodis a statistical learning method
    that uses an iterative method to find maximum likelihood or maximum posterior
    estimate, thus minimizing the error.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning in neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As previously mentioned, supervised learning is a learning method where there
    is a part of training data which acts as a teacher to the algorithm to determine
    the model. In the following section, an example of a regression predictive modeling
    problem is proposed to understand how to solve it with neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Boston dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset describes 13 numerical properties of houses in Boston suburbs,
    and is concerned with modeling the price of houses in those suburbs in thousands
    of dollars. As such, this is a regression predictive modeling problem. Input attributes
    include things like crime rate, proportion of non-retail business acres, chemical
    concentrations, and more. In the following list are shown all the variables followed
    by a brief description:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of instances: *506*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of attributes: *13* continuous attributes (including `class`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: attribute `MEDV`), and one binary-valued attribute
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Each of the attributes is detailed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`crim` per capita crime rate by town.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`zn` proportion of residential land zoned for lots over *25,000* square feet.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`indus` proportion of non-retail business acres per town.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`chas` Charles River dummy variable (*= 1* if tract bounds river; *0* otherwise).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`nox` nitric oxides concentration (parts per *10* million).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`rm` average number of rooms per dwelling.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`age` proportion of owner-occupied units built prior to *1940*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dis` weighted distances to five Boston employment centres'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`rad` index of accessibility to radial highways.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`tax` full-value property-tax rate per *$10,000*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ptratio` pupil-teacher ratio by town.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`black` *1000(Bk - 0.63)^2* where *Bk* is the proportion of blacks by town.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`lstat` percent lower status of the population.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`medv` median value of owner-occupied homes in *$1000''s*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of these, `medv` is the response variable, while the other thirteen variables
    are possible predictors. The goal of this analysis is to fit a regression model
    that best explains the variation in `medv`.
  prefs: []
  type: TYPE_NORMAL
- en: There is a relation between the first thirteen columns and the `medv` response
    variable. We can predict the `medv` value based on the input thirteen columns.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is already provided with R libraries (`MASS`), as we will see later,
    so we do not have to worry about retrieving the data.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network regression with the Boston dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will run a regression neural network for the `Boston` dataset.
    The `medv` value is predicted for the test data. The train to test split is *70:30*.
    The `neuralnet`function is used to model the data with a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Don't worry, now we will explain in detail the whole code, line by line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first two lines of the code are simple, as they load the libraries we will
    use for later calculations. Specifically, the `neuralnet` library will help us
    to build and train the network, while the `MASS` library will serve us to load
    the `Boston` dataset that we have previously introduced in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, for example, to install the `neuralnet` package, we should write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it should be emphasized that this function should be used only once
    and not every time you run the code. Instead, load the library through the following
    command and must be repeated every time you run the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The function `set.seed` sets the seed of R''s random number generator, which
    is useful for creating simulations or random objects that can be reproduced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You have to use this function every time you want to get a reproducible random
    result. In this case, the random numbers are the same, and they would continue
    to be the same no matter how far out in the sequence we go.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command loads the `Boston` dataset, which, as we anticipated,
    is contained in the `MASS` library and saves it in a given frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `str` function to view a compactly display the structure of an arbitrary
    R object. In our case, using `str(data)`, we will obtain the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result obtained for the given object is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s go back to parse the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We need this snippet of code to normalize the data.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, it is good practice to normalize the data before training a neural
    network. With normalization, data units are eliminated, allowing you to easily
    compare data from different locations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an extremely important procedure in building a neural network, as it
    avoids unnecessary results or very difficult training processes resulting in algorithm
    convergence problems. You can choose different methods for scaling the data (**z-normalization**,
    **min-max scale**, and so on). For this example, we will use the min-max method
    (usually called feature scaling) to get all the scaled data in the range *[0,1]*.
    The formula to achieve this is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00058.gif)'
  prefs: []
  type: TYPE_IMG
- en: Before applying the method chosen for normalization, you must calculate the
    minimum and maximum values of each database column. To do this, we use the `apply`
    function. This function returns a vector or an array or a list of values obtained
    by applying a function to margins of an array or matrix. Let's understand the
    meaning of the arguments used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The first argument of the `apply` function specifies the dataset to apply the
    function, in our case, the dataset named `data`. The second argument must contain
    a vector giving the subscripts which the function will be applied over. In our
    case, one indicates rows and `2` indicates columns. The third argument must contain
    the function to be applied; in our case, the `max` function.
  prefs: []
  type: TYPE_NORMAL
- en: To normalize the data, we use the `scale` function, which is a generic function
    whose default method centers and/or scales the columns of a numeric matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the first line of the code just suggested, the dataset is split into *70:30*, with
    the intention of using *70* percent of the data at our disposal to train the network
    and the remaining *30* percent to test the network. In the second and third lines,
    the data of the dataframe named `data` is subdivided into two new dataframes,
    called `train_data` and `test_data.`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Everything so far has only been used to prepare the data. It is now time to
    build the network. To do this, we first recover all the variable names using the
    `names` function. This function will get or set the name of an object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we build `formula` that we will use to build the network, so we use the
    `neuralnet` function to build and train the network. In this case, we will create
    a network with only one hidden layer with `10` nodes. Finally, we plot the neural
    network,as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have the network, what do we do? Of course, we use it to make predictions.
    We had set aside *30* percent of the available data to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, we applied the function to the `test_data` dataset, using only
    the first `13` columns representing the input variables of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: But how do we figure out whether the forecasts the network is able to perform
    are accurate? We can use the **Mean Squared Error** (**MSE**) as a measure of
    how far away our predictions are from the real data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this regard, it is worth remembering that before we built the network we
    had normalized the data. Now, in order to be able to compare, we need to step
    back and return to the starting position. Once the values of the dataset are restored,
    we can calculate the *MSE* through the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Well, we have calculated *MSE* now with what do we compare it to? To get an
    idea of the accuracy of the network prediction, we can build a linear regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We build a linear regression model using the `lm` function. This function is
    used to fit linear models. It can be used to perform regression, single stratum
    analysis of variance, and analysis of covariance. To produce result summaries
    of the results of model fitting obtained, we have used the `summary` function,
    which returns the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, for the regression model, we calculate the mean MSE. Finally, in order
    to assess the performance of the network, it is compared with a multiple linear
    regression model calculated with the same database as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: From the analysis of the results, it is possible to note that the neural network
    has a lower `MSE` than the linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning in neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we present unsupervised learning models in neural network, named
    competitive learning and Kohonen SOM. Kohonen SOM was invented by a professor
    named Teuvo Kohonen and is a way to represent multidimensional data in much lower
    dimensions: *1D* or *2D*. It can classify data without supervision. Unsupervised
    learning aims at finding hidden patterns within the dataset and clustering them
    into different classes of data.
  prefs: []
  type: TYPE_NORMAL
- en: There are many unsupervised learning techniques, namely K-means clustering,
    dimensionality reduction, EM, and so on. The common feature is that there is no
    input-output mapping and we work only on the input values to create a group or
    set of outputs.
  prefs: []
  type: TYPE_NORMAL
- en: For the case of neural networks, they can be used for unsupervised learning.
    They can group data into different buckets (clustering) or abstract original data
    into a different set of output data points (feature abstraction or dimensionality
    reduction). Unsupervised techniques require less processing power and memory than
    supervised technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'In unsupervised neural networks, there is no target variable and we cannot
    do backpropagation. Instead, we keep adjusting the weights without the error measure
    and try to group similar data together. There are two methods we will see for
    unsupervised neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: Competitive learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kohonen SOMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Competitive learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, the neural network nodes compete with each other for the right to respond
    to a subset of the input data. The hidden layer is called the **competitive layer**.
    Every competitive neuron has its own weight and we calculate the similarity measure
    between the individual input vector and the neuron weight. For each input vector,
    the hidden neurons compete with each other to see which one is the *most *similar
    to the particular input vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00061.gif)'
  prefs: []
  type: TYPE_IMG
- en: The output neurons are said to be in competition for input patterns.
  prefs: []
  type: TYPE_NORMAL
- en: During training, the output neuron that provides the highest activation to a
    given input pattern is declared the weights of the winner and is moved closer
    to the input pattern, whereas the rest of the neurons are left unchanged
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This strategy is also called **winner-takes-all**, since only the winning neuron
    is updated:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00062.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us see a simple competitive learning algorithm example to find three neurons
    within the given input data:'
  prefs: []
  type: TYPE_NORMAL
- en: We will have three input neurons in the input layer. Each input to the neuron
    is a continuous variable and let the weight at each input neuron be a random number
    between *0.0* and *1.0*. The output of each node is the product of the three weights
    and its input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each competitive layer neuron receives the sum of the product of weights and
    inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The competitive layer node with the highest output is regarded as the winner.
    The input is then categorized as being within the cluster corresponding to that
    node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The winner updates each of its weights, moving the weight from the connections
    that gave it weaker signals to the ones that gave it stronger signals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus, as we receive more data, each node converges on the center of the cluster
    that it has come to represent. It activates more strongly for inputs belonging
    to this cluster and more weakly for inputs that belong to other clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are basically two stopping conditions of competitive learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predefined number of epochs**: Only *N *epochs are run and this prevents
    the algorithm from running for a relatively long time without convergence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minimum value of weight update**: The algorithm is run until we have a minimum
    value of weight update'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kohonen SOM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of competitive learning combined with neighborhood neurons gives
    us Kohonen SOMs. Every neuron in the output layer has two neighbors. The neuron
    that fires the greatest value updates its weights in competitive learning, but
    in SOM, the neighboring neurons also update their weights at a relatively slow
    rate. The number of neighborhood neurons that the network updates the weights
    is based on the dimension of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a *2D* problem, the SOM is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Diagrammatically, this is how the SOM maps different colors into different
    clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us understand the working of Kohonen SOM step-by-step:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of inputs and the clusters that define the SOM structure and each
    node's weights are initialized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A vector is chosen at random from the set of training data and is presented
    to the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Every node in the network is examined to calculate which one's weights are most
    similar to the input vector. The winning node is commonly known as the **Best
    Matching Unit** (**BMU**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The radius of the neighborhood of the BMU is calculated. This value starts large
    and is typically set to be the radius of the network, diminishing each time-step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any neurons found within the radius of the BMU, calculated in step 4, are adjusted
    to make them more like the input vector. The closer a neuron is to the BMU, the
    more its weights are altered.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat from step 2 for *N* iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The steps are repeated for a set of *N* epochs or until a minimum weight update
    is obtained.
  prefs: []
  type: TYPE_NORMAL
- en: SOMs are used in the fields of clustering (grouping of data into different buckets),
    data abstraction (deriving output data from inputs), and dimensionality reduction
    (reducing the number of input features). SOMs handle the problem in a way similar
    to **Multi Dimensional Scaling** (**MDS**), but instead of minimizing the distances,
    they try regroup topology, or in other words, they try to keep the same neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Let us see an example of SOM implementation in R. The `kohonen` package is a
    package to be installed to use the functions offered in R for SOM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following R program explains some functions from the `kohonen` package
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The code uses a wine dataset, which contains a data frame with `177` rows and
    `13` columns; the object `vintages` contains the class labels. This data is obtained
    from the chemical analyses of wines grown in the same region in Italy (Piemonte)
    but derived from three different cultivars, namely, the `Nebbiolo`, `Barberas`,
    and `Grignolino` grapes. The wine from the `Nebbiolo` grape is called **Barolo**.
    The data consists of the amounts of several constituents found in each of the
    three types of wines, as well as some spectroscopic variables.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see the outputs at each section of the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The first line of the code is simple, as it loads the library we will use for
    later calculations. Specifically, the `kohonen` library will help us to train
    SOMs. Also, interrogation of the maps and prediction using trained maps are supported.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'These lines load the `wines` dataset, which, as we anticipated, is contained
    in the R distribution, and saves it in a dataframe named `data`. Then, we use
    the `str` function to view a compactly display the structure of the dataset. The
    function `head` is used to return the first or last parts of the dataframe. Finally,
    the `view` function is used to invoke a spreadsheet-style data viewer on the dataframe
    object, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will continue to analyze the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the wine data and setting `seed` for reproducibility, we call
    `som` to create a *5x5* matrix, in which the features have to be clustered. The
    function internally does the `kohonen` processing and the result can be seen by
    the clusters formed with the features. There are *25* clusters created, each of
    which has a combined set of features having common pattern, as shown in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next part of the code plots the mean distance to the closest unit versus
    the number of iterations done by `som`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following figure is shown mean distance to closest unit versus the number
    of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we create a `training` dataset with `150` rows and `test` dataset with `27`
    rows. We run the SOM and predict with the test data. The `supersom`function is
    used here. Here, the model is supervised SOM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we invoke the `table` function that uses the cross-classifying factors
    to build a contingency table of the counts at each combination of factor levels,
    as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `kohonen` package features standard SOMs and two extensions: for classification
    and regression purposes, and for data mining. Also, it has extensive graphics
    capability for visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists the functions available in the `kohonen` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Function name** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `som` | Standard SOM |'
  prefs: []
  type: TYPE_TB
- en: '| `xyf`, `bdk` | Supervised SOM; two parallel maps |'
  prefs: []
  type: TYPE_TB
- en: '| `supersom` | SOM with multiple parallel maps |'
  prefs: []
  type: TYPE_TB
- en: '| `plot.kohonen` | Generic plotting function |'
  prefs: []
  type: TYPE_TB
- en: '| `summary.kohonen` | Generic summary function |'
  prefs: []
  type: TYPE_TB
- en: '| `map.kohonen` | Map data to the most similar neuron |'
  prefs: []
  type: TYPE_TB
- en: '| `predict.kohonen` | Generic function to predict properties |'
  prefs: []
  type: TYPE_TB
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the machine learning field and we saw the learning
    process in a neural network. We learned to distinguish between supervised learning,
    unsupervised learning, and reinforcement learning. To understand in detail the
    necessary procedures, we also learned how to train and test the model.
  prefs: []
  type: TYPE_NORMAL
- en: Afterwards, we discovered the meaning of the data cycle and how the data must
    be collected, cleaned, converted, and then fed to the model for learning. So we
    went deeper into the evaluation model to see if the expected value is equal to
    the actual value during the test phase. We analyzed the different metrics available
    to control the model that depends on the status of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Then we discovered one of the concepts important for understanding the neural
    networks, the backpropagation algorithm, that is based on computing to update
    weights and bias ions at each level.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we covered two practical programs in R for the learning process, by
    applying the `neuralnet` and the `kohonen` libraries. We can systematically use
    these basics for further building of complex networks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discover the **Deep Neural Network** (**DNN**).
    We will see some basics of the `H2O` package. Overall, `H2O` is a highly user-friendly
    package that can be used to train feed-forward networks or deep auto-encoders.
    It supports distributed computations and provides a web interface. By including
    the `H2O` package, like any other package in R, we can do all kind of modeling
    and processing of DNN.
  prefs: []
  type: TYPE_NORMAL
