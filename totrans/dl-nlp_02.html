<html><head></head><body>
		<div class="Content" id="_idContainer041">
			<h1 id="_idParaDest-49"><em class="italics"><a id="_idTextAnchor051"/>Chapter 2</em></h1>
		</div>
		<div class="Content" id="_idContainer042">
			<h1 id="_idParaDest-50"><a id="_idTextAnchor052"/>Applications of Natural Language Processing</h1>
		</div>
		<div class="Content" id="_idContainer043">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Describe POS tagging and its applications</li>
				<li class="bullets">Differentiate between rule-based and stochastic POS taggers</li>
				<li class="bullets">Perform POS tagging, chunking, and chinking on text data</li>
				<li class="bullets">Perform named entity recognition for information extraction</li>
				<li class="bullets">Develop and train your own POS tagger and named entity recognizer</li>
				<li class="bullets">Use NLTK and spaCy to perform POS tagging, chunking, chinking, and named entity recognition</li>
			</ul>
			<p>This chapter aims to introduce you to the plethora of applications of NLP and the various techniques involved within.</p>
		</div>
		<div class="Content" id="_idContainer062">
			<h2 id="_idParaDest-51"><a id="_idTextAnchor053"/>Introduction</h2>
			<p>This chapter begins with a quick recap of what natural language processing is and what services it can help provide. Then, it discusses two applications of natural language processing: <strong class="keyword">Parts of Speech Tagging (POS tagging)</strong> and <strong class="keyword">Named Entity Recognition</strong>. The functioning, necessity, and purposes of both of these algorithms are explained. Additionally, there are exercises and activities that perform POS tagging and named entity recognition and build and develop these algorithms.</p>
			<p>Natural language processing consists of aiding machines to understand the natural language of humans in order to communicate with them effectively and automate a large number of tasks. The previous chapter discussed the applications of natural language processing along with examples of real-life use cases where these techniques could simplify the lives of humans. This chapter will specifically look into two of these algorithms and their real-life applications.</p>
			<p>Every aspect of natural language processing can be seen to follow the same analogy of teaching a language. In the last chapter, we saw how machines need to be told what parts of a corpus to pay attention to and what parts are irrelevant and unimportant. They need to be trained to remove stop words and noisy elements and focus on key words to reduce various forms of the same word to the word's root form so that it's easier to search for and interpret. In a similar fashion, the two algorithms discussed in this chapter also teach machines particular things about languages in the way we humans have been taught.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor054"/>POS Tagging</h2>
			<p>Before we dive straight into the algorithm, let's understand what parts of speech are. Parts of speech are something most of us are taught in our early years of learning the English language. They are categories assigned to words based on their syntactic or grammatical functions. These functions are the functional relationships that exist between different words.</p>
			<h3 id="_idParaDest-53"><a id="_idTextAnchor055"/>Parts of Speech</h3>
			<p>The English language has nine main parts of speech:</p>
			<ul>
				<li><em class="italics">Nouns</em>: Things or people</li>
				<li>Examples: table, dog, piano, London, towel</li>
				<li><em class="italics">Pronouns</em>: Words that replace nouns</li>
				<li>Examples: I, you, he, she, it</li>
				<li><em class="italics">Verbs</em>: Action words</li>
				<li>Examples: to be, to have, to study, to learn, to play</li>
				<li><em class="italics">Adjectives</em>: Words that describe nouns</li>
				<li>Examples: intelligent, small, silly, intriguing, blue</li>
				<li><em class="italics">Determiners</em>: Words that limit nouns</li>
				<li>Examples: a few, many, some, three<h4>Note</h4><p class="callout">For more examples of determiners, visit <a href="">https://www.ef.com/in/english-resources/english-grammar/determiners/</a>.</p></li>
				<li><em class="italics">Adverbs</em>: Words that describe verbs, adjectives, or adverbs themselves</li>
				<li>Examples: quickly, shortly, very, really, drastically</li>
				<li><em class="italics">Prepositions</em>: Words that link nouns to other words</li>
				<li>Examples: to, on, in, under, beside</li>
				<li><em class="italics">Conjunctions</em>: Words that join two sentences or words</li>
				<li>Examples: and, but, yet</li>
				<li><em class="italics">Interjections</em>: Words that are exclamations</li>
				<li>Examples: ouch! Ow! Wow!</li>
			</ul>
			<p>As you can see, each word falls under a specific Parts of speech tag assigned to it that helps us understand the meaning and purpose of the word, enabling us to better understand the context in which it is being used.</p>
			<h3 id="_idParaDest-54"><a id="_idTextAnchor056"/>POS Tagger</h3>
			<p>POS tagging is the process of assigning a tag to a word. This is done by an algorithm known as a POS tagger. The aim of the algorithm is really just as simple as this.</p>
			<p>Most POS taggers are supervised learning algorithms. If you don't remember what supervised learning algorithms are, they are machine learning algorithms that learn to perform a task based on previously labeled data. The algorithms take rows of data as input. This data contains feature columns—data used to predict something—and usually one label column—the something that needs to be predicted. The models are trained on this input to learn and understand what features correspond to which label, thus learning how to perform the task of predicting the labels. Ultimately, they are given unlabeled data (data that just consists of feature columns), for which they must predict labels.</p>
			<p>The following diagram is a general illustration of a supervised learning model:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer044">
					<img alt="Fig 2.1: Supervised learning&#13;&#10;" src="image/C13783_02_01.jpg"/>
				</div>
			</div>
			<h6>Fig 2.1: Supervised learning</h6>
			<h4>Note</h4>
			<p class="callout">For more information on supervised learning, go to <a href="">https://www.packtpub.com/big-data-and-business-intelligence/applied-supervised-learning-python</a>.</p>
			<p>Thus, POS taggers hone their predictive abilities by learning from previously labeled datasets. In this case, the datasets can consist of a variety of features, such as the word itself (obviously), the definition of the word, the relationships of the word with its preceding, proceeding, and other related word(s) that are present within the same sentence, phrase, or paragraph. These features together help the tagger predict what POS tag should be assigned to a word. The corpus used to train a supervised POS tagger is known as a pre-tagged corpus. Such corpora serve as the basis for the creation of a system for the POS tagger to tag untagged words. These systems/types of POS taggers will be discussed in the next section.</p>
			<p>Pre-tagged corpora, however, are not always readily available, and to accurately train a tagger, the corpus must be large. Thus, recently there have been iterations of the POS tagger that can be considered as unsupervised learning algorithms. These are algorithms that take data consisting solely of features as input. These features aren't associated with labels and thus the algorithm, instead of predicting labels, forms groups or clusters of the input data. </p>
			<p>In the case of POS tagging, the models use computational methods to automatically generate sets of POS tags. While, pre-tagged corpora are responsible for aiding the process of creating a system for the tagger in the case of supervised POS taggers, with unsupervised POS taggers, these computational methods serve as the basis for the creation of such systems. The drawback of unsupervised learning methods is that the cluster of POS tags generated automatically may not always be as accurate as those found in the pre-tagged corpora used to train supervised methods.</p>
			<p>To summarize, the key differences between supervised and unsupervised learning methods are as follows:</p>
			<ul>
				<li>Supervised POS taggers take pre-tagged corpora as input to be trained, while unsupervised POS taggers take untagged corpora as input to create a set of POS tags.</li>
				<li>Supervised POS taggers create dictionaries of words with their respective POS tags based on the tagged corpora, while unsupervised POS taggers generate these dictionaries using the self-created POS tag set.</li>
			</ul>
			<p>Several Python libraries (such as NLTK and spaCy) have trained POS taggers of their own. You will learn how to use one in the following sections, but let's understand the input and output of a POS tagger with an example for now. An important thing to remember is that since a POS tagger assigns a POS tag to each word in the given corpus, the input needs to be in the form of word tokens. Therefore, before performing POS tagging, tokenization needs to be carried out on the corpus. Let's say we give the trained POS tagger the following tokens as an input:</p>
			<p class="snippet">['I', 'enjoy', 'playing', 'the', 'piano']</p>
			<p>After POS tagging, the output would look something like this:</p>
			<p class="snippet">['I_PRO', 'enjoy_V', 'playing_V', 'the_DT', piano_N']</p>
			<p>Here, <strong class="bold">PRO</strong> = pronoun, <strong class="bold">V</strong> = verb, <strong class="bold">DT</strong> = determiner, and <strong class="bold">N</strong> = noun.</p>
			<p>The input and output for both a trained supervised and unsupervised POS tagger are the same: tokens, and tokens with POS tags, respectively.</p>
			<h4>Note</h4>
			<p class="callout">This is not the exact syntax of the output; you'll see the proper output later when you perform the exercise. This is just to give you an idea of what POS taggers do.</p>
			<p>The aforementioned parts of speech are very basic tags, and to ease the process of understanding natural language, POS algorithms create much more complicated tags that are variations of these basic ones. Here's a full list of the POS tags with their descriptions:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer045">
					<img alt="Figure 2.2: POS tags with descriptions&#13;&#10;" src="image/C13783_02_02.jpg"/>
				</div>
			</div>
			<h6>Figure 2.2: POS tags with descriptions</h6>
			<p>These tags are from the Penn Treebank tagset<a href=""> (https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a>), which is one of the most popular tagsets. A majority of the pre-trained taggers for the English language are trained on this tagset, including NLTK's POS tagger.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor057"/>Applications of Parts of Speech Tagging</h2>
			<p>Just like text pre-processing techniques help the machine understand natural language better by encouraging it to focus on only the important details, POS tagging helps the machine actually interpret the context of text and thus make sense of it. While text pre-processing is more of a cleaning phase, parts of speech tagging is actually the part where the machine is beginning to output valuable information about corpora on its own.</p>
			<p>Understanding what words correspond to which parts of speech can be beneficial in processing natural language in several ways for a machine:</p>
			<ul>
				<li>POS tagging is useful in differentiating between homonyms – words that have the same spelling but mean different things. For example, the word "play" can mean the verb to play, as in engage in an activity, and also the noun, as in a dramatic work to be performed on stage. A POS tagger can help the machine understand what context the word "play" is being used in by determining its POS tag.</li>
				<li>POS tagging builds on the need for sentence and word segmentation – one of the basic tasks of natural language processing.</li>
				<li>POS tags are used in performing higher-level tasks by other algorithms, one of which we will be discussing in this chapter, named entity recognition.</li>
				<li>POS tags contribute to the process of sentiment analysis and question answering too. For example, in the sentence "Tim Cook is the CEO of this technology company," you want the machine to be able to replace "this technology company" with the name of the company. POS tagging can help the machine recognize that the phrase "this technology company" is a determiner ((this) + a noun phrase (technology company)). It can use this information to, for example, search articles online and check how many times "Tim Cook is the CEO of Apple" appears in them to then decide whether Apple is the correct answer.</li>
			</ul>
			<p>Thus, POS tagging is an important step in the process of understanding natural language because it contributes to other tasks.</p>
			<h3 id="_idParaDest-56"><a id="_idTextAnchor058"/>Types of POS Taggers</h3>
			<p>As we saw in the previous section, POS taggers can be both of the supervised and unsupervised learning type. This difference largely affects how a tagger is trained. There is another distinction that impacts how the tagger actually assigns a tag to an untagged word, which is the approach used to train the taggers.</p>
			<p>The two types of POS taggers are rule-based and stochastic. Let's take a look at both of them.</p>
			<h3 id="_idParaDest-57"><a id="_idTextAnchor059"/>Rule-Based POS Taggers</h3>
			<p>These POS taggers work pretty much exactly as their name states – by rules. The purpose for giving the taggers sets of rules is to ensure that they tag an ambiguous/unknown word accurately most of the times, thus most of the rules are applied only when the taggers come across an ambiguous/unknown word.</p>
			<p>These rules are often known as context frame rules and provide the taggers with contextual information to understand what tag to give an ambiguous word. An example of a rule is as follows: If an ambiguous/unknown word, x, is preceded by a determiner and followed by a noun, then assign it the tag of an adjective. An example of this would be "one small girl," where "one" is a determiner and "girl" is a noun, therefore the tagger will assign adjective to the word "small."</p>
			<p>The rules depend on your theory of grammar. Additionally, they also often include rules such as capitalization and punctuation. This can help you recognize pronouns and differentiate them from words found at the start of a sentence (following a full stop).</p>
			<p>Most rule-based POS taggers are supervised learning algorithms, in order to be able to learn the correct rules and apply them to properly tag ambiguous words. Recently, though, there have been experiments with training these taggers the unsupervised way. Untagged text is given to the tagger to tag, and humans go through the output tags, correcting whatever tags are inaccurate. This correctly tagged text is then given to the tagger so that it can develop correction rules between the two different tagsets and learn how to accurately tag words.</p>
			<p>An example of this correction rule-based POS tagger is Brill's tagger, which follows the process mentioned earlier. Its functioning can be compared with the art of painting – when painting a house, it is easier to first paint the background of the house (for example, a brown square) and then paint the details, such as a door and windows, on top of that background using a finer brush. Similarly, Brill's rule-based POS tagger aims to first generally tag an untagged corpus, even if some of the tags may be wrong, and then revisit those tags to understand why some are wrong and learn from them.</p>
			<h4><strong class="bold">Note</strong></h4>
			<p class="callout">Exercises 10-16 can be performed in the same Jupyter Notebook.</p>
			<h3 id="_idParaDest-58"><a id="_idTextAnchor060"/>Exercise 10: Performing Rule-Based POS Tagging</h3>
			<p>NLTK has a POS tagger that is a rule-based tagger. In this exercise, we will perform POS tagging using NLTK's POS tagger. The following steps will help you with the solution:</p>
			<ol>
				<li>Open cmd or terminal, depending on your operating system.</li>
				<li>Navigate to the desired path and use the following command to initiate a <strong class="inline">Jupyter</strong> Notebook:<p class="snippet"><strong class="inline">jupyter notebook</strong></p></li>
				<li>Import <strong class="inline">nltk</strong> and <strong class="inline">punkt</strong>, as shown:<p class="snippet">import nltk</p><p class="snippet">nltk.download('punkt')</p><p class="snippet">nltk.download('averaged_perceptron_tagger')</p><p class="snippet">nltk.download('tagsets')</p></li>
				<li>Store an input string in a variable called <strong class="inline">s</strong>, as follows:<p class="snippet"><strong class="inline">s = 'i enjoy playing the piano'</strong></p></li>
				<li>Tokenize the sentence, as demonstrated:<p class="snippet"><strong class="bold">tokens = nltk.word_tokenize(s)</strong></p></li>
				<li>Apply the POS tagger on the tokens and then print the tagset, as shown:<p class="snippet">tags = nltk.pos_tag(tokens)</p><p class="snippet">tags</p><p>Your output will look like this:</p><div class="IMG---Figure" id="_idContainer046"><img alt="Fig 2.3: Tagged output&#13;&#10;" src="image/C13783_02_03.jpg"/></div><h6>Fig 2.3: Tagged output</h6></li>
				<li>To understand what the "<strong class="inline">NN</strong>" POS tag stands for, you can use the following line of code:<p class="snippet">nltk.help.upenn_tagset("NN")</p><p>The output will be as follows:</p><div class="IMG---Figure" id="_idContainer047"><img alt="Fig 2.4: Noun details&#13;&#10;" src="image/C13783_02_04.jpg"/></div><h6>Fig 2.4: Noun details</h6><p>You can do this for each POS tag by substituting "NN" with it.</p><p>Let's try this out with a sentence containing homonyms.</p></li>
				<li>Store an input string containing homonyms in a variable called sent:<p class="snippet">sent = 'and so i said im going to play the piano for the play tonight'</p></li>
				<li>Tokenize this sentence and then apply the POS tagger on the tokens, as shown:<p class="snippet">tagset = nltk.pos_tag(nltk.word_tokenize(sent))</p><p class="snippet">tagset</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer048">
					<img alt="Fig 2.5: Tagged output&#13;&#10;" src="image/C13783_02_05.jpg"/>
				</div>
			</div>
			<h6>Fig 2.5: Tagged output</h6>
			<p>As you can see, the first instance of the word play has been tagged as '<strong class="bold">VB</strong>', which stands for verb, base form, and the second instance of the word play has been tagged as '<strong class="bold">NN</strong>', which stands for noun. Thus, POS taggers are able to differentiate between homonyms and different instances of the same word. This helps machines understand natural language better. </p>
			<h3 id="_idParaDest-59"><a id="_idTextAnchor061"/>Stochastic POS Taggers</h3>
			<p>Stochastic POS taggers are taggers that use any method other than rule-based methods to assign tags to words. Thus, there are a large number of approaches that fall into the stochastic category. All models that incorporate statistical methods, such as probability and frequency, when determining the POS tags for words are stochastic models.</p>
			<p>We will discuss three models:</p>
			<ul>
				<li>The Unigram or Word Frequency Approach</li>
				<li>The n – gram approach</li>
				<li>The hidden Markov Model</li>
			</ul>
			<p><em class="italics">The Unigram or Word Frequency Approach</em></p>
			<p>The simplest stochastic POS taggers assign POS tags to ambiguous words solely based on the probability that a word occurs with a tag. This basically means that whatever tag the tagger found linked with a word most often in the training set is the tag that it will assign to an ambiguous instance of the same word. For example, let's say the training set has the word "beautiful" tagged as an adjective a majority of the time. When the POS tagger encounters "beaut", it won't be able to tag this directly because it isn't a proper word. This will be an ambiguous word, and so it will calculate the probability of it being each of the POS tags, based on how many times different instances of this word have been tagged with each of those POS tags. "beaut" can be seen as an ambiguous form of "beautiful", and since "beautiful" has been tagged as an adjective a majority of the time, the POS tagger will tag "beaut" as an adjective too. This is called the word frequency approach because the tagger is checking the frequency of the POS tags assigned to words.</p>
			<p><em class="italics">The n – gram Approach</em></p>
			<p>This builds on the previous approach. The <strong class="bold">n</strong> in the name stands for how many words are considered when determining the probability of a word belonging to a particular POS tag. In the Unigram tagger, <strong class="bold">n = 1</strong>, and thus only the word itself is taken into consideration. Increasing the value of n results in taggers calculating the probability of a specific sequence of n POS tags occurring together and assigning a word a tag based on this probability.</p>
			<p>When assigning a tag to a word, these POS taggers create a context of the word by factoring in the type of token it is, along with the POS tags of the n preceding words. Based on the context, the taggers select the tag that is most likely to be in sequence with the tags of the preceding words and assigns this to the word in question. The most popular n – gram tagger is known as the Viterbi algorithm.</p>
			<p><em class="italics">Hidden Markov Model</em></p>
			<p><em class="italics">The hidden Markov model combines both the word frequency approach and the n – gram approach. A Markov model is one that describes a sequence of events or states. The probability of each state occurring depends solely on the state attained by the previous event. These events are based on observations. The "hidden" aspect of the hidden Markov model is that the set of states that an event could possibly be is hidden.</em></p>
			<p><em class="italics">In the case of POS tagging, the observations are the word tokens, and the hidden set of states are the POS tags. The way this works is that the model calculates the probability of a word having a particular tag based on what the tag of the previous word was. For example, P (V | NN) is the probability of the current word being a verb given that the previous word is a noun.</em></p>
			<h4><em class="italics">Note</em></h4>
			<p class="callout"><em class="italics">This is a very basic explanation of the hidden Markov model. To learn more, go to </em><a href="">https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24</a>.</p>
			<p class="callout">To learn more about stochastic models, go to <a href="">http://ccl.pku.edu.cn/doubtfire/NLP/Lexical_Analysis/Word_Segmentation_Tagging/POS_Tagging_Overview/POS%20Tagging%20Overview.htm</a>.</p>
			<p>The three approaches mentioned earlier have been explained in an order where each model builds upon and improves the accuracy of the preceding model. However, each model that builds upon a preceding model involves more calculations of probability and thus will take more time to perform computations, depending on the size of the training corpus. Therefore, the decision of which approach to use depends on the size of the corpus.</p>
			<h3 id="_idParaDest-60"><a id="_idTextAnchor062"/>Exercise 11: Performing Stochastic POS Tagging</h3>
			<p>spaCy's POS tagger is a stochastic one. In this exercise, we will use spaCy's POS tagger on some sentences to see the difference in the results of rule-based and stochastic tagging. The following steps will help you with the solution:</p>
			<h4>Note</h4>
			<p class="callout">To install spaCy, click on the following link and follow the instructions: <a href="">https://spacy.io/usage</a></p>
			<ol>
				<li value="1">Import <strong class="inline">spaCy</strong>:<p class="snippet">import spacy</p></li>
				<li>Load spaCy's '<strong class="inline">en_core_web_sm</strong>' model:<p class="snippet">nlp = spacy.load('en_core_web_sm')</p><p>spaCy has models that are specific to different languages. The 'en_core_web_sm' model is the English language model and has been trained on written web text, such as blogs and news articles, and includes vocabulary, syntax, and entities.</p><h4>Note</h4><p class="callout">To learn more about spaCy models, click on <a href="">https://spacy.io/models</a>.</p></li>
				<li>Fit the model on the sentence you want to assign POS tags to. Let's use the sentence we gave NLTK's POS tagger:<p class="snippet">doc = nlp(u"and so i said i'm going to play the piano for the play tonight")</p></li>
				<li>Now, let's tokenize this sentence, assign the POS tags, and print them:<p class="snippet">for token in doc:</p><p class="snippet">    print(token.text, token.pos_, token.tag_)</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer049">
					<img alt="Figure 2.6: Output for POS tags&#13;&#10;" src="image/C13783_02_06.jpg"/>
				</div>
			</div>
			<h6>Figure 2.6: Output for POS tags</h6>
			<p>To understand what a POS tag stands for, use the following line of code:</p>
			<p class="snippet">spacy.explain("VBZ")</p>
			<p>Replace "VBZ" with the POS tag you'd like to know about. In this case, your output will be this:</p>
			<p class="snippet">'verb, 3rd person singular present'</p>
			<p>As you can see, the results are pretty much the same as the ones obtained from the NLTK POS tagger. This is the case due to the simplicity of our input.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor063"/>Chunking</h2>
			<p>POS taggers work on individual tokens of words. Tagging individual words isn't always the best way to understand corpora, though. For example, the words 'United' and 'Kingdom' don't make a lot of sense when they're separated, but 'United Kingdom' together tells the machine that this is a country, thus providing it with more context and information. This is where the process of chunking comes into the picture.</p>
			<p>Chunking is an algorithm that takes words and their POS tags as input. It processes these individual tokens and their tags to see whether they can be combined. The combination of one or more individual tokens is known as a chunk, and the POS tag assigned to such a chunk is known as a chunk tag.</p>
			<p>Chunk tags are combinations of basic POS tags. They are easier to define phrases by and are more efficient than simple POS tags. These phrases are chunks. There will be instances where a single word is considered a chunk and assigned a chunk tag too. There are five major chunk tags:</p>
			<ul>
				<li><em class="italics">Noun Phrase</em> (<em class="italics">NP</em>): These are phrases that have nouns as the head word. They act as a subject or an object to the verb or verb phrase.</li>
				<li><em class="italics">Verb Phrase</em> (<em class="italics">VP</em>): These are phrases that have verbs as the head word.</li>
				<li><em class="italics">Adjective Phrase</em> (<em class="italics">ADJP</em>): These are phrases that have adjectives as the head word. Describing and qualifying nouns or pronouns is the main function of adjective phrases. They are found either directly before or after the noun or pronoun.</li>
				<li><em class="italics">Adverb Phrase</em> (<em class="italics">ADVP</em>): These are phrases that have adverbs as the head word. They're used as modifiers for nouns and verbs by providing details that describe and qualify them.</li>
				<li><em class="italics">Prepositional Phrase</em> (<em class="italics">PP</em>): These are phrases that have prepositions as the head word. They position an action or an entity in time or space.</li>
			</ul>
			<p>For example, in the sentence 'the yellow bird is slow and is flying into the brown house', the following phrases will be assigned the following chunk tags:</p>
			<p>'the yellow bird' – NP</p>
			<p>'is' – VP</p>
			<p>'slow' – ADJP</p>
			<p>'is flying' – VP</p>
			<p>'into' – PP</p>
			<p>'the brown house' – NP</p>
			<p>Thus, chunking is performed after POS tagging has been applied on a corpus. This allows the text to be broken down into its simplest form (tokens of words), have its structure analyzed, and then be grouped back together into meaningful higher-level chunks. Chunking also benefits the process of named entity recognition. We'll see how in the coming section.</p>
			<p>The chunk parser present within the NLTK library is rule based and thus needs to be given a regular expression as a rule to output a chunk with its chunk tag. <strong class="bold">spaCy</strong> can perform chunking without the presence of rules. Let's take a look at both these approaches.</p>
			<h3 id="_idParaDest-62"><a id="_idTextAnchor064"/>Exercise 12: Performing Chunking with NLTK</h3>
			<p>In this exercise, we will generate chunks and chunk tags. <strong class="bold">nltk</strong> has a regular expression parser. This requires an input of a regular expression of a phrase and the corresponding chunk tag. It then searches the corpus for this expression and assigns it the tag.</p>
			<p>Since chunking works with POS tags, we can add on to our code from the POS tagging exercise. We saved the tokens with their respective POS tags in 'tagset'. Let's use this. The following steps will help you with the solution:</p>
			<ol>
				<li value="1">Create a regular expression that will search for a noun phrase, as shown:<p class="snippet">rule = r"""Noun Phrase: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}"""</p><p>This regular expression is searching for a determiner (optional), followed by one or more adjectives and then a single noun. This will form a chunk called <strong class="inline">Noun Phrase</strong>.</p><h4>Note</h4><p class="callout">If you don't know how to write Regular Expressions, check out these quick tutorials: <a href="">https://www.w3schools.com/python/python_regex.asp</a> <a href="">https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/</a></p></li>
				<li>Create an instance of <strong class="inline">RegexpParser</strong> and feed it the rule:<p class="snippet">chunkParser = nltk.RegexpParser(rule)</p></li>
				<li>Give <strong class="inline">chunkParser</strong> the <strong class="inline">tagset</strong> containing the tokens with their respective POS tags so that it can perform chunking, and then draw the chunks:<p class="snippet">chunked = chunkParser.parse(tagset)</p><p class="snippet">chunked.draw()</p><h4>Note</h4><p class="callout">matplotlib needs to be installed on your machine for the <strong class="inline">.draw()</strong> function to work.</p><p>Your output will look something like this:</p><div class="IMG---Figure" id="_idContainer050"><img alt="Figure 2.7: Parse tree.&#13;&#10;" src="image/C13783_02_07.jpg"/></div><h6>Figure 2.7: Parse tree.</h6><p>This is a parse tree. As you can see, the chunking process has recognized the noun phrases and labeled them, and the remaining tokens are shown with their POS tags.</p></li>
				<li>Let's try the same thing out with another sentence. Store an input sentence in another variable:<p class="snippet">a = "the beautiful butterfly flew away into the night sky"</p></li>
				<li>Tokenize the sentence and perform POS tagging using NLTK's POS tagger:<p class="snippet">tagged = nltk.pos_tag(nltk.word_tokenize(a))</p></li>
				<li>Repeat step 3:<p class="snippet">chunked2 = chunkParser.parse(tagged)</p><p class="snippet">chunked2.draw()              </p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer051">
					<img alt="Figure 2.8: Output for chunking.&#13;&#10;" src="image/C13783_02_08.jpg"/>
				</div>
			</div>
			<h6>Figure 2.8: Output for chunking.</h6>
			<h3 id="_idParaDest-63"><a id="_idTextAnchor065"/>Exercise 13: Performing Chunking with spaCy</h3>
			<p>In this exercise, we will implement chunking with spaCy. <strong class="bold">spaCy</strong> doesn't require us to formulate rules to recognize chunks; it identifies chunks on its own and tells us what the head word is, thus telling us what the chunk tag is. Let's identify some noun chunks using the same sentence from Exercise 12. The following steps will help you with the solution:</p>
			<ol>
				<li value="1">Fit <strong class="inline">spaCy</strong>'s English model on the sentence:<p class="snippet">doc = nlp(u"the beautiful butterfly flew away into the night sky")</p></li>
				<li>Apply <strong class="inline">noun_chunks</strong> on this model, and for each chunk, print the text of the chunk, the root word of the chunk, and the dependency relation that connects the root word to its head:<p class="snippet">for chunk in doc.noun_chunks:</p><p class="snippet">    print(chunk.text, chunk.root.text, chunk.root.dep_)</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer052">
					<img alt="Figure 2.9: Output for chunking with spaCy&#13;&#10;" src="image/C13783_02_09.jpg"/>
				</div>
			</div>
			<h6>Figure 2.9: Output for chunking with spaCy</h6>
			<p>As you can see, chunking with <strong class="bold">spaCy</strong> is a lot simpler than with NLTK.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor066"/>Chinking</h2>
			<p>Chinking is an extension of chunking, as you've probably guessed already from its name. It's not a mandatory step in processing natural language, but it can be beneficial.</p>
			<p>Chinking is performed after chunking. Post chunking, you have chunks with their chunk tags, along with individual words with their POS tags. Often, these extra words are unnecessary. They don't contribute to the final result or the entire process of understanding natural language and thus are a nuisance. The process of chinking helps us deal with this issue by extracting the chunks, and their chunk tags form the tagged corpus, thus getting rid of the unnecessary bits. These useful chunks are called chinks once they have been extracted from the tagged corpus.</p>
			<p>For example, if you need only the nouns or noun phrases from a corpus to answer questions such as "what is this corpus talking about?", you would apply chinking because it would extract just what you want and present it in front of your eyes. Let's check this out with an exercise.</p>
			<h3 id="_idParaDest-65"><a id="_idTextAnchor067"/>Exercise 14: Performing Chinking</h3>
			<p>Chinking is basically altering the things that you're looking for in a corpus. Thus, applying chinking involves altering the rule (regular expression) provided to <strong class="inline">chinkParser</strong>. The following steps will help you with the solution:</p>
			<ol>
				<li value="1">Create a rule that chunks the entire corpus and only creates chinks out of the words or phrases tagged as nouns or noun phrases:<p class="snippet">rule = r"""Chink: {&lt;.*&gt;+}</p><p class="snippet">                    }&lt;VB.?|CC|RB|JJ|IN|DT|TO&gt;+{"""</p><p>This rule is in the form of a regular expression. Basically, this regular expression is telling the machine to ignore all words that are not nouns or noun phrases. When it comes across a noun or a noun phrase, this rule will ensure that it is extracted as a chink.</p></li>
				<li>Create an instance of <strong class="inline">RegexpParser</strong> and feed it the rule:<p class="snippet">chinkParser = nltk.RegexpParser(rule)</p></li>
				<li>Give <strong class="inline">chinkParser</strong> the <strong class="inline">tagset</strong> containing the tokens with their respective POS tags so that it can perform chinking, and then draw the chinks:<p class="snippet">chinked = chinkParser.parse(tagset)</p><p class="snippet">chinked.draw()</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer053">
					<img alt="Figure 2.10: Output for chinking&#13;&#10;" src="image/C13783_02_10.jpg"/>
				</div>
			</div>
			<h6>Figure 2.10: Output for chinking</h6>
			<p>As you can see, the chinks have been highlighted and contain only nouns.</p>
			<h3 id="_idParaDest-66"><a id="_idTextAnchor068"/>Activity 2: Building and Training Your Own POS Tagger</h3>
			<p>We've already looked at POS tagging words using the existing and pre-trained POS taggers. In this activity, we will train our own POS tagger. This is like training any other machine learning algorithm. The following steps will help you with the solution:</p>
			<ol>
				<li value="1">Pick a corpus to train the tagger on. You can use the nltk treebank to work on. The following code should help you import the treebank corpus:<p class="snippet">nltk.download('treebank')</p><p class="snippet">tagged = nltk.corpus.treebank.tagged_sents()</p></li>
				<li>Determine what features the tagger will consider when assigning a tag to a word.</li>
				<li>Create a function to strip the tagged words of their tags so that we can feed them into our tagger.</li>
				<li>Build the dataset and split the data into training and testing sets. Assign the features to 'X' and append the POS tags to 'Y'. Apply this function on the training set.</li>
				<li>Use the decision tree classifier to train the tagger.</li>
				<li>Import the classifier, initialize it, fit the model on the training data, and print the accuracy score.<h4>Note</h4><p class="callout">The accuracy score in the output may vary, depending on the corpus used.</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer054">
					<img alt="Figure 2.11: Expected accuracy score.&#13;&#10;" src="image/C13783_02_11.jpg"/>
				</div>
			</div>
			<h6>Figure 2.11: Expected accuracy score.</h6>
			<h4>Note</h4>
			<p class="callout">The solution for the activity can be found on page 297.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor069"/>Named Entity Recognition</h2>
			<p>This is one of the first steps in the process of information extraction. Information extraction is the task of a machine extracting structured information from unstructured or semi-structured text. This furthers the comprehension of natural language by machines.</p>
			<p>After text preprocessing and POS tagging, our corpus becomes semi-structured and machine-readable. Thus, information extraction is performed after we've readied our corpus.</p>
			<p>The following diagram is an example of named entity recognition:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer055">
					<img alt="Figure 2.12: Example for named entity recognition&#13;&#10;" src="image/C13783_02_12.jpg"/>
				</div>
			</div>
			<h6>Figure 2.12: Example for named entity recognition</h6>
			<h3 id="_idParaDest-68"><a id="_idTextAnchor070"/>Named Entities</h3>
			<p>Named entities are real-world objects that can be classified into categories, such as people, places, and things. Basically, they are words that can be denoted by a proper name. Named entities can also include quantities, organizations, monetary values, and many more things.</p>
			<p>Some examples of named entities and the categories they fall under are as follows:</p>
			<ul>
				<li>Donald Trump, person</li>
				<li>Italy, location</li>
				<li>Bottle, object</li>
				<li>500 USD, money</li>
			</ul>
			<p>Named entities can be viewed as instances of entities. In the previous examples, the categories are basically entities in their own and the named entities are instances of those. For example, London is an instance of city, which is an entity.</p>
			<p>The most common named entity categories are as listed:</p>
			<ul>
				<li>ORGANIZATION</li>
				<li>PERSON</li>
				<li>LOCATION</li>
				<li>DATE</li>
				<li>TIME</li>
				<li>MONEY</li>
				<li>PERCENT</li>
				<li>FACILITY</li>
				<li>GPE (which stands Geo-Political Entity)</li>
			</ul>
			<h3 id="_idParaDest-69"><a id="_idTextAnchor071"/>Named Entity Recognizers</h3>
			<p>Named entity recognizers are algorithms that identify and extract named entities from corpora and assign them a category. The input provided to a trained named entity recognizer consists of tokenized words with their respective POS tags. The output of named entity recognition is named entities along with their categories, among the other tokenized words and their POS tags.</p>
			<p>The problem of named entity recognition takes place in two phases:</p>
			<ol>
				<li value="1">Identifying and recognizing named entities (for example, 'London')</li>
				<li>Classifying these names entities (for example, 'London' is a 'location')</li>
			</ol>
			<p>The first phase of identifying named entities is quite similar to the process of chunking, because the aim is to recognize things that are denoted by proper names. The named entity recognizer needs to look out for continuous sequences of tokens to be able to correctly spot named entities. For example, 'Bank of America' should be identified as a single named entity, despite the phrase containing the word 'America', which in itself is a named entity.</p>
			<p>Much like POS taggers, most named entity recognizers are supervised learning algorithms. They are trained on input that contains named entities along with the categories that they fall under, thus enabling the algorithm to learn how to classify unknown named entities in the future.</p>
			<p>This input containing named entities with their respective categories is often known as a knowledge base. Once a named entity recognizer has been trained and is given an unrecognized corpus, it refers to this knowledge base to search for the most accurate classification to assign to a named entity.</p>
			<p>However, due to the fact that supervised learning requires an excessive amount of labeled data, unsupervised learning versions of named entity recognizers are also being researched. These are trained on unlabeled corpora – text that doesn't have named entities categorized. Like POS taggers, named entity recognizers categorize the named entities, and then the incorrect categories are corrected manually by humans. This corrected data is fed back to the NERs so that they can simply learn from their mistakes.</p>
			<h3 id="_idParaDest-70"><a id="_idTextAnchor072"/>Applications of Named Entity Recognition</h3>
			<p>As mentioned earlier, named entity recognition is one of the first steps of information extraction and thus plays a major role in enabling machines to understand natural language and perform a variety of tasks based on it. Named entity recognition is and can be used in various industries and scenarios to simplify and automate processes. Let's take a look at a few use cases:</p>
			<ul>
				<li><em class="italics">Online content</em>, including articles, reports, and blog posts, are often tagged to enable users to search for it more easily and also to get a quick overview of what exactly the content is about. Named entity recognizers can be used to scour through this content and extract named entities to automatically generate these tags. These tags help categorize articles into predefined hierarchies as well.</li>
				<li><em class="italics">Search algorithms</em> also benefit from these tags. If a user were to enter a keyword into a search algorithm, instead of scouring through all the words of every article (which will take forever), the algorithm just needs to refer to the tags produced by named entity recognition to pull up articles containing or pertaining to the entered keyword. This reduces the computational time and operations by a lot.</li>
				<li>Another purpose for these tags is to create an <em class="italics">efficient recommendation system</em>. If you read an article that discusses the current political situation in India, and is thus maybe tagged as 'Indian Politics' (this is just an example), the news website can use this tag to suggest different articles with the same or similar tags. This also works in the case of visual entertainment such as movies and shows. Online streaming websites use tags assigned to content (for example, genres such as 'action', 'adventure', 'thriller', and so on) to understand your taste better and thus recommend similar content to you.</li>
				<li><em class="italics">Customer feedback</em> is important for any service or product providing company. Running customer complaints and reviews through named entity recognizers produces tags that can help classify them based on location, type of product, and type of feedback (positive or negative). These reviews and complaints can then be sent to the people responsible for that particular product or that particular area and can be dealt with based on whether the feedback is positive or negative. The same thing can be done with tweets, Instagram captions, Facebook posts, and so on.</li>
			</ul>
			<p>As you can see, there are many applications of named entity recognition. Thus, it is important to understand how it works and how to implement it.</p>
			<h3 id="_idParaDest-71"><a id="_idTextAnchor073"/>Types of Named Entity Recognizers</h3>
			<p>As is the case with POS taggers, there are two broad methods to design a named entity recognizer: a linguistic approach by defining rules to recognize entities, or a stochastic approach using statistical models to accurately determine which category a named entity falls into best.</p>
			<h3 id="_idParaDest-72"><a id="_idTextAnchor074"/>Rule-Based NERs</h3>
			<p>Rule-based NERs work in the same way that rule-based POS taggers do.</p>
			<h3 id="_idParaDest-73"><a id="_idTextAnchor075"/>Stochastic NERs</h3>
			<p>These include any and all models that use statistics to name and recognize entities. There are several approaches to stochastic named entity recognition. Let's take a look at two of them:</p>
			<ul>
				<li><em class="italics">Maximum Entropy Classification</em><p>This is a machine learning classification model. It calculates the probability of a named entity falling into a particular category solely on the basis of the information provided to it (the corpus).</p><h4>Note</h4><p class="callout">For more information on Maximum Entropy Classification, go to <a href="">http://blog.datumbox.com/machine-learning-tutorial-the-max-entropy-text-classifier/</a>.</p></li>
				<li><em class="italics">Hidden Markov Model</em><p><em class="italics">This method is the same as the one explained in the POS tagging section, but instead of the hidden set of states being the POS tags, they are the categories of the named entities.</em></p><h4>Note</h4><p class="callout">For more information on stochastic named entity recognition and when to use which approach, go to <a href="">http://www.datacommunitydc.org/blog/2013/04/a-survey-of-stochastic-and-gazetteer-based-approaches-for-named-entity-recognition-part-2</a>.</p></li>
			</ul>
			<h3 id="_idParaDest-74"><a id="_idTextAnchor076"/>Exercise 15: Perform Named Entity Recognition with NLTK</h3>
			<p>In this exercise, we'll use the <strong class="inline">ne_chunk</strong> algorithm of <strong class="inline">NLTK</strong> to perform named entity recognition on a sentence. Instead of using the sentences we used in the previous exercises, create a new sentence that contains proper names that can be classified into categories so that you can actually see the results:</p>
			<ol>
				<li value="1">Store an input sentence in a variable, as shown:<p class="snippet">ex = "Shubhangi visited the Taj Mahal after taking a SpiceJet flight from Pune."</p></li>
				<li>Tokenize the sentence and assign <strong class="inline">POS tags</strong> to the tokens:<p class="snippet">tags = nltk.pos_tag(nltk.word_tokenize(ex))</p></li>
				<li>Apply the <strong class="inline">ne_chunk()</strong> algorithm on the tagged words and either print or draw the results:<p class="snippet">ne = nltk.ne_chunk(tags, binary = True)</p><p class="snippet">ne.draw()</p><p>Assigning the value of '<strong class="inline">True</strong>' to the '<strong class="inline">binary</strong>' parameter tells the algorithm to just recognize the named entities and not classify them. Thus, your results will look something like this:</p><div class="IMG---Figure" id="_idContainer056"><img alt="Figure 2.13: Output for named entity recognition with POS tags&#13;&#10;" src="image/C13783_02_13.jpg"/></div><h6>Figure 2.13<a id="_idTextAnchor077"/>: Output for named entity recognition with POS tags</h6><p>As you can see, the named entities have been highlighted as '<strong class="inline">NE</strong>'.</p></li>
				<li>To know which categories the algorithm has assigned to these named entities, simply assign the value of '<strong class="inline">False</strong>' to the '<strong class="inline">binary</strong>' parameter:<p class="snippet">ner = nltk.ne_chunk(tags, binary = False)</p><p class="snippet">ner.draw()</p><p>Expected output:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer057">
					<img alt="Figure 2.14: Output with named entities&#13;&#10;" src="image/C13783_02_14.jpg"/>
				</div>
			</div>
			<h6>Figure 2.14: Output with named entities</h6>
			<p>The algorithm has accurately categorized 'Shubhangi' and 'SpiceJet'. 'Taj Mahal', however, shouldn't be an ORGANIZATION, it should be a FACILITY. Thus, NLTK's <strong class="inline">ne_chunk()</strong> algorithm isn't the best one.</p>
			<h3 id="_idParaDest-75"><a id="_idTextAnchor078"/>Exercise 16: Performing Named Entity Recognition with spaCy</h3>
			<p>In this exercise, we'll be implementing <strong class="bold">spaCy</strong>'s named entity recognizer on the sentence from the previous exercise and compare the results. spaCy has several NERs that have been trained on different corpora. Each model has a different set of categories; here's a list of all the categories spaCy can recognize:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer058">
					<img alt="Figure 2.15: Categories of spaCy&#13;&#10;" src="image/C13783_02_15.jpg"/>
				</div>
			</div>
			<h6>Figure 2.15: Categories of spaCy</h6>
			<p>The following steps will help you with the solution:</p>
			<ol>
				<li value="1">Fit <strong class="inline">spaCy</strong>'s English model on the sentence we used in the previous exercise:<p class="snippet">doc = nlp(u"Shubhangi visited the Taj Mahal after taking a SpiceJet flight from Pune.")</p></li>
				<li>For each entity in this sentence, print the text of the entity and the label:<p class="snippet">for ent in doc.ents:</p><p class="snippet">    print(ent.text, ent.label_)</p><p>Your output will look something like this:</p><div class="IMG---Figure" id="_idContainer059"><img alt="Figure 2.16: Output for named entity&#13;&#10;" src="image/C13783_02_16.jpg"/></div><h6>Figure 2.16: Output for named entity</h6><p>It's only recognizing 'SpiceJet' and 'Pune' as named entities, and not 'Shubhangi' and 'Taj Mahal'. Let's try adding a last name to 'Shubhangi' and check whether that makes a difference.</p></li>
				<li>Fit the model on the new sentence:<p class="snippet">doc1 = nlp(u"Shubhangi Hora visited the Taj Mahal after taking a SpiceJet flight from Pune.")</p></li>
				<li>Repeat step 2:<p class="snippet">for ent in doc1.ents:</p><p class="snippet">    print(ent.text, ent.label_)</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer060">
					<img alt="Figure 2.17: Output for named entity recognition with spaCy.&#13;&#10;" src="image/C13783_02_17.jpg"/>
				</div>
			</div>
			<h6>Figure 2.17: Output for named entity recognition with spaCy.</h6>
			<p>So now that we've added a last name, "Shubhangi Hora" is recognized as a PERSON, and "Taj Mahal" is recognized as a <strong class="bold">WORK_OF ART</strong>. The latter is incorrect, since if you check the table of categories, <strong class="bold">WORK_OF_ART</strong> is used to describe songs and books.</p>
			<p>Thus, the recognition and categorization of named entities strongly depends on the data that the recognizer has been trained on. This is something to keep in mind when implementing named entity recognition; it is often better to train and develop your own recognizer for specific use cases.</p>
			<h3 id="_idParaDest-76"><a id="_idTextAnchor079"/>Activity 3: Performing NER on a Tagged Corpus</h3>
			<p>Now that we've seen how to perform named entity recognition on a sentence, in this activity, we'll perform named entity recognition on a corpus that has been through POS tagging. Imagine that you're given a corpus that you've identified the POS tags for and now your job is to extract entities from it so that you can provide an overall summary of what the corpus is discussing. The following steps will help you with the solution:</p>
			<ol>
				<li value="1">Import NLTK and other necessary packages.</li>
				<li>Print <strong class="inline">nltk.corpus.treebank.tagged_sents()</strong> to see the tagged corpus that you need extract named entities from.</li>
				<li>Store the first sentence of the tagged sentences in a variable.</li>
				<li>Use <strong class="inline">nltk.ne_chunk</strong> to perform NER on the sentence. Set <strong class="bold">binary</strong> to <strong class="bold">True</strong> and print the named entities.</li>
				<li>Repeat steps 3 and 4 on any number of sentences to see the different entities that exist in the corpus. Set the <strong class="bold">binary</strong> parameter to <strong class="bold">False</strong> to see what the named entities are categorized as.<p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer061">
					<img alt="Figure 2.18: Expected output for NER on tagged corpus&#13;&#10;" src="image/C13783_02_18.jpg"/>
				</div>
			</div>
			<h6>Figure 2.18: Expected output for NER on tagged corpus</h6>
			<h4>Note</h4>
			<p class="callout">The solution for the activity can be found on page 300.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor080"/>Summary</h2>
			<p>Natural language processing enables a machine to understand the language of humans, and just as we learned how to comprehend and process language, machines are taught as well. Two ways of better understanding language that allow machines to contribute to the real world are POS tagging and named entity recognition.</p>
			<p>The former is the process of assigning POS tags to individual words so that the machine can learn context, and the latter is recognizing and categorizing named entities to extract valuable information from corpora.</p>
			<p>There are distinctions in the way these processes are performed: the algorithms can be supervised or unsupervised, and the approach can be rule-based or stochastic. Either way, the goal is the same, that is, to comprehend and communicate with humans in their natural language.</p>
			<p>In the next chapter, we will be discussing neural networks, how they work, and how they can be used for natural language processing.</p>
		</div>
	</body></html>