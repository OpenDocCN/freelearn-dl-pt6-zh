["```py\ngroupId: edu.stanford.nlp\nartifactId: stanford-corenlp\nversion: 3.8.0\n\ngroupId: edu.stanford.nlp\nartifactId: stanford-corenlp\nversion: 3.8.0\nclassifier: models\n```", "```py\ngroupId: edu.stanford.nlp\n artifactId: stanford-corenlp\n version: 3.8.0\n classifier: models-spanish\n```", "```py\nval sparkSession = SparkSession\n   .builder()\n   .appName(\"spark-corenlp example\")\n   .master(master)\n   .getOrCreate()\n```", "```py\nimport sparkSession.implicits._\n\n val input = Seq(\n   (1, \"<xml>Packt is a publishing company based in Birmingham and Mumbai. It is a great publisher.</xml>\")\n ).toDF(\"id\", \"text\")\n```", "```py\nval output = input\n     .select(cleanxml('text).as('doc))\n     .select(explode(ssplit('doc)).as('sen))\n     .select('sen, tokenize('sen).as('words), ner('sen).as('nerTags), sentiment('sen).as('sentiment))\n```", "```py\noutput.show(truncate = false)\n```", "```py\nsparkSession.stop()\n```", "```py\ngroupId: com.johnsnowlabs.nlp\n artifactId: spark-nlp_2.11\n version: 1.6.3\n```", "```py\nval sparkSession: SparkSession = SparkSession\n         .builder()\n         .appName(\"Ner DL Pipeline\")\n         .master(\"local[*]\")\n         .getOrCreate()\n```", "```py\nval document = new DocumentAssembler()\n     .setInputCol(\"text\")\n     .setOutputCol(\"document\")\n```", "```py\nval token = new Tokenizer()\n     .setInputCols(\"document\")\n     .setOutputCol(\"token\")\n```", "```py\nval normalizer = new Normalizer()\n     .setInputCols(\"token\")\n     .setOutputCol(\"normal\")\n```", "```py\nval ner = NerDLModel.pretrained()\n     .setInputCols(\"normal\", \"document\")\n     .setOutputCol(\"ner\")\n```", "```py\nval nerConverter = new NerConverter()\n     .setInputCols(\"document\", \"normal\", \"ner\")\n     .setOutputCol(\"ner_converter\")\n```", "```py\nval finisher = new Finisher()\n     .setInputCols(\"ner\", \"ner_converter\")\n     .setIncludeMetadata(true)\n     .setOutputAsArray(false)\n     .setCleanAnnotations(false)\n     .setAnnotationSplitSymbol(\"@\")\n     .setValueSplitSymbol(\"#\")\n```", "```py\nval pipeline = new Pipeline().setStages(Array(document, token, normalizer, ner, nerConverter, finisher))\n```", "```py\nval testing = Seq(\n     (1, \"Packt is a famous publishing company\"),\n     (2, \"Guglielmo is an author\")\n ).toDS.toDF( \"_id\", \"text\")\n```", "```py\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(testing)\n```", "```py\nresult.select(\"ner\", \"ner_converter\").show(truncate=false)\n```", "```py\nBenchmark.time(\"Time to convert and show\") {result.select(\"ner\", \"ner_converter\").show(truncate=false)}\n```", "```py\nsparkSession.stop\n```", "```py\nval sparkSession: SparkSession = SparkSession\n     .builder()\n     .appName(\"Tokenize with n-gram example\")\n     .master(\"local[*]\")\n     .config(\"spark.driver.memory\", \"1G\")\n     .config(\"spark.kryoserializer.buffer.max\",\"200M\")\n     .config(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\")\n     .getOrCreate()\n```", "```py\nval document = new DocumentAssembler()\n  .setInputCol(\"text\")\n  .setOutputCol(\"document\")\n\nval token = new Tokenizer()\n  .setInputCols(\"document\")\n  .setOutputCol(\"token\")\n\nval normalizer = new Normalizer()\n  .setInputCols(\"token\")\n  .setOutputCol(\"normal\")\n```", "```py\nval finisher = new Finisher()\n     .setInputCols(\"normal\")\n```", "```py\nval ngram = new NGram()\n   .setN(3)\n   .setInputCol(\"finished_normal\")\n   .setOutputCol(\"3-gram\")\n```", "```py\nval gramAssembler = new DocumentAssembler()\n   .setInputCol(\"3-gram\")\n   .setOutputCol(\"3-grams\")\n```", "```py\nval pipeline = new Pipeline().setStages(Array(document, token, normalizer, finisher, ngram, gramAssembler))\n```", "```py\nimport sparkSession.implicits._\nval testing = Seq(\n  (1, \"Packt is a famous publishing company\"),\n  (2, \"Guglielmo is an author\")\n).toDS.toDF( \"_id\", \"text\")\n```", "```py\nval result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(testing)\n```", "```py\nresult.show(truncate=false)\n```", "```py\nsparkSession.stop\n```", "```py\nval spark: SparkSession = SparkSession\n     .builder\n     .appName(\"Train Vivek N Sentiment Analysis\")\n     .master(\"local[*]\")\n     .config(\"spark.driver.memory\", \"2G\")\n     .config(\"spark.kryoserializer.buffer.max\",\"200M\")\n  .config(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\")\n     .getOrCreate\n```", "```py\nimport spark.implicits._\n\nval training = Seq(\n  (\"I really liked it!\", \"positive\"),\n  (\"The cast is horrible\", \"negative\"),\n  (\"Never going to watch this again or recommend it\", \"negative\"),\n  (\"It's a waste of time\", \"negative\"),\n  (\"I loved the main character\", \"positive\"),\n  (\"The soundtrack was really good\", \"positive\")\n).toDS.toDF(\"train_text\", \"train_sentiment\")\n\nWhile the testing data set could be a simple Array:\n\nval testing = Array(\n  \"I don't recommend this movie, it's horrible\",\n  \"Dont waste your time!!!\"\n)\n```", "```py\nval document = new DocumentAssembler()\n  .setInputCol(\"train_text\")\n  .setOutputCol(\"document\")\n\nval token = new Tokenizer()\n  .setInputCols(\"document\")\n  .setOutputCol(\"token\")\n\nval normalizer = new Normalizer()\n  .setInputCols(\"token\")\n  .setOutputCol(\"normal\")\n```", "```py\nval vivekn = new ViveknSentimentApproach()\n  .setInputCols(\"document\", \"normal\")\n  .setOutputCol(\"result_sentiment\")\n  .setSentimentCol(\"train_sentiment\")\n\nAnd finally we use a Finisher transformer as last stage:\n\nval finisher = new Finisher()\n  .setInputCols(\"result_sentiment\")\n  .setOutputCols(\"final_sentiment\")\n```", "```py\nval pipeline = new Pipeline().setStages(Array(document, token, normalizer, vivekn, finisher))\n```", "```py\nval sparkPipeline = pipeline.fit(training)\n```", "```py\nval testingDS = testing.toSeq.toDS.toDF(\"testing_text\")\n println(\"Updating DocumentAssembler input column\")\n document.setInputCol(\"testing_text\")\n sparkPipeline.transform(testingDS).show()\n```", "```py\nBenchmark.time(\"Spark pipeline benchmark\") {\n   val testingDS = testing.toSeq.toDS.toDF(\"testing_text\")\n   println(\"Updating DocumentAssembler input column\")\n   document.setInputCol(\"testing_text\")\n   sparkPipeline.transform(testingDS).show()\n }\n```"]