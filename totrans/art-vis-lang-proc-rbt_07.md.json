["```py\n    import nltk\n    import gensim.models.word2vec as w2v\n    import sklearn.manifold\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    ```", "```py\n    corpus = ['king is a happy man', \n              'queen is a funny woman',\n              'queen is an old woman',\n              'king is an old man', \n              'boy is a young man',\n              'girl is a young woman',\n              'prince is a young king',\n              'princess is a young queen',\n              'man is happy, \n              'woman is funny,\n              'prince is a boy will be king',\n              'princess is a girl will be queen']\n    ```", "```py\n    import spacy\n    import en_core_web_sm\n    nlp = en_core_web_sm.load()\n    def corpus_tokenizer(corpus):\n        sentences = []\n        for c in corpus:\n            doc = nlp(c)\n            tokens = []\n            for t in doc:\n                if t.is_stop == False:\n                    tokens.append(t.text)\n            sentences.append(tokens)\n        return sentences\n    sentences = corpus_tokenizer(corpus)\n    sentences\n    ```", "```py\n    num_features=2\n    window_size=1\n    workers=1\n    min_word_count=1\n    ```", "```py\n    model = w2v.Word2Vec(size=num_features, window=window_size,workers=workers,min_count=min_word_count,seed=0)\n    ```", "```py\n    model.build_vocab(sentences)\n    ```", "```py\n    model.train(sentences,total_words=model.corpus_count,epochs=1)\n    ```", "```py\n    model.wv['king']\n    model.wv.similarity('boy', 'prince')\n    ```", "```py\n    vocab = list(model.wv.vocab)\n    X = model.wv[vocab]\n    ```", "```py\n    df = pd.DataFrame(X, index=vocab, columns=['x', 'y'])\n    df\n    ```", "```py\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    for word, pos in df.iterrows():\n        ax.annotate(word, pos)\n    ax.scatter(df['x'], df['y'])\n    plt.show()\n    ```", "```py\n    from google.colab import drive\n    drive.mount('/content/drive')\n    ```", "```py\n    cd /content/drive/My Drive/C13550/Lesson07/Exercise25/\n    ```", "```py\n    from gensim.scripts.glove2word2vec import glove2word2vec\n    from gensim.models import KeyedVectors\n    import numpy as np\n    import pandas as pd\n    ```", "```py\n    glove_input_file = 'utils/glove.6B.50d.txt'\n    word2vec_output_file = 'utils/glove.6B.50d.txt.word2vec'\n    glove2word2vec(glove_input_file, word2vec_output_file)\n    ```", "```py\n    filename = 'utils/glove.6B.50d.txt.word2vec'\n    model = KeyedVectors.load_word2vec_format(filename, binary=False)\n    ```", "```py\n    model.similarity('woman', 'queen')\n    ```", "```py\n    model['woman']\n    ```", "```py\n    model.similar_by_word(woman)\n    ```", "```py\n    from sklearn.decomposition import TruncatedSVD\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    ```", "```py\n    close_words=model.similar_by_word('woman')\n\n    arr = np.empty((0,50), dtype='f')\n    labels = ['woman']\n    #Array with the vectors of the closest words\n    arr = np.append(arr, np.array([model['woman']]), axis=0)\n    print(\"Matrix with the word 'woman':\\n\", arr)\n    ```", "```py\n    for w in close_words:\n        w_vector = model[w[0]]\n        labels.append(w[0])\n        arr = np.append(arr, np.array([w_vector]), axis=0)\n    arr\n    ```", "```py\n    svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n    svdvals = svd.fit_transform(arr)\n    ```", "```py\n    df = pd.DataFrame(svdvals, index=labels, columns=['x', 'y'])\n    df\n    ```", "```py\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    for word, pos in df.iterrows():\n        ax.annotate(word, pos)\n    ax.scatter(df['x'], df['y'])\n    plt.show()\n    ```", "```py\n    from google.colab import drive\n    drive.mount('/content/drive')\n    ```", "```py\n    /content/drive/My Drive/C13550/Lesson07/Exercise25-26\n    ```", "```py\n    from gensim.scripts.glove2word2vec import glove2word2vec\n    from gensim.models import KeyedVectors\n    import numpy as np\n    from os import listdir\n    ```", "```py\n    import spacy\n    import en_core_web_sm\n    nlp = en_core_web_sm.load()\n    # return a list of tokens without punctuation marks\n    def pre_processing(sentences):\n        tokens = []\n        for s in sentences:\n            doc = nlp(s)\n            for t in doc:\n                if t.is_punct == False:\n                    tokens.append(t.lower_)\n        return tokens\n    ```", "```py\n    filename = 'utils/glove.6B.50d.txt.word2vec'\n    model = KeyedVectors.load_word2vec_format(filename, binary=False)\n    ```", "```py\n    intent_route = 'utils/training/'\n    response_route = 'utils/responses/'\n    intents = listdir(intent_route)\n    responses = listdir(response_route)\n    ```", "```py\n    def doc_vector(tokens):\n        feature_vec = np.zeros((50,), dtype=\"float32\")\n        for t in tokens:\n             feature_vec = np.add(feature_vec, model[t])\n        return np.array([np.divide(feature_vec,len(tokens))])\n    ```", "```py\n    doc_vectors = np.empty((0,50), dtype='f')\n    for i in intents:\n        with open(intent_route + i) as f:\n            sentences = f.readlines()\n        sentences = [x.strip() for x in sentences]\n        sentences = pre_processing(sentences)\n        # adding the document vector to the array doc_vectors\n        doc_vectors=np.append(doc_vectors,doc_vector(sentences),axis=0)\n    print(\"Vector representation of each document:\\n\",doc_vectors)\n    ```", "```py\n    from sklearn.metrics.pairwise import cosine_similarity\n    def select_intent(sent_vector, doc_vector):\n        index = -1\n        similarity = -1 #cosine_similarity is in the range of -1 to 1\n        for idx,v in zip(range(len(doc_vector)),doc_vector):\n            v = v.reshape(1,-1)\n            sent_vector = sent_vector.reshape(1,-1)\n            aux = cosine_similarity(sent_vector, v).reshape(1,)\n            if aux[0] > similarity:\n                index = idx\n                similarity = aux\n        return index\n    ```", "```py\n    user_sentence = \"How are you\"\n    user_sentence = pre_processing([user_sentence])\n    user_vector = doc_vector(user_sentence).reshape(50,)\n    intent = intents[select_intent(user_vector, doc_vectors)]\n    intent\n    ```", "```py\n    def send_response(intent_name):\n        with open(response_route + intent_name) as f:\n            sentences = f.readlines()\n        sentences = [x.strip() for x in sentences]\n        return sentences[np.random.randint(low=0, high=len(sentences)-1)]\n    send_response(intent)\n    ```", "```py\n    send_response(intent)\n    ```"]