- en: Building a Feedforward Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Feed-forward propagation from scratch in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building back-propagation from scratch in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a neural network in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A neural network is a supervised learning algorithm that is loosely inspired
    by the way the brain functions. Similar to the way neurons are connected to each
    other in the brain, a neural network takes input, passes it through a function,
    certain subsequent neurons get excited, and consequently the output is produced.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a feedforward neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How forward-propagation works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating loss values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How gradient descent works in back-propagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concepts of epochs and batch size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various loss functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a neural network from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a neural network in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture of a simple neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An artificial neural network is loosely inspired by the way the human brain
    functions. Technically, it is an improvement over linear and logistic regression
    as neural networks introduce multiple non-linear measures in estimating the output.
    Additionally, neural networks provide a great flexibility in modifying the network
    architecture to solve the problems across multiple domains leveraging structured
    and unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: The more complex the function, the greater the chance that the network has to
    tune to the data that is given as input, hence the better the accuracy of the
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical structure of a feed-forward neural network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/838c8087-2c87-495b-9064-13c3d634e009.png)'
  prefs: []
  type: TYPE_IMG
- en: A layer is a collection of one or more nodes (computation units), where each
    node in a layer is connected to every other node in the next immediate layer.
    The input level/layer is constituted of the input variables that are required
    to predict the output values.
  prefs: []
  type: TYPE_NORMAL
- en: The number of nodes in the output layer depends on whether we are trying to
    predict a continuous variable or a categorical variable. If the output is a continuous
    variable, the output has one unit.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the output is categorical with *n* possible classes, there will be *n* nodes
    in the output layer. The hidden level/layer is used to transform the input layer
    values into values in a higher-dimensional space, so that we can learn more features
    from the input. The hidden layer transforms the output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ae13d98-43f6-4c6a-83e8-687365e00a27.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, *x[1]*,*x*[*2*, ]..., *x[n]* are the independent variables,
    and *x[0]* is the bias term (similar to the way we have bias in linear/logistic
    regression).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that *w[1]*,*w[2]*, ..., *w[n]* are the weights given to each of the input
    variables. If *a* is one of the units in the hidden layer, it will be equal to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96089758-dfe2-470f-99c0-79f087b84c7b.png)'
  prefs: []
  type: TYPE_IMG
- en: The *f* function is the activation function that is used to apply non-linearity
    on top of the sum-product of the input and their corresponding weight values.
    Additionally, higher non-linearity can be achieved by having more than one hidden
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In sum, a neural network is a collection of weights assigned to nodes with
    layers connecting them. The collection is organized into three main parts: the
    input layer, the hidden layer, and the output layer. Note that you can have *n* hidden
    layers, with the term deep learning implying multiple hidden layers. Hidden layers
    are necessary when the neural network has to make sense of something really complicated,
    contextual, or not obvious, such as image recognition. The intermediate layers
    (layers that are not input or output) are known as hidden, since they are practically
    not visible (there''s more on how to visualize the intermediate layers in [Chapter
    4](750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml), *Building a Deep Convolutional
    Neural Network*).'
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training a neural network basically means calibrating all of the weights in
    a neural network by repeating two key steps: forward-propagation and back-propagation.'
  prefs: []
  type: TYPE_NORMAL
- en: In forward-propagation, we apply a set of weights to the input data, pass it
    through the hidden layer, perform the nonlinear activation on the hidden layer
    output, and then connect the hidden layer to the output layer by multiplying the
    hidden layer node values with another set of weights. For the first forward-propagation,
    the values of the weights are initialized randomly.
  prefs: []
  type: TYPE_NORMAL
- en: In back-propagation, we try to decrease the error by measuring the margin of
    error of output and then adjust weight accordingly. Neural networks repeat both
    forward- and back-propagation to predict an output until the weights are calibrated.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recently, we have seen a huge adoption of neural networks in a variety of applications.
    In this section, let''s try to understand the reason why adoption might have increased
    considerably. Neural networks can be architected in multiple ways. Here are some
    of the possible ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78c3e0dd-0c85-40a9-ada6-5193ad120167.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The box at the bottom is the input, followed by the hidden layer (the middle
    box), and the box at the top is the output layer. The one-to-one architecture
    is a typical neural network with a hidden layer between the input and output layer.
    Examples of different architectures are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Architecture** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| One-to-many | The input is an image and the output is a caption for the image
    |'
  prefs: []
  type: TYPE_TB
- en: '| Many-to-one | The input is a movie review (multiple words) and the output
    is the sentiment associated with the review |'
  prefs: []
  type: TYPE_TB
- en: '| Many-to-many | Machine translation of a sentence in one language to a sentence
    in another language |'
  prefs: []
  type: TYPE_TB
- en: 'Apart from the preceding points, neural networks are also in a position to
    understand the content in an image and detect the position where the content is
    located using an architecture named **Convolutional Neural Network** (**CNN**),
    which looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ad769ae-ff98-4ed7-bd58-c9284e1c25f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we saw examples of recommender systems, image analysis, text analysis,
    and audio analysis, and we can see that neural networks give us the flexibility
    to solve a problem using multiple architectures, resulting in increased adoption
    as the number of applications increases.
  prefs: []
  type: TYPE_NORMAL
- en: Feed-forward propagation from scratch in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to build a strong foundation of how feed-forward propagation works,
    we'll go through a toy example of training a neural network where the input to
    the neural network is (1, 1) and the corresponding output is 0.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy that we''ll adopt is as follows: our neural network will have
    one hidden layer (with neurons) connecting the input layer to the output layer.
    Note that we have more neurons in the hidden layer than in the input layer, as
    we want to enable the input layer to be represented in more dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ede5ca3a-46a3-4e12-a031-089dd2e8c3da.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Calculating the hidden layer unit values**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now assign weights to all of the connections. Note that these weights are
    selected randomly (based on Gaussian distribution) since it is the first time
    we''re forward-propagating. In this specific case, let''s start with initial weights
    that are between 0 and 1, but note that the final weights after the training process
    of a neural network don''t need to be between a specific set of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12b639fb-0830-4075-b072-57b0d0d97885.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next step, we perform the multiplication of the input with weights to
    calculate the values of hidden units in the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden layer''s unit values are obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/c1c88e58-271f-4993-9e05-96a7f4bec990.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/624d72e6-fad9-46b7-a852-de01b0dedb17.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/3662b621-8504-4f08-ad28-46f825de99cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The hidden layer''s unit values are also shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b453e27-b9e7-4527-94a3-c32a3022c51f.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that in the preceding output we calculated the hidden values. For simplicity,
    we excluded the bias terms that need to be added at each unit of a hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will pass the hidden layer values through an activation function so
    that we attain non-linearity in our output.
  prefs: []
  type: TYPE_NORMAL
- en: If we do not apply the activation function in the hidden layer, the neural network
    becomes a giant linear connection from input to output.
  prefs: []
  type: TYPE_NORMAL
- en: '**Applying the activation function**'
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions are applied at multiple layers of a network. They are used
    so that we achieve high non-linearity in input, which can be useful in modeling
    complex relations between the input and output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The different activation functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12df72e4-af1b-4ce8-bca4-b125b185f18a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For our example, let’s use the sigmoid function for activation. The sigmoid
    function looks like this, graphically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41178e09-6508-41af-bf4d-616d4dbf7674.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By applying sigmoid activation, *S(x)*, to the three hidden=layer *sums*, we
    get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*final_h[1] = S(1.0) = 0.73*'
  prefs: []
  type: TYPE_NORMAL
- en: '*final_h[2] = S(1.3) = 0.78*'
  prefs: []
  type: TYPE_NORMAL
- en: '*final_h[3] = S(0.8) = 0.69*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculating the output layered values**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have calculated the hidden layer values, we will be calculating
    the output layer value. In the following diagram, we have the hidden layer values
    connected to the output through the randomly-initialized weight values. Using
    the hidden layer values and the weight values, we will calculate the output values
    for the following network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdd85755-2ae5-4148-b584-46b254589d77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We perform the sum product of the hidden layer values and weight values to
    calculate the output value. For simplicity, we excluded the bias terms that need
    to be added at each unit of the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0.73 * 0.3 + 0.79 * 0.5 + 0.69 * 0.9 = 1.235*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The values are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5689850e-73e4-4ce8-86b5-3081086f9410.png)'
  prefs: []
  type: TYPE_IMG
- en: Because we started with a random set of weights, the value of the output neuron
    is very different from the target, in this case by +1.235 (since the target is
    0).
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculating the loss values**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Loss values (alternatively called cost functions) are values that we optimize
    in a neural network. In order to understand how loss values get calculated, let''s
    look at two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous variable prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical variable prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculating loss during continuous variable prediction**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, when the variable is a continuous one, the loss value is calculated
    as the squared error, that is, we try to minimize the mean squared error by varying
    the weight values associated with the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/caeef8ef-a6a7-4b83-b3f8-c9961b637f7b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, *y(i)* is the actual value of output, *h(x)* is the
    transformation that we apply on the input (*x*) to obtain a predicted value of
    *y,* and *m* is the number of rows in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculating loss during categorical variable prediction**'
  prefs: []
  type: TYPE_NORMAL
- en: When the variable to predict is a discrete one (that is, there are only a few
    categories in the variable), we typically use a categorical cross-entropy loss
    function. When the variable to predict has two distinct values within it, the
    loss function is binary cross-entropy, and when the variable to predict has multiple
    distinct values within it, the loss function is a categorical cross-entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is binary cross-entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(ylog(p)+(1−y)log(1−p))*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is categorical cross-entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28b794dc-8596-48ba-8f95-23211716611e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*y* is the actual value of output *p,* is the predicted value of the output
     and n is the total number of data points. For now, let''s assume that the outcome
    that we are predicting in our toy example is continuous. In that case, the loss
    function value is the mean squared error, which is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*error = 1.235² = 1.52*'
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, we will try to minimize the loss function value using back-propagation
    (which we'll learn about in the next section), where we update the weight values
    (which were initialized randomly earlier) to minimize the loss (error).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we learned about performing the following steps on
    top of the input data to come up with error values in forward-propagation (the
    code file is available as `Neural_network_working_details.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize weights randomly
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the hidden layer unit values by multiplying input values with weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform activation on the hidden layer values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect the hidden layer values to the output layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the squared error loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A function to calculate the squared error loss values across all data points
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding function, we take the input variable values, weights (randomly
    initialized if this is the first iteration), and the actual output in the provided
    dataset as the input to the feed-forward function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We calculate the hidden layer values by performing the matrix multiplication
    (dot product) of the input and weights. Additionally, we add the bias values in
    the hidden layer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding scenario is valid when `weights[0]` is the weight value and `weights[1]`
    is the bias value, where the weight and bias are connecting the input layer to
    the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we calculate the hidden layer values, we perform activation on top of
    the hidden layer values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We now calculate the output at the hidden layer by multiplying the output of
    the hidden layer with weights that connect the hidden layer to the output, and
    then adding the bias term at the output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the output is calculated, we calculate the squared error loss at each
    row, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `pred_out` is the predicted output and `outputs` is the
    actual output.
  prefs: []
  type: TYPE_NORMAL
- en: We are then in a position to obtain the loss value as we forward-pass through
    the network.
  prefs: []
  type: TYPE_NORMAL
- en: While we considered the sigmoid activation on top of the hidden layer values
    in the preceding code, let's examine other activation functions that are commonly
    used.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tanh**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The tanh activation of a value (the hidden layer unit value) is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**ReLu**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Rectified Linear Unit** (**ReLU**) of a value (the hidden layer unit
    value) is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Linear**'
  prefs: []
  type: TYPE_NORMAL
- en: The linear activation of a value is the value itself.
  prefs: []
  type: TYPE_NORMAL
- en: '**Softmax**'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, softmax is performed on top of a vector of values. This is generally
    done to determine the probability of an input belonging to one of the *n* number
    of the possible output classes in a given scenario. Let's say we are trying to
    classify an image of a digit into one of the possible 10 classes (numbers from
    0 to 9). In this case, there are 10 output values, where each output value should
    represent the probability of an input image belonging to one of the 10 classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The softmax activation is used to provide a probability value for each class
    in the output and is calculated explained in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Apart from the preceding activation functions, the loss functions that are generally
    used while building a neural network are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean squared error**'
  prefs: []
  type: TYPE_NORMAL
- en: The error is the difference between the actual and predicted values of the output.
    We take a square of the error, as the error can be positive or negative (when
    the predicted value is greater than the actual value and vice versa). Squaring
    ensures that positive and negative errors do not offset each other. We calculate
    the mean squared error so that the error over two different datasets is comparable
    when the datasets are not the same size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean squared error between predicted values (`p`) and actual values (`y`)
    is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The mean squared error is typically used when trying to predict a value that
    is continuous in nature.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean absolute error**'
  prefs: []
  type: TYPE_NORMAL
- en: The mean absolute error works in a manner that is very similar to the mean squared
    error. The mean absolute error ensures that positive and negative errors do not
    offset each other by taking an average of the absolute difference between the
    actual and predicted values across all data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean absolute error between the predicted values (`p`) and actual values
    (`y`) is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the mean squared error, the mean absolute error is generally employed
    on continuous variables.
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical cross-entropy**'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy is a measure of the difference between two different distributions: actual
    and predicted. It is applied to categorical output data, unlike the previous two
    loss functions that we discussed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-entropy between two distributions is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37b30e1e-0e2d-47e7-b9d5-cdfab9cbc8a0.png)'
  prefs: []
  type: TYPE_IMG
- en: '*y* is the actual outcome of the event and *p* is the predicted outcome of
    the event.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Categorical cross-entropy between the predicted values (`p`) and actual values
    (`y`) is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that categorical cross-entropy loss has a high value when the predicted
    value is far away from the actual value and a low value when the values are close.
  prefs: []
  type: TYPE_NORMAL
- en: Building back-propagation from scratch in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In forward-propagation, we connected the input layer to the hidden layer to
    the output layer. In back-propagation, we take the reverse approach.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We change each weight within the neural network by a small amount – one at a
    time. A change in the weight value will have an impact on the final loss value
    (either increasing or decreasing loss). We'll update the weight in the direction
    of decreasing loss.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in some scenarios, for a small change in weight, the error increases/decreases
    considerably, while in some cases the error decreases by a small amount.
  prefs: []
  type: TYPE_NORMAL
- en: 'By updating the weights by a small amount and measuring the change in error
    that the update in weights leads to, we are able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine the direction of the weight update
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the magnitude of the weight update
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before implementing back-propagation, let''s understand one additional detail
    of neural networks: the learning rate.'
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, the learning rate helps us to build trust in the algorithm. For
    example, when deciding on the magnitude of the weight update, we would potentially
    not change it by a huge amount in one go, but take a more careful approach in
    updating the weights more slowly.
  prefs: []
  type: TYPE_NORMAL
- en: This results in obtaining stability in our model; we will look at how the learning
    rate helps with stability in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The whole process by which we update weights to reduce error is called a gradient-descent
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent** is the means by which error is minimized in
    the preceding scenario. More intuitively, **gradient** stands for difference (which
    is the difference between actual and predicted) and **descent** means reduce.
    **Stochastic** stands for the selection of number of random samples based on which
    a decision is taken.'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from stochastic gradient descent, there are many other optimization techniques
    that help to optimize for the loss values; the different optimization techniques
    will be discussed in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back-propagation works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculates the overall cost function from the feedforward process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varies all the weights (one at a time) by a small amount.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculates the impact of the variation of weight on the cost function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on whether the change has an increased or decreased the cost (loss)
    value, it updates the weight value in the direction of loss decrease. And then
    repeats this step across all the weights we have.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the preceding steps are performed *n* number of times, it essentially results
    in *n* **epochs**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to further cement our understanding of back-propagation in neural
    networks, let''s start with a known function and see how the weights could be
    derived:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we will have the known function as *y = 2x*, where we try to come
    up with the weight value and bias value, which are 2 and 0 in this specific case:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **x** | **y** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 8 |'
  prefs: []
  type: TYPE_TB
- en: If we formulate the preceding dataset as a linear regression, *(y = a*x+b)*,
    where we are trying to calculate the values of *a* and *b* (which we already know
    are 2 and 0, but are checking how those values are obtained using gradient descent),
    let's randomly initialize the *a* and *b* parameters to values of 1.477 and 0
    (the ideal values of which are 2 and 0).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build the back-propagation algorithm by hand so that
    we clearly understand how weights are calculated in a neural network. In this
    specific case, we will build a simple neural network where there is no hidden
    layer (thus we are solving a regression equation). The code file is available
    as `Neural_network_working_details.ipynb` in GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the dataset as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the weight and bias values randomly (we have only one weight and
    one bias value as we are trying to identify the optimal values of *a* and *b*
    in the *y = a*x + b* equation):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the feed-forward network and calculate the squared error loss value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we performed a matrix multiplication of the input with
    the randomly-initialized weight value and summed it up with the randomly-initialized
    bias value.
  prefs: []
  type: TYPE_NORMAL
- en: Once the value is calculated, we calculate the squared error value of the difference
    between the actual and predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: Increase each weight and bias value by a very small amount (0.0001) and calculate
    the squared error loss value one at a time for each of the weight and bias updates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the squared error loss value decreases as the weight increases, the weight
    value should be increased. The magnitude by which the weight value should be increased
    is proportional to the amount of loss value the weight change decreases by.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, ensure that you do not increase the weight value as much as the
    loss decrease caused by the weight change, but weigh it down with a factor called
    the learning rate. This ensures that the loss decreases more smoothly (there's
    more on how the learning rate impacts the model accuracy in the next chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we are creating a function named `update_weights`, which
    performs the back-propagation process to update weights that were obtained in
    *step 3*. We are also mentioning that the function needs to be run for `epochs`
    number of times (where `epochs` is a parameter we are passing to `update_weights`
    function):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass the input through a feed-forward network to calculate the loss with the
    initial set of weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Ensure that you `deepcopy` the list of weights, as the weights will be manipulated
    in further steps, and hence `deepcopy` takes care of any issues resulting from
    the change in the child variable impacting the parent variable that it is pointing
    to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Loop through all the weight values, one at a time, and change them by a small
    value (0.0001):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the updated feed-forward loss when the weight is updated by a small
    amount. Calculate the change in loss due to the small change in input. Divide
    the change in loss by the number of input, as we want to calculate the mean squared
    error across all the input samples we have:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Updating the weight by a small value and then calculating its impact on loss
    value is equivalent to performing a derivative with respect to change in weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the weights by the change in loss that they are causing. Update the
    weights slowly by multiplying the change in loss by a very small number (0.01),
    which is the learning rate parameter (more about the learning rate parameter in
    the next chapter):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The updated weights and bias value are returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: One of the other parameters in a neural network is the batch size considered
    in calculating the loss values.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding scenario, we considered all the data points in order to calculate
    the loss value. However, in practice, when we have thousands (or in some cases,
    millions) of data points, the incremental contribution of a greater number of
    data points while calculating loss value would follow the law of diminishing returns
    and hence we would be using a batch size that is much smaller compared to the
    total number of data points we have.
  prefs: []
  type: TYPE_NORMAL
- en: The typical batch size considered in building a model is anywhere between 32
    and 1,024.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we built a regression formula *(Y = a*x + b)* where
    we wrote a function to identify the optimal values of *a* and *b*. In this section,
    we will build a simple neural network with a hidden layer that connects the input
    to the output on the same toy dataset that we worked on in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the model as follows (the code file is available as `Neural_networks_multiple_layers.ipynb`
    in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: The input is connected to a hidden layer that has three units
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hidden layer is connected to the output, which has one unit in output layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us go ahead and code up the strategy discussed above, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the dataset and import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We use `deepcopy` so that the value of the original variable does not change
    when the variable to which the original variable's values are copied has its values
    changed.
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the weight and bias values randomly. The hidden layer has three units
    in it. Hence, there are a total of three weight values and three bias values –
    one corresponding to each of the hidden units.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, the final layer has one unit that is connected to the three units
    of the hidden layer. Hence, a total of three weights and one bias dictate the
    value of the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The randomly-initialized weights are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Implement the feed-forward network where the hidden layer has a ReLU activation
    in it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Define the back-propagation function similarly to what we did in the previous
    section. The only difference is that we now have to update the weights in more
    layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code, we are calculating the original loss at the start of
    an epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are copying weights into two sets of weight variables
    so that they can be reused in a later code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the following code, we are updating each weight value by a small amount and
    then calculating the loss value corresponding to the updated weight value (while
    every other weight is kept unchanged). Additionally, we are ensuring that the
    weight update happens across all weights and also across all layers in a network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The change in the squared loss (`del_loss`) is attributed to the change in
    the weight value. We repeat the preceding step for all the weights that exist
    in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The weight value is updated by weighing down by the learning rate parameter –
    a greater decrease in loss will update weights by a lot, while a lower decrease
    in loss will update the weight by a small amount:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Given that the weight values are updated one at a time in order to estimate
    their impact on the loss value, there is a potential to parallelize the process
    of weight updates. Hence, GPUs come in handy in such scenarios as they have more
    cores than a CPU and thus more weights can be updated using a GPU in a given amount
    of time compared to a CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we return the updated weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the function an epoch number of times to update the weights an epoch number
    of times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output (updated weights) of preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64c4ca0f-bfe2-4130-91a2-777979b04a6c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding steps, we learned how to build a neural network from scratch
    in Python. In the next section, we will learn about building a neural network
    in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Building a neural network in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we built a neural network from scratch, that is, we
    wrote functions that perform forward-propagation and back-propagation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be building a neural network using the Keras library, which provides
    utilities that make the process of building a complex neural network much easier.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tensorflow and Keras are implemented in Ubuntu, using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that it is preferable to install a GPU-compatible version, as neural networks
    work considerably faster when they are run on top of a GPU. Keras is a high-level
    neural network API, written in Python, and capable of running on top of TensorFlow, CNTK,
    or Theano.
  prefs: []
  type: TYPE_NORMAL
- en: 'It was developed with a focus on enabling fast experimentation, and it can
    be installed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Building our first model in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, let''s understand the process of building a model in Keras
    by using the same toy dataset that we worked on in the previous sections (the
    code file is available as `Neural_networks_multiple_layers.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate a model that can be called sequentially to add further layers on
    top of it. The `Sequential` method enables us to perform the model initialization
    exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a dense layer to the model. A dense layer ensures the connection between
    various layers in a model. In the following code, we are connecting the input
    layer to the hidden layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the dense layer initialized with the preceding code, we ensured that we provide
    the input shape to the model (we need to specify the shape of data that the model
    has to expect as this is the first dense layer).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we mentioned that there will be three connections made to each
    input (three units in the hidden layer) and also that the activation that needs
    to be performed in the hidden layer is the ReLu activation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect the hidden layer to the output layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note that in this dense layer, we don't need to specify the input shape, as
    the model would already infer the input shape from the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: Also, given that each output is one-dimensional, our output layer has one unit
    and the activation that we are performing is the linear activation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model summary can now be visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc9ae0ff-7a5e-4f4a-9234-fbd8c877225f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding output confirms our discussion in the previous section: that
    there will be a total of six parameters in the connection from the input layer
    to the hidden layer—three weights and three bias terms—we have a total of six
    parameters corresponding to the three hidden units. In addition, three weights
    and one bias term connect the hidden layer to the output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile the model. This ensures that we define the loss function and the optimizer
    to reduce the loss function and the learning rate corresponding to the optimizer
    (we will look at different optimizers and loss functions in next chapter):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding step, we specified that the optimizer is the stochastic gradient
    descent that we learned about in the previous section and the learning rate is
    0.01. Pass the predefined optimizer and its corresponding learning rate as a parameter
    and reduce the mean squared error value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the model. Update the weights so that the model is a better fit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fit` method expects that it receives two NumPy arrays: an input array
    and the corresponding output array. Note that `epochs` represents the number of
    times the total dataset is traversed through, and `batch_size` represents the
    number of data points that need to be considered in an iteration of updating the
    weights. Furthermore, `verbose` specifies that the output is more detailed, with
    information about losses in training and test datasets as well as the progress
    of the model training process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the weight values. The order in which the weight values are presented
    is obtained by calling the weights method on top of the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The order in which weights are obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/faaaee67-07e8-49fa-bd86-10ec8aafd9f8.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding output, we see that the order of weights is the three weights
    (`kernel`) and three bias terms in the `dense_1` layer (which is the connection
    between the input to the hidden layer) and the three weights (`kernel`) and one
    bias term connecting the hidden layer to the `dense_2` layer (the output layer).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the order in which weight values are presented, let''s
    extract the values of these weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the weights are presented as a list of arrays, where each array
    corresponds to the value that is specified in the `model.weights` output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of above lines of code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a280532-11b9-456d-b37c-311c31731327.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You should notice that the output we are observing here matches with the output
    we obtaining while hand-building the neural network
  prefs: []
  type: TYPE_NORMAL
- en: 'Predict the output for a new set of input using the `predict` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note that `x1` is the variable that holds the values for the new set of examples
    for which we need to predict the value of the output. Similarly to the `fit` method,
    the `predict` method also expects an array as its input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c287bfb-ee6c-4bf3-a301-f201cb71cbb2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that, while the preceding output is incorrect, the output when we run
    for 100 epochs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39ac7491-ac09-432f-af83-1e6a4d3dec01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding output will match the expected output (which are 10, 12) as we
    run for even higher number of epochs.
  prefs: []
  type: TYPE_NORMAL
