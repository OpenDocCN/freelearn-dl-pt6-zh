- en: Building a Feedforward Neural Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Feed-forward propagation from scratch in Python
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building back-propagation from scratch in Python
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a neural network in Keras
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A neural network is a supervised learning algorithm that is loosely inspired
    by the way the brain functions. Similar to the way neurons are connected to each
    other in the brain, a neural network takes input, passes it through a function,
    certain subsequent neurons get excited, and consequently the output is produced.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of a neural network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of a neural network
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a feedforward neural network
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How forward-propagation works
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating loss values
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How gradient descent works in back-propagation
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concepts of epochs and batch size
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various loss functions
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various activation functions
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a neural network from scratch
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a neural network in Keras
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture of a simple neural network
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An artificial neural network is loosely inspired by the way the human brain
    functions. Technically, it is an improvement over linear and logistic regression
    as neural networks introduce multiple non-linear measures in estimating the output.
    Additionally, neural networks provide a great flexibility in modifying the network
    architecture to solve the problems across multiple domains leveraging structured
    and unstructured data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The more complex the function, the greater the chance that the network has to
    tune to the data that is given as input, hence the better the accuracy of the
    predictions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical structure of a feed-forward neural network is as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/838c8087-2c87-495b-9064-13c3d634e009.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: A layer is a collection of one or more nodes (computation units), where each
    node in a layer is connected to every other node in the next immediate layer.
    The input level/layer is constituted of the input variables that are required
    to predict the output values.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: The number of nodes in the output layer depends on whether we are trying to
    predict a continuous variable or a categorical variable. If the output is a continuous
    variable, the output has one unit.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'If the output is categorical with *n* possible classes, there will be *n* nodes
    in the output layer. The hidden level/layer is used to transform the input layer
    values into values in a higher-dimensional space, so that we can learn more features
    from the input. The hidden layer transforms the output as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ae13d98-43f6-4c6a-83e8-687365e00a27.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, *x[1]*,*x*[*2*, ]..., *x[n]* are the independent variables,
    and *x[0]* is the bias term (similar to the way we have bias in linear/logistic
    regression).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that *w[1]*,*w[2]*, ..., *w[n]* are the weights given to each of the input
    variables. If *a* is one of the units in the hidden layer, it will be equal to
    the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96089758-dfe2-470f-99c0-79f087b84c7b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: The *f* function is the activation function that is used to apply non-linearity
    on top of the sum-product of the input and their corresponding weight values.
    Additionally, higher non-linearity can be achieved by having more than one hidden
    layer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'In sum, a neural network is a collection of weights assigned to nodes with
    layers connecting them. The collection is organized into three main parts: the
    input layer, the hidden layer, and the output layer. Note that you can have *n* hidden
    layers, with the term deep learning implying multiple hidden layers. Hidden layers
    are necessary when the neural network has to make sense of something really complicated,
    contextual, or not obvious, such as image recognition. The intermediate layers
    (layers that are not input or output) are known as hidden, since they are practically
    not visible (there''s more on how to visualize the intermediate layers in [Chapter
    4](750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml), *Building a Deep Convolutional
    Neural Network*).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training a neural network basically means calibrating all of the weights in
    a neural network by repeating two key steps: forward-propagation and back-propagation.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: In forward-propagation, we apply a set of weights to the input data, pass it
    through the hidden layer, perform the nonlinear activation on the hidden layer
    output, and then connect the hidden layer to the output layer by multiplying the
    hidden layer node values with another set of weights. For the first forward-propagation,
    the values of the weights are initialized randomly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: In back-propagation, we try to decrease the error by measuring the margin of
    error of output and then adjust weight accordingly. Neural networks repeat both
    forward- and back-propagation to predict an output until the weights are calibrated.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Applications of a neural network
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recently, we have seen a huge adoption of neural networks in a variety of applications.
    In this section, let''s try to understand the reason why adoption might have increased
    considerably. Neural networks can be architected in multiple ways. Here are some
    of the possible ways:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78c3e0dd-0c85-40a9-ada6-5193ad120167.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: 'The box at the bottom is the input, followed by the hidden layer (the middle
    box), and the box at the top is the output layer. The one-to-one architecture
    is a typical neural network with a hidden layer between the input and output layer.
    Examples of different architectures are as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '| **Architecture** | **Example** |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| One-to-many | The input is an image and the output is a caption for the image
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| Many-to-one | The input is a movie review (multiple words) and the output
    is the sentiment associated with the review |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| Many-to-many | Machine translation of a sentence in one language to a sentence
    in another language |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: 'Apart from the preceding points, neural networks are also in a position to
    understand the content in an image and detect the position where the content is
    located using an architecture named **Convolutional Neural Network** (**CNN**),
    which looks as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ad769ae-ff98-4ed7-bd58-c9284e1c25f6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Here, we saw examples of recommender systems, image analysis, text analysis,
    and audio analysis, and we can see that neural networks give us the flexibility
    to solve a problem using multiple architectures, resulting in increased adoption
    as the number of applications increases.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Feed-forward propagation from scratch in Python
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to build a strong foundation of how feed-forward propagation works,
    we'll go through a toy example of training a neural network where the input to
    the neural network is (1, 1) and the corresponding output is 0.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy that we''ll adopt is as follows: our neural network will have
    one hidden layer (with neurons) connecting the input layer to the output layer.
    Note that we have more neurons in the hidden layer than in the input layer, as
    we want to enable the input layer to be represented in more dimensions:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ede5ca3a-46a3-4e12-a031-089dd2e8c3da.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: '**Calculating the hidden layer unit values**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'We now assign weights to all of the connections. Note that these weights are
    selected randomly (based on Gaussian distribution) since it is the first time
    we''re forward-propagating. In this specific case, let''s start with initial weights
    that are between 0 and 1, but note that the final weights after the training process
    of a neural network don''t need to be between a specific set of values:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12b639fb-0830-4075-b072-57b0d0d97885.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: In the next step, we perform the multiplication of the input with weights to
    calculate the values of hidden units in the hidden layer.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden layer''s unit values are obtained as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/c1c88e58-271f-4993-9e05-96a7f4bec990.png)*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/624d72e6-fad9-46b7-a852-de01b0dedb17.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: '![](img/3662b621-8504-4f08-ad28-46f825de99cc.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: 'The hidden layer''s unit values are also shown in the following diagram:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b453e27-b9e7-4527-94a3-c32a3022c51f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Note that in the preceding output we calculated the hidden values. For simplicity,
    we excluded the bias terms that need to be added at each unit of a hidden layer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will pass the hidden layer values through an activation function so
    that we attain non-linearity in our output.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: If we do not apply the activation function in the hidden layer, the neural network
    becomes a giant linear connection from input to output.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '**Applying the activation function**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions are applied at multiple layers of a network. They are used
    so that we achieve high non-linearity in input, which can be useful in modeling
    complex relations between the input and output.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'The different activation functions are as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12df72e4-af1b-4ce8-bca4-b125b185f18a.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: 'For our example, let’s use the sigmoid function for activation. The sigmoid
    function looks like this, graphically:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41178e09-6508-41af-bf4d-616d4dbf7674.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'By applying sigmoid activation, *S(x)*, to the three hidden=layer *sums*, we
    get the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '*final_h[1] = S(1.0) = 0.73*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '*final_h[2] = S(1.3) = 0.78*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '*final_h[3] = S(0.8) = 0.69*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculating the output layered values**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have calculated the hidden layer values, we will be calculating
    the output layer value. In the following diagram, we have the hidden layer values
    connected to the output through the randomly-initialized weight values. Using
    the hidden layer values and the weight values, we will calculate the output values
    for the following network:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdd85755-2ae5-4148-b584-46b254589d77.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: 'We perform the sum product of the hidden layer values and weight values to
    calculate the output value. For simplicity, we excluded the bias terms that need
    to be added at each unit of the hidden layer:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '*0.73 * 0.3 + 0.79 * 0.5 + 0.69 * 0.9 = 1.235*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'The values are shown in the following diagram:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5689850e-73e4-4ce8-86b5-3081086f9410.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: Because we started with a random set of weights, the value of the output neuron
    is very different from the target, in this case by +1.235 (since the target is
    0).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculating the loss values**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Loss values (alternatively called cost functions) are values that we optimize
    in a neural network. In order to understand how loss values get calculated, let''s
    look at two scenarios:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Continuous variable prediction
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical variable prediction
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculating loss during continuous variable prediction**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, when the variable is a continuous one, the loss value is calculated
    as the squared error, that is, we try to minimize the mean squared error by varying
    the weight values associated with the neural network:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/caeef8ef-a6a7-4b83-b3f8-c9961b637f7b.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, *y(i)* is the actual value of output, *h(x)* is the
    transformation that we apply on the input (*x*) to obtain a predicted value of
    *y,* and *m* is the number of rows in the dataset.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculating loss during categorical variable prediction**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: When the variable to predict is a discrete one (that is, there are only a few
    categories in the variable), we typically use a categorical cross-entropy loss
    function. When the variable to predict has two distinct values within it, the
    loss function is binary cross-entropy, and when the variable to predict has multiple
    distinct values within it, the loss function is a categorical cross-entropy.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is binary cross-entropy:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '*(ylog(p)+(1−y)log(1−p))*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is categorical cross-entropy:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28b794dc-8596-48ba-8f95-23211716611e.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: '*y* is the actual value of output *p,* is the predicted value of the output
     and n is the total number of data points. For now, let''s assume that the outcome
    that we are predicting in our toy example is continuous. In that case, the loss
    function value is the mean squared error, which is calculated as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* 是输出的实际值，*p* 是预测的输出值，n 是数据点的总数。现在，假设我们在玩具示例中预测的结果是连续的。在这种情况下，损失函数值是均方误差，计算公式如下：'
- en: '*error = 1.235² = 1.52*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*error = 1.235² = 1.52*'
- en: In the next step, we will try to minimize the loss function value using back-propagation
    (which we'll learn about in the next section), where we update the weight values
    (which were initialized randomly earlier) to minimize the loss (error).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将尝试使用反向传播来最小化损失函数值（我们将在下一节学习），在反向传播中，我们更新权重值（之前随机初始化的）以最小化损失（误差）。
- en: How to do it...
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行...
- en: 'In the previous section, we learned about performing the following steps on
    top of the input data to come up with error values in forward-propagation (the
    code file is available as `Neural_network_working_details.ipynb` in GitHub):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们学习了如何对输入数据执行以下步骤，以便在前向传播过程中计算误差值（代码文件可在 GitHub 上的 `Neural_network_working_details.ipynb`
    找到）：
- en: Initialize weights randomly
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化权重
- en: Calculate the hidden layer unit values by multiplying input values with weights
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将输入值与权重相乘来计算隐藏层单元值
- en: Perform activation on the hidden layer values
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对隐藏层的值执行激活
- en: Connect the hidden layer values to the output layer
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将隐藏层的值连接到输出层
- en: Calculate the squared error loss
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算平方误差损失
- en: 'A function to calculate the squared error loss values across all data points
    is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 用于计算所有数据点的平方误差损失值的函数如下：
- en: '[PRE0]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding function, we take the input variable values, weights (randomly
    initialized if this is the first iteration), and the actual output in the provided
    dataset as the input to the feed-forward function.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数中，我们将输入变量值、权重（如果这是第一次迭代则随机初始化）以及提供的数据集中的实际输出作为输入传递给前馈函数。
- en: 'We calculate the hidden layer values by performing the matrix multiplication
    (dot product) of the input and weights. Additionally, we add the bias values in
    the hidden layer, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过执行输入和权重的矩阵乘法（点积）来计算隐藏层的值。此外，我们还会在隐藏层中加上偏置值，如下所示：
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding scenario is valid when `weights[0]` is the weight value and `weights[1]`
    is the bias value, where the weight and bias are connecting the input layer to
    the hidden layer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前述场景适用于`weights[0]`为权重值，`weights[1]`为偏置值，权重和偏置连接输入层与隐藏层。
- en: 'Once we calculate the hidden layer values, we perform activation on top of
    the hidden layer values, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出隐藏层的值，我们会对隐藏层的值进行激活，计算方法如下：
- en: '[PRE2]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now calculate the output at the hidden layer by multiplying the output of
    the hidden layer with weights that connect the hidden layer to the output, and
    then adding the bias term at the output, as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们通过将隐藏层的输出与连接隐藏层和输出的权重相乘，然后在输出处添加偏差项，来计算隐藏层的输出，如下所示：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the output is calculated, we calculate the squared error loss at each
    row, as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦输出被计算出来，我们会在每一行中计算平方误差损失，计算公式如下：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding code, `pred_out` is the predicted output and `outputs` is the
    actual output.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`pred_out` 是预测的输出，`outputs` 是实际的输出。
- en: We are then in a position to obtain the loss value as we forward-pass through
    the network.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以在通过网络进行前向传播时获得损失值。
- en: While we considered the sigmoid activation on top of the hidden layer values
    in the preceding code, let's examine other activation functions that are commonly
    used.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在前面的代码中，我们对隐藏层的值考虑了 Sigmoid 激活函数，但现在让我们看看其他常用的激活函数。
- en: '**Tanh**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tanh**'
- en: 'The tanh activation of a value (the hidden layer unit value) is calculated
    as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: tanh 激活值（隐藏层单元值）的计算如下：
- en: '[PRE5]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**ReLu**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLu**'
- en: 'The **Rectified Linear Unit** (**ReLU**) of a value (the hidden layer unit
    value) is calculated as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**修正线性单元**（**ReLU**）的值（隐藏层单元值）计算如下：'
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Linear**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**Linear**'
- en: The linear activation of a value is the value itself.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 线性激活值就是该值本身。
- en: '**Softmax**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**Softmax**'
- en: Typically, softmax is performed on top of a vector of values. This is generally
    done to determine the probability of an input belonging to one of the *n* number
    of the possible output classes in a given scenario. Let's say we are trying to
    classify an image of a digit into one of the possible 10 classes (numbers from
    0 to 9). In this case, there are 10 output values, where each output value should
    represent the probability of an input image belonging to one of the 10 classes.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'The softmax activation is used to provide a probability value for each class
    in the output and is calculated explained in the following sections:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Apart from the preceding activation functions, the loss functions that are generally
    used while building a neural network are as follows.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean squared error**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: The error is the difference between the actual and predicted values of the output.
    We take a square of the error, as the error can be positive or negative (when
    the predicted value is greater than the actual value and vice versa). Squaring
    ensures that positive and negative errors do not offset each other. We calculate
    the mean squared error so that the error over two different datasets is comparable
    when the datasets are not the same size.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean squared error between predicted values (`p`) and actual values (`y`)
    is calculated as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The mean squared error is typically used when trying to predict a value that
    is continuous in nature.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean absolute error**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: The mean absolute error works in a manner that is very similar to the mean squared
    error. The mean absolute error ensures that positive and negative errors do not
    offset each other by taking an average of the absolute difference between the
    actual and predicted values across all data points.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean absolute error between the predicted values (`p`) and actual values
    (`y`) is implemented as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Similar to the mean squared error, the mean absolute error is generally employed
    on continuous variables.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical cross-entropy**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy is a measure of the difference between two different distributions: actual
    and predicted. It is applied to categorical output data, unlike the previous two
    loss functions that we discussed.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-entropy between two distributions is calculated as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37b30e1e-0e2d-47e7-b9d5-cdfab9cbc8a0.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: '*y* is the actual outcome of the event and *p* is the predicted outcome of
    the event.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Categorical cross-entropy between the predicted values (`p`) and actual values
    (`y`) is implemented as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that categorical cross-entropy loss has a high value when the predicted
    value is far away from the actual value and a low value when the values are close.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Building back-propagation from scratch in Python
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In forward-propagation, we connected the input layer to the hidden layer to
    the output layer. In back-propagation, we take the reverse approach.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We change each weight within the neural network by a small amount – one at a
    time. A change in the weight value will have an impact on the final loss value
    (either increasing or decreasing loss). We'll update the weight in the direction
    of decreasing loss.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in some scenarios, for a small change in weight, the error increases/decreases
    considerably, while in some cases the error decreases by a small amount.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'By updating the weights by a small amount and measuring the change in error
    that the update in weights leads to, we are able to do the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Determine the direction of the weight update
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the magnitude of the weight update
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before implementing back-propagation, let''s understand one additional detail
    of neural networks: the learning rate.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, the learning rate helps us to build trust in the algorithm. For
    example, when deciding on the magnitude of the weight update, we would potentially
    not change it by a huge amount in one go, but take a more careful approach in
    updating the weights more slowly.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: This results in obtaining stability in our model; we will look at how the learning
    rate helps with stability in the next chapter.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The whole process by which we update weights to reduce error is called a gradient-descent
    technique.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent** is the means by which error is minimized in
    the preceding scenario. More intuitively, **gradient** stands for difference (which
    is the difference between actual and predicted) and **descent** means reduce.
    **Stochastic** stands for the selection of number of random samples based on which
    a decision is taken.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Apart from stochastic gradient descent, there are many other optimization techniques
    that help to optimize for the loss values; the different optimization techniques
    will be discussed in the next chapter.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Back-propagation works as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Calculates the overall cost function from the feedforward process.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varies all the weights (one at a time) by a small amount.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculates the impact of the variation of weight on the cost function.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on whether the change has an increased or decreased the cost (loss)
    value, it updates the weight value in the direction of loss decrease. And then
    repeats this step across all the weights we have.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the preceding steps are performed *n* number of times, it essentially results
    in *n* **epochs**.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to further cement our understanding of back-propagation in neural
    networks, let''s start with a known function and see how the weights could be
    derived:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we will have the known function as *y = 2x*, where we try to come
    up with the weight value and bias value, which are 2 and 0 in this specific case:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '| **x** | **y** |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| 2 | 4 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| 3 | 6 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| 4 | 8 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: If we formulate the preceding dataset as a linear regression, *(y = a*x+b)*,
    where we are trying to calculate the values of *a* and *b* (which we already know
    are 2 and 0, but are checking how those values are obtained using gradient descent),
    let's randomly initialize the *a* and *b* parameters to values of 1.477 and 0
    (the ideal values of which are 2 and 0).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build the back-propagation algorithm by hand so that
    we clearly understand how weights are calculated in a neural network. In this
    specific case, we will build a simple neural network where there is no hidden
    layer (thus we are solving a regression equation). The code file is available
    as `Neural_network_working_details.ipynb` in GitHub.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the dataset as follows:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Initialize the weight and bias values randomly (we have only one weight and
    one bias value as we are trying to identify the optimal values of *a* and *b*
    in the *y = a*x + b* equation):'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define the feed-forward network and calculate the squared error loss value:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding code, we performed a matrix multiplication of the input with
    the randomly-initialized weight value and summed it up with the randomly-initialized
    bias value.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Once the value is calculated, we calculate the squared error value of the difference
    between the actual and predicted values.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Increase each weight and bias value by a very small amount (0.0001) and calculate
    the squared error loss value one at a time for each of the weight and bias updates.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the squared error loss value decreases as the weight increases, the weight
    value should be increased. The magnitude by which the weight value should be increased
    is proportional to the amount of loss value the weight change decreases by.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, ensure that you do not increase the weight value as much as the
    loss decrease caused by the weight change, but weigh it down with a factor called
    the learning rate. This ensures that the loss decreases more smoothly (there's
    more on how the learning rate impacts the model accuracy in the next chapter).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we are creating a function named `update_weights`, which
    performs the back-propagation process to update weights that were obtained in
    *step 3*. We are also mentioning that the function needs to be run for `epochs`
    number of times (where `epochs` is a parameter we are passing to `update_weights`
    function):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Pass the input through a feed-forward network to calculate the loss with the
    initial set of weights:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Ensure that you `deepcopy` the list of weights, as the weights will be manipulated
    in further steps, and hence `deepcopy` takes care of any issues resulting from
    the change in the child variable impacting the parent variable that it is pointing
    to:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Loop through all the weight values, one at a time, and change them by a small
    value (0.0001):'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Calculate the updated feed-forward loss when the weight is updated by a small
    amount. Calculate the change in loss due to the small change in input. Divide
    the change in loss by the number of input, as we want to calculate the mean squared
    error across all the input samples we have:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Updating the weight by a small value and then calculating its impact on loss
    value is equivalent to performing a derivative with respect to change in weight.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the weights by the change in loss that they are causing. Update the
    weights slowly by multiplying the change in loss by a very small number (0.01),
    which is the learning rate parameter (more about the learning rate parameter in
    the next chapter):'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The updated weights and bias value are returned:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: One of the other parameters in a neural network is the batch size considered
    in calculating the loss values.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding scenario, we considered all the data points in order to calculate
    the loss value. However, in practice, when we have thousands (or in some cases,
    millions) of data points, the incremental contribution of a greater number of
    data points while calculating loss value would follow the law of diminishing returns
    and hence we would be using a batch size that is much smaller compared to the
    total number of data points we have.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The typical batch size considered in building a model is anywhere between 32
    and 1,024.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we built a regression formula *(Y = a*x + b)* where
    we wrote a function to identify the optimal values of *a* and *b*. In this section,
    we will build a simple neural network with a hidden layer that connects the input
    to the output on the same toy dataset that we worked on in the previous section.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the model as follows (the code file is available as `Neural_networks_multiple_layers.ipynb`
    in GitHub):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: The input is connected to a hidden layer that has three units
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hidden layer is connected to the output, which has one unit in output layer
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us go ahead and code up the strategy discussed above, as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the dataset and import the relevant packages:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We use `deepcopy` so that the value of the original variable does not change
    when the variable to which the original variable's values are copied has its values
    changed.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the weight and bias values randomly. The hidden layer has three units
    in it. Hence, there are a total of three weight values and three bias values –
    one corresponding to each of the hidden units.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, the final layer has one unit that is connected to the three units
    of the hidden layer. Hence, a total of three weights and one bias dictate the
    value of the output layer.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'The randomly-initialized weights are as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Implement the feed-forward network where the hidden layer has a ReLU activation
    in it:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Define the back-propagation function similarly to what we did in the previous
    section. The only difference is that we now have to update the weights in more
    layers.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code, we are calculating the original loss at the start of
    an epoch:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the following code, we are copying weights into two sets of weight variables
    so that they can be reused in a later code:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the following code, we are updating each weight value by a small amount and
    then calculating the loss value corresponding to the updated weight value (while
    every other weight is kept unchanged). Additionally, we are ensuring that the
    weight update happens across all weights and also across all layers in a network.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'The change in the squared loss (`del_loss`) is attributed to the change in
    the weight value. We repeat the preceding step for all the weights that exist
    in the network:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The weight value is updated by weighing down by the learning rate parameter –
    a greater decrease in loss will update weights by a lot, while a lower decrease
    in loss will update the weight by a small amount:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Given that the weight values are updated one at a time in order to estimate
    their impact on the loss value, there is a potential to parallelize the process
    of weight updates. Hence, GPUs come in handy in such scenarios as they have more
    cores than a CPU and thus more weights can be updated using a GPU in a given amount
    of time compared to a CPU.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we return the updated weights:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Run the function an epoch number of times to update the weights an epoch number
    of times:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output (updated weights) of preceding code is as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64c4ca0f-bfe2-4130-91a2-777979b04a6c.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: In the preceding steps, we learned how to build a neural network from scratch
    in Python. In the next section, we will learn about building a neural network
    in Keras.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Building a neural network in Keras
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we built a neural network from scratch, that is, we
    wrote functions that perform forward-propagation and back-propagation.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be building a neural network using the Keras library, which provides
    utilities that make the process of building a complex neural network much easier.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Installing Keras
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tensorflow and Keras are implemented in Ubuntu, using the following commands:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that it is preferable to install a GPU-compatible version, as neural networks
    work considerably faster when they are run on top of a GPU. Keras is a high-level
    neural network API, written in Python, and capable of running on top of TensorFlow, CNTK,
    or Theano.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'It was developed with a focus on enabling fast experimentation, and it can
    be installed as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Building our first model in Keras
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, let''s understand the process of building a model in Keras
    by using the same toy dataset that we worked on in the previous sections (the
    code file is available as `Neural_networks_multiple_layers.ipynb` in GitHub):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate a model that can be called sequentially to add further layers on
    top of it. The `Sequential` method enables us to perform the model initialization
    exercise:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Add a dense layer to the model. A dense layer ensures the connection between
    various layers in a model. In the following code, we are connecting the input
    layer to the hidden layer:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the dense layer initialized with the preceding code, we ensured that we provide
    the input shape to the model (we need to specify the shape of data that the model
    has to expect as this is the first dense layer).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we mentioned that there will be three connections made to each
    input (three units in the hidden layer) and also that the activation that needs
    to be performed in the hidden layer is the ReLu activation.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect the hidden layer to the output layer:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that in this dense layer, we don't need to specify the input shape, as
    the model would already infer the input shape from the previous layer.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Also, given that each output is one-dimensional, our output layer has one unit
    and the activation that we are performing is the linear activation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'The model summary can now be visualized as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'A summary of model is as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc9ae0ff-7a5e-4f4a-9234-fbd8c877225f.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'The preceding output confirms our discussion in the previous section: that
    there will be a total of six parameters in the connection from the input layer
    to the hidden layer—three weights and three bias terms—we have a total of six
    parameters corresponding to the three hidden units. In addition, three weights
    and one bias term connect the hidden layer to the output layer.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile the model. This ensures that we define the loss function and the optimizer
    to reduce the loss function and the learning rate corresponding to the optimizer
    (we will look at different optimizers and loss functions in next chapter):'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In the preceding step, we specified that the optimizer is the stochastic gradient
    descent that we learned about in the previous section and the learning rate is
    0.01. Pass the predefined optimizer and its corresponding learning rate as a parameter
    and reduce the mean squared error value:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Fit the model. Update the weights so that the model is a better fit:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `fit` method expects that it receives two NumPy arrays: an input array
    and the corresponding output array. Note that `epochs` represents the number of
    times the total dataset is traversed through, and `batch_size` represents the
    number of data points that need to be considered in an iteration of updating the
    weights. Furthermore, `verbose` specifies that the output is more detailed, with
    information about losses in training and test datasets as well as the progress
    of the model training process.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the weight values. The order in which the weight values are presented
    is obtained by calling the weights method on top of the model, as follows:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The order in which weights are obtained is as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/faaaee67-07e8-49fa-bd86-10ec8aafd9f8.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: From the preceding output, we see that the order of weights is the three weights
    (`kernel`) and three bias terms in the `dense_1` layer (which is the connection
    between the input to the hidden layer) and the three weights (`kernel`) and one
    bias term connecting the hidden layer to the `dense_2` layer (the output layer).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand the order in which weight values are presented, let''s
    extract the values of these weights:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Notice that the weights are presented as a list of arrays, where each array
    corresponds to the value that is specified in the `model.weights` output.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of above lines of code is as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a280532-11b9-456d-b37c-311c31731327.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: You should notice that the output we are observing here matches with the output
    we obtaining while hand-building the neural network
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Predict the output for a new set of input using the `predict` method:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that `x1` is the variable that holds the values for the new set of examples
    for which we need to predict the value of the output. Similarly to the `fit` method,
    the `predict` method also expects an array as its input.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of preceding code is as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c287bfb-ee6c-4bf3-a301-f201cb71cbb2.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: 'Notice that, while the preceding output is incorrect, the output when we run
    for 100 epochs is as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39ac7491-ac09-432f-af83-1e6a4d3dec01.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: The preceding output will match the expected output (which are 10, 12) as we
    run for even higher number of epochs.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
