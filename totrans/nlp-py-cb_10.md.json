["```py\n>>> from __future__ import print_function\n>>> import numpy as np\n>>> import random\n>>> import sys\n```", "```py\n>>> path = 'C:\\\\Users\\\\prata\\\\Documents\\\\book_codes\\\\ NLP_DL\\\\ shakespeare_final.txt'\n>>&gt; text = open(path).read().lower()\n>>> characters = sorted(list(set(text)))\n>>> print('corpus length:', len(text))\n>>> print('total chars:', len(characters))\n```", "```py\n>>> char2indices = dict((c, i) for i, c in enumerate(characters))\n>>> indices2char = dict((i, c) for i, c in enumerate(characters))\n```", "```py\n# cut the text in semi-redundant sequences of maxlen characters\n>>> maxlen = 40\n>>> step = 3\n>>> sentences = []\n>>> next_chars = []\n>>> for i in range(0, len(text) - maxlen, step):\n... sentences.append(text[i: i + maxlen])\n... next_chars.append(text[i + maxlen])\n... print('nb sequences:', len(sentences))\n```", "```py\n# Converting indices into vectorized format\n>>> X = np.zeros((len(sentences), maxlen, len(characters)), dtype=np.bool)\n>>> y = np.zeros((len(sentences), len(characters)), dtype=np.bool)\n>>> for i, sentence in enumerate(sentences):\n... for t, char in enumerate(sentence):\n... X[i, t, char2indices[char]] = 1\n... y[i, char2indices[next_chars[i]]] = 1\n>>> from keras.models import Sequential\n>>> from keras.layers import Dense, LSTM,Activation,Dropout\n>>> from keras.optimizers import RMSprop\n```", "```py\n#Model Building\n>>> model = Sequential()\n>>> model.add(LSTM(128, input_shape=(maxlen, len(characters))))\n>>> model.add(Dense(len(characters)))\n>>> model.add(Activation('softmax'))\n>>> model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n>>> print (model.summary())\n```", "```py\n# Function to convert prediction into index\n>>> def pred_indices(preds, metric=1.0):\n... preds = np.asarray(preds).astype('float64')\n... preds = np.log(preds) / metric\n... exp_preds = np.exp(preds)\n... preds = exp_preds/np.sum(exp_preds)\n... probs = np.random.multinomial(1, preds, 1)\n... return np.argmax(probs)\n```", "```py\n# Train and Evaluate the Model\n>>> for iteration in range(1, 30):\n... print('-' * 40)\n... print('Iteration', iteration)\n... model.fit(X, y,batch_size=128,epochs=1)\n... start_index = random.randint(0, len(text) - maxlen - 1)\n... for diversity in [0.2, 0.7,1.2]:\n... print('\\n----- diversity:', diversity)\n... generated = ''\n... sentence = text[start_index: start_index + maxlen]\n... generated += sentence\n... print('----- Generating with seed: \"' + sentence + '\"')\n... sys.stdout.write(generated)\n... for i in range(400):\n... x = np.zeros((1, maxlen, len(characters)))\n... for t, char in enumerate(sentence):\n... x[0, t, char2indices[char]] = 1.\n... preds = model.predict(x, verbose=0)[0]\n... next_index = pred_indices(preds, diversity)\n... pred_char = indices2char[next_index]\n... generated += pred_char\n... sentence = sentence[1:] + pred_char\n... sys.stdout.write(pred_char)\n... sys.stdout.flush()\n... print(\"\\nOne combination completed \\n\")\n```", "```py\n>>> from __future__ import division, print_function\n>>> import collections\n>>> import itertools\n>>> import nltk\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> import os\n>>> import random\n>>> def get_data(infile):\n... stories, questions, answers = [], [], []\n... story_text = []\n... fin = open(Train_File, \"rb\")\n... for line in fin:\n... line = line.decode(\"utf-8\").strip()\n... lno, text = line.split(\" \", 1)\n... if \"\\t\" in text:\n... question, answer, _ = text.split(\"\\t\")\n... stories.append(story_text)\n... questions.append(question)\n... answers.append(answer)\n... story_text = []\n... else:\n... story_text.append(text)\n... fin.close()\n... return stories, questions, answers\n>>> file_location = \"C:/Users/prata/Documents/book_codes/NLP_DL\"\n>>> Train_File = os.path.join(file_location, \"qa1_single-supporting-fact_train.txt\")\n>>> Test_File = os.path.join(file_location, \"qa1_single-supporting-fact_test.txt\")\n# get the data\n>>> data_train = get_data(Train_File)\n>>> data_test = get_data(Test_File)\n>>> print(\"\\n\\nTrain observations:\",len(data_train[0]),\"Test observations:\", len(data_test[0]),\"\\n\\n\")\n```", "```py\n# Building Vocab dictionary from Train and Test data\n>>> dictnry = collections.Counter()\n>>> for stories,questions,answers in [data_train,data_test]:\n... for story in stories:\n... for sent in story:\n... for word in nltk.word_tokenize(sent):\n... dictnry[word.lower()] +=1\n... for question in questions:\n... for word in nltk.word_tokenize(question):\n... dictnry[word.lower()]+=1\n... for answer in answers:\n... for word in nltk.word_tokenize(answer):\n... dictnry[word.lower()]+=1\n>>> word2indx = {w:(i+1) for i,(w,_) in enumerate(dictnry.most_common() )}\n>>> word2indx[\"PAD\"] = 0\n>>> indx2word = {v:k for k,v in word2indx.items()}\n>>> vocab_size = len(word2indx)\n>>> print(\"vocabulary size:\",len(word2indx))\n```", "```py\n# compute max sequence length for each entity\n>>> story_maxlen = 0\n>>> question_maxlen = 0\n>>> for stories, questions, answers in [data_train,data_test]:\n... for story in stories:\n... story_len = 0\n... for sent in story:\n... swords = nltk.word_tokenize(sent)\n... story_len += len(swords)\n... if story_len > story_maxlen:\n... story_maxlen = story_len\n... for question in questions:\n... question_len = len(nltk.word_tokenize(question))\n... if question_len > question_maxlen:\n... question_maxlen = question_len>>> print (\"Story maximum length:\",story_maxlen,\"Question maximum length:\",question_maxlen)\n```", "```py\n>>> from keras.layers import Input\n>>> from keras.layers.core import Activation, Dense, Dropout, Permute\n>>> from keras.layers.embeddings import Embedding\n>>> from keras.layers.merge import add, concatenate, dot\n>>> from keras.layers.recurrent import LSTM\n>>> from keras.models import Model\n>>> from keras.preprocessing.sequence import pad_sequences\n>>> from keras.utils import np_utils\n```", "```py\n# Converting data into Vectorized form\n>>> def data_vectorization(data, word2indx, story_maxlen, question_maxlen):\n... Xs, Xq, Y = [], [], []\n... stories, questions, answers = data\n... for story, question, answer in zip(stories, questions, answers):\n... xs = [[word2indx[w.lower()] for w in nltk.word_tokenize(s)]\nfor s in story]\n... xs = list(itertools.chain.from_iterable(xs))\n... xq = [word2indx[w.lower()] for w in nltk.word_tokenize (question)]\n... Xs.append(xs)\n... Xq.append(xq)\n... Y.append(word2indx[answer.lower()])\n... return pad_sequences(Xs, maxlen=story_maxlen), pad_sequences(Xq, maxlen=question_maxlen),np_utils.to_categorical(Y, num_classes= len(word2indx))\n```", "```py\n>>> Xstrain, Xqtrain, Ytrain = data_vectorization(data_train, word2indx, story_maxlen, question_maxlen)\n>>> Xstest, Xqtest, Ytest = data_vectorization(data_test, word2indx, story_maxlen, question_maxlen)\n>>> print(\"Train story\",Xstrain.shape,\"Train question\", Xqtrain.shape,\"Train answer\", Ytrain.shape)\n>>> print( \"Test story\",Xstest.shape, \"Test question\",Xqtest.shape, \"Test answer\",Ytest.shape)\n```", "```py\n# Model Parameters\n>>> EMBEDDING_SIZE = 128\n>>> LATENT_SIZE = 64\n>>> BATCH_SIZE = 64\n>>> NUM_EPOCHS = 40\n```", "```py\n# Inputs\n>>> story_input = Input(shape=(story_maxlen,))\n>>> question_input = Input(shape=(question_maxlen,)) \n# Story encoder embedding\n>>> story_encoder = Embedding(input_dim=vocab_size, output_dim=EMBEDDING_SIZE,input_length= story_maxlen) (story_input)\n>>> story_encoder = Dropout(0.2)(story_encoder) \n# Question encoder embedding\n>>> question_encoder = Embedding(input_dim=vocab_size,output_dim= EMBEDDING_SIZE, input_length=question_maxlen) (question_input)\n>>> question_encoder = Dropout(0.3)(question_encoder) \n# Match between story and question\n>>> match = dot([story_encoder, question_encoder], axes=[2, 2]) \n# Encode story into vector space of question\n>>> story_encoder_c = Embedding(input_dim=vocab_size, output_dim=question_maxlen,input_length= story_maxlen)(story_input)\n>>> story_encoder_c = Dropout(0.3)(story_encoder_c) \n# Combine match and story vectors\n>>> response = add([match, story_encoder_c])\n>>> response = Permute((2, 1))(response) \n# Combine response and question vectors to answers space\n>>> answer = concatenate([response, question_encoder], axis=-1)\n>>> answer = LSTM(LATENT_SIZE)(answer)\n>>> answer = Dropout(0.2)(answer)\n>>> answer = Dense(vocab_size)(answer)\n>>> output = Activation(\"softmax\")(answer)\n>>> model = Model(inputs=[story_input, question_input], outputs=output)\n>>> model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n>>> print (model.summary())\n```", "```py\n# Model Training\n>>> history = model.fit([Xstrain, Xqtrain], [Ytrain], batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,validation_data= ([Xstest, Xqtest], [Ytest]))\n```", "```py\n# plot accuracy and loss plot\n>>> plt.title(\"Accuracy\")\n>>> plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n>>> plt.plot(history.history[\"val_acc\"], color=\"r\", label=\"validation\")\n>>> plt.legend(loc=\"best\")\n>>> plt.show()\n```", "```py\n# get predictions of labels\n>>> ytest = np.argmax(Ytest, axis=1)\n>>> Ytest_ = model.predict([Xstest, Xqtest])\n>>> ytest_ = np.argmax(Ytest_, axis=1)\n# Select Random questions and predict answers\n>>> NUM_DISPLAY = 10\n>>> for i in random.sample(range(Xstest.shape[0]),NUM_DISPLAY):\n... story = \" \".join([indx2word[x] for x in Xstest[i].tolist() if x != 0])\n... question = \" \".join([indx2word[x] for x in Xqtest[i].tolist()])\n... label = indx2word[ytest[i]]\n... prediction = indx2word[ytest_[i]]\n... print(story, question, label, prediction)\n```", "```py\n>>> from __future__ import print_function\n>>> import os\n\"\"\" First change the following directory link to where all input files do exist \"\"\"\n>>> os.chdir(\"C:\\\\Users\\\\prata\\\\Documents\\\\book_codes\\\\NLP_DL\")\n>>> from sklearn.model_selection import train_test_split\n>>> import nltk\n>>> import numpy as np\n>>> import string\n# File reading\n>>> with open('alice_in_wonderland.txt', 'r') as content_file:\n... content = content_file.read()\n>>> content2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in content]).split())\n>>> tokens = nltk.word_tokenize(content2)\n>>> tokens = [word.lower() for word in tokens if len(word)>=2]\n```", "```py\n# Select value of N for N grams among which N-1 are used to predict last Nth word\n>>> N = 3\n>>> quads = list(nltk.ngrams(tokens,N))\n>>> newl_app = []\n>>> for ln in quads:\n... newl = \" \".join(ln)\n... newl_app.append(newl)\n```", "```py\n# Vectorizing the words\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> vectorizer = CountVectorizer()\n>>> x_trigm = []\n>>> y_trigm = []\n>>> for l in newl_app:\n... x_str = \" \".join(l.split()[0:N-1])\n... y_str = l.split()[N-1]\n... x_trigm.append(x_str)\n... y_trigm.append(y_str)\n>>> x_trigm_check = vectorizer.fit_transform(x_trigm).todense()\n>>> y_trigm_check = vectorizer.fit_transform(y_trigm).todense()\n# Dictionaries from word to integer and integer to word\n>>> dictnry = vectorizer.vocabulary_\n>>> rev_dictnry = {v:k for k,v in dictnry.items()}\n>>> X = np.array(x_trigm_check)\n>>> Y = np.array(y_trigm_check)\n>>> Xtrain, Xtest, Ytrain, Ytest,xtrain_tg,xtest_tg = train_test_split(X, Y,x_trigm, test_size=0.3,random_state=42)\n>>> print(\"X Train shape\",Xtrain.shape, \"Y Train shape\" , Ytrain.shape)\n>>> print(\"X Test shape\",Xtest.shape, \"Y Test shape\" , Ytest.shape)\n```", "```py\n# Model Building\n>>> from keras.layers import Input,Dense,Dropout\n>>> from keras.models import Model\n>>> np.random.seed(42)\n>>> BATCH_SIZE = 128\n>>> NUM_EPOCHS = 100\n>>> input_layer = Input(shape = (Xtrain.shape[1],),name=\"input\")\n>>> first_layer = Dense(1000,activation='relu',name = \"first\")(input_layer)\n>>> first_dropout = Dropout(0.5,name=\"firstdout\")(first_layer)\n>>> second_layer = Dense(800,activation='relu',name=\"second\") (first_dropout)\n>>> third_layer = Dense(1000,activation='relu',name=\"third\") (second_layer)\n>>> third_dropout = Dropout(0.5,name=\"thirdout\")(third_layer)\n>>> fourth_layer = Dense(Ytrain.shape[1],activation='softmax',name = \"fourth\")(third_dropout)\n>>> history = Model(input_layer,fourth_layer)\n>>> history.compile(optimizer = \"adam\",loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n>>> print (history.summary())\n```", "```py\n# Model Training\n>>> history.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,epochs=NUM_EPOCHS, verbose=1,validation_split = 0.2)\n```", "```py\n# Model Prediction\n>>> Y_pred = history.predict(Xtest)\n# Sample check on Test data\n>>> print (\"Prior bigram words\", \"|Actual\", \"|Predicted\",\"\\n\")\n>>> for i in range(10):\n... print (i,xtest_tg[i],\"|\",rev_dictnry[np.argmax(Ytest[i])], \"|\",rev_dictnry[np.argmax(Y_pred[i])])\n```", "```py\n>>> import os\n\"\"\" First change the following directory link to where all input files do exist \"\"\"\n>>> os.chdir(\"C:\\\\Users\\\\prata\\\\Documents\\\\book_codes\\\\NLP_DL\")\n>>> import numpy as np\n>>> import pandas as pd\n# File reading\n>>> with open('bot.txt', 'r') as content_file:\n... botdata = content_file.read()\n>>> Questions = []\n>>> Answers = []\n```", "```py\n>>> for line in botdata.split(\"</pattern>\"):\n... if \"<pattern>\" in line:\n... Quesn = line[line.find(\"<pattern>\")+len(\"<pattern>\"):]\n... Questions.append(Quesn.lower())\n>>> for line in botdata.split(\"</template>\"):\n... if \"<template>\" in line:\n... Ans = line[line.find(\"<template>\")+len(\"<template>\"):]\n... Ans = Ans.lower()\n... Answers.append(Ans.lower())\n>>> QnAdata = pd.DataFrame(np.column_stack([Questions,Answers]),columns = [\"Questions\",\"Answers\"])\n>>> QnAdata[\"QnAcomb\"] = QnAdata[\"Questions\"]+\" \"+QnAdata[\"Answers\"]\n>>> print(QnAdata.head())\n```", "```py\n# Creating Vocabulary\n>>> import nltk\n>>> import collections\n>>> counter = collections.Counter()\n>>> for i in range(len(QnAdata)):\n... for word in nltk.word_tokenize(QnAdata.iloc[i][2]):\n... counter[word]+=1\n>>> word2idx = {w:(i+1) for i,(w,_) in enumerate(counter.most_common())}\n>>> idx2word = {v:k for k,v in word2idx.items()}\n>>> idx2word[0] = \"PAD\"\n>>> vocab_size = len(word2idx)+1\n>>> print (vocab_size)\n```", "```py\n>>> def encode(sentence, maxlen,vocab_size):\n... indices = np.zeros((maxlen, vocab_size))\n... for i, w in enumerate(nltk.word_tokenize(sentence)):\n... if i == maxlen: break\n... indices[i, word2idx[w]] = 1\n... return indices\n>>> def decode(indices, calc_argmax=True):\n... if calc_argmax:\n... indices = np.argmax(indices, axis=-1)\n... return ' '.join(idx2word[x] for x in indices)\n```", "```py\n>>> question_maxlen = 10\n>>> answer_maxlen = 20\n>>> def create_questions(question_maxlen,vocab_size):\n... question_idx = np.zeros(shape=(len(Questions),question_maxlen, vocab_size))\n... for q in range(len(Questions)):\n... question = encode(Questions[q],question_maxlen,vocab_size)\n... question_idx[i] = question\n... return question_idx\n>>> quesns_train = create_questions(question_maxlen=question_maxlen, vocab_size=vocab_size)\n>>> def create_answers(answer_maxlen,vocab_size):\n... answer_idx = np.zeros(shape=(len(Answers),answer_maxlen, vocab_size))\n... for q in range(len(Answers)):\n... answer = encode(Answers[q],answer_maxlen,vocab_size)\n... answer_idx[i] = answer\n... return answer_idx\n>>> answs_train = create_answers(answer_maxlen=answer_maxlen,vocab_size= vocab_size)\n>>> from keras.layers import Input,Dense,Dropout,Activation\n>>> from keras.models import Model\n>>> from keras.layers.recurrent import LSTM\n>>> from keras.layers.wrappers import Bidirectional\n>>> from keras.layers import RepeatVector, TimeDistributed, ActivityRegularization\n```", "```py\n>>> n_hidden = 128\n>>> question_layer = Input(shape=(question_maxlen,vocab_size))\n>>> encoder_rnn = LSTM(n_hidden,dropout=0.2,recurrent_dropout=0.2) (question_layer)\n>>> repeat_encode = RepeatVector(answer_maxlen)(encoder_rnn)\n>>> dense_layer = TimeDistributed(Dense(vocab_size))(repeat_encode)\n>>> regularized_layer = ActivityRegularization(l2=1)(dense_layer)\n>>> softmax_layer = Activation('softmax')(regularized_layer)\n>>> model = Model([question_layer],[softmax_layer])\n>>> model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n>>> print (model.summary())\n```", "```py\n# Model Training\n>>> quesns_train_2 = quesns_train.astype('float32')\n>>> answs_train_2 = answs_train.astype('float32')\n>>> model.fit(quesns_train_2, answs_train_2,batch_size=32,epochs=30, validation_split=0.05)\n```", "```py\n# Model prediction\n>>> ans_pred = model.predict(quesns_train_2[0:3])\n>>> print (decode(ans_pred[0]))\n>>> print (decode(ans_pred[1]))\n```"]