["```py\nobject topicmodelingwithLDA {\n def main(args: Array[String]): Unit = {\n val lda = \n new LDAforTM() \n// actual computations are done here\n val defaultParams = Params().copy(input = \"data/docs/\") //Loading parameters for training\n        lda.run(defaultParams) \n// Training the LDA model with the default parameters.\n      }\n}\n```", "```py\nimport edu.stanford.nlp.process.Morphology\nimport edu.stanford.nlp.simple.Document\nimport org.apache.log4j.{Level, Logger}\nimport scala.collection.JavaConversions._\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.linalg.{Vector => MLVector}\nimport org.apache.spark.mllib.clustering.{DistributedLDAModel, EMLDAOptimizer, LDA, OnlineLDAOptimizer, LDAModel}\nimport org.apache.spark.mllib.linalg.{ Vector, Vectors }\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.{Row, SparkSession}\n```", "```py\nval spark = SparkSession\n    .builder\n    .master(\"local[*]\")\n    .config(\"spark.sql.warehouse.dir\", \"C:/data/\")\n    .appName(s\"LDA\")\n    .getOrCreate()\n```", "```py\ndef run(params: Params)\n```", "```py\n// Load documents, and prepare them for LDA.\nval preprocessStart = System.nanoTime()\nval (corpus, vocabArray, actualNumTokens) = preprocess(params.input, params.vocabSize, params.stopwordFile)  \n```", "```py\n//Setting the parameters before training the LDA model\ncase class Params(var input: String = \"\", var ldaModel: LDAModel = null,\n    k: Int = 5,\n    maxIterations: Int = 100,\n    docConcentration: Double = 5,\n    topicConcentration: Double = 5,\n    vocabSize: Int = 2900000,\n    stopwordFile: String = \"data/docs/stopWords.txt\",\n    algorithm: String = \"em\",\n    checkpointDir: Option[String] = None,\n    checkpointInterval: Int = 100)\n```", "```py\nif (params.checkpointDir.nonEmpty) {\n    spark.sparkContext.setCheckpointDir(params.checkpointDir.get)\n     }\n```", "```py\nval initialrdd = spark.sparkContext.wholeTextFiles(paths).map(_._2) \ninitialrdd.cache()  \n```", "```py\nval rdd = initialrdd.mapPartitions { partition =>\n val morphology = new Morphology()\n    partition.map { value => helperForLDA.getLemmaText(value, morphology) }\n}.map(helperForLDA.filterSpecialCharacters)\n```", "```py\ndef getLemmaText(document: String, morphology: Morphology) = {\n val string = \n new StringBuilder()\n val value = \n new Document(document).sentences().toList.flatMap { \n        a =>\n val words = a.words().toList\n val tags = a.posTags().toList\n        (words zip tags).toMap.map { \n        a =>\n val newWord = morphology.lemma(a._1, a._2)\n val addedWoed = \n if (newWord.length > 3) {\n        newWord\n            }\n else { \"\" }\n        string.append(addedWoed + \" \")\n        }\n        }\n    string.toString()\n} \n```", "```py\n<dependency>\n    <groupId>edu.stanford.nlp</groupId>\n    <artifactId>stanford-corenlp</artifactId>\n    <version>3.6.0</version>\n</dependency>\n<dependency>\n    <groupId>edu.stanford.nlp</groupId>\n    <artifactId>stanford-corenlp</artifactId>\n    <version>3.6.0</version>\n    <classifier>models</classifier>\n</dependency>\n```", "```py\ndef filterSpecialCharacters(document: String) = document.replaceAll(\"\"\"[! @ # $ % ^ & * ( ) _ + - âˆ’ , \" ' ; : . ` ? --]\"\"\", \" \")\n```", "```py\nrdd.cache()\ninitialrdd.unpersist()\nval df = rdd.toDF(\"docs\")\ndf.show() \n```", "```py\nval tokenizer = new RegexTokenizer()\n                .setInputCol(\"docs\")\n                .setOutputCol(\"rawTokens\")\n```", "```py\nval stopWordsRemover = new StopWordsRemover()\n                        .setInputCol(\"rawTokens\")\n                        .setOutputCol(\"tokens\")\nstopWordsRemover.setStopWords(stopWordsRemover.getStopWords ++ customizedStopWords)\n```", "```py\nval countVectorizer = new CountVectorizer()\n                    .setVocabSize(vocabSize)\n                    .setInputCol(\"tokens\")\n                    .setOutputCol(\"features\")\n```", "```py\nval pipeline = new Pipeline().setStages(Array(tokenizer, stopWordsRemover, countVectorizer)) \n```", "```py\nval model = pipeline.fit(df)\nval documents = model.transform(df).select(\"features\").rdd.map {\n case Row(features: MLVector) => Vectors.fromML(features)\n    }.zipWithIndex().map(_.swap) \n```", "```py\n(documents, model.stages(2).asInstanceOf[CountVectorizerModel].vocabulary, documents.map(_._2.numActives).sum().toLong) Now let's see the statistics of the training data: \n\nprintln() println(\"Training corpus summary:\") \nprintln(\"-------------------------------\")\nprintln(\"Training set size: \" + actualCorpusSize + \" documents\")\nprintln(\"Vocabulary size: \" + actualVocabSize + \" terms\")\nprintln(\"Number of tockens: \" + actualNumTokens + \" tokens\")\nprintln(\"Preprocessing time: \" + preprocessElapsed + \" sec\")\nprintln(\"-------------------------------\")\nprintln()\n>>>\nTraining corpus summary:\n-------------------------------\nTraining set size: 19 documents\nVocabulary size: 21611 terms\nNumber of tockens: 75784 tokens\nPreprocessing time: 46.684682086 sec\n```", "```py\nval lda = new LDA() \n```", "```py\nval optimizer = params.algorithm.toLowerCase \n match {\n case \"em\" => \n new EMLDAOptimizer\n// add (1.0 / actualCorpusSize) to MiniBatchFraction be more robust on tiny datasets.\n case \"online\" => \n new OnlineLDAOptimizer().setMiniBatchFraction(0.05 + 1.0 / actualCorpusSize)\n case _ => \n thrownew IllegalArgumentException(\"Only em, online are supported but got \n            ${params.algorithm}.\")\n    } \n```", "```py\nlda.setOptimizer(optimizer)\n    .setK(params.k)\n    .setMaxIterations(params.maxIterations)\n    .setDocConcentration(params.docConcentration)\n    .setTopicConcentration(params.topicConcentration)\n    .setCheckpointInterval(params.checkpointInterval)\n```", "```py\nval startTime = System.nanoTime()\nldaModel = lda.run(corpus)\n\nval elapsed = (System.nanoTime() - startTime) / 1e9\nprintln(\"Finished training LDA model. Summary:\")\nprintln(\"Training time: \" + elapsed + \" sec\")\n```", "```py\n//Saving the model for future use\nparams.ldaModel.save(spark.sparkContext, \"model/LDATrainedModel\")\n```", "```py\nval topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 10)\nprintln(topicIndices.length)\nval topics = topicIndices.map {\n case (terms, termWeights) => terms.zip(termWeights).map {\n case (term, weight) => (vocabArray(term.toInt), weight) \n   }\n}\n```", "```py\nvar sum = 0.0\nprintln(s\"${params.k} topics:\")\ntopics.zipWithIndex.foreach {\n case (topic, i) =>\n        println(s\"TOPIC $i\")\n        println(\"------------------------------\")\n        topic.foreach {\n case (term, weight) =>\n        term.replaceAll(\"\\s\", \"\")\n        println(s\"$termt$weight\")\n        sum = sum + weight\n    }\nprintln(\"----------------------------\")\nprintln(\"weight: \" + sum)\nprintln()\n```", "```py\n 5 topics:\n TOPIC 0\n ------------------------------\n come 0.0070183359426213635\n make 0.006893251344696077\n look 0.006629265338364568\n know 0.006592594912464674\n take 0.006074234442310174\n little 0.005876330712306203\n think 0.005153843469004155\n time 0.0050685675513282525\n hand 0.004524837827665401\n well 0.004224698942533204\n ----------------------------\n weight: 0.05805596048329406\n TOPIC 1\n ------------------------------\n thus 0.008447268016707914\n ring 0.00750959344769264\n fate 0.006802070476284118\n trojan 0.006310545607626158\n bear 0.006244268350438889\n heav 0.005479939900136969\n thro 0.005185211621694439\n shore 0.004618008184651363\n fight 0.004161178536600401\n turnus 0.003899151842042464\n ----------------------------\n weight: 0.11671319646716942\n TOPIC 2\n ------------------------------\n aladdin 7.077183389325728E-4\n sultan 6.774311890861097E-4\n magician 6.127791175835228E-4\n genie 6.06094509479989E-4\n vizier 6.051618911188781E-4\n princess 5.654756758514474E-4\n fatima 4.050749957608771E-4\n flatland 3.47788388834721E-4\n want 3.4263963705536023E-4\n spaceland 3.371784715458026E-4\n ----------------------------\n weight: 0.1219205386824187\n TOPIC 3\n ------------------------------\n aladdin 7.325869707607238E-4\n sultan 7.012354862373387E-4\n magician 6.343184784726607E-4\n genie 6.273921840260785E-4\n vizier 6.264266945018852E-4\n princess 5.849046214967484E-4\n fatima 4.193089052802858E-4\n flatland 3.601371993827707E-4\n want 3.5398019331108816E-4\n spaceland 3.491505202713831E-4\n ----------------------------\n weight: 0.12730997993615964\n TOPIC 4\n ------------------------------\n captain 0.02931475169407467\n fogg 0.02743105575940755\n nautilus 0.022748371008515483\n passepartout 0.01802140608022664\n nemo 0.016678258146358142\n conseil 0.012129894049747918\n phileas 0.010441664411654412\n canadian 0.006217638883315841\n vessel 0.00618937301246955\n land 0.00615311666365297\n ----------------------------\n weight: 0.28263550964558276\n```", "```py\nif (ldaModel.isInstanceOf[DistributedLDAModel]) {\n val distLDAModel = ldaModel.asInstanceOf[DistributedLDAModel]\n val avgLogLikelihood = distLDAModel.logLikelihood / actualCorpusSize.toDouble\n    println(\"The average log likelihood of the training data: \" +\navgLogLikelihood)\n    println()\n}\n```", "```py\nThe average log likelihood of the training data: -209692.79314860413\n```", "```py\nparams.ldaModel.save(spark.sparkContext, \"model/LDATrainedModel\")\n```", "```py\n//Restoring the model for reuse\nval savedLDAModel = DistributedLDAModel.load(spark.sparkContext, \"model/LDATrainedModel/\")\n\n//Then we execute the following workflow:\nval lda = new LDAforTM() \n// actual computations are done here \n\n // Loading the parameters to train the LDA model \nval defaultParams = Params().copy(input = \"data/4UK1UkTX.csv\", savedLDAModel)\nlda.run(defaultParams) \n// Training the LDA model with the default parameters.\nspark.stop()\n```", "```py\n>>>\n Training corpus summary:\n -------------------------------\n Training set size: 1 documents\n Vocabulary size: 14670 terms\n Number of tockens: 14670 tokens\n Preprocessing time: 12.921435786 sec\n -------------------------------\n Finished training LDA model.\n Summary:\n Training time: 23.243336895 sec\n The average log likelihood of the training data: -1008739.37857908\n 5 topics:\n TOPIC 0\n ------------------------------\n rrb 0.015234818404037585\n lrb 0.015154125349208018\n sequence 0.008924621534990771\n gene 0.007391453509409655\n cell 0.007020265462594214\n protein 0.006479622004524878\n study 0.004954523307983932\n show 0.0040023453035193685\n site 0.0038006126784248945\n result 0.0036634344941610534\n ----------------------------\n weight: 0.07662582204885438\n TOPIC 1\n ------------------------------\n rrb 1.745030693927338E-4\n lrb 1.7450110447001028E-4\n sequence 1.7424254444446083E-4\n gene 1.7411236867642102E-4\n cell 1.7407234230511066E-4\n protein 1.7400587965300172E-4\n study 1.737407317498879E-4\n show 1.7347354627656383E-4\n site 1.7339989737227756E-4\n result 1.7334522348574853E-4\n ---------------------------\n weight: 0.07836521875668061\n TOPIC 2\n ------------------------------\n rrb 1.745030693927338E-4\n lrb 1.7450110447001028E-4\n sequence 1.7424254444446083E-4\n gene 1.7411236867642102E-4\n cell 1.7407234230511066E-4\n protein 1.7400587965300172E-4\n study 1.737407317498879E-4\n show 1.7347354627656383E-4\n site 1.7339989737227756E-4\n result 1.7334522348574853E-4\n ----------------------------\n weight: 0.08010461546450684\n TOPIC 3\n ------------------------------\n rrb 1.745030693927338E-4\n lrb 1.7450110447001028E-4\n sequence 1.7424254444446083E-4\n gene 1.7411236867642102E-4\n cell 1.7407234230511066E-4\n protein 1.7400587965300172E-4\n study 1.737407317498879E-4\n show 1.7347354627656383E-4\n site 1.7339989737227756E-4\n result 1.7334522348574853E-4\n ----------------------------\n weight: 0.08184401217233307\n TOPIC 4\n ------------------------------\n rrb 1.745030693927338E-4\n lrb 1.7450110447001028E-4\n sequence 1.7424254444446083E-4\n gene 1.7411236867642102E-4\n cell 1.7407234230511066E-4\n protein 1.7400587965300172E-4\n study 1.737407317498879E-4\n show 1.7347354627656383E-4\n site 1.7339989737227756E-4\n result 1.7334522348574853E-4\n ----------------------------\n weight: 0.0835834088801593\n```", "```py\npackage com.packt.ScalaML.Topicmodeling\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.mllib.clustering.{DistributedLDAModel, LDA}\n\nobject LDAModelReuse {\n def main(args: Array[String]): Unit = {\n val spark = SparkSession\n                    .builder\n                    .master(\"local[*]\")\n                    .config(\"spark.sql.warehouse.dir\", \"data/\")\n                    .appName(s\"LDA_TopicModelling\")\n                    .getOrCreate()\n\n//Restoring the model for reuse\n val savedLDAModel = DistributedLDAModel.load(spark.sparkContext, \"model/LDATrainedModel/\")\n val lda = new LDAforTM() \n// actual computations are done here\n val defaultParams = Params().copy(input = \"data/4UK1UkTX.csv\", savedLDAModel) \n//Loading params \n    lda.run(defaultParams) \n// Training the LDA model with the default parameters.\n    spark.stop()\n        }\n    }\n```"]