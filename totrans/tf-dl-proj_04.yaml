- en: Building GANs for Conditional Image Creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yann LeCun, Director of Facebook AI, has recently stated that "*Generative Adversarial
    Networks is the most interesting idea in the last ten years in machine learning*",
    and that is certainly confirmed by the elevated interest in academia about this
    deep learning solution. If you look at recent papers on deep learning (but also
    look at the leading trends on LinkedIn or Medium posts on the topic), there has
    really been an overproduction of variants of GANs.
  prefs: []
  type: TYPE_NORMAL
- en: You can get an idea of what a *zoo* the world of GANs has become just by glancing
    the continuously updated reference table, created by Hindu Puravinash, which can
    be found at [https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv](https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv) or
    by studying the GAN timeline prepared by Zheng Liu, which can be found at [https://github.com/dongb5/GAN-Timeline](https://github.com/dongb5/GAN-Timeline) and
    can help you putting everything into time perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs have the power to strike the imagination because they can demonstrate
    the creative power of AI, not just its computational strength. In this chapter,
    we are going to:'
  prefs: []
  type: TYPE_NORMAL
- en: Demystify the topic of GANs by providing you with all the necessary concepts
    to understand what GANs are, what they can do at the moment, and what they are
    expected to do
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demonstrate how to generate images both based on the initial distribution of
    example images (the so-called unsupervised GANs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain how to condition the GAN to the kind of resulting image you expect them
    to generate for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a basic yet complete project that can work with different datasets of
    handwritten characters and icons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide you with basic instructions how to train your GANs in the Cloud (specifically
    on Amazon AWS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The success of GANs much depends, besides the specific neural architecture you
    use, on the problem they face and the data you feed them with. The datasets we
    have chosen for this chapter should provide satisfactory results. We hope you
    will enjoy and be inspired by the creative power of GANs!
  prefs: []
  type: TYPE_NORMAL
- en: Introducing GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start with some quite recent history because GANs are among the newest
    ideas you'll find around AI and deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything started in 2014, when Ian Goodfellow and his colleagues (there is
    also Yoshua Bengio closing the list of contributors) at the *Departement d''informatique et
    de recherche opérationnelle* at Montreal University published a paper on **Generative
    Adversarial Nets** (**GANs**), a framework capable of generating new data based
    on a set of initial examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '*GOODFELLOW*, Ian, et al. Generative Adversarial Nets. In: *Advances in Neural
    Information Processing Systems*. 2014\. p. 2672-2680: [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial images produced by such networks were astonishing, considering
    the previous attempts using Markov chains which were far from being credible.
    In the image, you can see some of the examples proposed in the paper, showing
    examples reproduced from MNIST, **Toronto Face Dataset** (**TFD**) a non-public
    dataset and CIFAR-10 datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5550d1dd-463a-469c-a3ef-f4f11b3c5386.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Samples from the first paper on GANs using different datasets for
    learning to generate fresh images: a) MNIST b) TFD c) and d) CIFAR-10'
  prefs: []
  type: TYPE_NORMAL
- en: 'SOURCE: *GOODFELLOW*, Ian, et al. Generative Adversarial Nets. In: *Advances
    in Neural Information Processing Systems*. 2014\. p. 2672-2680'
  prefs: []
  type: TYPE_NORMAL
- en: The paper was deemed quite innovative because it put working together deep neural
    networks and game theory in a really smart architecture that didn't require much
    more than the usual back-propagation to train. GANs are generative models, models
    that can generate data because they have inscribed a model distribution (they
    learned it, for instance). Consequently when they generate something it is just
    like if they were sampling from that distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The key is in the adversarial approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key to understanding how GANs can be such successful generative models resides
    in the term adversarial. Actually, the GANs architecture is made up of two distinct
    networks that are optimized based on the pooling of respective errors, and that's
    called an **adversarial process**.
  prefs: []
  type: TYPE_NORMAL
- en: You start with a real dataset, let's call it R, containing your images or your
    data of a different kind (GANs are not limited to images only, though they constitute
    the major application). You then set up a generator network, G, which tries to
    make fake data that looks like the genuine data, and you set up a discriminator,
    D, whose role is to compare the data produced by G mixed against the real data,
    R, and figures out which is genuine and which is not.
  prefs: []
  type: TYPE_NORMAL
- en: Goodfellow used the art forgers metaphor to describe the process, being, the
    generator the forgers, and the discriminator the detective (or the art critic)
    that has to disclose their misdeed. There is a challenge between the forgers and
    the detective because while the forgers have to become more skillful in order
    not to be detected, the detective has to become better at detecting fakes. Everything
    turns into an endless fight between the forgers and the detective until the forged
    artifacts are completely similar to the originals. When GANs overfit, in fact,
    they just reproduce the originals. It really seems an explanation of a competitive
    market, and it really is, because the idea comes from competitive game theory.
  prefs: []
  type: TYPE_NORMAL
- en: In GANs, the generator is incentivized to produce images that the discriminator
    cannot figure out if they are a fake or not. An obvious solution for the generator
    is simply to copy some training image or to just settle down for some produced
    image that seems successful with the discriminator. One solution is *one-sided
    label smoothing* a technique which we will be applying in our project. It is described
    in SALIMANS, Tim, et al. Improved techniques for training gans. In: <q>Advances
    in Neural Information Processing Systems</q><q>. 2016\. p. 2234-2242</q>: [https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss how things actually work a little bit more. At first, the generator,
    *G*, is clueless and produces completely random data (it has actually never seen
    a piece of original data), it is therefore punished by the discriminator, *D--*an
    easy job figuring out the real versus the fake data. *G* takes full blame and
    starts trying something different to get better feedback from *D*. This is done
    completely randomly because the only data the generator sees is a random input
    called *Z*, it never touches the real data. After many trials and fails, hinted
    by the discriminator, the generator at last figures out what to do and starts
    to produce credible outputs. In the end, given enough time, the generator will
    exactly replicate all the original data without ever having seen a single example
    of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c2256ef-2d1e-4ded-a5ca-0ac8cd9f16b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2: Illustrative example of how a vanilla GAN architecture works
  prefs: []
  type: TYPE_NORMAL
- en: A cambrian explosion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, there are new papers on GANs coming out every month (as you can
    check on the reference table made by Hindu Puravinash that we mentioned at the
    beginning of the chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, apart from the vanilla implementation described in the initial paper
    from Goodfellow and his colleagues, the most notable implementations to take notice
    of are **deep convolutional generative adversarial networks **(**DCGANs**) and
    **conditional GANs (**CGANs**)**.
  prefs: []
  type: TYPE_NORMAL
- en: DCGANs are GANs based on CNN architecture (<q>RADFORD, Alec; METZ, Luke; CHINTALA,
    Soumith. Unsupervised representation learning with deep convolutional generative
    adversarial networks. arXiv preprint arXiv:1511.06434</q><q>, 2015</q>: [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CGANs are DCGANs which are conditioned on some input label so that you can obtain
    as a result an image with certain desired characteristics (<q>MIRZA, Mehdi; OSINDERO,
    Simon. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784</q><q>,
    2014</q>: [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)).
    Our project will be programming a `CGAN` class and training it on different datasets
    in order to prove its functioning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But there are also other interesting examples around (which are not covered
    by our project) offering practical solutions to problems related to image creation
    or improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A CycleGAN translates an image into another (the classic example is the horse
    that becomes a zebra: <q>ZHU, Jun-Yan, et al. Unpaired image-to-image translation
    using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593</q><q>,
    2017</q>: [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A StackGAN creates a realistic image from a text describing the image (<q>ZHANG,
    Han, et al. Stackgan: Text to photo-realistic image synthesis with stacked generative
    adversarial networks. arXiv preprint arXiv:1612.03242</q><q>, 2016</q>: [https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Discovery GAN (DiscoGAN) transfers stylistic elements from one image to another,
    thus transferring texture and decoration from a fashion item such as a bag to
    another fashion item such as a pair of shoes (<q>KIM, Taeksoo, et al. Learning
    to discover cross-domain relations with generative adversarial networks. arXiv
    preprint arXiv:1703.05192</q><q>, 2017</q>: [https://arxiv.org/abs/1703.05192](https://arxiv.org/abs/1703.05192))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A SRGAN can convert low-quality images into high-resolution ones (<q>LEDIG,
    Christian, et al. Photo-realistic single image super-resolution using a generative
    adversarial network. arXiv preprint arXiv:1609.04802</q><q>, 2016</q>: [https://arxiv.org/abs/1609.04802](https://arxiv.org/abs/1609.04802))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DCGANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DCGANs are the first relevant improvement on the GAN architecture. DCGANs always
    successfully complete their training phase and, given enough epochs and examples,
    they tend to generate satisfactory quality outputs. That soon made them the baseline
    for GANs and helped to produce some amazing achievements, such as generating new
    Pokemon from known ones: [https://www.youtube.com/watch?v=rs3aI7bACGc](https://www.youtube.com/watch?v=rs3aI7bACGc)
    or creating faces of celebrities that actually never existed but are incredibly
    realistic (nothing uncanny), just as NVIDIA did: [https://youtu.be/XOxxPcy5Gr4](https://youtu.be/XOxxPcy5Gr4) using
    a new training approach called **progressing growing**: [http://research.nvidia.com/sites/default/files/publications/karras2017gan-paper.pdf](http://research.nvidia.com/sites/default/files/publications/karras2017gan-paper.pdf).
    They have their root in using the same convolutions used in image classification
    by deep learning supervised networks, and they use some smart tricks:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization in both networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No fully hidden connected layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No pooling, just stride-in convolutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conditional GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In **conditional GANs** (**CGANs**), adding a vector of features controls the
    output and provides a better guide to the generator in figuring out what to do. Such
    a vector of features could encode the class the image should be derived be from
    (that is an image of a woman or a man if we are trying to create faces of imaginary
    actors) or even a set of specific characteristics we expect from the image (for
    imaginary actors, it could be the type of hair, eyes or complexion). The trick
    is done by incorporating the information into the images to be learned and into
    the *Z* input, which is not completely random anymore. The evaluation by the discriminator
    is done not only on the resemblance of fake data to the original data but also
    on the correspondence of the fake data image to its input label (or features):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf357ac9-ebea-4b71-b3d1-868b0738625d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3: Combining Z input with Y input (a labeling feature vector) allows
    generating controlled images
  prefs: []
  type: TYPE_NORMAL
- en: The project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Importing the right libraries is where we start. Apart from `tensorflow`, we
    will be using `numpy` and math for computations, `scipy`, `matplolib` for images
    and graphics, and `warnings`, `random`, and `distutils` for support in specific
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Dataset class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first step is to provide the data. We will rely on datasets that have already
    been preprocessed, but our readers could use different kinds of images for their
    own GAN implementation. The idea is to keep separate a `Dataset` class that will
    have the task of providing batches of normalized and reshaped images to the GANs
    class we will build later.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the initialization, we will deal with both images and their labels (if available).
    Images are first reshaped (if their shape differs from the one defined when instantiating
    the class), then shuffled. Shuffling helps GANs learning better if any order,
    for instance by class, is initially inscribed into the dataset - and this is actually
    true for any machine learning algorithm based on stochastic gradient descent:
    <q>BOTTOU, Léon. Stochastic gradient descent tricks. In: Neural networks: Tricks
    of the trade</q><q>. Springer, Berlin, Heidelberg, 2012\. p. 421-436</q>: [https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf).
    Labels instead are encoded using one-hot encoding, that is, a binary variable
    is created for each one of the classes, which is set to one (whereas others are
    set to zero) to represent the label as a vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if our classes are `{dog:0, cat:1}`, we will have these two one-hot
    encoded vectors to represent them: `{dog:[1, 0], cat:[0, 1]}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In such a way, we can easily add the vector to our image, as a further channel,
    and inscribe into it some kind of visual characteristic to be replicated by our
    GAN. Moreover, we could arrange the vectors in order to inscribe even more complex
    classes with special characteristics. For instance, we could specify the code
    for a class we prefer to be generated, and we can also specify some of its characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get_batches` method will just release a batch subset of the dataset and
    normalize the data by dividing the pixel values by the maximum (256) and subtracting
    -0.5\. The resulting images will have float values in the interval [-0.5, +0.5]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: CGAN class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `CGAN` class contains all the functions necessary for running a conditional
    GAN based on the `CGAN` model. The deep convolutional generative adversarial networks
    proved to have the performance in generating photo-like quality outputs. We have
    previously introduced CGANs, so just to remind you, their reference paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: RADFORD, Alec; METZ, Luke; CHINTALA, Soumith. Unsupervised representation learning
    with deep convolutional Generative Adversarial Networks. arXiv preprint arXiv:1511.06434,
    2015 at [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434).
  prefs: []
  type: TYPE_NORMAL
- en: In our project, we will then add the conditional form of the `CGAN` that uses
    label information as in a supervised learning task. Using labels and integrating
    them with images (this is the trick) will result in much better images and in
    the possibility of deciding the characteristics of the generated image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reference paper for conditional GANs is:'
  prefs: []
  type: TYPE_NORMAL
- en: MIRZA, Mehdi; OSINDERO, Simon. *Conditional Generative Adversarial Nets*. arXiv
    preprint arXiv:1411.1784, 2014, [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784).
  prefs: []
  type: TYPE_NORMAL
- en: Our `CGAN` class expects as input a dataset class object, the number of epochs,
    the image `batch_size`, the dimension of the random input used for the generator
    (`z_dim`), and a name for the GAN (for saving purposes). It also can be initialized
    with different values for alpha and smooth. We will discuss later what these two
    parameters can do for the GAN network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The instantiation sets all the internal variables and performs a performance
    check on the system, raising a warning if a GPU is not detected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `instantiate_inputs` function creates the TensorFlow placeholders for the
    inputs, both real and random. It also provides the labels (treated as images of
    the same shape of the original but for a channel depth equivalent to the number
    of classes), and for the learning rate of the training procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we pass to work on the architecture of the network, defining some basic
    functions such as the `leaky_ReLU_activation` function (that we will be using
    for both the generator and the discriminator, contrary to what is prescribed in
    the original paper on deep convolutional GANs):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Our next function represents a discriminator layer. It creates a convolution
    using Xavier initialization, operates batch normalization on the result, sets
    a `leaky_ReLU_activation`, and finally applies `dropout` for regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Xavier initialization assures that the initial weights of the convolution are
    not too small, nor too large, in order to allow a better transmission of the signals
    through the network since the initial epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Xavier initialization provides a Gaussian distribution with a zero mean whose
    variance is given by 1.0 divided by the number of neurons feeding into a layer.
    It is because of this kind of initialization that deep learning moved away from
    pre-training techniques, previously used to set initial weights that could transmit
    back propagation even in the presence of many layers. You can read more about
    it and about the Glorot and Bengio's variant of the initialization in this post: [http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization.](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch normalization is described by this paper:'
  prefs: []
  type: TYPE_NORMAL
- en: 'IOFFE, Sergey; SZEGEDY, Christian. Batch normalization: Accelerating deep network
    training by reducing internal covariate shift. In: *International Conference on
    Machine Learning*. 2015\. p. 448-456.'
  prefs: []
  type: TYPE_NORMAL
- en: As noted by the authors, the batch normalization algorithm for normalization
    deals with covariate shift ([http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html](http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html)),
    that is, changing distribution in the inputs which could cause the previously
    learned weights not to work properly anymore. In fact, as distributions are initially
    learned in the first input layers, they are transmitted to all the following layers,
    and shifting later because suddenly the input distribution has changed (for instance,
    initially you had more input photos of cats than dogs, now it's the contrary)
    could prove quite daunting unless you have set the learning rate very low.
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch normalization solves the problem of changing distribution in the inputs
    because it normalizes each batch by both mean and variance (using batch statistics),
    as illustrated by the paper <q>IOFFE, Sergey; SZEGEDY, Christian</q>. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. <q>In: International
    Conference on Machine Learning. 2015\. p. 448-456</q> (it can be found on the
    Internet at [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)).'
  prefs: []
  type: TYPE_NORMAL
- en: '``g_reshaping `and` g_conv_transpose`` are two functions that are part of the
    generator. They operate by reshaping the input, no matter if it is a flat layer
    or a convolution. Practically, they just reverse the work done by convolutions,
    restoring back the convolution-derived features into the original ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The discriminator architecture operates by taking images as input and, by various
    convolutions, transforming them until the result is flattened and turned into
    logits and probabilities (using the sigmoid function). Practically, everything
    is the same as in an ordinal convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As for the generator, the architecture is exactly the opposite of the discriminator.
    Starting from an input vector, `z`, a dense layer is first created, then a series
    of transpositions aims to rebuild the inverse process of convolutions in the discriminator,
    ending in a tensor of the same shape of the input images, which undergoes a further
    transformation by a `tanh` activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The architecture is very similar to the one depicted in the paper introducing
    CGANs, depicting how to reconstruct a 64 x 64 x 3 image from an initial input
    of a vector of size 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4918bff-eb00-41c2-aea4-ff729f556f37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The DCGAN architecture of the generator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SOURCE: arXiv, 1511.06434,2015'
  prefs: []
  type: TYPE_NORMAL
- en: After defining the architecture, the loss function is the next important element
    to define. It uses two outputs, the output from the generator, which is pipelined
    into the discriminator outputting logits, and the output from the real images
    pipelined themselves into the discriminator. For both, a loss measure is then calculated.
    Here, the smooth parameter comes in handy because it helps to smooth the probabilities
    of the real images into something that is not 1.0, allowing a better, more probabilistic
    learning by the GAN network (with full penalization it could become more difficult
    for the fake images to have a chance against the real ones).
  prefs: []
  type: TYPE_NORMAL
- en: 'The final discriminator loss is simply the sum of the loss calculated on the
    fake and on the real images. The loss is calculated on the fake comparing the
    estimated logits against the probability of zero. The loss on the real images
    is calculated comparing the estimated logit against the smoothed probability (in
    our case it is 0.9), in order to prevent overfitting and having the discriminator
    learn simply to spot the real images because it memorized them. The generator
    loss is instead calculated from the logits estimated by the discriminator for
    the fake images against a probability of 1.0\. In this way, the generator should
    strive to produce fake images that are estimated by the discriminator as most
    likely true (thus using a high probability). Therefore, the loss simply transmits
    from the discriminator evaluation on fake images to the generator in a feedback
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the work of the GAN is visual, there are a few functions for visualizing
    a sample of the current production from the generator, as well as a specific set
    of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the Adam optimizer, both the discriminator loss and the generator one
    are reduced, starting first from the discriminator (establishing how good is the
    generator''s production against true images) and then propagating the feedback
    to the generator, based on the evaluation of the effect the fake images produced
    by the generator had on the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'At last, we have the complete training phase. In the training, there are two
    parts that require attention:'
  prefs: []
  type: TYPE_NORMAL
- en: 'How the optimization is done in two steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the discriminator optimization
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Working on the generator's one
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: How the random input and the real images are preprocessed by mixing them with
    labels in a way that creates further image layers containing the one-hot encoded
    information of the class relative to the image's label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this way, the class is incorporated into the image, both in input and in
    output, conditioning the generator to take this information into account also,
    since it is penalized if it doesn''t produce realistic images, that is, images
    with the right label attached. Let''s say that our generator produces the image
    of a cat, but gives it the label of a dog. In this case, it will be penalized
    by the discriminator because the discriminator will notice how the generator cat
    is different from the real cats because of the different labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'During the training, the network is constantly saved on disk. When it is necessary
    to generate new images, you don''t need to retrain, but just upload the network
    and specify the label you want the GAN to produce images for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The class is completed by the `fit` method, which accepts both the learning
    rate parameter and the beta1 (an Adam optimizer parameter, adapting the parameter
    learning rates based on the average first moment, that is, the mean), and plots
    the resulting losses from the discriminator and the generator after the training
    is completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Putting CGAN to work on some examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that the `CGAN` class is completed, let''s go through some examples in
    order to provide you with fresh ideas on how to use this project. First of all,
    we will have to get everything ready for both downloading the necessary data and
    training our GAN. We start by importing the routine libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We then proceed by loading in the dataset and `CGAN` classes that we previously
    prepared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The class `TqdmUpTo` is just a `tqdm` wrapper that enables the use of the progress
    display also for downloads. The class has been taken directly from the project''s
    page at [https://github.com/tqdm/tqdm](https://github.com/tqdm/tqdm):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, if we are using a Jupyter notebook (warmly suggested for this roadshow),
    you have to enable the inline plotting of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to proceed with the first example.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `MNIST` database of handwritten digits was provided by Yann LeCun when
    he was at Courant Institute, NYU, and by Corinna Cortes (Google Labs) and Christopher
    J.C. Burges (Microsoft Research). It is considered the standard for learning from
    real-world image data with minimal effort in preprocessing and formatting. The
    database consists of handwritten digits, offering a training set of 60,000 examples
    and a test set of 10,000\. It is actually a subset of a larger set available from
    NIST. All the digits have been size-normalized and centered in a fixed-size image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce2ca276-49f1-411a-be98-f281615230de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A sample of the original MNIST helps to understand the quality of
    the images to be reproduced by the CGAN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we upload the dataset from the Internet and store it locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to learn this set of handwritten numbers, we apply a batch of 32 images,
    a learning rate of `0.0002`, a `beta1` of `0.35`, a `z_dim` of `96`, and `15`
    epochs for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image represents a sample of the numbers generated by the GAN
    at the second epoch and at the last one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fe5beef-20c5-45ab-8e30-18b7ab0c260f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The GAN''s results as they appear epoch after epoch'
  prefs: []
  type: TYPE_NORMAL
- en: After 16 epochs, the numbers appear to be well shaped and ready to be used.
    We then extract a sample of all the classes arranged by row.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the performances of a GAN is still most often the matter of visual
    inspecting some of its results by a human judge, trying to figure out if the image
    could be a fake (like a discriminator) from its overall aspect or by precisely
    revealing details. GANs lack an objective function to help to evaluate and compare
    them, though there are some computational techniques that could be used as a metric
    such as the *log-likelihood*, as described by <q>THEIS, Lucas; OORD, Aäron van
    den; BETHGE, Matthias. A note on the evaluation of generative models. arXiv preprint
    arXiv:1511.01844</q><q>, 2015</q>: [https://arxiv.org/abs/1511.01844](https://arxiv.org/abs/1511.01844).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will keep our evaluation simple and empirical and thus we will use a sample
    of images generated by the trained GAN in order to evaluate the performances of
    the network and we also try to inspect the training loss for both the generator
    and the discriminator in order to spot any particular trend:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57ca84ee-b495-4231-a862-a24a0c50afcc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A sample of the final results after training on MNIST reveals it
    is an accessible task for a GAN network'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observing the training fit chart, represented in the figure the following,
    we notice how the generator reached the lowest error when the training was complete.
    The discriminator, after a previous peak, is struggling to get back to its previous
    performance values, pointing out a possible generator''s breakthrough. We can
    expect that even more training epochs could improve the performance of this GAN
    network, but as you progress in the quality the output, it may take exponentially
    more time. In general, a good indicator of convergence of a GAN is having a downward
    trend of both the discriminator and generator, which is something that could be
    inferred by fitting a linear regression line to both loss vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/246d5831-c0aa-4504-947f-1236b006eea9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The training fit along the 16 epochs'
  prefs: []
  type: TYPE_NORMAL
- en: Training an amazing GAN network may take a very long time and a lot of computational
    resources. By reading this recent article appeared in the New York Times, [https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html](https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html),
    you can find a chart from NVIDIA showing the progress in time for the training
    of a progressive GAN learning from photos of celebrities. Whereas it can take
    a few days to get a decent result, for an astonishing one you need at least a
    fortnight. In the same way, even with our examples, the more training epochs you
    put in, the better the results.
  prefs: []
  type: TYPE_NORMAL
- en: Zalando MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fashion `MNIST` is a dataset of Zalando's article images, composed of a training
    set of 60,000 examples and a test set of 10,000 examples. As with `MNIST`, each
    example is a 28x28 grayscale image, associated with a label from 10 classes. It
    was intended by authors from Zalando Research ([https://github.com/zalandoresearch/fashion-mnist/graphs/contributors](https://github.com/zalandoresearch/fashion-mnist/graphs/contributors))
    as a replacement for the original MNIST dataset in order to better benchmark machine
    learning algorithms since it is more challenging to learn and much more representative
    of deep learning in real-world tasks ([https://twitter.com/fchollet/status/852594987527045120](https://twitter.com/fchollet/status/852594987527045120)).
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72d9410c-dc59-41ba-b547-13ba2893f256.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: A sample of the original Zalando dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'We download the images and their labels separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to learn this set of images, we apply a batch of 32 images, a learning
    rate of `0.0002`, a `beta1` of `0.35`, a `z_dim` of `96`, and `10` epochs for
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The training takes a long time to go through all the epochs, but the quality
    appears to soon stabilize, though some problems take more epochs to disappear
    (for instance holes in shirts):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ef7ad88-3460-48b3-a62d-11ed0db43d9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The evolution of the CGAN''s training through epochs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the result after 64 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7dc822bf-59d2-48af-8183-da1d5bd141b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: An overview of the results achieved after 64 epochs on Zalando dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The result is fully satisfactory, especially for clothes and men's shoes. Women's
    shoes, however, seem more difficult to be learned because smaller and more detailed
    than the other images.
  prefs: []
  type: TYPE_NORMAL
- en: EMNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `EMNIST` dataset is a set of handwritten character digits derived from
    the `NIST` Special Database and converted to a 28 x 28 pixel image format and
    dataset structure that directly matches the `MNIST` dataset. We will be using
    `EMNIST` Balanced, a set of characters with an equal number of samples per class,
    which consists of 131,600 characters spread over 47 balanced classes. You can
    find all the references to the dataset in:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). EMNIST: an extension
    of MNIST to handwritten letters. Retrieved from [http://arxiv.org/abs/1702.05373](http://arxiv.org/abs/1702.05373).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also explore complete information about `EMNIST` by browsing the official
    page of the dataset: [https://www.nist.gov/itl/iad/image-group/emnist-dataset](https://www.nist.gov/itl/iad/image-group/emnist-dataset).
    Here is an extraction of the kind of characters that can be found in the EMNIST
    Balanced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aff40bc1-76d6-4e47-b6ea-bb6e1b027db8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: A sample of the original EMNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After downloading from the NIST website, we unzip the downloaded package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We remove the unused ZIP file after checking that the unzipping was successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to learn this set of handwritten numbers, we apply a batch of 32 images,
    a learning rate of `0.0002`, a `beta1` of `0.35`, a `z_dim` of `96`, and 10 epochs
    for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a sample of some handwritten letters when completing the training after
    32 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9e19ccb-f85c-451d-837e-4c8b081285cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: An overview of the results obtained training a CGAN on the EMNIST
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As for MNIST, a GAN can learn in a reasonable time to replicate handwritten
    letters in an accurate and credible way.
  prefs: []
  type: TYPE_NORMAL
- en: Reusing the trained CGANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After training a CGAN, you may find useful to use the produced images in other
    applications. The method `generate_new` can be used to extract single images as
    well as a set of images (in order to check the quality of results for a specific
    image class). It operates on a previously trained `CGan` class, so all you have
    to do is just to pickle it in order first to save it, then to restore it again
    when needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the training is complete, you can save your `CGan` class using `pickle`,
    as shown by these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we have saved the `CGAN` trained on the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you have restarted the Python session and memory is clean of any variable,
    you can just `import` again all the classes and restore the pickled `CGan`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'When done, you set the target class you would like to be generated by the `CGan`
    (in the example we ask for the number `8` to be printed) and you can ask for a
    single example, a grid 5 x 5 of examples or a larger 10 x 10 grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: If you just want to obtain an overview of all the classes, just set the parameter
    `target_class` to -1.
  prefs: []
  type: TYPE_NORMAL
- en: 'After having set out target class to be represented, the `generate_new` is
    called three times and the last one the returned values are stored into the `images`
    variable, which is sized (100, 28, 28, 1) and contains a Numpy array of the produced
    images that can be reused for our purposes. Each time you call the method, a grid
    of results is plotted as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9f18203-dfc6-4fdc-8306-d108b47f9c45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The plotted grid is a composition of the produced images, that is
    an image itself. From left to right, the plot of a'
  prefs: []
  type: TYPE_NORMAL
- en: request for a 1 x 1, 5 x 5, 10 x 10 grid of results. The real images are returned
    by the method and can be reused.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't need `generate_new` to plot the results, you simply set the `plot`
    parameter to False: `images = gan.generate_new(target_class=nclass, rows=10, cols=10,
    plot=False)`.
  prefs: []
  type: TYPE_NORMAL
- en: Resorting to Amazon Web Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As previously noticed, it is warmly suggested you use a GPU in order to train
    the examples proposed in this chapter. Managing to obtain results in a reasonable
    time using just a CPU is indeed impossible, and also using a GPU may turn into
    quite long hours waiting for the computer to complete the training. A solution,
    requiring the payment of a fee, could be to resort to Amazon Elastic Compute Cloud,
    also known as Amazon EC2 ([https://aws.amazon.com/it/ec2/](https://aws.amazon.com/it/ec2/)),
    part of the **Amazon Web Services** (**AWS**). On EC2 you can launch virtual servers
    that you can control from your computer using the Internet connection. You can
    require servers with powerful GPUs on EC2 and make your life with TensorFlow projects
    much easier.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon EC2 is not the only cloud service around. We have suggested you this
    service because it is the one we used in order to test the code in this book.
    Actually, there are alternatives, such as Google Cloud Compute ([cloud.google.com](http://cloud.google.com)),
    Microsoft Azure (azure.microsoft.com) and many others.
  prefs: []
  type: TYPE_NORMAL
- en: Running the chapter’s code on EC2 requires having an account in AWS. If you
    don’t have one, the first step is to register at [aws.amazon.com](https://aws.amazon.com/),
    complete all the necessary forms and start with a free Basic Support Plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you are registered on AWS, you just sign in and visit the EC2 page ([https://aws.amazon.com/ec2](https://aws.amazon.com/ec2)).
    There you will:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a region which is both cheap and near to you which allows the kind of
    GPU instances we need, from EU (Ireland), Asia Pacific (Tokyo), US East (N. Virginia)
    and US West (Oregon).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upgrade your EC2 Service Limit report at: [https://console.aws.amazon.com/ec2/v2/home?#Limits](https://console.aws.amazon.com/ec2/v2/home?#Limits).
    You will need to access a **p3.2xlarge** instance. Therefore if your actual limit
    is zero, that should be taken at least to one, using the *Request Limit Increase* form
    (this may take up to 24 hours, but before it''s complete, you won’t be able to
    access this kind of instance).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get some AWS credits (providing your credit card, for instance).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After setting your region and having enough credit and request limit increase,
    you can start a **p3.2xlarge** server (a GPU compute server for deep learning
    applications) set up with an OS already containing all the software you need (thanks
    to an AMI, an image prepared by Amazon):'
  prefs: []
  type: TYPE_NORMAL
- en: Get to the EC2 Management Console, and click on the **Launch Instance** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on AWS Marketplace, and search for **Deep Learning AMI with Source Code
    v2.0 (ami-bcce6ac4)** AMI. This AMI has everything pre-installed: CUDA, cuDNN
    ([https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)), Tensorflow.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the *GPU* compute **p3.2xlarge** instance. This instance has a powerful
    NVIDIA Tesla V100 GPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure a security group (which you may call **Jupyter**) by adding **Custom
    TCP Rule**, with TCP protocol, on `port 8888`, accessible from anywhere. This
    will allow you to run a Jupyter server on the machine and see the interface from
    any computer connected to the Internet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an **Authentication Key Pair**. You can call it `deeplearning_jupyter.pem`
    for instance. Save it on your computer in a directory you can easily access.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the instance. Remember that you will be paying since this moment unless
    you **stop** it from the AWS menu—you still will incur in some costs, but minor
    ones and you will have the instance available for you, with all your data—or simply
    **terminate** it and don’t pay any more for it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After everything is launched, you can access the server from your computer using
    ssh.
  prefs: []
  type: TYPE_NORMAL
- en: Take notice of the IP of the machine. Let’s say it is `xx.xx.xxx.xxx`, as an
    example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From a shell pointing to the directory where you `.pem` file is, type:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssh -i deeplearning_jupyter.pem ubuntu@ xx.xx.xxx.xxx`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When you have accessed the server machine, configure its Jupyter server by
    typing these commands:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jupyter notebook --generate-config`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`sed -ie "s/#c.NotebookApp.ip = ''localhost''/#c.NotebookApp.ip = ''*''/g"
    ~/.jupyter/jupyter_notebook_config.py`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Operate on the server by copying the code (for instance by git cloning the
    code repository) and installing any library you may require. For instance, you
    could install these packages for this specific project:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sudo pip3 install tqdm`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`sudo pip3 install conda`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Launch the Jupyter server by running the command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jupyter notebook --ip=0.0.0.0 --no-browser`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this point, the server will run and your ssh shell will prompt you the logs
    from Jupyter. Among the logs, take note of the token (it is something like a sequence
    of numbers and letters).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open your browser and write in the address bar:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http:// xx.xx.xxx.xxx:8888/`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When required type the token and you are ready to use the Jupiter notebook as
    you were on your local machine, but it is actually operating on the server. At
    this point, you will have a powerful server with GPU for running all your experiments
    with GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In concluding this chapter, we would like to thank Udacity and Mat Leonard for
    their DCGAN tutorial, licensed under MIT ([https://github.com/udacity/deep-learning/blob/master/LICENSE](https://github.com/udacity/deep-learning/blob/master/LICENSE))
    which provided a good starting point and a benchmark for this project.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed at length the topic of Generative Adversarial
    Networks, how they work, and how they can be trained and used for different purposes.
    As a project, we have created a conditional GAN, one that can generate different
    types of images, based on your input and we learned how to process some example
    datasets and train them in order to have a pickable class capable of creating
    new images on demand.
  prefs: []
  type: TYPE_NORMAL
