- en: Building GANs for Conditional Image Creation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yann LeCun, Director of Facebook AI, has recently stated that "*Generative Adversarial
    Networks is the most interesting idea in the last ten years in machine learning*",
    and that is certainly confirmed by the elevated interest in academia about this
    deep learning solution. If you look at recent papers on deep learning (but also
    look at the leading trends on LinkedIn or Medium posts on the topic), there has
    really been an overproduction of variants of GANs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: You can get an idea of what a *zoo* the world of GANs has become just by glancing
    the continuously updated reference table, created by Hindu Puravinash, which can
    be found at [https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv](https://github.com/hindupuravinash/the-gan-zoo/blob/master/gans.tsv) or
    by studying the GAN timeline prepared by Zheng Liu, which can be found at [https://github.com/dongb5/GAN-Timeline](https://github.com/dongb5/GAN-Timeline) and
    can help you putting everything into time perspective.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs have the power to strike the imagination because they can demonstrate
    the creative power of AI, not just its computational strength. In this chapter,
    we are going to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Demystify the topic of GANs by providing you with all the necessary concepts
    to understand what GANs are, what they can do at the moment, and what they are
    expected to do
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demonstrate how to generate images both based on the initial distribution of
    example images (the so-called unsupervised GANs)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain how to condition the GAN to the kind of resulting image you expect them
    to generate for you
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a basic yet complete project that can work with different datasets of
    handwritten characters and icons
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide you with basic instructions how to train your GANs in the Cloud (specifically
    on Amazon AWS)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The success of GANs much depends, besides the specific neural architecture you
    use, on the problem they face and the data you feed them with. The datasets we
    have chosen for this chapter should provide satisfactory results. We hope you
    will enjoy and be inspired by the creative power of GANs!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Introducing GANs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start with some quite recent history because GANs are among the newest
    ideas you'll find around AI and deep learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything started in 2014, when Ian Goodfellow and his colleagues (there is
    also Yoshua Bengio closing the list of contributors) at the *Departement d''informatique et
    de recherche opérationnelle* at Montreal University published a paper on **Generative
    Adversarial Nets** (**GANs**), a framework capable of generating new data based
    on a set of initial examples:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '*GOODFELLOW*, Ian, et al. Generative Adversarial Nets. In: *Advances in Neural
    Information Processing Systems*. 2014\. p. 2672-2680: [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial images produced by such networks were astonishing, considering
    the previous attempts using Markov chains which were far from being credible.
    In the image, you can see some of the examples proposed in the paper, showing
    examples reproduced from MNIST, **Toronto Face Dataset** (**TFD**) a non-public
    dataset and CIFAR-10 datasets:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5550d1dd-463a-469c-a3ef-f4f11b3c5386.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Samples from the first paper on GANs using different datasets for
    learning to generate fresh images: a) MNIST b) TFD c) and d) CIFAR-10'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'SOURCE: *GOODFELLOW*, Ian, et al. Generative Adversarial Nets. In: *Advances
    in Neural Information Processing Systems*. 2014\. p. 2672-2680'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The paper was deemed quite innovative because it put working together deep neural
    networks and game theory in a really smart architecture that didn't require much
    more than the usual back-propagation to train. GANs are generative models, models
    that can generate data because they have inscribed a model distribution (they
    learned it, for instance). Consequently when they generate something it is just
    like if they were sampling from that distribution.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The key is in the adversarial approach
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key to understanding how GANs can be such successful generative models resides
    in the term adversarial. Actually, the GANs architecture is made up of two distinct
    networks that are optimized based on the pooling of respective errors, and that's
    called an **adversarial process**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: You start with a real dataset, let's call it R, containing your images or your
    data of a different kind (GANs are not limited to images only, though they constitute
    the major application). You then set up a generator network, G, which tries to
    make fake data that looks like the genuine data, and you set up a discriminator,
    D, whose role is to compare the data produced by G mixed against the real data,
    R, and figures out which is genuine and which is not.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Goodfellow used the art forgers metaphor to describe the process, being, the
    generator the forgers, and the discriminator the detective (or the art critic)
    that has to disclose their misdeed. There is a challenge between the forgers and
    the detective because while the forgers have to become more skillful in order
    not to be detected, the detective has to become better at detecting fakes. Everything
    turns into an endless fight between the forgers and the detective until the forged
    artifacts are completely similar to the originals. When GANs overfit, in fact,
    they just reproduce the originals. It really seems an explanation of a competitive
    market, and it really is, because the idea comes from competitive game theory.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: In GANs, the generator is incentivized to produce images that the discriminator
    cannot figure out if they are a fake or not. An obvious solution for the generator
    is simply to copy some training image or to just settle down for some produced
    image that seems successful with the discriminator. One solution is *one-sided
    label smoothing* a technique which we will be applying in our project. It is described
    in SALIMANS, Tim, et al. Improved techniques for training gans. In: <q>Advances
    in Neural Information Processing Systems</q><q>. 2016\. p. 2234-2242</q>: [https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss how things actually work a little bit more. At first, the generator,
    *G*, is clueless and produces completely random data (it has actually never seen
    a piece of original data), it is therefore punished by the discriminator, *D--*an
    easy job figuring out the real versus the fake data. *G* takes full blame and
    starts trying something different to get better feedback from *D*. This is done
    completely randomly because the only data the generator sees is a random input
    called *Z*, it never touches the real data. After many trials and fails, hinted
    by the discriminator, the generator at last figures out what to do and starts
    to produce credible outputs. In the end, given enough time, the generator will
    exactly replicate all the original data without ever having seen a single example
    of it:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c2256ef-2d1e-4ded-a5ca-0ac8cd9f16b8.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Figure 2: Illustrative example of how a vanilla GAN architecture works
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: A cambrian explosion
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, there are new papers on GANs coming out every month (as you can
    check on the reference table made by Hindu Puravinash that we mentioned at the
    beginning of the chapter).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, apart from the vanilla implementation described in the initial paper
    from Goodfellow and his colleagues, the most notable implementations to take notice
    of are **deep convolutional generative adversarial networks **(**DCGANs**) and
    **conditional GANs (**CGANs**)**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: DCGANs are GANs based on CNN architecture (<q>RADFORD, Alec; METZ, Luke; CHINTALA,
    Soumith. Unsupervised representation learning with deep convolutional generative
    adversarial networks. arXiv preprint arXiv:1511.06434</q><q>, 2015</q>: [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CGANs are DCGANs which are conditioned on some input label so that you can obtain
    as a result an image with certain desired characteristics (<q>MIRZA, Mehdi; OSINDERO,
    Simon. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784</q><q>,
    2014</q>: [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)).
    Our project will be programming a `CGAN` class and training it on different datasets
    in order to prove its functioning.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But there are also other interesting examples around (which are not covered
    by our project) offering practical solutions to problems related to image creation
    or improvement:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'A CycleGAN translates an image into another (the classic example is the horse
    that becomes a zebra: <q>ZHU, Jun-Yan, et al. Unpaired image-to-image translation
    using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593</q><q>,
    2017</q>: [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593))'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN将一张图像转换为另一张图像（经典例子是马变成斑马：<q>ZHU, Jun-Yan, et al. Unpaired image-to-image
    translation using cycle-consistent adversarial networks. arXiv预印本arXiv:1703.10593</q><q>,
    2017</q>: [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)）
- en: 'A StackGAN creates a realistic image from a text describing the image (<q>ZHANG,
    Han, et al. Stackgan: Text to photo-realistic image synthesis with stacked generative
    adversarial networks. arXiv preprint arXiv:1612.03242</q><q>, 2016</q>: [https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242))'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'StackGAN通过描述图像的文本生成逼真的图像（<q>ZHANG, Han, et al. Stackgan: Text to photo-realistic
    image synthesis with stacked generative adversarial networks. arXiv预印本arXiv:1612.03242</q><q>,
    2016</q>: [https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242)）'
- en: A Discovery GAN (DiscoGAN) transfers stylistic elements from one image to another,
    thus transferring texture and decoration from a fashion item such as a bag to
    another fashion item such as a pair of shoes (<q>KIM, Taeksoo, et al. Learning
    to discover cross-domain relations with generative adversarial networks. arXiv
    preprint arXiv:1703.05192</q><q>, 2017</q>: [https://arxiv.org/abs/1703.05192](https://arxiv.org/abs/1703.05192))
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Discovery GAN（DiscoGAN）将一种图像的风格元素转移到另一张图像上，从而将纹理和装饰从一种时尚物品（例如包包）转移到另一种时尚物品（例如鞋子）上（<q>KIM,
    Taeksoo, et al. Learning to discover cross-domain relations with generative adversarial
    networks. arXiv预印本arXiv:1703.05192</q><q>, 2017</q>: [https://arxiv.org/abs/1703.05192](https://arxiv.org/abs/1703.05192)）
- en: A SRGAN can convert low-quality images into high-resolution ones (<q>LEDIG,
    Christian, et al. Photo-realistic single image super-resolution using a generative
    adversarial network. arXiv preprint arXiv:1609.04802</q><q>, 2016</q>: [https://arxiv.org/abs/1609.04802](https://arxiv.org/abs/1609.04802))
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SRGAN能够将低质量图像转换为高分辨率图像（<q>LEDIG, Christian, et al. Photo-realistic single image
    super-resolution using a generative adversarial network. arXiv预印本arXiv:1609.04802</q><q>,
    2016</q>: [https://arxiv.org/abs/1609.04802](https://arxiv.org/abs/1609.04802)）
- en: DCGANs
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DCGAN
- en: 'DCGANs are the first relevant improvement on the GAN architecture. DCGANs always
    successfully complete their training phase and, given enough epochs and examples,
    they tend to generate satisfactory quality outputs. That soon made them the baseline
    for GANs and helped to produce some amazing achievements, such as generating new
    Pokemon from known ones: [https://www.youtube.com/watch?v=rs3aI7bACGc](https://www.youtube.com/watch?v=rs3aI7bACGc)
    or creating faces of celebrities that actually never existed but are incredibly
    realistic (nothing uncanny), just as NVIDIA did: [https://youtu.be/XOxxPcy5Gr4](https://youtu.be/XOxxPcy5Gr4) using
    a new training approach called **progressing growing**: [http://research.nvidia.com/sites/default/files/publications/karras2017gan-paper.pdf](http://research.nvidia.com/sites/default/files/publications/karras2017gan-paper.pdf).
    They have their root in using the same convolutions used in image classification
    by deep learning supervised networks, and they use some smart tricks:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN是GAN架构的首次相关改进。DCGAN始终能够成功完成训练阶段，并且在足够的训练周期和示例下，通常能够生成令人满意的输出。这使得它们很快成为了GAN的基准，并帮助产生了一些令人惊叹的成就，比如从已知的宝可梦生成新的宝可梦：[https://www.youtube.com/watch?v=rs3aI7bACGc](https://www.youtube.com/watch?v=rs3aI7bACGc)，或者创造出实际上从未存在过但极其真实的名人面孔（毫无诡异感），正如NVIDIA所做的：[https://youtu.be/XOxxPcy5Gr4](https://youtu.be/XOxxPcy5Gr4)，使用一种新的训练方法叫做**逐步生长**：[http://research.nvidia.com/sites/default/files/publications/karras2017gan-paper.pdf](http://research.nvidia.com/sites/default/files/publications/karras2017gan-paper.pdf)。它们的根基在于使用深度学习监督网络中用于图像分类的相同卷积操作，并且它们使用了一些聪明的技巧：
- en: Batch normalization in both networks
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个网络中都使用批量归一化
- en: No fully hidden connected layers
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无完全隐藏连接层
- en: No pooling, just stride-in convolutions
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无池化，仅使用步幅卷积
- en: ReLU activation functions
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU激活函数
- en: Conditional GANs
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件GAN
- en: 'In **conditional GANs** (**CGANs**), adding a vector of features controls the
    output and provides a better guide to the generator in figuring out what to do. Such
    a vector of features could encode the class the image should be derived be from
    (that is an image of a woman or a man if we are trying to create faces of imaginary
    actors) or even a set of specific characteristics we expect from the image (for
    imaginary actors, it could be the type of hair, eyes or complexion). The trick
    is done by incorporating the information into the images to be learned and into
    the *Z* input, which is not completely random anymore. The evaluation by the discriminator
    is done not only on the resemblance of fake data to the original data but also
    on the correspondence of the fake data image to its input label (or features):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf357ac9-ebea-4b71-b3d1-868b0738625d.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Figure 3: Combining Z input with Y input (a labeling feature vector) allows
    generating controlled images
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The project
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Importing the right libraries is where we start. Apart from `tensorflow`, we
    will be using `numpy` and math for computations, `scipy`, `matplolib` for images
    and graphics, and `warnings`, `random`, and `distutils` for support in specific
    operations:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Dataset class
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first step is to provide the data. We will rely on datasets that have already
    been preprocessed, but our readers could use different kinds of images for their
    own GAN implementation. The idea is to keep separate a `Dataset` class that will
    have the task of providing batches of normalized and reshaped images to the GANs
    class we will build later.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'In the initialization, we will deal with both images and their labels (if available).
    Images are first reshaped (if their shape differs from the one defined when instantiating
    the class), then shuffled. Shuffling helps GANs learning better if any order,
    for instance by class, is initially inscribed into the dataset - and this is actually
    true for any machine learning algorithm based on stochastic gradient descent:
    <q>BOTTOU, Léon. Stochastic gradient descent tricks. In: Neural networks: Tricks
    of the trade</q><q>. Springer, Berlin, Heidelberg, 2012\. p. 421-436</q>: [https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2012/01/tricks-2012.pdf).
    Labels instead are encoded using one-hot encoding, that is, a binary variable
    is created for each one of the classes, which is set to one (whereas others are
    set to zero) to represent the label as a vector.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if our classes are `{dog:0, cat:1}`, we will have these two one-hot
    encoded vectors to represent them: `{dog:[1, 0], cat:[0, 1]}`.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'In such a way, we can easily add the vector to our image, as a further channel,
    and inscribe into it some kind of visual characteristic to be replicated by our
    GAN. Moreover, we could arrange the vectors in order to inscribe even more complex
    classes with special characteristics. For instance, we could specify the code
    for a class we prefer to be generated, and we can also specify some of its characteristics:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `get_batches` method will just release a batch subset of the dataset and
    normalize the data by dividing the pixel values by the maximum (256) and subtracting
    -0.5\. The resulting images will have float values in the interval [-0.5, +0.5]:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: CGAN class
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `CGAN` class contains all the functions necessary for running a conditional
    GAN based on the `CGAN` model. The deep convolutional generative adversarial networks
    proved to have the performance in generating photo-like quality outputs. We have
    previously introduced CGANs, so just to remind you, their reference paper is:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: RADFORD, Alec; METZ, Luke; CHINTALA, Soumith. Unsupervised representation learning
    with deep convolutional Generative Adversarial Networks. arXiv preprint arXiv:1511.06434,
    2015 at [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: In our project, we will then add the conditional form of the `CGAN` that uses
    label information as in a supervised learning task. Using labels and integrating
    them with images (this is the trick) will result in much better images and in
    the possibility of deciding the characteristics of the generated image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'The reference paper for conditional GANs is:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: MIRZA, Mehdi; OSINDERO, Simon. *Conditional Generative Adversarial Nets*. arXiv
    preprint arXiv:1411.1784, 2014, [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Our `CGAN` class expects as input a dataset class object, the number of epochs,
    the image `batch_size`, the dimension of the random input used for the generator
    (`z_dim`), and a name for the GAN (for saving purposes). It also can be initialized
    with different values for alpha and smooth. We will discuss later what these two
    parameters can do for the GAN network.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'The instantiation sets all the internal variables and performs a performance
    check on the system, raising a warning if a GPU is not detected:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `instantiate_inputs` function creates the TensorFlow placeholders for the
    inputs, both real and random. It also provides the labels (treated as images of
    the same shape of the original but for a channel depth equivalent to the number
    of classes), and for the learning rate of the training procedure:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we pass to work on the architecture of the network, defining some basic
    functions such as the `leaky_ReLU_activation` function (that we will be using
    for both the generator and the discriminator, contrary to what is prescribed in
    the original paper on deep convolutional GANs):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our next function represents a discriminator layer. It creates a convolution
    using Xavier initialization, operates batch normalization on the result, sets
    a `leaky_ReLU_activation`, and finally applies `dropout` for regularization:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Xavier initialization assures that the initial weights of the convolution are
    not too small, nor too large, in order to allow a better transmission of the signals
    through the network since the initial epochs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Xavier initialization provides a Gaussian distribution with a zero mean whose
    variance is given by 1.0 divided by the number of neurons feeding into a layer.
    It is because of this kind of initialization that deep learning moved away from
    pre-training techniques, previously used to set initial weights that could transmit
    back propagation even in the presence of many layers. You can read more about
    it and about the Glorot and Bengio's variant of the initialization in this post: [http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization.](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch normalization is described by this paper:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'IOFFE, Sergey; SZEGEDY, Christian. Batch normalization: Accelerating deep network
    training by reducing internal covariate shift. In: *International Conference on
    Machine Learning*. 2015\. p. 448-456.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: As noted by the authors, the batch normalization algorithm for normalization
    deals with covariate shift ([http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html](http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html)),
    that is, changing distribution in the inputs which could cause the previously
    learned weights not to work properly anymore. In fact, as distributions are initially
    learned in the first input layers, they are transmitted to all the following layers,
    and shifting later because suddenly the input distribution has changed (for instance,
    initially you had more input photos of cats than dogs, now it's the contrary)
    could prove quite daunting unless you have set the learning rate very low.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch normalization solves the problem of changing distribution in the inputs
    because it normalizes each batch by both mean and variance (using batch statistics),
    as illustrated by the paper <q>IOFFE, Sergey; SZEGEDY, Christian</q>. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. <q>In: International
    Conference on Machine Learning. 2015\. p. 448-456</q> (it can be found on the
    Internet at [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '``g_reshaping `and` g_conv_transpose`` are two functions that are part of the
    generator. They operate by reshaping the input, no matter if it is a flat layer
    or a convolution. Practically, they just reverse the work done by convolutions,
    restoring back the convolution-derived features into the original ones:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The discriminator architecture operates by taking images as input and, by various
    convolutions, transforming them until the result is flattened and turned into
    logits and probabilities (using the sigmoid function). Practically, everything
    is the same as in an ordinal convolution:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As for the generator, the architecture is exactly the opposite of the discriminator.
    Starting from an input vector, `z`, a dense layer is first created, then a series
    of transpositions aims to rebuild the inverse process of convolutions in the discriminator,
    ending in a tensor of the same shape of the input images, which undergoes a further
    transformation by a `tanh` activation function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The architecture is very similar to the one depicted in the paper introducing
    CGANs, depicting how to reconstruct a 64 x 64 x 3 image from an initial input
    of a vector of size 100:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4918bff-eb00-41c2-aea4-ff729f556f37.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The DCGAN architecture of the generator.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'SOURCE: arXiv, 1511.06434,2015'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: After defining the architecture, the loss function is the next important element
    to define. It uses two outputs, the output from the generator, which is pipelined
    into the discriminator outputting logits, and the output from the real images
    pipelined themselves into the discriminator. For both, a loss measure is then calculated.
    Here, the smooth parameter comes in handy because it helps to smooth the probabilities
    of the real images into something that is not 1.0, allowing a better, more probabilistic
    learning by the GAN network (with full penalization it could become more difficult
    for the fake images to have a chance against the real ones).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'The final discriminator loss is simply the sum of the loss calculated on the
    fake and on the real images. The loss is calculated on the fake comparing the
    estimated logits against the probability of zero. The loss on the real images
    is calculated comparing the estimated logit against the smoothed probability (in
    our case it is 0.9), in order to prevent overfitting and having the discriminator
    learn simply to spot the real images because it memorized them. The generator
    loss is instead calculated from the logits estimated by the discriminator for
    the fake images against a probability of 1.0\. In this way, the generator should
    strive to produce fake images that are estimated by the discriminator as most
    likely true (thus using a high probability). Therefore, the loss simply transmits
    from the discriminator evaluation on fake images to the generator in a feedback
    loop:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Since the work of the GAN is visual, there are a few functions for visualizing
    a sample of the current production from the generator, as well as a specific set
    of images:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Using the Adam optimizer, both the discriminator loss and the generator one
    are reduced, starting first from the discriminator (establishing how good is the
    generator''s production against true images) and then propagating the feedback
    to the generator, based on the evaluation of the effect the fake images produced
    by the generator had on the discriminator:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At last, we have the complete training phase. In the training, there are two
    parts that require attention:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'How the optimization is done in two steps:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the discriminator optimization
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Working on the generator's one
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: How the random input and the real images are preprocessed by mixing them with
    labels in a way that creates further image layers containing the one-hot encoded
    information of the class relative to the image's label
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this way, the class is incorporated into the image, both in input and in
    output, conditioning the generator to take this information into account also,
    since it is penalized if it doesn''t produce realistic images, that is, images
    with the right label attached. Let''s say that our generator produces the image
    of a cat, but gives it the label of a dog. In this case, it will be penalized
    by the discriminator because the discriminator will notice how the generator cat
    is different from the real cats because of the different labels:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'During the training, the network is constantly saved on disk. When it is necessary
    to generate new images, you don''t need to retrain, but just upload the network
    and specify the label you want the GAN to produce images for:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The class is completed by the `fit` method, which accepts both the learning
    rate parameter and the beta1 (an Adam optimizer parameter, adapting the parameter
    learning rates based on the average first moment, that is, the mean), and plots
    the resulting losses from the discriminator and the generator after the training
    is completed:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Putting CGAN to work on some examples
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that the `CGAN` class is completed, let''s go through some examples in
    order to provide you with fresh ideas on how to use this project. First of all,
    we will have to get everything ready for both downloading the necessary data and
    training our GAN. We start by importing the routine libraries:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We then proceed by loading in the dataset and `CGAN` classes that we previously
    prepared:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The class `TqdmUpTo` is just a `tqdm` wrapper that enables the use of the progress
    display also for downloads. The class has been taken directly from the project''s
    page at [https://github.com/tqdm/tqdm](https://github.com/tqdm/tqdm):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, if we are using a Jupyter notebook (warmly suggested for this roadshow),
    you have to enable the inline plotting of images:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We are now ready to proceed with the first example.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: MNIST
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `MNIST` database of handwritten digits was provided by Yann LeCun when
    he was at Courant Institute, NYU, and by Corinna Cortes (Google Labs) and Christopher
    J.C. Burges (Microsoft Research). It is considered the standard for learning from
    real-world image data with minimal effort in preprocessing and formatting. The
    database consists of handwritten digits, offering a training set of 60,000 examples
    and a test set of 10,000\. It is actually a subset of a larger set available from
    NIST. All the digits have been size-normalized and centered in a fixed-size image:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce2ca276-49f1-411a-be98-f281615230de.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A sample of the original MNIST helps to understand the quality of
    the images to be reproduced by the CGAN.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we upload the dataset from the Internet and store it locally:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In order to learn this set of handwritten numbers, we apply a batch of 32 images,
    a learning rate of `0.0002`, a `beta1` of `0.35`, a `z_dim` of `96`, and `15`
    epochs for training:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following image represents a sample of the numbers generated by the GAN
    at the second epoch and at the last one:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fe5beef-20c5-45ab-8e30-18b7ab0c260f.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The GAN''s results as they appear epoch after epoch'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: After 16 epochs, the numbers appear to be well shaped and ready to be used.
    We then extract a sample of all the classes arranged by row.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the performances of a GAN is still most often the matter of visual
    inspecting some of its results by a human judge, trying to figure out if the image
    could be a fake (like a discriminator) from its overall aspect or by precisely
    revealing details. GANs lack an objective function to help to evaluate and compare
    them, though there are some computational techniques that could be used as a metric
    such as the *log-likelihood*, as described by <q>THEIS, Lucas; OORD, Aäron van
    den; BETHGE, Matthias. A note on the evaluation of generative models. arXiv preprint
    arXiv:1511.01844</q><q>, 2015</q>: [https://arxiv.org/abs/1511.01844](https://arxiv.org/abs/1511.01844).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'We will keep our evaluation simple and empirical and thus we will use a sample
    of images generated by the trained GAN in order to evaluate the performances of
    the network and we also try to inspect the training loss for both the generator
    and the discriminator in order to spot any particular trend:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57ca84ee-b495-4231-a862-a24a0c50afcc.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A sample of the final results after training on MNIST reveals it
    is an accessible task for a GAN network'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'Observing the training fit chart, represented in the figure the following,
    we notice how the generator reached the lowest error when the training was complete.
    The discriminator, after a previous peak, is struggling to get back to its previous
    performance values, pointing out a possible generator''s breakthrough. We can
    expect that even more training epochs could improve the performance of this GAN
    network, but as you progress in the quality the output, it may take exponentially
    more time. In general, a good indicator of convergence of a GAN is having a downward
    trend of both the discriminator and generator, which is something that could be
    inferred by fitting a linear regression line to both loss vectors:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/246d5831-c0aa-4504-947f-1236b006eea9.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The training fit along the 16 epochs'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Training an amazing GAN network may take a very long time and a lot of computational
    resources. By reading this recent article appeared in the New York Times, [https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html](https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html),
    you can find a chart from NVIDIA showing the progress in time for the training
    of a progressive GAN learning from photos of celebrities. Whereas it can take
    a few days to get a decent result, for an astonishing one you need at least a
    fortnight. In the same way, even with our examples, the more training epochs you
    put in, the better the results.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Zalando MNIST
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fashion `MNIST` is a dataset of Zalando's article images, composed of a training
    set of 60,000 examples and a test set of 10,000 examples. As with `MNIST`, each
    example is a 28x28 grayscale image, associated with a label from 10 classes. It
    was intended by authors from Zalando Research ([https://github.com/zalandoresearch/fashion-mnist/graphs/contributors](https://github.com/zalandoresearch/fashion-mnist/graphs/contributors))
    as a replacement for the original MNIST dataset in order to better benchmark machine
    learning algorithms since it is more challenging to learn and much more representative
    of deep learning in real-world tasks ([https://twitter.com/fchollet/status/852594987527045120](https://twitter.com/fchollet/status/852594987527045120)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72d9410c-dc59-41ba-b547-13ba2893f256.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: A sample of the original Zalando dataset'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'We download the images and their labels separately:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In order to learn this set of images, we apply a batch of 32 images, a learning
    rate of `0.0002`, a `beta1` of `0.35`, a `z_dim` of `96`, and `10` epochs for
    training:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The training takes a long time to go through all the epochs, but the quality
    appears to soon stabilize, though some problems take more epochs to disappear
    (for instance holes in shirts):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ef7ad88-3460-48b3-a62d-11ed0db43d9a.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The evolution of the CGAN''s training through epochs'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the result after 64 epochs:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7dc822bf-59d2-48af-8183-da1d5bd141b7.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: An overview of the results achieved after 64 epochs on Zalando dataset'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The result is fully satisfactory, especially for clothes and men's shoes. Women's
    shoes, however, seem more difficult to be learned because smaller and more detailed
    than the other images.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: EMNIST
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `EMNIST` dataset is a set of handwritten character digits derived from
    the `NIST` Special Database and converted to a 28 x 28 pixel image format and
    dataset structure that directly matches the `MNIST` dataset. We will be using
    `EMNIST` Balanced, a set of characters with an equal number of samples per class,
    which consists of 131,600 characters spread over 47 balanced classes. You can
    find all the references to the dataset in:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). EMNIST: an extension
    of MNIST to handwritten letters. Retrieved from [http://arxiv.org/abs/1702.05373](http://arxiv.org/abs/1702.05373).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also explore complete information about `EMNIST` by browsing the official
    page of the dataset: [https://www.nist.gov/itl/iad/image-group/emnist-dataset](https://www.nist.gov/itl/iad/image-group/emnist-dataset).
    Here is an extraction of the kind of characters that can be found in the EMNIST
    Balanced:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aff40bc1-76d6-4e47-b6ea-bb6e1b027db8.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: A sample of the original EMNIST dataset'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After downloading from the NIST website, we unzip the downloaded package:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We remove the unused ZIP file after checking that the unzipping was successful:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In order to learn this set of handwritten numbers, we apply a batch of 32 images,
    a learning rate of `0.0002`, a `beta1` of `0.35`, a `z_dim` of `96`, and 10 epochs
    for training:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is a sample of some handwritten letters when completing the training after
    32 epochs:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9e19ccb-f85c-451d-837e-4c8b081285cc.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: An overview of the results obtained training a CGAN on the EMNIST
    dataset'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: As for MNIST, a GAN can learn in a reasonable time to replicate handwritten
    letters in an accurate and credible way.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Reusing the trained CGANs
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After training a CGAN, you may find useful to use the produced images in other
    applications. The method `generate_new` can be used to extract single images as
    well as a set of images (in order to check the quality of results for a specific
    image class). It operates on a previously trained `CGan` class, so all you have
    to do is just to pickle it in order first to save it, then to restore it again
    when needed.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'When the training is complete, you can save your `CGan` class using `pickle`,
    as shown by these commands:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this case, we have saved the `CGAN` trained on the MNIST dataset.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'After you have restarted the Python session and memory is clean of any variable,
    you can just `import` again all the classes and restore the pickled `CGan`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'When done, you set the target class you would like to be generated by the `CGan`
    (in the example we ask for the number `8` to be printed) and you can ask for a
    single example, a grid 5 x 5 of examples or a larger 10 x 10 grid:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If you just want to obtain an overview of all the classes, just set the parameter
    `target_class` to -1.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'After having set out target class to be represented, the `generate_new` is
    called three times and the last one the returned values are stored into the `images`
    variable, which is sized (100, 28, 28, 1) and contains a Numpy array of the produced
    images that can be reused for our purposes. Each time you call the method, a grid
    of results is plotted as shown in the following figure:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9f18203-dfc6-4fdc-8306-d108b47f9c45.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The plotted grid is a composition of the produced images, that is
    an image itself. From left to right, the plot of a'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: request for a 1 x 1, 5 x 5, 10 x 10 grid of results. The real images are returned
    by the method and can be reused.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: If you don't need `generate_new` to plot the results, you simply set the `plot`
    parameter to False: `images = gan.generate_new(target_class=nclass, rows=10, cols=10,
    plot=False)`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Resorting to Amazon Web Service
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As previously noticed, it is warmly suggested you use a GPU in order to train
    the examples proposed in this chapter. Managing to obtain results in a reasonable
    time using just a CPU is indeed impossible, and also using a GPU may turn into
    quite long hours waiting for the computer to complete the training. A solution,
    requiring the payment of a fee, could be to resort to Amazon Elastic Compute Cloud,
    also known as Amazon EC2 ([https://aws.amazon.com/it/ec2/](https://aws.amazon.com/it/ec2/)),
    part of the **Amazon Web Services** (**AWS**). On EC2 you can launch virtual servers
    that you can control from your computer using the Internet connection. You can
    require servers with powerful GPUs on EC2 and make your life with TensorFlow projects
    much easier.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Amazon EC2 is not the only cloud service around. We have suggested you this
    service because it is the one we used in order to test the code in this book.
    Actually, there are alternatives, such as Google Cloud Compute ([cloud.google.com](http://cloud.google.com)),
    Microsoft Azure (azure.microsoft.com) and many others.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Running the chapter’s code on EC2 requires having an account in AWS. If you
    don’t have one, the first step is to register at [aws.amazon.com](https://aws.amazon.com/),
    complete all the necessary forms and start with a free Basic Support Plan.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'After you are registered on AWS, you just sign in and visit the EC2 page ([https://aws.amazon.com/ec2](https://aws.amazon.com/ec2)).
    There you will:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Select a region which is both cheap and near to you which allows the kind of
    GPU instances we need, from EU (Ireland), Asia Pacific (Tokyo), US East (N. Virginia)
    and US West (Oregon).
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upgrade your EC2 Service Limit report at: [https://console.aws.amazon.com/ec2/v2/home?#Limits](https://console.aws.amazon.com/ec2/v2/home?#Limits).
    You will need to access a **p3.2xlarge** instance. Therefore if your actual limit
    is zero, that should be taken at least to one, using the *Request Limit Increase* form
    (this may take up to 24 hours, but before it''s complete, you won’t be able to
    access this kind of instance).'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get some AWS credits (providing your credit card, for instance).
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After setting your region and having enough credit and request limit increase,
    you can start a **p3.2xlarge** server (a GPU compute server for deep learning
    applications) set up with an OS already containing all the software you need (thanks
    to an AMI, an image prepared by Amazon):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Get to the EC2 Management Console, and click on the **Launch Instance** button.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on AWS Marketplace, and search for **Deep Learning AMI with Source Code
    v2.0 (ami-bcce6ac4)** AMI. This AMI has everything pre-installed: CUDA, cuDNN
    ([https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)), Tensorflow.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the *GPU* compute **p3.2xlarge** instance. This instance has a powerful
    NVIDIA Tesla V100 GPU.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure a security group (which you may call **Jupyter**) by adding **Custom
    TCP Rule**, with TCP protocol, on `port 8888`, accessible from anywhere. This
    will allow you to run a Jupyter server on the machine and see the interface from
    any computer connected to the Internet.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an **Authentication Key Pair**. You can call it `deeplearning_jupyter.pem`
    for instance. Save it on your computer in a directory you can easily access.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the instance. Remember that you will be paying since this moment unless
    you **stop** it from the AWS menu—you still will incur in some costs, but minor
    ones and you will have the instance available for you, with all your data—or simply
    **terminate** it and don’t pay any more for it.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After everything is launched, you can access the server from your computer using
    ssh.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Take notice of the IP of the machine. Let’s say it is `xx.xx.xxx.xxx`, as an
    example.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From a shell pointing to the directory where you `.pem` file is, type:'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssh -i deeplearning_jupyter.pem ubuntu@ xx.xx.xxx.xxx`'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When you have accessed the server machine, configure its Jupyter server by
    typing these commands:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jupyter notebook --generate-config`'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`sed -ie "s/#c.NotebookApp.ip = ''localhost''/#c.NotebookApp.ip = ''*''/g"
    ~/.jupyter/jupyter_notebook_config.py`'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Operate on the server by copying the code (for instance by git cloning the
    code repository) and installing any library you may require. For instance, you
    could install these packages for this specific project:'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sudo pip3 install tqdm`'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`sudo pip3 install conda`'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Launch the Jupyter server by running the command:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jupyter notebook --ip=0.0.0.0 --no-browser`'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this point, the server will run and your ssh shell will prompt you the logs
    from Jupyter. Among the logs, take note of the token (it is something like a sequence
    of numbers and letters).
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open your browser and write in the address bar:'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http:// xx.xx.xxx.xxx:8888/`'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When required type the token and you are ready to use the Jupiter notebook as
    you were on your local machine, but it is actually operating on the server. At
    this point, you will have a powerful server with GPU for running all your experiments
    with GANs.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In concluding this chapter, we would like to thank Udacity and Mat Leonard for
    their DCGAN tutorial, licensed under MIT ([https://github.com/udacity/deep-learning/blob/master/LICENSE](https://github.com/udacity/deep-learning/blob/master/LICENSE))
    which provided a good starting point and a benchmark for this project.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed at length the topic of Generative Adversarial
    Networks, how they work, and how they can be trained and used for different purposes.
    As a project, we have created a conditional GAN, one that can generate different
    types of images, based on your input and we learned how to process some example
    datasets and train them in order to have a pickable class capable of creating
    new images on demand.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
