["```py\npip install gym\npip install gym[atari]\n```", "```py\npip install gym[all]\n```", "```py\nimport gym\n\nenv = gym.make('CartPole-v0')\nenv.reset()\n\nfor _ in range(1000):\n    env.render()\n    action = env.action_space.sample()\n    next_state, reward, done, info = env.step(action)\n    if done:\n        env.reset()\n```", "```py\nfrom multiprocessing import *\nmanager = Manager()\nweight_dict = manager.dict()\nmem_queue = manager.Queue(args.queue_size)\n\npool = Pool(args.processes + 1, init_worker)\n\nfor i in range(args.processes):\n    pool.apply_async(generate_experience_proc, (mem_queue, weight_dict, i))\n\npool.apply_async(learn_proc, (mem_queue, weight_dict))\n\npool.close()\npool.join()\n```", "```py\nenv = gym.make(args.game)\n\nload_net = build_networks(observation_shape, env.action_space.n)\n\nload_net.compile(optimizer='rmsprop', loss='mse', loss_weights=[0.5, 1.])\n\nwhile 'weights' not in weight_dict:\n    time.sleep(0.1)\nload_net.set_weights(weight_dict['weights'])\n```", "```py\nobservation, reward, done, _ = env.step(action)\n```", "```py\nif done or counter >= args.n_step:\n    r = 0.\n    if not done:\n        r = value_net.predict(observations[None, ...])[0]\n    for i in range(counter):\n        r = n_step_rewards[i] + discount * r\n        mem_queue.put((n_step_observations[i], n_step_actions[i], r))\n```", "```py\nload_net.set_weights(weight_dict['weights'])\n```", "```py\nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, Flatten, Dense\n\ndef build_networks(input_shape, output_shape):\n    state = Input(shape=input_shape)\n    h = Conv2D(16, (8, 8) , strides=(4, 4), activation='relu', data_format=\"channels_first\")(state)\n    h = Conv2D(32, (4, 4) , strides=(2, 2), activation='relu', data_format=\"channels_first\")(h)\n    h = Flatten()(h)\n    h = Dense(256, activation='relu')(h)\n\n    value = Dense(1, activation='linear', name='value')(h)\n    policy = Dense(output_shape, activation='softmax', name='policy')(h)\n\n    value_network = Model(inputs=state, outputs=value)\n    policy_network = Model(inputs=state, outputs=policy)\n    train_network = Model(inputs=state, outputs=[value, policy])\n\n    return value_network, policy_network, train_network\n```", "```py\n_, _, train_network = build_networks(observation_shape, env.action_space.n)\nweight_dict['weights'] = train_net.get_weights()\n\nfrom keras import backend as K\n\ndef policy_loss(advantage=0., beta=0.01):\n    def loss(y_true, y_pred):\n        return -K.sum(K.log(K.sum(y_true * y_pred, axis=-1) + \\K.epsilon()) * K.flatten(advantage)) + \\\n           \tbeta * K.sum(y_pred * K.log(y_pred + K.epsilon()))\n    return loss\n\ndef value_loss():\n    def loss(y_true, y_pred):\n        return 0.5 * K.sum(K.square(y_true - y_pred))\n    return loss\n\ntrain_net.compile(optimizer=RMSprop(epsilon=0.1, rho=0.99),\n            loss=[value_loss(), policy_loss(advantage, args.beta)])\n```", "```py\nloss = train_net.train_on_batch([last_obs, advantage], [rewards, targets])\n```", "```py\npip install -r requirements.txt\n\npython 1-train.py --game=Breakout-v0 --processes=16\npython 2-play.py --game=Breakout-v0 --model=model-Breakout-v0-35750000.h5\n```"]