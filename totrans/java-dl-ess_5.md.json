["```py\n<properties>\n   <nd4j.version>0.4-rc3.6</nd4j.version>\n</properties>\n\n<dependencies>\n   <dependency>\n       <groupId>org.nd4j</groupId>\n       <artifactId>nd4j-jblas</artifactId>\n       <version>${nd4j.version}</version>\n   </dependency>\n   <dependency>\n       <groupId>org.nd4j</groupId>\n       <artifactId>nd4j-perf</artifactId>\n       <version>${nd4j.version}</version>\n   </dependency>\n</dependencies>\n```", "```py\n<dependency>\n   <groupId>org.nd4j</groupId>\n   <artifactId>nd4j-jcublas-7.0</artifactId>\n   <version>${nd4j.version}</version>\n</dependency>\n```", "```py\nimport org.nd4j.linalg.api.ndarray.INDArray;\nimport org.nd4j.linalg.factory.Nd4j;\n```", "```py\nINDArray x = Nd4j.create(new double[]{1, 2, 3, 4, 5, 6}, new int[]{3, 2});\nSystem.out.println(x);\n```", "```py\n[[1.00,2.00]\n [3.00,4.00]\n [5.00,6.00]]\n```", "```py\nx.add(1);\n\n```", "```py\n[[2.00,3.00]\n [4.00,5.00]\n [6.00,7.00]]\n\n```", "```py\nINDArray y = Nd4j.create(new double[]{6, 5, 4, 3, 2, 1}, new int[]{3, 2});\n```", "```py\nx.add(y)\nx.sub(y)\nx.mul(y)\nx.div(y)\n```", "```py\n[[7.00,7.00]\n [7.00,7.00]\n [7.00,7.00]]\n[[-5.00,-3.00]\n [-1.00,1.00]\n [3.00,5.00]]\n[[6.00,10.00]\n [12.00,12.00]\n [10.00,6.00]]\n[[0.17,0.40]\n [0.75,1.33]\n [2.50,6.00]]\n\n```", "```py\n[[7.00,7.00]\n [7.00,7.00]\n [7.00,7.00]]\n\n```", "```py\nimport org.nd4j.linalg.api.ndarray.INDArray;\nimport org.nd4j.linalg.factory.Nd4j;\n```", "```py\npublic int nIn;       // dimensions of input data\npublic INDArray w;\n```", "```py\npublic Perceptrons(int nIn) {\n\n   this.nIn = nIn;\n   w = Nd4j.create(new double[nIn], new int[]{nIn, 1});\n\n}\n```", "```py\nINDArray train_X = Nd4j.create(new double[train_N * nIn], new int[]{train_N, nIn});  // input data for training\nINDArray train_T = Nd4j.create(new double[train_N], new int[]{train_N, 1});          // output data (label) for training\n\nINDArray test_X = Nd4j.create(new double[test_N * nIn], new int[]{test_N, nIn});  // input data for test\nINDArray test_T = Nd4j.create(new double[test_N], new int[]{test_N, 1});          // label of inputs\nINDArray predicted_T = Nd4j.create(new double[test_N], new int[]{test_N, 1});     // output data predicted by the model\n```", "```py\ntrain_X.put(i, 0, Nd4j.scalar(g1.random()));\ntrain_X.put(i, 1, Nd4j.scalar(g2.random()));\ntrain_T.put(i, Nd4j.scalar(1));\n```", "```py\n// construct perceptrons\nPerceptrons classifier = new Perceptrons(nIn);\n\n// train models\nwhile (true) {\n   int classified_ = 0;\n\n   for (int i=0; i < train_N; i++) {\n       classified_ += classifier.train(train_X.getRow(i), train_T.getRow(i), learningRate);\n   }\n\n   if (classified_ == train_N) break;  // when all data classified correctly\n\n   epoch++;\n   if (epoch > epochs) break;\n}\n```", "```py\npublic int train(INDArray x, INDArray t, double learningRate) {\n\n   int classified = 0;\n\n   // check if the data is classified correctly\n   double c = x.mmul(w).getDouble(0) * t.getDouble(0);\n\n   // apply steepest descent method if the data is wrongly classified\n   if (c > 0) {\n       classified = 1;\n   } else {\n       w.addi(x.transpose().mul(t).mul(learningRate));\n   }\n\n   return classified;\n}\n```", "```py\n   // check if the data is classified correctly\n   double c = x.mmul(w).getDouble(0) * t.getDouble(0);\n```", "```py\n   double c = 0.;\n\n   // check if the data is classified correctly\n   for (int i = 0; i < nIn; i++) {\n       c += w[i] * x[i] * t;\n   }\n```", "```py\n       w.addi(x.transpose().mul(t).mul(learningRate));\n```", "```py\nfor (int i = 0; i < nIn; i++) {\n   w[i] += learningRate * x[i] * t;\n}\n```", "```py\npublic int predict(INDArray x) {\n\n   return step(x.mmul(w).getDouble(0));\n}\n```", "```py\n<dependency>\n   <groupId>org.deeplearning4j</groupId>\n   <artifactId>deeplearning4j-nlp</artifactId>\n   <version>${dl4j.version}</version>\n</dependency>\n\n<dependency>\n   <groupId>org.deeplearning4j</groupId>\n   <artifactId>deeplearning4j-core</artifactId>\n   <version>${dl4j.version}</version>\n</dependency>\n```", "```py\n<dependency>\n   <groupId>org.nd4j</groupId>\n   <artifactId>nd4j-x86</artifactId>\n   <version>${nd4j.version}</version>\n</dependency>\n```", "```py\n<dependency>\n   <groupId>org.nd4j</groupId>\n   <artifactId>nd4j-jcublas-XXX</artifactId>\n   <version>${nd4j.version}</version>\n</dependency>\n```", "```py\n<dependency>\n   <artifactId>canova-nd4j-image</artifactId>\n   <groupId>org.nd4j</groupId>\n   <version>${canova.version}</version>\n</dependency>\n<dependency>\n   <artifactId>canova-nd4j-codec</artifactId>\n   <groupId>org.nd4j</groupId>\n   <version>${canova.version}</version>\n</dependency>\n```", "```py\nfinal int numRows = 4;\nfinal int numColumns = 1;\nint outputNum = 3;\nint numSamples = 150;\nint batchSize = 150;\nint iterations = 5;\nint splitTrainNum = (int) (batchSize * .8);\nint seed = 123;\nint listenerFreq = 1;\n```", "```py\nDataSetIterator iter = new IrisDataSetIterator(batchSize, numSamples);\n```", "```py\nDataSet next = iter.next();\nnext.normalizeZeroMeanZeroUnitVariance();\n```", "```py\nSplitTestAndTrain testAndTrain = next.splitTestAndTrain(splitTrainNum, new Random(seed));\nDataSet train = testAndTrain.getTrain();\nDataSet test = testAndTrain.getTest();\n```", "```py\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder().layer().layer() … .layer().build();\nMultiLayerNetwork model = new MultiLayerNetwork(conf);\nmodel.init();\n```", "```py\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n   .seed(seed)\n   .iterations(iterations)\n   .learningRate(1e-6f)\n   .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)\n   .l1(1e-1).regularization(true).l2(2e-4)\n   .useDropConnect(true)\n   .list(2)\n```", "```py\n.layer(0, new RBM.Builder(RBM.HiddenUnit.RECTIFIED, RBM.VisibleUnit.GAUSSIAN)\n .nIn(numRows * numColumns)\n .nOut(3)\n .weightInit(WeightInit.XAVIER)\n .k(1)\n .activation(\"relu\")\n .lossFunction(LossFunctions.LossFunction.RMSE_XENT)\n .updater(Updater.ADAGRAD)\n .dropOut(0.5)\n .build()\n)\n```", "```py\n .layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)\n   .nIn(3)\n   .nOut(outputNum)\n   .activation(\"softmax\")\n   .build()\n)\n```", "```py\nmodel.setListeners(Arrays.asList((IterationListener) new ScoreIterationListener(listenerFreq)));\nmodel.fit(train);\n```", "```py\nEvaluation eval = new Evaluation(outputNum);\nINDArray output = model.output(test.getFeatureMatrix());\n```", "```py\neval.eval(test.getLabels(), output);\nlog.info(eval.stats());\n```", "```py\n==========================Scores=====================================\n Accuracy:  0.7667\n Precision: 1\n Recall:    0.7667\n F1 Score:  0.8679245283018869\n=====================================================================\n\n```", "```py\nfor (int i = 0; i < output.rows(); i++) {\n   String actual = test.getLabels().getRow(i).toString().trim();\n   String predicted = output.getRow(i).toString().trim();\n   log.info(\"actual \" + actual + \" vs predicted \" + predicted);\n}\n```", "```py\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n       .seed(seed)\n       .iterations(iterations)\n       .learningRate(1e-6f)\n       .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)\n       .l1(1e-1).regularization(true).l2(2e-4)\n       .useDropConnect(true)\n       .list(3)\n       .layer(0, new RBM.Builder(RBM.HiddenUnit.RECTIFIED, RBM.VisibleUnit.GAUSSIAN)\n                       .nIn(numRows * numColumns)\n                       .nOut(4)\n                       .weightInit(WeightInit.XAVIER)\n                       .k(1)\n                       .activation(\"relu\")\n                       .lossFunction(LossFunctions.LossFunction.RMSE_XENT)\n                       .updater(Updater.ADAGRAD)\n                       .dropOut(0.5)\n                       .build()\n       )\n       .layer(1, new RBM.Builder(RBM.HiddenUnit.RECTIFIED, RBM.VisibleUnit.GAUSSIAN)\n                       .nIn(4)\n                       .nOut(3)\n                       .weightInit(WeightInit.XAVIER)\n                       .k(1)\n                       .activation(\"relu\")\n                       .lossFunction(LossFunctions.LossFunction.RMSE_XENT)\n                       .updater(Updater.ADAGRAD)\n                       .dropOut(0.5)\n                       .build()\n       )\n       .layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)\n                       .nIn(3)\n                       .nOut(outputNum)\n                       .activation(\"softmax\")\n                       .build()\n       )\n       .build();\n```", "```py\nRecordReader recordReader = new CSVRecordReader(0,\",\");\n```", "```py\nrecordReader.initialize(new FileSplit(new ClassPathResource(\"iris.txt\").getFile()));\n```", "```py\nDataSetIterator iterator = new RecordReaderDataSetIterator(recordReader,4,3);\nDataSet next = iterator.next();\n```", "```py\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n       .seed(seed)\n       .iterations(iterations)\n       .constrainGradientToUnitNorm(true).useDropConnect(true)\n       .learningRate(1e-1)\n       .l1(0.3).regularization(true).l2(1e-3)\n       .constrainGradientToUnitNorm(true)\n       .list(3)\n       .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(3)\n               .activation(\"relu\").dropOut(0.5)\n               .weightInit(WeightInit.XAVIER)\n               .build())\n       .layer(1, new DenseLayer.Builder().nIn(3).nOut(2)\n               .activation(\"relu\")\n               .weightInit(WeightInit.XAVIER)\n               .build())\n       .layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n               .weightInit(WeightInit.XAVIER)\n               .activation(\"softmax\")\n               .nIn(2).nOut(outputNum).build())\n       .backprop(true).pretrain(false)\n       .build();\n```", "```py\nMultiLayerNetwork model = new MultiLayerNetwork(conf);\nmodel.init();\n```", "```py\nSplitTestAndTrain testAndTrain = next.splitTestAndTrain(splitTrainNum, new Random(seed));\n```", "```py\nnext.shuffle();\nSplitTestAndTrain testAndTrain = next.splitTestAndTrain(0.6);\n```", "```py\nmodel.fit(testAndTrain.getTrain());\n```", "```py\nEvaluation eval = new Evaluation(3);\nDataSet test = testAndTrain.getTest();\nINDArray output = model.output(test.getFeatureMatrix());\neval.eval(test.getLabels(), output);\nlog.info(eval.stats());\n```", "```py\n==========================Scores=====================================\n Accuracy:  1\n Precision: 1\n Recall:    1\n F1 Score:  1.0\n=====================================================================\n\n```", "```py\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n       .seed(seed)\n       .iterations(iterations)\n       .constrainGradientToUnitNorm(true).useDropConnect(true)\n       .learningRate(0.01)\n       .l1(0.0).regularization(true).l2(1e-3)\n       .constrainGradientToUnitNorm(true)\n       .list(4)\n       .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(4)\n               .activation(\"relu\").dropOut(0.5)\n               .weightInit(WeightInit.XAVIER)\n               .build())\n       .layer(1, new DenseLayer.Builder().nIn(4).nOut(4)\n               .activation(\"relu\").dropOut(0.5)\n               .weightInit(WeightInit.XAVIER)\n               .build())\n       .layer(2, new DenseLayer.Builder().nIn(4).nOut(4)\n               .activation(\"relu\").dropOut(0.5)\n               .weightInit(WeightInit.XAVIER)\n               .build())\n       .layer(3, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n               .weightInit(WeightInit.XAVIER)\n               .activation(\"softmax\")\n               .nIn(4).nOut(outputNum).build())\n       .backprop(true).pretrain(false)\n       .build();\n```", "```py\nint numRows = 28;\nint numColumns = 28;\nint nChannels = 1;\nint outputNum = 10;\nint numSamples = 2000;\nint batchSize = 500;\nint iterations = 10;\nint splitTrainNum = (int) (batchSize*.8);\nint seed = 123;\nint listenerFreq = iterations/5;\n```", "```py\nList<INDArray> testInput = new ArrayList<>();\nList<INDArray> testLabels = new ArrayList<>();\n```", "```py\nDataSetIterator mnistIter = new MnistDataSetIterator(batchSize,numSamples, true);\n```", "```py\nMultiLayerConfiguration.Builder builder = new NeuralNetConfiguration.Builder().layer().layer(). … .layer()\nnew ConvolutionLayerSetup(builder,numRows,numColumns,nChannels);\nMultiLayerConfiguration conf = builder.build();\nMultiLayerNetwork model = new MultiLayerNetwork(conf);\nmodel.init();\n```", "```py\n.layer(0, new ConvolutionLayer.Builder(10, 10)\n       .stride(2,2)\n       .nIn(nChannels)\n       .nOut(6)\n       .weightInit(WeightInit.XAVIER)\n       .activation(\"relu\")\n       .build())\n```", "```py\n.layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX, new int[] {2,2})\n       .build())\n```", "```py\n.layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n       .nOut(outputNum)\n       .weightInit(WeightInit.XAVIER)\n       .activation(\"softmax\")\n       .build())\n```", "```py\nmodel.setListeners(Arrays.asList((IterationListener) new ScoreIterationListener(listenerFreq)));\nwhile(mnistIter.hasNext()) {\n   mnist = mnistIter.next();\n   trainTest = mnist.splitTestAndTrain(splitTrainNum, new Random(seed));\n   trainInput = trainTest.getTrain();\n   testInput.add(trainTest.getTest().getFeatureMatrix());\n   testLabels.add(trainTest.getTest().getLabels());\n   model.fit(trainInput);\n}\n```", "```py\nfor(int i = 0; i < testInput.size(); i++) {\n   INDArray output = model.output(testInput.get(i));\n   eval.eval(testLabels.get(i), output);\n}\n```", "```py\nlog.info(eval.stats());\n```", "```py\n==========================Scores=====================================\n Accuracy:  0.832\n Precision: 0.8783\n Recall:    0.8334\n F1 Score:  0.8552464933704985\n=====================================================================\n\n```", "```py\nMultiLayerConfiguration.Builder builder = new NeuralNetConfiguration.Builder()\n       .seed(seed)\n       .batchSize(batchSize)\n       .iterations(iterations)\n       .regularization(true).l2(0.0005)\n       .learningRate(0.01)\n       .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n       .updater(Updater.NESTEROVS).momentum(0.9)\n       .list(6)\n       .layer(0, new ConvolutionLayer.Builder(5, 5)\n               .nIn(nChannels)\n               .stride(1, 1)\n               .nOut(20).dropOut(0.5)\n               .weightInit(WeightInit.XAVIER)\n               .activation(\"relu\")\n               .build())\n       .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX, new int[]{2, 2})\n               .build())\n       .layer(2, new ConvolutionLayer.Builder(5, 5)\n               .nIn(20)\n               .nOut(50)\n               .stride(2,2)\n               .weightInit(WeightInit.XAVIER)\n               .activation(\"relu\")\n               .build())\n       .layer(3, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX, new int[]{2, 2})\n               .build())\n       .layer(4, new DenseLayer.Builder().activation(\"tanh\")\n               .nOut(500).build())\n       .layer(5, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n               .nOut(outputNum)\n               .weightInit(WeightInit.XAVIER)\n               .activation(\"softmax\")\n               .build())\n       .backprop(true).pretrain(false);\nnew ConvolutionLayerSetup(builder,28,28,1);\n```", "```py\n==========================Scores=====================================\n Accuracy:  0.8656\n Precision: 0.8827\n Recall:    0.8645\n F1 Score:  0.873490476878917\n=====================================================================\n\n```"]