- en: Text classification Using Long Short-Term Memory Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we used a recurrent neural network to develop a movie
    review sentiment classification model for text data that are characterized by
    a sequence of words. **Long Short-Term Memory** (**LSTM**) neural networks are
    a special type of **Recurrent Neural Networks** (**RNNs**) that are useful with
    data involving sequences and provide advantages that we will discuss in the next
    section. This chapter illustrates the steps for using an LSTM neural network for
    sentiment classification. The steps involved in applying an LSTM network to a
    business problem may include text data preparation, creating the LSTM model, training
    the model, and assessing the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we use LSTM networks?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing text data for model building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a long short-term memory network model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting the LSTM model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why do we use LSTM networks?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen, in the previous chapter, that recurrent neural networks provide
    decent performance when working with data involving sequences. One of the key
    advantages of using LSTM networks lies in the fact that they address the vanishing
    gradient problem that makes network training difficult for a long sequence of
    words or integers. Gradients are used for updating RNN parameters and for a long
    sequence of words or integers; these gradients become smaller and smaller to the
    extent that, effectively, no network training can take place. LSTM networks help
    to overcome this problem and make it possible to capture long-term dependencies
    between keywords or integers in sequences that are separated by a large distance.
    For example, consider the following two sentences, where the first sentence is
    short and the second sentence is relatively longer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentence-1**: I like to eat chocolates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence-2**: I like, whenever there is a chance and usually there are many
    of them, to eat chocolates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In these sentences, the two important words that capture the main essence of
    the sentence are **like** and **chocolates**. In the first sentence, the words
    **like** and **chocolates** are closer to each other and they are separated by
    just two words in between. On the other hand, in the second sentence, these two
    words are separated by as many as 14 words that lie between them. LSTM networks
    are designed to deal with such long-term dependencies that are observed in longer
    sentences or longer sequences of integers. In this chapter, we focus on applying
    LSTM networks for developing a movie review sentiment classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing text data for model building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will continue to use IMDB movie review data that we used in the previous
    chapter on recurrent neural networks. This data is already available in a format
    where we can use it for developing deep network models with minimum need for data
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The sequence of integers capturing train and test data is stored in `train_x` and `test_x` respectively.
    Similarly, `train_y` and `test_y` store labels capturing information about whether
    movie reviews are positive or negative. We have specified the number of most frequent
    words to be 500\. For padding, we are using 200 as the maximum length of a sequence
    of integers for both train and test data.
  prefs: []
  type: TYPE_NORMAL
- en: When the actual length of integers is less than 200, then zeros get added at
    the beginning of the sequence to artificially increase the length of integers
    to 200\. However, when the length of integers is more than 200, integers at the
    beginning are removed so that the total length of integers is maintained at 200.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, both train and test datasets are balanced and contain
    data involving 25,000 movie reviews each. For each movie review, positive or negative
    labels are also available.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the choice of value for `maxlen` can impact model performance. If
    the value chosen is too small, more words or integers in a sequence will get truncated.
    On the other hand, if the value chosen is too large, then more words or integers
    in a sequence will need padding, with zeroes getting added. One way to avoid too
    much padding or too much truncation is to choose a value closer to the median.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a long short-term memory network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will start with a simple LSTM network architecture and look
    at calculations to arrive at the number of parameters. Subsequently, we will compile
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM network architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start with a simple flow chart of the LSTM network architecture, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac568b12-cbe5-4312-b47e-1e95783f27ee.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding flow chart for the LSTM network highlights the layers in the architecture
    and activation functions used. In the LSTM layer, the `tanh` activation function
    is used which is the default activation function for the layer. In the dense layer, the
    `sigmoid` activation function is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following code and summary of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Apart from what we used for the RNN model in the last chapter, we are replacing
    `layer_simple_rnn` with `layer_lstm` for the LSTM network in this example. For
    the embedding layer, we have a total of 16,000 (500 x 32) parameters. The calculation
    shown as follows calculates the number of parameters for the LSTM layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*=4 x [units in LSTM layer x (units in LSTM layer + output dimension) + units
    in LSTM layer] *'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 4 x [32(32+32) + 32]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 8320*'
  prefs: []
  type: TYPE_NORMAL
- en: For a similar architecture involving the RNN layer, we will have 2,080 parameters.
    The four-fold increase in the number of parameters for the LSTM layer also leads
    to more training time and hence requires relatively higher processing costs. The
    number of parameters for the dense layer is *[(32x1) + 1]*, which comes to 33\.
    Hence, overall there are 24,353 parameters in this network.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the LSTM network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For compiling the LSTM network model, we will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We are using `rmsprop` as optimizer and `binary_crossentropy` for loss, since
    movie reviews have a binary response or, in other words, they are either positive
    or negative. For metrics, we are making use of classification accuracy. After
    compiling the model, we are ready to go to the next step of fitting the LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For training the LSTM model, we will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We will use train data to fit the LSTM model with ten epochs and use a batch
    size of 128\. We will also reserve 20% of train data as validation data for assessing
    loss and accuracy values during model training.
  prefs: []
  type: TYPE_NORMAL
- en: Loss and accuracy plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following screenshot shows the loss and accuracy plot for `model_one`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e6e0763-3476-4566-b4ab-2006bc2b2e06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The plot for loss and accuracy based on training and validation data shows
    overall closeness between the curves. The observations from the plot are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: There is no major divergence between the two lines, which indicates the lack
    of an over-fitting problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An increase in the number of epochs may not provide any significant improvement
    in model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the loss and accuracy values based on the validation data show some
    amount of unevenness or oscillation where they deviate from the training loss
    and accuracy by a relatively high amount.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Epochs 4 and 8 especially stand out in this regard showing significant deviation
    from the loss and accuracy based on training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will move toward evaluating `model_one` and use it for prediction of
    the movie review sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will evaluate the model based on both training and test
    data. We will also create a confusion matrix for both train and test data to gain
    further insights into the movie review sentiment classification performance of
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation with train data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will first evaluate the model performance with train data using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen from the preceding output, for the training data, we obtain a loss
    value of `0.375` and an accuracy of about `0.828`. This is a decent performance
    considering a relatively simple LSTM architecture. We next use this model to make
    predictions for the movie review sentiment and summarize the results by developing
    a confusion matrix using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can make the following observations from the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: It is observed that this model seems to be more accurate in predicting positive
    movie reviews (11,430 correct predictions) compared to negative movie reviews
    (9,258 correct predictions). In other words, this model correctly classifies positive
    reviews at the rate of about 91.4% (also called the sensitivity of the model)
    for the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, this model correctly classifies negative reviews at the rate of about
    74.1% (also called specificity of the model) for the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also observed that the negative movie reviews are being misclassified
    as a positive review at the rate of about three times (3,242 reviews) more compared
    to a positive review being misclassified as negative (1,070 reviews).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, although overall, this model seems to perform well for the training data,
    looking deeper, we observe some bias toward correctly classifying positive movie
    reviews at the cost of lower accuracy in correctly classifying negative reviews.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will be interesting to see whether the model performance observed, based
    on training data, results in similar behavior for the test data or not.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation with test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now use the test data to obtain loss and accuracy values for the model
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As seen from the preceding output, for the test data, we obtain a loss value
    of 0.399 and an accuracy of about 0.819\. These values, as expected, are slightly
    inferior to those obtained for the train data. However, they are close enough
    to results based on the train data to consider this model behavior consistent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to obtain a confusion matrix using test data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'From the confusion matrix shown above, the following observations can be made:'
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix based on predictions using the test data shows a similar
    pattern that we observed earlier for the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model also seems to perform better when accurately classifying positive
    movie reviews (at a rate of about 90.7%), compared to correctly classifying negative
    reviews (at a rate of about 73.3%).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, the model continues to show bias in the performance when correctly classifying
    positive movie reviews.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will carry out some experimentation to explore possible
    improvements for the model's movie review sentiment classification performance.
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will carry out three different experiments to search for
    an improved LSTM based movie review sentiment classification model. This will
    involve trying a different optimizer at the time of compiling the model, adding
    another LSTM layer when developing the model architecture, and using a bidirectional
    LSTM layer in the network.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with the Adam optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the `adam` (Adaptive Moment Optimization) optimizer instead of
    the `rmsprop` (Root Mean Square Propagation) optimizer that we used earlier when
    compiling the model. To make a comparison of model performance easier, we will
    keep everything else the same as earlier, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding codes and training the model, the accuracy and
    loss values for each epoch are stored in `model_two`. We use the loss and accuracy
    values in `model_two` to develop the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c80e3bf-c6f5-4c71-b9a8-008cd2ff70c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding loss and accuracy plot, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy plot based on the training and validation data shows a
    slightly improved pattern compared to the plot for the first model that we built
    with `model_one`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the plot based on `model_one`, we observed that loss and accuracy values
    for validation data occasionally showed major deviations from the values based
    on the training data. In this plot, we do not see any such major deviation between
    the two lines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, the loss and accuracy values based on the last few values of validation
    data seem flat, suggesting that the ten epochs that we have used are sufficient
    to train the model and an increasing number of epochs is not likely to help in
    improving the model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, let''s obtain the loss, accuracy, and confusion matrix for the training
    data using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: By using the `adam` optimizer, we obtain loss and accuracy for training data
    as 0.360 and 0.843 respectively. Both these numbers show an improvement compared
    to the earlier model where we had used the `rmsprop` optimizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another difference can be observed from the confusion matrix. This model performs
    better when correctly classifying negative movie reviews (at a rate of about 88.9%)
    compared to the correct classification of positive reviews (at a rate of about
    79.7%).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This behavior is the opposite of what was observed in the previous model. This
    model seems to be biased toward correctly classifying negative movie review sentiment
    compared to correctly classifying positive reviews.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Having reviewed the performance of the model using the training data, we will
    now repeat the process with the test data, with the following code for obtaining
    the loss, accuracy, and confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy based on test data are 0.385 and 0.829 respectively. These
    results, based on the test data, also show better model performance compared to
    the previous model with the test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The confusion matrix shows a similar pattern that we observed for the training
    data. Negative movie review sentiments are correctly classified at a rate of about
    86.9% for the test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, positive movie review sentiments are correctly classified by the
    model at a rate of about 78.8% for the test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This behavior is consistent with the model performance that was obtained using
    the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although trying the `adam` optimizer improves overall movie review sentiment
    classification performance, it still retains bias when correctly classifying one
    category compared to the other. A good model should not only improve the overall
    performance, but it should also minimize any bias when correctly classifying a
    category. The following code provides a table showing the number of negative and
    positive reviews in the `train` and `test` data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: It can be seen from the preceding output of code that this movie review data
    is balanced where both train and test data has 25,000 reviews each. This data
    is also balanced in terms of the number of positive or negative reviews. Both
    train and test datasets have 12,500 positive and 12,500 negative movie reviews
    each. Hence, there is no bias in the amount of negative or positive reviews provided
    to the model for training. However, the bias seen when correctly classifying negative
    and positive movie reviews is certainly something that needs improvement.
  prefs: []
  type: TYPE_NORMAL
- en: In the next experiment, let's explore with more LSTM layers and see whether
    or not we can obtain a better movie review sentiment classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with the LSTM network having an additional layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this second experiment to improve the performance of the classification
    model, we will add an extra LSTM layer. Let''s have a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: By adding an extra LSTM layer to the network, as shown in the preceding code,
    the total number of parameters with these two LSTM layers will now increase to
    32,673 compared to 24,353 that we had previously with one LSTM layer. This increase
    in the number of parameters will also lead to higher training time when training
    the network. We are also retaining the use of the Adam optimizer when compiling
    the model. We are keeping everything else the same as what we had used in the
    previous model.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple flow chart for the network architecture with two LSTM layers used
    in this experiment, is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0418119a-4dae-4bfa-a9b1-0897d0815d18.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding flow chart shown for the LSTM network highlights the two layers
    in the architecture and activation functions used. In both LSTM layers, `tanh`
    is used as the default activation function. In the dense layer, we continue to
    use the `sigmoid` activation function that we used earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'After training the model, the accuracy and loss values for each epoch is stored
    in `model_three`. We use the loss and accuracy values in `model_three` to develop
    the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9cc7906f-0224-47de-8ec8-ef9ed8d7b438.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the loss and accuracy plot shown, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The plot for loss and accuracy values doesn't indicate the presence of an over-fitting
    problem since the curves for the training and validation data are close to each
    other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As in the earlier model, the loss and accuracy for the validation data seem
    to remain flat for the last few epochs, indicating ten epochs are sufficient for
    training the model, and increasing the number of epochs is not likely to improve
    results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can now obtain the loss, accuracy, and confusion matrix for the training
    data using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy values based on training data are obtained as `0.339`
    and `0.855` respectively. Both loss and accuracy show improvement compared to
    the earlier two models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use this model to make predictions for each review in the training data,
    compare them with actual labels, and then summarize the results in the form of
    a confusion matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the training data, the confusion matrix shows that the model correctly classifies
    negative movie reviews about 90% of the time and correctly classifies positive
    reviews about 81% of the time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, although there is an overall improvement in the model performance, we continue
    to observe bias when correctly classifying one category compared to the other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After reviewing the performance of the model using training data, we will now
    repeat the process with the test data. Following is the code for obtaining the
    loss, accuracy, and confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: For the test data, the loss and accuracy values are 0.376 and 0.837 respectively.
    Both results show a better classification performance compared to the previous
    two models for the test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The confusion matrix shows that negative movie reviews are correctly classified
    at a rate of about 87.3%, and positive reviews are correctly classified at a rate
    of about 80%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, these results are consistent with those obtained using the training data
    and show a similar bias to that we observed for the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To summarize, by adding an extra LSTM layer, we were able to improve the movie
    review sentiment classification performance of the model. However, we continue
    to observe bias when correctly classifying one category compared to the other
    category. Hence, although we obtained moderate success in improving model performance,
    there is scope to further improve the classification performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with a bidirectional LSTM layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A bidirectional LSTM, as the name indicates, not only uses the sequence of integers
    provided as input but also makes use of its reverse order as additional input.
    There could be situations where this approach may help to achieve further model
    classification performance improvements by capturing useful patterns in the data
    that may not have been captured by the original LSTM network.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this experiment, we will modify the LSTM layer in the first experiment,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: We converted the LSTM layer into a bidirectional LSTM layer using the bidirectional
    `()` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This change doubles the number of parameters related to the LSTM layer to 16,640,
    as can be seen from the model summary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total number of parameters for this architecture now increases to 32,705\.
    This increase in the number of parameters will further reduce the speed at which
    the network will be trained.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a simple flow chart for the bidirectional LSTM network architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c0b3651-c617-4563-816c-1469d82c371b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The flow chart for the bidirectional LSTM network shows embedding, bidirectional,
    and dense layers. In the bidirectional LSTM layer, `tanh` is used as the activation
    function and the dense layer uses the `sigmoid` activation function. The code
    for compiling and training the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As seen from the preceding code, we will continue to use the `adam` optimizer
    and keep the other settings the same as earlier for compiling and then fitting
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we train the model, the accuracy and loss values for each epoch are stored
    in `model_four`. We use the loss and accuracy values in `model_four` to develop
    the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab926820-55f4-4c95-8056-ad5ee3a57beb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy plot doesn't show any cause for concern regarding over-fitting
    as the lines for training and validation are reasonably close to each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The plot also shows that we do not need more than ten epochs to train this model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will obtain the loss, accuracy, and confusion matrix for the training data
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: For the training data, we obtain loss and accuracy values of 0.341 and 0.852
    respectively. These results are only marginally inferior to the previous results
    and are not significantly different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The confusion matrix this time shows a more even performance for correctly classifying
    positive and negative movie reviews.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For negative movie reviews, the correct classification rate is about 84.8% and
    for positive reviews, it is about 85.7%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This difference of about 1% is much smaller than what we observed for the earlier
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will now repeat the preceding process with the test data. Following is the
    code for obtaining the loss, accuracy, and confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: For the test data, the loss and accuracy values are 0.374 and 0.834 respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The confusion matrix shows that the negative reviews are correctly classified
    by the model at a rate of about 82.8%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model correctly classifies positive movie reviews at a rate of about 84.1%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These results are consistent with those obtained for the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The experiment with bidirectional LSTM helped to obtain somewhat comparable
    performance in terms of loss and accuracy than that were obtained with two LSTM
    layers in the previous experiment. However, the main gain that is observed is
    in achieving results where we can correctly classify a negative or positive movie
    review with much better consistency.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we used the LSTM network to develop a movie review sentiment
    classification model. When data involves sequences, LSTM networks help to capture
    long term dependencies in the sequence of words or integers. We experimented with
    four different LSTM models by making some changes to the model and the results
    for the same are summarized in the following table.
  prefs: []
  type: TYPE_NORMAL
- en: 'This table summarizes the performance of the four LSTM models:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **LSTM Layers** | **Optimizer** | **Data** | **Loss** | **Accuracy**
    | **Accuracy for Negative Reviews or Specificity** | **Accuracy for Positive Reviews
    or Sensitivity** |'
  prefs: []
  type: TYPE_TB
- en: '| One | 1 | `rmsprop` | Train | 0.375 | 82.8% | 74.1% | 91.4% |'
  prefs: []
  type: TYPE_TB
- en: '|   |   |   | Test | 0.399 | 81.9% | 73.3% | 90.7% |'
  prefs: []
  type: TYPE_TB
- en: '| Two | 1 | `adam` | Train | 0.360 | 84.3% | 88.9% | 79.7% |'
  prefs: []
  type: TYPE_TB
- en: '|   |   |   | Test | 0.385 | 82.9% | 86.9% | 78.8% |'
  prefs: []
  type: TYPE_TB
- en: '| Three | 2 | `adam` | Train | 0.339 | 85.5% | 90.0% | 81.0% |'
  prefs: []
  type: TYPE_TB
- en: '|   |   |   | Test | 0.376 | 83.7% | 87.3% | 80.0% |'
  prefs: []
  type: TYPE_TB
- en: '| Four | bidirectional | `adam` | Train | 0.341 | 85.2% | 84.8% | 85.7% |'
  prefs: []
  type: TYPE_TB
- en: '|   |   |   | Test | 0.374 | 83.4% | 82.8% | 84.1% |'
  prefs: []
  type: TYPE_TB
- en: 'We can make the following observations from the preceding table:'
  prefs: []
  type: TYPE_NORMAL
- en: Out of the four models that were tried, the bidirectional LSTM model provided
    better performance compared to the other three models. It has the lowest loss
    value based on test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although overall accuracy is slightly lower for the fourth model compared to
    the third model, accuracy for correctly classifying negative and positive reviews
    is much more consistent, varying from 82.8% to 84.1%, or a spread of only about
    1.3%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third model seems biased toward negative reviews that correctly classifies
    such reviews at a rate of 87.3% for the test data. For the third model, the correct
    classification of positive reviews in the test data is only at 80%. Hence, the
    spread between the correct classification of negative and positive reviews for
    the third model is more than 7%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The spread between sensitivity and specificity is even higher for the first
    two models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the fourth model provides good results, additional improvements can
    certainly be explored by experimenting further with other variables. Variables
    that can be used for further experiments may include the number of most frequent
    words, use of pre versus post for padding and/or truncation, the maximum length
    used for padding, the number of units in the LSTM layer, and the choice of another
    optimizer at the time of compiling the model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we illustrated the use of LSTM networks for developing a movie
    review sentiment classification model. One of the problems faced by recurrent
    neural networks that we used in the previous chapter is that it involves difficulty
    in capturing long-term dependency that may exist between two words/integers in
    a sequence of words or integers. **Long Short-Term Memory** (**LSTM**) networks
    are designed to artificially retain long-term memories that are important when
    dealing with long sentences or a long sequence of integers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue to work with text data and explore the
    use of **Convolutional Recurrent Neural Network****s** (**CRNNs**), which combine
    the benefits of **Convolutional Neural Networks** (**CNNs**) and **Recurrent Neural
    Networks** (**RNNs**) into a single network. We will illustrate the use of this
    type of network with the help of an interesting and publicly available text dataset, `reuter_50_50`.
  prefs: []
  type: TYPE_NORMAL
