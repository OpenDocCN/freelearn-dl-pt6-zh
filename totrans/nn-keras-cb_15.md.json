["```py\nimport sys, re, numpy as np, pandas as pd, music21, IPython, pickle, librosa, librosa.dsiplay, os\nfrom glob import glob\nfrom tqdm import tqdm\nfrom keras.utils import np_utils\n```", "```py\nsong_specs=[]\ngenres = []\nfor genre in os.listdir('...'): # Path to genres folder\n  song_folder = '...' # Path to songs folder\n  for song in os.listdir(song_folder):\n    if song.endswith('.au'):\n      signal, sr = librosa.load(os.path.join(song_folder, song), sr=16000)\n      melspec = librosa.feature.melspectrogram(signal, sr=sr).T[:1280,]\n      song_specs.append(melspec)\n      genres.append(genre)\n      print(song)\n  print('Done with:', genre)\n```", "```py\nplt.subplot(121)\nlibrosa.display.specshow(librosa.power_to_db(song_specs[302].T),\n y_axis='mel',\n x_axis='time',)\nplt.title('Classical audio spectrogram')\nplt.subplot(122)\nlibrosa.display.specshow(librosa.power_to_db(song_specs[402].T),\n y_axis='mel',\n x_axis='time',)\nplt.title('Rock audio spectrogram')\n```", "```py\nsong_specs = np.array(song_specs)\n\nsong_specs2 = []\nfor i in range(len(song_specs)):\n     tmp = song_specs[i]\n     song_specs2.append(tmp[:900][:])\nsong_specs2 = np.array(song_specs2)\n```", "```py\ngenre_one_hot = pd.get_dummies(genres)\n```", "```py\nx_train, x_test, y_train, y_test = train_test_split(song_specs2, np.array(genre_one_hot),test_size=0.1,random_state = 42)\n```", "```py\ninput_shape = (1280, 128)\ninputs = Input(input_shape)\nx = inputs\nlevels = 64\nfor level in range(7):\n     x = Conv1D(levels, 3, activation='relu')(x)\n     x = BatchNormalization()(x)\n     x = MaxPooling1D(pool_size=2, strides=2)(x)\n     levels *= 2\n     x = GlobalMaxPooling1D()(x)\nfor fc in range(2):\n     x = Dense(256, activation='relu')(x)\n     x = Dropout(0.5)(x)\nlabels = Dense(10, activation='softmax')(x)\n```", "```py\nmodel = Model(inputs=[inputs], outputs=[labels])\nadam = keras.optimizers.Adam(lr=0.0001)\nmodel.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])\n```", "```py\nhistory = model.fit(x_train, y_train,batch_size=128,epochs=100,verbose=1,validation_data=(x_test, y_test))\n```", "```py\nfrom keras.models import Model\nlayer_name = 'dense_14'\nintermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\nintermediate_output = intermediate_layer_model.predict(song_specs2)\n```", "```py\nfrom sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0)\ntsne_img_label = tsne_model.fit_transform(intermediate_output)\ntsne_df = pd.DataFrame(tsne_img_label, columns=['x', 'y'])\ntsne_df['image_label'] = genres\n```", "```py\nfrom ggplot import *\nchart = ggplot(tsne_df, aes(x='x', y='y', color='genres'))+ geom_point(size=70,alpha=0.5)\nchart\n```", "```py\n!pip install mido music21\nimport mido, glob, os\nfrom mido import MidiFile, MidiTrack, Message\nimport numpy as np\nfrom music21 import converter, instrument, note, chord\nfrom keras.utils import np_utils\nfrom keras.layers import Input, LSTM, Dropout, Dense, Activation\nfrom keras.models import Model\n\nfname = '/content/nintendo.mid'\n```", "```py\nmidi = converter.parse(fname)\n```", "```py\ndef parse_with_silence(midi=midi):\n     notes = []\n     notes_to_parse = None\n     parts = instrument.partitionByInstrument(midi)\n     if parts: # file has instrument parts\n         notes_to_parse = parts.parts[0].recurse()\n     else: # file has notes in a flat structure\n         notes_to_parse = midi.flat.notes\n     for ix, element in enumerate(notes_to_parse):\n         if isinstance(element, note.Note):\n             _note = str(element.pitch)\n             notes.append(_note)\n         elif isinstance(element, chord.Chord):\n             _note = '.'.join(str(n) for n in element.normalOrder)\n             notes.append(_note)\n         elif isinstance(element, note.Rest):\n             _note = '#'+str(element.seconds)\n             notes.append(_note)\n     return notes\n```", "```py\nnotes = parse_with_silence()\n```", "```py\n# get all unique values in notes\npitchnames = sorted(set(item for item in notes))\n# create a dictionary to map pitches to integers\nnote_to_int = dict((note, number) for number, note in enumerate(pitchnames))\nnetwork_input = []\nnetwork_output = []\n```", "```py\nsequence_length = 100\nfor i in range(0, len(notes) - sequence_length, 1):\n     sequence_in = notes[i:i + sequence_length]\n     sequence_out = notes[i + sequence_length]\n     network_input.append([note_to_int[char] for char in sequence_in])\n     network_output.append(note_to_int[sequence_out])\n```", "```py\nn_patterns = len(network_input)\n# reshape the input into a format compatible with LSTM layers\nnetwork_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n# normalize input\nnetwork_input = network_input / np.max(network_input)\nnetwork_output = np_utils.to_categorical(network_output)\n\nN = 9 * len(network_input)//10\nprint(network_input.shape, network_output.shape)\n# (36501, 100, 1) (36501, 50)\n```", "```py\nmodel.fit(network_input, network_output, epochs=100, batch_size=32, verbose = 1)\n```", "```py\nfrom tqdm import trange\nprint('generating prediction stream...')\nstart = np.random.randint(0, len(network_input)-1)\nint_to_note = dict((number, note) for number, note in enumerate(pitchnames))\npattern = network_input[start].tolist()\nprediction_output = []\n```", "```py\nfor note_index in trange(500):\n     prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n     prediction = model.predict(prediction_input, verbose=0)\n     index = np.argmax(prediction)\n     result = int_to_note[index]\n     prediction_output.append(result)\n     pattern.append([index/49])\n     pattern = pattern[1:len(pattern)]\n```", "```py\noffset = 0\noutput_notes = []\n\n# create note and chord objects based on the values generated by the model\nprint('creating notes and chords')\nfor pattern in prediction_output:\n\n    # pattern is a chord\n    if (('.' in pattern) or pattern.isdigit()) and pattern[0]!='#':\n        notes_in_chord = pattern.split('.')\n        notes = []\n        for current_note in notes_in_chord:\n            new_note = note.Note(int(current_note))\n            new_note.storedInstrument = instrument.Piano()\n            notes.append(new_note)\n        new_chord = chord.Chord(notes)\n        new_chord.offset = offset\n        output_notes.append(new_chord)\n\n    # pattern is a note\n    elif pattern[0]!='#':\n        new_note = note.Note(pattern)\n        new_note.offset = offset\n        new_note.storedInstrument = instrument.Piano()\n        output_notes.append(new_note)\n\n    # pattern is silence\n    else:\n        new_note = note.Rest()\n        new_note.offset = offset\n        new_note.storedInstrument = instrument.Piano()\n        new_note.quarterLength = float(pattern[1:])\n        output_notes.append(new_note)\n    # increase offset each iteration so that notes do not stack\n    offset += 0.5\n```", "```py\nfrom music21 import stream\nmidi_stream = stream.Stream(output_notes)\nmidi_stream.write('midi', fp='OP.mid')\n```", "```py\n$wget http://www.openslr.org/resources/12/train-clean-100.tar.gz\n$tar xzvf train-clean-100.tar.gz\n\nimport librosa\nimport numpy as np\nimport pandas as pd\n```", "```py\nimport os, numpy as np\norg_path = '/content/LibriSpeech/train-clean-100/'\ncount = 0\ninp = []\nk=0\naudio_name = []\naudio_trans = []\nfor dir1 in os.listdir(org_path):\n     dir2_path = org_path+dir1+'/'\n     for dir2 in os.listdir(dir2_path):\n     dir3_path = dir2_path+dir2+'/'\n\n     for audio in os.listdir(dir3_path):\n         if audio.endswith('.txt'):\n             k+=1\n             file_path = dir3_path + audio\n             with open(file_path) as f:\n                 line = f.readlines()\n                 for lines in line:\n                     audio_name.append(dir3_path+lines.split()[0]+'.flac')\n                     words2 = lines.split()[1:]\n                     words4=' '.join(words2)\n                     audio_trans.append(words4)\n```", "```py\nimport re\nlen_audio_name=[]\nfor i in range(len(audio_name)):\n     tmp = audio_trans[i]\n     len_audio_name.append(len(tmp))\n```", "```py\nfinal_audio_name = []\nfinal_audio_trans = []\nfor i in range(len(audio_name)):\n     if(len_audio_name[i]<100):\n         final_audio_name.append(audio_name[i])\n         final_audio_trans.append(audio_trans[i])\n```", "```py\ninp = []\ninp2 = []\nop = []\nop2 = []\nfor j in range(len(final_audio_name)):\n     t = librosa.core.load(final_audio_name[j],sr=16000, mono= True) \n     if(t[0].shape[0]<160000):\n         t = np.fft.rfft(t[0])\n         t2 = np.zeros(160000)\n         t2[:len(t)] = t\n         inp = []\n         for i in range(t2.shape[0]//160):\n             inp.append(t2[(i*160):((i*160)+320)])\n             inp2.append(inp)\n             op2.append(final_audio_trans[j])\n```", "```py\nimport itertools\nlist2d = op2\ncharList = list(set(list(itertools.chain(*list2d))))\n```", "```py\nnum_audio = len(op2)\ny2 = []\ninput_lengths = np.ones((num_audio,1))*243\nlabel_lengths = np.zeros((num_audio,1))\nfor i in range(num_audio):\n     val = list(map(lambda x: charList.index(x), op2[i]))\n     while len(val)<243:\n         val.append(29)\n     y2.append(val)\n     label_lengths[i] = len(op2[i])\n     input_lengths[i] = 243\n```", "```py\nimport keras.backend as K\ndef ctc_loss(args):\n    y_pred, labels, input_length, label_length = args\n    return K.ctc_batch_cost(labels, y_pred, input_length, label_length\n```", "```py\ninput_data = Input(name='the_input', shape = (999,161), dtype='float32')\ninp = BatchNormalization(name=\"inp\")(input_data)\nconv= Conv1D(filters=220, kernel_size = 11,strides = 2, padding='valid',activation='relu')(inp)\nconv = BatchNormalization(name=\"Normal0\")(conv)\nconv1= Conv1D(filters=220, kernel_size = 11,strides = 2, padding='valid',activation='relu')(conv)\nconv1 = BatchNormalization(name=\"Normal1\")(conv1)\ngru_3 = GRU(512, return_sequences = True, name = 'gru_3')(conv1)\ngru_4 = GRU(512, return_sequences = True, go_backwards = True, name = 'gru_4')(conv1)\nmerged = concatenate([gru_3, gru_4])\nnormalized = BatchNormalization(name=\"Normal\")(merged)\ndense = TimeDistributed(Dense(30))(normalized)\ny_pred = TimeDistributed(Activation('softmax', name='softmax'))(dense)\nModel(inputs = input_data, outputs = y_pred).summary()\n```", "```py\nfrom keras.optimizers import Adam\nOptimizer = Adam(lr = 0.001)\nlabels = Input(name = 'the_labels', shape=[243], dtype='float32')\ninput_length = Input(name='input_length', shape=[1],dtype='int64')\nlabel_length = Input(name='label_length',shape=[1],dtype='int64')\noutput = Lambda(ctc_loss, output_shape=(1,),name='ctc')([y_pred, labels, input_length, label_length])\n```", "```py\nmodel = Model(inputs = [input_data, labels, input_length, label_length], outputs= output)\nmodel.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = Optimizer, metrics = ['accuracy'])\n```", "```py\nfor i in range(2500):\n     samp=random.sample(range(x2.shape[0]-25),32)\n     batch_input=[inp2[i] for i in samp]\n     batch_input = np.array(batch_input)\n     batch_input = batch_input/np.max(batch_input)\n     batch_output = [y2[i] for i in samp]\n     batch_output = np.array(batch_output)\n     input_lengths2 = [input_lengths[i] for i in samp]\n     label_lengths2 = [label_lengths[i] for i in samp]\n     input_lengths2 = np.array(input_lengths2)\n     label_lengths2 = np.array(label_lengths2)\n     inputs = {'the_input': batch_input,\n             'the_labels': batch_output,\n             'input_length': input_lengths2,\n             'label_length': label_lengths2}\n     outputs = {'ctc': np.zeros([32])} \n     model.fit(inputs, outputs,batch_size = 32, epochs=1, verbose =1)\n```", "```py\nmodel2 = Model(inputs = input_data, outputs = y_pred)\n\nk=-12\npred= model2.predict(np.array(inp2[k]).reshape(1,999,161)/100)\n```", "```py\ndef decoder(pred):\n     pred_ints = (K.eval(K.ctc_decode(pred,[243])[0][0])).flatten().tolist()\n     out = \"\"\n     for i in range(len(pred_ints)):\n         if pred_ints[i]<28:\n         out = out+charList[pred_ints[i]]\n     print(out)\n```", "```py\ndecoder(pred)\n```"]