["```py\npublic override void CollectObservations()\n{\n  jdController.GetCurrentJointForces();\n\n  AddVectorObs(dirToTarget.normalized);\n  AddVectorObs(body.transform.position.y);\n  AddVectorObs(body.forward);\n  AddVectorObs(body.up);\n  foreach (var bodyPart in jdController.bodyPartsDict.Values)\n  {\n    CollectObservationBodyPart(bodyPart);\n  }\n}\n```", "```py\nmlagents-learn config/trainer_config.yaml --run-id=crawler --train\n```", "```py\nCrawlerDynamicLearning:\n  normalize: true\n  num_epoch: 3\n  time_horizon: 1000\n batch_size: 2024\n buffer_size: 20240\n  gamma: 0.995\n  max_steps: 1e6\n  summary_freq: 3000\n  num_layers: 3\n  hidden_units: 512\n```", "```py\ntime_horizon: 2000\nbatch_size: 4048\nbuffer_size: 40480\n```", "```py\nWalkerLearning:\n    normalize: true\n    num_epoch: 3\n    time_horizon: 1000\n    batch_size: 2048\n    buffer_size: 20480\n    gamma: 0.995\n    max_steps: 2e6\n    summary_freq: 3000\n    num_layers: 3\n hidden_units: 512\n```", "```py\nmlagents-learn config/trainer_config.yaml --run-id=walker --train\n```", "```py\nWalkerLearning:\n    normalize: true\n    num_epoch: 3\n    time_horizon: 1000\n    batch_size: 2048\n    buffer_size: 20480\n    gamma: 0.995\n    max_steps: 2e6\n    summary_freq: 3000\n    num_layers: 1\n hidden_units: 128\n```", "```py\nVisualHallwayLearning:\n    use_recurrent: false\n    sequence_length: 64\n    num_layers: 1\n hidden_units: 128\n    memory_size: 256\n    beta: 1.0e-2\n    gamma: 0.99\n    num_epoch: 3\n    buffer_size: 1024\n    batch_size: 64\n    max_steps: 5.0e5\n    summary_freq: 1000\n    time_horizon: 64\n```", "```py\nCrawlerDynamicLearning:\n    normalize: true\n    num_epoch: 3\n    time_horizon: 1000\n    batch_size: 1024\n    buffer_size: 20240\n    gamma: 0.995\n    max_steps: 1e6\n    summary_freq: 3000\n    num_layers: 3\n    hidden_units: 512\n    epsilon: .1\n beta: .1 \n```", "```py\nmlagents-learn config/trainer_config.yaml --run-id=crawler_policy --train\n```", "```py\nWalkerLearning:\n    normalize: true\n    num_epoch: 3\n    time_horizon: 1000\n    batch_size: 2048\n    buffer_size: 20480\n    gamma: 0.995\n    max_steps: 2e6\n    summary_freq: 3000\n    num_layers: 3\n    hidden_units: 512\n    lambd: .99\n```", "```py\nmlagents-learn config/trainer_config.yaml --run-id=walker_lambd --train\n```", "```py\npublic GameObject pendulumA;\npublic GameObject pendulumB;\npublic GameObject pendulumC;\npublic GameObject hand;\npublic GameObject goal;\nprivate ReacherAcademy myAcademy;\nfloat goalDegree;\nprivate Rigidbody rbA;\nprivate Rigidbody rbB;\nprivate Rigidbody rbC;\nprivate float goalSpeed;\nprivate float goalSize;\n```", "```py\npublic override void CollectObservations()\n    {\n        AddVectorObs(pendulumA.transform.localPosition);\n        AddVectorObs(pendulumA.transform.rotation);\n        AddVectorObs(rbA.angularVelocity);\n        AddVectorObs(rbA.velocity);\n\n        AddVectorObs(pendulumB.transform.localPosition);\n        AddVectorObs(pendulumB.transform.rotation);\n        AddVectorObs(rbB.angularVelocity);\n        AddVectorObs(rbB.velocity);\n\n        AddVectorObs(pendulumC.transform.localPosition);\n AddVectorObs(pendulumC.transform.rotation);\n AddVectorObs(rbC.angularVelocity);\n AddVectorObs(rbC.velocity);\n\n        AddVectorObs(goal.transform.localPosition);\n        AddVectorObs(hand.transform.localPosition);\n\n        AddVectorObs(goalSpeed);\n  }\n```", "```py\npublic override void AgentAction(float[] vectorAction, string textAction)\n  {\n        goalDegree += goalSpeed;\n        UpdateGoalPosition();\n\n        var torqueX = Mathf.Clamp(vectorAction[0], -1f, 1f) * 150f;\n        var torqueZ = Mathf.Clamp(vectorAction[1], -1f, 1f) * 150f;\n        rbA.AddTorque(new Vector3(torqueX, 0f, torqueZ));\n\n        torqueX = Mathf.Clamp(vectorAction[2], -1f, 1f) * 150f;\n        torqueZ = Mathf.Clamp(vectorAction[3], -1f, 1f) * 150f;\n        rbB.AddTorque(new Vector3(torqueX, 0f, torqueZ));\n\n        torqueX = Mathf.Clamp(vectorAction[3], -1f, 1f) * 150f;\n torqueZ = Mathf.Clamp(vectorAction[4], -1f, 1f) * 150f;\n rbC.AddTorque(new Vector3(torqueX, 0f, torqueZ));\n    }\n```", "```py\npublic override void AgentReset()\n    {\n        pendulumA.transform.position = new Vector3(0f, -4f, 0f) + transform.position;\n        pendulumA.transform.rotation = Quaternion.Euler(180f, 0f, 0f);\n        rbA.velocity = Vector3.zero;\n        rbA.angularVelocity = Vector3.zero;\n\n        pendulumB.transform.position = new Vector3(0f, -10f, 0f) + transform.position;\n        pendulumB.transform.rotation = Quaternion.Euler(180f, 0f, 0f);\n        rbB.velocity = Vector3.zero;\n        rbB.angularVelocity = Vector3.zero;\n\n        pendulumC.transform.position = new Vector3(0f, -6f, 0f) + transform.position;\n pendulumC.transform.rotation = Quaternion.Euler(180f, 0f, 0f);\n rbC.velocity = Vector3.zero;\n rbC.angularVelocity = Vector3.zero;\n\n        goalDegree = Random.Range(0, 360);\n        UpdateGoalPosition();\n\n        goalSize = myAcademy.goalSize;\n        goalSpeed = Random.Range(-1f, 1f) * myAcademy.goalSpeed;\n\n        goal.transform.localScale = new Vector3(goalSize, goalSize, goalSize);\n    }\n```", "```py\nHallwayLearning:\n    use_recurrent: true\n    sequence_length: 64\n    num_layers: 2\n    hidden_units: 128\n    memory_size: 256\n    beta: 1.0e-2\n    gamma: 0.99\n    num_epoch: 3\n    buffer_size: 1024\n    batch_size: 128\n    max_steps: 5.0e5\n    summary_freq: 1000\n    time_horizon: 64\n```", "```py\nHallwayLearning:\n    use_recurrent: true\n    sequence_length: 64\n    num_layers: 2\n    hidden_units: 128\n    memory_size: 256\n    beta: 1.0e-2\n    gamma: 0.99\n    num_epoch: 10\n    buffer_size: 1024\n    batch_size: 1000\n    max_steps: 5.0e5\n    summary_freq: 1000\n    time_horizon: 64\n```", "```py\nmlagents-learn config/trainer_config.yaml --run-id=hallway_e10b1000 --train\n```", "```py\ntime_horizon: 2000\nbatch_size: 4048\nbuffer_size: 40480\n```", "```py\nnum_layers: 3\nhidden_units: 512\n```", "```py\nlambd: .99\n```"]