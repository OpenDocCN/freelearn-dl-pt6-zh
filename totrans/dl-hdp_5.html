<html><head></head><body><div id="book-columns"><div id="book-inner"><div class="chapter" title="Chapter 5.  Restricted Boltzmann Machines"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/><span class="koboSpan" id="kobo.1.1">Chapter 5.  Restricted Boltzmann Machines </span></h1></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"><span class="koboSpan" id="kobo.2.1"> </span></td><td valign="top"><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.3.1">"What I cannot create, I do not understand."</span></em></span>
</p></td><td valign="top"><span class="koboSpan" id="kobo.4.1"> </span></td></tr><tr><td valign="top"><span class="koboSpan" id="kobo.5.1"> </span></td><td colspan="2" align="right" valign="top" style="text-align: center"><span class="koboSpan" id="kobo.6.1">--</span><span class="attribution"><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">Richard Feynman</span></em></span></span></td></tr></table></div><p><span class="koboSpan" id="kobo.8.1">So far in this book, we have only discussed the discriminative models. </span><span class="koboSpan" id="kobo.8.2">The use of these in deep learning is to model the dependencies of an unobserved variable y on an observed variable </span><span class="emphasis"><em><span class="koboSpan" id="kobo.9.1">x</span></em></span><span class="koboSpan" id="kobo.10.1">. </span><span class="koboSpan" id="kobo.10.2">Mathematically, it is formulated as </span><span class="emphasis"><em><span class="koboSpan" id="kobo.11.1">P(y|x)</span></em></span><span class="koboSpan" id="kobo.12.1">. </span><span class="koboSpan" id="kobo.12.2">In this chapter, we will discuss deep generative models to be used in deep learning.</span></p><p><span class="koboSpan" id="kobo.13.1">Generative models are models, which when given some hidden parameters, can randomly generate some observable data values out of them. </span><span class="koboSpan" id="kobo.13.2">The model works on a joint probability distribution over label sequences and observation.</span></p><p><span class="koboSpan" id="kobo.14.1">The generative models are used in machine and deep learning either as an intermediate step to generate a conditional probability density function or modeling observations directly from a probability density function.</span></p><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.15.1">Restricted Boltzmann machines</span></strong></span><span class="koboSpan" id="kobo.16.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.17.1">RBMs</span></strong></span><span class="koboSpan" id="kobo.18.1">) are a popular generative model that will be discussed in this chapter. </span><span class="koboSpan" id="kobo.18.2">RBMs are basically probabilistic graphical models that can also be interpreted as stochastic neural networks.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note18"/><span class="koboSpan" id="kobo.19.1">Note</span></h3><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.20.1">Stochastic neural networks</span></strong></span><span class="koboSpan" id="kobo.21.1"> can be defined as a type of artificial neural network that is generated by providing random variations into the network. </span><span class="koboSpan" id="kobo.21.2">The random variation can be supplied in various ways, such as providing stochastic weights or by giving a network's neurons stochastic transfer functions.</span></p></div></div><p><span class="koboSpan" id="kobo.22.1">In this chapter, we will discuss, a special type of Boltzmann machine called RBM, which is the main topic of this chapter. </span><span class="koboSpan" id="kobo.22.2">We will discuss how </span><span class="strong"><strong><span class="koboSpan" id="kobo.23.1">Energy-Based models</span></strong></span><span class="koboSpan" id="kobo.24.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.25.1">EBMs</span></strong></span><span class="koboSpan" id="kobo.26.1">) are related to RBM, and their functionalities. </span><span class="koboSpan" id="kobo.26.2">Later in this chapter, we will introduce </span><span class="strong"><strong><span class="koboSpan" id="kobo.27.1">Deep Belief network</span></strong></span><span class="koboSpan" id="kobo.28.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.29.1">DBN</span></strong></span><span class="koboSpan" id="kobo.30.1">), which is an extension of the RBM. </span><span class="koboSpan" id="kobo.30.2">The chapter will then discuss the large-scale implementation of these in distributed environments. </span><span class="koboSpan" id="kobo.30.3">The chapter will conclude by giving examples of RBM and DBN with Deeplearning4j.</span></p><p><span class="koboSpan" id="kobo.31.1">The organization of this chapter is as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.32.1">Energy-based models</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.33.1">Boltzmann machine</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.34.1">Restricted Boltzmann machine</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.35.1">Convolutional Restricted Boltzmann machine</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.36.1">Deep Belief network</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.37.1">Distributed Deep Belief network</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.38.1">Implementation of RBM and DBN with Deeplearning4j</span></li></ul></div><div class="section" title="Energy-based models"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec32"/><span class="koboSpan" id="kobo.39.1">Energy-based models</span></h1></div></div></div><p><span class="koboSpan" id="kobo.40.1">The main goal of deep learning and statistical modeling is to encode the dependencies between variables. </span><span class="koboSpan" id="kobo.40.2">By getting an idea of those dependencies, from the values of the known variables, a model can answer questions about the unknown variables.</span></p><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.41.1">Energy-based models</span></strong></span><span class="koboSpan" id="kobo.42.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.43.1">EBMs</span></strong></span><span class="koboSpan" id="kobo.44.1">) [120] gather and collect the dependencies by identifying scaler energy, which generally is a measure of compatibility to each configuration of the variable. </span><span class="koboSpan" id="kobo.44.2">In EBMs, the predictions are made by setting the value of observed variables and finding the value of the unobserved variables, which minimize the overall energy. </span><span class="koboSpan" id="kobo.44.3">Learning in EBMs consists of formulating an energy function, which assigns low energies to the correct values of unobserved variables and higher energies to the incorrect ones. </span><span class="koboSpan" id="kobo.44.4">Energy-based learning can be treated as an alternative to probabilistic estimation for classification, decision-making, or prediction tasks.</span></p><p><span class="koboSpan" id="kobo.45.1">To give a clear idea about how EBMs work, let us look at a simple example.</span></p><p><span class="koboSpan" id="kobo.46.1">As shown in F</span><span class="emphasis"><em><span class="koboSpan" id="kobo.47.1">igure 5.1</span></em></span><span class="koboSpan" id="kobo.48.1">, let us consider two sets of variables, observed and unobserved, namely, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.49.1">X</span></em></span><span class="koboSpan" id="kobo.50.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.51.1">Y</span></em></span><span class="koboSpan" id="kobo.52.1"> respectively. </span><span class="koboSpan" id="kobo.52.2">Variable </span><span class="emphasis"><em><span class="koboSpan" id="kobo.53.1">X</span></em></span><span class="koboSpan" id="kobo.54.1">, in the figure, represents the collection of pixels from an image. </span><span class="koboSpan" id="kobo.54.2">Variable </span><span class="emphasis"><em><span class="koboSpan" id="kobo.55.1">Y</span></em></span><span class="koboSpan" id="kobo.56.1"> is discrete, and contains the possible categories of the object needed for classification. </span><span class="koboSpan" id="kobo.56.2">Variable </span><span class="emphasis"><em><span class="koboSpan" id="kobo.57.1">Y</span></em></span><span class="koboSpan" id="kobo.58.1"> in this case, consists of six possible values, namely: air-plane, animal, human, car, truck, and none of the above. </span><span class="koboSpan" id="kobo.58.2">The model is used as an energy function that will measure the correctness of the mapping between </span><span class="emphasis"><em><span class="koboSpan" id="kobo.59.1">X</span></em></span><span class="koboSpan" id="kobo.60.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.61.1">Y</span></em></span><span class="koboSpan" id="kobo.62.1">.</span></p><p><span class="koboSpan" id="kobo.63.1">The model uses a convention that small energy values imply highly related configuration of the variables. </span><span class="koboSpan" id="kobo.63.2">On the other hand, with the increasing energy values, the incompatibility of the variables also increases equally. </span><span class="koboSpan" id="kobo.63.3">The function that is related to both the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.64.1">X</span></em></span><span class="koboSpan" id="kobo.65.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.66.1">Y</span></em></span><span class="koboSpan" id="kobo.67.1"> variable is termed as energy function, denoted as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.68.1"><img src="graphics/B05883_05_01.jpg" alt="Energy-based models"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.69.1">In the case of energy models, the input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.70.1">X</span></em></span><span class="koboSpan" id="kobo.71.1"> is collected from the surroundings and the model generates an output Y, which is more likely to answer about the observed variable </span><span class="emphasis"><em><span class="koboSpan" id="kobo.72.1">X</span></em></span><span class="koboSpan" id="kobo.73.1">. </span><span class="koboSpan" id="kobo.73.2">The model is required to produce the value </span><span class="emphasis"><em><span class="koboSpan" id="kobo.74.1">Y</span><sup><span class="koboSpan" id="kobo.75.1">/</span></sup></em></span><span class="koboSpan" id="kobo.76.1">, chosen from a set </span><span class="emphasis"><em><span class="koboSpan" id="kobo.77.1">Y*</span></em></span><span class="koboSpan" id="kobo.78.1">, which will make the value of the energy function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.79.1">E(Y, X)</span></em></span><span class="koboSpan" id="kobo.80.1"> least. </span><span class="koboSpan" id="kobo.80.2">Mathematically, this is represented as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.81.1"><img src="graphics/image_05_001.jpg" alt="Energy-based models"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.82.1">The following </span><span class="emphasis"><em><span class="koboSpan" id="kobo.83.1">Figure 5.1</span></em></span><span class="koboSpan" id="kobo.84.1"> depicts the block diagram of the overall example mentioned in the preceding section:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.85.1"><img src="graphics/image_05_002.jpg" alt="Energy-based models"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.86.1">Figure 5.1: Figure shows a energy model which computes the compatibility between the observed variable X and unobserved variable Y. </span><span class="koboSpan" id="kobo.86.2">X in the image is a set of pixel and Y is the set of level used for categorization of X. </span><span class="koboSpan" id="kobo.86.3">The model finds choosing 'Animal' makes the values of energy function least. </span><span class="koboSpan" id="kobo.86.4">Image taken from [121]</span></p><p><span class="koboSpan" id="kobo.87.1">EBMs in deep learning are related to probability. </span><span class="koboSpan" id="kobo.87.2">Probability is proportional to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.88.1">e</span></em></span><span class="koboSpan" id="kobo.89.1"> to the power of negative energy:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.90.1"><img src="graphics/image_05_003.jpg" alt="Energy-based models"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.91.1">EBMs define probabilities indirectly by formulating the function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.92.1">E(x)</span></em></span><span class="koboSpan" id="kobo.93.1">. </span><span class="koboSpan" id="kobo.93.2">The exponential function makes sure that the probability will always be greater than zero. </span><span class="koboSpan" id="kobo.93.3">This also implies that in an energy-based model, one is always free to choose the energy function based on the observed and unobserved variables. </span><span class="koboSpan" id="kobo.93.4">Although the probabilities for a classification in an energy-based model can arbitrarily approach zero, it will never reach that.</span></p><p><span class="koboSpan" id="kobo.94.1">Distribution in the form of the preceding equation is a form of Boltzmann distribution. </span><span class="koboSpan" id="kobo.94.2">The EBMs are hence often termed as </span><span class="strong"><strong><span class="koboSpan" id="kobo.95.1">Boltzmann machines</span></strong></span><span class="koboSpan" id="kobo.96.1">. </span><span class="koboSpan" id="kobo.96.2">We will explain about Boltzmann machines and their various forms in the subsequent sections of this chapter.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Boltzmann machines"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec33"/><span class="koboSpan" id="kobo.1.1">Boltzmann machines</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Boltzmann machines [122] are a network of symmetrically connected, neuron-like units, which are used for stochastic decisions on the given datasets. </span><span class="koboSpan" id="kobo.2.2">Initially, they were introduced to learn the probability distributions over binary vectors. </span><span class="koboSpan" id="kobo.2.3">Boltzmann machines possess a simple learning algorithm, which helps them to infer and reach interesting conclusions about input datasets containing binary vectors. </span><span class="koboSpan" id="kobo.2.4">The learning algorithm becomes very slow in networks with many layers of feature detectors; however, with one layer of feature detector at a time, learning can be much faster.</span></p><p><span class="koboSpan" id="kobo.3.1">To solve a learning problem, Boltzmann machines consist of a set of binary data vectors, and update the weight on the respective connections so that the data vectors turn out to be good solutions for the optimization problem laid by the weights. </span><span class="koboSpan" id="kobo.3.2">The Boltzmann machine, to solve the learning problem, makes lots of small updates to these weights.</span></p><p><span class="koboSpan" id="kobo.4.1">The Boltzmann machine over a d-dimensional binary vector can be defined as </span><span class="emphasis"><em><span class="koboSpan" id="kobo.5.1">x </span><span class="inlinemediaobject"><span class="koboSpan" id="kobo.6.1"><img src="graphics/Belongs-t0.jpg" alt="Boltzmann machines"/></span></span><span class="koboSpan" id="kobo.7.1">
 {0, 1} </span><sup><span class="koboSpan" id="kobo.8.1">d</span></sup></em></span><span class="koboSpan" id="kobo.9.1">. </span><span class="koboSpan" id="kobo.9.2">As mentioned in the earlier section, the Boltzmann machine is a type of energy-based function whose joint probability function can be defined using the energy function given as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.10.1"><img src="graphics/image_05_004.jpg" alt="Boltzmann machines"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.11.1">Here, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.12.1">E(x)</span></em></span><span class="koboSpan" id="kobo.13.1"> is the energy function and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.14.1">Z</span></em></span><span class="koboSpan" id="kobo.15.1"> is termed as a partition function that confirms </span><span class="emphasis"><em><span class="koboSpan" id="kobo.16.1">Î£</span><sub><span class="koboSpan" id="kobo.17.1">x </span></sub><span class="koboSpan" id="kobo.18.1">P(x)= 1</span></em></span><span class="koboSpan" id="kobo.19.1">. </span><span class="koboSpan" id="kobo.19.2">The energy function of the Boltzmann machine is given as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.20.1"><img src="graphics/image_05_006.jpg" alt="Boltzmann machines"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.21.1">Here, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.22.1">W</span></em></span><span class="koboSpan" id="kobo.23.1"> is the weight matrix of the model parameters and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.24.1">b</span></em></span><span class="koboSpan" id="kobo.25.1"> is the vector of the bias parameter.</span></p><p><span class="koboSpan" id="kobo.26.1">Boltzmann machines such as EBMs work on observed and unobserved variables. </span><span class="koboSpan" id="kobo.26.2">The Boltzmann machine works more efficiently when the observed variables are not in higher numbers. </span><span class="koboSpan" id="kobo.26.3">In those cases, the unobserved or hidden variables behave like hidden units of multilayer perceptron and show higher order interactions among the visible units.</span></p><p><span class="koboSpan" id="kobo.27.1">Boltzmann machines have interlayer connections between the hidden layers as well as between visible units. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.28.1">Figure 5.2</span></em></span><span class="koboSpan" id="kobo.29.1"> shows a pictorial representation of the Boltzmann machine:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.30.1"><img src="graphics/image_05_007.jpg" alt="Boltzmann machines"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.31.1">Figure 5.2: Figure shows a graphical representation of a simple Boltzmann machine. </span><span class="koboSpan" id="kobo.31.2">The undirected edges in the figure signifies the dependency among nodes and w</span><sub><span class="koboSpan" id="kobo.32.1">i,j</span></sub><span class="koboSpan" id="kobo.33.1"> represents the weight associated between nodes i and j. </span><span class="koboSpan" id="kobo.33.2">Figure shows 3 hidden nodes and 4 visible nodes</span></p><p><span class="koboSpan" id="kobo.34.1">An interesting property of the Boltzmann machine is that the learning rule does not change with the addition of hidden units. </span><span class="koboSpan" id="kobo.34.2">This eventually helps to learn the binary features to capture the higher-order structure in the input data.</span></p><p><span class="koboSpan" id="kobo.35.1">The Boltzmann machine behaves as a universal approximator of probability mass function over discrete variables.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note19"/><span class="koboSpan" id="kobo.36.1">Note</span></h3><p><span class="koboSpan" id="kobo.37.1">In statistical learning, </span><span class="strong"><strong><span class="koboSpan" id="kobo.38.1">Maximize Likelihood Estimation</span></strong></span><span class="koboSpan" id="kobo.39.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.40.1">MLE</span></strong></span><span class="koboSpan" id="kobo.41.1">) is a procedure of finding the parameters of a statistical model given observations, by finding the value of one or more parameters, which maximizes the likelihood of making the observations with the parameters.</span></p></div></div><div class="section" title="How Boltzmann machines learn"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec45"/><span class="koboSpan" id="kobo.42.1">How Boltzmann machines learn</span></h2></div></div></div><p><span class="koboSpan" id="kobo.43.1">The learning algorithms for Boltzmann machines are generally based on maximum likelihood estimation method. </span><span class="koboSpan" id="kobo.43.2">When Boltzmann machines are trained with learning rules based on maximum likelihood estimation, the update of a particular weight connecting two units of the model will depend on only those two units concerned. </span><span class="koboSpan" id="kobo.43.3">The other units of the network take part in modifying the statistics that get generated. </span><span class="koboSpan" id="kobo.43.4">Therefore, the weight can be updated without letting the rest of the network know. </span><span class="koboSpan" id="kobo.43.5">In other words, the rest of the network can only know the final statistics, but would not know how the statistics are computed.</span></p></div><div class="section" title="Shortfall"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec46"/><span class="koboSpan" id="kobo.44.1">Shortfall</span></h2></div></div></div><p><span class="koboSpan" id="kobo.45.1">In Boltzmann machines, with many hidden layers, the network becomes extremely large. </span><span class="koboSpan" id="kobo.45.2">This makes the model typically slow. </span><span class="koboSpan" id="kobo.45.3">The Boltzmann machine stops learning with large scale data, as the machine's size also simultaneously grows exponentially. </span><span class="koboSpan" id="kobo.45.4">With a large network, the weights are generally very large and also the equilibrium distribution becomes very high. </span><span class="koboSpan" id="kobo.45.5">This unfortunately creates a significant problem for Boltzmann machines, which eventually results in a longer time duration to reach to an equilibrium state of distribution.</span></p><p><span class="koboSpan" id="kobo.46.1">This limitation can be overcome by restricting the connectivity between two layers, and thus simplifying the learning algorithm by learning one latent layer at a time.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Restricted Boltzmann machine"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec34"/><span class="koboSpan" id="kobo.1.1">Restricted Boltzmann machine</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">The </span><span class="strong"><strong><span class="koboSpan" id="kobo.3.1">Restricted Boltzmann machine</span></strong></span><span class="koboSpan" id="kobo.4.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.5.1">RBM</span></strong></span><span class="koboSpan" id="kobo.6.1">) is a classic example of building blocks of deep probabilistic models that are used for deep learning. </span><span class="koboSpan" id="kobo.6.2">The RBM itself is not a deep model but can be used as a building block to form other deep models. </span><span class="koboSpan" id="kobo.6.3">In fact, RBMs are undirected probabilistic graphical models that consist of a layer of observed variables and a single layer of hidden variables, which may be used to learn the representation for the input. </span><span class="koboSpan" id="kobo.6.4">In this section, we will explain how the RBM can be used to build many deeper models.</span></p><p><span class="koboSpan" id="kobo.7.1">Let us consider two examples to see the use case of RBM. </span><span class="koboSpan" id="kobo.7.2">RBM primarily operates on a binary version of factor analysis. </span><span class="koboSpan" id="kobo.7.3">Let us say we have a restaurant, and want to ask our customer to rate the food on a scale of 0 to 5. </span><span class="koboSpan" id="kobo.7.4">In the traditional approach, we will try to explain each food item and customer in terms of the variable's hidden factors. </span><span class="koboSpan" id="kobo.7.5">For example, foods such as pasta and lasagne will have a strong association with the Italian factors. </span><span class="koboSpan" id="kobo.7.6">RBM, on the other hand, works on a different approach. </span><span class="koboSpan" id="kobo.7.7">Instead of asking each customer to rate the food items on a continuous scale, they simply mention whether they like it or not, and then RBM will try to infer various latent factors, which can help to explain the activation of food choices of each customer.</span></p><p><span class="koboSpan" id="kobo.8.1">Another example could be to guess someone's movie choice based on the genre the person likes. </span><span class="koboSpan" id="kobo.8.2">Say Mr. </span><span class="koboSpan" id="kobo.8.3">X has supplied his five binary preferences on the set of movies given. </span><span class="koboSpan" id="kobo.8.4">The job of the RBM will be to activate his preferences based on the hidden units. </span><span class="koboSpan" id="kobo.8.5">So, in this case, the five movies will send messages to all the hidden units, asking them to update themselves. </span><span class="koboSpan" id="kobo.8.6">The RBM will then activate the hidden units with high probability based on some preferences given to the person earlier.</span></p><div class="section" title="The basic architecture"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec47"/><span class="koboSpan" id="kobo.9.1">The basic architecture</span></h2></div></div></div><p><span class="koboSpan" id="kobo.10.1">The RBM is a shallow, two-layer neural network used as a building block to create deep models. </span><span class="koboSpan" id="kobo.10.2">The first layer of the RBM is called the observed or visible layer, the second layer is called the latent or hidden layer. </span><span class="koboSpan" id="kobo.10.3">It is a bipartite graph, with no interconnection allowed between any variables in the observed layer, or between any units in the latent layer. </span><span class="koboSpan" id="kobo.10.4">As shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.11.1">Figure 5.3</span></em></span><span class="koboSpan" id="kobo.12.1">, there is no intra-layer communication between the layers. </span><span class="koboSpan" id="kobo.12.2">Due to this restriction, the model is termed as a </span><span class="strong"><strong><span class="koboSpan" id="kobo.13.1">Restricted Boltzmann machine</span></strong></span><span class="koboSpan" id="kobo.14.1">. </span><span class="koboSpan" id="kobo.14.2">Each node is used for computation that processed the input, and participated in the output by making stochastic (randomly determined) decisions about whether to convey that input or not.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note20"/><span class="koboSpan" id="kobo.15.1">Note</span></h3><p><span class="koboSpan" id="kobo.16.1">A bipartite graph is a graph wherein the vertices can be split into two disjoint sets so that every edge connects a vertex of one set to the other. </span><span class="koboSpan" id="kobo.16.2">However, there is no connection between the vertices of the same set. </span><span class="koboSpan" id="kobo.16.3">The vertex sets are usually termed as a part of the graph.</span></p></div></div><p><span class="koboSpan" id="kobo.17.1">The primary intuition behind the two layers of an RBM is that there are some visible random variables (for example, food reviews from different customers) and some latent variables (such as cuisines, nationality of the customers or other internal factors), and the task of training the RBM is to find the probability of how these two sets of variables are interconnected to each other.</span></p><p><span class="koboSpan" id="kobo.18.1">To mathematically formulate the energy function of an RBM, let's denote the observed layer that consists of a set of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.19.1">n</span><sub><span class="koboSpan" id="kobo.20.1">v</span></sub></em></span><span class="koboSpan" id="kobo.21.1"> binary variables collectively with the vector </span><span class="emphasis"><em><span class="koboSpan" id="kobo.22.1">v</span></em></span><span class="koboSpan" id="kobo.23.1">. </span><span class="koboSpan" id="kobo.23.2">The hidden or latent layers of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.24.1">n</span><sub><span class="koboSpan" id="kobo.25.1">h</span></sub></em></span><span class="koboSpan" id="kobo.26.1"> binary random variables are denoted as </span><span class="emphasis"><em><span class="koboSpan" id="kobo.27.1">h</span></em></span><span class="koboSpan" id="kobo.28.1">.</span></p><p><span class="koboSpan" id="kobo.29.1">Similar to the Boltzmann machine, the RBM is also an energy-based model, where the joint probability distribution is determined by its energy function:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.30.1"><img src="graphics/B05883_05_07.jpg" alt="The basic architecture"/></span></div><p>
</p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.31.1"><img src="graphics/image_05_009.jpg" alt="The basic architecture"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.32.1">Figure 5.3: Figure shows a simple RBM. </span><span class="koboSpan" id="kobo.32.2">The model is a symmetric bipartite graph, where each hidden node is connected to every visible nodes. </span><span class="koboSpan" id="kobo.32.3">Hidden units are represented as h</span><sub><span class="koboSpan" id="kobo.33.1">i</span></sub><span class="koboSpan" id="kobo.34.1"> and visible units as v</span><sub><span class="koboSpan" id="kobo.35.1">i</span></sub></p><p><span class="koboSpan" id="kobo.36.1">The energy function of an RBM with binary visible and latent units is as follows:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.37.1"><img src="graphics/B05883_05_08.jpg" alt="The basic architecture"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.38.1">Here, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.39.1">a</span></em></span><span class="koboSpan" id="kobo.40.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.41.1">b</span></em></span><span class="koboSpan" id="kobo.42.1">, and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.43.1">W</span></em></span><span class="koboSpan" id="kobo.44.1"> are unconstrained, learnable, real-valued parameters. </span><span class="koboSpan" id="kobo.44.2">From the preceding </span><span class="emphasis"><em><span class="koboSpan" id="kobo.45.1">Figure 5.3</span></em></span><span class="koboSpan" id="kobo.46.1">, we can see that the model is split into two groups of variables, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.47.1">v</span></em></span><span class="koboSpan" id="kobo.48.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.49.1">h</span></em></span><span class="koboSpan" id="kobo.50.1">. </span><span class="koboSpan" id="kobo.50.2">The interaction between the units is described by the matrix </span><span class="emphasis"><em><span class="koboSpan" id="kobo.51.1">W</span></em></span><span class="koboSpan" id="kobo.52.1">.</span></p></div><div class="section" title="How RBMs work"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec48"/><span class="koboSpan" id="kobo.53.1">How RBMs work</span></h2></div></div></div><p><span class="koboSpan" id="kobo.54.1">So, as we are now aware of the basic architecture of an RBM, in this section, we will discuss the basic working procedure for this model. </span><span class="koboSpan" id="kobo.54.2">The RBM is fed with a dataset from which it should learn. </span><span class="koboSpan" id="kobo.54.3">Each visible node of the model receives a low-level feature from an item of the dataset. </span><span class="koboSpan" id="kobo.54.4">For example, for a gray-scale image, the lowest level item would be one pixel value of the image, which the visible node would receive. </span><span class="koboSpan" id="kobo.54.5">Therefore, if an image dataset has n number of pixels, the neural network processing them must also possess n input nodes on the visible layer:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.55.1"><img src="graphics/B05883_05_04-300x232.jpg" alt="How RBMs work"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.56.1">Figure 5.4 : Figure shows the computation of an RBM for a one input path</span></p><p><span class="koboSpan" id="kobo.57.1">Now, let's propagate one single pixel value, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.58.1">p</span></em></span><span class="koboSpan" id="kobo.59.1"> through the two-layer network. </span><span class="koboSpan" id="kobo.59.2">At the first node of the hidden layer, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.60.1">p</span></em></span><span class="koboSpan" id="kobo.61.1"> is multiplied by a weight </span><span class="emphasis"><em><span class="koboSpan" id="kobo.62.1">w</span></em></span><span class="koboSpan" id="kobo.63.1">, and added to the bias. </span><span class="koboSpan" id="kobo.63.2">The final result is then fed into an activation function that generates the output of the node. </span><span class="koboSpan" id="kobo.63.3">The operation produces the outcome, which can be termed as the strength of the signal passing through that node, given an input pixel </span><span class="emphasis"><em><span class="koboSpan" id="kobo.64.1">p</span></em></span><span class="koboSpan" id="kobo.65.1">. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.66.1">Figure 5.4</span></em></span><span class="koboSpan" id="kobo.67.1"> shows the visual representation of the computation involved for a single input RBM.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.68.1"><img src="graphics/B05883_05_12.jpg" alt="How RBMs work"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.69.1">Every visible node of the RBM is associated with a separate weight. </span><span class="koboSpan" id="kobo.69.2">Inputs from various units get combined at one hidden node. </span><span class="koboSpan" id="kobo.69.3">Each </span><span class="emphasis"><em><span class="koboSpan" id="kobo.70.1">p</span></em></span><span class="koboSpan" id="kobo.71.1"> (pixel) from the input is multiplied by a separate weight associated with it. </span><span class="koboSpan" id="kobo.71.2">The products are summed up and added to a bias. </span><span class="koboSpan" id="kobo.71.3">This result is passed through an activation function to generate the output of the node. </span><span class="koboSpan" id="kobo.71.4">The following </span><span class="emphasis"><em><span class="koboSpan" id="kobo.72.1">Figure 5.5</span></em></span><span class="koboSpan" id="kobo.73.1"> shows the visual representation of the computation involved for multiple inputs to the visible layer of RBMs:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.74.1"><img src="graphics/B05883_05_05-300x232.jpg" alt="How RBMs work"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.75.1">Figure 5.5: Figure shows the computation of an RBM with multiple inputs and one hidden unit</span></p><p><span class="koboSpan" id="kobo.76.1">The preceding </span><span class="emphasis"><em><span class="koboSpan" id="kobo.77.1">Figure 5.5</span></em></span><span class="koboSpan" id="kobo.78.1"> shows how the weight associated with every visible node is used to compute the final outcome from a hidden node.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.79.1"><img src="graphics/B05883_05_06-300x203.jpg" alt="How RBMs work"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.80.1">Figure 5.6: Figure shows the computation involved with multiple visible units and hidden units for an RBM</span></p><p><span class="koboSpan" id="kobo.81.1">As mentioned earlier, RBMs are similar to a bipartitone graph. </span><span class="koboSpan" id="kobo.81.2">Further, the machine's structure is basically similar to a symmetrical bipartite graph because input received from all the visible nodes are being passed to all the latent nodes of the RBM.</span></p><p><span class="koboSpan" id="kobo.82.1">For every hidden node, each input p gets multiplied with its respective weight </span><span class="emphasis"><em><span class="koboSpan" id="kobo.83.1">w</span></em></span><span class="koboSpan" id="kobo.84.1">. </span><span class="koboSpan" id="kobo.84.2">Therefore, for a single input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.85.1">p</span></em></span><span class="koboSpan" id="kobo.86.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.87.1">m</span></em></span><span class="koboSpan" id="kobo.88.1"> number of hidden units, the input would have </span><span class="emphasis"><em><span class="koboSpan" id="kobo.89.1">m</span></em></span><span class="koboSpan" id="kobo.90.1"> weights associated with it. </span><span class="koboSpan" id="kobo.90.2">In </span><span class="emphasis"><em><span class="koboSpan" id="kobo.91.1">Figure 5.6</span></em></span><span class="koboSpan" id="kobo.92.1">, the input </span><span class="emphasis"><em><span class="koboSpan" id="kobo.93.1">p</span></em></span><span class="koboSpan" id="kobo.94.1"> would have three weights, making a total of 12 weights altogether: four input nodes from the visible layer and three hidden nodes in the next layer. </span><span class="koboSpan" id="kobo.94.2">All the weights associated between the two layers form a matrix, where the rows are equal to the visible nodes, and the columns are equal to the hidden units. </span><span class="koboSpan" id="kobo.94.3">In the preceding figure, each hidden node of the second layer accepts the four inputs multiplied by their respective weights. </span><span class="koboSpan" id="kobo.94.4">The final sum of the products is then again added to a bias. </span><span class="koboSpan" id="kobo.94.5">This result is then passed through an activation algorithm to produce one output for each hidden layer. </span><span class="emphasis"><em><span class="koboSpan" id="kobo.95.1">Figure 5.6</span></em></span><span class="koboSpan" id="kobo.96.1"> represents the overall computations that occur in such scenarios.</span></p><p><span class="koboSpan" id="kobo.97.1">With a stacked RBM, it will form a deeper layer neural network, where the output of the first hidden layer would be passed to the next hidden layer as input. </span><span class="koboSpan" id="kobo.97.2">This will be propagated through as many hidden layers as one uses to reach the desired classifying layer. </span><span class="koboSpan" id="kobo.97.3">In the subsequent section, we will explain how to use an RBM as deep neural networks.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Convolutional Restricted Boltzmann machines"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec35"/><span class="koboSpan" id="kobo.1.1">Convolutional Restricted Boltzmann machines</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Very high dimensional inputs, such as images or videos, put immense stress on the memory, computation, and operational requirements of traditional machine learning models. </span><span class="koboSpan" id="kobo.2.2">In </span><a class="link" href="ch03.html" title="Chapter 3.  Convolutional Neural Network"><span class="koboSpan" id="kobo.3.1">
Chapter 3
</span></a><span class="koboSpan" id="kobo.4.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.5.1">Convolutional Neural Network</span></em></span><span class="koboSpan" id="kobo.6.1">, we have shown how replacing the matrix multiplication by discrete convolutional operations with small kernel resolves these problems. </span><span class="koboSpan" id="kobo.6.2">Going forward, Desjardins and Bengio [123] have shown that this approach also works fine when applied to RBMs. </span><span class="koboSpan" id="kobo.6.3">In this section, we will discuss the functionalities of this model.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.7.1"><img src="graphics/B05883_05_07-266x300.jpg" alt="Convolutional Restricted Boltzmann machines"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.8.1">Figure 5.7 : Figure shows the observed variables or the visible units of an RBM can be associated with mini batches of image to a compute the final result. </span><span class="koboSpan" id="kobo.8.2">The weight connections represents a set of filters</span></p><p><span class="koboSpan" id="kobo.9.1">Further, in normal RBMs, the visible units are directly related to all the hidden variables through different parameters and weights. </span><span class="koboSpan" id="kobo.9.2">To describe an image in terms of spatially local features ideally needs fewer parameters, which can be generalized better. </span><span class="koboSpan" id="kobo.9.3">This helps in detecting and extracting the identical and local features from a high dimensional image. </span><span class="koboSpan" id="kobo.9.4">Therefore, using an RBM to retrieve all the global features from an image for object detection is not so encouraging, especially for high dimensional images. </span><span class="koboSpan" id="kobo.9.5">One simple approach is to train the RBM on mini batches sampled from the input image, placed in blocks on Hadoop's Datanodes to generate the local features. </span><span class="koboSpan" id="kobo.9.6">The representation of this approach, termed as patch-based RBM is shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.10.1">Figure 5.7</span></em></span><span class="koboSpan" id="kobo.11.1">. </span><span class="koboSpan" id="kobo.11.2">This, however, has some potential limitations. </span><span class="koboSpan" id="kobo.11.3">The patch-based RBM, used in the distributed environment on Hadoop, does not follow the spatial relationship of mini batches, and sees each image's mini batches as independent patches from the nearby patches. </span><span class="koboSpan" id="kobo.11.4">This makes the feature extracted from the neighboring patches independent and somewhat significantly redundant.</span></p><p><span class="koboSpan" id="kobo.12.1">To handle such a situation, a </span><span class="strong"><strong><span class="koboSpan" id="kobo.13.1">Convolutional Restricted Boltzmann machine</span></strong></span><span class="koboSpan" id="kobo.14.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.15.1">CRBM</span></strong></span><span class="koboSpan" id="kobo.16.1">) is used, which is an extension of the traditional RBM model. </span><span class="koboSpan" id="kobo.16.2">The CRBM is structurally almost similar to RBM, a two-layer model in which the visible and hidden random variables are structured as matrices. </span><span class="koboSpan" id="kobo.16.3">Hence, in CRBM, the locality and neighborhood can be defined for both visible and hidden units. </span><span class="koboSpan" id="kobo.16.4">In CRBM, the visible matrix represents the image and the small windows of the matrix define the mini batches of the image. </span><span class="koboSpan" id="kobo.16.5">The hidden units of CRBM are partitioned into different feature maps to locate the presence of multiple features at multiple positions of the visible units. </span><span class="koboSpan" id="kobo.16.6">Units within a feature map represent the same feature at different locations of the visible unit. </span><span class="koboSpan" id="kobo.16.7">The hidden-visible connections of CRBM are completely local, and weights are generally split across the clusters of hidden units.</span></p><p><span class="koboSpan" id="kobo.17.1">The CRBM's hidden units are used to extract features from the overlapping mini batches of visible units. </span><span class="koboSpan" id="kobo.17.2">Moreover, the features of neighboring mini batches complement each other and collaborate to model the input image.</span></p><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.18.1">Figure 5.8</span></em></span><span class="koboSpan" id="kobo.19.1"> shows a CRBM with a matrix of visible units </span><span class="strong"><strong><span class="koboSpan" id="kobo.20.1">V</span></strong></span><span class="koboSpan" id="kobo.21.1"> and a matrix of hidden units </span><span class="strong"><strong><span class="koboSpan" id="kobo.22.1">H</span></strong></span><span class="koboSpan" id="kobo.23.1">, which are connected with </span><span class="emphasis"><em><span class="koboSpan" id="kobo.24.1">K 3*3</span></em></span><span class="koboSpan" id="kobo.25.1"> filters, namely, </span><span class="strong"><strong><span class="koboSpan" id="kobo.26.1">W</span><sub><span class="koboSpan" id="kobo.27.1">1</span></sub></strong></span><span class="koboSpan" id="kobo.28.1">, </span><span class="strong"><strong><span class="koboSpan" id="kobo.29.1">W</span><sub><span class="koboSpan" id="kobo.30.1">2</span></sub></strong></span><span class="koboSpan" id="kobo.31.1">, </span><span class="strong"><strong><span class="koboSpan" id="kobo.32.1">W</span><sub><span class="koboSpan" id="kobo.33.1">3</span></sub></strong></span><span class="koboSpan" id="kobo.34.1">,...</span><span class="strong"><strong><span class="koboSpan" id="kobo.35.1">W</span><sub><span class="koboSpan" id="kobo.36.1">K</span></sub></strong></span><span class="koboSpan" id="kobo.37.1">. </span><span class="koboSpan" id="kobo.37.2">The hidden units in the figure are split into </span><span class="strong"><strong><span class="koboSpan" id="kobo.38.1">K</span></strong></span><span class="koboSpan" id="kobo.39.1"> submatrices called feature map, </span><span class="strong"><strong><span class="koboSpan" id="kobo.40.1">H</span><sub><span class="koboSpan" id="kobo.41.1">1</span></sub></strong></span><span class="koboSpan" id="kobo.42.1">, </span><span class="strong"><strong><span class="koboSpan" id="kobo.43.1">H</span><sub><span class="koboSpan" id="kobo.44.1">2</span></sub></strong></span><span class="koboSpan" id="kobo.45.1">,...</span><span class="strong"><strong><span class="koboSpan" id="kobo.46.1">H</span><sub><span class="koboSpan" id="kobo.47.1">K</span></sub></strong></span><span class="koboSpan" id="kobo.48.1">. </span><span class="koboSpan" id="kobo.48.2">Each hidden unit </span><span class="strong"><strong><span class="koboSpan" id="kobo.49.1">H</span><sub><span class="koboSpan" id="kobo.50.1">i</span></sub></strong></span><span class="koboSpan" id="kobo.51.1"> signifies the presence of a particular feature at a </span><span class="emphasis"><em><span class="koboSpan" id="kobo.52.1">3*3</span></em></span><span class="koboSpan" id="kobo.53.1"> neighborhood of visible units.</span></p><p><span class="koboSpan" id="kobo.54.1">A CRBM, unlike a patch-based RBM, is trained on the whole input image or a large region of the image, to learn the local features and exploit the spatial relationship of the overlapping mini batches, processed in a distributed manner on Hadoop. </span><span class="koboSpan" id="kobo.54.2">The hidden units of overlapping mini batches depend and cooperate with each other in a CRBM. </span><span class="koboSpan" id="kobo.54.3">Therefore, one hidden unit, once explained, does not need to be explained again in the neighborhood overlapping mini batch. </span><span class="koboSpan" id="kobo.54.4">This in turn helps in reducing the redundancy of the features.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.55.1"><img src="graphics/B05883_05_08-267x300.jpg" alt="Convolutional Restricted Boltzmann machines"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.56.1">Figure 5.8: The computation involved in a CRBM is shown in the figure</span></p><div class="section" title="Stacked Convolutional Restricted Boltzmann machines"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec49"/><span class="koboSpan" id="kobo.57.1">Stacked Convolutional Restricted Boltzmann machines</span></h2></div></div></div><p><span class="koboSpan" id="kobo.58.1">CRBMs can be stacked together to form deep neural networks. </span><span class="strong"><strong><span class="koboSpan" id="kobo.59.1">Stacked Convolutional Restricted Boltzmann machines </span></strong></span><span class="koboSpan" id="kobo.60.1">(</span><span class="strong"><strong><span class="koboSpan" id="kobo.61.1">Stacked CRBMs</span></strong></span><span class="koboSpan" id="kobo.62.1">) can be trained layer wise, with a bottom-up approach, similar to the layer wise training of fully connected neural networks. </span><span class="koboSpan" id="kobo.62.2">After each CRBM filtering layer, in a stacked network, a deterministic subsampling method is implemented. </span><span class="koboSpan" id="kobo.62.3">For subsampling the features, max-pooling is performed in non-overlapping image regions. </span><span class="koboSpan" id="kobo.62.4">The pooling layer, as explained in </span><a class="link" href="ch03.html" title="Chapter 3.  Convolutional Neural Network"><span class="koboSpan" id="kobo.63.1">
Chapter 3
</span></a><span class="koboSpan" id="kobo.64.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.65.1">Convolutional Neural Network</span></em></span><span class="koboSpan" id="kobo.66.1">, helps to minimize the dimensionality of the features. </span><span class="koboSpan" id="kobo.66.2">On top of that, it makes the feature robust to small shifts and helps to propagate the higher-level feature to grow over regions of the input image.</span></p><p><span class="koboSpan" id="kobo.67.1">Deep CRBMs require a pooling operation, so that the spatial size of each successive layer decreases. </span><span class="koboSpan" id="kobo.67.2">Although, most of the traditional convolutional models work fine with inputs of a variety of spatial size, for Boltzmann machines it becomes somewhat difficult to change the input sizes, mainly due to a couple of reasons. </span><span class="koboSpan" id="kobo.67.3">Firstly, the partition function of the energy function changes with the size of input. </span><span class="koboSpan" id="kobo.67.4">Secondly, convolutional networks attain the size invariance by increasing the size of the pooling function proportional to the input size. </span><span class="koboSpan" id="kobo.67.5">However, scaling the pooling regions for Boltzmann machines is very difficult to achieve.</span></p><p><span class="koboSpan" id="kobo.68.1">For CRBMs, the pixels residing at the boundary of the image also impose difficulty, which is worsened by the fact that Boltzmann machines are symmetric in nature. </span><span class="koboSpan" id="kobo.68.2">This can be nullified by implicitly zero-padding the input. </span><span class="koboSpan" id="kobo.68.3">Bear in mind that, zero-padding the input is often driven by lesser input pixels, which may not be activated when needed.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Deep Belief networks"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec36"/><span class="koboSpan" id="kobo.1.1">Deep Belief networks</span></h1></div></div></div><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.2.1">Deep Belief networks</span></strong></span><span class="koboSpan" id="kobo.3.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.4.1">DBNs</span></strong></span><span class="koboSpan" id="kobo.5.1">) were one of the most popular, non-convolutional models that could be successfully deployed as deep neural networks in the year 2006-07 [124] [125]. </span><span class="koboSpan" id="kobo.5.2">The renaissance of deep learning probably started from the invention of DBNs back in 2006. </span><span class="koboSpan" id="kobo.5.3">Before the introduction of DBNs, it was very difficult to optimize the deep models. </span><span class="koboSpan" id="kobo.5.4">By outperforming the </span><span class="strong"><strong><span class="koboSpan" id="kobo.6.1">Support Vector machines</span></strong></span><span class="koboSpan" id="kobo.7.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.8.1">SVMs</span></strong></span><span class="koboSpan" id="kobo.9.1">), DBNs had shown that deep models can be really successful; although, compared to the other generative or unsupervised learning algorithms, the popularity of DBNs has fallen a bit, and is rarely used these days. </span><span class="koboSpan" id="kobo.9.2">However, they still play a very important role in the history of deep learning.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note21"/><span class="koboSpan" id="kobo.10.1">Note</span></h3><p><span class="koboSpan" id="kobo.11.1">A DBN with only one hidden layer is just an RBM.</span></p></div></div><p><span class="koboSpan" id="kobo.12.1">DBNs are generative models composed of more than one layer of hidden variables. </span><span class="koboSpan" id="kobo.12.2">The hidden variables are generally binary in nature; however the visible units might consist of binary or real values. </span><span class="koboSpan" id="kobo.12.3">In DBNs, every unit of each layer is connected to every other unit of its neighbouring layer, although, there can be a DBN with sparsely connected units. </span><span class="koboSpan" id="kobo.12.4">There possess no connection between the intermediate layers. </span><span class="koboSpan" id="kobo.12.5">As shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.13.1">Figure 5.9</span></em></span><span class="koboSpan" id="kobo.14.1">, DBNs are basically a multilayer network composed of several RBMs. </span><span class="koboSpan" id="kobo.14.2">The connections between the top two layers are undirected. </span><span class="koboSpan" id="kobo.14.3">However, the connections between all other layers are directed, where the arrows point toward the layer nearest to the data.</span></p><p><span class="koboSpan" id="kobo.15.1">Except for the first and last layer of the stack, each layer of a DBN serves two purposes. </span><span class="koboSpan" id="kobo.15.2">First, it acts as a hidden layer for its predecessor layer, and as a visible layer or input for its next layer. </span><span class="koboSpan" id="kobo.15.3">DBNs are mainly used to cluster, recognize and generate video sequences and images.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.16.1"><img src="graphics/image_05_016.jpg" alt="Deep Belief networks"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.17.1">Figure 5.9: A DBN composed of three RBMs is shown in figure</span></p><div class="section" title="Greedy layer-wise training"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec50"/><span class="koboSpan" id="kobo.18.1">Greedy layer-wise training</span></h2></div></div></div><p><span class="koboSpan" id="kobo.19.1">A greedy layer-wise training algorithm for training DBNs was proposed in 2006 [126]. </span><span class="koboSpan" id="kobo.19.2">This algorithm trains the DBN one layer at a time. </span><span class="koboSpan" id="kobo.19.3">In this approach, an RBM is first trained, which takes the actual data as input and models it.</span></p><p><span class="koboSpan" id="kobo.20.1">A one-level DBN is an RBM. </span><span class="koboSpan" id="kobo.20.2">The core philosophy of greedy layer-wise approach is that after training the top-level RBM of an m-level DBN, the interpretation of the parameters changes while adding them in a (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.21.1">m+1</span></em></span><span class="koboSpan" id="kobo.22.1">) level DBN. </span><span class="koboSpan" id="kobo.22.2">In an RBM, between layers (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.23.1">m-1</span></em></span><span class="koboSpan" id="kobo.24.1">) and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.25.1">m</span></em></span><span class="koboSpan" id="kobo.26.1">, the probability distribution of layer </span><span class="emphasis"><em><span class="koboSpan" id="kobo.27.1">m</span></em></span><span class="koboSpan" id="kobo.28.1"> is defined in terms of the parameters of that RBM. </span><span class="koboSpan" id="kobo.28.2">However, in the case of a DBN, the probability distribution of layer </span><span class="emphasis"><em><span class="koboSpan" id="kobo.29.1">m</span></em></span><span class="koboSpan" id="kobo.30.1"> is defined in terms of the upper layer's parameters. </span><span class="koboSpan" id="kobo.30.2">This procedure can be repeated indefinitely, to connect with as many layers of DBNs as one desires.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Distributed Deep Belief network"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec37"/><span class="koboSpan" id="kobo.1.1">Distributed Deep Belief network</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">DBNs have so far achieved a lot in numerous applications such as speech and phone recognition [127], information retrieval [128], human motion modelling[129], and so on. </span><span class="koboSpan" id="kobo.2.2">However, the sequential implementation for both RBM and DBNs come with various limitations. </span><span class="koboSpan" id="kobo.2.3">With a large-scale dataset, the models show various shortcomings in their applications due to the long, time consuming computation involved, memory demanding nature of the algorithms, and so on. </span><span class="koboSpan" id="kobo.2.4">To work with Big data, RBMs and DBNs require distributed computing to provide scalable, coherent and efficient learning.</span></p><p><span class="koboSpan" id="kobo.3.1">To make DBNs acquiescent to the large-scale dataset stored on a cluster of computers, DBNs should acquire a distributed learning approach with Hadoop and Map-Reduce. </span><span class="koboSpan" id="kobo.3.2">The paper in [130] has shown a key-value pair approach for each level of an RBM, where the pre-training is accomplished with layer-wise, in a distributed environment in Map-Reduce framework. </span><span class="koboSpan" id="kobo.3.3">The learning is performed on Hadoop by an iterative computing method for the training RBM. </span><span class="koboSpan" id="kobo.3.4">Therefore, the distributed training of DBNs is achieved by stacking multiple RBMs.</span></p><div class="section" title="Distributed training of Restricted Boltzmann machines"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec51"/><span class="koboSpan" id="kobo.4.1">Distributed training of Restricted Boltzmann machines</span></h2></div></div></div><p><span class="koboSpan" id="kobo.5.1">As mentioned in the earlier sections, the energy function for an RBM is given by:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.6.1"><img src="graphics/image_05_017.jpg" alt="Distributed training of Restricted Boltzmann machines"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.7.1">Let an input dataset </span><span class="emphasis"><em><span class="koboSpan" id="kobo.8.1">I = {x</span><sub><span class="koboSpan" id="kobo.9.1">i</span></sub><span class="koboSpan" id="kobo.10.1"> = i= 1, 2,... </span><span class="koboSpan" id="kobo.10.2">N}</span></em></span><span class="koboSpan" id="kobo.11.1"> be used for the distributed learning of a RBM. </span><span class="koboSpan" id="kobo.11.2">As discussed in the earlier section, for the learning of a DBN, the weights and biases in each level of the RBM are initialized at first by using a greedy layer-wise unsupervised training. </span><span class="koboSpan" id="kobo.11.3">The purpose of the distributed training is to learn the weights and the associated biases </span><span class="emphasis"><em><span class="koboSpan" id="kobo.12.1">b</span></em></span><span class="koboSpan" id="kobo.13.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.14.1">c</span></em></span><span class="koboSpan" id="kobo.15.1">. </span><span class="koboSpan" id="kobo.15.2">For a distributed RBM using Map-Reduce, the one Map-Reduce job is essential in every epoch.</span></p><p><span class="koboSpan" id="kobo.16.1">For matrix-matrix multiplication, Gibbs sampling is used, and for training of RBMs it takes most of the computation time. </span><span class="koboSpan" id="kobo.16.2">Therefore, to truncate the computation time for this, Gibbs sampling can be distributed in the map phase among multiple datasets running different Datanodes on the Hadoop framework.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note22"/><span class="koboSpan" id="kobo.17.1">Note</span></h3><p><span class="koboSpan" id="kobo.18.1">Gibbs sampling is a </span><span class="strong"><strong><span class="koboSpan" id="kobo.19.1">Markov chain Monte Carlo</span></strong></span><span class="koboSpan" id="kobo.20.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.21.1">MCMC</span></strong></span><span class="koboSpan" id="kobo.22.1">) algorithm for determining the sequence of observations that are estimated from a specified multivariate probability distribution, when traditional direct sampling becomes difficult.</span></p></div></div><p><span class="koboSpan" id="kobo.23.1">Initially, different parameters needed for training, such as the number of neurons for visible and hidden layers, the input layer bias </span><span class="emphasis"><em><span class="koboSpan" id="kobo.24.1">a</span></em></span><span class="koboSpan" id="kobo.25.1">, the hidden layer bias </span><span class="emphasis"><em><span class="koboSpan" id="kobo.26.1">b</span></em></span><span class="koboSpan" id="kobo.27.1">, weight </span><span class="emphasis"><em><span class="koboSpan" id="kobo.28.1">W</span></em></span><span class="koboSpan" id="kobo.29.1">, number of epochs (say </span><span class="emphasis"><em><span class="koboSpan" id="kobo.30.1">N</span></em></span><span class="koboSpan" id="kobo.31.1">), learning rate, and so on, are initialized. </span><span class="koboSpan" id="kobo.31.2">The number of epochs signify that both the map and reduce phase will iterate for </span><span class="emphasis"><em><span class="koboSpan" id="kobo.32.1">N</span></em></span><span class="koboSpan" id="kobo.33.1"> number of times. </span><span class="koboSpan" id="kobo.33.2">For each epoch, the mapper runs for each block of the Datanodes, and performs Gibbs sampling to calculate the approximate gradients of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.34.1">W</span></em></span><span class="koboSpan" id="kobo.35.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.36.1">a</span></em></span><span class="koboSpan" id="kobo.37.1">, and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.38.1">b</span></em></span><span class="koboSpan" id="kobo.39.1">. </span><span class="koboSpan" id="kobo.39.2">The reducer then updates those parameters with the computed increments needed for the next epoch. </span><span class="koboSpan" id="kobo.39.3">Hence, from the second epoch, the input value for the map phase, the updated values of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.40.1">W</span></em></span><span class="koboSpan" id="kobo.41.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.42.1">a</span></em></span><span class="koboSpan" id="kobo.43.1">, and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.44.1">b</span></em></span><span class="koboSpan" id="kobo.45.1">, are calculated from the output of the reducer in the previous epoch.</span></p><p><span class="koboSpan" id="kobo.46.1">The input dataset I is split into a number of chunks, and stored in different blocks, running on each Datanode. </span><span class="koboSpan" id="kobo.46.2">Each mapper running on the blocks will compute the approximate gradient of the weights and biases for a particular chunk stored on that block. </span><span class="koboSpan" id="kobo.46.3">The reducers then calculate the increments of respective parameters and update them accordingly. </span><span class="koboSpan" id="kobo.46.4">The process treats the resulting parameters and the updated values as the final outcome of the Map-Reduce phase of that particular epoch. </span><span class="koboSpan" id="kobo.46.5">After every epoch, the reducer decides whether to store the learned weight, if it is the final epoch or whether to increment the epoch index and propagate the key-value pair value to the next mapper.</span></p></div><div class="section" title="Distributed training of Deep Belief networks"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec52"/><span class="koboSpan" id="kobo.47.1">Distributed training of Deep Belief networks</span></h2></div></div></div><p><span class="koboSpan" id="kobo.48.1">For distributed training of DBNs with </span><span class="emphasis"><em><span class="koboSpan" id="kobo.49.1">L</span></em></span><span class="koboSpan" id="kobo.50.1"> number of hidden layers, the learning is performed with pre-training </span><span class="emphasis"><em><span class="koboSpan" id="kobo.51.1">L</span></em></span><span class="koboSpan" id="kobo.52.1"> RBMs. </span><span class="koboSpan" id="kobo.52.2">The bottom level RBM is trained as discussed, however, for the rest of the (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.53.1">L-1</span></em></span><span class="koboSpan" id="kobo.54.1">) RBMs, the input dataset is changed for each level.</span></p><p><span class="koboSpan" id="kobo.55.1">The input data for </span><span class="emphasis"><em><span class="koboSpan" id="kobo.56.1">m</span><sub><span class="koboSpan" id="kobo.57.1">th</span></sub></em></span><span class="koboSpan" id="kobo.58.1"> level (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.59.1">L </span><span class="inlinemediaobject"><span class="koboSpan" id="kobo.60.1"><img src="graphics/greater-than-equal.jpg" alt="Distributed training of Deep Belief networks"/></span></span><span class="koboSpan" id="kobo.61.1">
¥ m &gt; 1</span></em></span><span class="koboSpan" id="kobo.62.1">) RBM will be the conditional probability of hidden nodes of (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.63.1">m-1</span></em></span><span class="koboSpan" id="kobo.64.1">)</span><sub><span class="koboSpan" id="kobo.65.1">th</span></sub><span class="koboSpan" id="kobo.66.1"> level RBMs:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.67.1"><img src="graphics/Capture-9.jpg" alt="Distributed training of Deep Belief networks"/></span></div><p>
</p><div class="section" title="Distributed back propagation algorithm"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec16"/><span class="koboSpan" id="kobo.68.1">Distributed back propagation algorithm</span></h3></div></div></div><p><span class="koboSpan" id="kobo.69.1">This is the second phase of distributed training of back propagation algorithm to tune the global network. </span><span class="koboSpan" id="kobo.69.2">In this procedure, while computing the gradient of weights, the feed-forward and back-propagation methods take up the majority of the computation time. </span><span class="koboSpan" id="kobo.69.3">Hence, for each epoch, for faster execution, this procedure should be run in parallel on each mini-batch of the input dataset.</span></p><p><span class="koboSpan" id="kobo.70.1">In the first step of the procedure, the learned weights for a </span><span class="emphasis"><em><span class="koboSpan" id="kobo.71.1">L</span></em></span><span class="koboSpan" id="kobo.72.1"> level DBN, namely, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.73.1">W1</span></em></span><span class="koboSpan" id="kobo.74.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.75.1">W2</span></em></span><span class="koboSpan" id="kobo.76.1">,</span><span class="emphasis"><em><span class="koboSpan" id="kobo.77.1">...WL</span></em></span><span class="koboSpan" id="kobo.78.1"> are loaded into the memory, and other hyper-parameters are initialized. </span><span class="koboSpan" id="kobo.78.2">In this fine-tuning phase, the primary jobs for map and reduce phase are similar to that of the RBMs distributed training. </span><span class="koboSpan" id="kobo.78.3">The mapper will determine the gradients of the weights and eventually update the weight increment. </span><span class="koboSpan" id="kobo.78.4">The reducer updates the weight increments from one or more weights and passes the output to the mapper to perform the next iteration.</span></p><p><span class="koboSpan" id="kobo.79.1">The main purpose of this procedure is to obtain some discriminative power of the model by placing the label layer on top of the global network and tuning the weights of the entire layers iteratively.</span></p></div><div class="section" title="Performance evaluation of RBMs and DBNs"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec17"/><span class="koboSpan" id="kobo.80.1">Performance evaluation of RBMs and DBNs</span></h3></div></div></div><p><span class="koboSpan" id="kobo.81.1">The paper [130] performed an experiment of distributed RBM and DBN on Hadoop cluster to provide a comparative study with the traditional sequential approach. </span><span class="koboSpan" id="kobo.81.2">The experiments were carried out on MNIST datasets for hand-written digits recognition. </span><span class="koboSpan" id="kobo.81.3">There were 60,000 images for the training set, and 10,000 images for the testing set. </span><span class="koboSpan" id="kobo.81.4">The block size of HDFS is set to 64 MB with a replication factor of 4. </span><span class="koboSpan" id="kobo.81.5">All the nodes are set to run 26 mappers and 4 reducers maximum. </span><span class="koboSpan" id="kobo.81.6">Interested readers can modify the block size and replication factor to see the final results of the experiments with these parameters.</span></p><div class="section" title="Drastic improvement in training time"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec3"/><span class="koboSpan" id="kobo.82.1">Drastic improvement in training time</span></h4></div></div></div><p><span class="koboSpan" id="kobo.83.1">The purpose of this experiment is to the compare the distributed RBMs and DBNs with the traditional training policy (sequential) in terms of training time. </span><span class="koboSpan" id="kobo.83.2">The sequential programs were performed on one CPU, whereas the distributed programs were on 16 CPUs of a node. </span><span class="koboSpan" id="kobo.83.3">Both the experiments were performed on the MNIST datasets mentioned earlier. </span><span class="koboSpan" id="kobo.83.4">The results obtained are summarized in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.84.1">Table 5.1</span></em></span><span class="koboSpan" id="kobo.85.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.86.1">Table 5.2:</span></em></span>
</p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.87.1"><img src="graphics/Capture-7.jpg" alt="Drastic improvement in training time"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.88.1">Table 5.1:The table represents the time needed to complete the training of distributed and sequential RBMs</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.89.1"><img src="graphics/Capture-8.jpg" alt="Drastic improvement in training time"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.90.1">Table 5.2:The table represents the time needed to complete the training of distributed and sequential DBNs</span></p><p><span class="koboSpan" id="kobo.91.1">The data shown in the table clearly depicts the advantages of using distributed RBMs and DBNs using Hadoop, over the traditional sequential approach. </span><span class="koboSpan" id="kobo.91.2">The training time for the distributed approach for the models has shown drastic improvement over the sequential one. </span><span class="koboSpan" id="kobo.91.3">Also, one crucial advantage of using the Hadoop framework for distribution is that it scales exceptionally well with the size of the training datasets, as well as the number of machines used to distribute it.</span></p><p><span class="koboSpan" id="kobo.92.1">The next section of the chapter will demonstrate the programming approach for both the models using Deeplearning4j.</span></p></div></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Implementation using Deeplearning4j"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec38"/><span class="koboSpan" id="kobo.1.1">Implementation using Deeplearning4j</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">This section of the chapter will provide a basic idea of how to write the code for RBMs and DBNs using Deeplearning4j. </span><span class="koboSpan" id="kobo.2.2">Readers will be able to learn the syntax for using the various hyperparameters mentioned in this chapter.</span></p><p><span class="koboSpan" id="kobo.3.1">To implement RBMs and DBNs using Deeplearning4j, the whole idea is very simple. </span><span class="koboSpan" id="kobo.3.2">The overall implementation can be split into three core phases: loading data or preparation of the data, network configuration, and training and evaluation of the model.</span></p><p><span class="koboSpan" id="kobo.4.1">In this section, we will first discuss RBMs on IrisDataSet, and then we will come to the implementation of DBNs.</span></p><div class="section" title="Restricted Boltzmann machines"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec53"/><span class="koboSpan" id="kobo.5.1">Restricted Boltzmann machines</span></h2></div></div></div><p><span class="koboSpan" id="kobo.6.1">For the building and training of RBMs, first we need to define and initialize the hyperparameter needed for the model:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.7.1">Nd4j.MAX_SLICES_TO_PRINT = -1;       </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.8.1">Nd4j.MAX_ELEMENTS_PER_SLICE = -1;       </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.9.1">Nd4j.ENFORCE_NUMERICAL_STABILITY = true;       </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.10.1">final int numRows = 4;       </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.11.1">final int numColumns = 1;       </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.12.1">int outputNum = 10;       </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.13.1">int numSamples = 150;   </span></strong></span>
</pre><p><span class="koboSpan" id="kobo.14.1">The batchsize here can be initialized as </span><code class="literal"><span class="koboSpan" id="kobo.15.1">150</span></code><span class="koboSpan" id="kobo.16.1">, which means </span><code class="literal"><span class="koboSpan" id="kobo.17.1">150</span></code><span class="koboSpan" id="kobo.18.1"> samples of the dataset will be submitted to the Hadoop framework at a time. </span><span class="koboSpan" id="kobo.18.2">Rest assured all other parameters are initialized just as we did it in the earlier chapters.</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.19.1">int batchSize = 150;       </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.20.1">int iterations = 100;       </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.21.1">int seed = 123;       </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.22.1">int listenerFreq = iterations/2;   </span></strong></span>
</pre><p><span class="koboSpan" id="kobo.23.1">In the next phase, the Irisdataset is loaded into the system based on the defined </span><code class="literal"><span class="koboSpan" id="kobo.24.1">batchsize</span></code><span class="koboSpan" id="kobo.25.1"> and number of samples per batch:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.26.1">log.info("Load data....");
</span><span class="strong"><strong><span class="koboSpan" id="kobo.27.1">DataSetIterator iter = new IrisDataSetIterator(batchSize, numSamples);       </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.28.1">DataSet iris = iter.next();</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.29.1">Here, the RBM is created as a layer using </span><code class="literal"><span class="koboSpan" id="kobo.30.1">NeuralNetConfiguration.Builder()</span></code><span class="koboSpan" id="kobo.31.1">. </span><span class="koboSpan" id="kobo.31.2">Similarly, the object of Restricted Boltzmann is used to store properties such as the transforms applied to the observed and hidden layer - Gaussian and Rectified Linear Transforms, respectively:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.32.1">NeuralNetConfiguration conf = new NeuralNetConfiguration.Builder()
.regularization(true)       
</span><span class="strong"><strong><span class="koboSpan" id="kobo.33.1">  .miniBatch(true)       </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.34.1">.layer(new RBM.Builder().l2(1e-1).l1(1e-3)       </span></strong></span><span class="koboSpan" id="kobo.35.1">
    .nIn(numRows * numColumns)       
    .nOut(outputNum)</span></pre><p>
<code class="literal"><span class="koboSpan" id="kobo.36.1">ReLU</span></code><span class="koboSpan" id="kobo.37.1"> is used for activation function:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.38.1">.activation("relu")       </span></strong></span>
</pre><p>
<code class="literal"><span class="koboSpan" id="kobo.39.1">weightInit()</span></code><span class="koboSpan" id="kobo.40.1"> function is used for initialization of the weight, which represents the starting value of the coefficients needed to amplify the input signal coming into each node:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.41.1">.weightInit(WeightInit.RELU)    </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.42.1">    .lossFunction(LossFunctions.LossFunction.RECONSTRUCTION
     _CROSSENTROPY.k(3)       </span></strong></span>
</pre><p><span class="koboSpan" id="kobo.43.1">Gaussian Transformation is used for visible units and Rectified Linear Transformation is used for hidden layers. </span><span class="koboSpan" id="kobo.43.2">This is very simple in Deeplearning4j. </span><span class="koboSpan" id="kobo.43.3">We need to pass the parameters </span><code class="literal"><span class="koboSpan" id="kobo.44.1">VisibleUnit</span></code><span class="koboSpan" id="kobo.45.1">.</span><code class="literal"><span class="koboSpan" id="kobo.46.1">GAUSSIAN</span></code><span class="koboSpan" id="kobo.47.1"> and </span><code class="literal"><span class="koboSpan" id="kobo.48.1">HiddenUnit.RECTIFIED</span></code><span class="koboSpan" id="kobo.49.1"> inside the </span><code class="literal"><span class="koboSpan" id="kobo.50.1">.visibleUnit</span></code><span class="koboSpan" id="kobo.51.1"> and </span><code class="literal"><span class="koboSpan" id="kobo.52.1">.hiddenUnit</span></code><span class="koboSpan" id="kobo.53.1"> methods:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.54.1">.hiddenUnit(HiddenUnit.RECTIFIED).visibleUnit(VisibleUnit.GAUSSIAN)       </span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.55.1">                          
    .updater(Updater.ADAGRAD).gradientNormalization(Gradient
     Normalization.ClipL2PerLayer)       </span></strong></span><span class="koboSpan" id="kobo.56.1">
    .build())       
  .seed(seed)    
  .iterations(iterations) 
</span></pre><p><span class="koboSpan" id="kobo.57.1">The Backpropagation step size is defined here:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.58.1">.learningRate(1e-3)</span></strong></span><span class="koboSpan" id="kobo.59.1">
  .optimizationAlgo(OptimizationAlgorithm.LBFGS)       
  .build();       
    
Layer model = LayerFactories.getFactory(conf.getLayer()).create(conf);       
model.setListeners(new ScoreIterationListener(listenerFreq));
    
log.info("Evaluate weights....");       
INDArray w = model.getParam(DefaultParamInitializer.WEIGHT_KEY);       
log.info("Weights: " + w);</span></pre><p><span class="koboSpan" id="kobo.60.1">To scale to dataset, </span><code class="literal"><span class="koboSpan" id="kobo.61.1">scale()</span></code><span class="koboSpan" id="kobo.62.1"> can be called with the object of the Dataset class:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.63.1">iris.scale();   </span></strong></span>
</pre><p><span class="koboSpan" id="kobo.64.1">After the evaluation was done in the earlier process, the model is now completely ready to be trained. </span><span class="koboSpan" id="kobo.64.2">It can be trained in a similar manner using the </span><code class="literal"><span class="koboSpan" id="kobo.65.1">fit()</span></code><span class="koboSpan" id="kobo.66.1"> method, as done for the earlier models, and passing </span><code class="literal"><span class="koboSpan" id="kobo.67.1">getFeatureMatrix</span></code><span class="koboSpan" id="kobo.68.1"> as the parameter:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.69.1">log.info("Train model....");       
for(int i = 0; i &lt; 20; i++)
  {       
   log.info("Epoch "+i+":");</span><span class="strong"><strong><span class="koboSpan" id="kobo.70.1">model.fit(iris.getFeatureMatrix());
  </span></strong></span><span class="koboSpan" id="kobo.71.1">}</span></pre></div><div class="section" title="Deep Belief networks"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec54"/><span class="koboSpan" id="kobo.72.1">Deep Belief networks</span></h2></div></div></div><p><span class="koboSpan" id="kobo.73.1">As explained in this chapter, a DBN is a stacked version of the number of RBMs. </span><span class="koboSpan" id="kobo.73.2">In this part, we will show how to deploy DBNs programmatically using Deeplearning4j. </span><span class="koboSpan" id="kobo.73.3">The flow of the program will follow the standard procedure as with other models. </span><span class="koboSpan" id="kobo.73.4">The implementation of simple DBNs is pretty simple using Deeplearning4j. </span><span class="koboSpan" id="kobo.73.5">The example will show how to train and traverse the input MNIST data with DBNs.</span></p><p><span class="koboSpan" id="kobo.74.1">For MNIST dataset, the following line specifies the batchsize and number of examples, which one user will specify to load the data in HDFS at one time:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.75.1">log.info("Load data....");</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.76.1">DataSetIterator iter = new MnistDataSetIterator(batchSize,numSamples,
true);</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.77.1">In the next phase, the model will be built by stacking 10 RBMs together. </span><span class="koboSpan" id="kobo.77.2">The following piece of code will specify the way this should be done using Deeplearning4j:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.78.1">log.info("Build model....");
MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
  .seed(seed)
  .iterations(iterations)
  </span><span class="strong"><strong><span class="koboSpan" id="kobo.79.1">.optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT)</span></strong></span><span class="koboSpan" id="kobo.80.1">
  .list()
  .layer(0, new RBM.Builder().nIn(numRows * numColumns).nOut(1000)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
  .layer(1, new RBM.Builder().nIn(1000).nOut(500)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
  .layer(2, new RBM.Builder().nIn(500).nOut(250)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
  .layer(3, new RBM.Builder().nIn(250).nOut(100)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
  .layer(4, new RBM.Builder().nIn(100).nOut(30)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())  
  .layer(5, new RBM.Builder().nIn(30).nOut(100)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())  
  .layer(6, new RBM.Builder().nIn(100).nOut(250)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
  .layer(7, new RBM.Builder().nIn(250).nOut(500)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
  .layer(8, new  RBM.Builder().nIn(500).nOut(1000)
  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
  .layer(9, new OutputLayer.Builder(LossFunctions.LossFunction.
   </span><span class="koboSpan" id="kobo.80.2">RMSE_XENT).nIn(1000).nOut(numRows*numColumns).build())
  </span><span class="strong"><strong><span class="koboSpan" id="kobo.81.1">.pretrain(true)
  .backprop(true)</span></strong></span><span class="koboSpan" id="kobo.82.1">
  .build();
    
MultiLayerNetwork model = new MultiLayerNetwork(conf);
model.init();
</span></pre><p><span class="koboSpan" id="kobo.83.1">In the last part, the code will be trained using the loaded MNIST dataset, by calling the </span><code class="literal"><span class="koboSpan" id="kobo.84.1">fit()</span></code><span class="koboSpan" id="kobo.85.1"> method:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.86.1">log.info("Train model....");
while(iter.hasNext())
  {
   DataSet next = iter.next();
   </span><span class="strong"><strong><span class="koboSpan" id="kobo.87.1">model.fit(new DataSet(next.getFeatureMatrix(),next.
   </span><span class="koboSpan" id="kobo.87.2">getFeatureMatrix()));</span></strong></span><span class="koboSpan" id="kobo.88.1">
  }
</span></pre><p><span class="koboSpan" id="kobo.89.1">Upon executing the code, the process will give a following output:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.90.1">Load data.... 
</span><span class="koboSpan" id="kobo.90.2">Build model.... 
</span><span class="koboSpan" id="kobo.90.3">Train model.... 
 
 
</span><span class="koboSpan" id="kobo.90.4">o.d.e.u.d.DeepAutoEncoderExample - Train model.... 
</span><span class="koboSpan" id="kobo.90.5">o.d.n.m.MultiLayerNetwork - Training on layer 1 with 1000 examples 
o.d.o.l.ScoreIterationListener - Score at iteration 0 is 394.462 
o.d.n.m.MultiLayerNetwork - Training on layer 2 with 1000 examples 
o.d.o.l.ScoreIterationListener - Score at iteration 1 is 506.785 
o.d.n.m.MultiLayerNetwork - Training on layer 3 with 1000 examples 
o.d.o.l.ScoreIterationListener - Score at iteration 2 is 255.582 
o.d.n.m.MultiLayerNetwork - Training on layer 4 with 1000 examples 
o.d.o.l.ScoreIterationListener - Score at iteration 3 is 128.227 
 
......................................... 
 
 
</span><span class="koboSpan" id="kobo.90.6">o.d.n.m.MultiLayerNetwork - Finetune phase 
o.d.o.l.ScoreIterationListener - Score at iteration 9 is 132.45428125 
 
 
........................... 
 
</span><span class="koboSpan" id="kobo.90.7">o.d.n.m.MultiLayerNetwork - Finetune phase 
o.d.o.l.ScoreIterationListener - Score at iteration 31 is 135.949859375 
o.d.o.l.ScoreIterationListener - Score at iteration 32 is 135.9501875 
o.d.n.m.MultiLayerNetwork - Training on layer 1 with 1000 examples 
o.d.o.l.ScoreIterationListener - Score at iteration 33 is 394.182 
o.d.n.m.MultiLayerNetwork - Training on layer 2 with 1000 examples 
o.d.o.l.ScoreIterationListener - Score at iteration 34 is 508.769 
o.d.n.m.MultiLayerNetwork - Training on layer 3 with 1000 examples 
 
 
 
............................ 
 
 
</span><span class="koboSpan" id="kobo.90.8">o.d.n.m.MultiLayerNetwork - Finetune phase 
o.d.o.l.ScoreIterationListener - Score at iteration 658 is 142.4304375 
o.d.o.l.ScoreIterationListener - Score at iteration 659 is 142.4311875 
</span></pre></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec39"/><span class="koboSpan" id="kobo.1.1">Summary</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">The RBM is a generative model, which can randomly produce visible data values when some latent or hidden parameters are supplied to it. </span><span class="koboSpan" id="kobo.2.2">In this chapter, we have discussed the concept and mathematical model of the Boltzmann machine, which is an energy-based model. </span><span class="koboSpan" id="kobo.2.3">The chapter then discusses and gives a visual representation of the RBM. </span><span class="koboSpan" id="kobo.2.4">Further, this chapter discusses CRBM, which is a combination of Convolution and RBMs to extract the features of high dimensional images. </span><span class="koboSpan" id="kobo.2.5">We then moved toward popular DBNs that are nothing but a stacked implementation of RBMs. </span><span class="koboSpan" id="kobo.2.6">The chapter further discusses the approach to distribute the training of RBMs as well as DBNs in the Hadoop framework.</span></p><p><span class="koboSpan" id="kobo.3.1">We conclude the chapter by providing code samples for both the models. </span><span class="koboSpan" id="kobo.3.2">The next chapter of the book will introduce one more generative model called autoencoder and its various forms such as de-noising autoencoder, deep autoencoder, and so on.</span></p></div></div></div></body></html>