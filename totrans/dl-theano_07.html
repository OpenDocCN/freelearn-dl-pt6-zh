<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;7.&#xA0;Classifying Images with Residual Networks" id="27GQ61-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07" class="calibre1"/>Chapter 7. Classifying Images with Residual Networks</h1></div></div></div><p class="calibre8">This chapter presents state-of-the-art deep networks for image classification.</p><p class="calibre8">Residual networks have become the latest architecture, with a huge improvement in accuracy and greater simplicity.</p><p class="calibre8">Before <a id="id284" class="calibre1"/>residual networks, there had been a long history of architectures, such <a id="id285" class="calibre1"/>as <span class="strong"><strong class="calibre2">AlexNet</strong></span>, <span class="strong"><strong class="calibre2">VGG</strong></span>, <span class="strong"><strong class="calibre2">Inception </strong></span>(<span class="strong"><strong class="calibre2">GoogLeNet</strong></span>), <span class="strong"><strong class="calibre2">Inception v2,v3, and v4</strong></span>. Researchers <a id="id286" class="calibre1"/>were searching for different concepts and <a id="id287" class="calibre1"/>discovered some underlying rules with which to design better architectures.</p><p class="calibre8">This <a id="id288" class="calibre1"/>chapter <a id="id289" class="calibre1"/>will address the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Main datasets for image classification evaluation</li><li class="listitem">Network architectures for image classification</li><li class="listitem">Batch normalization</li><li class="listitem">Global average pooling</li><li class="listitem">Residual connections</li><li class="listitem">Stochastic depth</li><li class="listitem">Dense connections</li><li class="listitem">Multi-GPU</li><li class="listitem">Data augmentation techniques</li></ul></div></div>

<div class="book" title="Chapter&#xA0;7.&#xA0;Classifying Images with Residual Networks" id="27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="Natural image datasets"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch07lvl1sec67" class="calibre1"/>Natural image datasets</h1></div></div></div><p class="calibre8">Image <a id="id290" class="calibre1"/>classification usually includes a wider range of objects and scenes than the MNIST handwritten digits. Most of them are natural images, meaning images that a human being would observe in the real world, such as landscapes, indoor scenes, roads, mountains, beaches, people, animals, and automobiles, as opposed to synthetic images or images generated by a computer.</p><p class="calibre8">To evaluate the performance of image classification networks for natural images, three main <a id="id291" class="calibre1"/>datasets are usually used by researchers to compare performance:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Cifar-10, a dataset of 60,000 small images (32x32) regrouped into 10 classes only, which you can easily download:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">wget</strong></span> https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -P /sharedfiles
<span class="strong"><strong class="calibre2">tar</strong></span> xvzf /sharedfiles/cifar-10-python.tar.gz -C /sharedfiles/</pre></div><p class="calibre24">Here are some example images for each class:</p><div class="mediaobject"><img src="../images/00106.jpeg" alt="Natural image datasets" class="calibre9"/><div class="caption"><p class="calibre29">Cifar 10 dataset classes with samples <a class="calibre1" href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></p></div></div><p class="calibre27"> </p></li><li class="listitem">Cifar-100, a dataset of 60,000 images, partitioned into 100 classes and 20 super-classes</li><li class="listitem">ImageNet, a dataset of 1.2 million images, labeled with a wide range of classes (1,000). Since ImageNet is intended for non-commercial use only, it is possible to download Food 101, a dataset of 101 classes of meals, and 1,000 images per class:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">wget</strong></span> http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz -P /sharedfiles
<span class="strong"><strong class="calibre2">tar</strong></span> xvzf food-101.tar.gz -C /sharedfiles/</pre></div></li></ul></div><p class="calibre8">Before <a id="id292" class="calibre1"/>introducing residual architectures, let us discuss two methods to improve classification net accuracy: batch normalization, and global average pooling.</p></div></div>

<div class="book" title="Chapter&#xA0;7.&#xA0;Classifying Images with Residual Networks" id="27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="Natural image datasets">
<div class="book" title="Batch normalization"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec23" class="calibre1"/>Batch normalization</h2></div></div></div><p class="calibre8">Deeper <a id="id293" class="calibre1"/>networks, with more than 100 layers <a id="id294" class="calibre1"/>can help image classification for a few hundred classes. The major issue with deep networks is to ensure that the flows of inputs, as well as the gradients, are well propagated from one end of the network to the other end.</p><p class="calibre8">Nevertheless, it is not unusual that nonlinearities in the network get saturated, and gradients <a id="id295" class="calibre1"/>become null. Moreover, each layer in the network has to adapt to constant changes in the distribution of its inputs, a phenomenon known as <span class="strong"><strong class="calibre2">internal covariate shift</strong></span>.</p><p class="calibre8">It is known that a network trains faster with input data linearly processed to have zero mean <a id="id296" class="calibre1"/>and unit variance (known as <span class="strong"><strong class="calibre2">network input normalization</strong></span>), and normalizing each input feature independently, instead of jointly.</p><p class="calibre8">To <a id="id297" class="calibre1"/>normalize the input of every layer in a network, it is a bit more complicated: zeroing the mean of the input will ignore the learned bias of the previous <a id="id298" class="calibre1"/>layer, and the problem is even worse with unit variance. The parameters of the previous layer may grow infinitely, while the loss stays constant, when inputs of the layer are normalized.</p><p class="calibre8">So, for <span class="strong"><strong class="calibre2">layer input normalization</strong></span>, a <span class="strong"><strong class="calibre2">batch normalization layer </strong></span>relearns the scale and the bias after normalization:</p><div class="mediaobject"><img src="../images/00107.jpeg" alt="Batch normalization" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Rather than using the entire dataset, it uses the batch to compute the statistics for normalization, with a moving average to get closer to entire dataset statistics while training.</p><p class="calibre8">A batch normalization layer has the following benefits:</p><div class="book"><ul class="itemizedlist"><li class="listitem">It reduces the influence of bad initializations or too high learning rates</li><li class="listitem">It increases the accuracy of the net by a margin</li><li class="listitem">It accelerates the training</li><li class="listitem">It reduces overfitting, regularizing the model</li></ul></div><p class="calibre8">When <a id="id299" class="calibre1"/>introducing batch normalization layers, you can remove dropout, increase the learning rate, and reduce L2 weight <a id="id300" class="calibre1"/>normalization.</p><p class="calibre8">Be careful to place nonlinearity after the BN layer, and to remove bias in the previous layer:</p><div class="informalexample"><pre class="programlisting">l = NonlinearityLayer(
      BatchNormLayer(
        ConvLayer(l_in,
          <span class="strong"><strong class="calibre2">num_filters</strong></span>=n_filters[0],
          <span class="strong"><strong class="calibre2">filter_size</strong></span>=(3,3),
          <span class="strong"><strong class="calibre2">stride</strong></span>=(1,1),
          <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>,
          <span class="strong"><strong class="calibre2">pad</strong></span>=<span class="strong"><strong class="calibre2">'same'</strong></span>,
          <span class="strong"><strong class="calibre2">W</strong></span>=he_norm)
      ),
      <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=rectify
  )</pre></div></div></div></div>

<div class="book" title="Chapter&#xA0;7.&#xA0;Classifying Images with Residual Networks" id="27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="Natural image datasets">
<div class="book" title="Global average pooling"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec24" class="calibre1"/>Global average pooling</h2></div></div></div><p class="calibre8">Traditionally, the <a id="id301" class="calibre1"/>two last layers of a classification net are a fully connected layer and a softmax layer. The fully connected layer <a id="id302" class="calibre1"/>outputs a number of features equal to the number of classes, and the softmax layer normalizes these values to probabilities that sum to 1.</p><p class="calibre8">Firstly, it is possible to replace max-pooling layers of stride 2 with new convolutional layers of stride 2: all-convolutional networks perform even better.</p><p class="calibre8">Secondly, removing the fully connected layer is also possible. If the number of featuremaps output by the last convolutional layer is chosen equal to the number of classes, a global spatial average reduces each featuremap to a scalar value, representing the score for the class averaged at the different <span class="strong"><em class="calibre12">macro</em></span> spatial locations:</p><div class="mediaobject"><img src="../images/00108.jpeg" alt="Global average pooling" class="calibre9"/></div><p class="calibre10"> </p></div></div></div>
<div class="book" title="Residual connections"><div class="book" id="28FAO2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec68" class="calibre1"/>Residual connections</h1></div></div></div><p class="calibre8">While <a id="id303" class="calibre1"/>very deep architectures (with many layers) perform better, they are harder to train, because the input signal decreases through the layers. Some have tried training the deep networks in multiple stages.</p><p class="calibre8">An <a id="id304" class="calibre1"/>alternative to this layer-wise training is to add a supplementary connection to shortcut a block of layers, named the <span class="strong"><strong class="calibre2">identity connection</strong></span>, passing <a id="id305" class="calibre1"/>the signal without modification, in addition to the classic convolutional <a id="id306" class="calibre1"/>layers, named the <span class="strong"><strong class="calibre2">residuals</strong></span>, forming a <span class="strong"><strong class="calibre2">residual block</strong></span>, as shown in the following image:</p><div class="mediaobject"><img src="../images/00109.jpeg" alt="Residual connections" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Such <a id="id307" class="calibre1"/>a residual block is composed of six layers.</p><p class="calibre8">A residual network is a network composed of multiple residual blocks. Input is processed by a first convolution, followed by batch normalization and non-linearity:</p><div class="mediaobject"><img src="../images/00110.jpeg" alt="Residual connections" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">For example, for a residual net composed of two residual blocks, and eight featuremaps in the first convolution on an input image of size <span class="strong"><em class="calibre12">28x28</em></span>, the layer output shapes will be the following:</p><div class="informalexample"><pre class="programlisting">InputLayer                       (None, 1, 28, 28)
Conv2DDNNLayer                   (None, 8, 28, 28)
BatchNormLayer                   (None, 8, 28, 28)
NonlinearityLayer                (None, 8, 28, 28)
Conv2DDNNLayer                   (None, 8, 28, 28)
BatchNormLayer                   (None, 8, 28, 28)
NonlinearityLayer                (None, 8, 28, 28)
Conv2DDNNLayer                   (None, 8, 28, 28)
ElemwiseSumLayer                 (None, 8, 28, 28)
BatchNormLayer                   (None, 8, 28, 28)
NonlinearityLayer                (None, 8, 28, 28)
Conv2DDNNLayer                   (None, 8, 28, 28)
BatchNormLayer                   (None, 8, 28, 28)
NonlinearityLayer                (None, 8, 28, 28)
Conv2DDNNLayer                   (None, 8, 28, 28)
ElemwiseSumLayer                 (None, 8, 28, 28)
BatchNormLayer                   (None, 8, 28, 28)
NonlinearityLayer                (None, 8, 28, 28)
Conv2DDNNLayer                   (None, 16, 14, 14)
BatchNormLayer                   (None, 16, 14, 14)
NonlinearityLayer                (None, 16, 14, 14)
Conv2DDNNLayer                   (None, 16, 14, 14)
Conv2DDNNLayer                   (None, 16, 14, 14)
ElemwiseSumLayer                 (None, 16, 14, 14)
BatchNormLayer                   (None, 16, 14, 14)
NonlinearityLayer                (None, 16, 14, 14)
Conv2DDNNLayer                   (None, 16, 14, 14)
BatchNormLayer                   (None, 16, 14, 14)
NonlinearityLayer                (None, 16, 14, 14)
Conv2DDNNLayer                   (None, 16, 14, 14)
ElemwiseSumLayer                 (None, 16, 14, 14)
BatchNormLayer                   (None, 16, 14, 14)
NonlinearityLayer                (None, 16, 14, 14)
Conv2DDNNLayer                   (None, 32, 7, 7)
BatchNormLayer                   (None, 32, 7, 7)
NonlinearityLayer                (None, 32, 7, 7)
Conv2DDNNLayer                   (None, 32, 7, 7)
Conv2DDNNLayer                   (None, 32, 7, 7)
ElemwiseSumLayer                 (None, 32, 7, 7)
BatchNormLayer                   (None, 32, 7, 7)
NonlinearityLayer                (None, 32, 7, 7)
Conv2DDNNLayer                   (None, 32, 7, 7)
BatchNormLayer                   (None, 32, 7, 7)
NonlinearityLayer                (None, 32, 7, 7)
Conv2DDNNLayer                   (None, 32, 7, 7)
ElemwiseSumLayer                 (None, 32, 7, 7)
BatchNormLayer                   (None, 32, 7, 7)
NonlinearityLayer                (None, 32, 7, 7)
GlobalPoolLayer                  (None, 32)
DenseLayer                       (None, 10)</pre></div><p class="calibre8">The <a id="id308" class="calibre1"/>number of output featuremaps increase while the size of each output featuremap decreases: such a technique in funnel of <span class="strong"><strong class="calibre2">decreasing featuremap sizes/increasing the number of dimensions keeps the number</strong></span> of parameters per layer constant which is a common best practice for building networks.</p><p class="calibre8">Three transitions to increase the number of dimensions occur, one before the first residual block, a second one after n residual blocks, and a third one after <span class="strong"><em class="calibre12">2xn</em></span> residual blocks. Between each transition, the number of filters are defined in an array:</p><div class="informalexample"><pre class="programlisting"># 8 -&gt; 8 -&gt; 16 -&gt; 32
n_filters = {0:8, 1:8, 2:16, 3:32}</pre></div><p class="calibre8">The dimensional increase is performed by the first layer of the corresponding residual block. Since the input is not the same shape as the output, the simple identity connection cannot <a id="id309" class="calibre1"/>be concatenated with the output of layers of the block, and is replaced by a dimensional projection to reduce the size of the output to the dimension of the block output. Such a projection can be done with a convolution of kernel <span class="strong"><em class="calibre12">1x1</em></span> with a stride of <code class="email">2</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> residual_block(l, transition=<span class="strong"><strong class="calibre2">False</strong></span>, first=<span class="strong"><strong class="calibre2">False</strong></span>, filters=16):
    <span class="strong"><strong class="calibre2">if</strong></span> transition:
        first_stride = (2,2)
    <span class="strong"><strong class="calibre2">else</strong></span>:
        first_stride = (1,1)

    <span class="strong"><strong class="calibre2">if</strong></span> first:
        bn_pre_relu = l
    <span class="strong"><strong class="calibre2">else</strong></span>:
        bn_pre_conv = BatchNormLayer(<span class="strong"><strong class="calibre2">l</strong></span>)
        bn_pre_relu = NonlinearityLayer(bn_pre_conv, rectify)

    conv_1 = NonlinearityLayer(BatchNormLayer(ConvLayer(bn_pre_relu, <span class="strong"><strong class="calibre2">num_filters</strong></span>=filters, <span class="strong"><strong class="calibre2">filter_size</strong></span>=(3,3), <span class="strong"><strong class="calibre2">stride</strong></span>=first_stride, 
          <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, 
          <span class="strong"><strong class="calibre2">pad</strong></span>='same', 
          <span class="strong"><strong class="calibre2">W</strong></span>=he_norm)),<span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">rectify</strong></span>)

    conv_2 = ConvLayer(conv_1, <span class="strong"><strong class="calibre2">num_filters</strong></span>=filters, <span class="strong"><strong class="calibre2">filter_size</strong></span>=(3,3), <span class="strong"><strong class="calibre2">stride</strong></span>=(1,1), <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, <span class="strong"><strong class="calibre2">pad</strong></span>=<span class="strong"><strong class="calibre2">'</strong></span>same', <span class="strong"><strong class="calibre2">W</strong></span>=he_norm)

  
  # add shortcut connections
    <span class="strong"><strong class="calibre2">if </strong></span>transition:
        # projection shortcut, as option B in paper
        projection = ConvLayer(bn_pre_relu, <span class="strong"><strong class="calibre2">num_filters</strong></span>=filters, <span class="strong"><strong class="calibre2">filter_size</strong></span>=(1,1), <span class="strong"><strong class="calibre2">stride</strong></span>=(2,2), <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, <span class="strong"><strong class="calibre2">pad</strong></span>='same', <span class="strong"><strong class="calibre2">b</strong></span>=None)
    <span class="strong"><strong class="calibre2">elif</strong></span> conv_2.output_shape<span class="strong"><strong class="calibre2"> ==</strong></span> l.output_shape:
        projection=l
    <span class="strong"><strong class="calibre2">else:</strong></span>
        projection = ConvLayer(bn_pre_relu, <span class="strong"><strong class="calibre2">num_filters</strong></span>=filters, <span class="strong"><strong class="calibre2">filter_size</strong></span>=(1,1), <span class="strong"><strong class="calibre2">stride</strong></span>=(1,1), <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, <span class="strong"><strong class="calibre2">pad</strong></span>='same', <span class="strong"><strong class="calibre2">b</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>)

    <span class="strong"><strong class="calibre2">return</strong></span> ElemwiseSumLayer([conv_2, projection])</pre></div><p class="calibre8">Some variants of residual blocks have been invented as well.</p><p class="calibre8">A <a id="id310" class="calibre1"/>wide version (Wide-ResNet) of the previous residual block simply consists of increasing the number of outputs per residual blocks by a factor as they come to the end:</p><div class="informalexample"><pre class="programlisting">n_filters = {0:num_filters, 1:num_filters*width, 2:num_filters*2*width, 3:num_filters*4*width}</pre></div><p class="calibre8">A bottleneck version consists of reducing the number of parameters per layer, to create a bottleneck that has the effect of dimension reduction, implementing the Hebbian theory <span class="strong"><em class="calibre12">Neurons that fire together wire together</em></span>, and to help residual blocks capture particular types of pattern in the signal:</p><div class="mediaobject"><img src="../images/00111.jpeg" alt="Residual connections" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Bottlenecks are reductions in both featuremap size and number of output at the same time, not keeping the number of parameters constant per layer as in the previous practice:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> residual_bottleneck_block(l, <span class="strong"><strong class="calibre2">transition</strong></span>=<span class="strong"><strong class="calibre2">False</strong></span>, <span class="strong"><strong class="calibre2">first</strong></span>=<span class="strong"><strong class="calibre2">False</strong></span>, <span class="strong"><strong class="calibre2">filters</strong></span>=16):
    <span class="strong"><strong class="calibre2">if</strong></span> transition:
        first_stride = (2,2)
    <span class="strong"><strong class="calibre2">else:</strong></span>
        first_stride = (1,1)

    <span class="strong"><strong class="calibre2">if</strong></span> first:
        bn_pre_relu = l
   <span class="strong"><strong class="calibre2"> else:</strong></span>
        bn_pre_conv = BatchNormLayer(l)
        bn_pre_relu = NonlinearityLayer(bn_pre_conv, rectify)

    bottleneck_filters = filters / 4

    conv_1 = NonlinearityLayer(BatchNormLayer(ConvLayer(bn_pre_relu, <span class="strong"><strong class="calibre2">num_filters</strong></span>=bottleneck_filters,<span class="strong"><strong class="calibre2"> filter_size</strong></span>=(1,1), <span class="strong"><strong class="calibre2">stride</strong></span>=(1,1), <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, <span class="strong"><strong class="calibre2">pad</strong></span>='same', <span class="strong"><strong class="calibre2">W</strong></span>=he_norm)),<span class="strong"><strong class="calibre2">nonlinearity</strong></span>=rectify)

    conv_2 = NonlinearityLayer(BatchNormLayer(ConvLayer(conv_1, <span class="strong"><strong class="calibre2">num_filters</strong></span>=bottleneck_filters, <span class="strong"><strong class="calibre2">filter_size</strong></span>=(3,3), <span class="strong"><strong class="calibre2">stride</strong></span>=first_stride, <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>,<span class="strong"><strong class="calibre2"> pad</strong></span>='same', <span class="strong"><strong class="calibre2">W</strong></span>=he_norm)),<span class="strong"><strong class="calibre2">nonlinearity</strong></span>=rectify)

    conv_3 = ConvLayer(conv_2, <span class="strong"><strong class="calibre2">num_filters</strong></span>=filters, <span class="strong"><strong class="calibre2">filter_size</strong></span>=(1,1), <span class="strong"><strong class="calibre2">stride</strong></span>=(1,1), <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=None, <span class="strong"><strong class="calibre2">pad</strong></span>='same', <span class="strong"><strong class="calibre2">W</strong></span>=he_norm)

   <span class="strong"><strong class="calibre2"> if</strong></span> transition:
        projection = ConvLayer(bn_pre_relu, <span class="strong"><strong class="calibre2">num_filters</strong></span>=filters, <span class="strong"><strong class="calibre2">filter_size</strong></span>=(1,1), <span class="strong"><strong class="calibre2">stride</strong></span>=(2,2), <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, <span class="strong"><strong class="calibre2">pad</strong></span>='same', <span class="strong"><strong class="calibre2">b</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>)
    <span class="strong"><strong class="calibre2">elif</strong></span> first:
        <span class="strong"><strong class="calibre2">projection</strong></span> = ConvLayer(bn_pre_relu, <span class="strong"><strong class="calibre2">num_filters</strong></span>=filters, <span class="strong"><strong class="calibre2">filter_size</strong></span>=(1,1), <span class="strong"><strong class="calibre2">stride</strong></span>=(1,1),<span class="strong"><strong class="calibre2"> nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, <span class="strong"><strong class="calibre2">pad</strong></span>='same', <span class="strong"><strong class="calibre2">b</strong></span>=None)
    <span class="strong"><strong class="calibre2">else:</strong></span>
        projection = l

    <span class="strong"><strong class="calibre2">return</strong></span> ElemwiseSumLayer([conv_3, projection])</pre></div><p class="calibre8">Now, the <a id="id311" class="calibre1"/>full network of three stacks of residual blocks is built with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> model(shape, n=18, num_filters=16, num_classes=10, width=1, <span class="strong"><strong class="calibre2">block</strong></span>='normal'):
  l_in = InputLayer(shape=(<span class="strong"><strong class="calibre2">None</strong></span>, shape[1], shape[2], shape[3]))
  l = NonlinearityLayer(BatchNormLayer(ConvLayer(l_in, <span class="strong"><strong class="calibre2">num_</strong></span>filters=n_filters[0], <span class="strong"><strong class="calibre2">filter_size</strong></span>=(3,3), <span class="strong"><strong class="calibre2">stride</strong></span>=(1,1), <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, <span class="strong"><strong class="calibre2">pad</strong></span>='same', <span class="strong"><strong class="calibre2">W</strong></span>=he_norm)),<span class="strong"><strong class="calibre2">nonlinearity</strong></span>=rectify)

  l = residual_block(l, <span class="strong"><strong class="calibre2">first</strong></span>=<span class="strong"><strong class="calibre2">True</strong></span>, <span class="strong"><strong class="calibre2">filters</strong></span>=n_filters[1])
 <span class="strong"><strong class="calibre2"> for </strong></span>_ <span class="strong"><strong class="calibre2">in</strong></span> range(1,n):
      l = residual_block(l, <span class="strong"><strong class="calibre2">filters</strong></span>=n_filters[1])

  l = residual_block(l, <span class="strong"><strong class="calibre2">transition</strong></span>=True, <span class="strong"><strong class="calibre2">filters</strong></span>=n_filters[2])
  <span class="strong"><strong class="calibre2">for</strong></span> _ <span class="strong"><strong class="calibre2">in</strong></span> range(1,n):
      l = residual_block(l, <span class="strong"><strong class="calibre2">filters</strong></span>=n_filters[2])

  l = residual_block(l, <span class="strong"><strong class="calibre2">transition</strong></span>=True, <span class="strong"><strong class="calibre2">filters</strong></span>=n_filters[3])
  <span class="strong"><strong class="calibre2">for</strong></span> _ <span class="strong"><strong class="calibre2">in</strong></span> range(1,n):
      l = residual_block(l, filters=n_filters[3])

  bn_post_conv = BatchNormLayer(l)
  bn_post_relu = NonlinearityLayer(bn_post_conv, rectify)
  avg_pool = GlobalPoolLayer(bn_post_relu)
  <span class="strong"><strong class="calibre2">return </strong></span>DenseLayer(avg_pool, <span class="strong"><strong class="calibre2">num_units</strong></span>=num_classes, <span class="strong"><strong class="calibre2">W</strong></span>=HeNormal(), <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=softmax)</pre></div><p class="calibre8">The <a id="id312" class="calibre1"/>command for a MNIST training:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">    python</strong></span> train.py --dataset=mnist --n=1 --num_filters=8 --batch_size=500</pre></div><p class="calibre8">This gives a top-1 accuracy of 98%.</p><p class="calibre8">On Cifar 10, residual networks with more than a 100 layers require the batch size to be reduced to 64 to fit into the GPU's memory:</p><div class="book"><ul class="itemizedlist"><li class="listitem">For ResNet-110 (6 x 18 + 2):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">    python</strong></span> train.py --dataset=cifar10 --n=18 --num_filters=16 --batch_size=64</pre></div></li><li class="listitem">ResNet-164 (6 x 27 + 2):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">    python</strong></span> train.py --dataset=cifar10 --n=27 --num_filters=16 --batch_size=64</pre></div></li><li class="listitem">Wide ResNet-110:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">    python</strong></span> train.py --dataset=cifar10 --n=18 --num_filters=16 --width=4 --batch_size=64</pre></div></li><li class="listitem">With ResNet-bottleneck-164:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">    python</strong></span> train.py --dataset=cifar10 --n=18 --num_filters=16 --block=bottleneck --batch_size=64</pre></div></li><li class="listitem">For Food-101, I reduce further the batch size for ResNet 110:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">    python</strong></span> train.py --dataset=food101 --batch_size=10 --n=18 --num_filters=16</pre></div></li></ul></div></div>
<div class="book" title="Stochastic depth" id="29DRA1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec69" class="calibre1"/>Stochastic depth</h1></div></div></div><p class="calibre8">Since the <a id="id313" class="calibre1"/>propagation of the signal through the layers might be prone to errors in any of the residual blocks, the idea of stochastic depth is to train the network to robustness by randomly removing some of the residual blocks, and replacing them with an identity connection.</p><p class="calibre8">First, the training is much faster, since the number of parameters is lower. Second,in practice, the <a id="id314" class="calibre1"/>robustness is proven and it provides better classification results:</p><div class="mediaobject"><img src="../images/00112.jpeg" alt="Stochastic depth" class="calibre9"/></div><p class="calibre10"> </p></div>
<div class="book" title="Dense connections" id="2ACBS1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec70" class="calibre1"/>Dense connections</h1></div></div></div><p class="calibre8">Stochastic <a id="id315" class="calibre1"/>depth skips some random layers by creating a direct connection. Going one step further, instead of removing some random layers, another way to do the same thing is to add an identity connection with previous layers:</p><div class="mediaobject"><img src="../images/00113.jpeg" alt="Dense connections" class="calibre9"/><div class="caption"><p class="calibre29">A dense block (densely connected convolutional networks)</p></div></div><p class="calibre10"> </p><p class="calibre8">As for residual blocks, a densely connected convolutional network consists of repeating dense blocks to create a stack of layer blocks:</p><div class="mediaobject"><img src="../images/00114.jpeg" alt="Dense connections" class="calibre9"/><div class="caption"><p class="calibre29">A network with dense blocks (densely connected convolutional networks)</p></div></div><p class="calibre10"> </p><p class="calibre8">Such <a id="id316" class="calibre1"/>an architecture choice follows the same principles as those seen in <a class="calibre1" title="Chapter 10. Predicting Times Sequences with Advanced RNN" href="part0096_split_000.html#2RHM01-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 10</a>, <span class="strong"><em class="calibre12">Predicting Times Sequence with Advanced RNN</em></span>, with highway networks: the identity connection helps the information to be correctly propagated and back-propagated through the network, reducing the effect of <span class="strong"><em class="calibre12">exploding/vanishing gradients</em></span> when the number of layers is high.</p><p class="calibre8">In Python, we replace our residual block with a densely connected block:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> dense_block(network, <span class="strong"><strong class="calibre2">transition</strong></span>=<span class="strong"><strong class="calibre2">False</strong></span>, <span class="strong"><strong class="calibre2">first</strong></span>=<span class="strong"><strong class="calibre2">False</strong></span>, <span class="strong"><strong class="calibre2">filters</strong></span>=16):
    <span class="strong"><strong class="calibre2">if</strong></span> transition:
        network = NonlinearityLayer(BatchNormLayer(network), <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=rectify)
        network = ConvLayer(network,network.output_shape[1], 1, <span class="strong"><strong class="calibre2">pad</strong></span>='same', <span class="strong"><strong class="calibre2">W</strong></span>=he_norm, <span class="strong"><strong class="calibre2">b</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>)
        network = Pool2DLayer(network, 2, <span class="strong"><strong class="calibre2">mode</strong></span>='average_inc_pad')

    network = NonlinearityLayer(BatchNormLayer(network), <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=rectify)
    conv = ConvLayer(network,filters, 3, <span class="strong"><strong class="calibre2">pad</strong></span>='same', <span class="strong"><strong class="calibre2">W</strong></span>=he_norm, <span class="strong"><strong class="calibre2">b</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>)
    <span class="strong"><strong class="calibre2">return</strong></span> ConcatLayer([network, conv], <span class="strong"><strong class="calibre2">axis</strong></span>=1)</pre></div><p class="calibre8">Note also that batch normalization is done feature by feature and, since the output of every block is already normalized, a second renormalization is not necessary. Replacing the batch normalization layer by a simple affine layer learning the scale and bias on the concated normalized features is sufficient:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> dense_fast_block(network, <span class="strong"><strong class="calibre2">transition</strong></span>=<span class="strong"><strong class="calibre2">False</strong></span>, <span class="strong"><strong class="calibre2">first</strong></span>=<span class="strong"><strong class="calibre2">False</strong></span>, <span class="strong"><strong class="calibre2">filters</strong></span>=16):
    <span class="strong"><strong class="calibre2">if</strong></span> transition:
        network = NonlinearityLayer(BiasLayer(ScaleLayer(network)), <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=rectify)
        network = ConvLayer(network,network.output_shape[1], 1, <span class="strong"><strong class="calibre2">pad</strong></span>='same', <span class="strong"><strong class="calibre2">W</strong></span>=he_norm, <span class="strong"><strong class="calibre2">b</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>)
        network = BatchNormLayer(Pool2DLayer(network, 2, <span class="strong"><strong class="calibre2">mode</strong></span>='average_inc_pad'))

    network = NonlinearityLayer(BiasLayer(ScaleLayer(network)), <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=rectify)
    conv = ConvLayer(network,filters, 3, <span class="strong"><strong class="calibre2">pad</strong></span>='same', W=he_norm, <span class="strong"><strong class="calibre2">b</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>, <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=<span class="strong"><strong class="calibre2">None</strong></span>)
    <span class="strong"><strong class="calibre2">return</strong></span> ConcatLayer([network, BatchNormLayer(conv)], <span class="strong"><strong class="calibre2">axis</strong></span>=<span class="strong"><strong class="calibre2">1</strong></span>)</pre></div><p class="calibre8">For <a id="id317" class="calibre1"/>training DenseNet-40:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> train.py --dataset=cifar10 --n=13 --num_filters=16 --block=dense_fast --batch_size=64</pre></div></div>
<div class="book" title="Multi-GPU" id="2BASE1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec71" class="calibre1"/>Multi-GPU</h1></div></div></div><p class="calibre8">Cifar <a id="id318" class="calibre1"/>and MNIST images are still small, below 35x35 pixels. Training on natural images requires the preservation of details in the images. So, for example, a good input size is 224x224, which is 40 times more. When image classification nets with such input size have a few hundred layers, GPU memory limits the batch size to a dozen images and so training a batch takes a long time.</p><p class="calibre8">To work in multi-GPU mode:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">The model parameters are in a shared variable, meaning shared between CPU / GPU 1 / GPU 2 / GPU 3 / GPU 4, as in single GPU mode.</li><li class="listitem" value="2">The batch is divided into four splits, and each split is sent to a different GPU for the computation. The network output is computed on the split, and the gradients retro-propagated to each weight. The GPU returns the gradient values for each weight.</li><li class="listitem" value="3">The gradients for each weight are fetched back from the multiple GPU to the CPU and stacked together. The stacked gradients represent the gradient of the full initial batch.</li><li class="listitem" value="4">The update rule applies to the batch gradients and updates the shared model weights.</li></ol><div class="calibre13"/></div><p class="calibre8">See the following figure:</p><div class="mediaobject"><img src="../images/00115.jpeg" alt="Multi-GPU" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Theano <a id="id319" class="calibre1"/>stable version supports only one GPU per process, so use the first GPU in your main program and launch sub-processes for each GPU to train on. Note that the cycle described in the preceding image requires the synchronization of the update of the model to avoid each GPU training on unsynchronized models. Instead of reprogramming <a id="id320" class="calibre1"/>it yourself, a Platoon (<a class="calibre1" href="https://github.com/mila-udem/platoon">https://github.com/mila-udem/platoon</a>) framework is dedicated to train your models across multiple GPUs inside one node.</p><p class="calibre8">Note, too, that it would also be more accurate to synchronize the batch normalization mean and variance across multiple GPUs.</p></div>
<div class="book" title="Data augmentation" id="2C9D01-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec72" class="calibre1"/>Data augmentation</h1></div></div></div><p class="calibre8">Data <a id="id321" class="calibre1"/>augmentation is a very important technique to improve classification accuracy. Data augmentation consists of creating new samples from existing samples, by adding some jitters such as:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Random scale</li><li class="listitem">Random sized crop</li><li class="listitem">Horizontal flip</li><li class="listitem">Random rotation</li><li class="listitem">Lighting noise</li><li class="listitem">Brightness jittering</li><li class="listitem">Saturation jittering</li><li class="listitem">Contrast jittering</li></ul></div><p class="calibre8">This will help the model to be more robust to different lighting conditions that are very common in real life.</p><p class="calibre8">Instead <a id="id322" class="calibre1"/>of always seeing the same dataset, the model discovers different samples at each epoch.</p><p class="calibre8">Note that input normalization is also important to get better results.</p></div>
<div class="book" title="Further reading" id="2D7TI1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec73" class="calibre1"/>Further reading</h1></div></div></div><p class="calibre8">You can refer to the following titles for further insights:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Densely Connected Convolutional Networks, by Gao Huang, Zhuang Liu, Kilian Q. Weinberger, and Laurens van der Maaten, Dec 2016</li><li class="listitem">Code has been inspired by the Lasagne repository:<div class="book"><ul class="itemizedlist1"><li class="listitem"><a class="calibre1" href="https://github.com/Lasagne/Recipes/blob/master/papers/deep_residual_learning/Deep_Residual_Learning_CIFAR-10.py">https://github.com/Lasagne/Recipes/blob/master/papers/deep_residual_learning/Deep_Residual_Learning_CIFAR-10.py</a></li><li class="listitem"><a class="calibre1" href="https://github.com/Lasagne/Recipes/tree/master/papers/densenet">https://github.com/Lasagne/Recipes/tree/master/papers/densenet</a></li></ul></div></li><li class="listitem">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex Alemi, 2016</li><li class="listitem">Deep Residual Learning for Image Recognition, Kaiming He, Xiangyu Zhang, and Shaoqing Ren, Jian Sun 2015</li><li class="listitem">Rethinking the Inception Architecture for Computer Vision, Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna, 2015</li><li class="listitem">Wide Residual Networks, Sergey Zagoruyko, and Nikos Komodakis, 2016</li><li class="listitem">Identity Mappings in Deep Residual Networks, Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Jul 2016</li><li class="listitem">Network In Network, Min Lin, Qiang Chen, Shuicheng Yan, 2013</li></ul></div></div>
<div class="book" title="Summary" id="2E6E41-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec74" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">New techniques have been presented to achieve state-of-the-art classification results, such as batch normalization, global average pooling, residual connections, and dense blocks.</p><p class="calibre8">These techniques have led to the building residual networks, and densely connected networks.</p><p class="calibre8">The use of multiple GPUs helps training image classification networks, which have numerous convolutional layers, large reception fields, and for which the batched inputs of images are heavy in memory usage.</p><p class="calibre8">Lastly, we looked at how data augmentation techniques will enable an increase of the size of the dataset, reducing the potential of model overfitting, and learning weights for more robust networks.</p><p class="calibre8">In the next chapter, we'll see how to use the early layers of these networks as features to build encoder networks, as well as how to reverse the convolutions to reconstruct an output image to perform pixel-wise predictions.</p></div></body></html>