["```py\n#####################################################################\n###Chapter 2 - Introduction to Neural Networks - using R ############\n###Simple R program to build, train, test regression neural networks#\n#########################flename: Boston.r###########################\n#####################################################################\n\nlibrary(\"neuralnet\")\nlibrary(MASS)\n\nset.seed(1)\n\ndata = Boston\n\nmax_data <- apply(data, 2, max) \nmin_data <- apply(data, 2, min)\ndata_scaled <- scale(data,center = min_data, scale = max_data - min_data) \n\nindex = sample(1:nrow(data),round(0.70*nrow(data)))\ntrain_data <- as.data.frame(data_scaled[index,])\ntest_data <- as.data.frame(data_scaled[-index,])\n\nn = names(data)\nf = as.formula(paste(\"medv ~\", paste(n[!n %in% \"medv\"], collapse = \" + \")))\nnet_data = neuralnet(f,data=train_data,hidden=10,linear.output=T)\nplot(net_data)\n\npredict_net_test <- compute(net_data,test_data[,1:13])\n\npredict_net_test_start <- predict_net_test$net.result*(max(data$medv)-min(data$medv))+min(data$medv)\ntest_start <- as.data.frame((test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv))\nMSE.net_data <- sum((test_start - predict_net_test_start)^2)/nrow(test_start)\n\nRegression_Model <- lm(medv~., data=data)\nsummary(Regression_Model)\ntest <- data[-index,]\npredict_lm <- predict(Regression_Model,test)\nMSE.lm <- sum((predict_lm - test$medv)^2)/nrow(test)\n\nMSE.net_data\nMSE.lm\n###########################################################################\n\n```", "```py\nlibrary(\"neuralnet\")\nlibrary(MASS)\n```", "```py\ninstall.neuralnet\n```", "```py\nlibrary (neuralnet)\n```", "```py\nset.seed(1)\n```", "```py\ndata = Boston\n```", "```py\n> str(data)\n'data.frame': 506 obs. of 14 variables:\n $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas : int 0 0 0 0 0 0 0 0 0 0 ...\n $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm : num 6.58 6.42 7.18 7 7.15 ...\n $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis : num 4.09 4.97 4.97 6.06 6.06 ...\n $ rad : int 1 2 2 3 3 3 5 5 5 5 ...\n $ tax : num 296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ black : num 397 397 393 395 397 ...\n $ lstat : num 4.98 9.14 4.03 2.94 5.33 ...\n $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n```", "```py\nmax_data <- apply(data, 2, max) \nmin_data <- apply(data, 2, min)\ndata_scaled <- scale(data,center = min_data, scale = max_data - min_data) \n```", "```py\nmax_data <- apply(data, 2, max) \n```", "```py\nindex = sample(1:nrow(data),round(0.70*nrow(data)))\ntrain_data <- as.data.frame(data_scaled[index,])\ntest_data <- as.data.frame(data_scaled[-index,])\n```", "```py\nn = names(data)\nf = as.formula(paste(\"medv ~\", paste(n[!n %in% \"medv\"], collapse = \" + \")))\nnet_data = neuralnet(f,data=train_data,hidden=10,linear.output=T)\nplot(net_data)\n```", "```py\npredict_net_test <- compute(net_data,test_data[,1:13])\n```", "```py\npredict_net_test_start <- predict_net_test$net.result*(max(data$medv)-       min(data$medv))+min(data$medv)\ntest_start <- as.data.frame((test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv))\nMSE.net_data <- sum((predict_net_test_start - test_start)^2)/nrow(test_start)\n```", "```py\nRegression_Model <- lm(medv~., data=data)\nsummary(Regression_Model)\ntest <- data[-index,]\npredict_lm <- predict(Regression_Model,test)\nMSE.lm <- sum((predict_lm - test$medv)^2)/nrow(test)\n```", "```py\n> summary(Regression_Model)\n\nCall:\nlm(formula = medv ~ ., data = data)\n\nResiduals:\n Min 1Q Median 3Q Max \n-15.5944739 -2.7297159 -0.5180489 1.7770506 26.1992710\n\nCoefficients:\n Estimate Std. Error t value Pr(>|t|) \n(Intercept) 36.4594883851 5.1034588106 7.14407 0.00000000000328344 ***\ncrim -0.1080113578 0.0328649942 -3.28652 0.00108681 ** \nzn 0.0464204584 0.0137274615 3.38158 0.00077811 ***\nindus 0.0205586264 0.0614956890 0.33431 0.73828807 \nchas 2.6867338193 0.8615797562 3.11838 0.00192503 ** \nnox -17.7666112283 3.8197437074 -4.65126 0.00000424564380765 ***\nrm 3.8098652068 0.4179252538 9.11614 < 0.000000000000000222 ***\nage 0.0006922246 0.0132097820 0.05240 0.95822931 \ndis -1.4755668456 0.1994547347 -7.39800 0.00000000000060135 ***\nrad 0.3060494790 0.0663464403 4.61290 0.00000507052902269 ***\ntax -0.0123345939 0.0037605364 -3.28001 0.00111164 ** \nptratio -0.9527472317 0.1308267559 -7.28251 0.00000000000130884 ***\nblack 0.0093116833 0.0026859649 3.46679 0.00057286 ***\nlstat -0.5247583779 0.0507152782 -10.34715 < 0.000000000000000222 ***\n---\nSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.745298 on 492 degrees of freedom\nMultiple R-squared: 0.7406427, Adjusted R-squared: 0.7337897 \nF-statistic: 108.0767 on 13 and 492 DF, p-value: < 0.00000000000000022204\n```", "```py\nMSE.net_data\nMSE.lm\n```", "```py\n> MSE.net_data\n[1] 12.0692812\n> MSE.lm\n[1] 26.99265692\n```", "```py\n######################################################################\n###Chapter 2 - Introduction to Neural Networks - using R    ##########\n###Usuervised ML technique using Kohonen package  ####################\n######################filename: kohonen.r#############################\n######################################################################\nlibrary(\"kohonen\")\n\ndata(\"wines\")\nstr(wines)\nhead(wines)\nView (wines)\n\nset.seed(1)\nsom.wines = som(scale(wines), grid = somgrid(5, 5, \"hexagonal\"))\nsom.wines\ndim(getCodes(som.wines))\n\nplot(som.wines, main = \"Wine data Kohonen SOM\")\npar(mfrow = c(1, 1))\nplot(som.wines, type = \"changes\", main = \"Wine data: SOM\")\n\ntraining = sample(nrow(wines), 150)\nXtraining = scale(wines[training, ])\nXtest = scale(wines[-training, ],\n center = attr(Xtraining, \"scaled:center\"),\n scale = attr(Xtraining, \"scaled:scale\"))\ntrainingdata = list(measurements = Xtraining,\n vintages = vintages[training])\ntestdata = list(measurements = Xtest, vintages = vintages[-training])\nmygrid = somgrid(5, 5, \"hexagonal\")\nsom.wines = supersom(trainingdata, grid = mygrid)\n\nsom.prediction = predict(som.wines, newdata = testdata)\ntable(vintages[-training], som.prediction$predictions[[\"vintages\"]])\n######################################################################\n```", "```py\nlibrary(\"kohonen\")\n```", "```py\ndata(\"wines\")\nstr(wines)\nhead(wines)\nview (wines)\n```", "```py\nset.seed(1)\nsom.wines = som(scale(wines), grid = somgrid(5, 5, \"hexagonal\"))\ndim(getCodes(som.wines))\nplot(som.wines, main = \"Wine data Kohonen SOM\")\n```", "```py\ngraphics.off()\npar(mfrow = c(1, 1))\nplot(som.wines, type = \"changes\", main = \"Wine data: SOM\")\n```", "```py\ntraining = sample(nrow(wines), 150)\nXtraining = scale(wines[training, ])\nXtest = scale(wines[-training, ],\n center = attr(Xtraining, \"scaled:center\"),\n scale = attr(Xtraining, \"scaled:scale\"))\ntrainingdata = list(measurements = Xtraining,\n vintages = vintages[training])\ntestdata = list(measurements = Xtest, vintages = vintages[-training])\nmygrid = somgrid(5, 5, \"hexagonal\")\nsom.wines = supersom(trainingdata, grid = mygrid)\n\nsom.prediction = predict(som.wines, newdata = testdata)\ntable(vintages[-training], som.prediction$predictions[[\"vintages\"]])\n```", "```py\n> table(vintages[-training], som.prediction$predictions[[\"vintages\"]])\n\n Barbera Barolo Grignolino\n Barbera            5      0          0\n Barolo             0     11          0\n Grignolino         0      0         11\n```"]