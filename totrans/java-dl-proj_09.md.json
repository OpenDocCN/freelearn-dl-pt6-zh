["```py\n// Generate the GridMap\nint size = 4;\nfloat[][] generateGridMap() {\n        int agent = rand.nextInt(size * size);\n        int goal = rand.nextInt(size * size);\n\n        while(goal == agent)\n            goal = rand.nextInt(size * size);\n        float[][] map = new float[size][size];\n\n        for(int i = 0; i < size * size; i++)\n            map[i / size][i % size] = 0;\n        map[goal / size][goal % size] = -1;\n        map[agent / size][agent % size] = 1;\n\n        return map;\n    }\n```", "```py\nvoid printGrid(float[][] Map) {\n        for(int x = 0; x < size; x++) {\n            for(int y = 0; y < size; y++) {\n                System.out.print((int) Map[x][y]);\n            }\n            System.out.println(\" \");\n        }\n        System.out.println(\" \");\n    }\n```", "```py\n// Calculate the position of agent\nint calcAgentPos(float[][] Map) {\n        int x = -1;\n        for(int i = 0; i < size * size; i++) {\n            if(Map[i / size][i % size] == 1)\n                return i;\n        }\n        return x;\n    }\n```", "```py\n// Calculate the position of goal. The method takes the grid space as input\nint calcGoalPos(float[][] Map) {\n        int x = -1;// start from the initial position\n\n        // Then we loop over the grid size say 4x4 times\n        for(int i = 0; i < size * size; i++) {\n            // If the mapped position is the initial position, we update the position \n            if(Map[i / size][i % size] == -1)\n                return i;\n        }\n        return x; // agent cannot move to any other cell\n    }\n```", "```py\n// Get action mask\nint[] getActionMask(float[][] CurrMap) {\n        int retVal[] = { 1, 1, 1, 1 };\n\n        int agent = calcAgentPos(CurrMap); //agent current position\n        if(agent < size) // if agent's current pos is less than 4, action mask is set to 0\n            retVal[0] = 0;\n        if(agent >= (size * size - size)) // if agent's current pos is 12, we set action mask to 0 too\n            retVal[1] = 0;\n        if(agent % size == 0) // if agent's current pos is 0 or 4, we set action mask to 0 too\n            retVal[2] = 0;\n        if(agent % size == (size - 1))// if agent's current pos is 7/11/15, we set action mask to 0 too\n            retVal[3] = 0;\n\n        return retVal; // finally, we return the updated action mask. \n    }\n```", "```py\n// Show guidance move to agent \nfloat[][] doMove(float[][] CurrMap, int action) {\n        float nextMap[][] = new float[size][size];\n        for(int i = 0; i < size * size; i++)\n            nextMap[i / size][i % size] = CurrMap[i / size][i % size];\n\n        int agent = calcAgentPos(CurrMap);\n        nextMap[agent / size][agent % size] = 0;\n\n        if(action == 0) {\n            if(agent - size >= 0)\n                nextMap[(agent - size) / size][agent % size] = 1;\n            else {\n                System.out.println(\"Bad Move\");\n                System.exit(0);\n            }\n        } else if(action == 1) {\n            if(agent + size < size * size)\n                nextMap[(agent + size) / size][agent % size] = 1;\n            else {\n                System.out.println(\"Bad Move\");\n                System.exit(0);\n            }\n        } else if (action == 2) {\n            if((agent % size) - 1 >= 0)\n                nextMap[agent / size][(agent % size) - 1] = 1;\n            else {\n                System.out.println(\"Bad Move\");\n                System.exit(0);\n            }\n        } else if(action == 3) {\n            if((agent % size) + 1 < size)\n                nextMap[agent / size][(agent % size) + 1] = 1;\n            else {\n                System.out.println(\"Bad Move\");\n                System.exit(0);\n            }\n        }\n        return nextMap;\n    }\n```", "```py\n// Compute reward for an action \nfloat calcReward(float[][] CurrMap, float[][] NextMap) {\n        int newGoal = calcGoalPos(NextMap);// first, we calculate goal position for each map\n        if(newGoal == -1) // if goal position is the initial position (i.e. no move)\n            return (size * size + 1); // we reward the agent to 4*4+ 1 = 17 (i.e. maximum reward)\n        return -1f; // else we reward -1.0 for each bad move \n    }\n```", "```py\nINDArray flattenInput(int TimeStep) {\n        float flattenedInput[] = new float[size * size * 2 + 1];\n\n        for(int a = 0; a < size; a++) {\n            for(int b = 0; b < size; b++) {\n                if(FrameBuffer[a][b] == -1)\n                    flattenedInput[a * size + b] = 1;\n                else\n                    flattenedInput[a * size + b] = 0;\n                if(FrameBuffer[a][b] == 1)\n                    flattenedInput[size * size + a * size + b] = 1;\n                else\n                    flattenedInput[size * size + a * size + b] = 0;\n            }\n        }\n        flattenedInput[size * size * 2] = TimeStep;\n        return Nd4j.create(flattenedInput);\n    }\n```", "```py\nint InputLength = size * size * 2 + 1;\nint HiddenLayerCount = 150;\n\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n                .seed(12345)    //Random number generator seed for improved repeatability. Optional.\n                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n                .weightInit(WeightInit.XAVIER)\n                .updater(new Adam(0.001))\n                .l2(0.001) // l2 regularization on all layers\n                .list()\n                .layer(0, new DenseLayer.Builder()\n                        .nIn(InputLength)\n                        .nOut(HiddenLayerCount)\n                        .weightInit(WeightInit.XAVIER)\n                        .activation(Activation.RELU)\n                        .build())\n                .layer(1, new DenseLayer.Builder()\n                        .nIn(HiddenLayerCount)\n                        .nOut(HiddenLayerCount)\n                        .weightInit(WeightInit.XAVIER)\n                        .activation(Activation.RELU)\n                        .build())\n                .layer(2,new OutputLayer.Builder(LossFunction.MSE)\n                        .nIn(HiddenLayerCount)\n                        .nOut(4) // for 4 possible actions\n                        .weightInit(WeightInit.XAVIER)\n                        .activation(Activation.IDENTITY)\n                        .weightInit(WeightInit.XAVIER)\n                        .build())\n                .pretrain(false).backprop(true).build();\n```", "```py\nDeepQNetwork RLNet = new DeepQNetwork(conf, 100000, .99f, 1d, 1024, 500, 1024, InputLength, 4);\n```", "```py\nint ReplayMemoryCapacity;\nList<Replay> ReplayMemory;\ndouble Epsilon;\nfloat Discount;\n\nMultiLayerNetwork DeepQ; // Initial DeepQNet\nMultiLayerNetwork TargetDeepQ; // Target DeepQNet\n\nint BatchSize;\nint UpdateFreq;\nint UpdateCounter;\nint ReplayStartSize;\nRandom r;\n\nint InputLength;\nint NumActions;\n\nINDArray LastInput;\nint LastAction;\n```", "```py\nDeepQNetwork(MultiLayerConfiguration conf, int replayMemoryCapacity, float discount, double epsilon, int batchSize, int updateFreq, int replayStartSize, int inputLength, int numActions){\n        // First, we initialize both the DeepQNets\n DeepQ = new MultiLayerNetwork(conf);\n        DeepQ.init();\n\n        TargetDeepQ = new MultiLayerNetwork(conf);\n        TargetDeepQ.init();\n\n        // Then we initialize the target DeepQNet's params\n        TargetDeepQ.setParams(DeepQ.params());\n        ReplayMemoryCapacity = replayMemoryCapacity;\n\n        Epsilon = epsilon;\n        Discount = discount;\n\n        r = new Random();\n        BatchSize = batchSize;\n        UpdateFreq = updateFreq;\n        UpdateCounter = 0;\n\n        ReplayMemory = new ArrayList<Replay>();\n        ReplayStartSize = replayStartSize;\n        InputLength = inputLength;\n        NumActions = numActions;\n    }\n```", "```py\nGridWorld grid = new GridWorld();\ngrid.networkConstruction();\n\n// We iterate for 100 episodes\nfor(int m = 0; m < 100; m++) {\n            System.out.println(\"Episode: \" + m);\n            float CurrMap[][] = grid.generateGridMap();\n\n            grid.FrameBuffer = CurrMap;\n            int t = 0;\n            grid.printGrid(CurrMap);\n\n            for(int i = 0; i < 2 * grid.size; i++) {\n                int a = grid.RLNet.getAction(grid.flattenInput(t), grid.getActionMask(CurrMap));\n\n                float NextMap[][] = grid.doMove(CurrMap, a);\n                float r = grid.calcReward(CurrMap, NextMap);\n                grid.addToBuffer(NextMap);\n                t++;\n\n                if(r == grid.size * grid.size + 1) {\n                    grid.RLNet.observeReward(r, null, grid.getActionMask(NextMap));\n                    break;\n                }\n\n                grid.RLNet.observeReward(r, grid.flattenInput(t), grid.getActionMask(NextMap));\n                CurrMap = NextMap;\n            }\n}\n```", "```py\nvoid observeReward(float Reward, INDArray NextInputs, int NextActionMask[]){\n        addReplay(Reward, NextInputs, NextActionMask);\n\n        if(ReplayStartSize <  ReplayMemory.size())\n            networkTraining(BatchSize);\n        UpdateCounter++;\n        if(UpdateCounter == UpdateFreq){\n            UpdateCounter = 0;\n            System.out.println(\"Reconciling Networks\");\n            reconcileNetworks();\n        }\n    }    \n```", "```py\nint getAction(INDArray Inputs , int ActionMask[]){\n        LastInput = Inputs;\n        INDArray outputs = DeepQ.output(Inputs);\n\n        System.out.print(outputs + \" \");\n        if(Epsilon > r.nextDouble()) {\n             LastAction = r.nextInt(outputs.size(1));\n             while(ActionMask[LastAction] == 0)\n                 LastAction = r.nextInt(outputs.size(1));\n             System.out.println(LastAction);\n             return LastAction;\n        }        \n        LastAction = findActionMax(outputs , ActionMask);\n        System.out.println(LastAction);\n        return LastAction;\n    }\n```", "```py\nint findActionMax(INDArray NetOutputs , int ActionMask[]){\n        int i = 0;\n        while(ActionMask[i] == 0) i++;\n\n        float maxVal = NetOutputs.getFloat(i);\n        int maxValI = i;\n\n        for(; i < NetOutputs.size(1) ; i++){\n            if(NetOutputs.getFloat(i) > maxVal && ActionMask[i] == 1){\n                maxVal = NetOutputs.getFloat(i);\n                maxValI = i;\n            }\n        }\n        return maxValI;\n    }    \n```", "```py\nINDArray combineInputs(Replay replays[]){\n        INDArray retVal = Nd4j.create(replays.length , InputLength);\n        for(int i = 0; i < replays.length ; i++){\n            retVal.putRow(i, replays[i].Input);\n        }\n        return retVal;\n    }\n```", "```py\nINDArray combineNextInputs(Replay replays[]){\n        INDArray retVal = Nd4j.create(replays.length , InputLength);\n        for(int i = 0; i < replays.length ; i++){\n            if(replays[i].NextInput != null)\n                retVal.putRow(i, replays[i].NextInput);\n        }\n        return retVal;\n    }\n```", "```py\nvoid addToBuffer(float[][] nextFrame) { \n          FrameBuffer = nextFrame;\n}\n```", "```py\nvoid networkTraining(int BatchSize){\n        Replay replays[] = getMiniBatch(BatchSize);\n        INDArray CurrInputs = combineInputs(replays);\n        INDArray TargetInputs = combineNextInputs(replays);\n\n        INDArray CurrOutputs = DeepQ.output(CurrInputs);\n        INDArray TargetOutputs = TargetDeepQ.output(TargetInputs);\n\n        float y[] = new float[replays.length];\n        for(int i = 0 ; i < y.length ; i++){\n            int ind[] = { i , replays[i].Action };\n            float FutureReward = 0 ;\n            if(replays[i].NextInput != null)\n                FutureReward = findMax(TargetOutputs.getRow(i) , replays[i].NextActionMask);\n            float TargetReward = replays[i].Reward + Discount * FutureReward ;\n            CurrOutputs.putScalar(ind , TargetReward ) ;\n        }\n        //System.out.println(\"Avgerage Error: \" + (TotalError / y.length) );\n\n        DeepQ.fit(CurrInputs, CurrOutputs);\n    }\n```", "```py\nfloat findMax(INDArray NetOutputs , int ActionMask[]){\n        int i = 0;\n        while(ActionMask[i] == 0) i++;\n\n        float maxVal = NetOutputs.getFloat(i);\n        for(; i < NetOutputs.size(1) ; i++){\n            if(NetOutputs.getFloat(i) > maxVal && ActionMask[i] == 1){\n                maxVal = NetOutputs.getFloat(i);\n            }\n        }\n        return maxVal;\n    }\n```", "```py\nvoid addReplay(float reward , INDArray NextInput , int NextActionMask[]){\n        if(ReplayMemory.size() >= ReplayMemoryCapacity )\n            ReplayMemory.remove( r.nextInt(ReplayMemory.size()) );\n\n        ReplayMemory.add(new Replay(LastInput , LastAction , reward , NextInput , NextActionMask));\n    }\n```", "```py\nReplay[] getMiniBatch(int BatchSize){\n        int size = ReplayMemory.size() < BatchSize ? ReplayMemory.size() : BatchSize ;\n        Replay[] retVal = new Replay[size];\n\n        for(int i = 0 ; i < size ; i++){\n            retVal[i] = ReplayMemory.get(r.nextInt(ReplayMemory.size()));\n        }\n        return retVal;        \n    }\n```", "```py\nDeepQNetwork RLNet = new DeepQNetwork(conf, 100000, .99f, 1d, 1024, 500, 1024, InputLength, 4);\n```", "```py\nScanner keyboard = new Scanner(System.in);\nfor(int m = 0; m < 10; m++) {\n            grid.RLNet.SetEpsilon(0);\n            float CurrMap[][] = grid.generateGridMap();\n            grid.FrameBuffer = CurrMap;\n\n            int t = 0;\n            float tReward = 0;\n\n            while(true) {\n                grid.printGrid(CurrMap);\n                keyboard.nextLine();\n\n                int a = grid.RLNet.getAction(grid.flattenInput(t), grid.getActionMask(CurrMap));\n                float NextMap[][] = grid.doMove(CurrMap, a);\n                float r = grid.calcReward(CurrMap, NextMap);\n\n                tReward += r;\n                grid.addToBuffer(NextMap);\n                t++;\n                grid.RLNet.observeReward(r, grid.flattenInput(t), grid.getActionMask(NextMap));\n\n                if(r == grid.size * grid.size + 1)\n                    break;\n                CurrMap = NextMap;\n            }\n            System.out.println(\"Net Score: \" + (tReward));\n        }\n        keyboard.close();\n    }\n\n>>>\n Episode: 0\n 0000\n 01-10\n 0000\n 0000\n [[ 0.2146, 0.0337, -0.0444, -0.0311]] 2\n [[ 0.1105, 0.2139, -0.0454, 0.0851]] 0\n [[ 0.0678, 0.3976, -0.0027, 0.2667]] 1\n [[ 0.0955, 0.3379, -0.1072, 0.2957]] 3\n [[ 0.2498, 0.2510, -0.1891, 0.4132]] 0\n [[ 0.2024, 0.4142, -0.1918, 0.6754]] 2\n [[ 0.1141, 0.6838, -0.2850, 0.6557]] 1\n [[ 0.1943, 0.6514, -0.3886, 0.6868]] 0\n Episode: 1\n 0000\n 0000\n 1000\n 00-10\n [[ 0.0342, 0.1792, -0.0991, 0.0369]] 0\n [[ 0.0734, 0.2147, -0.1360, 0.0285]] 1\n [[ 0.0044, 0.1295, -0.2472, 0.1816]] 3\n [[ 0.0685, 0.0555, -0.2153, 0.2873]] 0\n [[ 0.1479, 0.0558, -0.3495, 0.3527]] 3\n [[ 0.0978, 0.3776, -0.4362, 0.4475]] 0\n [[ 0.1010, 0.3509, -0.4134, 0.5363]] 2\n [[ 0.1611, 0.3717, -0.4411, 0.7929]] 3\n ....\n Episode: 9\n 0000\n 1-100\n 0000\n 0000\n [[ 0.0483, 0.2899, -0.1125, 0.0281]] 3\n 0000\n 0000\n 0-101\n 0000\n [[ 0.0534, 0.2587, -0.1539, 0.1711]] 1\n Net Score: 10.0\n```", "```py\n<properties>\n       <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n       <java.version>1.8</java.version>\n        <nd4j.version>1.0.0-alpha</nd4j.version>\n       <dl4j.version>1.0.0-alpha</dl4j.version>\n      <datavec.version>1.0.0-alpha</datavec.version>\n       <arbiter.version>1.0.0-alpha</arbiter.version>\n      <logback.version>1.2.3</logback.version>\n</properties>\n```", "```py\n<dependency>\n       <groupId>org.nd4j</groupId>\n        <artifactId>nd4j-cuda-9.0-platform</artifactId>\n        <version>${nd4j.version}</version>\n</dependency>\n<dependency>\n       <groupId>org.deeplearning4j</groupId>\n       <artifactId>deeplearning4j-cuda-9.0</artifactId>\n       <version>${dl4j.version}</version>\n</dependency>\n```", "```py\n<dependency>\n     <groupId>org.nd4j</groupId>\n     <artifactId>nd4j-native</artifactId>\n     <version>${nd4j.version}</version>\n</dependency>\n```", "```py\nvoid reconcileNetworks(){\n     TargetDeepQ.setParams(DeepQ.params());\n    }\n```", "```py\npublic boolean saveNetwork(String ParamFileName , String JSONFileName){\n        //Write the network parameters for later use:\n        try(DataOutputStream dos = new DataOutputStream(Files.newOutputStream(Paths.get(ParamFileName)))){\n            Nd4j.write(DeepQ.params(),dos);\n        } catch(IOException e) {\n            System.out.println(\"Failed to write params\");\n            return false;\n        }\n\n        //Write the network configuration:\n        try{\n            FileUtils.write(new File(JSONFileName), DeepQ.getLayerWiseConfigurations().toJson());\n        } catch (IOException e) {\n            System.out.println(\"Failed to write json\");\n            return false;\n        }\n        return true;\n    }\n```", "```py\npublic boolean restoreNetwork(String ParamFileName , String JSONFileName){\n        //Load network configuration from disk:\n        MultiLayerConfiguration confFromJson;\n        try{\n            confFromJson = MultiLayerConfiguration.fromJson(FileUtils.readFileToString(new \n                                                            File(JSONFileName)));\n        } catch(IOException e1) {\n            System.out.println(\"Failed to load json\");\n            return false;\n        }\n\n        //Load parameters from disk:\n        INDArray newParams;\n        try(DataInputStream dis = new DataInputStream(new FileInputStream(ParamFileName))){\n            newParams = Nd4j.read(dis);\n        } catch(FileNotFoundException e) {\n            System.out.println(\"Failed to load parems\");\n            return false;\n        } catch (IOException e) {\n            System.out.println(\"Failed to load parems\");\n            return false;\n        }\n        //Create a MultiLayerNetwork from the saved configuration and parameters \n        DeepQ = new MultiLayerNetwork(confFromJson); \n        DeepQ.init(); \n\n        DeepQ.setParameters(newParams); \n        reconcileNetworks();\n        return true;        \n    }   \n```"]