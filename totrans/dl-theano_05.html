<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;5.&#xA0;Analyzing Sentiment with a Bidirectional LSTM"><div class="book" id="1P71O2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05" class="calibre1"/>Chapter 5. Analyzing Sentiment with a Bidirectional LSTM</h1></div></div></div><p class="calibre8">This chapter is a bit more practical to get a better sense of the commonly used recurrent neural networks and word embeddings presented in the two previous chapters.</p><p class="calibre8">It is also an opportunity to introduce the reader to a new application of deep learning, sentiment <a id="id226" class="calibre1"/>analysis, which is another field of <span class="strong"><strong class="calibre2">Natural Language Processing</strong></span> (<span class="strong"><strong class="calibre2">NLP</strong></span>). It is a many-to-one scheme, where a variable-length sequence of words has to be assigned to one class. An NLP problem where such a scheme can be used similarly is language detection (english, french, german, italian, and so on).</p><p class="calibre8">While the previous chapter demonstrated how to build a recurrent neural network from scratch, this chapter shows how a high-level library built on top of Theano, Keras, can help implement and train the model with prebuilt modules. Thanks to this example, the reader should be able to decide when to use Keras in their projects.</p><p class="calibre8">The following points are developed in this chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem">A recap of recurrent neural networks and word embeddings</li><li class="listitem">Sentiment analysis</li><li class="listitem">The Keras library</li><li class="listitem">Bidirectional recurrent networks</li></ul></div><p class="calibre8">Automated sentiment analysis is the problem of identifying opinions expressed in text. It normally involves the classification of text into categories such as <span class="strong"><em class="calibre12">positive</em></span>, <span class="strong"><em class="calibre12">negative</em></span>, and <span class="strong"><em class="calibre12">neutral</em></span>. Opinions are central to almost all human activities and they are key influencers of our behaviors.</p><p class="calibre8">Recently, neural networks and deep learning approaches have been used to build sentiment analysis systems. Such systems have the ability to automatically learn a set of features to overcome the drawbacks of handcrafted approaches.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Recurrent Neural Networks</strong></span> (<span class="strong"><strong class="calibre2">RNN</strong></span>) have been proved in the literature to be a very useful technique to represent sequential inputs, such as text. A special extension of recurrent <a id="id227" class="calibre1"/>neural networks called <span class="strong"><strong class="calibre2">Bi-directional Recurrent Neural Networks</strong></span> (<span class="strong"><strong class="calibre2">BRNN</strong></span>) can capture both the preceding and the following <a id="id228" class="calibre1"/>contextual information in a text.</p><p class="calibre8">In this <a id="id229" class="calibre1"/>chapter, we'll present an example to show how a bidirectional recurrent neural network using the <span class="strong"><strong class="calibre2">Long Short Term Memory</strong></span> (<span class="strong"><strong class="calibre2">LSTM</strong></span>) architecture can be used to deal with the problem of the sentiment analysis. We aim to implement a model in which, given an input of text (that is, a sequence of words), the model attempts to predict whether it is positive, negative, or neutral.</p></div>

<div class="book" title="Chapter&#xA0;5.&#xA0;Analyzing Sentiment with a Bidirectional LSTM">
<div class="book" title="Installing and configuring Keras"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch05lvl1sec52" class="calibre1"/>Installing and configuring Keras</h1></div></div></div><p class="calibre8">Keras is <a id="id230" class="calibre1"/>a high-level neural network API, written in Python and capable of running on top of either TensorFlow or Theano. It was developed to make implementing <a id="id231" class="calibre1"/>deep learning models as fast and easy as possible for research and development. You can install Keras easily using conda, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">conda</strong></span> install keras</pre></div><p class="calibre8">When writing your Python code, importing Keras will tell you which backend is used:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import keras
<span class="strong"><em class="calibre12">Using Theano backend.</em></span>
<span class="strong"><em class="calibre12">Using cuDNN version 5110 on context None</em></span>
<span class="strong"><em class="calibre12">Preallocating 10867/11439 Mb (0.950000) on cuda0</em></span>
<span class="strong"><em class="calibre12">Mapped name None to device cuda0: Tesla K80 (0000:83:00.0)</em></span>
<span class="strong"><em class="calibre12">Mapped name dev0 to device cuda0: Tesla K80 (0000:83:00.0)</em></span>
<span class="strong"><em class="calibre12">Using cuDNN version 5110 on context dev1</em></span>
<span class="strong"><em class="calibre12">Preallocating 10867/11439 Mb (0.950000) on cuda1</em></span>
<span class="strong"><em class="calibre12">Mapped name dev1 to device cuda1: Tesla K80 (0000:84:00.0)</em></span>
</pre></div><p class="calibre8">If you have installed Tensorflow, it might not use Theano. To specify which backend to use, write a Keras configuration file, <code class="email">~/.keras/keras.json:</code>
</p><div class="informalexample"><pre class="programlisting">{
    "epsilon": 1e-07,
    "floatx": "float32",
    "image_data_format": "channels_last",
    "backend": "theano"
}</pre></div><p class="calibre8">It is also possible to specify the Theano backend directly with the environment variable:</p><div class="informalexample"><pre class="programlisting">KERAS_BACKEND=theano <span class="strong"><strong class="calibre2">python</strong></span>
</pre></div><p class="calibre8">Note that <a id="id232" class="calibre1"/>the device used is the device we specified for Theano in the <code class="email">~/.theanorc </code>file. It is also <a id="id233" class="calibre1"/>possible to modify these variables with Theano environment variables:</p><div class="informalexample"><pre class="programlisting">KERAS_BACKEND=theano THEANO_FLAGS=device=cuda,floatX=float32,mode=FAST_RUN <span class="strong"><strong class="calibre2">python</strong></span>
</pre></div></div></div>

<div class="book" title="Chapter&#xA0;5.&#xA0;Analyzing Sentiment with a Bidirectional LSTM">
<div class="book" title="Installing and configuring Keras">
<div class="book" title="Programming with Keras"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec17" class="calibre1"/>Programming with Keras</h2></div></div></div><p class="calibre8">Keras provides <a id="id234" class="calibre1"/>a set of methods for data preprocessing and for building models.</p><p class="calibre8">Layers and models are callable functions on tensors and return tensors. In Keras, there is no difference between a layer/module and a model: a model can be part of a bigger model and composed of multiple layers. Such a sub-model behaves as a module, with inputs/outputs.</p><p class="calibre8">Let's create a network with two linear layers, a ReLU non-linearity in between, and a softmax output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> keras.layers <span class="strong"><strong class="calibre2">import</strong></span> Input, Dense
<span class="strong"><strong class="calibre2">from</strong></span> keras.models <span class="strong"><strong class="calibre2">import</strong></span> Model

inputs = Input(<span class="strong"><strong class="calibre2">shape</strong></span>=(784,))

x = Dense(64, <span class="strong"><strong class="calibre2">activation</strong></span>='relu')(inputs)
predictions = Dense(10, <span class="strong"><strong class="calibre2">activation</strong></span>='softmax')(x)
model = Model(<span class="strong"><strong class="calibre2">inputs</strong></span>=inputs, <span class="strong"><strong class="calibre2">outputs</strong></span>=predictions)</pre></div><p class="calibre8">The <code class="email">model</code> module contains methods to get input and output shape for either one or multiple inputs/outputs, and list the submodules of our module:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; model.input_shape
<span class="strong"><em class="calibre12">(None, 784)</em></span>

&gt;&gt;&gt; model.get_input_shape_at(0)
<span class="strong"><em class="calibre12">(None, 784)</em></span>

&gt;&gt;&gt; model.output_shape
<span class="strong"><em class="calibre12">(None, 10)</em></span>

&gt;&gt;&gt; model.get_output_shape_at(0)
<span class="strong"><em class="calibre12">(None, 10)</em></span>

&gt;&gt;&gt; model.name
<span class="strong"><em class="calibre12">'sequential_1'</em></span>

&gt;&gt;&gt; model.input
<span class="strong"><em class="calibre12">/dense_3_input</em></span>

&gt;&gt;&gt; model.output
<span class="strong"><em class="calibre12">Softmax.0</em></span>

&gt;&gt;&gt; model.get_output_at(0)
<span class="strong"><em class="calibre12">Softmax.0</em></span>

&gt;&gt;&gt; model.layers
<span class="strong"><em class="calibre12">[&lt;keras.layers.core.Dense object at 0x7f0abf7d6a90&gt;, &lt;keras.layers.core.Dense object at 0x7f0abf74af90&gt;]</em></span>
</pre></div><p class="calibre8">In order <a id="id235" class="calibre1"/>to avoid specify inputs to every layer, Keras proposes a functional way of writing models with the <code class="email">Sequential</code> module, to build a new module or model composed.</p><p class="calibre8">The following definition of the model builds exactly the same model as shown previously, with <code class="email">input_dim</code> to specify the input dimension of the block that would be unknown otherwise and generate an error:</p><div class="informalexample"><pre class="programlisting">from keras.models import Sequential
from keras.layers import Dense, Activation

model = Sequential()
model.add(Dense(<span class="strong"><strong class="calibre2">units</strong></span>=64,<span class="strong"><strong class="calibre2"> input_dim</strong></span>=784, <span class="strong"><strong class="calibre2">activation</strong></span>='relu'))
model.add(Dense(<span class="strong"><strong class="calibre2">units</strong></span>=10, <span class="strong"><strong class="calibre2">activation</strong></span>='softmax'))</pre></div><p class="calibre8">The <code class="email">model</code> is considered a module or layer that can be part of a bigger model:</p><div class="informalexample"><pre class="programlisting">model2 = Sequential()
model2.add(model)
model2.add(Dense(<span class="strong"><strong class="calibre2">units</strong></span>=10,<span class="strong"><strong class="calibre2"> activation</strong></span>='softmax'))</pre></div><p class="calibre8">Each module/model/layer can be compiled then and trained with data :</p><div class="informalexample"><pre class="programlisting">model.compile(<span class="strong"><strong class="calibre2">optimizer</strong></span>='rmsprop',
              <span class="strong"><strong class="calibre2">loss</strong></span>='categorical_crossentropy',
              <span class="strong"><strong class="calibre2">metrics</strong></span>=['accuracy'])
model.fit(data, labels)</pre></div><p class="calibre8">Let us see Keras in practice.</p></div></div></div>

<div class="book" title="Chapter&#xA0;5.&#xA0;Analyzing Sentiment with a Bidirectional LSTM">
<div class="book" title="Installing and configuring Keras">
<div class="book" title="SemEval 2013 dataset"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec18" class="calibre1"/>SemEval 2013 dataset</h2></div></div></div><p class="calibre8">Let us <a id="id236" class="calibre1"/>start by preparing the data. In this chapter, we will use the standard dataset used in the supervised task of Twitter sentiment classification (message-level) presented in the SemEval 2013 competition. It contains 3662 tweets as a training set, 575 tweets as a development set, and 1572 tweets as a testing set. Each sample in this dataset consists of the tweet ID, the polarity (positive, negative, or neutral) and the tweet.</p><p class="calibre8">Let's download <a id="id237" class="calibre1"/>the dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">wget</strong></span> http://alt.qcri.org/semeval2014/task9/data/uploads/semeval2013_task2_train.zip
<span class="strong"><strong class="calibre2">wget</strong></span> http://alt.qcri.org/semeval2014/task9/data/uploads/semeval2013_task2_dev.zip
<span class="strong"><strong class="calibre2">wget</strong></span> http://alt.qcri.org/semeval2014/task9/data/uploads/semeval2013_task2_test_fixed.zip
<span class="strong"><strong class="calibre2">unzip</strong></span> semeval2013_task2_train.zip
<span class="strong"><strong class="calibre2">unzip</strong></span> semeval2013_task2_dev.zip
<span class="strong"><strong class="calibre2">unzip</strong></span> semeval2013_task2_test_fixed.zip</pre></div><p class="calibre8">
<span class="strong"><strong class="calibre2">A</strong></span> refers to subtask A, which is message-level sentiment classification <span class="strong"><em class="calibre12">our aim of study in this chapter</em></span>, where <span class="strong"><strong class="calibre2">B</strong></span> refers to subtask B term level sentiment analysis.</p><p class="calibre8">The <code class="email">input</code> directories do not contain the labels, just the tweets. <code class="email">full</code> contains one more level of classification, <span class="strong"><em class="calibre12">subjective</em></span> or <span class="strong"><em class="calibre12">objective</em></span>. Our interest is in the <code class="email">gold</code> or <code class="email">cleansed</code> directories.</p><p class="calibre8">Let's use the script to convert them:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">pip </strong></span>install bs4
<span class="strong"><strong class="calibre2">python</strong></span> download_tweets.py train/cleansed/twitter-train-cleansed-A.tsv &gt; sem_eval2103.train
<span class="strong"><strong class="calibre2">python</strong></span> download_tweets.py dev/gold/twitter-dev-gold-A.tsv &gt; sem_eval2103.dev
<span class="strong"><strong class="calibre2">python</strong></span> download_tweets.py SemEval2013_task2_test_fixed/gold/twitter-test-gold-A.tsv &gt; sem_eval2103.test</pre></div></div></div></div>
<div class="book" title="Preprocessing text data"><div class="book" id="1Q5IA2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec53" class="calibre1"/>Preprocessing text data</h1></div></div></div><p class="calibre8">As we <a id="id238" class="calibre1"/>know, it is common to use URLs, user mentions, and hashtags frequently on Twitter. Thus, first we need to preprocess the tweets as follow.</p><p class="calibre8">Ensure that all the tokens are separated using the space. Each tweet is lowercased.</p><p class="calibre8">The URLs, user mentions, and hashtags are replaced by the <code class="email">&lt;url&gt;</code>, <code class="email">&lt;user&gt;</code>, and <code class="email">&lt;hashtag&gt;</code> tokens respectively. This step is done using the <code class="email">process</code> function, it takes a tweet as input, tokenizes it using the NLTK <code class="email">TweetTokenizer</code>, preprocesses it, and returns the set of words (token) in the tweet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">import</strong></span> re
<span class="strong"><strong class="calibre2">from</strong></span> nltk.tokenize <span class="strong"><strong class="calibre2">import</strong></span> TweetTokenizer

<span class="strong"><strong class="calibre2">def</strong></span> process(tweet):
  tknz = TweetTokenizer()
  tokens = tknz.tokenize(tweet)
  tweet = " ".join(tokens)
  tweet = tweet.lower()
  tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;amp;+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+', '&lt;url&gt;', tweet) # URLs
  tweet = re.sub(r'(?:@[\w_]+)', '&lt;user&gt;', tweet)  # user-mentions
  tweet = re.sub(r'(?:\#+[\w_]+[\w\'_\-]*[\w_]+)', '&lt;hashtag&gt;', tweet)  # hashtags
  tweet = re.sub(r'(?:(?:\d+,?)+(?:\.?\d+)?)', '&lt;number&gt;', tweet)  # numbers
  <span class="strong"><strong class="calibre2">return</strong></span> tweet.split(" ")</pre></div><p class="calibre8">For <a id="id239" class="calibre1"/>example, if we have the tweet <code class="email">RT @mhj: just an example! :D http://example.com #NLP</code>, the function process is as follows:</p><div class="informalexample"><pre class="programlisting">tweet = 'RT @mhj: just an example! :D http://example.com #NLP'
<span class="strong"><strong class="calibre2">print</strong></span>(process(tweet))</pre></div><p class="calibre8">returns</p><div class="informalexample"><pre class="programlisting">[u'rt', u'\&lt;user\&gt;', u':', u'just', u'an', u'example', u'!', u':d', u'\&lt;url\&gt;', u'\&lt;hashtag\&gt;']</pre></div><p class="calibre8">The following function is used to read the datasets and return a list of tuples, where each tuple represents one sample of (tweet, class), with the class an integer in {0, 1, or 2} defining the polarity:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def </strong></span>read_data(file_name):
  tweets = []
  labels = []
  polarity2idx = {'positive': 0, 'negative': 1, 'neutral': 2}
  <span class="strong"><strong class="calibre2">with</strong></span> open(file_name) <span class="strong"><strong class="calibre2">as</strong></span> fin:
    <span class="strong"><strong class="calibre2">for</strong></span> line <span class="strong"><strong class="calibre2">in</strong></span> fin:
      _, _, _, _, polarity, tweet = line.strip().split("\t")
      tweet = process(tweet)
      cls = polarity2idx[polarity]
      tweets.append(tweet)
      labels.append(cls)
  <span class="strong"><strong class="calibre2">return</strong></span> tweets, labels

train_file = 'sem_eval2103.train'
dev_file = 'sem_eval2103.dev'

train_tweets, y_train = read_data(train_file)
dev_tweets, y_dev = read_data(dev_file)</pre></div><p class="calibre8">Now, we can build the vocabulary, which is a dictionary to map each word to a fixed index. The following function receives as input a set of data and returns the vocabulary and maximum length of the tweets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> get_vocabulary(data):
  max_len = 0
  index = 0
  word2idx = {'&lt;unknown&gt;': index}
 <span class="strong"><strong class="calibre2"> for</strong></span> tweet <span class="strong"><strong class="calibre2">in</strong></span> data:
    max_len = max(max_len, len(tweet))
    <span class="strong"><strong class="calibre2">for </strong></span>word <span class="strong"><strong class="calibre2">in</strong></span> tweet:
      <span class="strong"><strong class="calibre2">if</strong></span> word <span class="strong"><strong class="calibre2">not in</strong></span> word2idx:
        index += 1
        word2idx[word] = index
  <span class="strong"><strong class="calibre2">return</strong></span> word2idx, max_len

word2idx, max_len = get_vocabulary(train_tweets)
vocab_size = len(word2idx)</pre></div><p class="calibre8">We also <a id="id240" class="calibre1"/>need a function to transfer each tweet or set of tweets <a id="id241" class="calibre1"/>into the indices based on the vocabulary if the words exist, or replacing <span class="strong"><strong class="calibre2">out-of-vocabulary</strong></span> (<span class="strong"><strong class="calibre2">OOV</strong></span>) words with the unknown token (index 0) as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> transfer(data, word2idx):
  transfer_data = []
  <span class="strong"><strong class="calibre2">for</strong></span> tweet <span class="strong"><strong class="calibre2">in</strong></span> data:
    tweet2vec = []
    <span class="strong"><strong class="calibre2">for</strong></span> word <span class="strong"><strong class="calibre2">in</strong></span> tweet:
      <span class="strong"><strong class="calibre2">if</strong></span> word <span class="strong"><strong class="calibre2">in</strong></span> word2idx:
        tweet2vec.append(word2idx[word])
      <span class="strong"><strong class="calibre2">else:</strong></span>
        tweet2vec.append(0)
    transfer_data.append(tweet2vec)
  <span class="strong"><strong class="calibre2">return</strong></span> transfer_data

X_train = transfer(train_tweets, word2idx)
X_dev  = transfer(dev_tweets, word2idx)</pre></div><p class="calibre8">We can save some memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">del</strong></span> train_tweets, dev_tweets</pre></div><p class="calibre8">Keras provides a helper method to pad the sequences to ensure they all have the same length, so that a batch of sequences can be represented by a tensor, and use optimized operations on tensors, either on a CPU or on a GPU.</p><p class="calibre8">By default, the method pads at the beginning, which helps get us better classification results:</p><div class="mediaobject"><img src="../images/00085.jpeg" alt="Preprocessing text data" class="calibre9"/></div><p class="calibre10"> </p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> keras.preprocessing.sequence <span class="strong"><strong class="calibre2">import</strong></span> pad_sequences
X_train = pad_sequences(X_train, <span class="strong"><strong class="calibre2">maxlen</strong></span>=max_len, <span class="strong"><strong class="calibre2">truncating</strong></span>='post')
X_dev = pad_sequences(X_dev, <span class="strong"><strong class="calibre2">maxlen</strong></span>=max_len, <span class="strong"><strong class="calibre2">truncating</strong></span>='post')</pre></div><p class="calibre8">Lastly, Keras <a id="id242" class="calibre1"/>provides a method to convert the classes into their one-hot encoding representation, by adding a dimension:</p><div class="mediaobject"><img src="../images/00086.jpeg" alt="Preprocessing text data" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">With Keras <code class="email">to_categorical</code> method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> keras.utils.np_utils <span class="strong"><strong class="calibre2">import</strong></span> to_categorical
y_train = to_categorical(y_train)
y_dev = to_categorical(y_dev)</pre></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Designing the architecture for the model"><div class="book" id="1R42S2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec54" class="calibre1"/>Designing the architecture for the model</h1></div></div></div><p class="calibre8">The main <a id="id243" class="calibre1"/>blocks of the model in this example will be the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">First, the words of the input sentence are mapped to vectors of real numbers. This step is called vector representation of words or word embedding (for more details, see <a class="calibre1" title="Chapter 3. Encoding Word into Vector" href="part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 3</a>, <span class="strong"><em class="calibre12">Encoding Word into Vector</em></span>).</li><li class="listitem">Afterwards, this sequence of vectors is represented by one fixed-length and real-valued vector using a bi-LSTM encoder. This vector summarizes the input sentence <a id="id244" class="calibre1"/>and contains semantic, syntactic, and/or sentimental information based on the word vectors.</li><li class="listitem">Finally, this vector is passed through a softmax classifier to classify the sentence into positive, negative, or neutral.</li></ul></div></div>

<div class="book" title="Designing the architecture for the model">
<div class="book" title="Vector representations of words"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec19" class="calibre1"/>Vector representations of words</h2></div></div></div><p class="calibre8">Word embeddings <a id="id245" class="calibre1"/>are an approach to distributional semantics that represents words as vectors of real numbers. Such a representation has useful clustering properties, since the words that are semantically and syntactically related are represented by similar vectors (see <a class="calibre1" title="Chapter 3. Encoding Word into Vector" href="part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 3</a>, <span class="strong"><em class="calibre12">Encoding Word into Vector</em></span>).</p><p class="calibre8">The main aim of this step is to map each word into a continuous, low-dimensional, and real-valued vector, which can later be used as an input to any model. All the word vectors are stacked into a matrix <span class="strong"><img src="../images/00087.jpeg" alt="Vector representations of words" class="calibre23"/></span>; here, <span class="strong"><em class="calibre12">N</em></span> is the vocabulary size and d the vector dimension. This matrix is called the embedding layer or the lookup table layer. The embedding matrix can be initialized using a pre-trained model such as <span class="strong"><strong class="calibre2">Word2vec</strong></span> or <span class="strong"><strong class="calibre2">Glove</strong></span>.</p><p class="calibre8">In Keras, we can simply define the embedding layer as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> keras.layers <span class="strong"><strong class="calibre2">import</strong></span> Embedding
d = 100
emb_layer = Embedding(vocab_size + 1, <span class="strong"><strong class="calibre2">output_dim</strong></span>=d, <span class="strong"><strong class="calibre2">input_length</strong></span>=max_len)</pre></div><p class="calibre8">The first parameter represents the vocabulary size, <code class="email">output_dim</code> is the vector dimension, and <code class="email">input_length</code> is the length of the input sequences.</p><p class="calibre8">Let us add this layer as the input layer to the model and declare the model as a sequential model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> keras.models <span class="strong"><strong class="calibre2">import </strong></span>Sequential
model = Sequential()
model.add(emb_layer)</pre></div></div></div>

<div class="book" title="Designing the architecture for the model">
<div class="book" title="Sentence representation using bi-LSTM"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec20" class="calibre1"/>Sentence representation using bi-LSTM</h2></div></div></div><p class="calibre8">A recurrent <a id="id246" class="calibre1"/>neural network has the ability to represent sequences such as sentences. However, in practice, learning long-term dependencies with a vanilla RNN is difficult due to vanishing/exploding gradients. As presented in the previous chapter, <span class="strong"><strong class="calibre2">Long Short-Term Memory</strong></span> (<span class="strong"><strong class="calibre2">LSTM</strong></span>) networks were designed to have more persistent memory (that is, state), specialized in keeping <a id="id247" class="calibre1"/>and transmitting long-term information, making them very useful for capturing long-term dependencies between <a id="id248" class="calibre1"/>the elements of a sequence.</p><p class="calibre8">LSTM units are the basic components of the model used in this chapter.</p><p class="calibre8">Keras proposes a method, <code class="email">TimeDistributed</code>, to clone any model in multiple time steps and make it recurrent. But for commonly used recurrent units such as LSTM, there already exists a module in Keras:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> keras.layers <span class="strong"><strong class="calibre2">import </strong></span>LSTM
rnn_size = 64
lstm = LSTM(rnn_size, <span class="strong"><strong class="calibre2">input_shape</strong></span>=(max_len, d))</pre></div><p class="calibre8">The following is identical:</p><div class="informalexample"><pre class="programlisting">lstm = LSTM(rnn_size, <span class="strong"><strong class="calibre2">input_dim</strong></span>=d, <span class="strong"><strong class="calibre2">input_length</strong></span>=max_len)</pre></div><p class="calibre8">And for the subsequent layers, we do not need to specify the input size (this is the case since the LSTM layer comes after the embedding layer), thus we can define the <code class="email">lstm</code> unit simply as follows:</p><div class="informalexample"><pre class="programlisting">lstm = LSTM(rnn_size)</pre></div><p class="calibre8">Last but not least, in this model, we'd like to use a bidirectional LSTM. It has proved to lead to better results, capturing the meaning of the current word given the previous words, as well as words appearing after:</p><div class="mediaobject"><img src="../images/00088.jpeg" alt="Sentence representation using bi-LSTM" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">To make this unit process the input bidirectionally, we can simply use Bidirectional, a bidirectional wrapper for RNNs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> keras.layers <span class="strong"><strong class="calibre2">import </strong></span>Bidirectional
bi_lstm = Bidirectional(lstm)
model.add(bi_lstm)</pre></div></div></div>

<div class="book" title="Designing the architecture for the model">
<div class="book" title="Outputting probabilities with the softmax classifier"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec21" class="calibre1"/>Outputting probabilities with the softmax classifier</h2></div></div></div><p class="calibre8">Finally, we <a id="id249" class="calibre1"/>can pass the vector obtained from <code class="email">bi_lstm</code> to a softmax classifier as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> keras.layers<span class="strong"><strong class="calibre2"> import</strong></span> Dense, Activation

nb_classes = 3
fc = Dense(nb_classes)
classifier = Activation('softmax')
model.add(fc)
model.add(classifier)</pre></div><p class="calibre8">Now, let us print the summary of the model:</p><div class="informalexample"><pre class="programlisting">print(model.summary())
Which will end with the results:
Using Theano backend:
__________________________________________________________________________________________
Layer (type)                      Output Shape        Param #         Connected to                     
=========================================================================================
embedding_1 (Embedding)           (None, 30, 100)     10000100    embedding_input_1[0][0]          
_________________________________________________________________________________________
bidirectional_1 (Bidirectional)   (None, 128)            84480          embedding_1[0][0]                
__________________________________________________________________________________________
dense_1 (Dense)                   (None, 3)                387      bidirectional_1[0][0]            
__________________________________________________________________________________________
activation_1 (Activation)         (None, 3)                  0              dense_1[0][0]                    
=========================================================================================
Total params: 10,084,967
Trainable params: 10,084,967
Non-trainable params: 0
__________________________________________________________________________________________</pre></div></div></div>
<div class="book" title="Compiling and training the model" id="1S2JE1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec55" class="calibre1"/>Compiling and training the model</h1></div></div></div><p class="calibre8">Now that <a id="id250" class="calibre1"/>the model is defined, it is ready to be compiled. To compile the model in Keras, we need to determine the optimizer, the loss function, and optionally the evaluation metrics. As we mentioned previously, the problem is to <a id="id251" class="calibre1"/>predict if the tweet is positive, negative, or neutral. This problem is known as a multi-category classification problem. Thus, the loss (or the objective) function that will be used in this example is the <code class="email">categorical_crossentropy</code>. We will use the <code class="email">rmsprop</code> optimizer and the accuracy evaluation metric.</p><p class="calibre8">In Keras, you <a id="id252" class="calibre1"/>can find state-of-the-art optimizers, objectives, and evaluation metrics implemented. Compiling the model in Keras is very easy using the compile function:</p><div class="informalexample"><pre class="programlisting">model.compile(<span class="strong"><strong class="calibre2">optimizer</strong></span>='rmsprop',
          <span class="strong"><strong class="calibre2">loss</strong></span>='categorical_crossentropy',
          <span class="strong"><strong class="calibre2">metrics</strong></span>=['accuracy'])</pre></div><p class="calibre8">We have <a id="id253" class="calibre1"/>defined the model and compiled it, and it is now ready to be trained. We can train or fit the model on the defined data by calling the fit function.</p><p class="calibre8">The training process runs for a certain number of iterations through the dataset, called epochs, which can be specified using the <code class="email">epochs</code> parameter. We can also set the number of instances that are fed to the model at each step using the <code class="email">batch_size</code> argument. In this case, we will use a small number of <code class="email">epochs</code> = <code class="email">30</code> and use a small batch size of <code class="email">10</code>. We can also evaluate the model during training by explicitly feeding the development set using the <code class="email">validation_data</code> parameter, or choosing a sub set from the training set using the <code class="email">validation_split</code> parameter. In this case, we will use the development set that we defined previously:</p><div class="informalexample"><pre class="programlisting">model.fit(<span class="strong"><strong class="calibre2">x</strong></span>=X_train, <span class="strong"><strong class="calibre2">y</strong></span>=y_train, <span class="strong"><strong class="calibre2">batch_size</strong></span>=10, <span class="strong"><strong class="calibre2">epochs</strong></span>=30, <span class="strong"><strong class="calibre2">validation_data</strong></span>=[X_dev, y_dev])</pre></div></div>
<div class="book" title="Evaluating the model" id="1T1401-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec56" class="calibre1"/>Evaluating the model</h1></div></div></div><p class="calibre8">We have <a id="id254" class="calibre1"/>trained the model on the train test and now we can evaluate the performance of the network on the test set. This can be done using the <code class="email">evaluation()</code> function. This function returns the loss value and the metrics values for the model in test mode:</p><div class="informalexample"><pre class="programlisting">test_file = 'sem_eval2103.test'
test_tweets, y_test = read_data(test_file)

X_test  = transfer(test_tweets, word2idx)

<span class="strong"><strong class="calibre2">del </strong></span>test_twee 

X_test = pad_sequences(X_test, <span class="strong"><strong class="calibre2">maxlen</strong></span>=max_len, <span class="strong"><strong class="calibre2">truncating</strong></span>='post')

y_test = to_categorical(y_test)

test_loss, test_acc = model.evaluate(X_test, y_test)

<span class="strong"><strong class="calibre2">print</strong></span>("Testing loss: {:.5}; Testing Accuracy: {:.2%}" .format(test_loss, test_acc))</pre></div></div>
<div class="book" title="Saving and loading the model" id="1TVKI1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec57" class="calibre1"/>Saving and loading the model</h1></div></div></div><p class="calibre8">To save <a id="id255" class="calibre1"/>the weights of the Keras model, simply call the <code class="email">save</code> function, and the model is serialized into <code class="email">.hdf5</code> format:</p><div class="informalexample"><pre class="programlisting">model.save('bi_lstm_sentiment.h5')</pre></div><p class="calibre8">To load <a id="id256" class="calibre1"/>the model, use the <code class="email">load_model</code> function provided by Keras as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> keras.models <span class="strong"><strong class="calibre2">import</strong></span> load_model
loaded_model = load_model('bi_lstm_sentiment.h5')</pre></div><p class="calibre8">It is now ready for evaluation and does not need to be compiled. For example, on the same test set we must obtain the same results:</p><div class="informalexample"><pre class="programlisting">test_loss, test_acc = loaded_model.evaluate(X_test, y_test)
<span class="strong"><strong class="calibre2">print</strong></span>("Testing loss: {:.5}; Testing Accuracy: {:.2%}" .format(test_loss, test_acc))</pre></div></div>
<div class="book" title="Running the example" id="1UU541-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec58" class="calibre1"/>Running the example</h1></div></div></div><p class="calibre8">To run <a id="id257" class="calibre1"/>the model, we can execute the following command line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> bilstm.py</pre></div></div>
<div class="book" title="Further reading" id="1VSLM1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec59" class="calibre1"/>Further reading</h1></div></div></div><p class="calibre8">Please refer to the following articles:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre12">SemEval Sentiment Analysis in Twitter</em></span> <a class="calibre1" href="https://www.cs.york.ac.uk/semeval-2013/task2.html">https://www.cs.york.ac.uk/semeval-2013/task2.html</a></li><li class="listitem"><span class="strong"><em class="calibre12">Personality insights with IBM Watson demo</em></span> <a class="calibre1" href="https://personality-insights-livedemo.mybluemix.net/">https://personality-insights-livedemo.mybluemix.net/</a></li><li class="listitem"><span class="strong"><em class="calibre12">Tone analyzer</em></span> <a class="calibre1" href="https://tone-analyzer-demo.mybluemix.net/">https://tone-analyzer-demo.mybluemix.net/</a></li><li class="listitem"><span class="strong"><em class="calibre12">Keras</em></span> <a class="calibre1" href="https://keras.io/">https://keras.io/</a></li><li class="listitem">Deep Speech: Scaling up end-to-end speech recognition, Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, Andrew Y. Ng, 2014</li><li class="listitem">Speech Recognition with Deep Recurrent Neural Networks, Alex Graves, Abdel-Rahman Mohamed, Geoffrey Hinton, 2013</li><li class="listitem">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin, Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh,David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu, 2015</li></ul></div></div>
<div class="book" title="Summary" id="20R681-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec60" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">This chapter acted as a review of the basic concepts introduced in the previous chapters, while introducing a new application, sentiment analysis, and a high-level library, Keras, to simplify the development of models with the Theano engine.</p><p class="calibre8">Among these basic concepts were recurrent networks, word embeddings, batch sequence padding, and class one-hot encoding. Bidirectional recurrency was presented to improve the results.</p><p class="calibre8">In the next chapter, we'll see how to apply recurrency to images, with another library, Lasagne, which is more lightweight than Keras, and will let you mix the library modules with your own code for Theano more smoothly.</p></div></body></html>