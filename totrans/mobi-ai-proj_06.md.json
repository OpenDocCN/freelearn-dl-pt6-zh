["```py\npython --version\nsudo brew install python3\nbrew install python3\npip3 install --upgrade pip\npip3 install jupyter\njupyter notebook\n```", "```py\n# first basic understanding on PyTorch \n# book: AI for Mobile application projects\n\nimport torch\nimport numpy as np\n\n# convert numpy to tensor or vise versa\nnumpy_data = np.arange(8).reshape((2, 4))\ntorch_data = torch.from_numpy(numpy_data)\n#convert tensor to array\ntensor2array = torch_data.numpy()\n\n#Print the results\nprint\n(\n '\\nnumpy array:', numpy_data, # [[0 1 2 3], [4 5 6 7]]\n '\\ntorch tensor:', torch_data, # 0 1 2 3\\n 4 5 6 7 [torch.LongTensor of size 2x3]\n '\\ntensor to array:', tensor2array, # [[0 1 2 3], [4 5 6 7]]\n)\n```", "```py\n# abs method on numpy\nnumpy_data = [-1, -2, 1, 2]\ntensor = torch.FloatTensor(numpy_data) # 32-bit floating point\n\n#print the results\nprint\n(\n '\\nabs',\n '\\nnumpy: ', np.abs(numpy_data), # [1 2 1 2]\n '\\ntorch: ', torch.abs(tensor) # [1 2 1 2]\n)\n\n# sin method on numpy\n#print the results\nprint\n(\n '\\nsin',\n '\\nnumpy: ', np.sin(numpy_data), # [-0.84147098 -0.90929743 0.84147098 0.90929743]\n '\\ntorch: ', torch.sin(tensor) # [-0.8415 -0.9093 0.8415 0.9093]\n)\n```", "```py\n\n#print the results\nprint\n(\n '\\nmean',\n '\\nnumpy: ', np.mean(data), # 0.0\n '\\ntorch: ', torch.mean(tensor) # 0.0\n)\n\n# matrix multiplication with numpy\nnumpy_data = [[1,2], [3,4]]\ntensor = torch.FloatTensor(numpy_data) # 32-bit floating point\n# correct method and print the results\nprint(\n '\\nmatrix multiplication (matmul)',\n '\\nnumpy: ', np.matmul(numpy_data, numpy_data), # [[7, 10], [15, 22]]\n '\\ntorch: ', torch.mm(tensor, tensor) # [[7, 10], [15, 22]]\n)\n```", "```py\nnumpy array: [[0 1 2 3]\n [4 5 6 7]] \ntorch tensor: tensor([[0, 1, 2, 3],\n        [4, 5, 6, 7]]) \ntensor to array: [[0 1 2 3]\n [4 5 6 7]]\n\nabs \nnumpy:  [1 2 1 2] \ntorch:  tensor([1., 2., 1., 2.])\n\nsin \nnumpy:  [-0.84147098 -0.90929743  0.84147098  0.90929743] \ntorch:  tensor([-0.8415, -0.9093,  0.8415,  0.9093])\n\nmean \nnumpy:  0.0 \ntorch:  tensor(0.)\n\nmatrix multiplication (matmul) \nnumpy:  [[ 7 10]\n [15 22]] \ntorch:  tensor([[ 7., 10.],\n        [15., 22.]])\n```", "```py\nimport torch\nfrom torch.autograd import Variable\n\n# Variable in torch is to build a computational graph,\n# So torch does not have placeholder, torch can just pass variable to the computational graph.\n\ntensor = torch.FloatTensor([[1,2,3],[4,5,6]]) # build a tensor\nvariable = Variable(tensor, requires_grad=True) # build a variable, usually for compute gradients\n\nprint(tensor) # [torch.FloatTensor of size 2x3]\nprint(variable) # [torch.FloatTensor of size 2x3]\n\n# till now the tensor and variable looks similar.\n# However, the variable is a part of the graph, it's a part of the auto-gradient.\n\n#Now we will calculate the mean value on tensor(X^2)\nt_out = torch.mean(tensor*tensor)\n\n#Now we will calculate the mean value on variable(X^2)\nv_out = torch.mean(variable*variable) \n```", "```py\n\n#print the results\nprint(t_out)\nprint(v_out) \n#result will be 7.5\n\nv_out.backward() # backpropagation from v_out\n# v_out = 1/4 * sum(variable*variable)\n# the gradients with respect to the variable, \n\n#Let's print the variable gradient\n\nprint(variable.grad)\n'''\n 0.5000 1.0000\n 1.5000 2.0000\n'''\n\nprint(\"Resultant data in the variable: \"+str(variable)) # this is data in variable\n\n\"\"\"\nVariable containing:\n 1 2\n 3 4\nWe will consider the variable as a FloatTensor\n[torch.FloatTensor of size 2x2]\n\"\"\"\n\nprint(variable.data) # this is data in tensor format\n\"\"\"\n 1 2\n 3 4\nWe will consider the variable as FloatTensor\n[torch.FloatTensor of size 2x2]\n\"\"\"\n\n#we will print the result in the numpy format\nprint(variable.data.numpy()) \n\"\"\"\n[[ 1\\. 2.]\n [ 3\\. 4.]]\n\"\"\"\n```", "```py\ntensor([[1., 2., 3.],\n        [4., 5., 6.]])\ntensor([[1., 2., 3.],\n        [4., 5., 6.]], requires_grad=True)\ntensor(15.1667)\ntensor(15.1667, grad_fn=<MeanBackward1>)\ntensor([[0.3333, 0.6667, 1.0000],\n        [1.3333, 1.6667, 2.0000]])\nData in the variabletensor([[1., 2., 3.],\n        [4., 5., 6.]], requires_grad=True)\ntensor([[1., 2., 3.],\n        [4., 5., 6.]])\n[[1\\. 2\\. 3.]\n [4\\. 5\\. 6.]]\n```", "```py\n#This line is necessary to print the output inside jupyter notebook\n%matplotlib inline\n\nimport torch\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n# dummy data for the example\n#lets declare linspace\nx = torch.linspace(-5, 5, 200) # x data (tensor), shape=(100, 1)\nx = Variable(x)\n#call numpy array to plot the results \nx_np = x.data.numpy() \n```", "```py\n\n#RelU function\ny_relu = torch.relu(x).data.numpy()\n#sigmoid method\ny_sigmoid = torch.sigmoid(x).data.numpy()\n#tanh method\ny_tanh = torch.tanh(x).data.numpy()\n#softplus method\ny_softplus = F.softplus(x).data.numpy() # there's no softplus in torch\n# y_softmax = torch.softmax(x, dim=0).data.numpy() softmax is an activation function and it deals with probability\n```", "```py\n\n#we will plot the activation function with matplotlib\nplt.figure(1, figsize=(8, 6))\nplt.subplot(221)\nplt.plot(x_np, y_relu, c='red', label='relu')\nplt.ylim((-1, 5))\nplt.legend(loc='best')\n\n#sigmoid activation function\nplt.subplot(222)\nplt.plot(x_np, y_sigmoid, c='red', label='sigmoid')\nplt.ylim((-0.2, 1.2))\nplt.legend(loc='best')\n\n#tanh activation function\nplt.subplot(223)\nplt.plot(x_np, y_tanh, c='red', label='tanh')\nplt.ylim((-1.2, 1.2))\nplt.legend(loc='best')\n\n#softplus activation function\nplt.subplot(224)\nplt.plot(x_np, y_softplus, c='red', label='softplus')\nplt.ylim((-0.2, 6))\nplt.legend(loc='best')\n\n#call the show method to draw the graph on screen\nplt.show()\n```", "```py\n%matplotlib inline\n\n#Import all the necessary libraries\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n#we will define data points for both x-axis and y-axis\n# x data (tensor), shape=(100, 1)\nx = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) \n# noisy y data (tensor), shape=(100, 1)\ny = x.pow(2) + 0.2*torch.rand(x.size()) \n\n# torch can only train on Variable, so convert them to Variable\n# x, y = Variable(x), Variable(y)\n\n# plt.scatter(x.data.numpy(), y.data.numpy())\n# plt.show()\n```", "```py\n\nclass Net(torch.nn.Module):\n def __init__(self, n_feature, n_hidden, n_output):\n super(Net, self).__init__()\n self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer\n self.predict = torch.nn.Linear(n_hidden, n_output) # output layer\n\ndef forward(self, x):\n x = F.relu(self.hidden(x)) # activation function for hidden layer\n x = self.predict(x) # linear output\n return x\n\nnet = Net(n_feature=1, n_hidden=10, n_output=1) # define the network\nprint(net) # net architecture\n\noptimizer = torch.optim.SGD(net.parameters(), lr=0.2)\nloss_func = torch.nn.MSELoss() # this is for regression mean squared loss\n\nplt.ion() # something about plotting\n\nfor t in range(200):\n prediction = net(x) # input x and predict based on x\n loss = loss_func(prediction, y) # must be (1\\. nn output, 2\\. target)\n optimizer.zero_grad() # clear gradients for next train\n loss.backward() # backpropagation, compute gradients\n optimizer.step() # apply gradients\n if t % 50 == 0:\n```", "```py\n     plt.cla()\n     plt.scatter(x.data.numpy(), y.data.numpy())\n     plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n     plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color': 'black'})\n     plt.pause(0.1)\n\nplt.ioff()\nplt.show()\n```", "```py\n%matplotlib inline\n\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1) # reproducible\n\n# make fake data\nn_data = torch.ones(100, 2)\nx0 = torch.normal(2*n_data, 1) # class0 x data (tensor), shape=(100, 2)\ny0 = torch.zeros(100) # class0 y data (tensor), shape=(100, 1)\nx1 = torch.normal(-2*n_data, 1) # class1 x data (tensor), shape=(100, 2)\ny1 = torch.ones(100) # class1 y data (tensor), shape=(100, 1)\nx = torch.cat((x0, x1), 0).type(torch.FloatTensor) # shape (200, 2) FloatTensor = 32-bit floating\ny = torch.cat((y0, y1), ).type(torch.LongTensor) # shape (200,) LongTensor = 64-bit integer\n\nclass Net(torch.nn.Module):\n def __init__(self, n_feature, n_hidden, n_output):\n super(Net, self).__init__()\n self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer\n self.out = torch.nn.Linear(n_hidden, n_output) # output layer\n\ndef forward(self, x):\n x = F.relu(self.hidden(x)) # activation function for hidden layer\n x = self.out(x)\n return x\n\nnet = Net(n_feature=2, n_hidden=10, n_output=2) # define the network\nprint(net) # net architecture\n\noptimizer = torch.optim.SGD(net.parameters(), lr=0.02)\nloss_func = torch.nn.CrossEntropyLoss() # the target label is NOT an one-hotted\n\nplt.ion() # something about plotting\n\nfor t in range(100):\n out = net(x) # input x and predict based on x\n loss = loss_func(out, y) # must be (1\\. nn output, 2\\. target), the target label is NOT one-hotted\n\noptimizer.zero_grad() # clear gradients for next train\n loss.backward() # backpropagation, compute gradients\n optimizer.step() # apply gradients\n\nif t % 10 == 0:\n```", "```py\n plt.cla()\n prediction = torch.max(out, 1)[1]\n pred_y = prediction.data.numpy()\n target_y = y.data.numpy()\n plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='RdYlGn')\n accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)\n plt.text(1.5, -4, 'Accuracy=%.2f' % accuracy, fontdict={'size': 20, 'color': 'red'})\n plt.pause(0.1)\n\nplt.ioff()\nplt.show()\n```", "```py\nNet(\n  (hidden): Linear(in_features=2, out_features=10, bias=True)\n  (out): Linear(in_features=10, out_features=2, bias=True)\n)\n\n```", "```py\nimport torch\nimport torch.nn.functional as F\n\n# replace following class code with an easy sequential network\nclass Net(torch.nn.Module):\n def __init__(self, n_feature, n_hidden, n_output):\n super(Net, self).__init__()\n self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer\n self.predict = torch.nn.Linear(n_hidden, n_output) # output layer\n\ndef forward(self, x):\n x = F.relu(self.hidden(x)) # activation function for hidden layer\n x = self.predict(x) # linear output\n return x\n\nnet1 = Net(1, 10, 1)\n```", "```py\n net2 = torch.nn.Sequential(\n torch.nn.Linear(1, 10),\n torch.nn.ReLU(),\n torch.nn.Linear(10, 1)\n)\n\nprint(net1) # net1 architecture\n\"\"\"\nNet (\n (hidden): Linear (1 -> 10)\n (predict): Linear (10 -> 1)\n)\n\"\"\"\n\nprint(net2) # net2 architecture\n\"\"\"\nSequential (\n (0): Linear (1 -> 10)\n (1): ReLU ()\n (2): Linear (10 -> 1)\n)\n\"\"\"\n```", "```py\nNet(\n  (hidden): Linear(in_features=1, out_features=10, bias=True)\n  (predict): Linear(in_features=10, out_features=1, bias=True)\n)\nSequential(\n  (0): Linear(in_features=1, out_features=10, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=10, out_features=1, bias=True)\n)\n\nOut[1]:\n'\\nSequential (\\n  (0): Linear (1 -> 10)\\n  (1): ReLU ()\\n  (2): Linear (10 -> 1)\\n)\\n'\n```", "```py\n%matplotlib inline\n\nimport torch\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1) # reproducible\n\n# fake data\nx = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) # x data (tensor), shape=(100, 1)\ny = x.pow(2) + 0.2*torch.rand(x.size()) # noisy y data (tensor), shape=(100, 1)\n\n# The code below is deprecated in Pytorch 0.4\\. Now, autograd directly supports tensors\n# x, y = Variable(x, requires_grad=False), Variable(y, requires_grad=False)\n\ndef save():\n # save net1\n net1 = torch.nn.Sequential(\n torch.nn.Linear(1, 10),\n torch.nn.ReLU(),\n torch.nn.Linear(10, 1)\n )\n optimizer = torch.optim.SGD(net1.parameters(), lr=0.5)\n loss_func = torch.nn.MSELoss()\n\nfor t in range(100):\n prediction = net1(x)\n loss = loss_func(prediction, y)\n optimizer.zero_grad()\n loss.backward()\n optimizer.step()\n\n# plot result\n plt.figure(1, figsize=(10, 3))\n plt.subplot(131)\n plt.title('Net1')\n plt.scatter(x.data.numpy(), y.data.numpy())\n plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n```", "```py\n torch.save(net1, 'net.pkl') # save entire net\n torch.save(net1.state_dict(), 'net_params.pkl') # save only the parameters\n\ndef restore_net():\n # restore entire net1 to net2\n net2 = torch.load('net.pkl')\n prediction = net2(x)\n\n# plot result\n plt.subplot(132)\n plt.title('Net2')\n plt.scatter(x.data.numpy(), y.data.numpy())\n plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n\ndef restore_params():\n # restore only the parameters in net1 to net3\n net3 = torch.nn.Sequential(\n torch.nn.Linear(1, 10),\n torch.nn.ReLU(),\n torch.nn.Linear(10, 1)\n )\n\n# copy net1's parameters into net3\n net3.load_state_dict(torch.load('net_params.pkl'))\n prediction = net3(x)\n```", "```py\n# plot result\n plt.subplot(133)\n plt.title('Net3')\n plt.scatter(x.data.numpy(), y.data.numpy())\n plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n plt.show()\n\n# save net1\nsave()\n\n# restore entire net (may slow)\nrestore_net()\n\n# restore only the net parameters\nrestore_params()\n```", "```py\nimport torch\nimport torch.utils.data as Data\n\ntorch.manual_seed(1) # reproducible\n\nBATCH_SIZE = 5\n\nx = torch.linspace(1, 10, 10) # this is x data (torch tensor)\ny = torch.linspace(10, 1, 10) # this is y data (torch tensor)\n\ntorch_dataset = Data.TensorDataset(x, y)\nloader = Data.DataLoader(\n dataset=torch_dataset, # torch TensorDataset format\n batch_size=BATCH_SIZE, # mini batch size\n shuffle=True, # random shuffle for training\n num_workers=2, # subprocesses for loading data\n)\n\ndef show_batch():\n for epoch in range(3): # train entire dataset 3 times\n for step, (batch_x, batch_y) in enumerate(loader): # for each training step\n # train your data...\n print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',\n batch_x.numpy(), '| batch y: ', batch_y.numpy())\n\nif __name__ == '__main__':\n show_batch()\n```", "```py\nEpoch:  0 | Step:  0 | batch x:  [ 5\\.  7\\. 10\\.  3\\.  4.] | batch y:  [6\\. 4\\. 1\\. 8\\. 7.]\nEpoch:  0 | Step:  1 | batch x:  [2\\. 1\\. 8\\. 9\\. 6.] | batch y:  [ 9\\. 10\\.  3\\.  2\\.  5.]\nEpoch:  1 | Step:  0 | batch x:  [ 4\\.  6\\.  7\\. 10\\.  8.] | batch y:  [7\\. 5\\. 4\\. 1\\. 3.]\nEpoch:  1 | Step:  1 | batch x:  [5\\. 3\\. 2\\. 1\\. 9.] | batch y:  [ 6\\.  8\\.  9\\. 10\\.  2.]\nEpoch:  2 | Step:  0 | batch x:  [ 4\\.  2\\.  5\\.  6\\. 10.] | batch y:  [7\\. 9\\. 6\\. 5\\. 1.]\nEpoch:  2 | Step:  1 | batch x:  [3\\. 9\\. 1\\. 8\\. 7.] | batch y:  [ 8\\.  2\\. 10\\.  3\\.  4.]\n```", "```py\n%matplotlib inline\n\nimport torch\nimport torch.utils.data as Data\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1) # reproducible\n\nLR = 0.01\nBATCH_SIZE = 32\nEPOCH = 12\n\n# dummy dataset\nx = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)\ny = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))\n\n# plot dataset\nplt.scatter(x.numpy(), y.numpy())\nplt.show()\n```", "```py\ntorch_dataset = Data.TensorDataset(x, y)\nloader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)\n\n# default network\nclass Net(torch.nn.Module):\n def __init__(self):\n super(Net, self).__init__()\n self.hidden = torch.nn.Linear(1, 20) # hidden layer\n self.predict = torch.nn.Linear(20, 1) # output layer\n\ndef forward(self, x):\n x = F.relu(self.hidden(x)) # activation function for hidden layer\n x = self.predict(x) # linear output\n return x\n\nif __name__ == '__main__':\n # different nets\n net_SGD = Net()\n net_Momentum = Net()\n net_RMSprop = Net()\n net_Adam = Net()\n nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]\n\n# different optimizers\n opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR)\n opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)\n opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)\n opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))\n optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]\n\nloss_func = torch.nn.MSELoss()\n losses_his = [[], [], [], []] # record loss\n```", "```py\n\n for epoch in range(EPOCH):\n print('Epoch: ', epoch)\n for step, (b_x, b_y) in enumerate(loader): # for each training step\n for net, opt, l_his in zip(nets, optimizers, losses_his):\n output = net(b_x) # get output for every net\n loss = loss_func(output, b_y) # compute loss for every net\n opt.zero_grad() # clear gradients for next train\n loss.backward() # backpropagation, compute gradients\n opt.step() # apply gradients\n l_his.append(loss.data.numpy()) # loss recoder\n\nlabels = ['SGD', 'Momentum', 'RMSprop', 'Adam']\n for i, l_his in enumerate(losses_his):\n plt.plot(l_his, label=labels[i])\n plt.legend(loc='best')\n plt.xlabel('Steps')\n plt.ylabel('Loss')\n plt.ylim((0, 0.2))\n plt.show()\n```", "```py\nEpoch: 0\nEpoch:  1\nEpoch:  2\nEpoch:  3\nEpoch:  4\nEpoch:  5\nEpoch:  6\nEpoch:  7\nEpoch:  8\nEpoch:  9\nEpoch:  10\nEpoch:  11\n```", "```py\nimport torch\nfrom torch import nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1) # reproducible\n\n# Hyper Parameters\nEPOCH = 1 # train the training data n times, to save time, we just train 1 epoch\nBATCH_SIZE = 64\nTIME_STEP = 28 # rnn time step / image height\nINPUT_SIZE = 28 # rnn input size / image width\nLR = 0.01 # learning rate\nDOWNLOAD_MNIST = True # set to True if haven't download the data\n\n# Mnist digital dataset\ntrain_data = dsets.MNIST(\n root='./mnist/',\n train=True, # this is training data\n transform=transforms.ToTensor(), # Converts a PIL.Image or numpy.ndarray to\n # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n download=DOWNLOAD_MNIST, # download it if you don't have it\n)\n\n```", "```py\n\nprint(train_data.train_data.size()) # (60000, 28, 28)\nprint(train_data.train_labels.size()) # (60000)\nplt.imshow(train_data.train_data[0].numpy(), cmap='gray')\nplt.title('%i' % train_data.train_labels[0])\nplt.show()\n\n# Data Loader for easy mini-batch return in training\ntrain_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n```", "```py\n\ntest_data = dsets.MNIST(root='./mnist/', train=False, transform=transforms.ToTensor())\ntest_x = test_data.test_data.type(torch.FloatTensor)[:2000]/255\\. # shape (2000, 28, 28) value in range(0,1)\ntest_y = test_data.test_labels.numpy()[:2000] # covert to numpy array\n\nclass RNN(nn.Module):\n def __init__(self):\n super(RNN, self).__init__()\n\nself.rnn = nn.LSTM( # if use nn.RNN(), it hardly learns\n input_size=INPUT_SIZE,\n hidden_size=64, # rnn hidden unit\n num_layers=1, # number of rnn layer\n batch_first=True, # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n )\n\nself.out = nn.Linear(64, 10)\n\ndef forward(self, x):\n # x shape (batch, time_step, input_size)\n # r_out shape (batch, time_step, output_size)\n # h_n shape (n_layers, batch, hidden_size)\n # h_c shape (n_layers, batch, hidden_size)\n r_out, (h_n, h_c) = self.rnn(x, None) # None represents zero initial hidden state\n\n# choose r_out at the last time step\n out = self.out(r_out[:, -1, :])\n return out\n\nrnn = RNN()\nprint(rnn)\n\noptimizer = torch.optim.Adam(rnn.parameters(), lr=LR) # optimize all cnn parameters\nloss_func = nn.CrossEntropyLoss() # the target label is not one-hotted\n\n```", "```py\nfor epoch in range(EPOCH):\n for step, (b_x, b_y) in enumerate(train_loader): # gives batch data\n b_x = b_x.view(-1, 28, 28) # reshape x to (batch, time_step, input_size)\n\noutput = rnn(b_x) # rnn output\n loss = loss_func(output, b_y) # cross entropy loss\n optimizer.zero_grad() # clear gradients for this training step\n loss.backward() # backpropagation, compute gradients\n optimizer.step() # apply gradients\n\nif step % 50 == 0:\n test_output = rnn(test_x) # (samples, time_step, input_size)\n pred_y = torch.max(test_output, 1)[1].data.numpy()\n accuracy = float((pred_y == test_y).astype(int).sum()) / float(test_y.size)\n print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)\n\n# print 10 predictions from test data\ntest_output = rnn(test_x[:10].view(-1, 28, 28))\npred_y = torch.max(test_output, 1)[1].data.numpy()\nprint(pred_y, 'prediction number')\nprint(test_y[:10], 'real number')\n```", "```py\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n100.1%\nExtracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n113.5%\nExtracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n100.4%\nExtracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n180.4%\nExtracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\nProcessing...\nDone!\ntorch.Size([60000, 28, 28])\ntorch.Size([60000])\n/usr/local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n  warnings.warn(\"train_data has been renamed data\")\n/usr/local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n warnings.warn(\"train_labels has been renamed targets\")\n```", "```py\n/usr/local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n  warnings.warn(\"test_data has been renamed data\")\n/usr/local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n  warnings.warn(\"test_labels has been renamed targets\")\n\nRNN(\n  (rnn): LSTM(28, 64, batch_first=True)\n  (out): Linear(in_features=64, out_features=10, bias=True)\n)\n```", "```py\nEpoch:  0 | train loss: 2.3156 | test accuracy: 0.12\nEpoch:  0 | train loss: 1.1875 | test accuracy: 0.57\nEpoch:  0 | train loss: 0.7739 | test accuracy: 0.68\nEpoch:  0 | train loss: 0.8689 | test accuracy: 0.73\nEpoch:  0 | train loss: 0.5322 | test accuracy: 0.83\nEpoch:  0 | train loss: 0.3657 | test accuracy: 0.83\nEpoch:  0 | train loss: 0.2960 | test accuracy: 0.88\nEpoch:  0 | train loss: 0.3869 | test accuracy: 0.90\nEpoch:  0 | train loss: 0.1694 | test accuracy: 0.92\nEpoch:  0 | train loss: 0.0869 | test accuracy: 0.93\nEpoch:  0 | train loss: 0.2825 | test accuracy: 0.91\nEpoch:  0 | train loss: 0.2392 | test accuracy: 0.94\nEpoch:  0 | train loss: 0.0994 | test accuracy: 0.91\nEpoch:  0 | train loss: 0.3731 | test accuracy: 0.94\nEpoch:  0 | train loss: 0.0959 | test accuracy: 0.94\nEpoch:  0 | train loss: 0.1991 | test accuracy: 0.95\nEpoch:  0 | train loss: 0.0711 | test accuracy: 0.94\nEpoch:  0 | train loss: 0.2882 | test accuracy: 0.96\nEpoch:  0 | train loss: 0.4420 | test accuracy: 0.95\n[7 2 1 0 4 1 4 9 5 9] prediction number\n[7 2 1 0 4 1 4 9 5 9] real number\n```", "```py\n%matplotlib inline\n\nimport torch\nfrom torch import nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1) # reproducible\n\n# Hyper Parameters\nTIME_STEP = 10 # rnn time step\nINPUT_SIZE = 1 # rnn input size\nLR = 0.02 # learning rate\n\n# show data\nsteps = np.linspace(0, np.pi*2, 100, dtype=np.float32) # float32 for converting torch FloatTensor\nx_np = np.sin(steps)\ny_np = np.cos(steps)\nplt.plot(steps, y_np, 'r-', label='target (cos)')\nplt.plot(steps, x_np, 'b-', label='input (sin)')\nplt.legend(loc='best')\nplt.show()\n\n```", "```py\n\nclass RNN(nn.Module):\n def __init__(self):\n     super(RNN, self).__init__()\n\n self.rnn = nn.RNN(\n input_size=INPUT_SIZE,\n hidden_size=32, # rnn hidden unit\n num_layers=1, # number of rnn layer\n batch_first=True, # input & output will have batch size as 1s dimension. e.g. (batch, time_step, input_size)\n )\n self.out = nn.Linear(32, 1)\n\n def forward(self, x, h_state):\n     # x (batch, time_step, input_size)\n     # h_state (n_layers, batch, hidden_size)\n     # r_out (batch, time_step, hidden_size)\n     r_out, h_state = self.rnn(x, h_state)\n\n     outs = [] # save all predictions\n     for time_step in range(r_out.size(1)):                                              outs.append(self.out(r_out[:, time_step, :]))\n     return torch.stack(outs, dim=1), h_state\n\n//instantiate RNN\nrnn = RNN()\nprint(rnn)\n```", "```py\n\"\"\"\nRNN (\n (rnn): RNN(1, 32, batch_first=True)\n (out): Linear (32 -> 1)\n)\n\"\"\"\n\n```", "```py\noptimizer = torch.optim.Adam(rnn.parameters(), lr=LR) \nloss_func = nn.MSELoss()\nh_state = None\nplt.figure(1, figsize=(12, 5))\nplt.ion() \n```", "```py\nfor step in range(100):\n start, end = step * np.pi, (step+1)*np.pi # time range\n # use sin predicts cos\n steps = np.linspace(start, end, TIME_STEP, dtype=np.float32, endpoint=False) # float32 for converting torch FloatTensor\n x_np = np.sin(steps)\n y_np = np.cos(steps)\n\n x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis]) # shape (batch, time_step, input_size)\n y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])\n\n prediction, h_state = rnn(x, h_state) # rnn output\n\nh_state = h_state.data # repack the hidden state, break the connection from last iteration\n\n loss = loss_func(prediction, y) # calculate loss\n optimizer.zero_grad() # clear gradients for this training step\n loss.backward() # backpropagation, compute gradients\n optimizer.step() # apply gradients\n```", "```py\n\n plt.plot(steps, y_np.flatten(), 'r-')\n plt.plot(steps, prediction.data.numpy().flatten(), 'b-')\n plt.draw(); plt.pause(0.05)\n\nplt.ioff()\nplt.show()\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)\nlin = nn.Linear(6, 3) # maps from R^6 to R^3, parameters A, b\n# data is 2x5\\. A maps from 6 to 3... can we map \"data\" under A?\ndata = torch.randn(2, 6)\nprint(lin(data)\n```", "```py\n$ python3 torch_nlp.py\n```", "```py\ntensor([[ 1.1105, -0.1102, -0.3235],\n        [ 0.4800,  0.1633, -0.2515]], grad_fn=<AddmmBackward>)\n```", "```py\n#let's see more about non-linearities\n#Most of the non-linearities in PyTorch are present in torch.functional which we import as F)\n# Please make a note that unlike affine maps, there are mostly no parameters in non-linearites \n# That is, they don't have weights that are updated during training.\n#This means that during training the weights are not updated.\ndata = torch.randn(2, 2)\nprint(data)\nprint(F.relu(data))\n```", "```py\ntensor([[ 0.5848, 0.2149],\n [-0.4090, -0.1663]])\ntensor([[0.5848, 0.2149],\n [0.0000, 0.0000]])\n```", "```py\ndata = [(\"El que lee mucho y anda mucho, ve mucho y sabe mucho\".split(), \"SPANISH\"),\n (\"The one who reads a lot and walks a lot, sees a lot and knows a lot.\".split(), \"ENGLISH\"),\n (\"Nunca es tarde si la dicha es buena\".split(), \"SPANISH\"),\n (\"It is never late if the joy is good\".split(), \"ENGLISH\")]\n\ntest_data = [(\"Que cada palo aguante su vela\".split(), \"SPANISH\"),\n (\"May every mast hold its own sail\".split(), \"ENGLISH\")]\n\n#each word in the vocabulary is mapped to an unique integer using word_to_ix, and that will be considered as that word's index in BOW\n\nword_to_ix = {}\nfor sent, _ in data + test_data:\n for word in sent:\n if word not in word_to_ix:\n word_to_ix[word] = len(word_to_ix)\nprint(word_to_ix)\n\nVOCAB_SIZE = len(word_to_ix)\nNUM_LABELS = 2\n\nclass BoWClassifier(nn.Module): # inheriting from nn.Module!\n\ndef __init__(self, num_labels, vocab_size):\n\n#This calls the init function of nn.Module. The syntax might confuse you, but don't be confused. Remember to do it in nn.module \n\n super(BoWClassifier, self).__init__()\n```", "```py\n # let's look at the prarmeters required for affine mapping\n # nn.Linear() is defined using Torch that gives us the affine maps.\n#We need to ensure that we understand why the input dimension is vocab_size\n # num_labels is the output\n self.linear = nn.Linear(vocab_size, num_labels)\n\n# Important thing to remember: parameters are not present in the non-linearity log softmax. So, let's now think about that.\n\ndef forward(self, bow_vec):\n #first, the input is passed through the linear layer\n #then it is passed through log_softmax\n #torch.nn.functional contains other non-linearities and many other fuctions\n\n return F.log_softmax(self.linear(bow_vec), dim=1)\n\ndef make_bow_vector(sentence, word_to_ix):\n vec = torch.zeros(len(word_to_ix))\n for word in sentence:\n vec[word_to_ix[word]] += 1\n return vec.view(1, -1)\n\ndef make_target(label, label_to_ix):\n return torch.LongTensor([label_to_ix[label]])\n\nmodel = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n```", "```py\n#A component is assigned to a class variable in the __init__ function\n# of a module, which was done with the line\n# self.linear = nn.Linear(...)\n\n# Then from the PyTorch devs, knowledge of the nn.linear's parameters #is stored by the module (here-BoW Classifier)\n\nfor param in model.parameters():\n print(param)\n\n#Pass a BoW vector for running the model\n# the code is wrapped since we don't need to train it\ntorch.no_grad()\nwith torch.no_grad():\n sample = data[0]\n bow_vector = make_bow_vector(sample[0], word_to_ix)\n log_probs = model(bow_vector)\n print(log_probs)\n```", "```py\n\n{'El': 0, 'que': 1, 'lee': 2, 'mucho': 3, 'y': 4, 'anda': 5, 'mucho,': 6, 've': 7, 'sabe': 8, 'The': 9, 'one': 10, 'who': 11, 'reads': 12, 'a': 13, 'lot': 14, 'and': 15, 'walks': 16, 'lot,': 17, 'sees': 18, 'knows': 19, 'lot.': 20, 'Nunca': 21, 'es': 22, 'tarde': 23, 'si': 24, 'la': 25, 'dicha': 26, 'buena': 27, 'It': 28, 'is': 29, 'never': 30, 'late': 31, 'if': 32, 'the': 33, 'joy': 34, 'good': 35, 'Que': 36, 'cada': 37, 'palo': 38, 'aguante': 39, 'su': 40, 'vela': 41, 'May': 42, 'every': 43, 'mast': 44, 'hold': 45, 'its': 46, 'own': 47, 'sail': 48}\nParameter containing:\ntensor([[-0.0347, 0.1423, 0.1145, -0.0067, -0.0954, 0.0870, 0.0443, -0.0923,\n 0.0928, 0.0867, 0.1267, -0.0801, -0.0235, -0.0028, 0.0209, -0.1084,\n -0.1014, 0.0777, -0.0335, 0.0698, 0.0081, 0.0469, 0.0314, 0.0519,\n 0.0708, -0.1323, 0.0719, -0.1004, -0.1078, 0.0087, -0.0243, 0.0839,\n -0.0827, -0.1270, 0.1040, -0.0212, 0.0804, 0.0459, -0.1071, 0.0287,\n 0.0343, -0.0957, -0.0678, 0.0487, 0.0256, -0.0608, -0.0432, 0.1308,\n -0.0264],\n [ 0.0805, 0.0619, -0.0923, -0.1215, 0.1371, 0.0075, 0.0979, 0.0296,\n 0.0459, 0.1067, 0.1355, -0.0948, 0.0179, 0.1066, 0.1035, 0.0887,\n -0.1034, -0.1029, -0.0864, 0.0179, 0.1424, -0.0902, 0.0761, -0.0791,\n -0.1343, -0.0304, 0.0823, 0.1326, -0.0887, 0.0310, 0.1233, 0.0947,\n 0.0890, 0.1015, 0.0904, 0.0369, -0.0977, -0.1200, -0.0655, -0.0166,\n -0.0876, 0.0523, 0.0442, -0.0323, 0.0549, 0.0462, 0.0872, 0.0962,\n -0.0484]], requires_grad=True)\nParameter containing:\ntensor([ 0.1396, -0.0165], requires_grad=True)\ntensor([[-0.6171, -0.7755]])\n```", "```py\nlabel_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}\n```", "```py\n# Pass the BoW vector for running the model\n# the code is wrapped since we don't need to train it\ntorch.no_grad()\n\nwith torch.no_grad():\n sample = data[0]\n bow_vector = make_bow_vector(sample[0], word_to_ix)\n log_probs = model(bow_vector)\n print(log_probs)\n\n# We will run this on data that can be tested temporarily, before training, just to check the before and after difference using touch.no_grad():\n\nwith torch.no_grad():\n for instance, label in test_data:\n bow_vec = make_bow_vector(instance, word_to_ix)\n log_probs = model(bow_vec)\n print(log_probs)\n\n#The matrix column corresponding to \"creo\" is printed\nprint(next(model.parameters())[:, word_to_ix[\"mucho\"]])\n\nloss_function = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n```", "```py\nfor epoch in range(100):\n for instance, label in data:\n # Firstly, remember that gradients are accumulated by PyTorch\n # It's important that we clear those gradients before each instance\n model.zero_grad()\n\n#The next step is to prepare our BOW vector and the target should be #wrapped in also we must wrap the target in a tensor in the form of an #integer\n # For example, as considered above, if the target word is SPANISH, #then, the integer wrapped should be 0\n#The loss function is already trained to understand that when the 0th element among the log probabilities is the one that is in accordance to SPANISH label\n\n bow_vec = make_bow_vector(instance, word_to_ix)\n target = make_target(label, label_to_ix)\n\n# Next step is to run the forward pass\n log_probs = model(bow_vec)\n\n```", "```py\n\n loss = loss_function(log_probs, target)\n loss.backward()\n optimizer.step()\n\nwith torch.no_grad():\n for instance, label in test_data:\n bow_vec = make_bow_vector(instance, word_to_ix)\n log_probs = model(bow_vec)\n print(log_probs)\n\n# After computing and the results, we see that the index that corresponds to Spanish has gone up, and for English is has gone down!\nprint(next(model.parameters())[:, word_to_ix[\"mucho\"]])\n```", "```py\n\ntensor([[-0.7653, -0.6258]])\ntensor([[-1.0456, -0.4331]])\ntensor([-0.0071, -0.0462], grad_fn=<SelectBackward>)\ntensor([[-0.1546, -1.9433]])\ntensor([[-0.9623, -0.4813]])\ntensor([ 0.4421, -0.4954], grad_fn=<SelectBackward>)\n```"]