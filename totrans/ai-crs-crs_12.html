<html><head></head><body>
		<div id="_idContainer680">
			<h1 class="chapterNumber sigil_not_in_toc">12</h1>
			<h1 class="chapterTitle" xml:lang="en-GB" id="sigil_toc_id_200" lang="en-GB"><a id="_idTextAnchor276"/>Deep Convolutional Q-Learning</h1>
</div>

			<p class="normal">Now that you understand how <strong class="bold">Artificial Neural Networks</strong> (<strong class="bold">ANNs</strong>) work, you're ready to tackle an incredibly useful tool, mostly used when dealing with images—<strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>). To put it <a id="_idIndexMarker537"/>simply, CNNs allow your AI to see images in real time as if it had eyes.</p>
			<p class="normal">We will tackle them in the following steps:</p>
			<ol>
				<li class="list">What are CNNs used for?<a id="_idTextAnchor277"/></li>
				<li class="list">How do CNNs work?</li>
				<li class="list">Convolution</li>
				<li class="list">Max pooling</li>
				<li class="list">Flattening</li>
				<li class="list">Full connection</li>
			</ol>
			<p class="normal">Once you've understood those steps, you'll understand CNNs, and how they can be used in deep convolutional Q-learning.<a id="_idTextAnchor278"/></p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_201" lang="en-GB"><a id="_idTextAnchor279"/>What are CNNs used for?</h2>
			<p class="normal">CNNs are mostly used with images <a id="_idIndexMarker538"/>or videos, and sometimes with text to tackle <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>)
 problems. They are often used in object recognition, for example, 
predicting whether there is a cat or a dog in a picture or video. They 
are<a id="_idIndexMarker539"/> also often used with deep Q-learning 
(which we will discuss later on), when the environment returns 2D states
 of itself, for example, when we are trying to build a self-driving car 
that reads outputs from cameras around it.</p>
			<p class="normal">Remember the example in <em class="italics">Chapter 9</em>, <em class="italics">Going Pro with Artificial Brains - Deep Q-Learning</em>,
 where we were predicting houses' prices. As inputs, we had all of the 
values that define a house (area, age, number of bedrooms, and so on), 
and as output, we had the price of a house. In the case of CNNs, things 
are very similar. For example, if we wanted to solve the same problem 
using CNNs, we would have<a id="_idIndexMarker540"/> images of houses as inputs and the price of a house as output.</p>
			<p class="normal">This diagram should illustrate what I mean:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_01.png" alt=""/></figure>
			<p class="packt_figref">Figure 1: Input Image – CNN – Output Label</p>
			<p class="normal">As you can see, the input is an image that flows 
through a CNN and comes out as an output. In the case of this diagram, 
the output is a class to which the image corresponds. What do I mean by a
 class? For example, if we wanted to predict whether the inputted image 
is a smiling face or a sad face, then one class would be <em class="italics">smiling face</em>, and the other would be <em class="italics">sad face</em>. Our output should then correctly decide to which class the input image corresponds.</p>
			<p class="normal">Speaking of happy and sad faces, here's a diagram that represents it in more detail:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_02.png" alt=""/></figure>
			<p class="packt_figref">Figure 2: Two different classes to predict (Happy or Sad)</p>
			<p class="normal">In the preceding example, <a id="_idIndexMarker541"/>we've
 run two images through a CNN. The first one is a smiling face and the 
other one is a sad face. As I mentioned before, our network predicts 
whether the image is a happy or a sad face.</p>
			<p class="normal">I can imagine what you're thinking right now: how 
does it all work? What's inside this black box we call a CNN? I'll 
answer these questions in the following sections.</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_202" lang="en-GB"><a id="_idTextAnchor280"/>How do CNNs work?</h2>
			<p class="normal">Before we can go<a id="_idIndexMarker542"/> deep
 into the structure of CNNs, we need to understand a couple of points. I
 will introduce you to the first point with a question: how many 
dimensions does a colored RGB image have?</p>
			<p class="normal">The answer may surprise you: it's 3!</p>
			<p class="normal">Why? Because every RGB image is, in fact, 
represented by three 2D images, each one corresponding to a color in RGB
 architecture. So, there is one image corresponding to red, one 
corresponding to green, and one to blue. Grayscale images are only 2D, 
because they are represented by only one scale as there are no colors. 
The following diagram should make it clearer:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_03.png" alt=""/></figure>
			<p class="packt_figref">Figure 3: RGB versus black and white images</p>
			<p class="normal">As you can see, a colored<a id="_idIndexMarker543"/> image is represented by a 3D array. Each color has its own layer in the picture, and this layer is called a <strong class="bold">channel</strong>. A grayscale (black and white) image only has one channel and is, therefore, a 2D array.</p>
			<p class="normal">As you probably <a id="_idIndexMarker544"/>know,
 images are made out of pixels. Each of these is represented by a value 
that ranges from 0 to 255, where 0 is a pixel turned off and 255 is a 
fully bright pixel. It's important to understand that when we say that a
 pixel has the value (255, 255, 0), then that means this pixel is fully 
bright on the red and green channel and turned off on the blue channel.</p>
			<p class="normal">From now on, to understand everything better, we'll
 be dealing with very simple images. In fact, our images will be 
grayscale (1 channel, 2D) and the pixels will either be fully bright or 
turned off. In order to make pictures easier to read, we'll assign 1 to a
 turned off pixel (black) and 0 to a fully bright one (white).</p>
			<p class="normal">Going back to the case of happy and sad faces, this is what our 2D array representing a happy face would look like:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_04.png" alt=""/></figure>
			<p class="packt_figref">Figure 4: The pixel representation</p>
			<p class="normal">As you can see, we have an array where <strong class="bold">0</strong> corresponds to a white pixel and <strong class="bold">1</strong> corresponds to a black pixel. The picture on the right is our smiling face represented by an array.</p>
			<p class="normal">Now that we <a id="_idIndexMarker545"/>understand
 the foundations and that we've simplified the problem, we're ready to 
tackle CNNs. In order to fully understand them, we need to split our 
learning into the four steps that make up a CNN:</p>
			<ol>
				<li class="list" value="1">Convolution</li>
				<li class="list">Max pooling</li>
				<li class="list">Flattening</li>
				<li class="list">Full connection</li>
			</ol>
			<p class="normal">Now we'll get to know each of these four steps one by <a id="_idTextAnchor281"/>one.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_203" lang="en-GB"><a id="_idTextAnchor282"/>Step 1 – Convolution</h3>
			<p class="normal">This is the first crucial <a id="_idIndexMarker546"/>step of every CNN. In convolution, we apply something called <strong class="bold">feature detectors</strong> to the inputted image. Why do we have to do so? This is <a id="_idIndexMarker547"/>because
 all images contain certain features that define what is in the picture.
 For example, to recognize which face is sad and which one is happy, we 
need to understand the meaning of the shape of the mouth, which is a 
feature of this image. It's easier to understand this from a diagram:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_05.png" alt=""/></figure>
			<p class="packt_figref">Figure 5: Step 1 – Convolution (1/5)</p>
			<p class="normal">In the preceding diagram, we applied<a id="_idIndexMarker548"/>
 a feature detector, also known as a filter, to the smiling face we had 
as input. As you can see, a filter is a 2D array with some values 
inside. When we apply this feature detector to the image it covers (in 
this case it is a 3 x 3 grid), we check how many pixels from this part 
of the image match the filter's pixels. Then we put this number into a 
new 2D array called <strong class="bold">feature map</strong>. In other <a id="_idIndexMarker549"/>words, the more a part of the picture matches the picture detector, the higher the number we put into the feature map.</p>
			<p class="normal">Next, we <em class="italics">slide</em> the feature detector across the entire image. In the next iteration, this is what will happen:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_06.png" alt=""/></figure>
			<p class="packt_figref">Figure 6: Step 1 – Convolution (2/5)</p>
			<p class="normal">As you can see, we slide the filter one place to 
the right. This time, one pixel matches in both the filter and in this 
part of the image. That's why we put <strong class="bold">1</strong> in the feature map.</p>
			<p class="normal">What do you think<a id="_idIndexMarker550"/> happens when we hit the boundary of this image? What would you do? I'll show you what happens with these two diagrams:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_07.png" alt=""/></figure>
			<p class="packt_figref">Figure 7: Step 1 – Convolution (3/5)</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_08.png" alt=""/></figure>
			<p class="packt_figref">Figure 8: Step 1 – Convolution (4/5)</p>
			<p class="normal">Here, we had this exact situation: in the first 
image, our filter hits the boundary. It turns out that our feature 
detector simply <em class="italics">jumps</em> to the next line.</p>
			<p class="normal">The whole magic of the convolution wouldn't work if we had only one filter. In reality, we use many filters, which <a id="_idIndexMarker551"/>produce many different feature maps. This set of feature maps is called a <strong class="bold">convolution layer</strong>, or <strong class="bold">convolutional layer</strong>. Here's a<a id="_idIndexMarker552"/> diagram to recap:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_09.png" alt=""/></figure>
			<p class="packt_figref">Figure 9: Step 1 – Convolution (5/5)</p>
			<p class="normal">Here, we can see an input image to which many 
filters were applied. All together, they create a convolutional layer 
from many feature maps. This is the first step when building a CNN.</p>
			<p class="normal">Now that we understand convolution, we can proceed to another important step—max po<a id="_idTextAnchor283"/>oling.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_204" lang="en-GB"><a id="_idTextAnchor284"/>Step 2 – Max pooling</h3>
			<p class="normal">This step in CNNs is responsible for lowering the 
size of each feature map. When dealing with neural networks, we don't 
want to have too many inputs, otherwise our network wouldn't be able to 
learn properly because of the high complexity. Therefore, a method of 
reducing the size called <strong class="bold">max pooling</strong> needs to be<a id="_idIndexMarker553"/>
 introduced. It lets us reduce the size without losing any important 
features, and it makes features partially invariant to shifts 
(translations and rotations).</p>
			<p class="normal">Technically, a max pooling algorithm is also based 
on an array sliding across the entire feature map. In this case, we are 
not searching for any features but, rather, for the maximum value in a 
specific area of a feature map. </p>
			<p class="normal">Let me show you what I mean with this graphic:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_10.png" alt=""/></figure>
			<p class="packt_figref">Figure 10: Step 2 – Max pooling (1/5)</p>
			<p class="normal">In this example, we're<a id="_idIndexMarker554"/> taking the feature map, obtained after the convolution step we had before, and then we are running it through max pooling. As <a id="_idIndexMarker555"/>you
 can see, we have a window of size 2 x 2 looking for the highest values 
in the part of feature map it covers. In this case, it's 1.</p>
			<p class="normal">Can you tell what will happen in the next iteration?</p>
			<p class="normal">As you may have suspected, this window will slide 
to the right, although in a slightly different way than before. It moves
 like this:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_11.png" alt=""/></figure>
			<p class="packt_figref">Figure 11: Step 2 – Max Pooling (2/5)</p>
			<p class="normal">This window <em class="italics">jumps</em> its size to the right, which I hope you remember is different from the convolution step, where<a id="_idIndexMarker556"/> the feature detector slid one cell at a time. In this case, the highest value is 1 as well, and therefore<a id="_idIndexMarker557"/> we write <strong class="bold">1</strong> in the <strong class="bold">pooled feature map</strong>.</p>
			<p class="normal">What happens this time when we hit the boundary of 
the feature map? Things look slightly different from before once again. 
This is what happens:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_12.png" alt=""/></figure>
			<p class="packt_figref">Figure 12: Step 2 – Max pooling (3/5)</p>
			<p class="normal">The window crosses the boundary and searches for 
the highest value in the part of the feature map that is still inside 
the max pooling window. Yet again, the highest value is 1.</p>
			<p class="normal">But what happens now? After all, there's no space 
left to go to the right. There's also only one row at the bottom left 
for max pooling. This is what the algorithm does:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_13.png" alt=""/></figure>
			<p class="packt_figref">Figure 13: Step 2 – Max pooling (4/5)</p>
			<p class="normal">As we can see, it once again crosses the boundary and searches for the highest value in what is inside the<a id="_idIndexMarker558"/>
 window. In this case, it is 0. This process is repeated until the 
window hits the bottom right corner of the feature map. To recap what 
our CNN looks like for now, have a look at the following diagram:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_14.png" alt=""/></figure>
			<p class="packt_figref">Figure 14: Step 2 – Max pooling (5/5)</p>
			<p class="normal">We had a smiling face as input, then we ran it 
through convolution to obtain many feature maps, called the 
convolutional layer. Now we've run all the feature maps through max 
pooling and<a id="_idIndexMarker559"/> obtained many pooled feature maps, all together called the <strong class="bold">pooling layer</strong>.</p>
			<p class="normal">Now we can continue<a id="_idIndexMarker560"/> to the next step, which will let<a id="_idIndexMarker561"/> us input the pooling layer into a neural network. This step is called <a id="_idTextAnchor285"/><strong class="bold">flattening</strong>.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_205" lang="en-GB"><a id="_idTextAnchor286"/>Step 3 – Flattening</h3>
			<p class="normal">This is a very short step. As<a id="_idIndexMarker562"/>
 the name may suggest, we change all the pooled feature maps from 2D 
arrays to 1D ones. As I mentioned before, this will let us input the 
image into a neural network with ease. So, how exactly will we achieve 
this? The following diagram should help you understand:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_15.png" alt=""/></figure>
			<p class="packt_figref">Figure 15: Step 3 – Flattening (1/3)</p>
			<p class="normal">Here we go back to the pooled feature map we 
obtained before. To flatten it, we take pixel values starting from the 
top left, finishing at bottom right. An operation like this returns a 1D
 array, containing the same values as the 2D array we started with.</p>
			<p class="normal">But remember, we don't have one pooled feature map, we have an entire layer of them. What do you think we should do with that?</p>
			<p class="normal">The answer is simple: we put this entire layer into
 a single 1D flattened array, one pooled feature map after another. Why 
does it have to be 1D? This is because ANNs only accept 1D arrays as 
their inputs. All the layers in a traditional neural network are 1D, 
which means that the input has to be 1D as well. Therefore, we flatten 
all the pooled feature maps, like so:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_16.png" alt=""/></figure>
			<p class="packt_figref">Figure 16: Step 3 – Flattening (2/3)</p>
			<p class="normal">We've taken the entire layer<a id="_idIndexMarker563"/> and transformed it into a single flattened 1D array. We'll soon use this array as the input of a traditional neural network. </p>
			<p class="normal">First, let's remind ourselves of what our model looks like now:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_17.png" alt=""/></figure>
			<p class="packt_figref">Figure 17: Step 3 – Flattening (3/3)</p>
			<p class="normal">So, we have a Convolutional Layer, Pooling Layer, 
and a freshly added, flattened 1D layer. Now we can go back to a classic
 ANN, that is, a fully connected neural network, and treat this last 
layer as an input for this network. This leads us to the final step, <strong class="bold">full c<a id="_idTextAnchor287"/>onnection</strong>.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_206" lang="en-GB"><a id="_idTextAnchor288"/>Step 4 – Full connection</h3>
			<p class="normal">The final step of creating a CNN is to connect it 
to a classic fully-connected neural network. Remember that we already 
have a 1D array telling us in a compressed way what the image looks 
like, so why not just use it as an input to a fully-connected neural 
network? After all, it's the latter that's able to make predictions.</p>
			<p class="normal">That's exactly what we do next, just like this:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_18.png" alt=""/></figure>
			<p class="packt_figref">Figure 18: Step 4 – Full connection</p>
			<p class="normal">After flattening, we input those returned values 
straight into the fully-connected neural network, which then yields the 
prediction—the output value.</p>
			<p class="normal">You might be wondering how the back-propagation 
phase works now. In a CNN, back-propagation not only updates the weights
 in the fully-connected neural network, but also the filters used in the
 convolution step. The max pooling and flattening steps will remain the 
same, as there is nothing to update there.</p>
			<p class="normal">In conclusion, CNNs look for some specific 
features. This is why they're mostly used when we are dealing with 
images, where searching for features is crucial. For example, when 
trying to recognize a sad and a happy face, a CNN needs to understand 
which mouth's shape means a sad face and which means a happy face. In 
order to obtain an output, a CNN has to run these steps:</p>
			<ol>
				<li class="list" value="1"><strong class="bold">Convolution</strong> – Applying filters to<a id="_idIndexMarker564"/> the input image. This operation will find the features our CNN is looking for and save them in a feature map.</li>
				<li class="list"><strong class="bold">Max pooling</strong> – Lowering the<a id="_idIndexMarker565"/> feature map size, by taking a maximum value in a given area and saving these values in a new array called pooled feature map.</li>
				<li class="list"><strong class="bold">Flattening</strong> – Changing the <a id="_idIndexMarker566"/>entire pooling layer (all pooled feature maps) to a 1D vector. This will allow us to input this vector into a neural network.</li>
				<li class="list"><strong class="bold">Full connection</strong> – Creating a <a id="_idIndexMarker567"/>neural
 network, which takes as input a flattened pooling layer and returns a 
value that we would like to predict. This last step lets us mak<a id="_idTextAnchor289"/>e predictions.</li>
			</ol>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_207" lang="en-GB"><a id="_idTextAnchor290"/>Deep convolutional Q-learning</h2>
			<p class="normal">In the chapter on deep Q-learning (<em class="italics">Chapter 9</em>, <em class="italics">Going Pro with Artificial Brains – Deep Q-Learning</em>), our inputs were vectors of encoded values defining the states of the environment. When working with<a id="_idIndexMarker568"/>
 images or videos, encoded vectors aren't the best inputs to describe a 
state (the input frame), simply because an encoded vector doesn't 
preserve the spatial structure of an image. The spatial structure is 
important because it gives us more information to help predict the next 
state, and predicting the next state is essential for our AI to learn 
the correct next move.</p>
			<p class="normal">Therefore, we need to preserve the spatial 
structure. To do that, our inputs must be 3D images (2D for the array of
 pixels plus one additional dimension for the colors, as illustrated at 
the beginning of this chapter). For example, if we train an AI to play a
 video game, the inputs are simply the images of the screen itself, 
exactly what a human sees when playing the game.</p>
			<p class="normal">Following this analogy, the AI acts like it has 
human eyes; it observes the input images on the screen when playing the 
game. Those input images go into a CNN (the eyes for a human), which 
detects the state in each image. Then they're forward-propagated through
 the pooling layers where max pooling is applied. Then the pooling 
layers are flattened into a 1D vector, which becomes the input of our 
deep Q-learning network (the exact same one as in <em class="italics">Chapter 9</em>, <em class="italics">Going Pro with Artificial Brains – Deep Q-Learning</em>). In the end, the same deep Q-learning process is run.</p>
			<p class="normal">The following graph illustrates deep convolutional Q-learning applied to the famous game of Doom:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_12_19.png" alt=""/></figure>
			<p class="packt_figref">Figure 19: Deep convolutional Q-learning for Doom</p>
			<p class="normal">In summary, deep <a id="_idIndexMarker569"/>convolutional
 Q-learning is the same as deep Q-learning, with the only differences 
being that the inputs are now images, and a CNN is added at the 
beginning of the fully-connected deep Q-learning network to<a id="_idTextAnchor291"/> detect the states of those images.</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_208" lang="en-GB"><a id="_idTextAnchor292"/>Summary</h2>
			<p class="normal">You've learned about another type of neural network—a Convolutional Neural Network.</p>
			<p class="normal">We established that this network is used mostly 
with images and searches for certain features in these pictures. It uses
 three additional steps that ANNs don't have: convolution, where we 
search for features; max pooling, where we shrink the image in size; and
 flattening, where we flatten 2D images to a 1D vector so that we can 
input it into a neural network.</p>
			<p class="normal">In the next chapter, you’ll build a deep convolutional Q-learning model to solve a classic gaming problem: Snake.</p>
</body></html>