- en: Machine Learning for NLP Problems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习在NLP问题中的应用
- en: We have seen the basic and the advanced levels of feature engineering. We have
    also seen how rule-based systems can be used to develop NLP applications. In this
    chapter, we will develop NLP applications, and to develop the applications, we
    will use **machine learning** (**ML**) algorithms. We will begin with the basics
    of ML. After this, we will see the basic development steps of NLP applications
    that use ML. We will mostly see how to use ML algorithms in the NLP domain. Then,
    we will move towards the features selection section. We will also take a look
    at hybrid models and post-processing techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了特征工程的基础和高级层面。我们还了解了基于规则的系统如何用于开发自然语言处理（NLP）应用程序。在本章中，我们将开发NLP应用程序，并且为了开发这些应用程序，我们将使用**机器学习**（**ML**）算法。我们将从机器学习的基础开始。接下来，我们将看到使用机器学习的NLP应用程序的基本开发步骤。我们主要会看到如何在NLP领域使用机器学习算法。然后，我们将进入特征选择部分。我们还将看看混合模型和后处理技术。
- en: 'This is the outline of this chapter given as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的大纲如下：
- en: Understanding the basics of machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解机器学习的基础
- en: Development steps for NLP application
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP应用程序的开发步骤
- en: Understanding ML algorithms and other concepts
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解机器学习算法和其他概念
- en: Hybrid approaches for NLP applications
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP应用的混合方法
- en: Let's explore the world of ML!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一起探索机器学习的世界吧！
- en: Understanding the basics of machine learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习的基础
- en: First of all, we will understand what machine learning is. Traditionally, programming
    is all about defining all the steps to reach a certain predefined outcome. During
    this process of programming, we define each of the minute steps using a programming
    language that help us achieve our outcome. To give you a basic understanding,
    I'll take a general example. Suppose that you want to write a program that will
    help you draw a face. You may first write the code that draws the left eye, then
    write the code that draws the right eye, then the nose, and so on. Here, you are
    writing the code for each facial attribute, but ML flips this approach. In ML,
    we define the outcome and the program learns the steps to achieve the defined
    output. So, instead of writing code for each facial attribute, we provide hundreds
    of samples of human faces to the machine. We expect the machine to learn the steps
    that are needed to draw a human face so that it can draw some new human faces.
    Apart from this, when we provide the new human face as well as some animal face,
    it should recognize which face looks like a human face.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将了解什么是机器学习。传统上，编程是关于定义所有步骤以达到某个预定结果。在编程的过程中，我们使用编程语言定义每一个细小的步骤，帮助我们实现目标。为了让你有一个基本的理解，我会举一个通用的例子。假设你想写一个程序来帮助你画一个面孔。你可能首先写绘制左眼的代码，然后写绘制右眼的代码，再写鼻子的代码，依此类推。在这里，你是在为每一个面部特征编写代码，但机器学习（ML）颠覆了这种方法。在机器学习中，我们定义结果，程序学习实现该结果的步骤。因此，取而代之的是为每个面部特征编写代码，我们向机器提供数百个人脸样本。我们期望机器能够学习绘制人脸所需的步骤，这样它就能画出一些新的面孔。除此之外，当我们提供新的人的面孔以及一些动物的面孔时，机器应该能够识别哪个面孔像是人的面孔。
- en: Let's take some general examples. If you want to recognize the valid license
    plates of certain states, in traditional programming, you need to write code such
    as what the shape of the license plate should be, what the color should be, what
    the fonts are, and so on. These coding steps are too lengthy if you are trying
    to manually code each single property of the license plate. Using ML, we will
    provide some example license plates to the machine and the machine will learn
    the steps so that it can recognize the new valid license plate.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一些常见的例子。如果你想识别某些州的有效车牌，在传统编程中，你需要写出车牌的形状应该是什么样的，颜色应该是什么，字体是什么等等。如果你试图手动为车牌的每个属性编写代码，这些步骤会非常冗长。使用机器学习，我们将向机器提供一些示例车牌，机器会学习步骤，从而能够识别新的有效车牌。
- en: Let's assume that you want to make a program that can play the game Super Mario
    and win the game as well. So, defining each game rule is too difficult for us.
    We usually define a goal such as you need to get to the endpoint without dying
    and the machine learns all the steps to reach the endpoint.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想制作一个可以玩超级马里奥并且能够赢得比赛的程序。那么，定义每个游戏规则对我们来说太困难了。我们通常会定义一个目标，比如你需要在不死的情况下到达终点，机器学习将学习到达终点所需的所有步骤。
- en: Sometimes, problems are too complicated, and even we don't know what steps should
    possibly be taken to solve these problems. For example, we are a bank and we suspect
    that there are some fraudulent activities happening, but we are not sure how to
    detect them or we don't even know what to look for. We can provide a log of all
    the user activities and find the users who are not behaving like the rest of the
    users. The machine learns the steps to detect the anomalies by itself.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，问题过于复杂，甚至我们自己也不知道应该采取哪些步骤来解决这些问题。例如，我们是一家银行，怀疑可能存在一些欺诈行为，但我们不确定如何检测这些行为，甚至不知道该寻找什么。我们可以提供所有用户活动的日志，并找到那些与其他用户行为不同的用户。机器通过自身学习来检测异常的步骤。
- en: ML is everywhere on the internet. Every big tech company is using it in some
    way. When you see any YouTube video, YouTube updates or provides you with suggestions
    of other videos that you may like to watch. Even your phone uses ML to provide
    you with facilities such as iPhone's Siri, Google Assistance, and so on. The ML
    field is currently advancing very fast. Researchers use old concepts, change some
    of them, or use other researchers, work to make it more efficient and useful.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ML无处不在于互联网上。每一家大型科技公司都在以某种方式使用它。当你观看任何YouTube视频时，YouTube会更新并向你推荐你可能喜欢观看的其他视频。甚至你的手机也使用ML提供像iPhone的Siri、Google助手等功能。ML领域目前正在迅速发展。研究人员使用旧的概念，改变其中的一些，或借用其他研究人员的工作，以使其更高效、更有用。
- en: 'Let''s look at the basic traditional definition of ML. In 1959, a researcher
    named Arthur Samuel gave computers the ability to learn without being explicitly
    programmed. He evolved this concept of ML from the study of pattern recognition
    and computational learning theory in AI. In 1997, Tom Mitchell gave us an accurate
    definition that has been useful to those who can understand basic math. The definition
    of ML as per Tom Mitchell is: A computer program is said to learn from experience
    E with respect to some task T and some performance measure P, if its performance
    on T, as measured by P, improves with experience E.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下传统的ML基本定义。1959年，一位名叫Arthur Samuel的研究员赋予了计算机无需明确编程即可学习的能力。他将ML的概念从人工智能中的模式识别和计算学习理论中演化出来。1997年，Tom
    Mitchell给出了一个准确的定义，这个定义对于那些能够理解基础数学的人来说非常有用。Tom Mitchell定义的ML是：如果一个计算机程序在某任务T上，通过经验E和性能评估P的衡量，其在T上的表现随着经验E的增加而提高，那么我们就说该程序已经从经验E中学习。
- en: Let's link the preceding definition with our previous example. To identify a
    license plate is called task **T**. You will run some ML programs using examples
    of license plates called experience **E**, and if it successfully learns, then
    it can predict the next unseen license plate that is called performance measure
    **P**. Now it's time to explore different types of ML and how it's related to
    AI.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将前面的定义与我们之前的示例联系起来。识别车牌号的任务称为任务**T**。你将使用车牌号的示例来运行一些ML程序，这些示例称为经验**E**，如果程序成功学习，那么它就能预测下一个未见过的车牌号，这个过程叫做性能评估**P**。现在是时候探索不同类型的ML以及它与AI的关系了。
- en: Types of ML
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML的类型
- en: In this section, we will look at different types of ML and some interesting
    sub-branch and super-branch relationships.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看不同类型的ML以及一些有趣的子分支和超分支关系。
- en: ML itself is derived from the branch called a**rtificial intelligence**. ML
    also has a branch that is creating lot of buzz nowadays called **deep learning**,
    but we will look at artificial intelligence and deep learning in detail in [Chapter
    9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml), *Deep Learning for NLP and NLG
    Problems*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ML本身源自于被称为**人工智能**的分支。ML还有一个近年来备受关注的分支，叫做**深度学习**，但我们将在[第9章](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml)中详细讨论人工智能和深度学习，*深度学习在NLP和NLG问题中的应用*。
- en: 'Learning techniques can be divided into different types. In this chapter, we
    are focusing on ML. Refer to *Figure 8.1*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 学习技术可以分为不同的类型。在本章中，我们重点关注的是ML。请参见*图8.1*：
- en: '![](img/b5760145-4ef9-4fb6-bc37-9bf1acdf7368.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5760145-4ef9-4fb6-bc37-9bf1acdf7368.png)'
- en: 'Figure 8.1: Subset and superset relationships of ML with other branches (Image
    credit: https://portfortune.files.wordpress.com/2016/10/ai-vs-ml.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：ML与其他分支的子集与超集关系（图片来源： https://portfortune.files.wordpress.com/2016/10/ai-vs-ml.jpg）
- en: 'ML techniques can be divided into three different types, which you can see
    in *Figure 8.2*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ML技术可以分为三种不同的类型，您可以在*图8.2*中看到：
- en: '![](img/94fa2c30-86c3-49b0-9044-c2b78556e670.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94fa2c30-86c3-49b0-9044-c2b78556e670.png)'
- en: 'Figure 8.2: Three types of ML (Image credit: https://cdn-images-1.medium.com/max/1018/1*Yf8rcXiwvqEAinDTWTnCPA.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：三种机器学习类型（图片来源：https://cdn-images-1.medium.com/max/1018/1*Yf8rcXiwvqEAinDTWTnCPA.jpeg）
- en: We will look at each type of ML in detail. So, let's begin!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细介绍每种机器学习类型。那么，让我们开始吧！
- en: Supervised learning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: In this type of ML, we will provide a labeled dataset as input to the ML algorithm
    and our ML algorithm knows what is correct and what is not correct. Here, the
    ML algorithm learns mapping between the labels and data. It generates the ML model
    and then the generated ML model can be used to solve some given task.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种类型的机器学习中，我们将提供一个已标记的数据集作为输入给 ML 算法，算法知道什么是正确的，什么是错误的。在这里，ML 算法学习标签与数据之间的映射关系。它生成
    ML 模型，然后生成的 ML 模型可以用来解决给定的任务。
- en: Suppose we have some text data that has labels such as spam emails and non-spam
    emails. Each text stream of the dataset has either of these two labels. When we
    apply the supervised ML algorithm, it uses the labeled data and generates an ML
    model that predicts the label as spam or non-spam for the unseen text stream.
    This is an example of supervised learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一些文本数据，其标签为垃圾邮件和非垃圾邮件。数据集中的每个文本流都有这两个标签之一。当我们应用监督学习算法时，它使用标记数据并生成一个 ML
    模型，预测未见过的文本流是垃圾邮件还是非垃圾邮件。这就是监督学习的一个例子。
- en: Unsupervised learning
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In this type of ML, we will provide an unlabeled dataset as input to the ML
    algorithm. So, our algorithm doesn't get any feedback on what is correct or not.
    It has to learn by itself the structure of the data to solve a given task. It
    is harder to use an unlabeled dataset, but it's more convenient because not everyone
    has a perfectly labeled dataset. Most data is unlabeled, messy, and complex.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种类型的机器学习中，我们将提供一个未标记的数据集作为输入给 ML 算法。因此，我们的算法无法获得任何关于数据是否正确的反馈。它必须通过自身学习数据的结构来解决给定的任务。使用未标记的数据集更具挑战性，但也更为便捷，因为并不是每个人都有一个完美标注的数据集。大部分数据都是未标记的、杂乱无章且复杂的。
- en: Suppose we are trying to develop a summarization application. We probably haven't
    summarized the documents corresponding to the actual document. Then, we will use
    raw and the actual text document to create a summary for the given documents.
    Here, the machine doesn't get any feedback as to whether the summary generated
    by the ML algorithm is right or wrong. We will also see an example of a computer
    vision application. For image recognition, we feed an unlabeled image dataset
    of some cartoon characters to the machine, and we expect the machine to learn
    how to classify each of the characters. When we provide an unseen image of a cartoon
    character, it should recognize the character and put that image in the proper
    class, which is generated by the machine itself.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在尝试开发一个摘要应用程序。我们可能没有总结出与实际文档相对应的文档摘要。那么，我们将使用原始数据和实际文本文档来为给定文档创建摘要。在这里，机器不会收到关于
    ML 算法生成的摘要是否正确的任何反馈。我们还将看到计算机视觉应用的一个例子。对于图像识别，我们将一些卡通角色的未标记图像数据集输入到机器中，并期望机器学习如何分类每个角色。当我们提供一个未见过的卡通角色图像时，机器应能够识别该角色，并将图像放入正确的类别，这是由机器自身生成的。
- en: Reinforcement learning
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'The third type of ML is reinforcement learning. Here, the ML algorithm doesn''t
    give you the feedback right after every prediction, but it generates feedback
    if the ML model achieves its goal. This type of learning is mostly used in the
    area of robotics and to develop intelligent bots to play games. Reinforcement
    learning is linked to the idea of interacting with an environment using the trial
    and error method. Refer to *Figure 8.3*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种机器学习（ML）类型是强化学习。在这种情况下，ML 算法不会在每次预测后立即给出反馈，而是当 ML 模型达到目标时才会生成反馈。这种学习方式通常用于机器人技术领域，以及开发智能机器人来进行游戏。强化学习与通过试错方法与环境交互的思想密切相关。请参见*图
    8.3*：
- en: '![](img/cc5fc564-d9fd-4745-9373-b79b16a3fd5d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc5fc564-d9fd-4745-9373-b79b16a3fd5d.png)'
- en: 'Figure 8.3: Reinforcement learning interacting with environment (Image credit:
    https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/04/aeloop-300x183.png)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：强化学习与环境的交互（图片来源：https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/04/aeloop-300x183.png）
- en: 'To learn the basics, let''s take an example. Say you want to make a bot that
    beats humans at chess. This type of bot would receive feedback only if it won
    the game. Recently, Google AlphaGo beat the world''s best Go player. If you want
    to read more on this, refer to the following link:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习基础知识，让我们举一个例子。假设你想做一个能击败人类的国际象棋机器人。这种类型的机器人只有在赢得比赛时才会收到反馈。最近，Google的AlphaGo击败了世界上最强的围棋选手。如果你想了解更多，可以参考以下链接：
- en: '[https://techcrunch.com/2017/05/24/alphago-beats-planets-best-human-go-player-ke-jie/.](https://techcrunch.com/2017/05/24/alphago-beats-planets-best-human-go-player-ke-jie/)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://techcrunch.com/2017/05/24/alphago-beats-planets-best-human-go-player-ke-jie/.](https://techcrunch.com/2017/05/24/alphago-beats-planets-best-human-go-player-ke-jie/)'
- en: We are not going into detail about this type of ML in this book because our
    main focus is NLP, not robotics or developing a game bot.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们不会详细探讨这种类型的机器学习，因为我们的主要重点是NLP，而不是机器人技术或开发游戏机器人。
- en: 'If you really want to learn **reinforcement learning** (**RL**) in detail,
    you can take up this course:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的想深入学习**强化学习**（**RL**），你可以参加这个课程：
- en: '[https://www.udacity.com/course/reinforcement-learning--ud600](https://in.udacity.com/course/reinforcement-learning--ud600/).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.udacity.com/course/reinforcement-learning--ud600](https://in.udacity.com/course/reinforcement-learning--ud600/).'
- en: I know you must be interested in knowing the differences between each type of
    ML. So, pay attention as you read the next paragraph.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道你一定对不同类型的机器学习有所兴趣。所以，在阅读下一个段落时，请集中注意力。
- en: 'For supervised learning, you will get feedback after every step or prediction.
    In reinforcement learning, we will receive feedback only if our model achieves
    the goal. In unsupervised learning, we will never get feedback, even if we achieve
    our goal or our predication is right. In reinforcement learning, it interacts
    with the existing environment and uses the trial and error method, whereas the
    other two types do not apply trial and error. In supervised learning, we will
    use labeled data, whereas in unsupervised learning, we will use unlabeled data,
    and in reinforcement learning, there are a bunch of goals and decision processes
    involved. You can refer to *Figure 8.4*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监督学习，每个步骤或预测后都会收到反馈。而在强化学习中，只有当我们的模型达成目标时，我们才会收到反馈。在无监督学习中，即使我们达成了目标或我们的预测是正确的，我们也不会得到反馈。强化学习与现有环境进行互动，并使用试错法，而另外两种类型则不适用试错法。在监督学习中，我们使用有标签的数据，而在无监督学习中，我们使用无标签的数据，强化学习涉及多个目标和决策过程。你可以参考*图8.4*：
- en: '![](img/a0779102-7e3e-47e8-ab25-5b06fb541b7e.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0779102-7e3e-47e8-ab25-5b06fb541b7e.png)'
- en: 'Figure 8.4: Comparison between supervised, unsupervised, and reinforcement
    learning (Image credit: http://www.techjini.com/wp-content/uploads/2017/02/mc-learning.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4：监督学习、无监督学习和强化学习的比较（图片来源：http://www.techjini.com/wp-content/uploads/2017/02/mc-learning.jpg）
- en: There are so many new things that you will be learning from this section onwards,
    if you don't understand some of the terminology at first, then don't worry! Just
    bear with me; I will explain each of the concepts practically throughout this
    chapter. So, let's start understanding the development steps for NLP applications
    that use ML.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一部分开始，你将学习到很多新东西，如果一开始你不理解一些术语，别担心！请耐心一点；我会在本章中通过实际的例子解释每个概念。那么，让我们开始理解使用机器学习的NLP应用程序的开发步骤吧。
- en: Development steps for NLP applications
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP应用程序的开发步骤
- en: In this section, we will discuss the steps of developing NLP applications using
    ML algorithms. These steps vary from domain to domain. For NLP applications, the
    visualization of data does not play that much of a critical role, whereas the
    visualization of data for an analytical application will give you a lot of insight.
    So, it will change from application to application and domain to domain. Here,
    my focus is the NLP domain and NLP applications, and when we look at the code,
    I will definitely recall the steps that I'm describing here so that you can connect
    the dots.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论使用机器学习算法开发自然语言处理（NLP）应用程序的步骤。这些步骤在不同领域之间有所不同。对于NLP应用程序，数据可视化并不像分析型应用程序那样起着至关重要的作用，而分析型应用程序的数据可视化会提供很多洞察力。因此，这会因应用程序和领域的不同而有所变化。在这里，我的重点是NLP领域和NLP应用程序，当我们查看代码时，我一定会回顾我在这里描述的步骤，帮助你将各个点连接起来。
- en: 'I have divided the development steps into two versions. The first version is
    taking into account that it''s the first iteration for your NLP application development.
    The second version will help you with the possible steps that you can consider
    after your first iteration of the NLP application development. Refer to *Figure
    8.5*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我将开发步骤分为两个版本。第一个版本考虑的是这是你 NLP 应用开发的第一次迭代。第二个版本将帮助你了解在第一次迭代后，你可以考虑的可能步骤。请参考*图
    8.5*：
- en: '![](img/64464987-d5ac-43e1-ae5a-2238e888de2c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64464987-d5ac-43e1-ae5a-2238e888de2c.png)'
- en: 'Figure 8.5: NLP application development steps version'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5：NLP 应用开发步骤版本
- en: Development step for the first iteration
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一次迭代的开发步骤
- en: 'First, we will look at the steps that we can generally use when we develop
    the first version of the NLP application using ML. I will refer to *Figure 8.6*
    during my explanation so that you can understand things properly:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看一下在使用 ML 开发 NLP 应用的第一次版本时，一般可以用到的步骤。在我的解释过程中，我将参考*图 8.6*，以便你能更好地理解：
- en: '![](img/f4899d64-c66f-4f55-8e92-2815628deb04.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4899d64-c66f-4f55-8e92-2815628deb04.png)'
- en: 'Figure 8.6: The first version and iteration to develop an application using
    ML algorithms'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6：使用 ML 算法开发应用的第一次版本和迭代
- en: 'I''m going to explain each step:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我将解释每一个步骤：
- en: The first step of this version is understanding your problem statement, application
    requirements, or the objective that you are trying to solve.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个版本的第一步是理解你的问题陈述、应用需求，或你正在尝试解决的目标。
- en: The second step is to get the data that you need to solve your objective or,
    if you have the dataset, then try to figure out what the dataset contains and
    what is your need in order to build an NLP application. If you need some other
    data, then first ask yourself; can you derive the sub-data attributes with the
    help of the available dataset? If yes, then there may be no need to get another
    dataset but if not, then try to get a dataset that can help you develop your NLP
    application.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步是获取你解决目标所需的数据，或者如果你已经有了数据集，那么尝试了解该数据集包含什么，以及你需要什么来构建 NLP 应用。如果你需要其他数据，那么首先问问自己：你能否借助现有的数据集推导出所需的子数据属性？如果可以，那么可能不需要获取新的数据集；如果不行，那么尝试获取一个可以帮助你开发
    NLP 应用的数据集。
- en: The third step is to think about what kind of end result you want, and according
    to that, start exploring the dataset. Do some basic analysis.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三步是思考你想要什么样的最终结果，并根据这个结果开始探索数据集，进行一些基本分析。
- en: The fourth step is after doing a general analysis of the data, you can apply
    preprocessing techniques on it.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第四步是在对数据进行一般分析后，可以对其应用预处理技术。
- en: The fifth step is to extract features from the preprocessed data as part of
    feature engineering.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第五步是从预处理过的数据中提取特征，作为特征工程的一部分。
- en: The sixth is, using statistical techniques, you can visualize the feature values.
    This is an optional step for an NLP application.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第六步是，使用统计技术可视化特征值。这是 NLP 应用的一个可选步骤。
- en: The seventh step is to build a simple, basic model for your own benchmark.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第七步是为你的基准模型构建一个简单的基础模型。
- en: Last but not least, evaluate the basic model, and if it is up to the mark, then
    good; otherwise, you need more iterations and need to follow another version,
    which I will be describing in the next section.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，评估基本模型，如果达到预期标准，那就很好；否则，你需要更多的迭代，并且需要遵循另一个版本，我将在下一节中进行描述。
- en: Development steps for the second to nth iteration
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二次到第n次迭代的开发步骤
- en: We have seen the steps that you can take in the first iteration; now we will
    see how we can execute the second iteration so that we can improvise our model
    accuracy as well as efficiency. Here, we are also trying to make our model as
    simple as possible. All these goals will be part of this development version.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过第一次迭代的步骤，现在我们将看看如何执行第二次迭代，以便提高我们的模型准确性和效率。在这里，我们还试图使我们的模型尽可能简单。所有这些目标将成为这个开发版本的一部分。
- en: 'Now we will see the steps that you can follow after the first iteration. For
    basic understanding, refer to *Figure 8.7*:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看第一次迭代后的步骤。为了基本理解，请参考*图 8.7*：
- en: '![](img/405bc9c6-8ec2-4f01-b723-ebfa91fe1c26.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/405bc9c6-8ec2-4f01-b723-ebfa91fe1c26.jpg)'
- en: 'Figure 8.7: ML building cycle'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7：ML 构建周期
- en: 'Some of the basic steps for the second iteration are as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次迭代的一些基本步骤如下：
- en: After the first iteration, you have already built a model, and now you need
    to improve it. I would recommend you try out different ML algorithms to solve
    the same NLP application and compare the accuracy. Choose the best three ML algorithms
    based on accuracy. This will be the first step.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一次迭代后，你已经构建了一个模型，现在需要改进它。我建议你尝试不同的机器学习（ML）算法来解决同一个自然语言处理（NLP）应用，并比较其准确度。根据准确度选择最好的三个ML算法。这将是第一步。
- en: As a second step, generally, you can apply hyperparameter tuning for each of
    the selected ML algorithms in order to achieve better accuracy.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为第二步，通常你可以对每个选择的ML算法进行超参数调优，以获得更好的准确度。
- en: If parameter optimization doesn't help you so much, then you need to really
    concentrate on the feature engineering part and this will be your step three.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果参数优化对你帮助不大，那么你需要真正集中精力在特征工程部分，这将是你的第三步。
- en: 'Now, feature engineering has two major parts: feature extraction and feature
    selection. So in the first iteration, we have already extracted feature, but in
    order to optimize our ML model, we need to work on feature selection. We will
    look at all the feature selection techniques later in this chapter.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，特征工程有两个主要部分：特征提取和特征选择。因此，在第一次迭代中，我们已经提取了特征，但为了优化我们的ML模型，我们需要在特征选择上进行工作。我们将在本章后面详细介绍所有的特征选择技术。
- en: In feature selection, you basically choose those feature, variable, or data
    attributes that are really critical or contribute a lot in order to derive the
    outcome. So, we will consider only important feature and remove others.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在特征选择中，你基本上是选择那些真正重要的特征、变量或数据属性，这些特征对结果的生成至关重要或者贡献很大。因此，我们将只考虑重要的特征，去除其他的。
- en: You can also remove outliers, perform data normalization, and apply cross validation
    on your input data, which will help you improvise your ML model.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以去除离群值，执行数据标准化，并在输入数据上应用交叉验证，这将有助于你改进ML模型。
- en: After performing all these tricks, if you don't get an accurate result, then
    you need to spend some time deriving new features and use them.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行了这些技巧后，如果你没有得到准确的结果，那么你需要花一些时间推导出新的特征并使用它们。
- en: You can reiterate all the preceding steps until you get a satisfactory outcome.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以反复执行之前的所有步骤，直到你得到一个令人满意的结果。
- en: This is how you can approach the development of an NLP application. You should
    observe your results and then take sensible, necessary steps in the next iteration.
    Be smart in your analysis, think about all the problems, and then reiterate to
    solve them. If you don't analyze your result thoroughly, then reiteration never
    helps you. So keep calm, think wisely, and reiterate. Don't worry; we will look
    at the previous process when we develop NLP applications using ML algorithms.
    If you are on the research side, then I strongly recommend you understand the
    math behind the ML algorithms, but if you are a beginner and not very familiar
    with math, then you can read the documentation of the ML library. Those who lay
    between these two zones, try to figure out the math and then implement it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你可以开发NLP应用的方法。你应该观察你的结果，然后在下一次迭代中采取合理且必要的步骤。聪明地分析，思考所有问题，然后反复调整来解决它们。如果你没有彻底分析你的结果，那么反复调整永远不会帮助你。因此，保持冷静，明智地思考，并不断迭代。别担心；当我们使用ML算法开发NLP应用时，我们会回顾之前的过程。如果你是研究人员，我强烈建议你理解ML算法背后的数学原理，但如果你是初学者且对数学不太熟悉，你可以阅读ML库的文档。在这两者之间的同学，尝试理解数学原理并加以实施。
- en: Now, it's time to dive deep into the ML world and learn some really great algorithms.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候深入探索ML世界，学习一些真正出色的算法了。
- en: Understanding ML algorithms and other concepts
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解ML算法和其他概念
- en: Here, we will look at the most widely used ML algorithms for the NLP domain.
    We will look at algorithms as per the types of ML. First, we will start with supervised
    ML algorithms, then unsupervised ML algorithms, and lastly, semi-supervised ML
    algorithms. Here, we will understand the algorithm as well as the mathematics
    behind it. I will keep it easy so that those who are not from a strong mathematical
    background can understand the intuitive concept behind the algorithm. After that,
    we will see how we can practically use these algorithms to develop an NLP application.
    We will develop a cool NLP application which will help you understand algorithms
    without any confusion.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将介绍 NLP 领域中最广泛使用的机器学习算法。我们将根据机器学习的类型来看不同的算法。首先，我们将从监督学习算法开始，然后是无监督学习算法，最后是半监督学习算法。在这里，我们不仅会了解每个算法，还会学习其背后的数学原理。我会尽量简化内容，以便那些没有强大数学背景的人也能理解算法背后的直观概念。之后，我们将看到如何将这些算法应用于实际开发一个
    NLP 应用程序。我们将开发一个很酷的 NLP 应用，它将帮助你理解这些算法，而不会让你感到困惑。
- en: So, let's begin!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始吧！
- en: Supervised ML
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习（Supervised ML）
- en: We saw the introduction to supervised machine learning earlier in this chapter.
    Whatever techniques and datasets we see and use include their outcome, result,
    or labels that are already given in the dataset. So, this means that whenever
    you have a labeled dataset, you can use supervised ML algorithms.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章前面看到过监督学习的简介。我们所看到和使用的所有技术和数据集都包含了它们的结果、输出或标签，这些内容已经在数据集中给定了。因此，这意味着每当你拥有一个带标签的数据集时，你可以使用监督学习算法。
- en: 'Before starting off with algorithms, I will introduce two major concepts for
    supervised ML algorithms. This will also help you decide which algorithm to choose
    to solve NLP or any other data science-related problem:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始学习算法之前，我会先介绍两个监督学习算法的主要概念。这也将帮助你决定在解决 NLP 或其他数据科学相关问题时，选择哪个算法：
- en: Regression
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归
- en: Classification
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Regression
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归
- en: Regression is a statistical process that estimates the relationships between
    variables. Suppose you have a bunch of variables and you want to find out the
    relationship between them. First, you need to find out which are the dependent
    variable, and which are the independent variables. Regression analysis helps you
    understand how the dependent variable changes its behavior or value for given
    values of independent variables. Here, dependent variables depend on the values
    of independent variables, whereas independent variables take values that are not
    dependent on the other variables.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 回归是一个统计过程，用于估算变量之间的关系。假设你有一堆变量，想要找出它们之间的关系。首先，你需要确定哪些是因变量，哪些是自变量。回归分析可以帮助你理解因变量如何根据自变量的取值变化其行为或值。在这里，因变量依赖于自变量的值，而自变量的取值则不依赖于其他变量。
- en: 'Let''s take an example to give you a clear understanding. If you have a dataset
    that has the height of a human and you need to decide the weight based on the
    height, this is supervised ML and you already have the age in your dataset. So,
    you have two attributes, which are also called variables: height, and weight.
    Now, you need to predict the weight as per the given height. So, think for some
    seconds and let me know which data attribute or variable is dependent and which
    is independent. I hope you have some thoughts. So, let me answer now. Here, weight
    is the dependent data attribute or variable that is going to be dependent on the
    variable-height. Height is the independent variable. The independent variable
    is also called a **predictor**(**s**). So if you have a certain mapping or relationship
    between the dependent variable and independent variables, then you can also predict
    the weight for any given height.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来帮助你更清楚地理解。如果你有一个数据集，里面包含人的身高数据，你需要根据身高来预测体重，这就是监督学习，而你数据集里已经包含了年龄数据。因此，你有两个属性，也叫变量：身高和体重。现在，你需要根据给定的身高预测体重。那么，稍微思考一下，告诉我哪个数据属性或变量是因变量，哪个是自变量。我希望你有一些想法。现在让我来解答。这里，体重是因变量，它依赖于身高这个自变量。身高是自变量。自变量也被称为**预测变量**（**s**）。所以，如果你有因变量和自变量之间的某种映射关系，你就可以根据身高预测体重。
- en: 'Note that regression methods are used when our output or dependent variable
    takes a continuous value. In our example, weight can be any value such as 20 kg,
    20.5 kg, 20.6 kg, 60 kg, and so on. For other datasets or applications, the values
    of the dependent variable can be any real number. Refer to *Figure 8.8*:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们的输出或因变量是连续值时，使用的是回归方法。在我们的示例中，体重可以是任何值，例如20公斤、20.5公斤、20.6公斤、60公斤等等。对于其他数据集或应用，因变量的值可以是任何实数。参见*图8.8*：
- en: '![](img/005390a5-d64d-4f8f-a003-a9604980576e.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/005390a5-d64d-4f8f-a003-a9604980576e.png)'
- en: 'Figure 8.8: Linear regression example'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8：线性回归示例
- en: Classification
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: In this section, we will look at the other major concept of supervised ML, which
    is called **classification techniques**. This is also called **statistical classification**.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨监督学习的另一个主要概念，即**分类技术**。这也叫做**统计分类**。
- en: Statistical classification is used to identify a category for a given new observation.
    So, we have many categories in which we can put the new observation. However,
    we are not going to blindly choose any category, but we will use the given dataset,
    and based on this dataset, we will try to identify the best suited category for
    the new observation and put our observation in this category or class.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 统计分类用于为给定的新观察值确定类别。因此，我们有许多类别可以将新观察值归类。然而，我们不会盲目地选择任何类别，而是会使用给定的数据集，并根据这个数据集，尝试确定最适合新观察值的类别，并将我们的观察值放入这个类别或类中。
- en: Let's take an example from the NLP domain itself. You have a dataset that contains
    a bunch of emails and those emails already have a class label, which is either
    spam or non-spam. So, our dataset is categorized into two classes--spam and non-spam.
    Now if we get a new email, then can we categorize that particular e-mail into
    the spam or not-spam class? The answer is yes. So, to classify the new e-mail
    we use our dataset and ML algorithm and provide the best suited class for the
    new mail. The algorithm that implements the classification is called a **classifier**.
    Sometimes, the term classifier also refers to the mathematical function which
    is implemented by the classifier algorithm that maps the input data to a category.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从自然语言处理领域本身举个例子。你有一个数据集，里面包含一堆电子邮件，并且这些电子邮件已经有了分类标签，标签是垃圾邮件（spam）或非垃圾邮件（non-spam）。所以，我们的数据集分为两个类别——垃圾邮件和非垃圾邮件。那么，如果我们收到一封新邮件，我们能否将其分类为垃圾邮件或非垃圾邮件呢？答案是肯定的。为了对新邮件进行分类，我们使用我们的数据集和机器学习算法，给新邮件提供最合适的分类。实现分类的算法叫做**分类器**。有时，分类器这个术语也指由分类器算法实现的数学函数，这个函数将输入数据映射到某个类别。
- en: 'Note that this point helps you identify the difference between regression and
    classification. In classification, the output variable takes the class label that
    is basically a discrete or categorical value. In regression, our output variable
    takes a continuous value. Refer to *Figure 8.9*:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这一点有助于你识别回归和分类之间的区别。在分类中，输出变量采用的是类标签，基本上是离散的或类别性的值。而在回归中，我们的输出变量则是连续的值。参见*图8.9*：
- en: '![](img/d7b7bed5-10ec-4e77-b2ee-519df7118234.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7b7bed5-10ec-4e77-b2ee-519df7118234.png)'
- en: 'Figure 8.9: Classification visualization for intuitive purposes'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9：为了直观理解，展示的分类可视化
- en: 'Now that we have an idea about regression and classification, let''s understand
    the basic terminology that I am going to use constantly while explaining ML algorithms
    specially for classification:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对回归和分类有了一个大致的了解，让我们理解一些基本术语，这些术语我会在解释机器学习算法，特别是分类时不断使用：
- en: '**Instance:** This is referred to as input and generally, they are in the form
    of vectors. These are vectors of attributes. In the POS tagger example, we used
    features that we derived from each word and converted them to vectors using `scikit-learns`
    API `DictVectorizer`. The vector values were fed into the ML algorithm so these
    input vectors are the instances.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实例：** 这指的是输入，通常它们是以向量的形式存在。这些是属性的向量。在POS标注器示例中，我们使用了从每个单词派生的特征，并通过`scikit-learn`的API
    `DictVectorizer`将它们转换为向量。这些向量值被输入到机器学习算法中，所以这些输入向量就是实例。'
- en: '**Concept:** The concept is referred to as a function that maps input to output.
    So, if we have an e-mail content and we are tying to find out whether that e-mail
    content is spam or non-spam, we have to focus on some certain parameters from
    the instance or input and then generate the result. The process of how to identify
    certain output from certain input is called **concept**. For example, you have
    some data about the height of a human in feet. After seeing the data, you can
    decide whether the person is tall or short. Here, the concept or function helps
    you to find the output for a given input or instance. So, if I put this in mathematical
    format, then the concept is a mapping between an object in a world and membership
    in a set.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概念：** 概念是指将输入映射到输出的一个函数。所以，如果我们有一封电子邮件内容，并且我们试图判断该电子邮件是垃圾邮件还是非垃圾邮件，我们必须关注输入或实例中的某些特定参数，然后生成结果。如何从某个输入中识别出特定输出的过程称为**概念**。例如，你有一些关于人类身高的数据，以英尺为单位。看到这些数据后，你可以判断这个人是高还是矮。在这里，概念或函数帮助你根据给定的输入或实例找到输出。所以，如果我用数学的形式来表达，那么概念就是一个世界中对象与集合中的成员之间的映射。'
- en: '**Target concept:** The target concept is referred to as the actual answer
    or specific function or some particular idea that we are trying to find. As humans,
    we have understood a lot of concepts in our head, such as by reading the e-mail,
    we can judge that it''s spam or non-spam, and if your judgments is true, then
    you can get the actual answer. You know what is called **spam** and what is not,
    but unless we actually have it written down somewhere, we don''t know whether
    it''s right or wrong. If we note these actual answers for each of the raw data
    in our dataset, then it will be much easier for us to identify which e-mails should
    be considered as spam e-mails and which not. This helps you find out the actual
    answer for a new instance.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标概念：** 目标概念是指我们试图找到的实际答案、特定函数或某个特定的想法。作为人类，我们已经理解了很多概念，比如通过阅读电子邮件，我们可以判断它是垃圾邮件还是非垃圾邮件，如果你的判断是正确的，那么你就可以得到实际的答案。你知道什么是**垃圾邮件**，什么不是，但除非我们实际将其写下来，否则我们无法知道它是否正确。如果我们为数据集中的每个原始数据记录下这些实际答案，那么我们就可以更轻松地识别哪些电子邮件应被视为垃圾邮件，哪些不应被视为垃圾邮件。这有助于你为新的实例找到实际答案。'
- en: '**Hypothesis class:** Is the class of all possible functions that can help
    us classify our instance. We have just seen the target concept where we are trying
    to find out a specific function, but here we can think of a subset of all the
    possible and potential functions that can help us figure out our target concept
    for classification problems. Here, I want to point out that we are seeing this
    terminology for classification tasks so don''t consider the x2 function, because
    it''s a linear function and we are performing classification not regression.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假设类：** 假设类是所有可能的函数的集合，它们可以帮助我们对实例进行分类。我们刚刚看到了目标概念，我们正在试图找出一个特定的函数，但在这里我们可以考虑所有可能且潜在的函数的一个子集，这些函数可以帮助我们找到分类问题的目标概念。在这里，我想指出的是，我们看到的这个术语是针对分类任务的，因此不要考虑x2函数，因为它是线性函数，而我们正在进行的是分类，而不是回归。'
- en: '**Training dataset:** In classification we are trying to find the target concept
    or actual answer. Now, how can we actually get this final answer? To get the final
    answer using ML techniques, we will use some sample set, training set, or training
    dataset that will help us find out the actual answer. Let''s see what a training
    set is. A training set contains all the input paired with a label. Supervised
    classification problems need a training dataset that has been labeled with the
    actual answer or actual output. So, we are not just passing our knowledge to the
    machine about what is spam or non-spam; we are also providing a lot of examples
    to the machine, such as this is a spam mail, this is non-spam mail, and so on.
    So, for the machine it will be easy to understand the target concept.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据集：** 在分类中，我们试图找出目标概念或实际答案。那么，我们如何实际获得这个最终答案呢？为了使用机器学习技术得到最终答案，我们将使用一些样本集、训练集或训练数据集，帮助我们找出实际答案。让我们看看什么是训练集。训练集包含所有与标签配对的输入。监督式分类问题需要一个带有实际答案或实际输出标签的训练数据集。所以，我们不仅是向机器传授什么是垃圾邮件或非垃圾邮件，我们还为机器提供了很多例子，比如这是垃圾邮件，这是非垃圾邮件，等等。对于机器来说，这样就能更容易理解目标概念。'
- en: '**ML-model:** We will use the training dataset and feed this data to the ML
    algorithm. Then, the ML algorithm will try to learn the concept using a lot of
    training examples and generate the output model. This output model can be used
    later on to predict or decide whether the given new mail is spam or not-spam.
    This generated output is called the **ML-model**. We will use a generated ML-model
    and give the new mail as input and this ML-model will generate the answer as to
    whether the given mail belongs to the spam category or not-spam category.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ML模型：** 我们将使用训练数据集，并将这些数据输入到ML算法中。然后，ML算法将尝试通过大量训练示例学习概念，并生成输出模型。这个输出模型可以在以后用于预测或判断给定的新邮件是垃圾邮件还是非垃圾邮件。这个生成的输出被称为**ML模型**。我们将使用生成的ML模型，输入新的邮件，这个ML模型将生成是否该邮件属于垃圾邮件类别或非垃圾邮件类别的答案。'
- en: '**Candidate:** The candidate is the potential target concept that our ML-model
    tells us for the new example. So, you can say that the candidate is the predicted
    target concept by the machine, but we don''t know whether the predicted or generated
    output that is the candidate here is actually the correct answer or not. So, let''s
    take an example. We have provided a lot of examples of emails to the machine.
    The machine may generalize the concept of spam and not-spam mails. We will provide
    a new e-mail and our ML-model will say that it''s non-spam, however, we need to
    check whether our ML-model''s answer is right or wrong. This answer is referred
    to as a candidate. How can we check whether the answer generated by the ML-model
    matches with the target concept or not? To answer your question, I will introduce
    the next term, that is, testing set.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**候选：** 候选是我们的ML模型告诉我们新示例的潜在目标概念。所以，你可以说候选是机器预测的目标概念，但我们并不知道这个预测或生成的输出——即候选——是否是正确答案。让我们举个例子。我们已经提供了很多邮件示例给机器，机器可能会概括垃圾邮件和非垃圾邮件的概念。我们提供一封新的电子邮件，ML模型可能会说它是非垃圾邮件，然而，我们需要检查ML模型的答案是否正确。这个答案被称为候选。那么，如何检查ML模型生成的答案是否与目标概念匹配呢？为了回答这个问题，我将介绍下一个术语——测试集。'
- en: '**Testing set:** The testing set looks similar to the training dataset. Our
    training dataset has e-mails with labels such as spam or non-spam. So, I will
    take the answer that is considered as the candidate and we will check in our testing
    set whether it is non-spam or spam. We will compare our answer with the testing
    set''s answer and try to figure out whether the candidate has a true answer or
    false answer. Suppose that not-spam is the right answer. Now, you will take another
    e-mail and the ML-model will generate a non-spam answer again. We will again check
    this with our testing set, and this time the ML-model generates a wrong answer-
    the mail is actually spam but the ML-model misclassifies it in the non-spam category.
    So the testing set helps us validate our ML-model. Note that the training and
    testing sets should not be the same. This is because, if your machine uses the
    training dataset to learn the concept and you test your ML-model on the training
    dataset, then you are not evaluating your ML-model fairly. This is considered
    cheating in ML. So, your training dataset and testing set should always be different;
    the testing set is the dataset that has never been seen by your machine. We are
    doing this because we need to check the machines ability on how much the given
    problem can be generalized. Here, generalized means how the ML-model reacts to
    unknown and unseen examples. If you are still confused, then let me give you another
    example. You are a student and a teacher taught you some facts and gave you some
    examples. Initially, you just memorized the facts. So as to check that you got
    the right concept, the teacher will give a test and give you novel examples where
    you need to apply your learning. If you are able to apply your learning perfectly
    to the new example in the test, then you actually got the concept. This proves
    that we can generalize the concept that has been taught by a teacher. We are doing
    the same thing with the machine.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集：** 测试集看起来与训练数据集相似。我们的训练数据集包含带标签的电子邮件，例如垃圾邮件或非垃圾邮件。所以，我会拿一个被认为是候选答案的结果，我们将检查在测试集中的答案是否是非垃圾邮件还是垃圾邮件。我们将把我们的答案与测试集中的答案进行比较，并尝试找出候选答案是正确的还是错误的。假设非垃圾邮件是正确答案。现在，你会拿到另一封电子邮件，ML
    模型将再次生成非垃圾邮件答案。我们将再次与测试集核对，结果这次 ML 模型生成了错误的答案——邮件实际上是垃圾邮件，但 ML 模型错误地将其分类为非垃圾邮件。所以，测试集帮助我们验证我们的
    ML 模型。需要注意的是，训练集和测试集不应相同。因为，如果你的机器使用训练数据集来学习概念，并且你在训练数据集上测试 ML 模型，那么你就没有公平地评估你的
    ML 模型。这被认为是在 ML 中作弊。因此，训练集和测试集应该始终不同；测试集是机器从未见过的数据集。我们这么做是因为我们需要检查机器在多大程度上能将给定问题进行泛化。这里，泛化意味着
    ML 模型如何应对未知和未见过的例子。如果你仍然感到困惑，那么让我给你另一个例子。你是学生，老师教了你一些事实，并给了你一些例子。起初，你只是死记硬背这些事实。为了检查你是否掌握了正确的概念，老师将给你一场测试，并给你一些新颖的例子，你需要应用所学的知识。如果你能完美地将所学知识应用到测试中的新例子上，那么你实际上已经掌握了概念。这证明了我们可以将老师教的概念进行泛化。我们和机器做的事情是一样的。'
- en: Now let's understand ML algorithms.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来了解 ML 算法。
- en: ML algorithms
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML 算法
- en: We have understood enough about the essential concepts of ML, and now we will
    explore ML algorithms. First, we will see the supervised ML algorithms that are
    mostly used in the NLP domain. I'm not going to cover all the supervised ML algorithms
    here, but I'm explaining those that are most widely used in the NLP domain.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了 ML 的基本概念，现在我们将探索 ML 算法。首先，我们将了解在 NLP 领域中最常用的监督学习 ML 算法。我不会在这里涵盖所有的监督学习
    ML 算法，但我会解释那些在 NLP 领域最广泛使用的算法。
- en: In NLP applications, we mostly perform classification applying various ML techniques.
    So, here, our focus is mostly on the classification type of an algorithm. Other
    domains, such as analytics use various types of linear regression algorithms,
    as well as analytical applications but we are not going to look at those algorithms
    because this book is all about the NLP domain. As some concepts of linear regression
    help us understand deep learning techniques, we will look at linear regression
    and gradient descent in great detail with examples in [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml),
    *Deep Learning for NLP and NLG Problems*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 应用中，我们主要通过应用各种 ML 技术进行分类。因此，在这里，我们的重点主要是算法的分类类型。其他领域，例如分析学，使用各种类型的线性回归算法，以及分析应用，但我们不会讨论这些算法，因为本书专注于
    NLP 领域。由于一些线性回归的概念有助于我们理解深度学习技术，因此我们将在[第九章](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml)中详细探讨线性回归和梯度下降，并通过实例展示，*深度学习在
    NLP 和 NLG 问题中的应用*。
- en: We will develop some NLP applications using various algorithms so that you can
    see how the algorithm works and how NLP applications develop using ML algorithms.
    We will look at applications such as spam filtering.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用各种算法开发一些自然语言处理应用程序，您可以看到算法是如何工作的，以及如何使用机器学习算法开发自然语言处理应用程序。我们将研究像垃圾邮件过滤这样的应用。
- en: 'Refer to *Figure 8.10*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅*图 8.10*：
- en: '![](img/a1f7e7c8-ae21-4498-946a-6cd12db78927.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1f7e7c8-ae21-4498-946a-6cd12db78927.png)'
- en: 'Figure 8.10: Supervised classification ML algorithms that we are going to understand'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10：我们将要理解的监督分类机器学习算法
- en: Now let's start with our core ML part.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们从核心的机器学习部分开始。
- en: '**Logistic regression**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归**'
- en: I know you must be confused as to why I put logistic regression in the classification
    category. Let me tell you that it's just the name that is given to this algorithm,
    but it's used to predict the discrete output, so this algorithm belongs to the
    classification category.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道你一定很困惑，为什么我把逻辑回归放在分类类别里。让我告诉你，这只是给这个算法起的名字，但它用于预测离散的输出，因此这个算法属于分类类别。
- en: For this classification algorithm I will give you an idea how the logistic regression
    algorithm works and we will look at some basic mathematics related to it. Then,
    we will look the spam filtering application.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个分类算法，我将向您介绍逻辑回归算法的工作原理，并且我们将学习与之相关的一些基本数学知识。然后，我们将看看垃圾邮件过滤应用。
- en: First, we will consider binary classes such as spam or not-spam, good or bad,
    win or lose, 0 or 1, and so on to understand the algorithm and its application.
    Suppose I want to classify e-mails into the spam and not-spam category. Spam and
    not-spam are discrete output labels or target concepts. Our goal is to predict
    whether the new e-mail is spam or not-spam. Not-spam is also called **ham**. In
    order to build this NLP application, we will use logistic regression.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将考虑二元类别，比如垃圾邮件或非垃圾邮件、好或坏、赢或输、0或1等等，以理解算法及其应用。假设我想将电子邮件分类为垃圾邮件和非垃圾邮件。垃圾邮件和非垃圾邮件是离散的输出标签或目标概念。我们的目标是预测新的电子邮件是垃圾邮件还是非垃圾邮件。非垃圾邮件也称为**ham**。为了构建这个自然语言处理应用，我们将使用逻辑回归。
- en: Let's understand the technicality of the algorithm first.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先理解算法的技术细节。
- en: 'Here, I''m stating facts related to mathematics and this algorithm in a very
    simple manner. A general approach to understanding this algorithm is as follows.
    If you know some part of ML, then you can connect the dots, and if you are new
    to ML, then don''t worry, because we are going to understand every part:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我正在以非常简单的方式陈述与数学和这个算法相关的事实。理解这个算法的一般方法如下。如果你了解一些机器学习的知识，那么你可以连接这些点；如果你是机器学习的新手，那么不用担心，因为我们将逐步理解每个部分：
- en: We are defining our hypothesis function that helps us generate our target output
    or target concept
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在定义我们的假设函数，它帮助我们生成目标输出或目标概念。
- en: We are defining the cost function or error function and we choose the error
    function in such a way that we can derive the partial derivate of the error function
    so that we can calculate gradient descent easily
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在定义成本函数或误差函数，并且我们选择误差函数的方式是使得我们可以推导出误差函数的偏导数，从而能够轻松计算梯度下降。
- en: We are trying to minimize the error so that we can generate a more accurate
    label and classify the data accurately
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在尝试最小化误差，以便生成更准确的标签，并准确分类数据。
- en: In statistics, logistic regression is also called **logit regression** or the
    **logit model**. This algorithm is mostly used as a binary class classifier, which
    means that there should be two different classes to classify the data. The binary
    logistic model is used to estimate the probability of a binary response and it
    generates the response based on one or more predictors or independent variables
    or features. This is the ML algorithm that uses basic mathematics concepts in
    deep learning as well.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，逻辑回归也被称为**logit回归**或**logit模型**。这个算法通常作为二分类器使用，这意味着数据应该被分为两个不同的类别。二元逻辑回归模型用于估计二元响应的概率，并根据一个或多个预测变量、独立变量或特征生成响应。这也是一个在深度学习中使用基本数学概念的机器学习算法。
- en: First, I want to explain why this algorithm is called **logistic regression**.
    The reason is that the algorithm uses a logistic function or sigmoid function.
    Logistic function and sigmoid function are synonyms.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我想解释一下为什么这个算法叫做**逻辑回归**。原因是该算法使用了逻辑函数或S型函数。逻辑函数和S型函数是同义词。
- en: We use the sigmoid function as a hypothesis function. What do you mean by hypothesis
    function? Well, as we saw earlier, the machine has to learn mapping between data
    attributes and the given label in such a way that it can predict the label for
    new data. This can be achieved by the machine if it learns this mapping via a
    mathematical function. The mathematical function is the hypothesis function that
    the machine will use to classify the data and predict labels or the target concept.
    We want to build a binary classifier, so our label is either spam or ham. So,
    mathematically, I can assign 0 for ham or not-spam and 1 for spam or vice versa.
    These mathematically assigned labels are our dependent variables. Now, we need
    our output labels to be either zero or one. Mathematically, the label is *y* and
    *y* *ε {0, 1}*. So we need to choose the hypothesis function that will convert
    our output value to zero or one. The logistic function or sigmoid function does
    exactly that and this is the main reason why logistic regression uses a sigmoid
    function as the hypothesis function.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Sigmoid 函数作为假设函数。假设函数是什么意思呢？嗯，正如我们之前所看到的，机器必须学习数据属性和给定标签之间的映射，以便它能够预测新数据的标签。如果机器能够通过一个数学函数学习这种映射，就能够实现这一目标。这个数学函数就是假设函数，机器将使用它来对数据进行分类并预测标签或目标概念。我们想构建一个二元分类器，所以我们的标签要么是垃圾邮件，要么是非垃圾邮件。因此，从数学上讲，我可以为非垃圾邮件赋值
    0，为垃圾邮件赋值 1，或者反之。这些数学分配的标签就是我们的因变量。现在，我们需要让输出标签为零或一。从数学上讲，标签是 *y* 且 *y* *ε {0,
    1}*。所以我们需要选择一个假设函数，将输出值转换为零或一。逻辑函数或 Sigmoid 函数正是做到了这一点，这也是为什么逻辑回归使用 Sigmoid 函数作为假设函数的主要原因。
- en: '**Logistic or sigmoid function**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑或 Sigmoid 函数**'
- en: 'The mathematical equation for the logistic or sigmoid function is as shown:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑或 Sigmoid 函数的数学方程如图所示：
- en: '![](img/863d0a74-fdc4-4500-8a23-1feba25ac799.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/863d0a74-fdc4-4500-8a23-1feba25ac799.png)'
- en: 'Figure 8.11: Logistic or sigmoid function'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11：逻辑或 Sigmoid 函数
- en: 'You can see the plot showing *g(z)*. Here, *g(z)= Φ(z)*. Refer to *Figure 8.12*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到显示 *g(z)* 的图形。这里，*g(z)= Φ(z)*。参见 *图 8.12*：
- en: '![](img/b8e22a24-1fe8-4409-a7e5-7239e4cdcbeb.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8e22a24-1fe8-4409-a7e5-7239e4cdcbeb.png)'
- en: 'Figure 8.12: Graph of the sigmoid or logistic function'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12：Sigmoid 或逻辑函数的图形
- en: 'From the preceding graph, you can find the following facts:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，你可以得出以下结论：
- en: If you have *z* value greater or equal to zero, then the logistic function gives
    the output value one
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *z* 值大于或等于零，则逻辑函数给出输出值一。
- en: If you have value of *z* less than zero, then the logistic function generates
    the output zero
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *z* 值小于零，则逻辑函数生成输出零。
- en: 'You can see the mathematical condition for the logistic function as shown:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到逻辑函数的数学条件，如下所示：
- en: '![](img/8cbb67fc-3b82-4d35-8629-c80017d3f01e.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8cbb67fc-3b82-4d35-8629-c80017d3f01e.png)'
- en: 'Figure 8.13: Logistic function mathematical property'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13：逻辑函数的数学属性
- en: We can use this function to perform binary classification.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个函数来执行二元分类。
- en: 'Now it''s time to show how this sigmoid function will be represented as the
    hypothesis function:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候展示这个 Sigmoid 函数如何作为假设函数来表示：
- en: '![](img/0a6d0f42-1fbe-4895-83db-02ef9a540cfb.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a6d0f42-1fbe-4895-83db-02ef9a540cfb.png)'
- en: 'Figure 8.14: Hypothesis function for logistic regression'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14：逻辑回归的假设函数
- en: 'If we take the preceding equation and substitute the value of *z* with *θ^Tx*,
    then the equation given in *Figure 8.11* is converted to the equation in *Figure
    8.15*:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取前面的方程并将 *z* 的值替换为 *θ^Tx*，那么 *图 8.11* 中的方程将转化为 *图 8.15* 中的方程：
- en: '![](img/62212b3a-fafb-4ab7-828d-14cd2085a571.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62212b3a-fafb-4ab7-828d-14cd2085a571.png)'
- en: 'Figure 8.15: Actual hypothesis function after mathematical manipulation'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15：数学操作后的实际假设函数
- en: Here, *h[θ]x* is the hypothesis function, *θ^T* is the matrix of the features
    or independent variables and transpose representation of it, *x* is for all independent
    variables or all possible features set. In order to generate the hypothesis equation,
    we replace the *z* value of the logistic function with *θ^Tx*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*h[θ]x* 是假设函数，*θ^T* 是特征或自变量的矩阵及其转置表示，*x* 是所有自变量或所有可能的特征集。为了生成假设方程，我们将逻辑函数的
    *z* 值替换为 *θ^Tx*。
- en: 'Using the hypothesis equation, the machine actually tries to learn mapping
    between input variables or input features and output labels. Let''s talk a bit
    about the interpretation of this hypothesis function. Can you think of the best
    way to predict the class label? According to me, we can predict the target class
    label using the probability concept. We need to generate probability for both
    classes and whatever class has a high probability will be assigned to that particular
    instance of features. In binary classification, the value of *y* or the target
    class is either zero or one. If you are familiar with probability, then you can
    represent the probability equation given in *Figure 8.16*:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用假设方程，机器实际上尝试学习输入变量或输入特征与输出标签之间的映射。让我们稍微谈谈这个假设函数的解释。你能想出预测类别标签的最佳方法吗？据我看，我们可以利用概率概念预测目标类别标签。我们需要为两类生成概率，无论哪一类的概率较高，都将分配给该特定特征实例。在二元分类中，*y*
    的值或目标类别为零或一。如果你熟悉概率，那么你可以表示在*图 8.16*中给出的概率方程：
- en: '![](img/dadb2f88-1ff5-47d0-bf8c-f88ab92a44c8.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dadb2f88-1ff5-47d0-bf8c-f88ab92a44c8.png)'
- en: 'Figure 8.16: Interpretation of the hypothesis function using probabilistic
    representation'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16：使用概率表示假设函数的解释
- en: So those who are not familiar with probability, *P(y=1|x;θ )* can be read like
    this - probability of *y =1*, given *x*, and parameterized by *θ*. In simple language,
    you can say that this hypothesis function will generate the probability value
    for target output *1* where we give features matrix *x* and some parameter *θ*.
    We will see later on why we need to generate probability, as well as how we can
    generate probability values for each of the classes.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，那些对概率不熟悉的人，*P(y=1|x;θ )* 可以这样阅读 - 给定*x*，*θ* 参数化时，*y =1* 的概率。简单来说，你可以说这个假设函数将为目标输出*1*生成概率值，其中我们给出特征矩阵*x*和一些参数*θ*。稍后我们将看到为什么需要生成概率，以及如何为每个类别生成概率值。
- en: Here, we have completed the first step of a general approach to understanding
    logistic regression.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经完成了理解逻辑回归的一般方法的第一步。
- en: '**Cost or error function for logistic regression**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归的成本或误差函数**'
- en: First, let's understand the cost function or error function. The cost function,
    loss function, or error function is a very important concept in ML, so we will
    understand the definition of the cost function.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们理解成本函数或误差函数。成本函数、损失函数或误差函数是机器学习中非常重要的概念，因此我们将理解成本函数的定义。
- en: The cost function is used to check how accurately our ML classifier performs.
    In our training dataset, we have data and label. When we use the hypothesis function
    and generate the output, we need to check how near we are to the actual prediction.
    If we predict the actual output label, then the difference between our hypothesis
    function output and the actual label is zero or minimum and if our hypothesis
    function output and actual label are not the same, then we have a big difference
    between them. If the actual label of an e-mail is spam, that is one, and our hypothesis
    function also generates the result 1 then the difference between the actual target
    value and predicted output value is zero, therefore the error in the prediction
    is also zero. If our predicted output is 1, and the actual output is zero, then
    we have maximum error between our actual target concept and prediction. So, it
    is important for us to have minimum error in our prediction. This is the very
    basic concept of the error function. We will get to the mathematics in some time.
    There are several types of error functions available, such as r2 error, sum of
    squared error, and so on. As per the ML algorithm and hypothesis function, our
    error function also changes.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数用于检查我们的机器学习分类器的精度如何。在我们的训练数据集中，我们有数据和标签。当我们使用假设函数并生成输出时，我们需要检查我们的预测与实际预测的接近程度。如果我们预测实际输出标签，则我们的假设函数输出与实际标签之间的差异为零或最小，如果我们的假设函数输出与实际标签不同，则它们之间存在很大差异。如果一个电子邮件的实际标签是垃圾邮件，即为1，而我们的假设函数也生成结果1，则实际目标值与预测输出值之间的差异为零，因此预测中的误差也为零。如果我们预测的输出为1，而实际输出为零，则我们的实际目标概念和预测之间的误差最大。因此，对我们来说，预测中的误差最小非常重要。这是误差函数的非常基本的概念。我们稍后会讲到数学部分。有几种类型的误差函数可用，例如r2误差、平方误差和其他。根据机器学习算法和假设函数，我们的误差函数也会发生变化。
- en: What will the error function be for logistic regression? What is θ and, if I
    need to choose some value of θ, how can I approach it? So, here, I will give all
    the answers.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于逻辑回归，误差函数将是什么？什么是 θ，如果我需要选择 θ 的某个值，我该如何接近它？那么，这里我将给出所有答案。
- en: 'Let me give you some background on linear regression. We generally use sum
    of squared error or residual error as the cost function in linear regression.
    In linear regression, we are trying to generate the line of best fit for our dataset.
    In the previous example, given height, I want to predict the weight. We first
    draw a line and measure the distance from each of the data points to the line.
    We will square these distances, sum them and try to minimize the error function.
    Refer to *Figure 8.17*:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我给你一些线性回归的背景知识。我们通常使用平方误差和残差误差作为线性回归中的代价函数。在线性回归中，我们试图为数据集生成最佳拟合直线。在前面的例子中，给定身高，我想预测体重。我们首先画一条线，并测量每个数据点到这条线的距离。我们将这些距离平方，求和并尝试最小化误差函数。参考*图
    8.17*：
- en: '![](img/4228b2a0-1e45-487d-af77-a217986d8611.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4228b2a0-1e45-487d-af77-a217986d8611.png)'
- en: 'Figure 8.17: Sum of squared error representation for reference'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17：平方误差和残差误差表示图示
- en: You can see the distance of each data point from the line is denoted using small
    vertical lines. We will take these distances, square them and then sum them. We
    will use this error function. We have generated a partial derivative with respect
    to the slope of line *m* and intercept *b*. Here, in *Figure 8.17,* our *b* is
    approximately *0.9* and *m* is approximately two thirds. Every time, we calculate
    the error and update the value of *m* and *b* so that we can generate the line
    of best fit. The process of updating *m* and *b* is called **gradient descent**.
    Using gradient descent, we update *m* and *b* in such a way that our error function
    has minimum error value and we can generate the line of best fit. Gradient descent
    gives us a direction in which we need to plot a line. You can find a detailed
    example in [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml), *Deep Learning
    for NLP and NLG Problems*. So, by defining the error function and generating partial
    derivatives, we can apply the gradient descent algorithm that helps us minimize
    our error or cost function.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，每个数据点到直线的距离通过小的垂直线表示。我们将这些距离平方后再求和。我们将使用这个误差函数。我们已经生成了一个关于直线斜率 *m* 和截距
    *b* 的偏导数。在*图 8.17*中，我们的 *b* 大约是 *0.9*，而 *m* 大约是三分之二。每次，我们都会计算误差并更新 *m* 和 *b* 的值，以便生成最佳拟合直线。更新
    *m* 和 *b* 的过程叫做**梯度下降**。通过使用梯度下降，我们更新 *m* 和 *b* 的方式使得误差函数的误差值最小化，从而可以生成最佳拟合直线。梯度下降给了我们需要绘制直线的方向。你可以在[第
    9 章](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml)《深度学习在自然语言处理和生成中的应用》中找到详细的例子。所以，通过定义误差函数并生成偏导数，我们可以应用梯度下降算法，帮助我们最小化误差或代价函数。
- en: 'Now back to the main question: Can we use the error function for logistic regression?
    If you know functions and calculus well, then probably your answer is no. That
    is the correct answer. Let me explain this for those who aren''t familiar with
    functions and calculus.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到主要问题：我们能否将误差函数用于逻辑回归？如果你对函数和微积分非常了解，那么你的答案可能是否定的。那是正确的答案。让我为那些不熟悉函数和微积分的人解释一下。
- en: 'In linear regression, our hypothesis function is linear, so it is very easy
    for us to calculate the sum of squared errors, but here, we will use the sigmoid
    function, which is a non-linear function. If you apply the same function that
    we used in linear regression, it will not turn out well because if you take the
    sigmoid function, put in the sum of squared error function, and try to visualize
    all the possible values, then you will get a non-convex curve. Refer to *Figure
    8.18*:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，我们的假设函数是线性的，所以我们可以很容易地计算平方误差的总和，但在这里，我们将使用Sigmoid函数，这是一种非线性函数。如果你应用我们在线性回归中使用的相同函数，结果将不太理想，因为如果你把Sigmoid函数带入平方误差函数并尝试可视化所有可能的值，那么你将得到一个非凸曲线。参考*图
    8.18*：
- en: '![](img/45a5426c-5346-4720-93d3-ede748ddf07a.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45a5426c-5346-4720-93d3-ede748ddf07a.png)'
- en: 'Figure 8.18: Non-convex and convex curve (Image credit: http://www.yuthon.com/images/non-convex_and_convex_function.png)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.18：非凸和凸曲线（图片来源：[http://www.yuthon.com/images/non-convex_and_convex_function.png](http://www.yuthon.com/images/non-convex_and_convex_function.png)）
- en: In ML, we mostly use functions that are able to provide a convex curve because
    then we can use the gradient descent algorithm to minimize the error function
    and reach a global minimum. As you can see in *Figure 8.18*, a non-convex curve
    has many local minima, so to reach a global minimum is very challenging and time-consuming
    because you need to apply second order or *n*th order optimization to reach a
    global minimum, whereas in a convex curve, you can reach a global minimum certainly
    and quickly.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们通常使用能够提供凸曲线的函数，因为这样我们就可以使用梯度下降算法来最小化误差函数并达到全局最小值。正如你在*图 8.18*中看到的，非凸曲线有许多局部最小值，因此要达到全局最小值非常具有挑战性并且耗时，因为你需要应用二阶或*n*阶优化才能达到全局最小值，而在凸曲线中，你可以肯定且快速地达到全局最小值。
- en: So, if we plug our sigmoid function into sum of squared error, you get the non-convex
    function so we are not going to define the same error function that we used in
    linear regression.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们将我们的sigmoid函数代入平方误差之和，就会得到非凸函数，因此我们不会定义与线性回归中使用的相同的误差函数。
- en: 'We need to define a different cost function that is convex so that we can apply
    the gradient descent algorithm and generate a global minimum. We will use the
    statistical concept called **likelihood**. To derive the likelihood function,
    we will use the equation of probability that is given in *Figure 8.16* and we
    will consider all the data points in the training dataset. So, we can generate
    the following equation, which is called likelihood function. Refer to *Figure
    8.19*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要定义一个不同的代价函数，它是凸的，这样我们才能应用梯度下降算法并生成全局最小值。我们将使用统计学中称为**似然**的概念。为了推导似然函数，我们将使用*图
    8.16*中给出的概率方程，并考虑训练数据集中的所有数据点。这样，我们可以生成以下方程，这就是所谓的似然函数。参见*图 8.19*：
- en: '![](img/2ad6cf39-70c4-46c2-9f83-6d84ac1e84f6.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ad6cf39-70c4-46c2-9f83-6d84ac1e84f6.png)'
- en: 'Figure 8.19: The likelihood function for logistic regression (Image credit:
    http://cs229.stanford.edu/notes/cs229-notes1.pdf)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.19：逻辑回归的似然函数（图片来源：http://cs229.stanford.edu/notes/cs229-notes1.pdf）
- en: 'Now, in order to simplify the derivative process, we need to convert the likelihood
    function to a monotonically increasing function. That can be achieved by taking
    the natural logarithm of the likelihood function, which is called **log likelihood**.
    This log-likelihood is our cost function for logistic regression. Refer to the
    following equation in F*igure 8.20*:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了简化求导过程，我们需要将似然函数转换为单调递增的函数。这可以通过对似然函数取自然对数来实现，这就是**对数似然**。这个对数似然就是我们用于逻辑回归的代价函数。参见*图
    8.20*中的以下方程：
- en: '![](img/56af5c51-a448-42a1-99eb-e518f279afb7.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56af5c51-a448-42a1-99eb-e518f279afb7.png)'
- en: 'Figure 8.20: Cost function for logistic regression'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.20：逻辑回归的代价函数
- en: 'We will plot the cost function and understand the benefits it provides us with.
    In the *x* axis, we have our hypothesis function. Our hypothesis function range
    is *0* to *1*, so we have these two points on the *x* axis. Start with the first
    case, where *y = 1*. You can see the generated curve that is on the top right-hand
    side in *Figure 8.21*:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将绘制代价函数并理解它为我们提供的好处。在*x*轴上，我们有我们的假设函数。我们的假设函数的范围是*0*到*1*，因此我们在*x*轴上有这两个点。从第一个情况开始，*y
    = 1*。你可以看到在*图 8.21*中右上方生成的曲线：
- en: '![](img/3c093e59-0d98-4584-8f2f-a741bd4c3454.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c093e59-0d98-4584-8f2f-a741bd4c3454.png)'
- en: 'Figure 8.21: Logistic function cost function graph'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.21：逻辑函数代价函数图
- en: 'If you look at any log function plot, then it will look like the graph for
    error function *y=0*. Here, we flip that curve because we have a negative sign,
    then you get the curve which we have plotted for *y=1* value. In *Figure 8.21,*
    you can see the log graph, as well as the flipped graph in *Figure 8.22*:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看任何对数函数的图像，它看起来像是误差函数*y=0*的图形。在这里，由于我们有一个负号，我们翻转了该曲线，得到的就是我们为*y=1*值绘制的曲线。在*图
    8.21*中，你可以看到对数图形，以及在*图 8.22*中翻转后的图形：
- en: '![](img/b9d20413-29f8-4b0c-878f-056b20e642f7.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9d20413-29f8-4b0c-878f-056b20e642f7.png)'
- en: 'Figure 8.22: Comparing log(x) and -log(x) graph for better understanding of
    the cost function (Image credit : http://www.sosmath.com/algebra/logs/log4/log42/log422/gl30.gif)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.22：对数(x)和-log(x)图形比较，以更好地理解代价函数（图片来源：http://www.sosmath.com/algebra/logs/log4/log42/log422/gl30.gif）
- en: Here, we are interested in the values *0* and *1,* so we are considering that
    part of the graph that we have depicted in *Figure 8.21*. This cost function has
    some interesting and useful properties. If the predicted label or candidate label
    is the same as the actual target label, then the cost will be zero. You can put
    this as *y = 1* and if the hypothesis function predicts *H[θ](x) = 1* then *cost
    = 0*; if *H[θ](x)* tends to *0*, meaning that if it is more toward the zero, then
    the cost function blows up to *∞*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们关注的是*0*和*1*的值，因此我们正在考虑我们在*图8.21*中描绘的图形的那一部分。这个代价函数有一些有趣且有用的特性。如果预测标签或候选标签与实际目标标签相同，那么代价将为零。你可以将其表示为*y
    = 1*，如果假设函数预测为*H[θ](x) = 1*，那么*cost = 0*；如果*H[θ](x)*趋向于*0*，即如果它更接近零，那么代价函数将飙升至*∞*。
- en: For *y = 0*, you can see the graph that is on the top left-hand side in *Figure
    8.21.* This case condition also has the same advantages and properties that we
    saw earlier. It will blow to *∞* when the actual value is *0* and the hypothesis
    function predicts *1*. If the hypothesis function predicts *0* and the actual
    target is also *0*, then *cost = 0*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*y = 0*，你可以看到位于*图8.21*左上角的图形。这个情况也具有我们之前看到的相同优点和特性。当实际值为*0*且假设函数预测为*1*时，代价将飙升至*∞*。如果假设函数预测为*0*且实际目标也是*0*，那么*cost
    = 0*。
- en: Now, we will see why we are choosing this cost function. The reason is that
    this function makes our optimization easy, as we will use maximum log-likelihood
    because it has a convex curve that will help us run gradient descent.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来看看为什么我们选择这个代价函数。原因是这个函数使我们的优化变得简单，因为我们将使用最大对数似然函数，因为它有一个凸曲线，这将帮助我们进行梯度下降。
- en: 'In order to apply gradient descent, we need to generate the partial derivative
    with respect to *θ* and generate the equation that is given in *Figure 8.23*:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用梯度下降，我们需要生成关于*θ*的偏导数，并生成*图8.23*中给出的方程：
- en: '![](img/f2409931-a8ae-48b4-af4f-5d76e54a5340.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2409931-a8ae-48b4-af4f-5d76e54a5340.png)'
- en: 'Figure 8.23: Partial derivative to perform gradient descent (Image credit:
    http://2.bp.blogspot.com/-ZxJ87cWjPJ8/TtLtwqv0hCI/AAAAAAAAAV0/9FYqcxJ6dNY/s1600/gradient+descent+algorithm+OLS.png)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.23：执行梯度下降的偏导数（图片来源：http://2.bp.blogspot.com/-ZxJ87cWjPJ8/TtLtwqv0hCI/AAAAAAAAAV0/9FYqcxJ6dNY/s1600/gradient+descent+algorithm+OLS.png）
- en: This equation is used to update the parameter value of *θ*; ![](img/332b6e21-d2d7-4c80-af9a-678da52e59a3.png)
    defines the learning rate. This is the parameter that you can use to set how fast
    or how slow your algorithm should learn or train. If you set the learning rate
    too high, then the algorithm cannot learn, and if you set it too low, then it
    will take a lot of time to train. So you need to choose the learning rate wisely.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程用于更新参数*θ*的值；![](img/332b6e21-d2d7-4c80-af9a-678da52e59a3.png)定义了学习率。这个参数用于设置你的算法学习或训练的速度。如果你设置的学习率过高，算法将无法学习；如果设置过低，训练将需要很长时间。所以你需要明智地选择学习率。
- en: 'This is the end of our second point and we can begin with the third part, which
    is more of an implementation part. You can check out this GitHub link:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们第二部分的结束，接下来我们可以开始第三部分，它更侧重于实现部分。你可以查看这个GitHub链接：
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch8/Own_Logistic_Regression/logistic.py](https://github.com/jalajthanaki/NLPython/blob/master/ch8/Own_Logistic_Regression/logistic.py)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/blob/master/ch8/Own_Logistic_Regression/logistic.py](https://github.com/jalajthanaki/NLPython/blob/master/ch8/Own_Logistic_Regression/logistic.py)'
- en: This has an implementation of logistic regression and you can find its comparison
    with the given implementation in the `scikit-learn` library. Here, the code credit
    goes to Harald Borgen.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这实现了逻辑回归，你可以将其与给定的`scikit-learn`库中的实现进行比较。这里，代码归功于Harald Borgen。
- en: 'We will use this algorithm for spam filtering. Spam filtering is one of the
    basic NLP applications. Using this algorithm, we want to make an ML-model that
    classifies the given mail into either the spam or ham category. So, let''s make
    a spam-filtering application. The entire code is on this GitHub link:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个算法进行垃圾邮件过滤。垃圾邮件过滤是自然语言处理（NLP）中的基本应用之一。使用这个算法，我们希望构建一个机器学习模型，将给定的邮件分类为垃圾邮件或正常邮件。接下来，让我们制作一个垃圾邮件过滤应用程序。完整的代码在这个GitHub链接上：
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch8/Spamflteringapplication/Spam_filtering_logistic_regression.ipynb](https://github.com/jalajthanaki/NLPython/blob/master/ch8/Spamflteringapplication/Spam_filtering_logistic_regression.ipynb)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/blob/master/ch8/Spamflteringapplication/Spam_filtering_logistic_regression.ipynb](https://github.com/jalajthanaki/NLPython/blob/master/ch8/Spamflteringapplication/Spam_filtering_logistic_regression.ipynb)'
- en: 'In spam filtering, we will use the `CountVectorizer` API of `scikit-learn`
    to generate the features, then train using `LogisticRegression`. You can see the
    code snippet in *Figure 8.24*:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在垃圾邮件过滤中，我们将使用`scikit-learn`的`CountVectorizer` API来生成特征，然后使用`LogisticRegression`进行训练。你可以在*图
    8.24*中看到代码片段：
- en: '![](img/5000911e-5de5-4636-b878-4d45dc08b855.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5000911e-5de5-4636-b878-4d45dc08b855.png)'
- en: 'Figure 8.24: Spam filtering using logistic regression'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.24：使用逻辑回归的垃圾邮件过滤
- en: 'First, we perform some basic text analysis that will help us understand our
    data. Here, we have converted the text data to a vector format using `scikit-learn`
    API, `Count Vectorizer()`. This API uses **term frequency-inverse document frequency**
    (**tf-idf**) underneath. We have divided our dataset into a training dataset and
    a testing set so that we can check how our classifier model performs on the test
    dataset. You can see the output in *Figure 8.25*:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们进行一些基本的文本分析，帮助我们理解数据。在这里，我们使用`scikit-learn`的API `Count Vectorizer()`将文本数据转换为向量格式。这个API在底层使用了**词频-逆文档频率**（**tf-idf**）。我们将数据集划分为训练集和测试集，以便检查我们的分类器模型在测试集上的表现。你可以在*图
    8.25*中看到输出结果：
- en: '![](img/619e5db3-c446-4836-bff6-046b9222724c.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/619e5db3-c446-4836-bff6-046b9222724c.png)'
- en: 'Figure 8.25: Output of spam filtering using logistic regression'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.25：使用逻辑回归的垃圾邮件过滤输出结果
- en: '**Advantages of logistic regression**'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归的优点**'
- en: 'The following are the advantages of logistic regression:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是逻辑回归的优点：
- en: It can handle non-linear effects
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它能够处理非线性效应
- en: It can generate the probability score for each of the classes, which makes interpretation
    easy
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以为每个类别生成概率得分，这使得解释变得简单
- en: '**Disadvantages of logistic regression**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归的缺点**'
- en: 'The following are the disadvantages of logistic regression:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是逻辑回归的缺点：
- en: This classification technique is used for binary classification only. There
    are other algorithms which we can use if we want to classify data into more than
    two categories. We can use algorithms like random forest and decision tree for
    classifying the data into more than two categories.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种分类技术仅用于二分类。如果我们希望将数据分类到超过两个类别，我们可以使用其他算法，比如随机森林和决策树。
- en: If you provide lots of features as input to this algorithm, then the features
    space increases and this algorithm doesn't perform well
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你为该算法提供大量特征作为输入，那么特征空间会增大，导致算法表现不佳
- en: Chances of overfitting are high, which means that the classifier performs well
    on the training dataset but cannot generalize enough that it can predict the right
    target label for unseen data
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合的风险较高，这意味着分类器在训练集上表现良好，但无法很好地推广，无法为未见过的数据预测正确的目标标签
- en: Now it's time to explore the next algorithm, called **decision tree**.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候探索下一个算法——**决策树**了。
- en: '**Decision tree**'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**'
- en: '**Decision tree** (**DT**) is one of the oldest ML algorithms. This algorithm
    is very simple, yet robust. This algorithm provides us with a tree structure to
    make any decision. Logistic regression is used for binary classification, but
    you can use a decision tree if you have more than two classes.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**（**DT**）是最古老的机器学习算法之一。这个算法非常简单，但又很强大。它为我们提供了一种树状结构来做决策。逻辑回归用于二分类问题，但如果你的数据集有超过两个类别，可以使用决策树。'
- en: 'Let''s understand the decision tree by way of an example. Suppose, Chris likes
    to windsurf, but he has preferences - he generally prefers sunny and windy days
    to enjoy it and doesn''t surf on rainy or overcast days or less windy days either.
    Refer to *Figure 8.26*:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个例子来理解决策树。假设，Chris喜欢风帆冲浪，但他有一些偏好——他通常喜欢阳光明媚和有风的日子来享受冲浪，而不喜欢雨天、阴天或者风力较小的日子。参考*图
    8.26*：
- en: '![](img/abfe5342-5a76-4c78-ada6-9ca85e0b8d49.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/abfe5342-5a76-4c78-ada6-9ca85e0b8d49.png)'
- en: 'Figure 8.26: Toy dataset to understand the concept (Image credit: https://classroom.udacity.com)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.26：理解概念的玩具数据集（图片来源：https://classroom.udacity.com）
- en: As you can see, *o* (dots) are the happy weather conditions when Chris likes
    to wind surf and the *x* (crosses) are the bad weather conditions when Chris doesn't
    like to wind surf.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，*o*（点）代表的是克里斯喜欢风帆冲浪的好天气条件，而*x*（叉）代表的是克里斯不喜欢风帆冲浪的坏天气条件。
- en: The data that I have drawn is not linearly separable, which means that you can't
    classify or separate the red crosses and blue dots just using a single line. You
    might think that if the goal is just to separate the blue dots and red crosses,
    then I can use two lines and achieve this. However, can a single line separate
    the blue dots and red crosses? The answer is no, and that is the reason why I
    have told you that this dataset is not linearly separable. So for this kind of
    scenario, we will use a decision tree.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我画出的数据是不可线性分割的，这意味着你无法仅使用一条直线将红色的叉号和蓝色的点分开。你可能会认为，如果目标只是将蓝色的点和红色的叉号分开，那么我可以使用两条线来实现这一点。然而，一条直线能分开蓝色的点和红色的叉号吗？答案是否定的，这就是为什么我告诉你这个数据集是不可线性分割的原因。所以在这种情况下，我们将使用决策树。
- en: What does a decision tree actually do for you? In layman's terms, decision tree
    learning is actually about asking multiple linear questions. Let's understand
    what I mean by linear questions.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树到底能为你做什么呢？通俗来说，决策树学习实际上就是在问多个线性问题。让我们来理解一下线性问题的意思。
- en: 'Suppose we ask a question: Is it windy? You have two answers: Yes or No. We
    have a question that is related to wind, so we need to focus on the *x* axis of
    *Figure 8.26*. If our answer is: Yes, it''s windy, then we should consider the
    right-hand side area that has red crosses as well as blue dots; if we answer:
    No, it isn''t windy, then we need to consider all the red crosses on the left-hand
    side. For better understanding, you can refer to *Figure 8.27*:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们问一个问题：有风吗？你有两个答案：是或否。这个问题与风有关，因此我们需要关注*图 8.26*中的*x*轴。如果我们的回答是：是的，有风，那么我们应该考虑右侧区域，这里有红色的叉号和蓝色的点；如果我们回答：不，没有风，那么我们需要考虑左侧的所有红色叉号。为了更好地理解，你可以参考*图
    8.27*：
- en: '![](img/37e82d6a-66bf-4869-8bdb-1a141570a935.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37e82d6a-66bf-4869-8bdb-1a141570a935.png)'
- en: 'Figure 8.27: Representation of the question: Is it windy? (Image credit: https://classroom.udacity.com/courses/ud120)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.27：问题的表示：有风吗？（图片来源：https://classroom.udacity.com/courses/ud120）
- en: 'As you can see in *Figure 8.27*, I have put one line that passes through the
    midpoint on the *x* axis. I have just chosen a midpoint, there is no specific
    reason for that. So I have drawn a black line. Red crosses on the left-hand side
    of the line represent: No, it''s not windy, and red crosses on the right-hand
    side of the line represent: Yes, it''s windy. On the left-hand side of the line,
    there are only red crosses and not a single blue dot there. If you select the
    answer, No, then actually you traverse with the branch labeled as No. The area
    on the left side has only red crosses, so you end up having all the data points
    belonging to the same class, which is represented by red crosses, and you will
    not ask further questions for that branch of the tree. Now, if you select the
    answer, Yes, then we need to focus on the data points that are on the right-hand
    side. You can see that there are two types of data points, blue dots as well as
    red crosses. So, in order to classify them, you need to come up with a linear
    boundary in such a way that the section formed by the line has only one type of
    data point. We will achieve this by asking another question: Is it sunny? This
    time, again, you have two possible answers - Yes or No. Remember that I have traversed
    the tree branch that has the answer of our first question in the form of Yes.
    So my focus is on the right-hand side of the data points, because there I have
    data points that are represented in the form of red crosses as well as blue dots.
    We have described the sun on the *y* axis, so you need to look at that axis and
    if you draw a line that passes through the midpoint of the *y* axis, then the
    section above the line represents the answer, Yes, it is a sunny day. All data
    points below the line represent the answer, No, it is not a sunny day. When you
    draw such a line and stop extending that line after the first line, then you can
    successfully segregate the data points that reside on the right-hand side. So
    the section above the line contains only blue dots and the section below the line,
    red crosses. You can see the horizontal line, in *Figure 8.28*:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在*图 8.27*中看到的，我画了一条通过* x *轴中点的线。我只是选择了一个中点，这没有特定的理由。所以我画了一条黑线。左侧红色的叉号表示：不，有风，右侧的红色叉号表示：是的，有风。在线的左侧，只有红色的叉号，没有一个蓝色点。如果你选择了“不”这个答案，那么实际上你会遍历标有“不”标签的分支。左侧的区域只有红色叉号，没有其他数据点，因此你会得到所有数据点都属于同一类别，即红色叉号表示的类别，对于该分支，你将不会继续提问。如果你选择了“是”，那么我们需要关注右侧的数据点。你可以看到那里有两种类型的数据点，蓝色的点和红色的叉号。所以，为了对它们进行分类，你需要提出一个线性边界，使得该线分隔出的区域内只有一种类型的数据点。我们将通过提问另一个问题来实现：今天是晴天吗？这时，你又有两个可能的答案——是或否。记住，我刚才已经遍历了第一个问题答案为“是”的树枝。所以我的关注点在数据点的右侧，因为那里有红色的叉号和蓝色的点。我们已经在*
    y *轴上描述了太阳，因此你需要查看那个轴，如果你画一条通过* y *轴中点的线，那么线上的上方表示“是，今天是晴天”。线下方的所有数据点则表示“不，今天不是晴天”。当你画出这样的线并停止扩展时，你就成功地将右侧的数据点分隔开来。所以，线上的上方只包含蓝色点，线下方则是红色叉号。你可以看到*图
    8.28*中的水平线：
- en: '![](img/11926a62-a06a-4230-ae7a-8c47b3546a38.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11926a62-a06a-4230-ae7a-8c47b3546a38.png)'
- en: 'Figure 8.28: Grey line representing the classification done based on the first
    question'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.28：灰色线表示基于第一个问题进行的分类
- en: We can observe that by asking a series of questions or a series of linear questions,
    we actually classify the red crosses that represent when Chris doesn't surf and
    blue dots that represent when Chris surfs.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，通过一系列的问题或线性问题，我们实际上对表示Chris不冲浪的红色叉号和表示Chris冲浪的蓝色点进行了分类。
- en: 'This is a very basic example that can help you understand how a decision tree
    works for classification problems. Here, we have built the tree by asking a series
    of questions as well as generating multiple linear boundaries to classify the
    data points. Let''s take one numeric example so that it will be clearer to you.
    Refer to *Figure 8.29*:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常基础的示例，可以帮助你理解决策树是如何处理分类问题的。在这里，我们通过一系列问题构建了这棵树，并生成了多个线性边界来对数据点进行分类。让我们来看一个数字示例，这样会更清晰。请参见*图
    8.29*：
- en: '![](img/40caf52c-e4df-495e-8bb7-716249e3c647.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40caf52c-e4df-495e-8bb7-716249e3c647.png)'
- en: 'Figure 8.29: See the data points in the 2D graph (Image credit: https://classroom.udacity.com/courses/ud120)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.29：请看二维图中的数据点（图片来源：https://classroom.udacity.com/courses/ud120）
- en: 'You can see the given data points. Let''s start with the *x* axis first. Which
    threshold value on the *x* axis do you want to choose so that you can obtain the
    best split for these data points? Think for a minute! I would like to select a
    line that passes the *x* axis at point 3\. So now you have two sections. Mathematically,
    I choose the best split for the given data points, that is, x < = 3 and *x > 3.*
    Refer to *Figure 8.30*:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到给定的数据点。让我们首先从*x*轴开始。你希望在*x*轴上选择哪个阈值，以便为这些数据点获得最佳分割？想一想！我想选择一个通过*x*轴上3点的线。现在你有了两个区域。在数学上，我选择了给定数据点的最佳分割，即，*x
    <= 3* 和 *x > 3*。参考*图 8.30*：
- en: '![](img/bba4c6c6-f80b-45cc-a994-1986d34ace47.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bba4c6c6-f80b-45cc-a994-1986d34ace47.png)'
- en: 'Figure 8.30: See the first linear question and decision boundary graph (Image
    credit : https://classroom.udacity.com/courses/ud120)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.30：参见第一个线性问题和决策边界图（图片来源：https://classroom.udacity.com/courses/ud120）
- en: 'Let''s focus on the left-hand side section first. Which value on the *y* axis
    would you prefer to choose so that you have only one type of data point in one
    section after drawing that line? What is the threshold on *y* axis that you select
    so you have one type of dataset in one section and the other type of dataset in
    the other section? I will choose the line that passes through the point 2 on the
    *y* axis. So, the data points above the line belong to one class and the data
    points below the line belong to the other class. Mathematically, *y < = 2* gives
    you one class and *y > 2* gives you the other class. Refer to *Figure 8.31*:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先专注于左侧部分。你希望在*y*轴上选择哪个值，以便在绘制该线后一个区域中只有一种数据点？你选择的*y*轴阈值是多少，以便在一个区域中有一种数据集，而在另一个区域中有另一种数据集？我将选择通过*y*轴上的点2的线。因此，线上方的数据点属于一类，线下方的数据点属于另一类。在数学上，*y
    <= 2* 给出一类，*y > 2* 给出另一类。参考*图 8.31*：
- en: '![](img/e8db2a8b-b00f-4a4e-a6a5-379ff9433601.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8db2a8b-b00f-4a4e-a6a5-379ff9433601.png)'
- en: 'Figure 8.31: See the second linear question and decision boundary graph (Image
    credit: https://classroom.udacity.com/courses/ud120)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.31：参见第二个线性问题和决策边界图（图片来源：https://classroom.udacity.com/courses/ud120）
- en: 'Now focus on the right-hand side part; for that part also, we need to choose
    a threshold with respect to the *y* axis. Here, the best threshold for a separation
    boundary is *y = 4*, so the section *y < 4* has only red crosses and the section
    *y > =4* has only blue dots. So finally, with a series of linear questions, we
    are able to classify our data points. Refer to *Figure 8.32*:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在专注于右侧部分；对于那部分，我们也需要根据*y*轴选择一个阈值。这里，分隔边界的最佳阈值是*y = 4*，所以*y < 4*部分只有红叉，而*y >=
    4*部分只有蓝点。最终，通过一系列线性问题，我们能够对数据点进行分类。参考*图 8.32*：
- en: '![](img/16d73b3e-f2f7-4275-a62c-d49ce8739440.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16d73b3e-f2f7-4275-a62c-d49ce8739440.png)'
- en: 'Figure 8.32: The final linear question and decision boundary graph (Image credit:
    https://classroom.udacity.com/courses/ud120)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.32：最终的线性问题和决策边界图（图片来源：https://classroom.udacity.com/courses/ud120）
- en: Now you get an idea about the algorithm, but there may be a couple of questions
    in your mind. We have a visualization of the obtaining line, but how does the
    decision tree algorithm choose the best possible way to split data points and
    generate a decision boundary using the given features? Suppose I have more than
    two features, say ten features; then how will the decision tree know that it needs
    to use the second feature and not the third feature in the first time? So I'm
    going to answer all these questions by explaining the math behind a decision tree.
    We will look at an NLP-related example, so that you can see how a decision tree
    is used in NLP applications.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对算法有了一个概念，但你心中可能还有一些疑问。我们有一个获取线的可视化，但决策树算法如何选择最佳的方式来分割数据点，并使用给定的特征生成决策边界呢？假设我有超过两个特征，比如十个特征；那么决策树如何知道它需要在第一次使用第二个特征而不是第三个特征呢？因此，我将通过解释决策树背后的数学来回答所有这些问题。我们将看一个与自然语言处理相关的例子，这样你就可以看到决策树在自然语言处理应用中的使用。
- en: I have some questions related to decision trees let's answer them one by one.
    We will use visualization to obtain a linear boundary, but how does a decision
    tree recognize using which features and which feature value should it split the
    data? Let's see the mathematical term that is entropy. So, decision tree uses
    the concept of entropy to decide where to split the data. Let's understand entropy.
    Entropy is the measure of the impurity in a tree branch. So, if all data points
    in a tree branch belong to the same class, then entropy *E = 0*; otherwise, entropy
    *E > 0* and *E <= 1.* If Entropy *E = 1*, then it indicates that the tree branch
    is highly impure or data points are evenly split between all the available classes.
    Let's see an example so that you can understand the concept of entropy and impurity.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一些与决策树相关的问题，我们一个个地回答它们。我们将使用可视化来获取线性边界，但决策树是如何识别应该使用哪些特征以及哪个特征值来分裂数据的呢？让我们来看一下熵这一数学术语。所以，决策树使用熵的概念来决定在哪里分裂数据。让我们来理解熵。熵是树枝中杂质的度量。所以，如果树枝中的所有数据点都属于同一类别，那么熵
    *E = 0*；否则，熵 *E > 0* 且 *E <= 1*。如果熵 *E = 1*，则表示该树枝非常不纯，或者数据点在所有可用类别之间均匀分布。让我们看一个例子，以便你理解熵和杂质的概念。
- en: 'We are making a spam filtering application and we have one feature, that is,
    words and phrases type. Now we will introduce another feature, that is, the minimum
    threshold count of appearing phrases in the dataset. Refer to *Figure 8.33*:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在制作一个垃圾邮件过滤应用程序，并且有一个特征，那就是词汇和短语类型。现在我们将引入另一个特征，即数据集中出现短语的最小阈值计数。请参见*图 8.33*：
- en: '![](img/5af94fd1-bd66-4e30-a027-c6fa89f9f409.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5af94fd1-bd66-4e30-a027-c6fa89f9f409.png)'
- en: 'Figure 8.33: Graphs for entropy discussion'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.33：熵讨论的图表
- en: Now focus on the right-hand side graph. In this graph, the right-hand side section
    has only one kind of data points that are denoted by red crosses. So technically,
    all data points are homogenous as they belong to the same class. So, there is
    no impurity and the value of entropy is approximately zero. Now if you focus on
    the left-hand side graph and see its right-hand side section, you will find data
    points that belong to the other class label. This section has impurity and, thus,
    has high entropy. So, during the implementation of a decision tree, you need to
    find out the variables that can be used to define the split points, along with
    the variables. Another thing that you need to keep in mind is that you are trying
    to minimize the impurity in the data, so try to split the data according to that.
    We will see how to choose variables to perform a split in some time.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，聚焦右侧的图表。在这个图表中，右侧部分只有一种类型的数据点，使用红色叉号表示。严格来说，所有数据点都是同质的，因为它们属于同一类别。因此，没有杂质，熵的值大约为零。如果你聚焦于左侧图表并观察其右侧部分，你会发现这些数据点属于其他类别标签。这个部分有杂质，因此熵较高。所以，在实现决策树时，你需要找出可以用来定义分裂点的变量以及这些变量。另一个需要记住的事情是，你需要尽量减少数据中的杂质，因此应根据这个原则来进行数据分裂。稍后我们将看到如何选择变量来进行分裂。
- en: 'Now, let''s first see the mathematical formula of entropy. Refer to *Figure
    8.34*:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们首先来看一下熵的数学公式。请参见*图 8.34*：
- en: '![](img/1948c592-b74a-4ff4-bb10-25b4c5adf312.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1948c592-b74a-4ff4-bb10-25b4c5adf312.png)'
- en: 'Figure 8.34: Entropy mathematical formula (Image credit: http://dni-institute.in/blogs/wp-content/uploads/2015/08/Entrop.png)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.34：熵的数学公式（图片来源：http://dni-institute.in/blogs/wp-content/uploads/2015/08/Entrop.png）
- en: 'Let''s see what *pi* is here. It is the fraction value for a given class. Let''s
    say *i* is the class. *T* is the total value of the available class. You have
    four data points; if two points belong to class A and the other two belong to
    class B, then *T = 2*. We perform summation after generating log values using
    the fraction value. Now it''s time to perform mathematical calculations for entropy
    and then I will let you know how we can use entropy to perform splitting on variables
    or features. Let''s see an example to calculate entropy. You can find the data
    for the example in *Figure 8.35*:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一下这里的 *pi* 是什么。它是给定类别的比例值。假设 *i* 是类别，*T* 是可用类别的总值。假设你有四个数据点，如果其中两个点属于类别A，另外两个属于类别B，那么
    *T = 2*。我们通过使用比例值生成对数值后进行求和。现在是时候进行熵的数学计算了，接下来我将告诉你如何使用熵来对变量或特征进行分裂。我们来看一个计算熵的例子。你可以在*图
    8.35*中找到该例子的相关数据：
- en: '![](img/b0b448b9-bca8-424f-b001-303d77cd6f3a.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0b448b9-bca8-424f-b001-303d77cd6f3a.png)'
- en: 'Figure 8.35: The dataset values for spam filtering calculation'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.35：垃圾邮件过滤计算的数据显示集
- en: 'If you focus on the **Filtering** column, you have two labels with the value
    **Spam** and two values with **Ham**-that is *SSHH*. Now answer some of the following
    questions:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你专注于**Filtering**列，你会看到有两个标签值为**Spam**，两个标签值为**Ham**——即*SSHH*。现在请回答以下问题：
- en: How many total data rows do we have? The answer is four
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有多少行数据？答案是四行
- en: How many times does the data label *S* occur in the **Filtering** column? The
    answer is two
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据标签*S*在**Filtering**列中出现了多少次？答案是两次
- en: How many times does the data label *H* occur in the **Filtering** column? The
    answer is two
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据标签*H*在**Filtering**列中出现了多少次？答案是两次
- en: 'To generate the fraction value for the class label *S*, you need to perform
    mathematics using the following formula:'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了生成类标签*S*的分数值，你需要使用以下公式进行计算：
- en: '*pS = No. of time S occurred / Total no. of data rows = 2/4 = 0.5*'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*pS = S出现的次数 / 数据总行数 = 2/4 = 0.5*'
- en: 'Now we need to calculate *p* for *H* as well:'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们还需要计算*S*的*p*值：
- en: '*pH = No. of time H occurred / Total no. of data rows = 2/4 = 0.5*'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*pH = H出现的次数 / 数据总行数 = 2/4 = 0.5*'
- en: 'Now we have all the necessary values to generate the entropy. Focus on the
    formula given in *Figure 8.34*:'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了所有生成熵所需的值。请关注*图 8.34*中的公式：
- en: '*Entropy = -pS* log[2](pS) -pH*log[2](pH) = -0.5 * log(0.5) -0.5*log(0.5) =
    1.0*'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*熵 = -pS* log[2](pS) -pH*log[2](pH) = -0.5 * log(0.5) -0.5*log(0.5) = 1.0*'
- en: You can do this calculation using Python's math module
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用Python的math模块来进行这个计算
- en: As you can see, we get the entropy *E = 1*. This is the most impure state, where
    data is evenly distributed among the available classes. So, entropy tells us about
    the state of the data whether classes are in an impure state or not.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们得到的熵值是*E = 1*。这是最不纯的状态，其中数据在可用类别中均匀分布。因此，熵值告诉我们数据的状态是否为不纯状态。
- en: 'Now we will look at the most awaited question: How will we know on which variable
    or using which feature we need to perform a split? To understand this, we need
    to understand information gain. This is one of the core concepts of the decision
    tree algorithm. Let me introduce the formula for **information gain** (**IG**):'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看最受期待的问题：我们怎么知道在哪个变量上，或者使用哪个特征进行分割呢？要理解这一点，我们需要了解信息增益。这是决策树算法的核心概念之一。让我介绍**信息增益**（**IG**）的公式：
- en: '*Information Gain (IG) = Entropy (Parent Node) - [Weight Average] Entropy (Children)*'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*信息增益（IG） = 父节点熵 - [加权平均] 子节点熵*'
- en: Now let's take a look at this formula. We are calculating the entropy of the
    parent node and subtracting the weighted entropy of the children. If we perform
    splitting on the parent node, decision tree will try to maximize the information
    gain. Using IG, the decision tree will choose the feature that we need to perform
    a split on. This calculation is done for all the available features, so the decision
    tree knows exactly where to split. You need to refer to *Figure 8.33*.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下这个公式。我们正在计算父节点的熵，并减去子节点的加权熵。如果我们对父节点进行分割，决策树会尝试最大化信息增益。通过使用IG，决策树会选择我们需要进行分割的特征。这个计算会对所有可用的特征进行，所以决策树会准确知道在哪个位置进行分割。你需要参考*图
    8.33*。
- en: 'We have calculated entropy for the parent node: *E (Parent Node) = 1*. Now
    we will focus on words and calculate IG. Let''s check whether we should perform
    a split using words with IG. Here, we are focusing on the **Words** column. So,
    let me answer some of the questions so that you understand the calculations of
    IG:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经计算了父节点的熵：*E (父节点) = 1*。现在我们将专注于单词，并计算信息增益（IG）。让我们检查是否应该使用IG通过单词进行分割。在这里，我们专注于**Words**列。那么，让我回答一些问题，以便你理解IG的计算：
- en: There are how many total positive meaning words? The answer is three
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总共有多少个正面意义的单词？答案是三个
- en: There are how many total negative meaning words? The answer is one
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总共有多少个负面意义的单词？答案是一个
- en: So, for this branch, our entropy *E = 0*. We will use this when we are calculating
    weighted average entropy for the child node
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所以，对于这个分支，我们的熵值是*E = 0*。我们将在计算子节点的加权平均熵时使用这个值。
- en: 'Here, our decision tree looks as given in *Figure 8.36*:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们的决策树如*图 8.36*所示：
- en: '![](img/a6ef5228-ffa6-4b55-aa2b-f6dbcb7a99d8.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6ef5228-ffa6-4b55-aa2b-f6dbcb7a99d8.png)'
- en: 'Figure 8.36: Decision trees first iteration'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.36：决策树的第一次迭代
- en: 'You can see that for the right-hand side node, the entropy is zero so there
    isn''t any impurity in that branch, so we can stop there. However, if you look
    at the left-hand side node, it has the **SSH** class so we need to calculate entropy
    for each of the class labels. Let''s do it step by step for the left-hand side
    node:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '*PS* = No. Of *S* label in branch/ Total no. of example in branch = 2/3'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PH* = No. Of *H* label in branch/ Total no. of example in branch = 1/3'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now entropy *E* = -2/3 log[2] (2/3) -1/3 log[2] (1/3) = 0.918
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next step, we need to calculate the weighted average entropy of the child
    nodes.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'We have three data points as part of the left-hand branch and one data point
    as the right-hand branch, which you can see in *Figure 8.36*. So the values and
    formula looks as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '*Weight average entropy of children = Left hand branch data points / Total
    no of data points * ( entropy for children in that branch) + Right hand branch
    data points / Total no of data points * ( entropy for children in that branch)*'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '*Weight average entropy of children = [Weight Average] Entropy (Children) =¾
    * 0.918 + ¼ * (0) = 0.6885*'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to obtain IG:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '*IG = Entropy (parent node) - [Weight Average] Entropy (Children). We have
    both parts with us-E (Parent Node) = 1* and *[Weight Average] Entropy (Children)
    = 0.6885*'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the final calculation is as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '*IG = 1 - 0.6885 = 0.3115*'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Let's focus on the phrase appeared count column and calculate the entropy for
    phrase count values three, which is *E[three(3)] = 1.0*, entropy for phrase count
    values four is *E[four(4)] = 1.0*; now *[Weight Average] Entropy (Children) =
    1.0 , IG = 1.0 -1.0 = 0*. So, we are not getting any information gain on this
    split of this feature. So, we should not choose this feature.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s focus on the column of phrases where we have mentioned the phrase
    category-**Unusual phrases** or **Usual phrases**. When we split data points using
    this column, we get the **Spam** class in one branch and **Ham** class in another
    branch. So here, you need to calculate IG on your own, but the *IG = 1*. We are
    getting the maximum IG. So we will choose this feature for the split. You can
    see the decision tree in *Figure 8.37*:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/927ac31c-22af-4554-b383-1f0caa5416de.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.37: Decision tree generated using phrases type feature'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: If you have a high number of features, then the decision tree performs training
    very slowly because it calculates IG for each feature and performs a split by
    choosing the feature that provides maximum IG.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to look at the NLP application that uses decision trees. We will
    redevelop spam filtering but this time, we will use a decision tree.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to just change the algorithm for the spam-filtering application and
    we have taken the same feature set that we generated previously so that you can
    compare the result of logistic regression and decision tree for spam filtering.
    Here, we will use the same features that are generated by the `CountVectorizer`
    API from `scikit-learn`. The code is at this GitHub link:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要更改算法来实现垃圾邮件过滤应用程序，并且我们采用了之前生成的相同特征集，这样你就可以比较逻辑回归和决策树在垃圾邮件过滤中的效果。这里，我们将使用`scikit-learn`中的`CountVectorizer`
    API生成的相同特征。代码请见这个GitHub链接：
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch8/Spamflteringapplication/Spam_filtering_logistic_regression.ipynb](https://github.com/jalajthanaki/NLPython/blob/master/ch8/Spamflteringapplication/Spam_filtering_logistic_regression.ipynb)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/blob/master/ch8/Spamflteringapplication/Spam_filtering_logistic_regression.ipynb](https://github.com/jalajthanaki/NLPython/blob/master/ch8/Spamflteringapplication/Spam_filtering_logistic_regression.ipynb)'
- en: 'You can see the code snippet in *Figure 8.38*:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图8.38*中看到代码片段：
- en: '![](img/81a7e45f-f385-464a-b08e-550d8ae22c57.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/81a7e45f-f385-464a-b08e-550d8ae22c57.png)'
- en: 'Figure 8.38: Spam filtering using decision tree'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.38：使用决策树进行垃圾邮件过滤
- en: 'You can see the output in *Figure 8.39*:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图8.39*中看到输出：
- en: '![](img/f09257b9-7c00-4984-aa3d-a390ffd1ea9b.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f09257b9-7c00-4984-aa3d-a390ffd1ea9b.png)'
- en: 'Figure 8.39: Spam filtering output using decision tree'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.39：使用决策树进行垃圾邮件过滤输出
- en: As you can see, we get low accuracy compared to logistic regression. Now it's
    time to see some tuning parameters that you can use to improve the accuracy of
    the ML-model.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们得到的准确度比逻辑回归低。现在是时候看看一些可以用来提高ML模型准确度的调节参数了。
- en: '**Tunable parameters**'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '**可调参数**'
- en: 'In this section, I will explain some tunable `scikit-learn`. You can check
    the documentation at:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将解释一些可调节的`scikit-learn`。你可以在以下链接查看文档：
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier).'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier).'
- en: 'I will explain the following parameters that you can use:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我将解释你可以使用的以下参数：
- en: There is one parameter in `scikit-learn`, which is `criterion`. You can set
    it as either `entropy` or `gini`. The `entropy` or `gini` are used to calculate
    IG. So they both have a similar mechanism to calculate IG, and decision tree will
    perform the split on the basis of the IG calculation given by `entropy` or `gini`.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`中有一个参数叫做`criterion`。你可以将其设置为`entropy`或`gini`。`entropy`或`gini`用于计算信息增益（IG）。因此，它们都有相似的机制来计算IG，决策树将根据`entropy`或`gini`计算得出的IG来进行分裂。'
- en: There is `min_sample_size` and its default value is `two`. So, the decision
    tree branch will split until it has more than or equal to two data elements per
    branch. Sometimes, decision tree tries to fit maximum training data and overfits
    the training data points. To prevent overfitting, you need to increase the `min_sample_size`
    from two to more like fifty or sixty.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个`min_sample_size`参数，默认值为`two`。因此，决策树分支会一直分裂，直到每个分支上有两个或更多的数据点。有时，决策树会尝试拟合最大量的训练数据，从而导致过拟合训练数据点。为了防止过拟合，你需要将`min_sample_size`从两个增加到五十或六十。
- en: We can use tree pruning techniques, for which we will follow the bottom-up approach.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用树剪枝技术，采用自底向上的方法。
- en: Now let's see the advantages and disadvantages of decision tree.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看决策树的优缺点。
- en: '**Advantages of decision tree**'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树的优点**'
- en: 'The following are the advantages that decision tree provide:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是决策树提供的优点：
- en: Decision tree is simple and easy to develop
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树简单且易于开发
- en: Decision tree can be interpreted by humans easily and it's a white box algorithm
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树易于被人类解读，并且是一种白盒算法
- en: It helps us determine the worst, best, and expected values for different scenarios
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它帮助我们确定不同场景下的最差、最好和预期值
- en: '**Disadvantages of decision tree**'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树的缺点**'
- en: 'The following are the disadvantages that decision tree has:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是决策树的缺点：
- en: If you have a lot of features, then decision tree may have overfitting issues
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有很多特征，那么决策树可能会出现过拟合问题
- en: You need to be careful about the parameters that you are passing while training
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练时需要小心你传递的参数
- en: We have seen the shortcomings of decision tree. Decision tree generally overfits
    the training dataset. We need to solve the problem using parameter tuning or a
    variant of the decision tree random forest ML algorithm. We will understand the
    random forest algorithm next.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '**Random forest**'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is a variant of decision tree that solves the overfitting problem.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Random forest is capable of developing linear regression as well as classification
    tasks. Here, we are focusing on a classification task. It uses a very simple trick
    and works very nicely. The trick is that random forest uses a voting mechanism
    to improve the accuracy of the test result.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: The random forest algorithm generates a random subset of the data from the training
    dataset and uses this to generate a decision tree for each of the subsets of the
    data. All these generated trees are called **random forest**. Now let's understand
    the voting mechanism. Once we have generated decision trees, we check the class
    label that each tree is provided for a specific data point. Suppose that we have
    generated three random forest decision trees. Two of them are saying some specific
    data point belongs to class A and the third decision tree predicts that the specific
    data point belongs to class B. The algorithm considers the higher vote and assigns
    the class label A for that specific data point.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'For random forest, all the calculations for classification are similar to a
    decision tree. As promised, I will refer to the example that I gave in [Chapter
    5](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml), *Feature Engineering and NLP Algorithms*,
    which is the custom POS tagger example. In that example, we used a decision tree.
    See the code on this GitHub link:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/CustomPOStagger/ownpostag.py](https://github.com/jalajthanaki/NLPython/blob/master/ch5/CustomPOStagger/ownpostag.py).'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit the example and understand the features and code given in *figure
    8.40*:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d00b6c77-8030-4936-b319-24f36575b253.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.40: Code snippet for the decision tree algorithm in scikit-learn'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '**Advantages of random forest**'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the advantages of random forest:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: It helps us prevent overfitting
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used for regression as well as classification
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disadvantages of random forest**'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the disadvantages of random forest:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: The random forest model can easily grow, which means that if the random subset
    of datasets is high, then we will get more decision trees, thus, we will get a
    group of trees, also referred as a forest of decision trees that may take a lot
    of memory
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For high-dimensional feature space, it is hard to interpret each node of the
    tree, especially when you have a high number of trees in one forest
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now it's time to understand our next ML algorithm - Naive Bayes.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '**Naive Bayes**'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will understand the probabilistic ML algorithm that is used
    heavily in many data science applications. We will use this algorithm to develop
    the most famous NLP application - sentiment analysis, but before jumping into
    the application, we will understand how the Naive Bayes algorithm works. So, let's
    begin!
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes ML algorithm is based on Bayes theorem. According to this theorem,
    our most important assumption is that events are independent, which is a naive
    assumption, and that is the reason this algorithm is called **Naive Bayes**. So,
    let me give you an idea of independent events. In classification tasks, we have
    many features. If we use the Naive Bayes algorithm, then we assume that each and
    every feature that we are going to provide to the classifier is independent from
    each other, which means that the presence of a particular feature of the class
    doesn't affect any other feature. Let's take an example. You want to find out
    the sentiment of the sentence, It is very good! You have features such as bag
    of words, adjective phrase, and so on. Even if all these features depend on each
    other or depend on the existence of other features, all the properties carried
    by these features independently contribute to the probability that this sentence
    carries positive sentiment. This is the reason we call this algorithm Naive.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is really simple, as well as a very powerful one. This works
    really well if you have a lot of data. It can classify more than two classes,
    so it is helpful in building a multi-class classifier. So, now, let's look at
    some points that will tell us how the Naive Bayes algorithm works. Let's understand
    the mathematics and probabilistic theorem behind it.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: We will understand Bayes rule first. In very simple language, you have prior
    probability for some event, and you find some evidence of the same event in your
    test data and multiply them. Then you get the posterior probability that helps
    you derive your final predication. Don't worry about the terminology, we will
    get into those details.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me give you an equation first and then we will take one example so that
    you know what the calculation that we need to do is. See the equation in *Figure
    8.41*:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e2eddae-ddd1-4ea5-9b33-efe75abc2a5c.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.41: Naive-Bayes algorithm uses Bayes theorem equation (Image credit:
    http://www.saedsayad.com/images/Bayes_rule.png)'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Here, *P(c|x)* is the probability of class *c*, class *c* is the target, and
    *x* are the features or data attributes. *P(C)* is the prior probability of class
    *c*, *P(x|c)* is the estimation of the likelihood that is the probability of the
    predictor given a target class, and *P(x)* is the prior probability of the predictor.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Let's use this equation to calculate an example. Suppose there is a medical
    test that helps identify whether a person has cancer or not. The prior probability
    of the person having that specific type of cancer is only *1 %*, which means *P(c)
    = 0.01 = 1%* and so *P(not c) =0.99 =99%*. There is a *90%* chance that the test
    will show positive if the person has cancer. So prior probability of *P(Positive
    result | c )* *= 0.9 = 90%* and there is a *10%* chance that even if the person
    doesn't have cancer, the result will still show positive, so *P(Positive result
    | not C )* *= 0.1 =10%.*
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to check whether the person really has cancer. If the result showed
    positive, that probability is written as *P(c | Positive result)*, and if the
    person doesn''t have cancer but still the result is positive, then that is denoted
    by *P( not c | Positive result)*. We need to calculate these two probabilities
    to derive the posterior probability. First, we need to calculate joint probability:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '*Joint P(c | Positive result) = P(c) * P(Positive result | c) = 0.01 x 0.9
    =0.009*'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '*Joint P(not c | Positive result) = P(not c) * P(Positive result | not c) =
    0.99 x 0.1 = 0.099*'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding probability is called **joint probability**. This will be helpful
    in deriving the final posterior probability. To get the posterior probability,
    we need to apply normalization:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '*P (Positive result) = P(c | Positive result) + P ( not c | Positive result)
    = 0.009 +0.099 = 0.108*'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the actual posterior probability is given as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '*Posterior probability of P(c | Positive result) = joint probability of P(c
    | Positive result) / P (Positive result) = 0.009 / 0.108 = 0.083*'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '*Posterior probability of P( not c | Positive result) = joint probability of
    P(not c | Positive result) / P (Positive result) = 0.099 / 0.108 = 0.916*'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: If you sum up the Posterior probability of *P(c | Positive result) + Posterior
    probability of P(not c | Positive result),* it should be *= 1.* And in this case,
    it does sum up to *1*.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a lot of mathematics going on, so I will draw a diagram for you that
    will help you understand these things. Refer to *Figure 8.42*:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89cf14d6-153e-402b-8161-ac72a18d8a60.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.42: Posterior probability calculation diagram (Image credit: https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30144285350923)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: We will extend this concept to an NLP application. Here, we will take an NLP-based
    basic example. Suppose there are two persons - Chris and Sara. We have the e-mail
    details of Chris and Sara. They both use words such as life, love, and deal. For
    simplicity, we are considering only three words. They both use these three words
    at different frequencies.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Chris uses the word love only 1% of the time in his mail, whereas he uses the
    word deal 80% of the time, and life 1% of the time. Sara, on the other hand, uses
    the word love 50% of the time, deal 20% of the time, and life 30 % of the time.
    If we have a new e-mail, then we need to decide whether it is written by Chris
    or Sara. Prior probability of P(Chris) = 0.5 and P(Sara) = 0.5.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'The mail has the sentence Life Deal so the probability calculation is for P(Chris|
    "Life Deal") = P(life) * P(deal) * P(Chris) = 0.04 and the calculation for P(Sara
    |"Life Deal") = P(life) * P(deal) * P(Sara) = 0.03\. Now, let''s apply normalization
    and generate the actual probability. For this, we need to calculate joint probability
    = P(Chris| "Life Deal") + P(Sara | "Life Deal") = 0.07\. The following are the
    actual probability values:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: P(Chris| "Life Deal") = 0.04 / 0.07 = 0.57
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: P(Sara| "Life Deal") = 0.03 / 0.07 = 0.43
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: The sentence Life Deal is more likely to be written by Chris. This is the end
    of the example and now it's time for practical implementation. Here, we are developing
    the most famous NLP application, that is, sentiment analysis. We will do sentiment
    analysis for text data so we can say that sentiment analysis is the text analysis
    of opinions that are generated by humans. Sentiment analysis helps us analyze
    what our customers are thinking about a certain product or event.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'For sentiment analysis, we will use the bag-of-words approach. You can also
    use artificial neural networks, but I''m explaining a basic and easy approach.
    You can see the code on this GitHub link:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch8/sentimentanalysis/sentimentanalysis_NB.py](https://github.com/jalajthanaki/NLPython/blob/master/ch8/sentimentanalysis/sentimentanalysis_NB.py)'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `TfidVectorizer` API of `scikit-learn` as well as `MultinomialNB`
    Naive Bayes. See the code snippet in *Figure 8.43*:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1226199b-9110-4614-a57e-d0aefa33743f.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.43: Code snippet for sentiment analysis using Naive Bayes'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'See the output in *Figure 8.44*:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1936b2e0-c390-4f1c-a9b7-b6b7b8b3b5a4.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.44: Output for sentiment analysis using Naive Bayes'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to look at some tuning parameters.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '**Tunable parameters**'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: For this algorithm, sometimes you need to apply smoothing. Now, what do I mean
    by smoothing? Let me give you a very brief idea about it. Some of the words come
    in the training data and our algorithm uses that data to generate an ML-model.
    If the ML-model sees the words that are not in the training data but present in
    the testing data, then at that time, our algorithm cannot predict things well.
    We need to solve this situation. So, as a solution, we need to apply smoothing,
    which means that we are also calculating the probability for rare words and that
    is the tunable parameter in `scikit-learn`. It's just a flag--if you enable it,
    it will perform smoothing or if you disable it, then smoothing will not be applied.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '**Advantages of Naive Bayes**'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the advantages that the Naive Bayes algorithm provides:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: You can deal with high-dimensional feature space using the Naive Bayes algorithm
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used to classify more than two classes
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disadvantages of Naive Bayes**'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the disadvantages of the Naive Bayes algorithm:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: If you have a phrase composed of different words having different meanings,
    then this algorithm will not help you. You have a phrase, Gujarat Lions. This
    is the name of a cricket team, but Gujarat is a state in India and lion is an
    animal. So, the Naive Bayes algorithm takes the individual words and interprets
    them separately, and so this algorithm cannot correctly interpret Gujarat Lions.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If some categorical data appears in the testing dataset only and not in the
    training data, then Naive Bayes won''t provide a prediction for that. So, to solve
    this kind of problem, we need to apply smoothing techniques. You can read about
    this on this link:'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://stats.stackexchange.com/questions/108797/in-naive-bayes-why-bother-with-laplacian-smoothing-when-we-have-unknown-words-i](https://stats.stackexchange.com/questions/108797/in-naive-bayes-why-bother-with-laplacian-smoothing-when-we-have-unknown-words-i)'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now it is time to look at the last classification algorithm, support vector
    machine. So, let's begin!
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support vector machine**'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: This is the last but not least supervised ML algorithm that we will look in
    this chapter. It is called **support vector machine** (**SVM**). This algorithm
    is used for classification tasks as well as regression tasks. This algorithm is
    also used for multi-class classification tasks.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: SVM takes labeled data and tries to classify the data points by separating them
    using a line that is called the **hyperplane**. The goal is to obtain an optimal
    hyperplane that will be used to categorize the existing as well as new, unseen
    examples. How to obtain an optimal hyperplane is what we are going to understand
    here.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the term optimal hyperplane first. We need to obtain the
    hyperplane in such a way that the obtained hyperplane maximizes the distances
    to its nearest points of all the classes and this distance is called **margin**.
    Here, we will talk about a binary classifier. Margin is the distance between the
    hyperplane (or line) and the nearest point of either of the two classes. SVM tries
    to maximize the margin. Refer to *Figure 8.45*:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd699229-166a-4da7-bcb8-ed8928c9fd38.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.45: SVM classifier basic image (Image credit: http://docs.opencv.org/2.4/_images/optimal-hyperplane.png)'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: In the given figure, there are three lines *a*, *b*, and *c*. Now, choose the
    line that you think best separates the data points. I would pick line *b* because
    it maximizes the margin from two classes and the other lines *a* and *c* don't
    do that.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Note that SVM first tries to perform the classification task perfectly and then
    tries to maximize the margin. So for SVM, performing the classification task correctly
    is the first priority. SVM can obtain the linear hyperplane as well as generate
    a non-linear hyperplane. So, let's understand the math behind this.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have n features, then using SVM, you can draw *n-1* dimensional hyperplane.
    If you have a two-dimensional feature space, then you can draw a hyperplane that
    is one-dimensional. If you have a three-dimensional feature space, then you can
    draw a two-dimensional hyperplane. In any ML algorithm, we actually try minimizing
    our loss function, so we first define the loss function for SVM. SVM uses the
    hinge loss function. We use this loss function and try to minimize our loss and
    obtain the maximum margin for our hyperplane. The hinge loss function equation
    is given as follows:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '*C (x, y, f(x)) = (1 - y * f(x))[+]*'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *x* is sample data points, *y* is true label, *f(x)* is the predicted
    label, and *C* is the loss function. What the *+* sign in the equation denotes
    is, when we calculate *y*f(x)* and it comes *> = 1*, then we try to subtract it
    from *1* and get a negative value. We don''t want this, and so to denote that,
    we put the *+* sign:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0cb54ce8-ce2b-4da9-892a-44b19176c8c1.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
- en: 'Now it''s time to define the objective function that takes the loss function,
    as well as a lambda term called a **regularization term**. We will see what it
    does for us. However, it is also a tuning parameter. See the mathematics equation
    in F*igure 8.46*:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b01145c-e3f4-4c4a-967c-aefc6dbe41e3.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.46: Objective function with regularization term lambda'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'SVM has two tuning parameters that we need to take care of. One of the terms
    is the lambda that denotes the regularization term. If the regularization term
    is too high, then our ML-model overfits and cannot generalize the unseen data.
    If it is too low, then it underfits and we get a huge training error. So, we need
    a precise value for the regularization term as well. We need to take care of the
    regularization term that helps us prevent overfitting and we need to minimize
    our loss. So, we take a partial derivative for both of these terms; the following
    are the derivatives for the regularization term and loss function that we can
    use to perform gradient descent so that we can minimize our loss and get an accurate
    regularization value. See the partial derivative equation in *Figure 8.47*:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae9b1450-205a-4a82-a91a-580ee5b89769.jpg)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.47: Partial derivative for regularization term'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 'See the partial derivative for the loss function in *Figure 8.48*:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9bf34f5-7be6-480d-9f3a-c4f8ac58c711.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.48: Partial derivative for loss function'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to calculate the values of the partial derivative and update the weight
    accordingly. If we misclassify the data point, then we need to use the following
    equation to update the weight. Refer to F*igure 8.49*:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/378e7ef1-0c0b-4f48-9170-d8252ee97f27.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.49: Misclassification condition'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: So if *y* is *< 1*, then we need to use the following equation in *Figure 8.50:*
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0279fae5-b9b4-4f0c-95d5-ce34c9420519.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.50: Weight update rule using this equation for the misclassification
    condition'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Here, the long n shape is called eta and it denotes the learning rate. Learning
    rate is a tuning parameter that shows you how fast your algorithm should run.
    This also needs an accurate value because, if it is too high, then the training
    will complete too fast and the algorithm will miss the global minimum. On the
    other hand, if it is too slow, then it will take too much time to train and may
    never converge at all.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: If misclassification happens, then we need to update our loss function as well
    as the regularization term.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what if the algorithm correctly classifies the data point? In this case,
    we don''t need to update the loss function; we just need to update our regularization
    parameter that you can see using the equation given in *Figure 8.51*:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd3e2dcd-ca70-4b8d-a2e3-d3a39b860fe6.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.51: Updating the weight for regularization'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: When we have an appropriate value of regularization and the global minima, then
    we can classify all the points in SVM; at that time, the margin value also becomes
    the maximum.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to use SVM for a non-linear classifier, then you need to apply
    the kernel trick. Briefly, we can say that the kernel trick is all about converting
    lower feature space into higher feature space that introduces the non-linear attributes
    so that you can classify the dataset. See the example in *Figure 8.52*:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3dc8b638-a228-4517-b19b-78c5d4961dc9.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.52: Non-linear SVM example'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: To classify this data, we have **X**, **Y** feature. We introduce the new non-linear
    feature, *X² + Y²,* which helps us draw a hyperplane that can classify the data
    correctly.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'So, now it''s time to implement the SVM algorithm and we will develop the sentiment
    analysis application again, but this time, I''m using SVM and seeing what the
    difference is in the accuracy. You can find the code on this GitHub link:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch8/sentimentanalysis/sentimentanalysis_SVM.py](https://github.com/jalajthanaki/NLPython/blob/master/ch8/sentimentanalysis/sentimentanalysis_SVM.py)'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the code snippet in *Figure 8.53*:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a70fbec9-193c-46a6-8830-64cc0e84aece.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.53: Sentiment analysis using SVM'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the output in *Figure 8.54*:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c885237b-74fb-470f-808a-254e1479cf8d.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.54: Output of SVM'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to look at some tuning parameters. Let's take a look!
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '**Tunable parameters**'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check out some of the SVM tuning parameters that can help us:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn` provides a tuning parameter for the kernel trick that is very
    useful. You can use various types of kernels, such as linear, rbf, and so on.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other parameters called **C** and **gamma**.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C controls the trade-off between a smooth decision boundary and classifying
    the training points correctly. A large value of C gives you more correct training
    points.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamma can be useful if you are trying to set your margin. If you set high values
    for the gamma, then only nearby data points are taken into account to draw a decision
    boundary, and if you have low values for gamma, then points that are far from
    the decision boundary are also taken into account to measure whether the decision
    boundary maximizes the margin or not.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now it's time to look at the advantages and disadvantages of SVM.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '**Advantages of SVM**'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the advantages that the SVM algorithm provides to us:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: It performs well for complicated datasets
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used for a multiclass classifier
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disadvantages of SVM**'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the disadvantages of the SVM algorithm:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: It will not perform well when you have a very large dataset because it takes
    a lot of training time
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will not work effectively when the data is too noisy
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the end of the supervised ML algorithms. You learned a lot of math and
    concepts, and if you want to explore more, then you can try out the following
    exercise.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You need to explore **K-Nearest Neighbor** (**KNN**) and its application in
    the NLP domain
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to explore AdaBoost and its application in the NLP domain
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have covered a lot of cool classification techniques used in NLP and converted
    the black box ML algorithms to white box. So, now you know what is happening inside
    the algorithms. We have developed NLP applications as well, so now this is the
    time to jump into unsupervised ML.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised ML
  id: totrans-450
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is another type of machine learning algorithm. When we don't have any labeled
    data, then we can use unsupervised machine learning algorithms. In the NLP domain,
    there is a common situation where you can't find the labeled dataset, then this
    type of ML algorithm comes to our rescue.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will discuss the unsupervised ML algorithm called K-means clustering.
    This algorithm has many applications. Google has used this kind of unsupervised
    learning algorithms for so many of their products. YouTube video suggestions use
    the clustering algorithm.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image will give you an idea of how data points are represented
    in unsupervised ML algorithms. Refer to F*igure 8.55*:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b20b3aa2-e7f2-4b45-854e-e03feb44867d.png)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.55: A general representation of data points in an unsupervised ML
    algorithm'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 8.55*, the data points don't have a label associated
    with them, but visually, you can see that they form some groups or clusters. We
    will actually try to figure out the structure in the data using unsupervised ML
    algorithms so that we can derive some fruitful insight for unseen data points.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will look at the k-means clustering algorithm and develop the document
    classification example that is related to the NLP domain. So, let's begin!
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss the k-means clustering algorithm. We will get
    an understanding of the algorithm first. k-means clustering uses the iterative
    refinement technique.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand some of the basics about the k-means algorithm. k refers to
    how many clusters we want to generate. Now, you can choose a random point and
    put the centroid at this point. The number of centroids in k-means clustering
    is not more than the value of k, which means not more than the cluster value k.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: 'This algorithm has the two following steps that we need to reiterate:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to assign the centroid.
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second step is to calculate the optimization step.
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To understand the steps of the k-means, we will look at an example. Before
    that, I would like to recommend that you check out this animated image that will
    give you a lot of understanding about k-means:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch8/K_means_clustering/K-means_convergence.gif](https://github.com/jalajthanaki/NLPython/blob/master/ch8/K_means_clustering/K-means_convergence.gif)[.](https://github.com/jalajthanaki/NLPython/blob/master/ch8/K_means_clustering/K-means_convergence.gif)'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take an example. You have five data points, which are given in
    the table, and we want to group these data points into two clusters, so *k = 2*.
    Refer to *Figure 8.56*:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5cc4774c-4e6d-419b-a363-650d16e86ea0.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.56: k-mean clustering data points for calculation'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: We have chosen point *A(1,1)* and point *C(0,2)* for the assignment of our centroid.
    This is the end of the assign step, now let's understand the optimization step.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: 'We will calculate the Euclidean distance from every point to this centroid.
    The equation for the Euclidean distance is given in *Figure 8.57*:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ecf9d67-81c4-424a-b0aa-81da3dc1f3f7.jpg)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.57: Euclidean distance for the k-means clustering algorithm'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: Every time, we need to calculate the Euclidean distance from both centroids.
    Let's check the calculation. The starting centroid mean is *C1 = (1,1)* and *C2
    = (0,2)*. Here, we want to make two cluster that is the reason we take two centroids.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 1**'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: 'For point *A = (1,1)*:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = (1,1)* so *ED = Square root ((1-1)² + (1-1)²) = 0*'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = (0,2)* so *ED = Square root ((1-0)² + (1-2)²) = 1.41*'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: Here, *C1 < C2*, so point *A* belongs to cluster 1.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: 'For point *B = (1,0)*:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = (1,1)* so *ED = Square root ((1-1)² + (0-1)²) = 1*'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = (0,2)* so *ED = Square root ((1-0)² + (0-2)²) = 2.23*'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: Here, *C1 < C2*, so point *B* belongs to cluster 1.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: 'For point *C = (0,2)*:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = (1,1)* so *ED = Square root ((0-1)² + (2-1)²) = 1.41*'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = (0,2)* so *ED = Square root ((0-0)² + (2-2)²) = 0*'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: Here, *C1 > C2*, so point *C* belongs to cluster 2.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: 'For point *D = (2,4)*:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = (1,1)* so *ED = Square root ((2-1)² + (4-1)²) = 3.16*'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = (0,2)* so *ED = Square root ((2-0)² + (4-2)²) = 2.82*'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: Here, *C1 > C2*, so point *C* belongs to cluster 2.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: 'For point *E = (3,5)*:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = (1,1)* so *ED = Square root ((3-1)² + (5-1)²)= 4.47*'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = (0,2)* so *ED = Square root ((3-0)² + (5-2)²)= 4.24*'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: Here, *C1 > C2*, so point *C* belongs to cluster 2.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: 'After the first iteration, our cluster looks as follows. Cluster *C1* has points
    *A* and *B*, and *C2* has points *C*, *D*, and *E*. So, here, we need to calculate
    the centroid mean value again, as per the new cluster point:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = XA + XB / 2 = (1+1) / 2 = 1*'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = YA + YB / 2 = (1+0) / 2 = 0.5*'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: So new *C1 = (1,0.5)*
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = Xc + XD + XE / 3 = (0+2+3) / 3 = 1.66*'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = Yc +YD + YE / 3 = (2+4+5) / 3 = 3.66*'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: So new *C2 = (1.66,3.66)*
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: We need to do all the calculations again in the same way as *Iteration 1*. So
    we get the values as follows.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 2**'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: 'For point *A = (1,1)*:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = (1,0.5)* so *ED = Square root ((1-1)² + (1-0.5)²) = 0.5*'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = (1.66,3.66)* so *ED = Square root ((1-1.66)² + (1-3.66)²) = 2.78*'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: Here, *C1 < C2*, so point *A* belongs to cluster 1.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: 'For point *B = (1,0)*:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = (1,0.5)* so *ED = Square root ((1-1)² + (0-0.5)²) = 1*'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = (1.66,3.66)* so *ED = Square root ((1-1.66)² + (0-3.66)²) = 3.76*'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: Here, *C1 < C2*, so point *B* belongs to cluster 1.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: 'For point *C = (0,2)*:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = (1,0.5)* so *ED = Square root ((0-1)² + (2-0.5)²)= 1.8*'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = (1.66, 3.66)* so *ED = Square root ((0-1.66)² + (2-3.66)²)= 2.4*'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: Here, *C1 < C2*, so point *C* belongs to cluster 1.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: 'For point *D = (2,4)*:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = (1,0.5)* so *ED = Square root ((2-1)² + (4-0.5)²)= 3.6*'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = (1.66,3.66)* so *ED = Square root ((2-1.66)² + (4-3.66)²)= 0.5*'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: Here, *C1 > C2*, so point *C* belongs to cluster 2.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: 'For point *E = (3,5)*:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = (1,0.5)* so *ED = Square root ((3-1)² + (5-0.5)²) = 4.9*'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = (1.66,3.66)* so *ED = Square root ((3-1.66)² + (5-3.66)²) = 1.9*'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: Here, *C1 > C2*, so point *C* belongs to cluster 2.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: 'After the second iteration, our cluster looks as follows. *C1* has points *A*,
    *B*, and *C*, and *C2* has points *D* and *E*:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = XA + XB + Xc / 3 = (1+1+0) / 3 = 0.7*'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: '*C1 = YA + YB + Yc / 3 = (1+0+2 ) / 3 = 1*'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: So new *C1 = (0.7,1)*
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = XD + XE / 2 = (2+3) / 2 = 2.5*'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: '*C2 = YD + YE / 2 = (4+5) / 2 = 4.5*'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: So new *C2 = (2.5,4.5)*
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: We need to do iterations until the clusters don't change. So this is the reason
    why this algorithm is called an **iterative algorithm**. This is the intuition
    of the K-means clustering algorithm. Now we will look at a practical example in
    the document classification application.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: Document clustering
  id: totrans-532
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Document clustering helps you with a recommendation system. Suppose you have
    a lot of research papers and you don't have tags for them. You can use the k-means
    clustering algorithm, which can help you to form clusters as per the words appearing
    in the documents. You can build an application that is news categorization. All
    news from the same category should be combined together; you have a superset category,
    such as sports news, and this sports news category contains news about cricket,
    football, and so on.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will categorize movies into five different genres. The code credit
    goes to Brandon Rose. You can check out the code on this GitHub link:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch8/K_means_clustering/K-mean_clustering.ipynb](https://github.com/jalajthanaki/NLPython/blob/master/ch8/K_means_clustering/K-mean_clustering.ipynb).'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: 'See the code snippet in *Figure 8.58*:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/664dc2ba-fce4-4862-a062-af382d9c57d4.png)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.58: A short code snippet of the K-means algorithm'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: 'See the output in F*igure 8.59*:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80fbf996-3b27-41fd-972c-a9d442b44c18.png)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.59: Output of k-means clustering'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to this link for hierarchical clustering:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: '[http://brandonrose.org/clustering](http://brandonrose.org/clustering).'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of k-means clustering
  id: totrans-544
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These are the advantages that k-means clustering provides us with:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: It is a very simple algorithm for an NLP application
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It solves the main problem as it doesn't need tagged data or result labels,
    you can use this algorithm for untagged data
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disadvantages of k-means clustering
  id: totrans-548
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These are the disadvantages of k-means clustering:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: Initialization of the cluster center is a really crucial part. Suppose you have
    three clusters and you put two centroids in the same cluster and the other one
    in the last cluster. Somehow, k-means clustering minimizes the Euclidean distance
    for all the data points in the cluster and it will become stable, so actually,
    there are two centroids in one cluster and the third one has one centroid. In
    this case, you end up having only two clusters. This is called the **local minimum**
    problem in clustering.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the end of the unsupervised learning algorithms. Here, you have learned
    about the k-means clustering algorithm and developed the document classification
    application. If you want to learn more about this technique, try out the exercise.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  id: totrans-552
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You need to explore hierarchical clustering and its application in the NLP domain.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: Our next section is very interesting. We will look at semi-supervised machine
    learning techniques. Here, we will get an overview of them. So, let's understand
    these techniques.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised ML
  id: totrans-555
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Semi-supervised ML** or **semi-supervised learning** (**SSL**) is basically
    used when you have a training dataset that has a target concept or target label
    for some data in the dataset, and the other part of the data doesn''t have any
    label. If you have this kind of dataset, then you can apply semi-supervised ML
    algorithms. When we have a very small amount of labeled data and a lot of unlabeled
    data, then we can use semi-supervised techniques. If you want to build an NLP
    tool for any local language (apart from English) and you have a very small amount
    of labeled data, then you can use the semi-supervised approach. In this approach,
    we will use a classifier that uses the labeled data and generates an ML-model.
    This ML-model is used to generate labels for the unlabeled dataset. The classifiers
    are used for high-confidence predictions on the unlabeled dataset. You can use
    any appropriate classifier algorithm to classify the labeled data.'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: 'Semi-supervised techniques are a major research area, especially for NLP applications.
    Last year, Google Research developed semi supervised techniques that are graph-based:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: '[https://research.googleblog.com/2016/10/graph-powered-machine-learning-at-google.html](https://research.googleblog.com/2016/10/graph-powered-machine-learning-at-google.html).'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: 'For better understanding, you can also read some of the really interesting
    stuff given here:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: '[https://medium.com/@jrodthoughts/google-expander-and-the-emergence-of-semi-supervised-learning-1919592bfc49](https://medium.com/@jrodthoughts/google-expander-and-the-emergence-of-semi-supervised-learning-1919592bfc49).'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/ftp/arxiv/papers/1511/1511.06833.pdf](https://arxiv.org/ftp/arxiv/papers/1511/1511.06833.pdf).'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.aclweb.org/anthology/W09-2208](http://www.aclweb.org/anthology/W09-2208).'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: '[http://cogprints.org/5859/1/Thesis-David-Nadeau.pdf](http://cogprints.org/5859/1/Thesis-David-Nadeau.pdf).'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.cmpe.boun.edu.tr/~ozgur/papers/896_Paper.pdf](https://www.cmpe.boun.edu.tr/~ozgur/papers/896_Paper.pdf).'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: '[http://graph-ssl.wdfiles.com/local--files/blog%3A_start/graph_ssl_acl12_tutorial_slides_final.pdf](http://graph-ssl.wdfiles.com/local--files/blog%3A_start/graph_ssl_acl12_tutorial_slides_final.pdf).'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: Those interested in research can develop novel SSL techniques for any NLP application.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: Now we have completed our ML algorithms section. There are some critical points
    that we need to understand. Now it's time to explore these important concepts.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: Other important concepts
  id: totrans-568
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at those concepts that help us know how the training
    on our dataset using ML algorithms is going, how you should judge whether the
    generated ML-model will be able to generalize unseen scenarios or not, and what
    signs tell you that your ML-model can't generalize the unseen scenarios properly.
    Once you detect these situations, what steps should you take? What are the widely
    used evaluation matrices for NLP applications?
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s find answers to all these questions. I''m going to cover the following
    topics. We will look at all of them one by one:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: Bias-variance trade-off
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Underfitting
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation matrix
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias-variance trade-off
  id: totrans-575
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will look at a high-level idea about the bias-variance trade-off. Let's
    understand each term one by one.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: Let's first understand the term bias. When you are performing training using
    an ML algorithm and you see that your generated ML-model doesn't perform differently
    with respect to your first round of training iteration, then you can immediately
    recognize that the ML algorithm has a high bias. In this situation, ML algorithms
    have no capacity to learn from the given data so it's not learning new things
    that you expect your ML algorithm to learn. If your algorithm has very high bias,
    then eventually it just stops learning. Suppose you are building a sentiment analysis
    application and you have come up with the ML-model. Now you are not happy with
    the ML-model's accuracy and you want to improve the model. You will train by adding
    some new features and changing some algorithmic parameters. Now this newly generated
    model will not perform well or perform differently on the testing data, which
    is an indication for you that you may have high bias. Your ML algorithm won't
    converge in the expected way so that you can improve the ML-model result.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand the second term, variance. So, you use any ML algorithm to
    train your model and you observe that you get very good training accuracy. However,
    you apply the same ML-model to generate the output for an unseen testing dataset
    and your ML-model doesn't work well. This situation, where you have very good
    training accuracy and the ML-model doesn't turn out well for the unseen data,
    is called a **high variance** situation. So, here, the ML-model can only replicate
    the predictions or output that it has seen in the training data and doesn't have
    enough bias that it can generalize the unseen situation. In other words, you can
    say that your ML algorithm is trying to remember each of the training examples
    and, at the end, it just mimics that output on your testing dataset. If you have
    a high variance problem, then your model converges in such a way that it tries
    to classify each and every example of the dataset in a certain category. This
    situation leads us to overfitting. I will explain what overfitting is, so don't
    worry! We will be there in a few minutes.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: To overcome both of the preceding bad situations, we really need something that
    lies in the middle, which means no high bias and no high variance. The art of
    generating the most bias and best variance for ML algorithms leads to the best
    optimal ML-Model. Your ML-model may not be perfect, but it's all about generating
    the best bias-variance trade-off.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn the concepts of *Underfitting* and *Overfitting*
    as well as tricks that help you get rid of these high bias and high variance scenarios.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting
  id: totrans-581
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss the term underfitting. What is underfitting
    and how is it related to the bias-variance trade-off?
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you train the data using any ML algorithm and you get a high training
    error. Refer to *Figure 8.60*:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8bffe6-9815-44f7-aac3-69da10bd7ebd.png)'
  id: totrans-584
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.60: Graph indicating a high training error (Image credit: http://www.learnopencv.com/wp-content/uploads/2017/02/bias-variance-tradeoff.png)'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding situation, where we get a very high training error, is called
    **underfitting**. ML algorithms just can''t perform well on the training data.
    Now, instead of a linear decision boundary, we will try a higher degree of polynomials.
    Refer to *Figure 8.61*:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdd3ffb3-49e1-438b-9d58-0f287f2df8a3.png)'
  id: totrans-587
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.61: High bias situation (Image credit: http://www.learnopencv.com/wp-content/uploads/2017/02/bias-variance-tradeoff.png)'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: This graph has a very squiggly line and it cannot do well on the training data.
    In other words, you can say that it is performing the same as per the previous
    iteration. This shows that the ML-model has a high bias and doesn't learn new
    things.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  id: totrans-590
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at the term overfitting. I put this term in front
    of you when I was explaining variance in the last section. So, it's time to explain
    overfitting and, in order to explain it, I want to take an example.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a dataset and we plot all the data points on a two dimensional
    plane. Now we are trying to classify the data and our ML algorithm draws a decision
    boundary in order to classify the data. You can see *Figure 8.62*:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/711503bc-3aa2-4e84-a68f-76edf05d2df4.png)'
  id: totrans-593
  prefs: []
  type: TYPE_IMG
- en: Figure 8:62 Overfitting and variance
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: '(Image credit: http://www.learnopencv.com/wp-content/uploads/2017/02/bias-variance-tradeoff-test-error.png)'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the left-hand side graph, then you will see the linear line used
    as a decision boundary. Now, this graph shows a training error, so you tune the
    parameter in your second iteration and you will get really good training accuracy;
    see the right-hand side graph. You hope that you will get good testing accuracy
    as well for the testing data, but the ML-model does really bad on the testing
    data prediction. So this situation, where an algorithm has very good training
    accuracy but doesn't perform well on the testing data, is called **overfitting**.
    This is the situation where ML-models have high variance and cannot generalize
    the unseen data.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have seen underfitting and overfitting, there are some rules of
    thumb that will help you so that you can prevent these situations. Always break
    your training data into three parts:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: 60% of the dataset should be considered as training dataset
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20% of the dataset should be considered as validation dataset or development
    dataset, which will be useful in getting intermediate accuracy for your ML algorithm
    so that you can capture the unexpected stuff and change your algorithm according
    to this
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20% of the dataset should be held out to just report the final accuracy and
    this will be the testing dataset
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should also apply k-fold cross validation. k indicates how many times you
    need the validation. Suppose we set it to three. We divide our training data into
    three equal parts. In the first timestamp of the training algorithm, we use two
    parts and test on a single part so technically, it will train on 66.66% and will
    be tested on 33.34%. Then, in the second timestamp, the ML algorithm uses one
    part and performs testing on two parts, and at the last timestamp, it will use
    the entire dataset for training as well as testing. After three timestamps, we
    will calculate the average error to find out the best model. Generally, for a
    reasonable amount of the dataset, k should be taken as 10.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: You cannot have 100% accuracy for ML-models and the main reason behind this
    is because there is some noise in your input data that you can't really remove,
    which is called **irreducible error**.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the final equation for error in an ML algorithm is as follows:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: '*Total error = Bias + Variance + Irreducible Error*'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: 'You really can''t get rid of the irreducible error, so you should concentrate
    on bias and variance. Refer to *Figure 8.63*, which will be useful in showing
    you how to handle the bias and variance trade-off:'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78840f2e-bd6c-4f8c-b3ad-da68d8db83cc.png)'
  id: totrans-606
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.63: Steps to get rid of high bias or high variance situations (Image
    credit: http://www.learnopencv.com/wp-content/uploads/2017/02/Machine-Learning-Workflow.png)'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen enough about ML, let's look at the evaluation matrix,
    that is quite useful.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation matrix
  id: totrans-609
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our code, we check the accuracy but we really don't understand which attributes
    play a major part when you evaluate an ML-model. So, here, we will consider a
    matrix that is widely used for NLP applications.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: 'This evaluation matrix is called **F1 score** or **F-measure**. It has three
    main components; before this, let''s cover some terminology:'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: '**True positive** (**TP**): This is a data point labeled as A by the classifier
    and is from class A in reality.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative** (**TN**): This is an appropriate rejection from any class
    in the classifier, which means that the classifier won''t classify data points
    into class A randomly, but will reject the wrong label.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive** (**FP**): This is called a **type-I error** as well. Let''s
    understand this measure by way of an example: A person gives blood for a cancer
    test. He actually doesn''t have cancer but his test result is positive. This is
    called FP.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative** (**FN**): This is called a **type-II error** as well. Let''s
    understand this measure by way of an example: A person gives blood for a cancer
    test. He has cancer but his test result is negative. So it actually overlooks
    the class labels. This is called FN.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: Precision is a measure of exactness; what percentage of data
    points the classifier labeled as positive and are actually positive:'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*precision=TP / TP + FP*'
  id: totrans-617
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Recall**: Recall is the measure of completeness; what percentage of positive
    data points did the classifier label as positive:'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recall = TP / TP + FN*'
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**F measure**: This is nothing but the weighed measure of precision and recall.
    See the equation:'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F= 2 * precision * recall / precision + recall
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Apart from this, you can use a confusion matrix to know each of the TP, TN,
    FP, and FN. You can use the area under an ROC curve that indicates how much your
    classifier is capable of discriminating between negative and positive classes.
    *ROC = 1.0* represents that the model predicted all the classes correctly; area
    of 0.5 represents that a model is just making random predictions.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: If you want to explore the new terms and techniques, you can do the following
    exercise.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  id: totrans-624
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read about undersampling and oversampling techniques.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to understand how we can improvise our model after the first iteration,
    and sometimes, feature engineering helps us a lot in this. In [Chapter 5](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml),
    *Feature Engineering and NLP Algorithms* and [Chapter 6](c4861b9e-2bcf-4fce-94d4-f1e2010831de.xhtml),
    *Advance Feature Engineering and NLP Algorithms,* we explained how to extract
    features from text data using various NLP concepts and statistical concepts as
    part of feature engineering. Feature engineering includes feature extraction and
    feature selection. Now it's time to explore the techniques that are a part of
    feature selection. Feature extraction and feature selection give us the most important
    features for our NLP application. Once we have these features set, you can use
    various ML algorithms to generate the final outcome.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: Let's start understanding the feature selection part.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  id: totrans-628
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I mentioned earlier, feature extraction and feature selection are a part
    of feature engineering, and in this section, we will look at feature selection.
    You might wonder why we are learning feature selection, but there are certain
    reasons for it and we will look at each of them. First, we will see basic understanding
    of feature selection.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: Features selection is also called variable selection, attribute selection, or
    variable subset selection. Features selection is the process of selecting the
    best relevant features, variables, or data attributes that can help us develop
    more efficient machine learning models. If you can identify which features contribute
    a lot and which contribute less, you can select the most important features and
    remove other less important ones.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: Just take a step back and first understand what the problems are that we are
    trying to solve using features selection.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: 'Using features selection techniques, we can get the following benefits:'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the relevant and appropriate features will help you simplify your
    ML-model. This will help you interpret the ML-Model easily, as well as reduce
    the complexity of the ML-Model.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing appropriate features using feature selection techniques will help us
    improve our ML-Models accuracy.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection helps the machine learning algorithms train faster.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection also prevents overfitting.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It helps us get rid of the curse of dimensionality.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Curse of dimensionality
  id: totrans-638
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's understand what I mean by the curse of dimensionality because this concept
    will help us understand why we need feature selection techniques. The curse of
    dimensionality says that, as the number of features or dimensions grows, which
    means adding new features to our machine learning algorithm, then the amount of
    data that we need to generalize accurately grows exponentially. Let's see with
    an example.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you have a line, one-dimensional feature space, and we put five points
    on that line. You can see that each point takes some space on this line. Each
    point takes one-fifth of the space on the line. Refer to *Figure 8.64*:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80a6664c-574a-4a71-98a0-c9950f2a8371.png)'
  id: totrans-641
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.64: A one-dimensional features space with five data points'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have two-dimensional feature space, then we need more than five data
    points to fill up this space. So, we need 25 data points for these two dimensions.
    Now each point is taking up 1/25 of the space. See *Figure 8.65*:'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56867430-d6f1-4c6c-9adb-50da1672a48a.png)'
  id: totrans-644
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.65: A two-dimensional features space with 25 data points'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have a three-dimensional feature space, which means that we have three
    features, then we need to fill up the cube. You can see this in *Figure 8.66*:'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/356be119-7aef-4f17-bf71-1fcab63a3243.png)'
  id: totrans-647
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.66: A three-dimensional features space with 1/25 data points'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: However, to fill up the cube, you need exactly 125 data points, as you can see
    in *Figure 8.66* (assume there are 125 points). So every time we add features,
    we need more data. I guess you will all agree that the growth in data points increases
    exponentially from 5, 25, 125, and so on. So, in general, you need *Xd* feature
    space, where *X* is your number of data points in training and *d* is the number
    of features or dimensions. If you just blindly put more and more features so that
    your ML algorithm gets a better understanding of the dataset, what you're actually
    doing is forcing your ML algorithm to fill the larger features space with data.
    You can solve this using a simple method. In this kind of situation, you need
    to give more data to your algorithm, not features.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: Now you really think I'm restricting you to adding new features. So, let me
    clarify this for you. If it is necessary to add features, then you can; you just
    need to select the best and minimum amount of features that help your ML algorithm
    learn from it. I really recommend that you don't add too many features blindly.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: Now, how can we derive the best features set? What is the best features set
    for the particular application that I'm building and how can I know that my ML
    algorithm will perform well with this features set? I will provide the answers
    to all these questions in the next section *Features selection techniques*. Here,
    I will give you a basic idea about feature selection techniques. I would recommend
    that you practically implement them in the NLP applications that we have developed
    so far.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection techniques
  id: totrans-652
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Make everything as simple as possible but not simpler
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: 'This quote by Albert Einstein, is very true when we are talking about feature
    selection techniques. We have seen that to get rid of the curse of dimensionality,
    we need feature selection techniques. We will look at the following feature selection
    techniques:'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: Filter method
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrapper method
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedded method
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let's begin with each method.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter method**'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature selection is altogether a separate activity and independent of the
    ML algorithm. For a numerical dataset, this method is generally used when we are
    preprocessing our data, and for the NLP domain, it should be performed once we
    convert text data to a numerical format or vector format. Let''s first see the
    basic steps of this method in *Figure 8.67*:'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2290d45b-3fa3-45d5-bb44-2a9cb6240912.png)'
  id: totrans-661
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.67: Filter method for feature selection (Image credit: https://upload.wikimedia.org/wikipedia/commons/2/2c/Filter_Methode.png)'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps are very clear and self-explanatory. Here, we use statistical techniques
    that give us a score and based on this, we will decide whether we should keep
    the feature or just remove it or drop it. Refer to *Figure 8.68*:'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66d34e05-93ba-47b1-b49e-b622806ab67c.png)'
  id: totrans-664
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.68: Features selection techniques list'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me simplify *Figure 8.68* for you:'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: If feature and response are both continuous, then we will perform correlation
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If feature and response are both are categorical, then we will use Chi-Square;
    in NLP (we mostly use this)
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If feature are continuous and response is categorical, then we will use **linear
    discriminant analysis** (**LDA**)
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If feature are categorical and response is continuous, then we will use Anova
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I will concentrate more on the NLP domain and explain the basics of LDA and
    Chi-Square.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: LDA is generally used to find a linear combination of features that characterize
    or separate more than one class of a categorical variable, whereas Chi-Square
    is mainly used in NLP as compared to LDA. Chi-Square is applied to a group of
    categorical features to get an idea of the likelihood of correlation or association
    between features using their frequency distribution.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: '**Wrapper method**'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: 'In this method, we are searching for the best features set. This method is
    computationally very expensive because we need to search for the best features
    subset for every iteration. See the basic steps in *Figure 8.69*:'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc0c4985-6bf8-4ea4-84d2-2d6bacddddea.png)'
  id: totrans-675
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.69: Wrapper method steps (Image credit: https://upload.wikimedia.org/wikipedia/commons/0/04/Feature_selection_Wrapper_Method.png)'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three submethods that we can use to select the best features subset:'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: Forward selection
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backward selection
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursive features elimination
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In forward selection, we start with no features and add the features that improve
    our ML-model, in each iteration. We continue this process until our model does
    not improve its accuracy further.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: Backward selection is the other method where we start with all the features
    and, in each iteration, we find the best features and remove other unnecessary
    features, and repeat until no further improvement is observed in the ML-model.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: Recursive feature elimination uses a greedy approach to find out the best performing
    features subset. It repeatedly creates the models and keeps aside the best or
    worst performing features for each iteration. The next time, it uses the best
    features and creates the model until all features are exhausted; lastly, it ranks
    the features based on its order of elimination of them.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: '**Embedded method**'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: 'In this method, we combine the qualities of the filter and wrapper methods.
    This method is implemented by the algorithms that have their own built-in feature
    selection methods. Refer to F*igure 8.70*:'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e92eddc2-b32a-42c2-ac6a-b510690f9140.png)'
  id: totrans-686
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.70: Embedded features selection method (Image credit: https://upload.wikimedia.org/wikipedia/commons/b/bf/Feature_selection_Embedded_Method.png)'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: The most popular examples of these methods are LASSO and RIDGE regression, which
    has some built-in parameters to reduce the chances of overfitting.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to these links, which will be very useful for you:'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/).'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
- en: '[http://machinelearningmastery.com/an-introduction-to-feature-selection/](http://machinelearningmastery.com/an-introduction-to-feature-selection/).'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: '[http://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/](http://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/).'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
- en: We will look at dimensionality reduction in the next section.
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  id: totrans-694
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimensionality reduction is a very useful concept in machine learning. If we
    include a lot of features to develop our ML-model, then sometimes we include features
    that are really not needed. Sometimes we need high-dimensional features space.
    What are the available ways to make certain sense about our features space? So
    we need some techniques that help us remove unnecessary features or convert our
    high-dimensional features space to two-dimensional or three-dimensional features
    so that we can see what all is happening. By the way, we have used this concept
    in [Chapter 6](c4861b9e-2bcf-4fce-94d4-f1e2010831de.xhtml), *Advance Features
    Engineering and NLP* *Algorithms*, when we developed an application that generated
    word2vec for the game of thrones dataset. At that time, we used **t-distributed
    stochastic neighbor embedding** (**t-SNE**) dimensionality reduction technique
    to visualize our result in two-dimensional space.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will look at the most famous two techniques, called **principal component
    analysis** (**PCA**) and t-SNE, which is used to visualize high-dimensional data
    in two-dimensional space. So, let's begin.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
- en: '**PCA**'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a statistical method that uses an orthogonal transformation to convert
    a set of data points of possibly correlated features to a set of values of linearly
    uncorrelated features, called **principal components**. The number of principal
    components is less than or equal to the number of the original features. This
    technique defines transformation in such a way that the first principal component
    has the largest possible variance to each succeeding feature.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to *Figure 8.71*:'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7686ca9-493b-4208-a31f-14e82363e109.png)'
  id: totrans-700
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.71: PCA (Image credit: https://www.nature.com/article-assets/npg/nbt/journal/v26/n3/images/nbt0308-303-F1.gif)'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
- en: This graph helps a lot in order to understand PCA. We have taken two principal
    components and they are orthogonal to each other as well as making variance as
    large as possible. In *c* graph, we have reduced the dimension from two-dimensional
    to one-dimensional by projecting on a single line.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: The disadvantage of PCA is that when you reduce the dimensionality, it loses
    the meaning that the data points represent. If interpretability is the main reason
    for dimensionality reduction, then you should not use PCA; you can use t-SNE.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: '**t-SNE**'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the technique that helps us visualize high-dimensional non-linear space.
    t-SNE tries to preserve the group of local data points that are close together.
    This technique will help you when you want to visualize high-dimensional space.
    You can use this to visualize applications that use techniques such as word2vec,
    image classification, and so on. For more information, you can refer to this link:'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: '[https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/).'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid approaches for NLP applications
  id: totrans-707
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hybrid approaches sometimes really help us improve the result, of our NLP applications.
    For example, if we are developing a grammar correction system, a module that identifies
    multiword expressions such as kick the bucket, and a rule-based module that identifies
    the wrong pattern and generates the right pattern. This is one kind of hybrid
    approach. Let's take a second example for the same NLP application. You are making
    a classifier that identifies the correct articles (determiners - a, an, and the)
    for the noun phrase in a sentence. In this system, you can take two categories
    - a/an and the. We need to develop a classifier that will generate the determiner
    category, either a/an or the. Once we generate the articles for the noun phrase,
    we can apply a rule-based system that further decides the actual determiner for
    the first category a/an. We also know some English grammar rules that we can use
    to decide whether we should go with a or an. This is also an example a of hybrid
    approach. For better sentiment analysis, we can also use hybrid approaches that
    include lexical-based approach, ML-based approach, or word2vec or GloVe pretrained
    models to get really high accuracy. So, be creative and understand your NLP problem
    so that you can take advantages from different types of techniques to make your
    NLP application better.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: Post-processing
  id: totrans-709
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Post processing is a kind of rule-based system. Suppose you are developing a
    machine translation application and your generated model makes some specific mistakes.
    You want that **machine translation** (**MT**) model to avoid these kinds of mistakes,
    but avoiding that takes a lot of features that make the training process slow
    and make the model too complex. On the other hand, if you know that there are
    certain straightforward rules or approximations that can help you once the output
    has been generated in order to make it more accurate, then we can use post-processing
    for our MT model. What is the difference between a hybrid model and post-processing?
    Let me clear your confusion. In the given example, I have used word approximation.
    So rather than using rules, you can also apply an approximation, such as applying
    a threshold value to tune your result, but you should apply approximation only
    when you know that it will give an accurate result. This approximation should
    complement the NLP system to be generalized enough.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-711
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have looked at the basic concepts of ML, as well as the
    various classification algorithms that are used in the NLP domain. In NLP, we
    mostly use classification algorithms, as compared to linear regression. We have
    seen some really cool examples such as spam filtering, sentiment analysis, and
    so on. We also revisited the POS tagger example to provide you with better understanding.
    We looked at unsupervised ML algorithms and important concepts such as bias-variance
    trade-off, underfitting, overfitting, evaluation matrix, and so on. We also understood
    features selection and dimensionality reduction. We touched on hybrid ML approaches
    and post-processing as well. So, in this chapter, we have mostly understood how
    to develop and fine-tune NLP applications.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see a new era of machine learning--deep learning.
    We will explore the basic concepts needed for AI. After that, we will discuss
    the basics of deep learning including linear regression and gradient descent.
    We will see why deep learning has become the most popular technique in the past
    few years. We will see the necessary concepts of math that are related to deep
    learning, explore the architecture of deep neural networks, and develop some cool
    applications such as machine translation from the NLU domain and text summarization
    from the NLG domain. We will do this using TensorFlow, Keras, and some other latest
    dependencies. We will also see basic optimization techniques that you can apply
    to traditional ML algorithms and deep learning algorithms. Let's dive deep into
    the deep learning world in the next chapter!
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
