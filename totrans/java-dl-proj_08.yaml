- en: Distributed Deep Learning – Video Classification Using Convolutional LSTM Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how to develop deep-learning-based projects on numerals
    and images. However, applying similar techniques to video clips, for example,
    for human activity recognition from video, is not straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will see how to apply deep learning approaches to a video
    dataset. We will describe how to process and extract features from a large collection
    of video clips. Then we will make the overall pipeline scalable and faster by
    distributing the training on multiple devices (CPUs and GPUs), and run them in
    parallel.
  prefs: []
  type: TYPE_NORMAL
- en: We will see a complete example of how to develop a deep learning application
    that accurately classifies a large collection of a video dataset, such as UCF101
    dataset, using a combined CNN and LSTM network with **Deeplearning4j** (**DL4J**).
    This overcomes the limitation of standalone CNN or RNN **Long Short-Term Memory**
    (**LSTM**) networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training will be carried out on an Amazon EC2 GPU compute cluster. Eventually,
    this end-to-end project can be treated as a primer for human activity recognition
    from a video or so. Concisely, we will learn the following topics throughout an
    end-to-end project:'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed deep learning across multiple GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset collection and description
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a video classifier using a convolutional-LSTM network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequently asked questions (FAQs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed deep learning across multiple GPUs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As stated earlier, we will see a systematic example for classifying a large
    collection of video clips from the `UCF101` dataset using a convolutional-LSTM
    network. However, first we need to know how to distribute the training across
    multiple GPUs. In previous chapters, we discussed several advanced techniques
    such as network weight initialization, batch normalization, faster optimizers,
    proper activation functions, etc. these certainly help the network to converge
    faster. However, still, training a large neural network on a single machine can
    take days or even weeks. Therefore, this is not a viable way for working with
    large-scale datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Theoretically, there are two main methods for the distributed training of neural
    networks: data parallelism and model parallelism. DL4J relies on data parallelism
    called distributed deep learning with parameter averaging. Nevertheless, multimedia
    analytics typically makes things even more complicated, since, from a single video
    clip, we can see thousands of frames and images, and so on. To get rid of this
    issue, we will first distribute computations across multiple devices on just one
    machine, and then do it on multiple devices across multiple machines as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecd11ffd-5ef5-47fa-8155-0910f3566db2.png)'
  prefs: []
  type: TYPE_IMG
- en: Executing a DL4J Java application across multiple devices in parallel
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you can typically train a neural network just as fast using eight
    GPUs on a single machine rather than 16 GPUs across multiple machines. The reason
    is simple—the extra delay imposed by network communications in a multi-machine
    setup. The following diagram shows how to configure DL4J that uses CUDA and cuDNN
    to control GPUs and boost DNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd8e6fc1-224d-4706-95e7-ec824a1046ce.png)'
  prefs: []
  type: TYPE_IMG
- en: DL4J uses CUDA and cuDNN to control GPUs and boost DNNs
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training on GPUs with DL4J
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DL4J works on distributed GPUs as well as on native (that is, ones with CPU
    backend). It allows users to run locally on a single GPU, such as the Nvidia Tesla,
    Titan, or GeForce GTX, and in the cloud on Nvidia GRID GPUs. We can also perform
    the training on an Amazon AWS EC2 GPU cluster,by having multiple GPUs installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train a neural network on GPUs, you need to make some changes to the `pom.xml`
    file in your root directory, such as properties and dependency management for
    pulling down the required dependencies provided by the DL4j team. First, we take
    care of the project properties, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding `<properties>` tag, as the entries explain, we will be using
    DL4J 1.0.0-alpha version with CUDA 9.0 platform as the backend. In addition, we
    plan to use Java 8\. Nonetheless, an additional property for `logback` is defined.
  prefs: []
  type: TYPE_NORMAL
- en: Logback is intended as a successor to the popular log4j project, picking up
    where log4j left off. Logback's architecture is sufficiently generic so as to
    apply under different circumstances. Presently, logback is divided into three
    modules, logback-core, logback-classic and logback-access. For more information,
    readers should refer to [https://logback.qos.ch/](https://logback.qos.ch/).
  prefs: []
  type: TYPE_NORMAL
- en: 'I am assuming you have already configured CUDA and cuDNN and set the path accordingly.
    Once we have defined the project properties, the next important task would be
    to define GPU-related dependencies, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Where ND4J is the numerical computing engine that powers DL4J and acts as the
    backends, or different types of hardware that it works on. If your system has
    multiple GPUs installed, you can train your model in data-parallel mode, which
    is called **multi-GPU data parallelism**. DL4J provides a simple wrapper that
    can be instantiated, something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'A more concrete example can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`ParallelWrapper` takes your existing model as the primary argument and does
    training in parallel by keeping the number of workers equal to or higher than
    the number of GPUs on your machine.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Within `ParallelWrapper`, the initial model will be duplicated, and each worker
    will be training its own model. After every *N* iterations in `averagingFrequency(X)`,
    all models will be averaged, and training continues. Now, to use this functionality,
    use the following dependency in the `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For more up-to-date documentation, interested readers can check out the following
    link: [https://deeplearning4j.org/gpu](https://deeplearning4j.org/gpu).'
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a theoretical understanding of how to distribute DL-based training
    across multiple GPUs. We will see a hands-on example soon in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Video classification using convolutional – LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will start combining convolutional, max pooling, dense,
    and recurrent layers to classify each frame of a video clip. Specifically, each
    video contains several human activities, which persist for multiple frames (though
    they move between frames) and may leave the frame. First, let's get a more detailed
    description of the dataset we will be using for this project.
  prefs: []
  type: TYPE_NORMAL
- en: UCF101 – action recognition dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`UCF101` is an action recognition dataset of realistic action videos, collected
    from YouTube and having 101 action categories covering 13,320 videos. The videos
    are collected with variations in camera motion, object appearance and pose, object
    scale, viewpoint, cluttered background, and illumination condition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The videos in 101 action categories are further clustered into 25 groups (clips
    in each group have common features, for example, background and viewpoint) having
    four to seven videos of an action in each group. There are five action categories:
    human-object interaction, body-motion only, human-human interaction, playing musical
    instruments, and dports.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A few more facts about this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`UCF101` videos contain different frame lengths, ranging between 100 and 300
    frames per video clip'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UCF101` uses the `XVID` compression standard (that is, `.avi` format)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `UCF101` dataset has picture size of 320 x 240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `UCF101` dataset contains different classes in different video files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A high-level glimpse of the dataset can be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a80f2a12-0bd4-42d6-bcf6-42a54b8d1869.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Some random clips from the `UCF50` dataset (source: [http://crcv.ucf.edu/data/UCF50.php](http://crcv.ucf.edu/data/UCF50.php))'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing and feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processing video files is a very challenging task. Especially when it comes
    to reading video clips by handling and interoperating different encodings; this
    is a tedious job. Also, video clips may contain distorted frames, which is an
    obstacle when extracting high-quality features.
  prefs: []
  type: TYPE_NORMAL
- en: Considering these, in this subsection, we will see how to preprocess video clips
    by dealing with the video encoding problem, and then we will describe the feature
    extraction process in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the encoding problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dealing with video data in Java is a troublesome job (given that we don't have
    many libraries like Python), especially if the videos come in old `.avi` or such
    formats. I have seen some blogs and examples on GitHub using JCodec Java library
    Version 0.1.5 (or 0.2.3) to read and parse `UCF101` video clips in an MP4 format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even DL4J depends on datavec-data-codec, which depends on old JCodec API and
    is incompatible with the new version. Unfortunately, even this newer version of
    JCodec cannot read `UCF101` videos. Therefore, I decided to use the FFmpeg to
    process the video in MP4 format. This comes under the JavaCV library, which I''ve
    discussed already in an earlier chapter. Anyway, to use this library, just include
    the following dependency in the `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As the `UCF101` comes in `.avi` format, I had a hard time processing them with
    either JCodec or FFfmpeg libraries in Java. Therefore, I converted the video to
    `MP4` format in a handcrafted way.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, I wrote a Python script (named `prepare.py`, which can be found under
    this chapter''s code repository). This Python does download, extract, and decode
    the full `UCF101` dataset, but it may take several hours, depending upon the hardware
    config and Internet speed. Although putting Python code is not relevant to this
    book, I still put it so that folks can get some idea of how the thing works, so
    take a look at this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As this code shows, you just need to download the `UCF101` dataset from [http://crcv.ucf.edu/data/UCF101.php](http://crcv.ucf.edu/data/UCF101.php)
    and put it in the `VideoData/UCF101` folder. Then Python uses the built-in FFmpeg
    package to convert all the `.avi` files to `.mp4` format, and saves in the `VideoData/UCF101_MP4`
    directory once it is executed using the `$ python3 prepare.py` command.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the files are in MP4 format, we can start extracting the features. Now,
    in order to process the `UCF101` dataset and extract the features, I wrote three
    more Java classes, outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`UCF101Reader.java`**:** This is the main entry point for video file reading,
    decoding, and conversion to ND4J vectors. It receives the full path to the dataset
    and creates the `DataSetIterator` required for the neural network. In addition,
    it generates a list of all classes, and it assigns sequential integers for them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UCF101ReaderIterable.java`: This reads all the clips and decodes using JCodec.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RecordReaderMultiDataSetIterator.java`: This is similar to the one provided
    by DL4J but an improved version, which works pretty well on the new version of
    JCodec.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, to prepare the train and test split, the `UCF101Reader.getDataSetIterator()`
    method has been used. The method reads each video clip, but, first, it decides
    on how many examples (video files) to read based on parameters and offset value.
    These parameters are then passed to `UCF101ReaderIterable`. The signature of this
    method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this method, `ExistingDataSetIterator` acts as a wrapper that provides a
    `DataSetIterator` interface to the existing Java `Iterable<DataSet>` and `Iterator<DataSet>`.
    Then the `UCF101Reader.UCF101ReaderIterable()` method is used to create the label
    map (class name to int index) and inverse label map, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the following, `dataDirectory` is the directory of the
    video in MP4 format, (`V_WIDTH`, `V_HEIGHT`) signifies the size of the video frame,
    and `labelMap()` provides the mapping for each video clip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the signature of `labelMap()` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, `UCF101ReaderIterable.iterator()` is used to create an iterator of `DataSet`
    required by the network. This iterator is transmitted to the `ExistingDataSetIterator`
    to be in a form required by the neural net API, `DataSetIterator`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, `AsyncDataSetIterator` is used to do all data processing in a
    separated thread. Whereas `UCF101ReaderIterable.rowStream()` lists all dataset
    files and creates a sequence of files and corresponding class labels, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Then, the `UCF101ReaderIterable.dataSetStreamFromFile()` method is used to convert
    the underlying iterator to java streams. It is just a technical step to convert
    iterators to streams. Because it is more convenient in Java to filter some elements
    and limit the number of elements in the stream. Take a look at this code!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `UCF101ReaderIterable.dataSetIteratorFromFile()` method receives the video
    file path and then creates frame reader (`FrameGrab`—JCodec class). Finally, it
    passes the frame reader to `RecordReaderMultiDataSetIterator.nextDataSet`, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, the `RecordReaderMultiDataSetIterator.nextDataSet()`
    method is used to convert each video frame to dataSet, compatible with DL4J. In
    its turn, DataSet is an association of the features vector generated from the
    frame and the labels vector generated from frame the label using one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, this logic is based on the `RecordReaderMultiDataSetIterator` class of
    DL4J but necessary support comes from the latest JCodec API. Then we used the
    `UCF101RecordIterable.labelToNdArray()` method to encode labels in ND4J `INDArray`
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The previously mentioned workflow steps can be depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef0da07a-2257-42e4-890f-05ba5ae19151.png)'
  prefs: []
  type: TYPE_IMG
- en: Data flow in the feature extraction process
  prefs: []
  type: TYPE_NORMAL
- en: Simple UI for checking video frames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I developed a simple UI application, using Java Swing to test whether the code
    correctly handles frames. This UI reads an input video file in MP4 format and
    shows frames to the reader one by one like a simple video player. The UI application
    is named `JCodecTest.java`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `JCodecTest.java` class, the `testReadFrame()` method utilizes the `getFrameFromFile()`
    method from the `FrameGrab` class (that is, from the JavaCV library) and checks
    whether the frame extraction process from each video clip works correctly. Here
    is the signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, the `rowsStream()` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the effectiveness of this approach, readers can execute the `JCodecTest.java`
    class containing the `main()` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it is executed, you will experience the following output, as shown in
    this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f0a6a42-50d5-484e-aa8b-f6633f143334.png)'
  prefs: []
  type: TYPE_IMG
- en: The JCodecTest.java class checks whether the frame extraction from each video
    clip works correctly
  prefs: []
  type: TYPE_NORMAL
- en: Preparing training and test sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As described earlier, the `UCF101Reader.java` class is used to extract the
    features and prepare the training and test sets. First, we set and show Java the
    MP4 file''s path, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'It is to be noted that training the network with the video clips took about
    45 hours for me on the `EC2 p2.8xlarge` machine. However, I did not have that
    patience for a second time; therefore, I performed the training by utilizing only
    these video categories having 1,112 video clips:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c169f36-4ab2-4b98-af94-a8a2f4ddfa7f.png)'
  prefs: []
  type: TYPE_IMG
- en: The UCF101 dataset directory structure (MP4 version)
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we define the minibatch size to be used for preparing the training and
    test sets. For our case, I put 128, as you can see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the offset from which file the extraction process will start taking
    place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we decide how many sample video clips are to be used for training the
    network, whereas the `UCF101Reader.fileCount()` method returns the number of video
    clips in the `UCF101_MP4` directory. Take a look at this code line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute the test set start index. We use 80% for training and the
    other 20% for testing . Let''s see the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we prepare the training set. For this, the `getDataSetIterator()` method
    does the trick by returning the `DataSetIterator` for all video clips except the
    ones that are planned to be used for the test set. Take a look at this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we prepare the test set. For this, again the `getDataSetIterator()` method
    does the trick by returning the `DataSetIterator` for all video clips except those
    planned to be used for the test set. Take a look at this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Fantastic! Up to this point, we have been able to prepare both training and
    test sets. Now the next step would be to create the network and perform the training.
  prefs: []
  type: TYPE_NORMAL
- en: Network creation and training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we start creating the network by combining convolutional, max pooling,
    dense (feedforward), and recurrent (LSTM) layers to classify each frame of a video
    clip. First, we need to define some hyperparameters and the necessary instantiation,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `NUM_CLASSES` is the number of classes from `UCF101` calculated as the
    quantity of directories in the dataset base directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Then we start the training by calling the `networkTrainer()` method. Well, as
    I stated earlier, we will be combining convolutional, max pooling, dense (feedforward),
    and recurrent (LSTM) layers to classify each frame of a video clip. The training
    data is first fed to the convolutional layer (layer 0), which then gets subsampled
    (layer 1) before being inputted into the second convolutional layer (layer 2).
    Then the second convolutional layer feeds the fully connected layer (layer 3).
  prefs: []
  type: TYPE_NORMAL
- en: It is to be noted that for the first CNN layer we have CNN preprocessor input
    width/height 13 x 18, which reflects a picture size of 320 x 240\. This way, the
    dense layer acts as the input layer for the LSTM layer (layer 4, but feel free
    to use regular LSTM too). However, it is important to note that dense layer inputs
    have a size of 2,340 (that is, 13 * 18 * 10).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the recurrent feedback is connected to RNN output layer, which has a softmax
    activation function for probability distribution over the classes. We also use
    gradient normalization to deal with the vanishing and exploding gradient problem,
    and the backpropagation in the last layer is truncated BPTT. Apart from these,
    we use some other hyperparameters; those are self-explanatory. The following diagram
    shows this network setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/afab5457-5573-4536-82c9-cc51e27e6255.png)'
  prefs: []
  type: TYPE_IMG
- en: Network architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, from the coding point of view, the `networkTrainer()` method has the following
    network configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, based on the preceding network configuration setting, we create a `MultiLayerNetwork`
    and initialize it, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can observe the number of parameters across each layer, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '<q>>>></q> Number of parameters in network: 149599'
  prefs: []
  type: TYPE_NORMAL
- en: Layer 0 nParams = 9030
  prefs: []
  type: TYPE_NORMAL
- en: Layer 1 nParams = 0
  prefs: []
  type: TYPE_NORMAL
- en: Layer 2 nParams = 2710
  prefs: []
  type: TYPE_NORMAL
- en: Layer 3 nParams = 117050
  prefs: []
  type: TYPE_NORMAL
- en: Layer 4 nParams = 20350
  prefs: []
  type: TYPE_NORMAL
- en: Layer 5 nParams = 459
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we start the training using this training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We save the trained network and video configuration using the `saveConfigs()`
    method, and the signature of this method is pretty straightforward, as you can
    see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we save the trained model for later inferencing purposes using the `saveNetwork()`
    method; it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Performance evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To evaluate the network performance, I wrote the `evaluateClassificationPerformance()`
    method, which takes the test set and `evalTimeSeries` evaluation, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to follow the aforementioned steps more clearly, here is the `main()`
    method containing the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We have not achieved higher accuracy. There could be many reasons for this.
    For example, we have used only a few categories (that is, only 9 out of 101).
    Therefore, our model did not get enough training data to learn. Also, most of
    the hyperparameters were set naively.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training on AWS deep learning AMI 9.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how to perform training and inferencing on a single GPU.
    However, to make the training even faster in a parallel and distributed way, having
    a machine or server with multiple GPUs is a viable option. An easy way to achieve
    this is by using AMAZON EC2 GPU compute instances.
  prefs: []
  type: TYPE_NORMAL
- en: For example, P2 is well suited for distributed deep learning frameworks that
    come with the latest binaries of deep learning frameworks (MXNet, TensorFlow,
    Caffe, Caffe2, PyTorch, Keras, Chainer, Theano, and CNTK) pre-installed in separate
    virtual environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'An even bigger advantage is that they are fully configured with NVidia CUDA
    and cuDNN. Interested readers can take a look at [https://aws.amazon.com/ec2/instance-types/p2/](https://aws.amazon.com/ec2/instance-types/p2/).
    A short glimpse of P2 instances configuration and pricing is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89805bb5-7998-48ee-b7fd-82965ba99374.png)'
  prefs: []
  type: TYPE_IMG
- en: P2 instance details
  prefs: []
  type: TYPE_NORMAL
- en: For this project, I decided to use `p2.8xlarge`. You can create it too, but
    make sure that you have already submitted an increased limit to at least one instance,
    which may take as long as three days. However, if you do not know how to do that,
    just create an account on AWS and finish the verification; then go to the EC2
    management console. On the left panel, click on the Limits tab, which will take
    you a page where you can submit an increase limit request by clicking on the Request
    limit increase link.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anyway, I assume that you know this simple stuff, so I''ll move forward to
    creating an instance of type `p2.8xlarge`. On the left panel, click on the Instances
    menu, which should take you to the following page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbbd6a83-181b-4bb3-9a61-fb0f258898e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting a deep learning AMI
  prefs: []
  type: TYPE_NORMAL
- en: An easy option would be creating a deep learning AMI (Ubuntu) Version 9.0, which
    has already configured CUDA and cuDNN, which can be used across eight GPUs. Another
    good thing is that it has 32 computing cores and 488 GB of RAM; this would be
    sufficient for our dataset too. Therefore, instead of using video clips of only
    nine categories, we can perform the training with the full dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, note that, since we will be using DL4J, which is based on JVM, Java
    has to be installed and configured (`JAVA_HOME` has to be set). First, connect
    with your instance from SSH or using an SFTP client. Then on Ubuntu, we can do
    this with a few commands, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, depending on the version you want to install, execute one of the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'After installing, do not forget to set Java home. Just apply the following
    commands (we assume Java is installed at `/usr/lib/jvm/java-8-oracle`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see the `Java_HOME`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you should observe the following result on the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s check to make sure that Java has been installed successfully
    by issuing this command (you may see the latest version!):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Fantastic! We have been able to set up and configure Java on our instance.
    Then let''s see whether the GPUs drivers are configured by issuing the `nvidia-smi`
    command on the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/889034af-b0b1-41b8-9cd3-dc5a9d8255fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Showing Tesla K80 GPUs on a p2.8 xlarge instance
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, initially, there''s no usage of GPUs, but it clearly says that,
    on the instance, there are eight Tesla K80 GPUs installed and configured. Now
    that our GPUs and machine are fully configured, we can focus on the project. We
    are going to use more or less the same code that we used previously but with some
    minimal modifications. The very first change we need to make is to add the following
    line at the beginning of our `main()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we perform the training across eight GPUs, using ParallelWrapper, which
    take cares of load balancing between GPUs. The network construction is the same
    as before, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we start the training by fitting the full test set, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'That is all we need to make. However, make sure you import the following at
    the beginning of the `VideoClassificationExample.java` file for the `CudaEnvironment`
    and `ParallelWrapper`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Nonetheless, I still believe that showing the code for the `main()` method
    and the `networkTrainer()` methods would be helpful. In addition, to avoid possible
    confusion, I have written two Java classes for single and multiple GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`VideoClassificationExample.java`**:** For a single GPU or CPU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VideoClassificationExample_MUltipleGPU.java`: For multiple GPUs on an AWS
    EC2 instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, the latter class has a method, `networkTrainer()`, which is used to create
    a network for distributed training, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the `main()` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s all we need before executing the `VideoClassificationExample_MUltipleGPU.java`
    class. It should also be noted that running a standalone Java class is not a good
    idea from the terminal. Therefore, I would suggest creating a `fat .jar` and including
    all the dependencies. To do that, move your code to the instance using any SFTP
    client. Then install `maven`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the maven is installed, we can start creating the fat JAR file containing
    all the dependencies, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, after a while, a fat JAR file will be generated in the target directory.
    We move to that directory and execute the JAR file, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, please make sure that you have set all the paths properly and
    have the necessary permissions. Well, I assume everything is set properly. Then,
    executing the preceding command will force DL4J to pick BLAS, CUDA, and cuDNN
    and perform the training and other steps. Roughly, you should see the following
    logs on the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command should start the training and you should observe the
    following logs on the Terminal/command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the training should start. Now let''s check whether DL4J is utilizing
    all the GPUs. To know this, again execute the `nvidia-smi` command on the Terminal,
    which should show the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cab146ff-6db0-4956-b71a-ef9068cd59a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Showing resource usage on Tesla K80 GPUs on the p2.8 xlarge instance
  prefs: []
  type: TYPE_NORMAL
- en: Since there are many video clips, training will take a few hours. Once the training
    is completed, the code should provide similar or slightly better classification
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Frequently asked questions (FAQs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have solved the video classification problem, but with low accuracy,
    there are other practical aspects of this problem and overall deep learning phenomena
    that need to be considered too. In this section, we will see some frequently asked
    questions that may be on your mind. Answers to these questions can be found in
    Appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: My machine has multiple GPUs installed (for example, two), but DL4J is using
    only one. How do I fix this problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I have configured a p2.8 xlarge EC2 GPU compute instance on AWS. However, it
    is showing low disk space while installing and configuring CUDA and cuDNN. How
    to fix this issue?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I understand how the distributed training happens on AWS EC2 AMI instance. However,
    my machine has a low-end GPU, and often I get OOP on the GPU. How can solve the
    issue?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can I treat this application as a human activity recognition from a video?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we developed a complete deep learning application that classifies
    a large collection of video datasets from the `UCF101` dataset. We applied a combined
    CNN-LSTM network with DL4J that overcome the limitation of standalone CNN or RNN
    LSTM networks.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw how to perform training in parallel and distributed ways across
    multiple devices (CPUs and GPUs). In summary, this end-to-end project can be treated
    as a primer for human activity recognition from a video. Although we did not achieve
    high accuracy after training, in the network with a full video dataset and hyperparameter
    tuning, the accuracy will definitely be increased.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is all about designing a machine learning system driven by
    criticisms and rewards. We will see how to develop a demo GridWorld game using
    DL4J, RL4J, and neural Q-learning, which acts as the Q-function. We will start
    from reinforcement learning and its theoretical background so that the concepts
    are easier to grasp.
  prefs: []
  type: TYPE_NORMAL
- en: Answers to questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Answer** **to question 1:** This means the training is not being distributed,
    which also means that your system is forcing you to use just one GPU. Now to solve
    this issue, just add the following line at the beginning of your `main()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '**Answer** **to question 2:** Well, this is certainly an AWS EC2-related question.
    However, I will provide a short explanation. If you see the default boot device,
    it allocates only 7.7 GB of space, but about 85% is allocated for the udev device,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ae219c5-ea53-4456-b9e6-e97d83f012c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Showing storage on a p2.8xlarge instance
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to get rid of this issue, you can specify sufficient storage in the boot
    device while creating the instance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c6f61f3-6b03-4f34-b6a0-feca85705ff1.png)'
  prefs: []
  type: TYPE_IMG
- en: Increasing storage on the default boot device the on p2.8xlarge instance
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 3:** Well, if this is the case, you can probably do
    the training on CPU instead of GPU. However, if performing training on a GPU is
    mandatory, I recommend using the `HALF` datatype.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If your machine and code can afford using half-precision math, you can enable
    this as the data type. It will then ensure 2x less GPU memory usage by DL4J. To
    enable this, just add the following line of code to the beginning of the `main()`
    method (even before the multi-GPU allows one):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Using the `HALF` datatype will force your network to squash less precision compared
    to `float` or `double` types. Nonetheless, tuning your network may be harder.
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question** **4:** We have not managed to achieve good accuracy.
    This is the main objective of this end-to-end chapter. Therefore, after training
    the network with the full video dataset and hyperparameter tuning, the accuracy
    will definitely increase.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, and to be honest, Java is not the perfect choice if you want to take
    an application to production. I am saying this because so many advanced feature-extraction
    libraries from video clips are in Python, and those can be used too.
  prefs: []
  type: TYPE_NORMAL
