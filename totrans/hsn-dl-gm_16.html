<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Obstacle Tower Challenge and Beyond</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, our final one, we will take a look at the current and future state of <strong>deep learning</strong> (<strong>DL</strong>) and <strong>deep reinforcement learning</strong> (<strong>DRL</strong>) for games. We take an honest and candid look to see whether these technologies are ready for prime-time commercial games or whether they are just novelties. Are we poised to see DRL agents beating human players at every game imaginable a few years from now? While that remains to be seen, and things are changing quickly, the question really is this: is DL ready for your game? It likely is a question you are asking yourself at this very moment, and it is hopefully one we will answer in this chapter.</p>
<p>This chapter will be a mix of hands-on exercises and general discussions with unfortunately no exercises. Well, there is one big exercise, but we will get to that shortly. Here is what we will cover in this chapter:</p>
<ul>
<li>The Unity Obstacle Tower challenge</li>
<li>Deep Learning for your game?</li>
<li>Building your game</li>
<li>More foundations of learning</li>
</ul>
<p>This chapter assumes you have covered numerous exercises in this book in order to understand the context. We will refer to those sections in order to remind the reader, but please don't jump to this chapter first.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Unity Obstacle Tower Challenge</h1>
                </header>
            
            <article>
                
<p>The <strong>Unity Obstacle Tower Challenge</strong> was introduced in February 2019 as a discrete visual learning problem. As we have seen before, this is the holy grail of learning for games, robotics, and other simulations. What makes it more interesting is this challenge was introduced outside of ML-Agents and requires the challenger to write their own Python code from scratch to control the game—something we have come close to learning how to do in this book, but we omitted the technical details. Instead, we focused on the fundamentals of tuning hyperparameters, understanding rewards, and the agent state. All of these fundamentals will come in handy if you decide to tackle the tower challenge.</p>
<p>At the time this book was written, the ML-Agents version used for developing was <kbd>0.6</kbd>. If you have run all the exercises to completion, you will have noticed that all of the visual learning environments using a discrete action space suffer from a vanishing or exploding gradient problem. What you will see happen is the agent essentially learning nothing and performing random actions; this often takes several hundred thousand iterations to see. But we don't see this problem in environments with a smaller state space using vector observations. In visual environments with a large input state, though, the problem can be seen quite regularly. This means that, essentially, at the time of writing anyway, you would not want to use the Unity code; it currently is a poor visual learner of discrete actions.</p>
<p>At the time of writing, the Unity Obstacle Tower Challenge has just started, and early metrics are already being reported. The current leading algorithm from Google, DeepMind, not surprisingly, is an algorithm called <strong>Rainbow</strong>. In short, Rainbow is the culmination of many different DRL algorithms and techniques all combined to better learn the discrete action visual-learning space that the tower so well defines.</p>
<p>Now that we have established that you likely want to write your own code, we will understand the high-level critical pieces your agent needs to address. It likely would take another book to explain how to do the coding and other technical aspects of that, so we will instead talk about the overall challenges and the critical elements you need to address. Also, the winners will more than likely need to use more probabilistic methods in order to address the problem, and that is currently not covered very well anywhere.</p>
<p>Let's set up the challenge and get it running in the next exercise:</p>
<ol>
<li>Download the Obstacle Tower Environment as a binary from <a href="https://github.com/Unity-Technologies/obstacle-tower-env">https://github.com/Unity-Technologies/obstacle-tower-env</a>.</li>
<li>Follow the instructions and download the zip file for your environment as directed. On most systems, this just requires downloading and unzipping the file into a folder you will execute from later.</li>
<li>Unzip the file into a well-known folder.</li>
</ol>
<ol start="4">
<li>Launch the program by double-clicking on it (Windows) to enter the name in a console. After you launch the challenge, you can actually play it as a human. Play the game and see how many floors you can climb. An example of the running challenge is shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4dbf2be9-5c36-431e-bdd0-504185403822.png" style="width:26.83em;height:15.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>The Obstacle Tower Challenge in player mode</span></div>
<p>One of the first things you will learn as you progress through the game is that the game starts out quite simply, but on the later floors, it gets quite difficult, even for a human.</p>
<p>Now, as we mentioned, solving this challenge is well beyond the scope of this book, but hopefully you can now appreciate some of the complexities that currently stifle the field of deep reinforcement learning. We have reviewed the major challenges that you will face when undertaking this method in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 14.4382%"><strong>Problem</strong></td>
<td style="width: 14.5618%"><strong>Chapter</strong></td>
<td style="width: 12%"><strong>Current</strong> <strong>Status</strong></td>
<td style="width: 55%"><strong>Future</strong></td>
</tr>
<tr>
<td style="width: 14.4382%">Visual observation state—you will need to build a complex enough CNN and possibly recurrent networks to encode enough details in the visual state.</td>
<td style="width: 14.5618%"><a href="9b7b6ff8-8daa-42bd-a80f-a7379c37c011.xhtml">Chapter 7</a>, <em>Agent and the Environment</em></td>
<td style="width: 12%">The current Unity visual encoder is far from acceptable.</td>
<td style="width: 55%">Fortunately, there is plenty of work always being done with CNN and recurrent networks for analysis of videos. Remember, you don't just want to capture static images; you also want to encode the sequence of the images.</td>
</tr>
<tr>
<td style="width: 14.4382%">DQN, DDQN, or Rainbow</td>
<td style="width: 14.5618%"><a href="6ca7a117-1a8c-49f9-89c0-ee2f2a1e8baf.xhtml">Chapter 5</a>, <em>Introducing DRL</em></td>
<td style="width: 12%">Rainbow is currently the best, and it is available on the GCP.</td>
<td style="width: 55%">As we have seen in this book, PPO only performs well on continuous action spaces. In order to tackle the discrete action space, we look back to more fundamental methods such as DQN or the newcomer Rainbow, which is the summation of all base methods. We will also discuss future ways in which further use of deep probabilistic methods may be the answer.</td>
</tr>
<tr>
<td style="width: 14.4382%">Intrinsic rewards</td>
<td style="width: 14.5618%"><a href="ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml">Chapter 9</a>, <em>Rewards and Reinforcement Learning</em></td>
<td style="width: 12%">The use of an intrinsic reward system shows promise for exploration.</td>
<td style="width: 55%">Being able to introduce intrinsic reward systems such as <strong>Curiosity Learning</strong> allows the agent to explore new environments based on some expectation of state. This method will be essential for any algorithm that plans to reach the higher levels of the tower.</td>
</tr>
<tr>
<td style="width: 14.4382%">Understanding</td>
<td style="width: 14.5618%"><a href="b422aff5-b743-4696-ba80-e0a222ea5b4d.xhtml">Chapter 6</a>, <em>Unity ML-Agents</em></td>
<td style="width: 12%">Unity provides an excellent sample environment to build and test models on.</td>
<td style="width: 55%">You can easily build and test a similar environment in Unity quite quickly and on your own. It is no wonder Unity never released the raw Unity environment as a project. This was more than likely because this would have attracted many novices, thinking they could overcome the problem with just training. Sometimes, training is just not the answer.</td>
</tr>
<tr>
<td style="width: 14.4382%">Sparse rewards</td>
<td style="width: 14.5618%">
<p class="mce-root"><a href="ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml">Chapter 9</a>, <em>Rewards and Reinforcement Learning</em></p>
<p><a href="1525f2f4-b9e1-4b7f-ac40-33e801c668ed.xhtml">Chapter 10</a>, <em>Imitation and Transfer Learning</em></p>
</td>
<td style="width: 12%">Could implement Curriculum or Imitation Learning.</td>
<td style="width: 55%">We have already covered many examples of ways to manage the sparse rewards problem. It will be interesting to see how much the winners depend on one of these methods, such as IL, to win.</td>
</tr>
<tr>
<td style="width: 14.4382%">Discrete actions</td>
<td style="width: 14.5618%"><a href="1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml">Chapter 8</a>, <em>Understanding PPO</em></td>
<td style="width: 12%">We learned how PPO allowed continuous action problems to learn, using stochastic methods.</td>
<td style="width: 55%">As we alluded to before, it will likely take new work into more deep probabilistic methods and techniques to work around some of the current problems. This will likely require the development of new techniques using new algorithms, and how long that takes remains to be seen.</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root">Each of the problems highlighted in the preceding table will likely need to be solved in part or wholly in order to get an agent from floor 1 to 100 to complete the entire challenge. It remains to be seen how this will play out for Unity, the winner, and DRL as a whole. In the next section, we discuss the practical applications of DL and DRL, and how they can be used for your game.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep Learning for your game?</h1>
                </header>
            
            <article>
                
<p>It's likely the reason you picked this book up was to learn about DL and DLR for games in the hope of landing your dream job or completing your dream game. In either case, we come to a point where you decide whether this technology is worth including in your own game and to what extent. The following is a list of ten questions you can use to determine whether DL is right for your game:</p>
<ol>
<li>Have you already made the decision and need to build the game with DL or DRL?
<ul>
<li>Yes – 10 points</li>
<li>No <span>–</span> 0 points</li>
</ul>
</li>
<li>Will your game benefit from some form of automation, either through testing or managing repetitious player tasks?
<ul>
<li>Yes <span>–</span> 10 points</li>
<li>No <span>–</span> 0 points</li>
</ul>
</li>
<li>Do you want to make training and AI or another similar activity part of the game?
<ul>
<li>Yes <span>–</span> (-5) points. <em>You may be better off using a more robust from of AI to simulate the training. Training DRL takes too many iterations and samples to be effective as an inline game-training tool, at least for now.</em></li>
<li>No <span>–</span> 0 points.</li>
</ul>
</li>
<li>Do you want cutting-edge AI to feature in your game?
<ul>
<li>Yes <span>–</span> 10 points. <em>There are certainly ways of layering AI technologies and making a DRL solution work. When it comes to current AI, there really is no better cutting-edge technology.</em></li>
<li>No <span>–</span> 0 points.</li>
</ul>
</li>
<li>Do you have hours of time to train an AI?
<ul>
<li>Yes <span>–</span> 10 points</li>
<li>No <span>–</span> (-10) points</li>
</ul>
</li>
<li>Have you read a good portion of this book and completed at least a few of the exercises?
<ul>
<li>Yes <span>–</span> 10 points, +5 if you completed more than 50%</li>
<li>No <span>–</span> (-10) points; thanks for the honesty</li>
</ul>
</li>
<li>Do you have a background or affinity for math?
<ul>
<li>Yes <span>–</span> 10 points</li>
<li>No <span>–</span> (-10) points</li>
</ul>
</li>
</ol>
<ol start="8">
<li>How many papers have you read on reinforcement learning at an academic level?
<ul>
<li>10+ <span>–</span> 25 points</li>
<li>5<span>–</span>10 <span>–</span> 10 points</li>
<li>1<span>–</span>5 <span>–</span> 5 points</li>
<li>0 <span>–</span> 0 points</li>
</ul>
</li>
<li>What is your completion timeline?
<ul>
<li>1<span>–</span>3 months <span>–</span> (-10) points</li>
<li>3<span>–</span>6 months <span>–</span> 0 points</li>
<li>6<span>–</span>12 months <span>–</span> 10 points</li>
<li>1<span>–</span>2+ years <span>–</span> 25 points</li>
</ul>
</li>
<li>What is the size of your team?
<ul>
<li>Solo <span>–</span> (-10) points</li>
<li>2<span>–</span>5 <span>–</span> 0 points</li>
<li>6<span>–</span>10 <span>–</span> 10 points</li>
<li>11+ <span>–</span> 25 points</li>
</ul>
</li>
</ol>
<p>Answer all the questions and score your points to determine your full readiness score. Consult the following to determine how ready you and/or your team are:</p>
<ul>
<li><strong>&lt;0 points</strong> - How did you even make it this far into the book? You're not ready, and it's best you just put this book down.</li>
<li><strong>0-50</strong> - You certainly show promise, but you are going to need some more help; check out the following section on next steps and further areas of learning.</li>
<li><strong>50-100</strong> - You certainly are on your way to building the knowledge base and implementing some fun DRL in games, but you may still need a little help. Check the section on next steps and further areas of learning.</li>
<li><strong>100+</strong> - You are well beyond ready, and we appreciate you taking the time to read this book. Perhaps take some of your own personal time and pass your own or your team members' knowledge on to people you know.</li>
</ul>
<p>Of course, there are no absolute rules to the results of the preceding test, and you may find that you score quite low but then go on to make the next great AI game. How you approach the results is up to you, and how you take your next steps is also entirely up to you.</p>
<p>In the next section, we look at the next steps you can take to learn more about DRL and how to build better automation and AI in games.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building your game </h1>
                </header>
            
            <article>
                
<p>Now that you have decided to use deep learning and/or deep reinforcement learning for your game, it is time to determine how you plan to implement various functionality in your game. In order to do that, we are going to go through a table outlining the steps you need to go through in order to build your game's AI agent:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Step</strong></p>
</td>
<td>
<p><strong>Action</strong></p>
</td>
<td>
<p><strong>Summary</strong></p>
</td>
</tr>
<tr>
<td>Start</td>
<td>Determine at what level you want the AI in the game to operate, from basic, perhaps for just testing and simple automation, to advanced, where the AI will complete against the player.</td>
<td>Determine the level of AI.</td>
</tr>
<tr>
<td>Resourcing</td>
<td>Determine the amount of resources. Basic AI or automation could be handled within the team itself, whereas more complex AI may require one or many experienced members of staff.</td>
<td>Team requirements.</td>
</tr>
<tr>
<td>Knowledge</td>
<td>Determine the level of knowledge the team possesses and what will be required. It is a given that any team implementing new AI will need to learn new skills. </td>
<td>Knowledge-gap analysis.</td>
</tr>
<tr>
<td>Demonstration</td>
<td>Always start by building a simple but workable proof of concept that demonstrates all critical aspects of the system.</td>
<td>Demonstrate the team can complete the basic premise.</td>
</tr>
<tr>
<td>Implementation</td>
<td>Build the actual system in a way that is simplistic and maintainable. Keep all the things you know simple and clean.</td>
<td>Build the system.</td>
</tr>
<tr>
<td>Testing</td>
<td>Test the system over and over again. It is critical that the system is tested thoroughly, and of course what better way to do that than with a DRL automated test system.</td>
<td>Test the system.</td>
</tr>
<tr>
<td>Fix</td>
<td>As anyone who has developed software for more than a few weeks will tell you, the process is build, test, fix, and repeat. That essentially is the software development process, so try not to add too many other bells and whistles to distract from that.</td>
<td>Fixing the system.</td>
</tr>
<tr>
<td>Release</td>
<td>Releasing software to users/players is absolutely critical to a successful game or software product of any kind. You will always want to release early and often, which means your players must be encouraged to test, and to provide feedback.</td>
<td>Let the bugs out.</td>
</tr>
<tr>
<td>Repeat</td>
<td><span>The cycle is endless and will continue as long as your product/game makes money.</span></td>
<td>Support the system.</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The preceding process is the basic premise and will work for most of your development needs. In most cases, you may want to track individual work items such as features or bugs on a work or task board. You may want to use a more defined process such as Scrum, but often keeping things simple is your best course of action.</p>
<p>Scrum and other software development processes are great examples to learn from, but unless you have formally trained staff, it's better to avoid trying to implement these yourself. There are often subtle rules that need to be enforced in these processes for them to work as they claim to. Even trained Scrum Masters may need to battle daily to enforce these rules in many organizations, and in the end their value becomes more management-driven than developer-focused. Use the previous table as a guide for the steps you take in building your next game, and always remember that build, release, fix, and repeat is the key to good software.</p>
<p>In the next section, we will look at other things you can use to expand your learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">More foundations of learning</h1>
                </header>
            
            <article>
                
<p>There is an ever-growing resource for learning about machine learning, DL, and of course DLR. The list is becoming very large, and there are many materials to choose from. For that reason, we will now summarize the areas we feel show the most promise for developing AI and DL for games:</p>
<ul>
<li><strong>Basic Data Science Course</strong>: If you have never taken a basic fundamentals course on data science, then you certainly should. The foundations of understanding the qualities of data, statistics, probability, and variability are too numerous to mention. Be sure to cover this foundation first.</li>
<li><strong>Probabilistic Programming</strong>: This is a combination of various variational inference methods by which to answer problems given a probability of events with an answer of the probability that some event may occur. These types of models and languages have been used to analyze financial information and risk for years, but they are now coming to the forefront in ML technologies.</li>
<li><strong>Deep Probabilistic Programming</strong>: This is the combination of variational inference and DL models. Variational inference is the process by which you answer a question with a probability given the input of possibly multiple probabilities. So, instead of using a series of weights to train a network, we use a series of probability distributions. This method has proven to be very effective and has recently performed visual image classification tasks with a modified probabilistic CNN model.</li>
<li><strong>Visual state classification and encoding</strong>: A critical aspect to a DL system is the development of CNN models to classify images. You will need to understand this space very well in order to build the networks for your game environment. Recall that different environments may require CNN models.</li>
<li><strong>Memory</strong>: Memory can of course come in all forms, but the primary one of interest is the <strong>recurrent neural network</strong> (<strong>RNN</strong>). Early on in this book, we looked at the current standard recurrent network model we use called the <strong>long short-term memory</strong> (<strong>LSTM</strong>) <strong>block</strong>. Even at the time of writing, there is a renewed interest in the <strong>gated recurrent unit</strong> (<strong>GRU</strong>), a more complex recurrent network that has been shown to handle the vanishing gradient problem better. There is always an interest in cloud or other supported technologies and how they may interact with new DL technologies.</li>
<li><strong>DL as a Service</strong>: Companies such as Google, Amazon, Microsoft, OpenAI, and others who claim to be all about openness are often far from it. In most cases, if you want to incorporate these technologies into your game, you will need to subscribe to their service—which of course has its own pluses and minuses. The major problem is that if your game becomes popular and if you rely heavily on the DL service, your profits will be tied to it. Fortunately, Unity has yet to take this approach, but that does remain to be seen depending on how easily the community solves the Obstacle Tower Challenge.</li>
<li><strong>Math</strong>: In general, you will want to always advance your math skills whether you plan to dig deep into building your own models or not. In the end, your gut understanding of the math will provide you with the insights you need to overcome these complex technologies.</li>
<li><strong>Perseverance</strong>: Learn to fail, and then move on. This is critical and something many new developers often get disgruntled with and then move on to something easier, simpler, and less rewarding. Be happy when you fail, as failing is learning to understand. If you never fail, you really never learn, so learn to fail.</li>
</ul>
<p>A hard-coded list of learning resources would likely get out of date before this book is even printed or released. Use the preceding list to generalize your learning and broaden your basic machine learning and data science knowledge as well. First and foremost, DL is a data science pursuit that serves respect to the data; never forget that as well.</p>
<p>In the next section for our final chapter, we will summarize this chapter and the book.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we took a short tour of many basic concepts involving your next steps in DL and DRL; perhaps you will decide to pursue the Unity Obstacle Tower Challenge and complete that or just use DRL in your own project. We looked at simple quizzes in order to evaluate your potential for diving in and using DRL in a game. From there, we looked at the next steps in development, and then finally we looked at other areas of learning may want to focus on.</p>
<p>This book was an exercise in understanding how effective DL can be when applied to your game project in the future. We explored many areas of basic DL principles early on and looked at more specific network types such as CNN and LSTM. Then, we looked at how these basics network forms could be applied to applications for driving and building a chatbot. From there, we looked at the current king of machine learning algorithms, reinforcement and deep reinforcement learning. We then looked at one of the current leaders, Unity ML-Agents, and how to implement this technology, over several chapters by looking at how simple environments are built to more complex multi-agent environments. This also allowed us to explore different forms of intrinsic/extrinsic rewards and learning systems, including curriculum, curiosity, imitation, and transfer learning.</p>
<p>Finally, before finishing this chapter, we completed a long exercise regarding using DRL for automatic testing and debugging with the added option of using IL as a way of enhancing testing.</p>


            </article>

            
        </section>
    </body></html>