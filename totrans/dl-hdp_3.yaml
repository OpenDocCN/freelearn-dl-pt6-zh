- en: Chapter 3.  Convolutional Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '|   | *"The question of whether a computer can think is no more interesting
    than the question of whether a submarine can swim."* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Edsger W. Dijkstra* |'
  prefs: []
  type: TYPE_TB
- en: '**Convolutional neural network (CNN)**--doesn''t it give an uncanny feeling
    about the combination of mathematics and biology with some negligible amount of
    computer science added? However, these type of networks have been some of the
    most dominant and powerful architectures in the field of computer vision. CNN
    started to gain its popularity after 2012, when there were huge improvements in
    the precision of classification, credit to some pioneer in the field of deep learning.
    Ever since then, a bunch of high-tech companies have been using deep CNN for various
    services. Amazon uses CNN for their product recommendations, Google uses it for
    their photo search, and Facebook primarily uses it for its automatic tagging algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: CNN [89] is a type of feed-forward neural network comprised of neurons, which
    have learnable weights and biases. These types of networks are basically used
    to process data, having the grid-like topology form. CNNs, as the name suggests,
    are a type of neural network where, unlike the general matrix multiplication,
    a special type of linear mathematical operation, convolution, is used in at least
    one of the subsequent layers. The architecture of CNN is designed to take the
    benefit of input with multidimensional structure. These include the 2D structure
    of an input image, speech signal, or even one-dimensional time series data. With
    all these advantages, CNN has been really successful with many practical applications.
    CNN is thus tremendously successful, specifically in fields such as natural language
    processing, recommender systems, image recognition, and video recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A bias unit is an *extra* neuron that has a value of 1 and is added to each
    pre-output layer. These units are not connected to the previous layer and so do
    not represent any *activity* in a real sense.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the building block of CNN in-depth. We will
    initially discuss what convolution is and the need of convolution operations in
    the neural network. Under that topic, we will also address pooling operation,
    which is the most important component of CNN. The next topic of this chapter will
    point out the major challenges of CNN while dealing with large-scale data. The
    last part of this chapter will help the reader to learn how to design CNN using
    Deeplearning4j.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics of the chapter are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Background of a CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic layers of CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed deep CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNN with Deeplearning4j
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the concept of convolution, let us take an example to determine
    the position of a lost mobile phone with the help of a laser sensor. Let's say
    the current location of the mobile phone at time *t* can be given by the laser
    as *f (t)*. The laser gives different readings of the location for all the values
    of *t*. The laser sensors are generally noisy in nature, which is undesirable
    for this scenario. Therefore, to derive a less noisy measurement of the location
    of the phone, we need to calculate the average various measurements. Ideally,
    the more the measurements, the greater the accuracy of the location. Hence, we
    should undergo a weighted average, which provides more weight to the measurements.
  prefs: []
  type: TYPE_NORMAL
- en: A weighted function can be given by the function *w (b)*, where *b* denotes
    the age of the measurement. To derive a new function that will provide a better
    estimate of the location of the mobile phone, we need to take the average of the
    weight at every moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new function can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding convolution](img/image_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding operation is termed as convolution. The conventional method of
    representing convolution is denoted by an asterisk or star, ''*'':'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding convolution](img/image_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Formally, convolution can be defined as an integral of the product of two functions,
    where one of the functions is reversed and shifted. Besides, taking the weighted
    averages, it may also be used for other purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of convolutional network terminology, the function *f* in our example
    is referred to as the input and the function *w*, the second parameter is called
    the kernel of the operation. The kernel is composed of a number of filters, which
    will be used on the input to get the output, referred to as *feature maps*. In
    a more convenient way, the kernel can be seen as a membrane, which will allow
    only the desirable features of the input to pass through it. *Figure 3.1* shows
    a pictorial view of the operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding convolution](img/image_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: The figure shows a simple representation of a convolutional network
    where the input has to pass through the kernel to provide the feature map.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a practical scenario, as our example shows, the laser sensor cannot really
    provide the measurements at every given instant of time. Ideally, when a computer
    works on data, it only works at some regular intervals; hence, the time will be
    discrete. So, the sensor will generally provide the results at some defined interval
    of time. If we assume that the instrument provides output once/second, then the
    parameter *t* will only take integer values. With these assumptions, the functions
    *f* and *w* will only be defined for integer values of *t*. The modified equation
    for discrete convolution can now be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding convolution](img/image_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In case of machine learning or deep learning applications, the inputs are generally
    a multidimensional array of data, and the kernel uses multidimensional arrays
    of different parameters taken by the algorithm. The basic assumption is that the
    values of the functions are non-zero only for a finite set of points for which
    we store the values, and zero elsewhere. So, the infinite summation can be represented
    as the summation for a range of a finite number of array elements. For example,
    for a 2D image *I* as an input and a corresponding 2D kernel *K*, the convolution
    function can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding convolution](img/image_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, with this, you have already got some background of convolution. In the next
    section of this chapter, we will discuss the application of convolution in a neural
    network and the building blocks of CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Background of a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNN, a particular form of deep learning models, is not a new concept, and they
    have been widely adopted by the vision community for a long time. The model worked
    well in recognizing the hand-written digit by LeCun et al in 1998 [90]. But unfortunately,
    due to the inability of CNNs to work with higher resolution images, its popularity
    has diminished with the course of time. The reason was mostly due to hardware
    and memory constraints, and also the lack of availability of large-scale training
    datasets. As the computational power increases with time, mostly due to the wide
    availability of CPUs and GPUs and with the generation of big data, various large-scale
    datasets, such as the MIT Places dataset (see Zhou et al., 2014), ImageNet [91]
    and so on. it became possible to train larger and complex models. This is initially
    shown by Krizhevsky et al [4] in their paper, *Imagenet classification using deep
    convolutional neural networks*. In that paper, they brought down the error rate
    with half-beating traditional approaches. Over the next few years, their paper
    became one of the most substantial papers in the field of computer vision. This
    popular network trained by Alex Krizhevsky, called AlexNet, could well have been
    the starting point of using deep networks in the field of computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We assume that the readers are already familiar with the traditional neural
    network. In this section, we will look at the general building blocks of CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The traditional neural network receives a single vector as input, and reaches
    the intermediate states through a series of latent (hidden) layers. Each hidden
    layer is composed of several neurons, where each neuron is fully connected to
    every other neuron of the previous layer. The last layer, called the ''output
    layer'', is fully-connected, and it is responsible for the class scores. A regular
    neural network composed of three layers is shown in *Figure 3.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture overview](img/image_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: The figure shows the block diagram of a three-layer regular neural
    network. The neurons of every layer are fully-connected to every other layer of
    the previous layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regular neural networks face tremendous challenges while dealing with large-scale
    images. For example, in the CIFAR-10 RGB database, the dimension of the images
    are *32x32x3*, hence, a single fully-connected neuron in a first hidden layer
    of the traditional neural network will have *32*32*3= 3072* number of weights.
    The number of weights, although seems to be reasonable at the outset, would really
    be a cumbersome task to manage with the increasing number of dimensions. For another
    RGB image, if the dimension becomes (*300x300x3*), the total number of weights
    of the neurons will result in *300*300*3 = 270000* weights. Also, as the number
    of layers will increase, this number will also increase drastically, and would
    quickly lead to overfitting. Moreover, visualization of an image completely neglects
    the complex 2D spatial structure of the image. Therefore, the fully-connected
    concept of the neural network, right from the initial phase, does not seem to
    work with the larger dimensional datasets. So, we need to build a model that will
    overcome both of these limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture overview](img/image_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: The arrangement of CNN in 3D (width, height, and depth) is represented
    in the figure. Every layer converts the 3D input volume to the corresponding 3D
    output volume of neuron activations. The red input layer keeps the image, hence,
    its width and height would be the dimensions of the image, where the depth would
    be three (Red, Green, and Blue). Image sourced from Wikipedia.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to solve this problem is to use convolution in place of matrix multiplication.
    Learning from a set of convolutional filters (kernel) is much easier than learning
    from the whole matrix (*300x300x3*). Unlike the traditional neural network, the
    layers of a CNN have their neurons arranged in three dimensions: width, height,
    and depth. *Figure 3.3* shows the representation for this. For example, in the
    previous example of CIFAR-10, the image has a dimension of *32x32x3*, which is
    width, depth, and height respectively. In a CNN, instead of the neurons in a fully-connected
    nature, the neurons in a layer will only be connected to a subset of neurons in
    the previous layer. Details of this will be explained in the subsequent portion
    of this section. Moreover, the final output layer CIFAR-10 image will have the
    dimension *1x1x10*, because the CNN will diminish the full image into a single
    vector of class score, placed along with the depth dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: Basic layers of CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A CNN is composed of a sequence of layers, where every layer of the network
    goes through a differentiable function to transform itself from one volume of
    activation to another. Four main types of layers are used to build a CNN: Convolutional
    layer, Rectified Linear Units layer, Pooling layer, and Fully-connected layer.
    All these layers are stacked together to form a full CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A regular CNN could have the following architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[INPUT - CONV - RELU - POOL - FC]'
  prefs: []
  type: TYPE_NORMAL
- en: However, in a deep CNN, there are generally more layers interspersed between
    these five basic layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'A classic deep neural network will have the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: Input -> Conv->ReLU->Conv->ReLu->Pooling->ReLU->Conv->ReLu->Pooling->Fully Connected
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet, as mentioned in the earlier section, can be taken as a perfect example
    for this kind of structure. The architecture of AlexNet is shown in *Figure 3.4*.
    After every layer, an implicit ReLU non-linearity has been added. We will explain
    this in detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'One might wonder, why do we need multiple layers in a CNN? The next section of
    this chapter shall explain this as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic layers of CNN](img/image_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: An illustration of the depth and weight of the AlexNet is shown
    in the figure. The number inside the curly braces denotes the number of filters
    with dimensions written above.'
  prefs: []
  type: TYPE_NORMAL
- en: Importance of depth in a CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the paper [96], the author has put forward a few statistics, to show how
    deep networks help in gaining more accuracy of the output. As noted, the Architecture
    of Krizhevsky et al. model uses eight layers, which are trained on ImageNet. When
    the fully connected top layer (7th layer) is removed, it drops approximately 16
    million parameters with a performance drop of 1.1%. Furthermore, when the top
    two layers (6th and 7th) are removed, nearly 50 million parameters get reduced
    along with a 5.7% drop in performance. Similarly, when the upper feature extractor
    layers (3rd and 4th) are removed, it results in a drop of around 1 million parameters
    with a performance drop of 3.0%. To get a better insight of the scenario, when
    the upper feature extractor layers and the fully connected (3rd, 4th, 6th, and
    7th) layers are removed, the model was left with only four layers. In that case,
    a 33.5% drop in performance is experienced.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it can be easily concluded that we need deep convolutional network
    to increase the performance of the model. However, as already stated, a deep network
    is extremely difficult to manage in a centralized system due to limitations of
    memory and performance management. So, a distributed way of implementing deep
    CNN is required. In the subsequent sections of this chapter, we will explain how
    to implement this with the help of Deeplearning4j, and integrating the processing
    with Hadoop's YARN.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As illustrated in the architecture overview, the main purpose of convolution
    is to allow the model to work with a limited number of inputs at a particular
    time. Moreover, convolution supports three most important features, which substantially
    help in improving the performance of a deep learning model. The features are listed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Sparse connectivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter sharing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equivariant representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now describe each of these features in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse connectivity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As already explained, the traditional network layers use matrix multiplication
    by a matrix of parameters with a different parameter describing the interaction
    between each output unit and input unit. On the other hand, CNNs use sparse connectivity,
    sometimes referred to as sparse interactions or sparse weights, for this purpose.
    This idea is attained by keeping the size of the kernel smaller than the input,
    which helps in reducing the time complexity of the algorithm. For example, for
    a large image dataset, the image could have thousands or millions of pixels; however,
    we can identify the small, significant features of the image, such as edges and
    contours from the kernels, which only have hundreds or tens of the whole pixels.
    Therefore, we need to keep only a small number of parameters, which, in turn,
    helps in the reduction of memory requirements of the models and datasets. The
    idea also alleviates the number of operations, which could enhance the overall
    computing power. This, in turn, decreases the running time complexity of the computation
    in a huge manner, which eventually ameliorates its efficiency. *Figure 3.5* diagrammatically
    shows, how with the sparse connectivity approach, we can reduce the number of
    receptive fields of each neuron.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each neuron in a Convolutional layer renders the response of the filters applied
    in the previous layer. The main purpose of these neurons is to pass the responses
    through some non-linearity. The total area of the previous layers, where that
    filter was applied, is termed as the receptive field of that neuron. So, the receptive
    field is always equivalent to the size of the filter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sparse connectivity](img/B05883_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: The figure shows how the input units of M affect the output unit
    N3 with sparse connectivity.Unlike matrix multiplication, the number of receptive
    fields in the sparse connectivity approach reduce from five to three (M2, M3,
    and M4). The arrows indicate the parameter sharing approach too. The connections
    from one neuron are shared with two neurons in the model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, with the sparse connectivity approach, the receptive fields for
    each layer are smaller than the receptive fields using the matrix multiplication
    approach. However, it is to be noted that for deep CNNs, the receptive field of
    the units is virtually larger than the receptive fields of the corresponding shallow
    networks. The reason is that all the units in the deep networks are indirectly
    connected to almost all the neurons of the network. *Figure 3.6* shows a visual
    representation of such a scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sparse connectivity](img/B05883_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Representation of sparse connectivity for deep layers of convolution
    neural networks. Unlike Figure 3.5, where unit N3 had three receptive fields,
    here the number of receptive fields of N3 has increased to five.'
  prefs: []
  type: TYPE_NORMAL
- en: Improved time complexity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to the example given in the previous section, if there are *p* inputs
    and *q* outputs in a layer, then the matrix multiplication will require *(p*q)*
    number of parameters. The running time complexity of the algorithm will become
    *O (p*q)*. With the sparsely connected approach, if we limit the number of upper
    limit connections associated with each output to *n*, then it will need only *n*q*
    parameters, and the runtime complexity will reduce to *O (n*q)*. For many real-life
    applications, the sparse connectivity approach provides good performance for the
    deep learning tasks while keeping the size of *n <<p*.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter sharing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parameter sharing can be defined as the process by which the same parameter
    for a function can be used for multiple functions in the model. In regular neural
    networks, each element of the weight matrix is applied exactly once, when calculating
    the output of a layer. The weight is multiplied by one element of the input, but
    never revisited. Parameter sharing can also be referred to as tied weights, as
    the value of the weight used to one input is tied to the value weight used for
    others. *Figure 3.5* can also be viewed as an example for parameter sharing. For
    example, a particular parameter from **M2** is used with both **N1** and **N3**.
  prefs: []
  type: TYPE_NORMAL
- en: The main purpose of the operation is to control the number of free parameters
    in the Convolutional layer. In a CNN, each element of the kernel is used at almost
    every position of the input. One logical assumption for this is that if one of
    the features is desirable at some spatial position, then it should also be necessary
    to calculate the other positions.
  prefs: []
  type: TYPE_NORMAL
- en: Since all the elements of a single depth slice share the same type of parametrization,
    the forward pass in each depth slice of the Convolutional layer can be measured
    as a convolutional of input volume with the weights of the neurons. The outcome
    of this convolution is an activation map. These collections of activation maps
    are stacked together with the association of depth dimension to result the output
    volume. Although the parameter sharing approach bestows the translation invariance
    of the CNN architecture, it does not enhance the runtime of forward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: Improved space complexity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In parameter sharing, the runtime of the model still remains *O (n*q)*. However,
    it helps to reduce the overall space complexity in a significant way, as the storage
    requirement of the model reduces to n number of parameters. Since *p* and *q*
    are generally of similar sizes, the value of *n* becomes almost negligible as
    compared to *p*q*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolution is considerably more productive than the traditional dense matrix
    multiplication approach, both in terms of time complexity as well as space complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Equivariant representations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the convolution layer, due to parameter sharing, the layers possess a property
    termed as equivariance to translation. An equivariant function is defined as a
    function whose output changes in the same way the input does.
  prefs: []
  type: TYPE_NORMAL
- en: "Mathematically, if X and Y both belong to a same group G, then a function *f:\
    \ X ![Equivariant representations](img/Arrow.jpg) \x86\x92 Y* is said to be equivariant\
    \ if *f (g.x) = g.f(x) for* all *g* *![Equivariant representations](img/Belongs-t0.jpg)\
    \ G* and all *x* in *X*."
  prefs: []
  type: TYPE_NORMAL
- en: 'In case of convolution, if we take *g* to be any function, which shifts the
    input in equal magnitude, then the convolution function is equivariant to *g*.
    For example, let *I* be a function that gives the image color for any even coordinate.
    Let *h* be another function, which maps one image function to another image function,
    given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equivariant representations](img/image_03_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*I^/* is an image function that moves every pixel of *I* five units to the
    right. Therefore, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equivariant representations](img/image_03_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, if we apply this translation to *I*, followed by the convolution, the result
    would be exactly the same when we apply convolution to *I^/*, followed by the
    transformation function *h* to the output.
  prefs: []
  type: TYPE_NORMAL
- en: In case of images, a convolution operation generates a two-dimensional map of
    all the definite features present in the input. So, similar to the earlier example,
    if we shift the object in the output by some fixed scale, the output representation
    will also move in the same scale. This concept is useful for some cases; for example,
    consider a group photo of cricket players of two different teams. We can find
    some common feature of the jersey in the image to detect some players. Now, the
    similar feature will obviously be present in others' t-shirts as well. So, it
    is quite practical to share the parameter across the entire image.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution also helps to process some special kinds of data, which are difficult,
    or rather not even possible, with the traditional fixed-shape matrix multiplication
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the hyperparameters for Convolutional layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far we have explained how each neuron in the convolution layers is connected
    to the input volume. In this section, we will discuss the ways of controlling
    the size of the output volume. In other words, controlling the number of neurons
    in the output volume, and how they are arranged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, there are three hyperparameters, which control the size of the output
    volume of the Convolutional layers. They are: the depth, stride, and zero-padding.'
  prefs: []
  type: TYPE_NORMAL
- en: How do we know how many Convolutional layers should we use, what should be the
    size of the filters, or the values of stride and padding? These are extremely
    subjective questions, and their solutions are not at all trivial in nature. No
    researchers have set any standard parameter to choose these hyperparameters. A
    neural network generally and largely depends on the type of data used for training.
    This data can vary in size, complexity of the input raw image, type of image processing
    tasks, and many other criteria. One general line of thought by looking at the
    big dataset, is that one has to think how to choose the hyperparameters to deduce
    the correct combination, which creates abstractions of the images at proper scale.
    We'll discuss all these in this subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Depth
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the output volume, depth is considered as an important parameter. The depth
    corresponds to the number of filters we would like to apply for each learning
    iteration on some changes in the input. If the first Convolutional layer takes
    a raw image as the input, then multiple neurons along the depth dimension might
    activate in the presence of various blobs of colors or different oriented edges.
    The set of neurons in the same regions of input are termed as a depth column.
  prefs: []
  type: TYPE_NORMAL
- en: Stride
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stride specifies the policy of allocation of depth columns around the spatial
    dimension (width and height). It basically controls how the filter convolves around
    the input volume. Stride can be formally defined as the amount by which the filter
    shifts during the convolution. Ideally, the value of stride should be an integer
    and not a fraction. Conceptually, this amount helps in deciding how much of the
    input image information one wants to retain before proceeding to the next layer.
    The more the stride, the more information that will be retained for the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when the stride is *1*, a new depth column is allocated to spatial
    positions, one spatial unit apart. This produces large output volumes due to heavily
    overlapping receptive fields in between the columns. On the other hand, if the
    value of stride is increased, there will be less overlapping among the receptive
    fields, which results in spatially smaller dimensional output volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take an example to simplify the concept a bit more. Let us imagine
    a *7*7* input volume and a *3*3* filter (we will ignore the third dimension for
    the sake of simplicity), with a stride of *1*. The output volume in this case
    would be of dimension *5*5*, as shown in *Figure 3.7*. However, this looks somewhat
    straightforward. Now, with stride *2*, keeping the other parameters the same,
    the output volume would have less dimensionality of the order *3*3*. In this case,
    the receptive field will shift by *2* units, and hence, the volume will shrink
    to a dimension of *3*3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stride](img/image_03_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Illustration of how the filter convolves around the input volume
    of 7x7 with stride 1 resulting in 5x5 output volume.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is illustrated in *Figure 3.8*. All these calculations are based on some
    formula mentioned in the next topic of this section. Now, if we want to increase
    the stride further to *3*, we will have difficulties with spacing and making sure
    the receptive field fits on the input volume. Ideally, a programmer will raise
    the value of the stride only if lesser overlapping of the receptive fields is
    required, and if they need smaller spatial dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stride](img/image_03_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Illustration of how the filter convolves around the input volume
    of 7x7 with stride 2 resulting in 3x3 output volume.'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-padding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We've already got enough information to infer that, as we keep applying more
    convolution layers to the input volume, the size of the output volume decreases
    further. However, in some cases, we might want to preserve almost all the information
    about the original input volume so that we can also extract the low-level features.
    In such scenarios, we pad the input volume with zeroes around the borders of the
    input volume.
  prefs: []
  type: TYPE_NORMAL
- en: This size of zero-padding is considered as a hyperparameter. It can be defined
    as a hyperparameter, which is directly used to control the spatial size of the
    output volume in scenarios where we want to exactly preserve the spatial size
    of input volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we apply a *5*5*3* filter to a *32*32*3* input volume, the
    output volume will reduce to *28*28*3*. However, let''s say we want to use the
    same Convolutional layer, but need to keep the output volume to *32*32*3*. We
    will use a zero-padding of size *2* to this layer. This will give us an output
    volume of *36*36*3*, as shown in the following figure. Now, if we apply three
    convolution layers with a *5*5*3* filter, it will produce an output volume of
    *32*32*3*, hence maintaining the exact spatial size of the input volume. *Figure
    3.9* represents the pictorial views of the scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Zero-padding](img/image_03_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: The input volume has a dimension of 32*32*3\. The two borders of
    zeros will generate an input volume of 36*36*3\. Further application of the Convolution
    layer, with three filters of size 5*5*3, having stride 1, will result in an output
    volume of 32*32*3.'
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical formulation of hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This part of the chapter will introduce an equation to calculate the spatial
    size of the output volume based on the hyperparameters that we have discussed
    so far. The equation is extremely useful to choose the hyperparameter for the
    CNN, as these are the deciding factors to 'fit' the neurons in the network. The
    spatial size of the output volume can be written as a function of the input volume
    size (*W*), the receptive field size, or the filter size of the Convolutional
    layer neurons(*K*), value of the applied stride(*S*), and the amount of zero-padding
    used (*P*) on the border.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation to compute the spatial size of output volume can be written as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mathematical formulation of hyperparameters](img/image_03_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering the examples given in *Figure 3.7* and *Figure 3.8*, where *W=7*,
    *K=3*, and with no padding, *P =0*. For stride *1*, we have *S=1*, and this will
    give the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mathematical formulation of hyperparameters](img/image_03_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly for stride **2**, the equation will give a value of **2**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mathematical formulation of hyperparameters](img/image_03_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, as shown in *Figure 3.7*, we would get an output of a spatial size of
    **3**. However, with this configuration, when a stride of **3 **is applied, it
    will not fit across the input volume, as this equation will return a fractional
    value **2.333** for the output volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mathematical formulation of hyperparameters](img/image_03_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This also signifies that the values of the hyperparameter have mutual constraints.
    The preceding example returns a fractional value, hence, the hyperparameters would
    be considered as invalid. However, we might resolve the issue by adding some zero-padding
    around the border.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The spatial arrangements of hyperparameters have mutual constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Effect of zero-padding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned in the zero-padding section, its main purpose is to preserve the
    information of input volume to the next layer. To ensure same spatial size of
    input and output volume, the conventional formula for zero-padding, with a stride,
    *S=1*, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of zero-padding](img/image_03_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Taking the example given in *Figure 3.9*, we can verify the authenticity of
    the formula. In the example, *W = 32*, *K=5*, and *S=1*. Therefore, to ensure
    the spatial output volume to be equal to 32, we choose the number of zero-padding
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of zero-padding](img/image_03_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, with *P=2*, the spatial size of the output volume is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of zero-padding](img/image_03_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, this equation worked out well to preserve the same spatial dimension for
    the input volume and output volume.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU (Rectified Linear Units) layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the convolution layer, the system basically computes the linear operations
    by doing element-wise multiplication and summations. Deep convolution usually
    performs the convolution operations followed by a non-linear operation after each
    layer. This is essential, because cascading linear operations produce another
    linear system. Adding non-linearity in between the layers corroborates a more
    expressive nature of the model than a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, after each convolution layer, an activation layer is applied on the
    current output. So, the main objective of this activation layer is to introduce
    some non-linearity to the system. Modern CNNs use **Rectified Linear Unit** (**ReLu**)
    as the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In artificial neural networks, the activation function, the rectifier, is defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ReLU (Rectified Linear Units) layers](img/image_03_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *x* is the input to a neuron.
  prefs: []
  type: TYPE_NORMAL
- en: A unit operating the rectifier is termed as ReLU. Earlier, many non-linear functions
    such as *tan h*, sigmoid, and the like were used in the network, but in the last
    few years, researchers have identified that ReLU layers work much better, because
    they help the network to train a lot faster, without compromising the accuracy
    of the outcome. A significant improvement in the computational efficiency is a
    major factor for this.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, this layer enhances the non-linear properties of the model and
    other overall networks without having any impact on the receptive fields of the
    Convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, in 2013, Mass et al. [94] introduced a new version of non-linearity,
    termed as leaky-ReLU. Leaky-ReLU can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ReLU (Rectified Linear Units) layers](img/image_03_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *Î±* is a predetermined parameter. Later, in 2015, He et al [95] updated
    this equation by suggesting that the parameter Î± can also be trained, which leads
    to a much-improved model.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of ReLU over the sigmoid function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ReLU helps to alleviate the vanishing gradient problem, which is explained
    in detail in [Chapter 1](ch01.html "Chapter 1. Introduction to Deep Learning")
    , *Introduction to Deep Learning*. ReLU applies the aforementioned function *f(x)*
    to all the values of the input volume, and transforms all the negative activations
    to **0**. For max function, the gradient is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Advantages of ReLU over the sigmoid function](img/image_03_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: However, for the Sigmoid function, the gradient tends to vanish as we increase
    or decrease the value of *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Sigmoid function is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Advantages of ReLU over the sigmoid function](img/image_03_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: "The Sigmoid function has a range of [*0, 1*], whereas the ReLU function has\
    \ the range [*0, ![Advantages of ReLU over the sigmoid function](img/Infinity.jpg)\
    \ \x88\x9E*]. Therefore, the Sigmoid function is applied to model the probability,\
    \ while ReLU can be used to model all the positive numbers."
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This layer is the third stage of a CNN. After applying some Rectified Linear
    Units later, the programmer might choose to apply a Pooling layer. The layer can
    also be referred to as a down-sampling layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pooling function is basically used to further modify the output of the
    layer. The primary function of the layer is to replace the output of the network
    at a certain location with a summarized statistics of the neighboring outputs.
    There are multiple options for this layer, Max-pooling being the most popular
    one. Max pooling operation [93] operates within a rectangular neighborhood, and
    reports the maximum output from it. Max-pooling basically takes a filter (generally
    of size *2x2*) and stride of the same length, that is, **2**. The filter is then
    applied to the input volume, and it outputs the maximum number in every region
    where the filter convolves around. *Figure 3.10* shows a representation of the
    same thing. Other popular options for Pooling layers are the average of an *L2*
    normal of a rectangular neighborhood, average of a rectangular neighborhood or
    a weighted average, which is based on the distance from the central pixel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pooling layer](img/B05883_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Example of Max-pool with a 2*2 filter and stride 2\. Image sourced
    from Wikipedia.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Invariance to local translation is extremely beneficial if we are interested
    in the neighboring features, rather than the exact position of the feature.
  prefs: []
  type: TYPE_NORMAL
- en: Where is it useful, and where is it not?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The intuitive reason behind the Pooling layer is that once some specific feature
    of the original input volume is known, its exact location becomes trivial as compared
    to its location relative to the other features. With the help of pooling, the
    representation becomes almost invariant to small translations of the input. Invariance
    to translation signifies that for a small amount of translation on the input,
    the values of most of the pooled output do not vary significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Invariance to local translation is extremely beneficial if we are interested
    in the neighboring features rather than the exact position of the feature. However,
    while dealing with computer vision tasks, it is required to be careful in the
    use of Pooling layer. Although pooling extensively helps in the reduction of complexity
    of the model, it might end up losing the location sensitivity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Let us take an example of image processing, which involves identifying a box
    in an image. Pooling layer in this case will help if we simply target to determine
    the existence of the box in the image. However, if the problem statement is more
    concerned with locating the exact position of the box, we will have to be careful
    enough while using the Pooling layer. As another example, let us say we are working
    on a language model, and are interested in identifying the contextual similarity
    between two words. In this case, the use of Pooling layer is not advisable, as
    it will lose out on some valuable feature information.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it can be concluded that Pooling layer is basically used for reducing
    the computational complexity of the model. The Pooling layer is more like an averaging
    process, where we are more interested in a group of neighboring features. The
    layer can be applied in scenarios where we can afford to let go of some of the
    localized information.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fully connected layer is the final layer of a CNN. The input volume for
    this layer comes from the output of the preceding Convolutional layer, ReLU, or
    Pooling layer. The fully connected layer takes this input and outputs an *N* dimensional
    vector, where *N* is the number of different classes present in the initial input
    datasets. The basic idea on which a fully connected layer works is that it works
    on the output received from the preceding layer, and identifies the specific feature
    that mostly correlates to a particular class. For example, if the model is predicting
    whether an image contains a cat or bird, it will have high values in the activation
    maps, which will represent some high-level features such as four legs or wings,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed deep CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section of the chapter will introduce some extremely aggressive deep CNN architecture,
    associated challenges for these networks, and the need of much larger distributed
    computing to overcome this. This section will explain how Hadoop and its YARN
    can provide a sufficient solution for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Most popular aggressive deep neural networks and their configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CNNs have shown stunning results in image recognition in recent years. However,
    unfortunately, they are extremely expensive to train. In the case of a sequential
    training process, the convolution operation takes around 95% of the total running
    time. With big datasets, even with low-scale distributed training, the training
    process takes many days to complete. The award winning CNN, AlexNet with ImageNet
    in 2012, took nearly an entire week to train on with two GTX 580 3 GB GPUs. The
    following table displays few of the most popular distributed deep CNNs with their
    configuration and corresponding time taken for the training process to complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Models** | **Computing power** | **Datasets** | **Number of depth** | **Time
    taken for the training process** |'
  prefs: []
  type: TYPE_TB
- en: '| AlexNet | Two NVIDIA GTX 580 3 GB GPUs | Trained the network on ImageNet
    data, which contained over 15 million high-resolution images from a total of over
    22,000 categories. | eight layers | Five to six days. |'
  prefs: []
  type: TYPE_TB
- en: '| ZFNet [97] | GTX 580 GPU | 1.3 million images, spread over 1000 different
    classes. | eight layers | Twelve days. |'
  prefs: []
  type: TYPE_TB
- en: '| VGG Net [98] | 4 Nvidia Titan Black GPUs | The dataset includes images of
    1000 classes, and is split into three sets: training (1.3 M images), validation
    (50 K images), and testing (100 K images with held out class labels). | 19 layers
    | Two to three weeks. |'
  prefs: []
  type: TYPE_TB
- en: '| GoogLeNet [99] | A few high-end GPUs | 1.2 millionimages for training. |
    The network is 22 layers deep when counting only layers with parameters (or 27
    layers if we also count pooling). The overall number of layers (independent building
    blocks) used for the construction of the network is about 100. | Within a week.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Microsoft ResNet [100] | 8 GPU machine | Trained on the 1.28 million training
    images, and evaluated on the 50k validation images. | 152 layers. | Two to three
    weeks. |'
  prefs: []
  type: TYPE_TB
- en: Training time - major challenges associated with deep neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the preceding table, it can be surely inferred that there have been loads
    of efforts made by researchers to increase the accuracy of the outcome. One key
    point that comes out from the table is that the numbers of layers have been one
    of the major criteria to improve the accuracy. Microsoft's Resnet uses a neural
    network, as deep as 152 layers, which turns to be an extremely aggressive deep
    neural network. This architecture has set many new records in 2015 in classification,
    localization, and detection through the deep CNN. Apart from this, ResNet has
    also won the ILSVRC 2015 with an incredible improvement in the error rate of only
    3.6%.
  prefs: []
  type: TYPE_NORMAL
- en: Although deep convolutional networks are almost about to reach the expected
    mark of accuracy, the major concern in almost all these deep CNNs is the rightmost
    column of the table. Hence, it shows that the current challenge for training deep
    CNNs is to build a large-scale distributed framework to parallelize the training
    across many CPUs and GPUs over a fast interconnected network.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop for deep CNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explain how Hadoop can be used to distribute the deep
    models at a large scale for faster processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The running times of CNNs can be divided into two major categories:'
  prefs: []
  type: TYPE_NORMAL
- en: All the Convolutional layers present in the network consume around 90-95% of
    the computation. They use approximately 5% of the parameters, and have large representations.
    [101]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest of the computation, around 5-10%, is taken by the fully connected layers.
    They use almost 95% of the parameters, and have small representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alex Krizhevsky in [101] has proposed an algorithm for training of CNN using
    distributed architecture. In a traditional CNN, the convolution operation itself
    consumes nearly the entire running time of the whole process; hence, data parallelism
    should be used for faster training. However, for a fully connected layer, it is
    advisable to use the model parallelism approach. We'll explain the algorithm using
    Hadoop and its YARN in this section of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Hadoop, the distributed system workers sit on each of the blocks of HDFS,
    and process the data synchronously in parallel. We will use a small batch size
    of 1024 numbers of examples from the raw input image, which will be split into
    N multiple blocks of **Hadoop Distributed File System** (**HDFS**). So, total
    *N* workers will be working for each small batch of data. The block size of HDFS
    would be kept as size *K*. Now, what should be the size of *K*? Although, a small
    size of *K* will increase the number of blocks and help to make the training even
    faster, a large number of *N* will also eventually increase the volume of metadata
    that resides in the NameNode. One major drawback in this case, is **Single Point
    of Failure** (**SPOF**) for Hadoop [81], which is more prone to smaller size of
    main memory for NameNode. However, with a bigger value of *K*, we will get a small
    number of blocks of HDFS, and hence, the number of workers working in parallel
    will be lesser in number. This will again make the training process slower. So,
    a better approach to choose the size of *K* will primarily depend on the following
    three factors:'
  prefs: []
  type: TYPE_NORMAL
- en: The availability of the primary memory's size of your NameNode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the input batch and the complexity of operations performed on each
    block of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How important or valuable your intermediate outcome of the data is. Based on
    these criteria, we can set the replication factor. However, the higher the replication
    factor, the higher the load of the NameNode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The blocks of HDFS are distributed across all the DataNodes of Hadoop where
    YARN will operate directly on them in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps for distributed training of the Convolutional layer are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Each of the *N* blocks is given a different small data batch of 1024 examples
    from the raw input image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The same size of filter and stride is applied on each of the N numbers of blocks,
    which results in individual spatial output based on the values of the inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ReLU is applied on all of these, synchronously, in parallel to get some non-linearity
    in the outcome.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Max-pooling, or any other down-sampling algorithm, if desired, is applied on
    these separate chunks of data based on the necessity of the outcome.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The outputs (transformed parameters) for each iteration of the *N* blocks are
    sent back to the master-node called Resource Manager, where their parameters are
    averaged. The newly updated parameter is sent back to each of the *N* blocks to
    perform the actions again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps 2 to 5 are repeated for a predefined number of epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a Fully-connected layer, one of the *N* number of workers working on any
    of the *N* blocks of data for a small batch of input image will send the last-stage
    convolutional activities to all other (*N-1*) numbers of workers. The workers
    will then perform the fully-connected operation on this batch of 1024 examples,
    then initiate to back-propagate the gradients for these 1024 examples. The next
    worker, in parallel to this operation, will then send its last-stage Convolutional
    layer activities to the other workers similar to the earlier situation. The workers
    will again work on the fully-connected activities for the second batch of 1024
    examples. The process will iterate until we get the outcome with the desired minimal
    error.
  prefs: []
  type: TYPE_NORMAL
- en: In this method, the workers broadcast their last-stage Convolutional layer's
    information to all other workers. The main benefit of this approach is that a
    large proportion (*(N-1)/N*) of the communication can be suppressed, and it can
    be run in parallel with the calculation of the Fully-connected layer. The approach
    is extremely advantageous in terms of communication of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is very clear that Hadoop can be highly beneficial for providing
    a distributed environment for a CNN with the help of HDFS and Hadoop YARN.
  prefs: []
  type: TYPE_NORMAL
- en: So, now that we are familiar with the approach of distributing the model in
    parallel with Hadoop, the next part of the section will discuss the coding part
    that each of the workers will be operating on each block of HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layer using Deeplearning4j
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section of the chapter will provide the basic idea on how to write the
    code for CNN using Deeplearning4j. You'll be able to learn the syntax for using
    the various hyperparameters mentioned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement CNN using Deeplearning4j, the whole idea can be split into three
    core phases: loading data or preparation of the data, network configuration, and
    training and evaluation of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Loading data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: "For CNNs, generally, we only work on the image data to train the model. In\
    \ Deeplearning4j, the images can be read using `ImageRecordReader`. The following\
    \ code snippet shows how to load *16Ã\x9716* color images for the model:"
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, using `CSVRecordReader`, we can load all the image labels from
    the input CSV files, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To combine both the images and labels data, we can use `ComposableRecordReader`.
    `ComposableRecordReader` can also be beneficial in cases where we need to merge
    data from multiple sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, instead of imageset, in some cases, if someone needs to load MNIST
    datasets into the model; for that, we can use the following part. This example
    uses a random number seed of `12345`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Model configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next part of the operation is to configure the CNN. Deeplearning4j provides
    a simple builder to define the deep neural network layer by layer, setting the
    different desired hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The first layer, Convolutional layer, can be called using the `ConvolutionLayer.Builder`
    method. The `.build()` function is used to build the layer. `.stride()` is used
    to set the amount of stride for this Convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`nIn` and `nOut` signify the depth. `nIn` here is `nChannels`, and `nOut` is
    the number of filters to be applied for the convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To add the identity function as the activation function, we will define it
    in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To add a Pooling layer of type Max-pooling, we will call the `SubsamplingLayer.Builder()`
    method after the first layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The **Rectifier Linear Unit** (**ReLU**) layer can be added by calling new
    `DenseLayer.Builder().activation("relu")`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The model can be initialized by calling the `init()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Training and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned in the earlier section, for the training part, we need to divide
    the whole big dataset into a number of batches. The model will then work on those
    batches one by one in Hadoop. Let''s say we divide the dataset into 5,000 batches,
    each batch of size 1024 examples. The 1024 examples will then split into multiple
    numbers of blocks where the workers will work in parallel. The split operation
    of the big dataset is done using the `RecordReaderDataSetIterator()` method. Let''s
    first initialize the parameters needed to call the method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let the total number of classes in the image be `10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, as we have set the number of parameters for `RecordReaderDataSetIterator()`,
    we can call the method to set up the training platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the training phase, we can randomly split the batch into train and test
    datasets. If we want 70 samples for the training set and the rest 30 for the test
    set, we can set this configuration by using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When the model is fully trained, for each batch, the test data can be saved
    so as to validate the model. Hence, by defining only one object of the `Evaluation`
    class, we will be able to collect the statistics of the entire dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is now completely ready to be trained. This can be done by calling
    the `fit()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs, although not a new concept, has gained immense popularity in the last
    half a decade. The network primarily finds its application in the field of vision.
    The last few years have seen some major research on CNN by various technological
    companies such as Google, Microsoft, Apple, and the like, and also from various
    eminent researchers. Starting from the beginning, this chapter talked about the
    concept of convolution, which is the backbone of this type of network. Going forward,
    the chapter introduced the various layers of this network. Then it provided in-depth
    explanations for every associated layer of the deep CNN. After that, the various
    hyperparameters and their relations with the network were explained, both theoretically
    and mathematically. Later, the chapter talked about the approach of how to distribute
    the deep CNN across various machines with the help of Hadoop and its YARN. The
    last part discussed how to implement this network using Deeplearning4j for every
    worker working on each block of Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss another popular deep neural network called
    recurrent neural network. Recurrent neural network has recently gained immense
    popularity mainly for its ability to model the sequences of variable length. Till
    now, this network is successfully implemented in different problems such as language
    modeling, handwriting recognition, speech recognition, and so on.
  prefs: []
  type: TYPE_NORMAL
