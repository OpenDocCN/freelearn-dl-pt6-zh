- en: Deep Neural Networks for Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we worked with a dataset that had a categorical target
    variable, and we went over the steps for developing a classification model using
    Keras. In situations where the response variable is numeric, supervised learning
    problems are categorized as regression problems. In this chapter, we will develop
    a prediction model for numeric response variables. To illustrate the process of
    developing the prediction model, we will make use of the Boston Housing dataset,
    which is available within the `mlbench` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Boston Housing dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and fitting a deep neural network model for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation and prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Boston Housing dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use six libraries. These libraries are as listed in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The structure of the `BostonHousing` data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the preceding output, this dataset has `506` observations
    and `14` variables. Out of the 14 variables, 13 are numeric and 1 variable (`chas`)
    is of the factor type. The last variable, `medv` (the median value of owner-occupied
    homes in thousand-USD units), is the dependent, or target, variable. The remaining
    13 variables are independent. The following is a brief description of all the
    variables, drawn up in a table for easy reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Variables** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `crim` | Per-capita crime rate by town |'
  prefs: []
  type: TYPE_TB
- en: '| `zn` | Proportion of residential land zoned for lots over 25,000 sq ft |'
  prefs: []
  type: TYPE_TB
- en: '| `indus` | Proportion of nonretail business acres per town |'
  prefs: []
  type: TYPE_TB
- en: '| `chas` | Charles River dummy variable (1 if the tract bounds a river; 0 otherwise)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nox` | Nitric-oxides concentration (parts per 10 million) |'
  prefs: []
  type: TYPE_TB
- en: '| `rm` | Average number of rooms per dwelling |'
  prefs: []
  type: TYPE_TB
- en: '| `age` | Proportion of owner-occupied units built prior to 1940 |'
  prefs: []
  type: TYPE_TB
- en: '| `dis` | Weighted distances to five Boston employment centers |'
  prefs: []
  type: TYPE_TB
- en: '| `rad` | Index of accessibility to radial highways |'
  prefs: []
  type: TYPE_TB
- en: '| `tax` | Full-value property-tax rate per 10,000 USD |'
  prefs: []
  type: TYPE_TB
- en: '| `ptratio` | Pupil–teacher ratio by town |'
  prefs: []
  type: TYPE_TB
- en: '| `lstat` | Percentage of lower-income status members of the population |'
  prefs: []
  type: TYPE_TB
- en: '| `medv` | Median value of owner-occupied homes in thousand-USD units |'
  prefs: []
  type: TYPE_TB
- en: 'This data is based on the 1970 census. A detailed statistical study using this
    data was published by Harrison and Rubinfeld in 1978 (reference: [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.926.5532&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.926.5532&rep=rep1&type=pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start by changing the name of the `BostonHousing` data to simply `data` for
    ease of use. Independent variables that are of the factor type are then converted
    to the numeric type using the `lapply` function.
  prefs: []
  type: TYPE_NORMAL
- en: Note that for this data, the only factor variable is `chas`; however, for any
    other dataset with more factor variables, this code will work fine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, after converting factor variables to the `numeric` type,
    we also change the format of `data` to `data.frame`.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To visualize a neural network with hidden layers, we will use the `neuralnet`
    function. For illustration, two hidden layers with 10 and 5 units will be used
    in this example. The input layer has 13 nodes based on 13 independent variables.
    The output layer has only one node for the target variable, `medv`. The code used
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, the result is saved in `n`, and it is then
    used for plotting the architecture of the neural network, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/411058c3-98fe-4dd2-b54c-865c61368359.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from the preceding diagram, the input layer has 13 nodes for
    13 independent variables. There are two hidden layers: the first hidden layer
    has 10 nodes and the second hidden layer has 5 nodes. Each node in the hidden
    layer is connected to all the nodes in the previous and the following layer. The
    output layer has one node for the response variable, `medv`.'
  prefs: []
  type: TYPE_NORMAL
- en: Data partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we change the data into a matrix format. We also set dimension names
    to `NULL`, which changes the names of the variables to the default names, `V1`,
    `V2`, `V3`, ..., `V14`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We then the partition data into training and test datasets using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: A data split of 70:30 is used in this example. To maintain the repeatability
    of the data split, we use a random seed of `1234`. This will allow the same samples
    to be included in the training and test data each time data partitioning is carried
    out on any computer. The data for the independent variables are stored in `training`
    for the training data and in `test` for the test data. Similarly, the data for
    the dependent variable, `medv`, based on the corresponding split data, are stored
    in `trainingtarget` and `testtarget`.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To normalize the data, the mean and standard deviations are obtained for all
    independent variables in the training data. Normalization is then carried out
    using the `scale` function:'
  prefs: []
  type: TYPE_NORMAL
- en: For both the train and test data, the mean and standard deviations are based
    on the training data used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the data preparation step for this data. It should be noted that
    different datasets may need extra steps that are unique to that dataset—for example,
    many large datasets may have very high amounts of missing data values, and they
    may require additional data preparation steps in the form of arriving at a strategy
    for handling missing values and inputting missing values wherever necessary.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will create a deep neural network architecture and then
    fit a model for the accurate prediction of the numeric target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and fitting a deep neural network model for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create and fit a deep neural network model for a regression problem, we
    will make use of Keras. The code used for the model architecture is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the input layer having 13 units and the output layer having 1 unit
    is fixed based on the data; however, to arrive at a suitable number of hidden
    layers and the number of units in each layer, you need to experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding code, we use the `keras_model_sequential`
    function to create a sequential model. The structure of the neural network is
    defined using the `layer_dense` function. Since there are 13 independent variables,
    `input_shape` is used to specify 13 units. The first hidden layer has `10` units
    and the rectified linear unit, or `relu`, is used as the activation function in
    this first hidden layer. The second hidden layer has `5` units, with `relu` as
    the activation function. The last, `layer_dense`, has `1` unit, which represents
    one dependent variable, `medv`. Using the `summary` function, you can print a
    model summary, which shows 201 total parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the total number of parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now see how a total of 201 parameters are obtained for this model. The
    `dense_1` layer shows `140` parameters. These parameters are based on there being
    13 units in the input layer that connect with each of the 10 units in the first
    hidden layer, meaning that there are 130 parameters (13 x 10). The remaining 10
    parameters come from the bias term for each of the 10 units in the first hidden
    layer. Similarly, 50 parameters (10 x 5) are from the connections between two
    hidden layers and the remaining 5 parameters come from the bias term from each
    of the 5 units in the second hidden layer. Finally, `dense_3` has `6` parameters
    ((5 x 1) + 1). Thus, in all, there are 201 parameters based on the architecture
    of the neural network model that was chosen in this example.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After the model architecture is defined, the model is compiled to configure
    the learning process using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, we define the loss function as the mean square
    error, or `mse`. At this step, the `rmsprop` optimizer and mean absolute error,
    or `mae`, metric is also defined. We choose these because our response variable
    is numeric.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, the model is trained using the `fit` function. Note that, as the training
    of the model proceeds, we get a visual as well as a numerical summary after each
    epoch. The output from the last three epochs is shown in the following code. We
    get the mean absolute error and loss values for both the training and the validation
    data. Note that, as pointed out in [Chapter 1](db6a812d-2bad-4f40-9e99-0e20abbe665c.xhtml),
    *Revisiting Deep Learning Architecture and Techniques*, each time we train a network,
    the training and validation errors can vary because of the random initialization
    of network weights. Such an outcome is expected even when the data is partitioned
    using the same random seed. To obtain repeatable results, it is always a good
    idea to save the model using the `save_model_hdf5` function and then reload it
    when needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code used for training the network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the preceding code, the model is trained in small batches
    of size `32`, and 20% of the data is reserved for validation to avoid overfitting.
    Here, `100` epochs or iterations are run to train the network. Once the training
    process is completed, information related to the training process is saved in
    `model_one`, which can then be used to plot the loss and mean absolute error based
    on the training and validation data for all epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code will return the following output. Let''s have a
    look at the loss and mean absolute error for training and validation data (`model_one`) plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37680ec9-e55b-4c9a-9f21-607ae941db74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The `mae` and `loss` values decrease for both the training and validation data
    as the training proceeds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rate of decrease in errors for the training data reduces after about 60
    epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After developing the prediction model, we can assess its performance by evaluating
    the prediction quality of the model, which we will look at in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation and prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model evaluation is an important step in the process of arriving at a suitable
    prediction model. A model may show good performance with training data that was
    used for developing the model; however, the real test of a model is with data
    that the model has not yet seen. Let's look at the model performance based on
    the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The performance of the model is evaluated using the `evaluate` function with
    the help of the test data shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding output, we can see that the loss and mean absolute error
    for the test data are `31.15` and `3.61` respectively. We will use these numbers
    later to compare and assess whether or not the changes that we will make to the
    current model help to improve the prediction performance.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s predict the `medv` values for the `test` data and store the results
    in `pred` using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can take a look at the first 10 predicted and actual values using the `cbind`
    function. The first column in the output shows the predicted values based on the
    model and the second column shows the actual values. We can make the following
    observations from the output:'
  prefs: []
  type: TYPE_NORMAL
- en: The prediction for the first sample in the test data is about `33.19` and the
    actual value is `36.2`. The model underestimates the response by about `3` points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the second sample, the model underestimates the response by over `2` points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the tenth sample, the predicted and actual values are very close.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the sixth sample, the model overestimates the response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To get an overall picture of the prediction performance, we can develop a scatter
    plot of the predicted versus the actual values. We will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The scatter plot shows the predicted versus the actual response values based
    on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16a25a48-a15a-4d4f-9ac8-ce7dd907adc3.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding graph, we can see the overall performance of the prediction
    model. The relationship between the actual and predicted values is positive and
    approximately linear. Although we can see that the model has decent performance,
    clearly there is scope for further improvement that makes data points closer to
    the ideal line that has zero intercepts and a slope of 1\. We will further explore
    making improvements to the model by developing a deeper neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: Improvements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the modified new model, we will build a deeper network by adding more layers.
    The additional layers are expected to show patterns in the data that the smaller
    network we used earlier was not able to show.
  prefs: []
  type: TYPE_NORMAL
- en: Deeper network architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code used for this experiment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can observe that we now have three hidden layers
    with `100`, `50`, and `20` units respectively. We have also added a dropout layer
    after each hidden layer with rates of `0.4`, `0.3`, and `0.2` respectively. As
    an example of what a dropout layer's rate means, a rate of 0.4 means that 40%
    of the units in the first hidden layer are dropped to zero at the time of training,
    which helps to avoid overfitting. The total number of parameters in this model
    has now increased to `7,491`. Note that, in the previous model, the total number
    of parameters was `201`, and clearly we are going for a significantly bigger neural
    network. Next, we compile the model with the same settings that we used earlier,
    and subsequently, we will fit the model and store the results in `model_two`.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following figure provides the loss and mean absolute error for `model_two`
    over 100 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b48f8055-750e-4560-9978-92c2fe0abfe3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding figure, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The mean absolute error and loss values for the training and validation data
    drop very quickly to low values, and after about 30 epochs, we do not see any
    major improvement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no evidence of overfitting as the training and validation errors seem
    closer to each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can obtain the loss and mean absolute error values for the test data using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The loss and mean absolute error values using the `test` data and `model_two`
    are obtained as `24.70` and `3.02` respectively. This is a significant improvement
    compared to the results that we obtained from `model_one`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visually see this improvement using the scatter plot for the predicted
    values versus the actual response values in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cd957b8-3c86-4bc4-bdc7-ab0ae18ccb17.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding graph, we can see that the spread in the scatter plot of
    actual versus predicted values is visibly less than that of the earlier scatter
    plot. This indicates better prediction performance compared to the previous model.
    Although `model_two` performs better than the previous model, at higher values,
    we can see the occurrence of significant underestimation of the target values.
    So, although we have developed a better model, we can also further explore the
    potential for the further improvement of this prediction model.
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Improving model performance can involve different strategies. Here, we will
    discuss two main strategies. One strategy is to make changes to the model architecture
    and observe the results to get any useful insights or indications of improvement.
    Another strategy could involve exploring the transformation of the target variable.
    In this section, we will try a combination of both of these strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Log transformation on the output variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To overcome the issue of significant underestimation of the target variable at
    higher values, let''s try log transformation on the target variable and see whether
    or not this helps us to further improve the model. Our next model has some minor
    changes to the architecture as well. In `model_two`, we did not notice any major
    issue or evidence related to overfitting, and as a result, we can increase the
    number of units a little and also slightly reduce the percentages for dropout.
    The following is the code for this experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We will increase the number of units in the third hidden layer from `20` to
    `25`. Dropout rates for the second and third hidden layers are also reduced to
    `0.2` and `0.1` respectively. Note that the overall number of parameters has now
    increased to `7751`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We next compile the model and then fit the model. The model results are stored
    in `model_three`, which we use for plotting the graph, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following shows the output of the loss and mean absolute error for training
    and validation data (`model_three`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/840b6abb-532f-41be-9f1d-880f25acd57d.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see from the preceding plot that although the values in the plot are
    not directly comparable to earlier figures because of the log transformation,
    we can see that the overall errors decrease and become stable after about 50 epochs
    for both the mean absolute error and the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We also obtain `loss` and `mae` values for this new model, but again, the numbers
    obtained are not directly comparable to the earlier two models for the log scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain a scatter plot of the actual values (log transformed) versus the
    predicted values based on the test data. We also get a scatter plot of the actual
    versus predicted values in the original scale for comparison with earlier plots.
    The scatter plots for predicted versus actual response values (`model_three`) are
    as shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70341185-4756-4a9d-a62c-46d2eaf4dafb.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding graph, we can see that the significant underestimation pattern
    observed in earlier models shows improvement, both in the log scale and in the
    original scale. In the original scale, the data points at higher values are relatively
    closer to the diagonal line, indicating improved prediction performance by the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through the steps for developing a prediction model
    when the response variable is of a numeric type. We started with a neural network
    model that had 201 parameters and then developed deep neural network models with
    over 7,000 parameters. You may have noticed that, in this chapter, we made use
    of comparatively deeper and more complex neural network models compared to the
    previous chapter, where we developed a classification model for the target variable
    that was of a categorical nature. In both [Chapter 2](c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml), *Deep
    Neural Networks for Multiclass Classification*, and [Chapter 3](07c9aa4a-1c93-490a-bfcd-7c4bcde639d5.xhtml),
    *Deep Neural Networks for Regression*, we developed models based on data that
    was structured. In the next chapter, we move on to problems where the data type
    is unstructured. More specifically, we'll deal with the image type of data and
    go over the problem of image classification and recognition using deep neural
    network models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover the steps required to develop an image recognition
    and prediction model using deep neural networks.
  prefs: []
  type: TYPE_NORMAL
