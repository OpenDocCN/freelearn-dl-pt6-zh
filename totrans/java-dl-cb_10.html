<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Developing Applications in a Distributed Environment</h1>
                </header>
            
            <article>
                
<p><span>As the demand increases regarding the quantity of data and resource requirements for parallel computations, legacy approaches may not perform well. So far, we have seen how big data development has become famous and is the most followed approach by enterprises due to the same reasons. DL4J supports neural network training, evaluation, and inference on distributed clusters.</span></p>
<p>Modern approaches to heavy training, or output generation tasks, distribute training effort across multiple machines. This also brings additional challenges. We need to ensure that we have the following constraints checked before we use Spark to perform distributed training/evaluation/inference:</p>
<ul>
<li>Our data should be significantly large enough to justify the need for distributed clusters. Small network/data on Spark doesn't really gain any performance improvements and local machine execution may have much better results in such scenarios.</li>
<li>We have more than a single machine to perform training/evaluation or inference.</li>
</ul>
<p>Let's say we have a single machine with multiple GPU processors. We could simply use a parallel wrapper rather than Spark in this case. A parallel wrapper enables parallel training on a single machine with multiple cores. Parallel wrappers will be discussed in <a href="0db31248-e40b-4479-9939-0baccb0e11d1.xhtml" target="_blank">Chapter 12</a>, <em>Benchmarking and Neural Network Optimization</em>, where you will find out how to configure them. Also, if the neural network takes more than 100 ms for one single iteration, it may be worth considering distributed training.</p>
<p class="mce-root"/>
<p><span>In this chapter, we will discuss how to configure DL4J for distributed training, evaluation, and inference. We will develop a distributed neural network f</span>or the <kbd>TinyImageNet</kbd> cla<span>ssifier.</span> In this chapter, we will cover the following recipes:</p>
<ul>
<li>Setting up DL4J and the required dependencies</li>
<li>Creating an uber-JAR for training</li>
<li>CPU/GPU-specific configuration for training</li>
<li>Memory settings and garbage collection for Spark</li>
<li>Configuring encoding thresholds</li>
<li>Performing a distributed test set evaluation</li>
<li>Saving and loading trained neural network models</li>
<li>Performing distributed inference</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>The source code for this chapter can be found at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app</a>.</p>
<p>After cloning our GitHub repository, navigate to the <kbd>Java-Deep-Learning-Cookbook/10_Developing_applications_in_distributed_environment/sourceCode</kbd> directory. Then, import the <kbd>cookbookapp</kbd> project as a Maven project by importing the <kbd>pom.xml</kbd> file.</p>
<p>You need to run either of the following preprocessor scripts (<kbd>PreProcessLocal.java</kbd> or <kbd>PreProcessSpark.java</kbd>) before running the actual source code:</p>
<ul>
<li><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app/PreProcessLocal.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app/PreProcessLocal.java</a></li>
<li><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app/PreprocessSpark.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app/PreprocessSpark.java</a><span> </span></li>
</ul>
<div class="packt_infobox">These scripts can be found in the <kbd>cookbookapp</kbd> project.</div>
<p>You will also need the <kbd>TinyImageNet</kbd> dataset, which can be found at <a href="http://cs231n.stanford.edu/tiny-imagenet-200.zip">http://cs231n.stanford.edu/tiny-imagenet-200.zip</a>. The home page can be found at <a href="https://tiny-imagenet.herokuapp.com/">https://tiny-imagenet.herokuapp.com/</a>.</p>
<p>It is desirable if you have some prior knowledge of working with Apache Spark and Hadoop so that you get the most out of this chapter. Also, this chapter assumes that Java is already installed on your machine and has been added to your environment variables. We recommend Java version 1.8.</p>
<p>Note that the source code requires good hardware in terms of memory/processing power. We recommend that you have at least 16 GB of RAM on your host machine in case you're running the source on a laptop/desktop. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up DL4J and the required dependencies</h1>
                </header>
            
            <article>
                
<p><span>We are discussing setting up DL4J again because we are now dealing with a distributed environment. For demonstration purposes, we will use Spark's local mode. Due to this, we can focus on DL4J rather than setting up clusters, worker nodes, and so on. In this recipe, we will set up a single node Spark cluster (Spark local), as well as configure DL4J-specific dependencies. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In order to demonstrate the use of a distributed neural network, you will need the following:</p>
<ul>
<li>A distributed filesystem (Hadoop) for file management</li>
<li>Distributed computing (Spark) in order to process big data</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Add the following Maven dependency for Apache Spark:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/>    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;<br/>    &lt;version&gt;2.1.0&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<ol start="2">
<li style="color: black">Add the following Maven dependency for <kbd>DataVec</kbd> for Spark:</li>
</ol>
<pre style="color: black;padding-left: 60px"><span>&lt;dependency&gt;</span><br/> <span>   &lt;groupId&gt;org.datavec&lt;/groupId&gt;</span><br/> <span>   &lt;artifactId&gt;datavec-spark_2.11&lt;/artifactId&gt;</span><br/> <span>   &lt;version&gt;1.0.0-beta3_spark_2&lt;/version&gt;</span><br/><span>&lt;/dependency&gt;</span></pre>
<ol start="3">
<li>Add the following Maven dependency for parameter averaging:</li>
</ol>
<pre style="padding-left: 60px"><span>&lt;dependency&gt;</span><br/><span>    &lt;groupId&gt;org.datavec&lt;/groupId&gt;</span><br/><span>    &lt;artifactId&gt;datavec-spark_2.11&lt;/artifactId&gt;</span><br/><span>    &lt;version&gt;1.0.0-beta3_spark_2&lt;/version&gt;</span><br/><span>&lt;/dependency&gt;</span></pre>
<ol start="4">
<li>Add the following Maven dependency for gradient sharing:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/>    &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>    &lt;artifactId&gt;dl4j-spark-parameterserver_2.11&lt;/artifactId&gt;<br/>    &lt;version&gt;1.0.0-beta3_spark_2&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<ol start="5">
<li>Add the following Maven dependency for the ND4J backend:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/>    &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>    &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;<br/>    &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<p class="mce-root"/>
<ol start="6">
<li>Add the following Maven dependency for CUDA:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/>    &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>    &lt;artifactId&gt;nd4j-cuda-x.x&lt;/artifactId&gt;<br/>    &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<ol start="7">
<li>Add the following Maven dependency for JCommander:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/>    &lt;groupId&gt;com.beust&lt;/groupId&gt;<br/>    &lt;artifactId&gt;jcommander&lt;/artifactId&gt;<br/>    &lt;version&gt;1.72&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<ol start="8">
<li>Download Hadoop from the official w<span>ebsite at </span><a href="https://hadoop.apache.org/releases.html">https://hadoop.apache.org/releases.html</a> and add the required environment variables.</li>
</ol>
<p style="padding-left: 60px">Extract the downloaded Hadoop package and create the following environment variables:</p>
<pre style="padding-left: 60px">HADOOP_HOME = {PathDownloaded}/hadoop-x.x <br/> HADOOP_HDFS_HOME = {PathDownloaded}/hadoop-x.x <br/> HADOOP_MAPRED_HOME = {PathDownloaded}/hadoop-x.x <br/> HADOOP_YARN_HOME = {PathDownloaded}/hadoop-x.x </pre>
<p style="padding-left: 60px">Add the following entry to the <kbd>PATH</kbd> environment variable:</p>
<pre style="padding-left: 60px"><strong>${HADOOP_HOME}\bin</strong></pre>
<ol start="9">
<li>Create name/data node directories for Hadoop. Navigate to the Hadoop home directory (which is set in the <kbd>HADOOP_HOME</kbd> environment variable) and create a directory named <kbd>data</kbd>. Then, create two subdirectories named <kbd>datanode</kbd> and <kbd>namenode</kbd> underneath it. Make sure that access for read/write/delete has been provided for these directories. </li>
<li>Navigate to <kbd>hadoop-x.x/etc/hadoop</kbd> and open <kbd>hdfs-site.xml</kbd>. Then, add the following configuration:</li>
</ol>
<pre style="padding-left: 60px">&lt;configuration&gt;<br/>     &lt;property&gt;<br/>      &lt;name&gt;dfs.replication&lt;/name&gt;<br/>      &lt;value&gt;1&lt;/value&gt;<br/>     &lt;/property&gt;<br/>     &lt;property&gt;<br/>      &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;<br/>      &lt;value&gt;file:/{NameNodeDirectoryPath}&lt;/value&gt;<br/>     &lt;/property&gt;<br/>     &lt;property&gt;<br/>      &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;<br/>      &lt;value&gt;file:/{DataNodeDirectoryPath}&lt;/value&gt;<br/>     &lt;/property&gt;<br/>   &lt;/configuration&gt;</pre>
<ol start="11">
<li>Navigate to <kbd>hadoop-x.x/etc/hadoop</kbd> and open <kbd>mapred-site.xml</kbd>. Then, add the following configuration:</li>
</ol>
<pre style="padding-left: 60px">&lt;configuration&gt;<br/>  &lt;property&gt;<br/>   &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br/>   &lt;value&gt;yarn&lt;/value&gt;<br/>  &lt;/property&gt;<br/> &lt;/configuration&gt;</pre>
<ol start="12">
<li>Navigate to <kbd>hadoop-x.x/etc/hadoop</kbd> and open <kbd>yarn-site.xml</kbd>. Then, add the following configuration:</li>
</ol>
<pre style="padding-left: 60px">&lt;configuration&gt;<br/>  &lt;!-- Site specific YARN configuration properties --&gt;<br/>  &lt;property&gt;<br/>   &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br/>   &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>   &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt;<br/>   &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;<br/>  &lt;/property&gt;<br/> &lt;/configuration&gt;</pre>
<ol start="13">
<li>Navigate to <kbd>hadoop-x.x/etc/hadoop</kbd> and open <kbd>core-site.xml</kbd>. Then, add the following configuration:</li>
</ol>
<pre style="padding-left: 60px">&lt;configuration&gt;<br/>  &lt;property&gt;<br/>   &lt;name&gt;fs.default.name&lt;/name&gt;<br/>   &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;<br/>  &lt;/property&gt;<br/> &lt;/configuration&gt; </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="14">
<li>Navigate to <kbd>hadoop-x.x/etc/hadoop</kbd> and open <kbd>hadoop-env.cmd</kbd>. Then, replace <kbd>set JAVA_HOME=%JAVA_HOME%</kbd> with <kbd>set JAVA_HOME={JavaHomeAbsolutePath}</kbd>.</li>
</ol>
<p style="padding-left: 60px">Add<span> the </span><kbd>winutils</kbd><span> H</span><span>adoop fix (o</span><span>nly applicable for Windows</span><span>). </span>You can download this from <a href="http://tiny.cc/hadoop-config-windows">http://tiny.cc/hadoop-config-windows</a>. <span>Alternatively, you can navigate to</span> the respective GitHub repository<span>, </span><a href="https://github.com/steveloughran/winutils"><span>https://github.com/steveloughran/winutils</span></a>, and get the fix that matches your installed Hadoop version. Replace the <kbd>bin</kbd> folder at<span> <kbd>${HADOOP_HOME}</kbd> with th</span>e <kbd>bin</kbd> folder i<span>n the fix.<br/></span></p>
<ol start="15">
<li>Run the following Hadoop command to format <kbd>namenode</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>hdfs namenode –format</strong></pre>
<p style="padding-left: 60px">You should see the following output:<br/></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1228 image-border" src="assets/41f46736-3d0a-40a0-82e8-27e2fdf9a69b.png" style="width:56.50em;height:29.42em;"/></p>
<ol start="16">
<li>Navigate to <kbd>${HADOOP_HOME}\sbin</kbd> and start the Hadoop services:
<ul>
<li>Fo<span>r Windows, run <kbd>start-all.cmd</kbd></span>.</li>
<li>For Linux or any other OS, run <kbd>start-all.sh</kbd> from Terminal.</li>
</ul>
</li>
</ol>
<p style="padding-left: 60px">You should see the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1229 image-border" src="assets/9727f210-5dd4-40c4-b67b-f32a47ebc8f4.png" style="width:52.92em;height:31.17em;"/></p>
<ol start="17">
<li>Hit <kbd>http://localhost:50070/</kbd> in your browser and verify whether Hadoop is up and running:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1230 image-border" src="assets/3632ac4d-fbcf-4421-bf28-9534434e821f.png" style="width:45.17em;height:31.50em;"/></p>
<ol start="18">
<li>Install Spark from <a href="https://spark.apache.org/downloads.html" target="_blank">https://spark.apache.org/downloads.html</a> and add the required environment variables. Extract the package and add the following environment variables:</li>
</ol>
<pre style="padding-left: 60px">SPARK_HOME = {PathDownloaded}/spark-x.x-bin-hadoopx.x<br/>SPARK_CONF_DIR = ${<span>SPARK_HOME}\conf</span></pre>
<ol start="19">
<li>Configure Spark's properties. Navigate to the directory location at <kbd>SPARK_CONF_DIR</kbd> and open the <kbd>spark-env.sh</kbd> file. Then, add the following configuration:</li>
</ol>
<pre style="padding-left: 60px">SPARK_MASTER_HOST=localhost<span><br/></span></pre>
<ol start="20">
<li>Run the Spark master by running the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>spark-class org.apache.spark.deploy.master.Master</strong></pre>
<p class="mce-root"/>
<p style="padding-left: 60px">You should see the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1427 image-border" src="assets/4f00b4cd-c9ea-432c-ad46-ae77b4414f33.png" style="width:81.33em;height:23.33em;"/></p>
<ol start="21">
<li>Hit <kbd>http://localhost:8080/</kbd> in your browser and verify whether Hadoop is up and running:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1428 image-border" src="assets/65bfa3c8-838f-4d6e-bdc9-1d1f95d3f690.png" style="width:106.92em;height:46.75em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 2, dependencies were added for <kbd>DataVec</kbd>. We need to use data transformation functions in Spark just like in regular training. Transformation is a data requirement for neural networks and is not Spark-specific.</p>
<p class="mce-root"/>
<p>For example, we talked about <kbd>LocalTransformExecutor</kbd> in <a href="6ac5dff5-cc98-4d52-bc59-1da01b2aeded.xhtml" target="_blank">Chapter 2</a>, <em>Data Extraction, Transformation, and Loading</em>. <kbd>LocalTransformExecutor</kbd> is used for <kbd>DataVec</kbd> transformation in non-distributed environments. <span><kbd>SparkTransformExecutor</kbd> will be used fo</span>r the <kbd>DataVec</kbd> <span>transformation process in Spark. </span></p>
<p>In step 4, we added dependencies for gradient sharing. Training times are faster for gradient sharing and it is designed to be <span>scalable</span> and fault-tolerant. Therefore, gradient sharing is preferred over parameter averaging. In gradient sharing, instead of relaying all the parameter updates/gradients across the network, it only updates those that are above the specified threshold. Let's say we have an update vector at the beginning that we want to communicate across the network. Due to this, we will be creating a sparse binary vector for the large values (as specified by a threshold) in the update vector. We will us<span>e this sparse binary vector for further communication. The main idea is to decrease the communication effort.</span> Note that the rest of the updates will not be discarded and are added in a residual vector for processing later. Residual vectors will be kept for future updates (delayed communication) and not lost. Gradient sharing in DL4J is an asynchronous SGD implementation. You can read more about this in detail at <a href="http://nikkostrom.com/publications/interspeech2015/strom_interspeech2015.pdf">http://nikkostrom.com/publications/interspeech2015/strom_interspeech2015.pdf</a>.</p>
<p><span>In step 5, we added CUDA dependencies for the Spark distributed training application.</span></p>
<p>Here are the uber-JAR requirements for this:</p>
<ul>
<li><span>If the OS that's building the uber-JAR is the same as that of the cluster OS (for example, run it on Linux and then execute it on a Spark Linux cluster), include the </span><kbd>nd4j-cuda-x.x</kbd><span> dependency in the </span><kbd>pom.xml</kbd> file.</li>
<li><span>If the OS that's building the uber-JAR is not the same as that of the cluster OS </span><span>(for example, run it on Windows and then execute it on a Spark Linux cluster), i</span><span>nclude the </span><kbd>nd4j-cuda-x.x-<span>platform</span></kbd><span> depende</span>ncy in the <kbd>pom.xml</kbd> file.</li>
</ul>
<p>Just replace <kbd>x.x</kbd> with the CUDA version you have installed (for example, <kbd>nd4j-cuda-9.2</kbd> for CUDA 9.2<em><span>).</span></em></p>
<p><span>In cases where the clusters don't ha</span>ve CUDA/cuDNN set up, we c<span>an includ</span>e <kbd>redist javacpp-</kbd> presets for the cluster OS. You can refer to the respective dependen<span>cies here: <a href="https://deeplearning4j.org/docs/latest/deeplearning4j-config-cudnn">https://deeplearning4j.org/docs/latest/deeplearning4j-config-cuDNN</a>. That way, we don't have to insta</span>ll CUDA or cuDNN in each and e<span>very cluster machine.</span></p>
<p class="mce-root"/>
<p><span>In step 6, we added a Maven dependenc</span>y for JCommander. JCommander is used <span>to parse command-line arguments that are supplied with <kbd>spark-submit</kbd>. We need this because we will be passing directory locations (HDFS/local) of the train/test data as command-line arguments in <kbd>spark-submit</kbd>. </span></p>
<p><span>From steps 7 to 16, we downloaded and configured Hadoop. Remember to replace <kbd>{PathDownloaded}</kbd> with the actual location of the extracted Hadoop package. Also, replace <kbd>x.x</kbd> with the Hadoop version you've downloaded. We need to specify the disk location where we will store the metadata and the data represented in HDFS. Due to this, we created name/data directories in step 8/step 9. </span><span>To make changes, in step 10, we configured <kbd>mapred-site.xml</kbd>. If you can't locate the file in the directory, just create an XML</span> f<span>ile by copying all the content from the <kbd>mapred-site.xml.template</kbd> file, and then make the changes that were mentioned in step 10.</span></p>
<p><span>In step 13, we replaced t</span>he <kbd>JAVA_HOME</kbd> path variable with the actual Java home directory location. This was done to avoid certain <kbd>ClassNotFound</kbd> ex<span>ceptions from being encountered at runtime. </span></p>
<p><span>In step 18, make sure that you are downloading the Spark version that matches your Hadoop version. For example, if you have Hadoop 2.7.3, then get the Spark version that looks like <kbd>spark-x.x-bin-hadoop2.7</kbd>. When we made changes in step 19, if the <kbd>spark-env.sh</kbd> file isn't present, then just create a new file named <kbd>spark-env.sh</kbd> by copying the content from the <kbd>spark-env.sh.template</kbd> file. Then, make the changes that were mentioned in step 19. After completing all the steps in this recipe, you should be able to perform distributed neural network training via the <kbd>spark-submit</kbd> command. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an uber-JAR for training</h1>
                </header>
            
            <article>
                
<p class="mce-root">The training job that's executed by <kbd>spark-submit</kbd> will need to resolve all the required dependencies at runtime. In order to manage this task, we will create an uber-JAR that has the application runtime and its required dependencies. <span>We will use the Maven configurations in</span> <kbd>pom.xml</kbd> to create an uber-JAR so that we can perform distributed training. Effectively, we will create an uber-JAR and submit it to <kbd>spark-submit</kbd> to perform the training job in Spark.</p>
<p class="mce-root">In this recipe, we will create an uber-JAR usi<span>ng the Maven shade plugin for Spark training.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create an uber-JAR (shaded JAR) by adding the Maven shade plugin to the <kbd>pom.xml</kbd> file, as shown here:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1233 image-border" src="assets/4d8158cc-6169-4fd5-960e-8c6d1fa051d9.png" style="width:76.83em;height:26.83em;"/></p>
<p style="padding-left: 60px">Refer to the <kbd>pom.xml</kbd> file in this book's GitHub repository for more information: <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing%20applications%20in%20distributed%20environment/sourceCode/cookbookapp/pom.xml">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing%20applications%20in%20distributed%20environment/sourceCode/cookbookapp/pom.xml</a>. Add the following filter to the Maven configuration:</p>
<pre style="padding-left: 60px">&lt;filters&gt;<br/>   &lt;filter&gt;<br/>    &lt;artifact&gt;*:*&lt;/artifact&gt;<br/>    &lt;excludes&gt;<br/>     &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;<br/>     &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;<br/>     &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;<br/>    &lt;/excludes&gt;<br/>   &lt;/filter&gt;<br/> &lt;/filters&gt;</pre>
<ol start="2">
<li>Hit the Maven command to build an uber-JAR for the project:</li>
</ol>
<pre style="padding-left: 60px"><strong>mvn package -DskipTests</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, you need to specify the main class that should run while executing the JAR file. In the preceding demonstration, <kbd><span>SparkExample</span></kbd> is our main class that invokes a training session. You may come across exceptions that look as follows:</p>
<pre><strong>Exception in thread “main” java.lang.SecurityException: Invalid signature file digest for Manifest main attributes.</strong></pre>
<p>Some of the dependencies that were added to the Maven configuration may have a signed JAR, which may cause issues like these.</p>
<p>In step 2, we added the filters to prevent the addition of signed <kbd>.jars</kbd> during the Maven build.</p>
<p><span>In step 3, we generated an executable <kbd>.jar</kbd> file with all the required dependencies. We can submit this <kbd>.jar</kbd> file to <kbd>spark-submit</kbd> to train our networks on Spark.</span> The <kbd>.jar</kbd> file is created in the <kbd>target</kbd> directory of the project:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1234 image-border" src="assets/58b2d8d3-043e-43ac-b3ba-104fb70c7fdc.png" style="width:24.92em;height:20.25em;"/></p>
<p>The Maven shade plugin is not the only way to build an uber-JAR file. However, the Maven shade plugin is recommended over other alternatives. Other alternatives may not be able to include the required files from source <kbd>.jars</kbd>. Some of those files act as dependencies for the Java service loader's functionality. ND4J makes use of Java's service loader functionality. Therefore, other alternative plugins can cause issues. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CPU/GPU-specific configuration for training</h1>
                </header>
            
            <article>
                
<p>Hardware-specific changes are generic configurations that can't be ignored in a distributed environment. DL4J supp<span>orts GPU-accelerated training in NVIDIA GPUs with CUDA/cuDNN enabled. We can also perform Spark distributed training using GPUs.</span></p>
<p><span>In this recipe, we will configure CPU/GPU-specific changes. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Download, install, and set up the CUDA toolkit from <a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a>. OS-specific setup instructions are available at the NVIDIA CUDA official website. </li>
<li>Configure the GPU for Spark distributed training by adding a Maven dependency for ND4J's CUDA backend:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/>   &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>   &lt;artifactId&gt;nd4j-cuda-x.x&lt;/artifactId&gt;<br/>   &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt; </pre>
<ol start="3">
<li>Configure the CPU for Spark distributed training by adding an ND4J-native dependency:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/>    &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>    &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;<br/>    &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>We need to enable a proper ND4J backend so that we can utilize GPU resources, as we mentioned in step 1. </span><span>Enable the </span><span><kbd>nd4j-cuda-x.x</kbd> dependency in you</span>r <kbd>pom.xml</kbd> file for GPU traini<span>ng, where <kbd>x.x</kbd> refers to the CUDA version that you have installed. </span></p>
<p class="mce-root"/>
<p>We may include both ND4J backends (CUDA/native dependencies) if the master node is running on the CPU and the worker nodes are running on the GPU, as we mentioned in the previous recipe. If both backends are present in the classpath, the CUDA backend will be tried out first. If it doesn't load for some reason, then the CPU backend (native) will be loaded. The priority can also be changed by changing the <kbd>BACKEND_PRIORITY_CPU</kbd> and <kbd>BACKEND_PRIORITY_GPU</kbd> environment variables in the master node. The backend will be picked depending on which one of these environment variables has the highest value.</p>
<p>In step 3, we added CPU-specific configuration that targets CPU-only hardware. We don't have to keep this configuration if both the master/worker nodes have GPU hardware in place. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span>We can further optimize the training throughput by configuring cuDNN into CUDA devices. We can run a training instance in Spark without CUDA/cuDNN installed o</span>n every n<span>ode. To gain optimal performance with cuDNN support, we can add the DL4J CUDA dependency. For that, the following components must be added and made available:</span></p>
<ul>
<li><span>The DL4J </span>CUDA Maven <span>dependency:</span></li>
</ul>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/>  &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>  &lt;artifactId&gt;deeplearning4j-cuda-x.x&lt;/artifactId&gt;<br/>  &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;<span><br/></span></pre>
<ul>
<li><span>The cuDNN library files at <a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cuDNN</a>. Note that you need to sign up to the NVIDIA website to download cuDNN libraries. Signup is free. Refer to the installation guide here: <a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html">https://docs.nvidia.com/deeplearning/sdk/cuDNN-install/index.html</a>.</span></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Memory settings and garbage collection for Spark</h1>
                </header>
            
            <article>
                
<p class="mce-root">Memory management is very crucial for distributed training with large datasets in production. It directly influences the resource consumption and performance of the neural network. <span>Memory management involves configuring off-heap and on-heap memory spaces. DL4J/ND4J-specific memory configuration will be discussed in detail in <a href="0db31248-e40b-4479-9939-0baccb0e11d1.xhtml" target="_blank">Chapter 12</a>, <em>Benchmarking and Neural Network Optimization</em>.</span></p>
<p class="mce-root"><span>In this recipe, we will focus on memory configuration in the context of Spark. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Add the <kbd>--executor-memory</kbd> command-line argument while submitting a job to <kbd>spark-submit</kbd> to set on-heap memory for the worker node. For example, we could use <kbd>--executor-memory 4g</kbd> to allocate 4 GB of memory.<span><br/></span></li>
<li>Add the <kbd>--conf</kbd> command-line argument to set the off-heap memory for the worker node:</li>
</ol>
<pre style="padding-left: 60px">--conf "spark.executor.extraJavaOptions=-Dorg.bytedeco.javacpp.maxbytes=8G"</pre>
<ol start="3">
<li>Add the <kbd>--conf</kbd> command-line argument to set the off-heap memory for the master node. For example, we could use <kbd>--conf "spark.driver.memoryOverhead=-Dorg.bytedeco.javacpp.maxbytes=8G"</kbd> to allocate 8 GB of memory.</li>
<li>Add the <kbd>--driver-memory</kbd> command-line argument to specify the on-heap memory for the master node. For example, we could use <kbd>--driver-memory 4g</kbd> to allocate 4 GB of memory.</li>
<li>Configure garbage collection for the worker nodes by calling <kbd>workerTogglePeriodicGC()</kbd> and <kbd>workerPeriodicGCFrequency()</kbd> while you set up the distributed neural network using <kbd>SharedTrainingMaster</kbd>:</li>
</ol>
<pre style="padding-left: 60px">new SharedTrainingMaster.Builder(voidConfiguration, minibatch)<br/>   .workerTogglePeriodicGC(true) <br/>   .workerPeriodicGCFrequency(frequencyIntervalInMs) <br/>   .build();<span><br/></span></pre>
<ol start="6">
<li>Enable Kryo optimization in DL4J by adding the following dependency to the <kbd>pom.xml</kbd> file:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/>   &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>   &lt;artifactId&gt;nd4j-kryo_2.11&lt;/artifactId&gt;<br/>  &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>
<ol start="7">
<li>Configure <kbd>KryoSerializer</kbd> with <kbd>SparkConf</kbd>:</li>
</ol>
<pre style="padding-left: 60px">SparkConf conf = new SparkConf();<br/> conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");<br/> conf.set("spark.kryo.registrator", "org.nd4j.Nd4jRegistrator");</pre>
<ol start="8">
<li>Add locality configuration to <kbd>spark-submit</kbd>, as shown here:</li>
</ol>
<pre style="padding-left: 60px">--conf spark.locality.wait=0 </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we discussed Spark-specific memory configurations. We mentioned that this can be configured for master/worker nodes. Also, these memory configurations can be dependent on the cluster resource manager. </p>
<p><span>Note that the <kbd>--executor-memory 4g</kbd> command-line argument is for YARN. </span><span>Please refer to the respective cluster resource manager documentation to find out the respective command-line argument for the following:<br/></span></p>
<ul>
<li><span>Spark Standalone: <a href="https://spark.apache.org/docs/latest/spark-standalone.html">https://spark.apache.org/docs/latest/spark-standalone.html</a><br/></span></li>
<li><span>Mesos: <a href="https://spark.apache.org/docs/latest/running-on-mesos.html">https://spark.apache.org/docs/latest/running-on-mesos.html</a><br/></span></li>
<li><span>YARN: <a href="https://spark.apache.org/docs/latest/running-on-yarn.html">https://spark.apache.org/docs/latest/running-on-yarn.html</a><br/></span></li>
</ul>
<p><span>For </span>Spark Standalone, use<span> the following command-line options to configure the memory space:<br/></span></p>
<ul>
<li><span>The on-heap memory for the driver can be configured like so (<kbd>8G</kbd> -&gt; 8 GB of memory):</span></li>
</ul>
<pre style="padding-left: 60px">SPARK_DRIVER_MEMORY=8G<span><br/></span></pre>
<p class="mce-root"/>
<ul>
<li><span>The off-heap memory for the driver can be configured like so:</span></li>
</ul>
<pre style="padding-left: 60px">SPARK_DRIVER_OPTS=-Dorg.bytedeco.javacpp.maxbytes=8G<span><br/></span></pre>
<ul>
<li><span>The on-heap memory for the worker can be configured like so:</span></li>
</ul>
<pre style="padding-left: 60px">SPARK_WORKER_MEMORY=8G<span><br/></span></pre>
<ul>
<li><span>The off-heap me</span><span>mory for the worker can be configured like so:</span></li>
</ul>
<pre style="padding-left: 60px">SPARK_WORKER_OPTS=-Dorg.bytedeco.javacpp.maxbytes=8G<span> <br/></span></pre>
<p>In step 5, we discussed garbage collection for worker nodes. Generally speaking, t<span>here are two ways in which we can control the frequency of garbage collection. The following is the first approach:</span></p>
<pre>Nd4j.getMemoryManager().setAutoGcWindow(frequencyIntervalInMs);</pre>
<p><span>This will limit the frequency of garbage collector calls to the specified time interval, that is, </span><span><kbd>frequencyIntervalInMs</kbd>. The second approach is as follows:</span></p>
<pre>Nd4j.getMemoryManager().togglePeriodicGc(false);</pre>
<p><span>This will totally disable the garbage collector's calls. However, the these approaches will not alter the worker node's memory configuration. We can configure the worker node's memory using the builder methods that are available in <kbd>SharedTrainingMaster</kbd>.</span></p>
<p><span>We call <kbd>workerTogglePeriodicGC()</kbd> to disable/enable periodic <strong>garbage collector</strong> (<strong>GC</strong>) calls and <kbd>workerPeriodicGCFrequency()</kbd> to set the frequency in which GC needs to be called. </span></p>
<p>In step 6, we added support for Kryo serialization in ND4J. The Kryo serializer is a Java serialization framework that helps to increase the speed/efficiency during training in Spark.</p>
<p>For more information, refer to <a href="https://spark.apache.org/docs/latest/tuning.html">https://spark.apache.org/docs/latest/tuning.html</a>. In step 8, locality configuration is an optional configuration that can be used to improve training performance. Data locality can have a major impact on the performance of Spark jobs. The idea is to ship the data and code together so that the computation can be performed really quickly. For more information, please refer to <a href="https://spark.apache.org/docs/latest/tuning.html#data-locality">https://spark.apache.org/docs/latest/tuning.html#data-locality</a>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Memory configurations are often applied to master/worker nodes separately. Therefore, memory configuration on worker nodes alone may not bring the required results. The approach we take can vary, depending on the cluster resource manager we use. Therefore, it is important to refer to the respective documentation on the different approaches for a specific cluster resource manager. Also, note that the default memory settings in the cluster resource managers are not appropriate (too low) for libraries (ND4J/DL4J) that heavily rely on off-heap memory space. <kbd>spark-submit</kbd> can load the configurations in two different ways. One way is to use the<span> </span><em>command line</em><span>, </span>as we discussed previously, while another one is to specify the configuration in the<span> </span><span><kbd>spark-defaults.conf</kbd> file, like so:</span></p>
<pre><strong>spark.master spark://5.6.7.8:7077</strong><br/><strong>spark.executor.memory 4g</strong></pre>
<p><span>Spark can accept any Spark properties using the <kbd>--conf</kbd> flag. We used it to specify off-heap memory space in this recipe. You can read more about Spark configuration here: <a href="http://spark.apache.org/docs/latest/configuration.html">http://spark.apache.org/docs/latest/configuration.html</a>:</span></p>
<ul>
<li>The dataset should justify<span> </span>the memory allocation in the driver/executor. For 10 MB of data, we don't have to assign too much of the memory to the executor/driver. In this case, 2 GB to 4 GB of memory would be enough. Allotting too much memory won't make any difference and it can actually reduce the performance.</li>
<li>The <em>driver</em> is the process where the main Spark job runs. <em>Executors</em> are worker node tasks that have individual tasks allotted to run. If the application runs in local mode, the driver memory is not necessarily allotted. The driver memory is connected to the master node and it is relevant while the application is running in <em>cluster</em> mode. In <em>cluster</em> mode, the Spark job will not run on the local machine it was submitted from. The Spark driver component will launch inside the cluster.</li>
<li>Kryo is a fast and efficient serialization framework for Java. Kryo can also perform automatic deep/shallow copying of objects in order to attain a high speed, low size, and easy-to-use API. The DL4J API can make use of Kryo serialization to optimize the performance a bit further. However, note that since INDArrays consume off-heap memory space, Kryo may not result in much performance gain. Check the respective logs to ensure your Kryo configuration is correct while using it with the <kbd>SparkDl4jMultiLayer</kbd> or <kbd>SparkComputationGraph</kbd> classes. </li>
<li><span>Just like in regular training, we need to add the proper ND4J backend for DL4J Spark to function. For newer versions of YARN, some additional configurations may be required. Refer to the </span>YARN documentation<span> for more details: </span><span><a href="https://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-site/UsingGpus.html">https://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-site/UsingGpus.html</a>.</span></li>
</ul>
<div class="packt_infobox" style="padding-left: 60px"><span>Also, note that older versions (2.7.x or earlier) will not support GPUs natively (GPU and CPU). For these versions, we need to use node labels to ensure that jobs are running in GPU-only machines. </span></div>
<ul>
<li>If you perform Spark training, you need to be aware of data locality in order to optimize the throughput. Data locality ensures that the data and the code that operates on the Spark job are together and not separate. Data locality ships the serialized code from place to place (instead of chunks of data) where the data operates. It will speed up its performance and won't introduce further issues since the size of the code will be significantly smaller than the data. Spark provides a configuration property named <kbd>spark.locality.wait</kbd> to specify the timeout before moving the data to a free CPU. If you set it to zero, then data will be immediately moved to a free executor rather than wait for a specific executor to become free. If the freely available executor is distant from the executor where the current task is executed, then it is an additional effort. However, we are saving time by waiting for a nearby executor to become free. So, the computation time can still be reduced. You can read more about data locality on Spark<span> </span>here: <a href="https://spark.apache.org/docs/latest/tuning.html#data-locality">https://spark.apache.org/docs/latest/tuning.html#data-locality</a>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring encoding thresholds</h1>
                </header>
            
            <article>
                
<p><span>The DL4J Spark implementation makes use of a threshold encoding scheme to perform parameter updates across nodes in order to reduce the commuted message size across the network and thereby reduce the cost of traffic. The threshold encoding scheme introduces a new distributed training-specific hyperparamet</span>er called <strong>encoding threshold</strong>.</p>
<p><span>In this recipe, we will configure the threshold algorithm in a distributed training implementation. </span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Configure the threshold algorithm in <kbd>SharedTrainingMaster</kbd>:</li>
</ol>
<pre style="padding-left: 60px">TrainingMaster tm = new SharedTrainingMaster.Builder(voidConfiguration, minibatchSize)<br/>   .thresholdAlgorithm(new AdaptiveThresholdAlgorithm(gradientThreshold))<br/>  .build();</pre>
<ol start="2">
<li>Configure the residual vectors by calling <kbd>residualPostProcessor()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">TrainingMaster tm = new SharedTrainingMaster.Builder(voidConfiguration, minibatch)<br/> .residualPostProcessor(new ResidualClippingPostProcessor(clipValue, frequency))<br/> .build();</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we configured the threshold algorithm in <kbd>SharedTrainingMaster</kbd>, where the default algorithm is <kbd>AdaptiveThresholdAlgorithm</kbd>. Threshold algorithms will determine the encoding threshold for distributed training, which is a hyperparameter that's specific to distributed training. Also, note that we are not discarding the rest of the parameter updates. As we mentioned earlier, we put them into separate residual vectors and process them later. We do this to reduce the network traffic/load during training. <strong><kbd>AdaptiveThresholdAlgorithm</kbd> </strong>is preferred in most cases for better performance.</p>
<p>In step 2, we used <kbd>ResidualPostProcessor</kbd> to post process the residual vector. The residual vector was created internally by the gradient sharing implementation to collect parameter updates that were not marked by the specified bound. Most implementations of <kbd>ResidualPostProcessor</kbd> will clip/decay the residual vector so that the values in them will not become too large compared to the threshold value. <kbd>ResidualClippingPostProcessor</kbd> is one such implementation. <kbd>ResidualPostProcessor</kbd> will prevent the residual vector from becoming too large in size as it can take too much time to communicate and may lead to stale gradient issues.</p>
<p class="mce-root"/>
<p>In step 1, w<span>e called <kbd>thresholdAlgorithm()</kbd> to set the threshold algorithm. In step 2, we called <kbd>residualPostProcessor()</kbd> to post process the residual vector for the gradient sharing implementation in DL4</span>J. <kbd>ResidualClippingPostProcessor</kbd> ac<span>cepts two attributes: <kbd>clipValue</kbd> and <kbd>frequency</kbd>. <kbd>clipValue</kbd> is the multiple of the current threshold that we use for clipping. For example, i</span>f threshold is <kbd>t</kbd> and <span><kbd>clipValue</kbd> is</span> <kbd>c</kbd>, <span>then the residual vectors will be</span> clipped to the range <span><strong><kbd>[-c*t , c*t]</kbd></strong>. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span>The idea behind the threshold</span> (the encoding threshold, in our context) is that the parameter updates will happen across clusters, but only for the values that come under the user-defined limit (threshold). This threshold value is what we refer to as the encoding threshold. Parameter updates refer to the changes in gradient values during the training process. High/low encoding threshold values are not good for optimal results. So, it is reasonable to come up with a range of acceptable values for the encoding threshold. This is also termed as the sparsity ratio, in which the parameter updates happen across clusters. </p>
<p><span>In this recipe, we also discussed how to configure threshold algorithms for distributed training. The default choice would be to use </span><span><kbd>AdaptiveThresholdAlgorithm</kbd> if <kbd>AdaptiveThresholdAlgorithm</kbd> provides undesired results.</span></p>
<p><span>The following are the various threshold algorithms that are available in DL4J:</span></p>
<ul>
<li><kbd>AdaptiveThresholdAlgorithm</kbd><span>: This is the default threshold algorithm that works well in most scenarios. </span></li>
<li><kbd>FixedThresholdAlgorithm</kbd><span>: This is a fixed and non-adaptive threshold strategy.</span></li>
<li><kbd>TargetSparsityThresholdAlgorithm</kbd><span>: This is an adaptive threshold strategy with a specific target. It decreases or increases the threshold to try and match the target. </span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing a distributed test set evaluation</h1>
                </header>
            
            <article>
                
<p>There are challenges involved in distributed neural network <span>training</span>. Some of these challenges include <span>managing different hardware dependencies across master and worker nodes, </span><span>configuring distributed training to produce good performance, memory benchmarks across the distributed clusters, and more. We discussed some of those concerns in the previous recipes. While keeping such configurations in place, we'll move on to the actual distributed training/evaluation. </span>In this recipe, we will perform the following tasks:</p>
<ul>
<li>ETL for DL4J Spark training</li>
<li>Create a neural network for Spark training</li>
<li>Perform a test set evaluation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Download, extract, and copy the contents of the <kbd>TinyImageNet</kbd> dataset to the following directory location:</li>
</ol>
<pre style="padding-left: 60px">* Windows: C:\Users\&lt;username&gt;\.deeplearning4j\data\TINYIMAGENET_200<br/> * Linux: ~/.deeplearning4j/data/TINYIMAGENET_200</pre>
<ol start="2">
<li>Create batches of images for training using the <kbd>TinyImageNet</kbd> dataset:</li>
</ol>
<pre style="padding-left: 60px">File saveDirTrain = new File(batchSavedLocation, "train");<br/> SparkDataUtils.createFileBatchesLocal(dirPathDataSet, NativeImageLoader.ALLOWED_FORMATS, true, saveDirTrain, batchSize);</pre>
<ol start="3">
<li>Create batches of images for testing using the <kbd>TinyImageNet</kbd> dataset:</li>
</ol>
<pre style="padding-left: 60px">File saveDirTest = new File(batchSavedLocation, "test");<br/> SparkDataUtils.createFileBatchesLocal(dirPathDataSet, NativeImageLoader.ALLOWED_FORMATS, true, saveDirTest, batchSize);</pre>
<ol start="4">
<li>Create an <kbd>ImageRecordReader</kbd> that holds a reference of the dataset:</li>
</ol>
<pre style="padding-left: 60px">PathLabelGenerator labelMaker = new ParentPathLabelGenerator();<br/> ImageRecordReader rr = new ImageRecordReader(imageHeightWidth, imageHeightWidth, imageChannels, labelMaker);<br/> rr.setLabels(new TinyImageNetDataSetIterator(1).getLabels());</pre>
<ol start="5">
<li>Create <kbd>RecordReaderFileBatchLoader</kbd> from <kbd>ImageRecordReader</kbd> to load the batch data:</li>
</ol>
<pre style="padding-left: 60px">RecordReaderFileBatchLoader loader = new RecordReaderFileBatchLoader(rr, batchSize, 1, TinyImageNetFetcher.NUM_LABELS);<br/> loader.setPreProcessor(new ImagePreProcessingScaler()); </pre>
<ol start="6">
<li>Use JCommander at the beginning of your source code to parse command-line arguments:</li>
</ol>
<pre style="padding-left: 60px">JCommander jcmdr = new JCommander(this);<br/> jcmdr.parse(args);</pre>
<ol start="7">
<li>Create a parameter server configuration (gradient sharing) for Spark training using <kbd>VoidConfiguration</kbd>, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">VoidConfiguration voidConfiguration = VoidConfiguration.builder()<br/> .unicastPort(portNumber)<br/> .networkMask(netWorkMask)<br/> .controllerAddress(masterNodeIPAddress)<br/> .build();</pre>
<ol start="8">
<li>Configure a distributed training network using <kbd>SharedTrainingMaster</kbd>, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">TrainingMaster tm = new SharedTrainingMaster.Builder(voidConfiguration, batchSize)<br/> .rngSeed(12345)<br/> .collectTrainingStats(false)<br/> .batchSizePerWorker(batchSize) // Minibatch size for each worker<br/> .thresholdAlgorithm(new AdaptiveThresholdAlgorithm(1E-3)) //Threshold algorithm determines the encoding threshold to be use.<br/> .workersPerNode(1) // Workers per node<br/> .build();</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="9">
<li>Create a <kbd>GraphBuilder</kbd> for <kbd>ComputationGraphConfguration</kbd>, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">ComputationGraphConfiguration.GraphBuilder builder = new NeuralNetConfiguration.Builder()<br/> .convolutionMode(ConvolutionMode.Same)<br/> .l2(1e-4)<br/> .updater(new AMSGrad(lrSchedule))<br/> .weightInit(WeightInit.RELU)<br/> .graphBuilder()<br/> .addInputs("input")<br/> .setOutputs("output");<br/> </pre>
<ol start="10">
<li>Use <kbd>DarknetHelper</kbd> from the DL4J Model Zoo to power up our CNN architecture, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">DarknetHelper.addLayers(builder, 0, 3, 3, 32, 0); //64x64 out<br/> DarknetHelper.addLayers(builder, 1, 3, 32, 64, 2); //32x32 out<br/> DarknetHelper.addLayers(builder, 2, 2, 64, 128, 0); //32x32 out<br/> DarknetHelper.addLayers(builder, 3, 2, 128, 256, 2); //16x16 out<br/> DarknetHelper.addLayers(builder, 4, 2, 256, 256, 0); //16x16 out<br/> DarknetHelper.addLayers(builder, 5, 2, 256, 512, 2); //8x8 out</pre>
<ol start="11">
<li>Configure the output layers while considering the number of labels and loss functions, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">builder.addLayer("convolution2d_6", new ConvolutionLayer.Builder(1, 1)<br/> .nIn(512)<br/> .nOut(TinyImageNetFetcher.NUM_LABELS) // number of labels (classified outputs) = 200<br/> .weightInit(WeightInit.XAVIER)<br/> .stride(1, 1)<br/> .activation(Activation.IDENTITY)<br/> .build(), "maxpooling2d_5")<br/> .addLayer("globalpooling", new GlobalPoolingLayer.Builder(PoolingType.AVG).build(), "convolution2d_6")<br/> .addLayer("loss", new LossLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).activation(Activation.SOFTMAX).build(), "globalpooling")<br/> .setOutputs("loss");</pre>
<ol start="12">
<li>Create <kbd>ComputationGraphConfguration</kbd> from the <kbd>GraphBuilder</kbd>:</li>
</ol>
<pre style="padding-left: 60px">ComputationGraphConfiguration configuration = builder.build(); </pre>
<ol start="13">
<li>Create the <kbd>SparkComputationGraph</kbd> model from the defined configuration and set training listeners to it:</li>
</ol>
<pre style="padding-left: 60px">SparkComputationGraph sparkNet = new SparkComputationGraph(context,configuration,tm);<br/> sparkNet.setListeners(new PerformanceListener(10, true));</pre>
<ol start="14">
<li>Create <kbd>JavaRDD</kbd> objects that represent the HDFS paths of the batch files that we created earlier for training:</li>
</ol>
<pre style="padding-left: 60px">String trainPath = dataPath + (dataPath.endsWith("/") ? "" : "/") + "train";<br/> JavaRDD&lt;String&gt; pathsTrain = SparkUtils.listPaths(context, trainPath);</pre>
<ol start="15">
<li>Invoke the training instance by calling <kbd>fitPaths()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">for (int i = 0; i &lt; numEpochs; i++) {<br/>   sparkNet.fitPaths(pathsTrain, loader);<br/> }<br/> </pre>
<ol start="16">
<li>Create <kbd>JavaRDD</kbd> objects that represent the HDFS paths to batch files that we created earlier for testing:</li>
</ol>
<pre style="padding-left: 60px">String testPath = dataPath + (dataPath.endsWith("/") ? "" : "/") + "test";<br/> JavaRDD&lt;String&gt; pathsTest = SparkUtils.listPaths(context, testPath);</pre>
<ol start="17">
<li>Evaluate the distributed neural network by calling <kbd>doEvaluation()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">Evaluation evaluation = new Evaluation(TinyImageNetDataSetIterator.getLabels(false), 5);<br/> evaluation = (Evaluation) sparkNet.doEvaluation(pathsTest, loader, evaluation)[0];<br/> log.info("Evaluation statistics: {}", evaluation.stats());</pre>
<ol start="18">
<li>Run the distributed training instance on <kbd>spark-submit</kbd> in the following format:</li>
</ol>
<pre style="padding-left: 60px">spark-submit --master spark://{sparkHostIp}:{sparkHostPort} --class {clssName} {JAR File location absolute path} --dataPath {hdfsPathToPreprocessedData} --masterIP {masterIP}<br/> <br/>Example:<br/> spark-submit --master spark://192.168.99.1:7077 --class com.javacookbook.app.SparkExample cookbookapp-1.0-SNAPSHOT.jar --dataPath hdfs://localhost:9000/user/hadoop/batches/imagenet-preprocessed --masterIP 192.168.99.1</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works....</h1>
                </header>
            
            <article>
                
<p><span>Step 1 can be automated usi</span>ng <kbd>TinyImageNetFetcher</kbd>, as shown here<span>:</span></p>
<pre>TinyImageNetFetcher fetcher = new TinyImageNetFetcher();<br/> fetcher.downloadAndExtract();</pre>
<p><span>For any OS, the data needs to be copied to the user's home directory. Once it is executed, we can get a reference to the train/test dataset directory, as shown here:</span></p>
<pre>File baseDirTrain = DL4JResources.getDirectory(ResourceType.DATASET, f.localCacheName() + "/train");<br/> File baseDirTest = DL4JResources.getDirectory(ResourceType.DATASET, f.localCacheName() + "/test");</pre>
<p><span>You can also mention your own input directory location from your local disk or HDFS. You will need to mention that in place of <kbd>dirPathDataSet</kbd> in step 2.</span></p>
<p>In step 2 and step 3, we created batches of images so that we could optimize the distributed training. We used <kbd>createFileBatchesLocal()</kbd> to create these batches, where the source of the data is a local disk. If you want to create batches from the HDFS source, then use <kbd>createFileBatchesSpark()</kbd> instead. These compressed batch files will save space and reduce bottlenecks in computation. Suppose we loaded 64 images in a compressed batch <span>–</span> we don't require 64 different disk reads to process the batch file. These batches contain the contents of raw files from multiple files.</p>
<p>In step 5, we used <kbd>RecordReaderFileBatchLoader</kbd> to process file batch objects that were created using either <kbd><span>createFileBatchesLocal()</span></kbd> or <kbd>createFileBatchesSpark()</kbd><span>. As we mentioned in step 6, you can us</span>e JCommander to process the command-line arguments from <kbd>spark-submit</kbd> or write your own logic to handle them.</p>
<p>In step 7, we configured the parameter server using the <kbd>VoidConfiguration</kbd> class. This is a basic configuration POJO class for the parameter server. We can mention the po<span>rt number, network mask, and so on for the parameter server. The network mask is a very important configuration in a shared network environment and YARN. </span></p>
<p><span>In step 8, we started configuring the distributed network for training using</span> <kbd>SharedTrainingMaster</kbd>. W<span>e added important configurations such as threshold algorithms, worker node count, minibatch size, and so on.</span></p>
<p>Starting from steps 9 and 10, we focused on distributed neural network layer configuration. We used <kbd>DarknetHelper</kbd> from the DL4J Model Zoo to borrow functionalities from DarkNet, TinyYOLO and YOLO2. </p>
<p class="mce-root"/>
<p>In step 11, we added the output layer configuration for our tiny <kbd>ImageNet</kbd> classifier. There are 200 labels in which the image classifier makes a prediction. In step 13, we created a Spark-based <kbd>ComputationGraph</kbd> using <kbd>SparkComputationGraph</kbd>. If the underlying network structure is <kbd>MultiLayerNetwork</kbd>, then you could use <kbd>SparkDl4jMultiLayer</kbd> instead.</p>
<p>In step 17, we created an evaluation instance, as shown here:</p>
<pre>Evaluation evaluation = new Evaluation(TinyImageNetDataSetIterator.getLabels(false), 5);</pre>
<p>The second attribute (<kbd>5</kbd>, in the preceding code) represents the value <kbd>N</kbd>, which is used to measure the top <kbd>N</kbd> accuracy metrics. For example, evaluation on a sample will be correct if the probability for the <kbd>true</kbd> class is one of the highest <kbd>N</kbd> values. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Saving and loading trained neural network models</h1>
                </header>
            
            <article>
                
<p>Training the neural network over and over to perform an evaluation is not a good idea since training is a very costly operation. This is why model persistence is important in distributed systems as well.</p>
<p>In this recipe, we will persist the distributed neural network models to disk and load them for further use. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Save the distributed neural network model using <kbd>ModelSerializer</kbd>:</li>
</ol>
<pre style="padding-left: 60px">MultiLayerNetwork model = sparkModel.getNetwork();<br/> File file = new File("MySparkMultiLayerNetwork.bin");<br/> ModelSerializer.writeModel(model,file, saveUpdater);</pre>
<ol start="2">
<li>Save the distributed neural network model using <kbd>save()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">MultiLayerNetwork model = sparkModel.getNetwork();<br/>  File locationToSave = new File("MySparkMultiLayerNetwork.bin);<br/> model.save(locationToSave, saveUpdater);</pre>
<ol start="3">
<li>Load the distributed neural network model using <kbd>ModelSerializer</kbd>:</li>
</ol>
<pre style="padding-left: 60px">ModelSerializer.restoreMultiLayerNetwork(new File("MySparkMultiLayerNetwork.bin"));</pre>
<ol start="4">
<li>Load the distributed neural network model using <kbd>load()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">MultiLayerNetwork restored = MultiLayerNetwork.load(savedModelLocation, saveUpdater);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Although we used <kbd>save()</kbd> or <kbd>load()</kbd> for the model's persistence in a local machine, it is not an ideal practice in production. For a distributed cluster environment, we can make use of <kbd>BufferedInputStream</kbd>/<kbd>BufferedOutputStream</kbd> in steps 1 and 2 to save/load models to/from clusters. We can use <kbd>ModelSerializer</kbd> or <kbd>save()</kbd>/<kbd>load()</kbd> just like we demonstrated earlier. We just need to be aware of the cluster resource manager and model persistence, which can be performed across clusters. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><kbd>SparkDl4jMultiLayer</kbd> and <kbd>SparkComputationGraph</kbd> internally make use of the standard implementations of <kbd>MultiLayerNetwork</kbd> and <kbd>ComputationGraph</kbd>, re<span>spectively. Thus, their internal structure can be accessed by calling the </span><kbd>getNetwork()</kbd><span> method. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing distributed inference</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we have discussed how to perform distributed training using DL4J. We have also performed distributed evaluation to evaluate the trained distributed model. Now, let's discuss how to utilize the distributed model to solve use cases such as predictions. This is referred to as inference. Let's go over how we can perform </span><em>distributed</em><span> inference in a Spark environment. </span></p>
<p>In this recipe, we will perform distributed inference on Spark using DL4J.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Perform distributed inference for <kbd>SparkDl4jMultiLayer</kbd> by calling <kbd>feedForwardWithKey()</kbd>, as shown here:</li>
</ol>
<pre style="padding-left: 60px">SparkDl4jMultiLayer.feedForwardWithKey(JavaPairRDD&lt;K, INDArray&gt; featuresData, int batchSize);</pre>
<ol start="2">
<li>Perform distributed inference for <kbd>SparkComputationGraph</kbd> by calling <kbd>feedForwardWithKey()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">SparkComputationGraph.feedForwardWithKey(JavaPairRDD&lt;K, INDArray[]&gt; featuresData, int batchSize) ;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>The intent of the <kbd>feedForwardWithKey()</kbd> method in step 1 and 2 is to generate output/predictions for the given input dataset. A map is </span>re<span>turned from this method. Th</span>e input data is represented by the keys in the map and the results (output) are represented by values (<kbd>INDArray</kbd>).</p>
<p><kbd>feedForwardWithKey()</kbd> accepts two attributes: input data and the minibatch size for feed-forward operations. The input data (features) is in the format of <kbd>JavaPairRDD&lt;K</kbd>, <kbd>INDArray&gt;</kbd>.</p>
<p><span>Note that RDD data is unordered. We need a way to map each input to the respective results (output). Hence, we need to have a key-value pair that maps each input to its respective output. That's the mai</span>n reason why we use key values here. It has nothing to do with the inference process. Values for the minibatch size <span>are used for the trade-off between memory versus computational efficiency.<br/></span></p>


            </article>

            
        </section>
    </body></html>