<html><head></head><body>
		<div class="Content" id="_idContainer294">
			<h1 id="_idParaDest-207"><em class="italics"><a id="_idTextAnchor235"/>Appendix</em></h1>
		</div>
		<div>
			<div class="Content" id="_idContainer295">
			</div>
		</div>
		<div class="Content" id="_idContainer296">
			<h2>About</h2>
			<p>This section is included to assist the learners to perform the activities present in the book. It includes detailed steps that are to be performed by the learners to complete and achieve the objectives of the book.</p>
		</div>
		<div class="Content" id="_idContainer326">
			<h2 id="_idParaDest-208"><a id="_idTextAnchor236"/>Chapter 1: Introduction to Natural Language Processing</h2>
			<h3 id="_idParaDest-209"><a id="_idTextAnchor237"/>Activity 1: Generating word embeddings from a corpus using Word2Vec. </h3>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li>Upload the text corpus from the link aforementioned.</li>
				<li>Import the word2vec from gensim models<p class="snippet">from gensim.models import word2vec</p></li>
				<li>Store the corpus in a variable. <p class="snippet">sentences = word2vec.Text8Corpus('text8')</p></li>
				<li>Fit the word2vec model on the corpus. <p class="snippet">model = word2vec.Word2Vec(sentences, size = 200)</p></li>
				<li>Find the most similar word to 'man'. <p class="snippet">model.most_similar(['man'])</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer297"><img alt="Figure 1.29: Output for similar word embeddings" src="image/C13783_01_29.jpg"/></div><h6>Figure 1.29: Output for similar word embeddings</h6></li>
				<li>'Father' is to 'girl', 'x' is to boy. Find the top 3 words for x. <p class="snippet">model.most_similar(['girl', 'father'], ['boy'], topn=3)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer298">
					<img alt="Figure 1.30: Output for top three words for ‘x’&#13;&#10;" src="image/C13783_01_30.jpg"/>
				</div>
			</div>
			<h6>Figure 1.30: Output for top three words for 'x'</h6>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor238"/>Chapter 2: Applications of Natural Language Processing</h2>
			<h3 id="_idParaDest-211"><a id="_idTextAnchor239"/>Activity 2: Building and training your own POS tagger</h3>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">The first thing to do is pick a corpus that we want to train our tagger on. Import the necessary Python packages. Here, we use the <strong class="inline">nltk</strong> <strong class="inline">treebank</strong> corpus to work on:<p class="snippet">import nltk</p><p class="snippet">nltk.download('treebank')</p><p class="snippet">tagged_sentences = nltk.corpus.treebank.tagged_sents()</p><p class="snippet">print(tagged_sentences[0])</p><p class="snippet">print("Tagged sentences: ", len(tagged_sentences))</p><p class="snippet">print ("Tagged words:", len(nltk.corpus.treebank.tagged_words()))</p></li>
				<li>Next, we need to determine what features our tagger will take into consideration when determining what tag to assign to a word. These can include whether the word is all capitalized, is in lowercase, or has one capital letter:<p class="snippet">def features(sentence, index):</p><p class="snippet">    """ sentence: [w1, w2, ...], index: the index of the word """</p><p class="snippet">    return {</p><p class="snippet">        'word': sentence[index],</p><p class="snippet">        'is_first': index == 0,</p><p class="snippet">        'is_last': index == len(sentence) - 1,</p><p class="snippet">        'is_capitalized': sentence[index][0].upper() == sentence[index][0],</p><p class="snippet">        'is_all_caps': sentence[index].upper() == sentence[index],</p><p class="snippet">        'is_all_lower': sentence[index].lower() == sentence[index],</p><p class="snippet">        'prefix-1': sentence[index][0],</p><p class="snippet">        'prefix-2': sentence[index][:2],</p><p class="snippet">        'prefix-3': sentence[index][:3],</p><p class="snippet">        'suffix-1': sentence[index][-1],</p><p class="snippet">        'suffix-2': sentence[index][-2:],</p><p class="snippet">        'suffix-3': sentence[index][-3:],</p><p class="snippet">        'prev_word': '' if index == 0 else sentence[index - 1],</p><p class="snippet">        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],</p><p class="snippet">        'has_hyphen': '-' in sentence[index],</p><p class="snippet">        'is_numeric': sentence[index].isdigit(),</p><p class="snippet">        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]</p><p class="snippet">    }</p><p class="snippet">import pprint </p><p class="snippet">pprint.pprint(features(['This', 'is', 'a', 'sentence'], 2))</p><p class="snippet"> </p><p class="snippet">{'capitals_inside': False,</p><p class="snippet"> 'has_hyphen': False,</p><p class="snippet"> 'is_all_caps': False,</p><p class="snippet"> 'is_all_lower': True,</p><p class="snippet"> 'is_capitalized': False,</p><p class="snippet"> 'is_first': False,</p><p class="snippet"> 'is_last': False,</p><p class="snippet"> 'is_numeric': False,</p><p class="snippet"> 'next_word': 'sentence',</p><p class="snippet"> 'prefix-1': 'a',</p><p class="snippet"> 'prefix-2': 'a',</p><p class="snippet"> 'prefix-3': 'a',</p><p class="snippet"> 'prev_word': 'is',</p><p class="snippet"> 'suffix-1': 'a',</p><p class="snippet"> 'suffix-2': 'a',</p><p class="snippet"> 'suffix-3': 'a',</p><p class="snippet"> 'word': 'a'}</p></li>
				<li>Create a function to strip the tagged words of their tags so that we can feed them into our tagger:<p class="snippet">def untag(tagged_sentence):</p><p class="snippet">    return [w for w, t in tagged_sentence]</p></li>
				<li>Now we need to build our training set. Our tagger needs to take features individually for each word, but our corpus is actually in the form of sentences, so we need to do a little transforming. Split the data into training and testing sets. Apply this function on the training set.<p class="snippet"># Split the dataset for training and testing</p><p class="snippet">cutoff = int(.75 * len(tagged_sentences))</p><p class="snippet">training_sentences = tagged_sentences[:cutoff]</p><p class="snippet">test_sentences = tagged_sentences[cutoff:]</p><p class="snippet"> </p><p class="snippet">print(len(training_sentences))   # 2935</p><p class="snippet">print(len(test_sentences))      # 979</p><p class="snippet"> and create a function to assign the features to 'X' and append the POS tags to 'Y'.</p><p class="snippet">def transform_to_dataset(tagged_sentences):</p><p class="snippet">    X, y = [], []</p><p class="snippet"> </p><p class="snippet">    for tagged in tagged_sentences:</p><p class="snippet">        for index in range(len(tagged)):</p><p class="snippet">            X.append(features(untag(tagged), index))</p><p class="snippet">            y.append(tagged[index][1])</p><p class="snippet"> </p><p class="snippet">    return X, y</p><p class="snippet"> </p><p class="snippet">X, y = transform_to_dataset(training_sentences)</p><p class="snippet">from sklearn.tree import DecisionTreeClassifier</p><p class="snippet">from sklearn.feature_extraction import DictVectorizer</p><p class="snippet">from sklearn.pipeline import Pipeline</p></li>
				<li>Apply this function on the training set. Now we can train our tagger. It's basically a classifier since it's categorizing words into classes, so we can use a classification algorithm. You can use any that you like or try out a bunch of them to see which works best. Here, we'll use the decision tree classifier. Import the classifier, initialize it, and fit the model on the training data. Print the accuracy score.<p class="snippet">clf = Pipeline([</p><p class="snippet">    ('vectorizer', DictVectorizer(sparse=False)),</p><p class="snippet">    ('classifier', DecisionTreeClassifier(criterion='entropy'))</p><p class="snippet">])</p><p class="snippet"> </p><p class="snippet">clf.fit(X[:10000], y[:10000])   # Use only the first 10K samples if you're running it multiple times. It takes a fair bit :)</p><p class="snippet"> </p><p class="snippet">print('Training completed')</p><p class="snippet"> </p><p class="snippet">X_test, y_test = transform_to_dataset(test_sentences)</p><p class="snippet"> </p><p class="snippet">print("Accuracy:", clf.score(X_test, y_test))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer299">
					<img alt="" src="image/C13783_02_19.jpg"/>
				</div>
			</div>
			<h6>Figure 2.19: Accuracy score</h6>
			<h3 id="_idParaDest-212"><a id="_idTextAnchor240"/>Activity 3: Performing NER on a Tagged Corpus</h3>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Import the necessary Python packages and classes.<p class="snippet">import nltk</p><p class="snippet">nltk.download('treebank')</p><p class="snippet">nltk.download('maxent_ne_chunker')</p><p class="snippet">nltk.download('words')</p></li>
				<li>Print the <strong class="inline">nltk.corpus.treebank.tagged_sents()</strong> to see the tagged corpus that you need extract named entities from.<p class="snippet">nltk.corpus.treebank.tagged_sents()</p><p class="snippet">sent = nltk.corpus.treebank.tagged_sents()[0]</p><p class="snippet">print(nltk.ne_chunk(sent, binary=True))</p></li>
				<li>Store the first sentence of the tagged sentences in a variable.<p class="snippet">sent = nltk.corpus.treebank.tagged_sents()[1]</p></li>
				<li>Use <strong class="inline">nltk.ne_chunk</strong> to perform NER on the sentence. Set binary to True and print the named entities.<p class="snippet">print(nltk.ne_chunk(sent, binary=False))</p><p class="snippet">sent = nltk.corpus.treebank.tagged_sents()[2]</p><p class="snippet">rint(nltk.ne_chunk(sent))</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer300"><img alt="" src="image/C13783_02_20.jpg"/></div></li>
			</ol>
			<h6>Figure 2.20: NER on tagged corpus</h6>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor241"/>Chapter 3: Introduction to Neural Networks</h2>
			<h3 id="_idParaDest-214"><a id="_idTextAnchor242"/>Activity 4: Sentiment Analysis of Reviews</h3>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Open a new <strong class="inline">Jupyter</strong> notebook. Import <strong class="inline">numpy</strong>, <strong class="inline">pandas</strong> and <strong class="inline">matplotlib.pyplot</strong>. Load the dataset into a dataframe.<p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">import pandas as pd</p><p class="snippet">dataset = pd.read_csv('train_comment_small_100.csv', sep=',')</p></li>
				<li>Next step is to clean and prepare the data. Import <strong class="inline">re</strong> and <strong class="inline">nltk</strong>. From <strong class="inline">nltk.corpus</strong> import <strong class="inline">stopwords</strong>. From <strong class="inline">nltk.stem.porter</strong>, import <strong class="inline">PorterStemmer</strong>. Create an array for your cleaned text to be stored in.<p class="snippet">import re</p><p class="snippet">import nltk</p><p class="snippet">nltk.download('stopwords')</p><p class="snippet">from nltk.corpus import stopwords</p><p class="snippet">from nltk.stem.porter import PorterStemmer</p><p class="snippet">corpus = []</p></li>
				<li>Using a for loop, iterate through every instance (every review). Replace all non-alphabets with a ' ' (whitespace). Convert all alphabets into lowercase. Split each review into individual words. Initiate the <strong class="inline">PorterStemmer</strong>. If the word is not a stopword, perform stemming on the word. Join all the individual words back together to form a cleaned review. Append this cleaned review to the array you created.<p class="snippet">for i in range(0, dataset.shape[0]-1):</p><p class="snippet">    review = re.sub('[^a-zA-Z]', ' ', dataset['comment_text'][i])</p><p class="snippet">    review = review.lower()</p><p class="snippet">    review = review.split()</p><p class="snippet">ps = PorterStemmer()</p><p class="snippet">    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]</p><p class="snippet">    review = ' '.join(review)</p><p class="snippet">    corpus.append(review)</p></li>
				<li>Import <strong class="inline">CountVectorizer</strong>. Convert the reviews into word count vectors using <strong class="inline">CountVectorizer</strong>.<p class="snippet">from sklearn.feature_extraction.text import CountVectorizer</p><p class="snippet">cv = CountVectorizer(max_features = 20)</p></li>
				<li>Create an array to store each unique word as its own column, hence making them independent variables.<p class="snippet">X = cv.fit_transform(corpus).toarray()</p><p class="snippet">y = dataset.iloc[:,0]</p><p class="snippet">y1 = y[:99]</p><p class="snippet">y1</p></li>
				<li>Import <strong class="inline">LabelEncoder</strong> from <strong class="inline">sklearn.preprocessing</strong>. Use the <strong class="inline">LabelEncoder</strong> on the target output (<strong class="inline">y</strong>).<p class="snippet">from sklearn import preprocessing</p><p class="snippet">labelencoder_y = preprocessing.LabelEncoder()</p><p class="snippet">y = labelencoder_y.fit_transform(y1)</p></li>
				<li>Import <strong class="inline">train_test_split</strong>. Divide the dataset into a training set and a validation set.<p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)</p></li>
				<li>Import <strong class="inline">StandardScaler</strong> from <strong class="inline">sklearn.preprocessing</strong>. Use the <strong class="inline">StandardScaler</strong> on the features of both the training set and the validation set (<strong class="inline">X</strong>).<p class="snippet">from sklearn.preprocessing import StandardScaler</p><p class="snippet">sc = StandardScaler()</p><p class="snippet">X_train = sc.fit_transform(X_train)</p><p class="snippet">X_test = sc.transform(X_test)</p></li>
				<li>Now the next task is to create the neural network. Import <strong class="inline">keras</strong>. Import <strong class="inline">Sequential</strong> from <strong class="inline">keras.models</strong> and <strong class="inline">Dense</strong> from Keras layers.<p class="snippet">import tensorflow</p><p class="snippet">import keras</p><p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.layers import Dense</p></li>
				<li>Initialize the neural network. Add the first hidden layer with '<strong class="inline">relu</strong>' as the activation function. Repeat step for the second hidden layer. Add the output layer with '<strong class="inline">softmax</strong>' as the activation function. Compile the neural network, using '<strong class="inline">adam</strong>' as the optimizer, '<strong class="inline">binary_crossentropy</strong>' as the loss function and '<strong class="inline">accuracy</strong>' as the performance metric.<p class="snippet">classifier = Sequential()</p><p class="snippet">classifier.add(Dense(output_dim = 20, init = 'uniform', activation = 'relu', input_dim = 20))</p><p class="snippet">classifier.add(Dense(output_dim =20, init = 'uniform', activation = 'relu'))</p><p class="snippet">classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'softmax'))</p><p class="snippet">classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])</p></li>
				<li>Now we need to train the model. Fit the neural network on the training dataset with a <strong class="inline">batch_size</strong> of 3 and a <strong class="inline">nb_epoch</strong> of 5.<p class="snippet">classifier.fit(X_train, y_train, batch_size = 3, nb_epoch = 5)</p><p class="snippet">X_test</p></li>
				<li>Validate the model. Evaluate the neural network and print the accuracy scores to see how it's doing.<p class="snippet">y_pred = classifier.predict(X_test)</p><p class="snippet">scores = classifier.evaluate(X_test, y_pred, verbose=1)</p><p class="snippet">print("Accuracy:", scores[1])</p></li>
				<li>(Optional) Print the confusion matrix by importing <strong class="inline">confusion_matrix</strong> from <strong class="inline">sklearn.metrics</strong>.<p class="snippet">from sklearn.metrics import confusion_matrix</p><p class="snippet">cm = confusion_matrix(y_test, y_pred)</p><p class="snippet">scores</p><p>Your output should look similar to this:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer301">
					<img alt="" src="image/C13783_03_21.jpg"/>
				</div>
			</div>
			<h6>Figure 3.21: Accuracy score for sentiment analysis</h6>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor243"/>Chapter 4: Introduction to convolutional networks</h2>
			<h3 id="_idParaDest-216"><a id="_idTextAnchor244"/>Activity 5: Sentiment Analysis on a real-life dataset</h3>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Import the necessary classes<p class="snippet">from keras.preprocessing.text import Tokenizer</p><p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras import layers</p><p class="snippet">from keras.preprocessing.sequence import pad_sequences</p><p class="snippet">import numpy as np</p><p class="snippet">import pandas as pd</p></li>
				<li>Define your variables and parameters.<p class="snippet">epochs = 20</p><p class="snippet">maxlen = 100</p><p class="snippet">embedding_dim = 50</p><p class="snippet">num_filters = 64</p><p class="snippet">kernel_size = 5</p><p class="snippet">batch_size = 32</p></li>
				<li>Import the data. <p class="snippet">data = pd.read_csv('data/sentiment labelled sentences/yelp_labelled.txt',names=['sentence', 'label'], sep='\t')</p><p class="snippet">data.head()</p><p>Printing this out on a <strong class="inline">Jupyter</strong> notebook should display:</p><div class="IMG---Figure" id="_idContainer302"><img alt="Figure 4.27: Labelled dataset&#13;&#10;" src="image/C13783_04_271.jpg"/></div><h6>Figure 4.27: Labelled dataset</h6></li>
				<li>Select the '<strong class="inline">sentence</strong>' and '<strong class="inline">label</strong>' columns<p class="snippet">sentences=data['sentence'].values</p><p class="snippet">labels=data['label'].values</p></li>
				<li>Split your data into training and test set <p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet">X_train, X_test, y_train, y_test = train_test_split(</p><p class="snippet">    sentences, labels, test_size=0.30, random_state=1000)</p></li>
				<li>Tokenize<p class="snippet">tokenizer = Tokenizer(num_words=5000)</p><p class="snippet">tokenizer.fit_on_texts(X_train)</p><p class="snippet">X_train = tokenizer.texts_to_sequences(X_train)</p><p class="snippet">X_test = tokenizer.texts_to_sequences(X_test)</p><p class="snippet">vocab_size = len(tokenizer.word_index) + 1 #The vocabulary size has an additional 1 due to the 0 reserved index</p></li>
				<li>Pad in order to ensure that all sequences have the same length<p class="snippet">X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)</p><p class="snippet">X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)</p></li>
				<li>Create the model. Note that we use a sigmoid activation function on the last layer and the binary cross entropy for calculating loss. This is because we are doing a binary classification.<p class="snippet">model = Sequential()</p><p class="snippet">model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))</p><p class="snippet">model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))</p><p class="snippet">model.add(layers.GlobalMaxPooling1D())</p><p class="snippet">model.add(layers.Dense(10, activation='relu'))</p><p class="snippet">model.add(layers.Dense(1, activation='sigmoid'))</p><p class="snippet">model.compile(optimizer='adam',</p><p class="snippet">              loss='binary_crossentropy',</p><p class="snippet">              metrics=['accuracy'])</p><p class="snippet">model.summary()</p><p>The above code should yield </p><div class="IMG---Figure" id="_idContainer303"><img alt="" src="image/C13783_04_28.jpg"/></div><h6>Figure 4.28: Model summary</h6><p>The model can be visualized as follows as well:</p><div class="IMG---Figure" id="_idContainer304"><img alt="Figure 4.29: Model visualization&#13;&#10;" src="image/C13783_04_29.jpg"/></div><h6>Figure 4.29: Model visualization</h6></li>
				<li>Train and test the model.<p class="snippet">model.fit(X_train, y_train,</p><p class="snippet">                    epochs=epochs,</p><p class="snippet">                    verbose=False,</p><p class="snippet">                    validation_data=(X_test, y_test),</p><p class="snippet">                    batch_size=batch_size)</p><p class="snippet">loss, accuracy = model.evaluate(X_train, y_train, verbose=False)</p><p class="snippet">print("Training Accuracy: {:.4f}".format(accuracy))</p><p class="snippet">loss, accuracy = model.evaluate(X_test, y_test, verbose=False)</p><p class="snippet">print("Testing Accuracy:  {:.4f}".format(accuracy))</p><p>The accuracy output should be as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer305">
					<img alt="Figure 4.30: Accuracy score" src="image/C13783_04_30.jpg"/>
				</div>
			</div>
			<h6>Figure 4.30: Accuracy score</h6>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor245"/>Chapter 5: Foundations of Recurrent Neural Network</h2>
			<h3 id="_idParaDest-218"><a id="_idTextAnchor246"/>Activity 6: Solve a problem with RNN – Author Attribution</h3>
			<p><strong class="bold">Solution:</strong></p>
			<h3 id="_idParaDest-219"><a id="_idTextAnchor247"/>Prepare the data</h3>
			<p>We begin by setting up the data pre-processing pipeline. For each one of the authors, we aggregate all the known papers into a single long text. We assume that style does not change across the various papers, hence a single text is equivalent to multiple small ones yet it is much easier to deal with programmatically.</p>
			<p>For each paper of each author we perform the following steps:</p>
			<ol>
				<li value="1">Convert all text into lower-case (ignoring the fact that capitalization may be a stylistic property)</li>
				<li>Converting all newlines and multiple whitespaces into single whitespaces</li>
				<li>Remove any mention of the authors' names, otherwise we risk data leakage (authors names are <em class="italics">hamilton</em> and <em class="italics">madison</em>)</li>
				<li>Do the above steps in a function as it is needed for predicting the unknown papers.<p class="snippet">import numpy as np</p><p class="snippet">import os</p><p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet"># Classes for A/B/Unknown</p><p class="snippet">A = 0</p><p class="snippet">B = 1</p><p class="snippet">UNKNOWN = -1</p><p class="snippet">def preprocess_text(file_path):</p><p class="snippet">    with open(file_path, 'r') as f:</p><p class="snippet">        lines = f.readlines()</p><p class="snippet">        text = ' '.join(lines[1:]).replace("\n", ' ').replace('  ',' ').lower().replace('hamilton','').replace('madison', '')</p><p class="snippet">        text = ' '.join(text.split())</p><p class="snippet">        return text</p><p class="snippet"># Concatenate all the papers known to be written by A/B into a single long text</p><p class="snippet">all_authorA, all_authorB = '',''</p><p class="snippet">for x in os.listdir('./papers/A/'):</p><p class="snippet">    all_authorA += preprocess_text('./papers/A/' + x)</p><p class="snippet">for x in os.listdir('./papers/B/'):</p><p class="snippet">    all_authorB += preprocess_text('./papers/B/' + x)</p><p class="snippet">    </p><p class="snippet"># Print lengths of the large texts</p><p class="snippet">print("AuthorA text length: {}".format(len(all_authorA)))</p><p class="snippet">print("AuthorB text length: {}".format(len(all_authorB)))</p><p>The output for this should be as follows:</p><div class="IMG---Figure" id="_idContainer306"><img alt="Figure 5.34: Text length count&#13;&#10;" src="image/C13783_05_341.jpg"/></div><h6>Figure 5.34: Text length count</h6><p>The next step is to break the long text for each author into many small sequences. As described above, we empirically choose a length for the sequence and use it throughout the model's lifecycle. We get our full dataset by labeling each sequence with its author.</p><p>To break the long texts into smaller sequences we use the <strong class="inline">Tokenizer</strong> class from the <strong class="inline">keras</strong> framework. In particular, note that we set it up to tokenize according to characters and not words.</p></li>
				<li>Choose <strong class="inline">SEQ_LEN</strong> hyper parameter, this might have to be changed if the model doesn't fit well to training data.</li>
				<li>Write a function <strong class="inline">make_subsequences</strong> to turn each document into sequences of length SEQ_LEN and give it a correct label.</li>
				<li>Use Keras <strong class="inline">Tokenizer</strong> with <strong class="inline">char_level=True</strong></li>
				<li>Fit the tokenizer on all the texts</li>
				<li>Use this tokenizer to convert all texts into sequences using <strong class="inline">texts_to_sequences()</strong></li>
				<li>Use <strong class="inline">make_subsequences()</strong> to turn these sequences into appropriate shape and length<p class="snippet">from keras.preprocessing.text import Tokenizer</p><p class="snippet"># Hyperparameter - sequence length to use for the model</p><p class="snippet">SEQ_LEN = 30</p><p class="snippet">def make_subsequences(long_sequence, label, sequence_length=SEQ_LEN):</p><p class="snippet">    len_sequences = len(long_sequence)</p><p class="snippet">    X = np.zeros(((len_sequences - sequence_length)+1, sequence_length))</p><p class="snippet">    y = np.zeros((X.shape[0], 1))</p><p class="snippet">    for i in range(X.shape[0]):</p><p class="snippet">        X[i] = long_sequence[i:i+sequence_length]</p><p class="snippet">        y[i] = label</p><p class="snippet">    return X,y</p><p class="snippet">        </p><p class="snippet"># We use the Tokenizer class from Keras to convert the long texts into a sequence of characters (not words)</p><p class="snippet">tokenizer = Tokenizer(char_level=True)</p><p class="snippet"># Make sure to fit all characters in texts from both authors</p><p class="snippet">tokenizer.fit_on_texts(all_authorA + all_authorB)</p><p class="snippet">authorA_long_sequence = tokenizer.texts_to_sequences([all_authorA])[0]</p><p class="snippet">authorB_long_sequence = tokenizer.texts_to_sequences([all_authorB])[0]</p><p class="snippet"># Convert the long sequences into sequence and label pairs</p><p class="snippet">X_authorA, y_authorA = make_subsequences(authorA_long_sequence, A)</p><p class="snippet">X_authorB, y_authorB = make_subsequences(authorB_long_sequence, B)</p><p class="snippet"># Print sizes of available data</p><p class="snippet">print("Number of characters: {}".format(len(tokenizer.word_index)))</p><p class="snippet">print('author A sequences: {}'.format(X_authorA.shape))</p><p class="snippet">print('author B sequences: {}'.format(X_authorB.shape))</p><p>The output should be as follows:</p><div class="IMG---Figure" id="_idContainer307"><img alt="Figure 5.35: Character count of sequences&#13;&#10;" src="image/C13783_05_351.jpg"/></div><h6>Figure 5.35: Character count of sequences</h6></li>
				<li>Compare the number of raw characters to the number of labeled sequences for each author. Deep Learning requires many examples of each input. The following code calculates the number of total and unique words in the texts.<p class="snippet"># Calculate the number of unique words in the text</p><p class="snippet">word_tokenizer = Tokenizer()</p><p class="snippet">word_tokenizer.fit_on_texts([all_authorA, all_authorB])</p><p class="snippet">print("Total word count: ", len((all_authorA + ' ' + all_authorB).split(' ')))</p><p class="snippet">print("Total number of unique words: ", len(word_tokenizer.word_index))</p><p>The output should be as follows:</p><div class="IMG---Figure" id="_idContainer308"><img alt="Figure 5.36: Total word count and unique word count&#13;&#10;" src="image/C13783_05_36.jpg"/></div><h6>Figure 5.36: Total word count and unique word count</h6><p>We now proceed to create our train, validation sets.</p></li>
				<li>Stack <strong class="inline">x</strong> data together and y data together.</li>
				<li>Use <strong class="inline">train_test_split</strong> to split the dataset into 80% training and 20% validation.</li>
				<li>Reshape the data to make sure that they are sequences of correct length.<p class="snippet"># Take equal amounts of sequences from both authors</p><p class="snippet">X = np.vstack((X_authorA, X_authorB))</p><p class="snippet">y = np.vstack((y_authorA, y_authorB))</p><p class="snippet"># Break data into train and test sets</p><p class="snippet">X_train, X_val, y_train, y_val = train_test_split(X,y, train_size=0.8)</p><p class="snippet"># Data is to be fed into RNN - ensure that the actual data is of size [batch size, sequence length]</p><p class="snippet">X_train = X_train.reshape(-1, SEQ_LEN)</p><p class="snippet">X_val =  X_val.reshape(-1, SEQ_LEN) </p><p class="snippet"># Print the shapes of the train, validation and test sets</p><p class="snippet">print("X_train shape: {}".format(X_train.shape))</p><p class="snippet">print("y_train shape: {}".format(y_train.shape))</p><p class="snippet">print("X_validate shape: {}".format(X_val.shape))</p><p class="snippet">print("y_validate shape: {}".format(y_val.shape))</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer309"><img alt="Figure 5.37: Testing and training datasets" src="image/C13783_05_37.jpg"/></div><h6>Figure 5.37: Testing and training datasets</h6><p>Finally, we construct the model graph and perform the training procedure.</p></li>
				<li>Create a model using <strong class="inline">RNN</strong> and <strong class="inline">Dense</strong> layers.</li>
				<li>Since its a binary classification problem, the output layer should be <strong class="inline">Dense</strong> with <strong class="inline">sigmoid</strong> activation.</li>
				<li>Compile the model with <strong class="inline">optimizer</strong>, appropriate loss function and metrics.</li>
				<li>Print the summary of the model.<p class="snippet">from keras.layers import SimpleRNN, Embedding, Dense</p><p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.optimizers import SGD, Adadelta, Adam</p><p class="snippet">Embedding_size = 100</p><p class="snippet">RNN_size = 256</p><p class="snippet">model = Sequential()</p><p class="snippet">model.add(Embedding(len(tokenizer.word_index)+1, Embedding_size, input_length=30))</p><p class="snippet">model.add(SimpleRNN(RNN_size, return_sequences=False))</p><p class="snippet">model.add(Dense(1, activation='sigmoid'))</p><p class="snippet">model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])</p><p class="snippet">model.summary()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer310"><img alt="" src="image/C13783_05_38.jpg"/></div><h6>Figure 5.38: Model summary</h6></li>
				<li>Decide upon the batch size, epochs and train the model using training data and validate with validation data</li>
				<li>Based on the results, go back to model above, change it if needed (use more layers, use regularization, dropout, etc., use different optimizer, or a different learning rate, etc.)</li>
				<li>Change <strong class="inline">Batch_size</strong>, <strong class="inline">epochs</strong> if needed.<p class="snippet">Batch_size = 4096</p><p class="snippet">Epochs = 20</p><p class="snippet">model.fit(X_train, y_train, batch_size=Batch_size, epochs=Epochs, validation_data=(X_val, y_val))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer311">
					<img alt="" src="image/C13783_05_39.jpg"/>
				</div>
			</div>
			<h6>Figure 5.39: Epoch training</h6>
			<h3 id="_idParaDest-220"><a id="_idTextAnchor248"/>Applying the Model to the Unknown Papers</h3>
			<p>Do this all the papers in the Unknown folder</p>
			<ol>
				<li value="1">Preprocess them same way as training set (lower case, removing white lines, etc.)</li>
				<li>Use <strong class="inline">tokenizer</strong> and <strong class="inline">make_subsequences</strong> function above to turn them into sequences of required size.</li>
				<li>Use the model to predict on these sequences.</li>
				<li>Count the number of sequences assigned to author <strong class="bold">A</strong> and the ones assigned to author <strong class="bold">B</strong></li>
				<li>Based on the count, pick the author with highest votes/count<p class="snippet">for x in os.listdir('./papers/Unknown/'):</p><p class="snippet">    unknown = preprocess_text('./papers/Unknown/' + x)</p><p class="snippet">    unknown_long_sequences = tokenizer.texts_to_sequences([unknown])[0]</p><p class="snippet">    X_sequences, _ = make_subsequences(unknown_long_sequences, UNKNOWN)</p><p class="snippet">    X_sequences = X_sequences.reshape((-1,SEQ_LEN))</p><p class="snippet">    </p><p class="snippet">    votes_for_authorA = 0</p><p class="snippet">    votes_for_authorB = 0</p><p class="snippet">    </p><p class="snippet">    y = model.predict(X_sequences)</p><p class="snippet">    y = y&gt;0.5</p><p class="snippet">    votes_for_authorA = np.sum(y==0)</p><p class="snippet">    votes_for_authorB = np.sum(y==1)</p><p class="snippet">    </p><p class="snippet">    </p><p class="snippet">    print("Paper {} is predicted to have been written by {}, {} to {}".format(</p><p class="snippet">                x.replace('paper_','').replace('.txt',''), </p><p class="snippet">                ("Author A" if votes_for_authorA &gt; votes_for_authorB else "Author B"),</p><p class="snippet">                max(votes_for_authorA, votes_for_authorB), min(votes_for_authorA, votes_for_authorB)))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer312">
					<img alt="Figure 5.40: Output for author attribution" src="image/C13783_05_40.jpg"/>
				</div>
			</div>
			<h6>Figure 5.40: Output for author attribution</h6>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor249"/>Chapter 6: Foundations of GRUs</h2>
			<h3 id="_idParaDest-222"><a id="_idTextAnchor250"/>Activity 7: Develop a sentiment classification model using Simple RNN</h3>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Load the dataset.<p class="snippet">from keras.datasets import imdb</p><p class="snippet">max_features = 10000</p><p class="snippet">maxlen = 500</p><p class="snippet"> </p><p class="snippet">(train_data, y_train), (test_data, y_test) = imdb.load_data(num_words=max_features)</p><p class="snippet">print('Number of train sequences: ', len(train_data))</p><p class="snippet">print('Number of test sequences: ', len(test_data))</p></li>
				<li>Pad sequences so that each sequence has the same number characters.<p class="snippet">from keras.preprocessing import sequence</p><p class="snippet">train_data = sequence.pad_sequences(train_data, maxlen=maxlen)</p><p class="snippet">test_data = sequence.pad_sequences(test_data, maxlen=maxlen)</p></li>
				<li>Define and compile model using <strong class="inline">SimpleRNN</strong> with 32 hidden units.<p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.layers import Embedding</p><p class="snippet">from keras.layers import Dense</p><p class="snippet">from keras.layers import GRU</p><p class="snippet">from keras.layers import SimpleRNN</p><p class="snippet">model = Sequential()</p><p class="snippet">model.add(Embedding(max_features, 32))</p><p class="snippet">model.add(SimpleRNN(32))</p><p class="snippet">model.add(Dense(1, activation='sigmoid'))</p><p class="snippet"> </p><p class="snippet">model.compile(optimizer='rmsprop',</p><p class="snippet">              loss='binary_crossentropy',</p><p class="snippet">              metrics=['acc'])</p><p class="snippet"> </p><p class="snippet">history = model.fit(train_data, y_train,</p><p class="snippet">                    epochs=10,</p><p class="snippet">                    batch_size=128,</p><p class="snippet">                    validation_split=0.2)</p></li>
				<li>Plot the validation and training accuracy and losses.<p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet"> </p><p class="snippet">def plot_results(history):</p><p class="snippet">    acc = history.history['acc']</p><p class="snippet">    val_acc = history.history['val_acc']</p><p class="snippet">    loss = history.history['loss']</p><p class="snippet">    val_loss = history.history['val_loss']</p><p class="snippet">    </p><p class="snippet">    epochs = range(1, len(acc) + 1)</p><p class="snippet">    plt.plot(epochs, acc, 'bo', label='Training Accuracy')</p><p class="snippet">    plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')</p><p class="snippet"> </p><p class="snippet">    plt.title('Training and validation Accuracy')</p><p class="snippet">    plt.legend()</p><p class="snippet">    plt.figure()</p><p class="snippet">    plt.plot(epochs, loss, 'bo', label='Training Loss')</p><p class="snippet">    plt.plot(epochs, val_loss, 'b', label='Validation Loss')</p><p class="snippet">    plt.title('Training and validation Loss')</p><p class="snippet">    plt.legend()</p><p class="snippet">    plt.show()</p></li>
				<li>Plot the model<p class="snippet">plot_results(history)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer313">
					<img alt="Figure 6.29: Training and validation accuracy loss&#13;&#10;" src="image/C13783_06_29.jpg"/>
				</div>
			</div>
			<h6>Figure 6.29: Training and validation accuracy loss</h6>
			<h3 id="_idParaDest-223"><a id="_idTextAnchor251"/>Activity 8: Train your own character generation model with a dataset of your choice</h3>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Load the text file and import the necessary Python packages and classes.<p class="snippet">import sys</p><p class="snippet">import random</p><p class="snippet">import string</p><p class="snippet">import numpy as np</p><p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.layers import Dense</p><p class="snippet">from keras.layers import LSTM, GRU</p><p class="snippet">from keras.optimizers import RMSprop</p><p class="snippet">from keras.models import load_model</p><p class="snippet"># load text</p><p class="snippet">def load_text(filename):</p><p class="snippet">    with open(filename, 'r') as f:</p><p class="snippet">        text = f.read()</p><p class="snippet">    return text</p><p class="snippet">in_filename = 'drive/shakespeare_poems.txt' # Add your own text file here</p><p class="snippet">text = load_text(in_filename)</p><p class="snippet">print(text[:200])</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer314"><img alt="Figure 6.30: Sonnets from Shakespeare" src="image/C13783_06_30.jpg"/></div><h6>Figure 6.30: Sonnets from Shakespeare</h6></li>
				<li>Create dictionaries mapping characters to indices and vice-versa.<p class="snippet">chars = sorted(list(set(text)))</p><p class="snippet">print('Number of distinct characters:', len(chars))</p><p class="snippet">char_indices = dict((c, i) for i, c in enumerate(chars))</p><p class="snippet">indices_char = dict((i, c) for i, c in enumerate(chars))</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer315"><img alt="Figure 6.31: Distinct character count" src="image/C13783_06_31.jpg"/></div><h6>Figure 6.31: Distinct character count</h6></li>
				<li>Create sequences from the text.<p class="snippet">max_len_chars = 40</p><p class="snippet">step = 3</p><p class="snippet">sentences = []</p><p class="snippet">next_chars = []</p><p class="snippet">for i in range(0, len(text) - max_len_chars, step):</p><p class="snippet">    sentences.append(text[i: i + max_len_chars])</p><p class="snippet">    next_chars.append(text[i + max_len_chars])</p><p class="snippet">print('nb sequences:', len(sentences))</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer316"><img alt="Figure 6.32: nb sequence count" src="image/C13783_06_32.jpg"/></div><h6>Figure 6.32: nb sequence count</h6></li>
				<li>Make input and output arrays to feed the model.<p class="snippet">x = np.zeros((len(sentences), max_len_chars, len(chars)), dtype=np.bool)</p><p class="snippet">y = np.zeros((len(sentences), len(chars)), dtype=np.bool)</p><p class="snippet">for i, sentence in enumerate(sentences):</p><p class="snippet">    for t, char in enumerate(sentence):</p><p class="snippet">        x[i, t, char_indices[char]] = 1</p><p class="snippet">    y[i, char_indices[next_chars[i]]] = 1</p></li>
				<li>Build and train the model using GRU and save the model.<p class="snippet">print('Build model...')</p><p class="snippet">model = Sequential()</p><p class="snippet">model.add(GRU(128, input_shape=(max_len_chars, len(chars))))</p><p class="snippet">model.add(Dense(len(chars), activation='softmax'))</p><p class="snippet">optimizer = RMSprop(lr=0.01)</p><p class="snippet">model.compile(loss='categorical_crossentropy', optimizer=optimizer)</p><p class="snippet">model.fit(x, y,batch_size=128,epochs=10)</p><p class="snippet">model.save("poem_gen_model.h5")</p></li>
				<li>Define sampling and generation functions.<p class="snippet">def sample(preds, temperature=1.0):</p><p class="snippet">    # helper function to sample an index from a probability array</p><p class="snippet">    preds = np.asarray(preds).astype('float64')</p><p class="snippet">    preds = np.log(preds) / temperature</p><p class="snippet">    exp_preds = np.exp(preds)</p><p class="snippet">    preds = exp_preds / np.sum(exp_preds)</p><p class="snippet">    probas = np.random.multinomial(1, preds, 1)</p><p class="snippet">    return np.argmax(probas)</p></li>
				<li>Generate text.<p class="snippet">from keras.models import load_model</p><p class="snippet">model_loaded = load_model('poem_gen_model.h5')</p><p class="snippet">def generate_poem(model, num_chars_to_generate=400):</p><p class="snippet">    start_index = random.randint(0, len(text) - max_len_chars - 1)</p><p class="snippet">    generated = ''</p><p class="snippet">    sentence = text[start_index: start_index + max_len_chars]</p><p class="snippet">    generated += sentence</p><p class="snippet">    print("Seed sentence: {}".format(generated))</p><p class="snippet">    for i in range(num_chars_to_generate):</p><p class="snippet">        x_pred = np.zeros((1, max_len_chars, len(chars)))</p><p class="snippet">        for t, char in enumerate(sentence):</p><p class="snippet">            x_pred[0, t, char_indices[char]] = 1.</p><p class="snippet">            </p><p class="snippet">        preds = model.predict(x_pred, verbose=0)[0]</p><p class="snippet">        next_index = sample(preds, 1)</p><p class="snippet">        next_char = indices_char[next_index]</p><p class="snippet">        generated += next_char</p><p class="snippet">        sentence = sentence[1:] + next_char</p><p class="snippet">    return generated</p><p class="snippet">generate_poem(model_loaded, 100)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer317">
					<img alt="Figure 6.33: Generated text output&#13;&#10;" src="image/C13783_06_33.jpg"/>
				</div>
			</div>
			<h6>Figure 6.33: Generated text output</h6>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor252"/>Chapter 7: Foundations of LSTM</h2>
			<p>Activity 9: Build a Spam or Ham classifier using a Simple RNN</p>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Import required Python packages<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">from keras.models import Model, Sequential</p><p class="snippet">from keras.layers import SimpleRNN, Dense,Embedding</p><p class="snippet">from keras.preprocessing.text import Tokenizer</p><p class="snippet">from keras.preprocessing import sequence</p></li>
				<li>Read the input file containing a column that contains text and another column that contains the label for the text depicting whether the text is spam or not.<p class="snippet">df = pd.read_csv("drive/spam.csv", encoding="latin")</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer318"><img alt="Figure 7.35: Input data file" src="image/C13783_07_351.jpg"/></div><h6>Figure 7.35: Input data file</h6></li>
				<li>Label the columns in the input data.<p class="snippet">df = df[["v1","v2"]]</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer319"><img alt="Figure 7.36: Labelled input data" src="image/C13783_07_361.jpg"/></div><h6>Figure 7.36: Labelled input data</h6></li>
				<li>Count spam, ham characters in the <strong class="inline">v1</strong> column.<p class="snippet">df["v1"].value_counts()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer320"><img alt="Figure 7.37: Value counts for spam or ham" src="image/C13783_07_371.jpg"/></div><h6>Figure 7.37: Value counts for spam or ham</h6></li>
				<li>Get <strong class="inline">X </strong>as feature and <strong class="inline">Y</strong> as target.<p class="snippet">lab_map = {"ham":0, "spam":1}</p><p class="snippet">X = df["v2"].values</p><p class="snippet">Y = df["v1"].map(lab_map).values</p></li>
				<li>Convert to sequences and pad the sequences.<p class="snippet">max_words = 100</p><p class="snippet">mytokenizer = Tokenizer(nb_words=max_words,lower=True, split=" ")</p><p class="snippet">mytokenizer.fit_on_texts(X)</p><p class="snippet">text_tokenized = mytokenizer.texts_to_sequences(X)</p><p class="snippet">text_tokenized</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer321"><img alt="Figure 7.38: Tokenized data" src="image/C13783_07_381.jpg"/></div><h6>Figure 7.38: Tokenized data</h6></li>
				<li>Train the sequences<p class="snippet">max_len = 50</p><p class="snippet">sequences = sequence.pad_sequences(text_tokenized,maxlen=max_len)</p><p class="snippet">sequences</p></li>
				<li>Build the model<p class="snippet">model = Sequential()</p><p class="snippet">model.add(Embedding(max_words, 20, input_length=max_len))</p><p class="snippet">model.add(SimpleRNN(64))</p><p class="snippet">model.add(Dense(1, activation="sigmoid"))</p><p class="snippet">model.compile(loss='binary_crossentropy',</p><p class="snippet">              optimizer='adam',</p><p class="snippet">              metrics=['accuracy'])</p><p class="snippet">model.fit(sequences,Y,batch_size=128,epochs=10,</p><p class="snippet">          validation_split=0.2)</p></li>
				<li>Predict the mail category on new test data.<p class="snippet">inp_test_seq = "WINNER! U win a 500 prize reward &amp; free entry to FA cup final tickets! Text FA to 34212 to receive award"</p><p class="snippet">test_sequences = mytokenizer.texts_to_sequences(np.array([inp_test_seq]))</p><p class="snippet">test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)</p><p class="snippet">model.predict(test_sequences_matrix)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer322">
					<img alt="Figure 7.39: Output for new test data" src="image/C13783_07_391.jpg"/>
				</div>
			</div>
			<h6>Figure 7.39: Output for new test data</h6>
			<h3 id="_idParaDest-225"><a id="_idTextAnchor253"/>Activity 10: Create a French to English translation model</h3>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Import the necessary Python packages and classes.<p class="snippet">import os</p><p class="snippet">import re</p><p class="snippet">import numpy as np</p></li>
				<li>Read the file in sentence pairs.<p class="snippet">with open("fra.txt", 'r', encoding='utf-8') as f:</p><p class="snippet">    lines = f.read().split('\n')</p><p class="snippet">num_samples = 20000 # Using only 20000 pairs for this example</p><p class="snippet">lines_to_use = lines[: min(num_samples, len(lines) - 1)]</p></li>
				<li>Remove <strong class="inline">\u202f</strong> character<p class="snippet">for l in range(len(lines_to_use)):</p><p class="snippet">    lines_to_use[l] = re.sub("\u202f", "", lines_to_use[l])</p><p class="snippet">for l in range(len(lines_to_use)):</p><p class="snippet">    lines_to_use[l] = re.sub("\d", " NUMBER_PRESENT ", lines_to_use[l])</p></li>
				<li>Append '<strong class="bold">BEGIN_</strong> ' and ' <strong class="bold">_END</strong>' words to target sequences. Map words to integers.<p class="snippet">input_texts = []</p><p class="snippet">target_texts = []</p><p class="snippet">input_words = set()</p><p class="snippet">target_words = set()</p><p class="snippet">for line in lines_to_use:</p><p class="snippet">    target_text, input_text = line.split('\t')</p><p class="snippet">    target_text = 'BEGIN_ ' + target_text + ' _END'</p><p class="snippet">    input_texts.append(input_text)</p><p class="snippet">    target_texts.append(target_text)</p><p class="snippet">    for word in input_text.split():</p><p class="snippet">        if word not in input_words:</p><p class="snippet">            input_words.add(word)</p><p class="snippet">    for word in target_text.split():</p><p class="snippet">        if word not in target_words:</p><p class="snippet">            target_words.add(word)</p><p class="snippet">max_input_seq_length = max([len(i.split()) for i in input_texts])</p><p class="snippet">max_target_seq_length = max([len(i.split()) for i in target_texts])</p><p class="snippet">input_words = sorted(list(input_words))</p><p class="snippet">target_words = sorted(list(target_words))</p><p class="snippet">num_encoder_tokens = len(input_words)</p><p class="snippet">num_decoder_tokens = len(target_words)</p></li>
				<li>Define encoder-decoder inputs.<p class="snippet">input_token_index = dict(</p><p class="snippet">    [(word, i) for i, word in enumerate(input_words)])</p><p class="snippet">target_token_index = dict(</p><p class="snippet">    [(word, i) for i, word in enumerate(target_words)])</p><p class="snippet">encoder_input_data = np.zeros(</p><p class="snippet">    (len(input_texts), max_input_seq_length),</p><p class="snippet">    dtype='float32')</p><p class="snippet">decoder_input_data = np.zeros(</p><p class="snippet">    (len(target_texts), max_target_seq_length),</p><p class="snippet">    dtype='float32')</p><p class="snippet">decoder_target_data = np.zeros(</p><p class="snippet">    (len(target_texts), max_target_seq_length, num_decoder_tokens),</p><p class="snippet">    dtype='float32')</p><p class="snippet">for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):</p><p class="snippet">    for t, word in enumerate(input_text.split()):</p><p class="snippet">        encoder_input_data[i, t] = input_token_index[word]</p><p class="snippet">    for t, word in enumerate(target_text.split()):</p><p class="snippet">        decoder_input_data[i, t] = target_token_index[word]</p><p class="snippet">        if t &gt; 0:</p><p class="snippet">            # decoder_target_data is ahead of decoder_input_data #by one timestep</p><p class="snippet">            decoder_target_data[i, t - 1, target_token_index[word]] = 1.</p></li>
				<li>Build the model.<p class="snippet">from keras.layers import Input, LSTM, Embedding, Dense</p><p class="snippet">from keras.models import Model</p><p class="snippet">embedding_size = 50</p></li>
				<li>Initiate encoder training.<p class="snippet">encoder_inputs = Input(shape=(None,))</p><p class="snippet">encoder_after_embedding =  Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)</p><p class="snippet">encoder_lstm = LSTM(50, return_state=True)_, </p><p class="snippet">state_h, state_c = encoder_lstm(encoder_after_embedding)</p><p class="snippet">encoder_states = [state_h, state_c]</p></li>
				<li>Initiate decoder training.<p class="snippet">decoder_inputs = Input(shape=(None,))</p><p class="snippet">decoder_after_embedding = Embedding(num_decoder_tokens, embedding_size)(decoder_inputs)</p><p class="snippet">decoder_lstm = LSTM(50, return_sequences=True, return_state=True)</p><p class="snippet">decoder_outputs, _, _ = decoder_lstm(decoder_after_embedding,</p><p class="snippet">                                     initial_state=encoder_states)</p><p class="snippet">decoder_dense = Dense(num_decoder_tokens, activation='softmax')</p><p class="snippet">decoder_outputs = decoder_dense(decoder_outputs)</p></li>
				<li>Define the final model.<p class="snippet">model = Model([encoder_inputs, decoder_inputs], decoder_outputs)</p><p class="snippet">model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])</p><p class="snippet">model.fit([encoder_input_data, decoder_input_data], </p><p class="snippet">          decoder_target_data,</p><p class="snippet">          batch_size=128,</p><p class="snippet">          epochs=20,</p><p class="snippet">          validation_split=0.05)</p></li>
				<li>Provide inferences to encoder and decoder<p class="snippet"># encoder part</p><p class="snippet">encoder_model = Model(encoder_inputs, encoder_states)</p><p class="snippet"># decoder part</p><p class="snippet">decoder_state_input_h = Input(shape=(50,))</p><p class="snippet">decoder_state_input_c = Input(shape=(50,))</p><p class="snippet">decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]</p><p class="snippet">decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(decoder_after_embedding, initial_state=decoder_states_inputs)</p><p class="snippet">decoder_states_inf = [state_h_inf, state_c_inf]</p><p class="snippet">decoder_outputs_inf = decoder_dense(decoder_outputs_inf)</p><p class="snippet">decoder_model = Model(</p><p class="snippet">    [decoder_inputs] + decoder_states_inputs,</p><p class="snippet">    [decoder_outputs_inf] + decoder_states_inf)</p></li>
				<li>Reverse-lookup token index to decode sequences<p class="snippet">reverse_input_word_index = dict(</p><p class="snippet">    (i, word) for word, i in input_token_index.items())</p><p class="snippet">reverse_target_word_index = dict(</p><p class="snippet">    (i, word) for word, i in target_token_index.items())</p><p class="snippet">def decode_sequence(input_seq):</p></li>
				<li>Encode input as a state vector<p class="snippet">    states_value = encoder_model.predict(input_seq)</p></li>
				<li>Generate empty target sequence of length 1.<p class="snippet">    target_seq = np.zeros((1,1))</p></li>
				<li>Populate the first character of target sequence with the start character.<p class="snippet">    target_seq[0, 0] = target_token_index['BEGIN_']</p></li>
				<li>Sampling loop for a batch of sequences<p class="snippet">stop_condition = False</p><p class="snippet">    decoded_sentence = ''</p><p class="snippet">    </p><p class="snippet">    while not stop_condition:</p><p class="snippet">        output_tokens, h, c = decoder_model.predict(</p><p class="snippet">            [target_seq] + states_value)</p></li>
				<li>Sample a token.<p class="snippet">        sampled_token_index = np.argmax(output_tokens)</p><p class="snippet">        sampled_word = reverse_target_word_index[sampled_token_index]</p><p class="snippet">        decoded_sentence += ' ' + sampled_word</p></li>
				<li> Exit condition: either hit max length or find stop character.<p class="snippet">        if (sampled_word == '_END' or</p><p class="snippet">           len(decoded_sentence) &gt; 60):</p><p class="snippet">            stop_condition = True</p></li>
				<li>Update the target sequence (of length 1).<p class="snippet">        target_seq = np.zeros((1,1))</p><p class="snippet">        target_seq[0, 0] = sampled_token_index</p></li>
				<li>Update states<p class="snippet">        states_value = [h, c]</p><p class="snippet">        </p><p class="snippet">    return decoded_sentence</p></li>
				<li>Inference for user input: take in a word sequence, convert the sequence word by word into encoded.<p class="snippet">text_to_translate = "Où est ma voiture??"</p><p class="snippet">encoder_input_to_translate = np.zeros(</p><p class="snippet">    (1, max_input_seq_length),</p><p class="snippet">    dtype='float32')</p><p class="snippet">for t, word in enumerate(text_to_translate.split()):</p><p class="snippet">    encoder_input_to_translate[0, t] = input_token_index[word]</p><p class="snippet">decode_sequence(encoder_input_to_translate)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer323">
					<img alt="Figure 7.47: French to English translator" src="image/C13783_07_47.jpg"/>
				</div>
			</div>
			<h6>Figure 7.47: French to English translator</h6>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor254"/>Chapter 8: State of the art in Natural Language Processing</h2>
			<h3 id="_idParaDest-227"><a id="_idTextAnchor255"/>Activity 11: Build a Text Summarization Model</h3>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Import the necessary Python packages and classes.<p class="snippet"><span class="None">import os</span></p><p class="snippet"><span class="None">import re</span></p><p class="snippet"><span class="None">import pdb</span></p><p class="snippet"><span class="None">import string</span></p><p class="snippet"><span class="None">import numpy as np</span></p><p class="snippet"><span class="None">import pandas as pd</span></p><p class="snippet"><span class="None">from keras.utils import to_categorical</span></p><p class="snippet"><span class="None">import matplotlib.pyplot as plt</span></p><p class="snippet"><span class="None">%matplotlib inline</span></p></li>
				<li><span class="None">Load the dataset and read the file.</span><p class="snippet"><span class="None">path_data = "news_summary_small.csv"</span></p><p class="snippet"><span class="None">df_text_file = pd.read_csv(path_data)</span></p><p class="snippet"><span class="None">df_text_file.headlines = df_text_file.headlines.str.lower()</span></p><p class="snippet"><span class="None">df_text_file.text = df_text_file.text.str.lower()</span></p><p class="snippet"><span class="None">lengths_text = df_text_file.text.apply(len)</span></p><p class="snippet"><span class="None">dataset = list(zip(df_text_file.text.values, df_text_file.headlines.values))</span></p></li>
				<li><span class="None">Make vocab dictionary.</span><p class="snippet"><span class="None">input_texts = []</span></p><p class="snippet"><span class="None">target_texts = []</span></p><p class="snippet"><span class="None">input_chars = set()</span></p><p class="snippet"><span class="None">target_chars = set()</span></p><p class="snippet"><span class="None">for line in dataset:</span></p><p class="snippet"><span class="None">    input_text, target_text = list(line[0]), list(line[1])</span></p><p class="snippet"><span class="None">    target_text = ['BEGIN_'] + target_text + ['_END']</span></p><p class="snippet"><span class="None">    input_texts.append(input_text)</span></p><p class="snippet"><span class="None">    target_texts.append(target_text)</span></p><p class="snippet"><span class="None">    </span></p><p class="snippet"><span class="None">    </span></p><p class="snippet"><span class="None">    for character in input_text:</span></p><p class="snippet"><span class="None">        if character not in input_chars:</span></p><p class="snippet"><span class="None">            input_chars.add(character)</span></p><p class="snippet"><span class="None">    for character in target_text:</span></p><p class="snippet"><span class="None">        if character not in target_chars:</span></p><p class="snippet"><span class="None">            target_chars.add(character)</span></p><p class="snippet"><span class="None">input_chars.add("&lt;unk&gt;")</span></p><p class="snippet"><span class="None">input_chars.add("&lt;pad&gt;")</span></p><p class="snippet"><span class="None">target_chars.add("&lt;pad&gt;")</span></p><p class="snippet"><span class="None">input_chars = sorted(input_chars)</span></p><p class="snippet"><span class="None">target_chars = sorted(target_chars)</span></p><p class="snippet"><span class="None">human_vocab = dict(zip(input_chars, range(len(input_chars))))</span></p><p class="snippet"><span class="None">machine_vocab = dict(zip(target_chars, range(len(target_chars))))</span></p><p class="snippet"><span class="None">inv_machine_vocab = dict(enumerate(sorted(machine_vocab)))</span></p><p class="snippet"><span class="None">def string_to_int(string_in, length, vocab):</span></p><p class="snippet"><span class="None">    """</span></p><p class="snippet"><span class="None">    Converts all strings in the vocabulary into a list of integers representing the positions of the</span></p><p class="snippet"><span class="None">    input string's characters in the "vocab"</span></p><p class="snippet"><span class="None">    Arguments:</span></p><p class="snippet"><span class="None">    string -- input string</span></p><p class="snippet"><span class="None">    length -- the number of time steps you'd like, determines if the output will be padded or cut</span></p><p class="snippet"><span class="None">    vocab -- vocabulary, dictionary used to index every character of your "string"</span></p><p class="snippet"><span class="None">    Returns:</span></p><p class="snippet"><span class="None">    rep -- list of integers (or '&lt;unk&gt;') (size = length) representing the position of the string's character in the vocabulary</span></p><p class="snippet"><span class="None">    """</span></p></li>
				<li><span class="None">    Convert lowercase to standardize.</span><p class="snippet"><span class="None">    string_in = string_in.lower()</span></p><p class="snippet"><span class="None">    string_in = string_in.replace(',','')</span></p><p class="snippet"><span class="None">    if len(string_in) &gt; length:</span></p><p class="snippet"><span class="None">        string_in = string_in[:length]</span></p><p class="snippet"><span class="None">    rep = list(map(lambda x: vocab.get(x, '&lt;unk&gt;'), string_in))</span></p><p class="snippet"><span class="None">    if len(string_in) &lt; length:</span></p><p class="snippet"><span class="None">        rep += [vocab['&lt;pad&gt;']] * (length - len(string_in))</span></p><p class="snippet"><span class="None">        </span></p><p class="snippet"><span class="None">    return rep</span></p><p class="snippet"><span class="None">def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):</span></p><p class="snippet"><span class="None">    X, Y = zip(*dataset)</span></p><p class="snippet"><span class="None">    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])</span></p><p class="snippet"><span class="None">    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]</span></p><p class="snippet"><span class="None">    print("X shape from preprocess: {}".format(X.shape))</span></p><p class="snippet"><span class="None">    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))</span></p><p class="snippet"><span class="None">    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))</span></p><p class="snippet"><span class="None">    return X, np.array(Y), Xoh, Yoh</span></p><p class="snippet"><span class="None">def softmax(x, axis=1):</span></p><p class="snippet"><span class="None">    """Softmax activation function.</span></p><p class="snippet"><span class="None">    # Arguments</span></p><p class="snippet"><span class="None">        x : Tensor.</span></p><p class="snippet"><span class="None">        axis: Integer, axis along which the softmax normalization is applied.</span></p><p class="snippet"><span class="None">    # Returns</span></p><p class="snippet"><span class="None">        Tensor, output of softmax transformation.</span></p><p class="snippet"><span class="None">    # Raises</span></p><p class="snippet"><span class="None">        ValueError: In case 'dim(x) == 1'.</span></p><p class="snippet"><span class="None">    """</span></p><p class="snippet"><span class="None">    ndim = K.ndim(x)</span></p><p class="snippet"><span class="None">    if ndim == 2:</span></p><p class="snippet"><span class="None">        return K.softmax(x)</span></p><p class="snippet"><span class="None">    elif ndim &gt; 2:</span></p><p class="snippet"><span class="None">        e = K.exp(x - K.max(x, axis=axis, keepdims=True))</span></p><p class="snippet"><span class="None">        s = K.sum(e, axis=axis, keepdims=True)</span></p><p class="snippet"><span class="None">        return e / s</span></p><p class="snippet"><span class="None">    else:</span></p><p class="snippet"><span class="None">        raise ValueError('Cannot apply softmax to a tensor that is 1D')</span></p></li>
				<li>Run the previous code snippet to load data, get vocab dictionaries and define some utility functions to be used later. Define length of input characters and output characters.<p class="snippet"><span class="None">Tx = 460</span></p><p class="snippet"><span class="None">Ty = 75</span></p><p class="snippet"><span class="None">X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)</span></p><p class="snippet">Define the model functions (Repeator, Concatenate, Densors, Dotor)</p><p class="snippet"><span class="None"># Defined shared layers as global variables</span></p><p class="snippet"><span class="None">repeator = RepeatVector(Tx)</span></p><p class="snippet"><span class="None">concatenator = Concatenate(axis=-1)</span></p><p class="snippet"><span class="None">densor1 = Dense(10, activation = "tanh")</span></p><p class="snippet"><span class="None">densor2 = Dense(1, activation = "relu")</span></p><p class="snippet"><span class="None">activator = Activation(softmax, name='attention_weights')</span></p><p class="snippet"><span class="None">dotor = Dot(axes = 1)</span></p><p class="snippet">Define one-step-attention function:</p><p class="snippet"><span class="None">def one_step_attention(h, s_prev):</span></p><p class="snippet"><span class="None">    """</span></p><p class="snippet"><span class="None">    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights</span></p><p class="snippet"><span class="None">    "alphas" and the hidden states "h" of the Bi-LSTM.</span></p><p class="snippet"><span class="None">    </span></p><p class="snippet"><span class="None">    Arguments:</span></p><p class="snippet"><span class="None">    h -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_h)</span></p><p class="snippet"><span class="None">    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)</span></p><p class="snippet"><span class="None">    </span></p><p class="snippet"><span class="None">    Returns:</span></p><p class="snippet"><span class="None">    context -- context vector, input of the next (post-attetion) LSTM cell</span></p><p class="snippet"><span class="None">    """  </span></p></li>
				<li><span class="None">Use </span><strong class="inline">repeator</strong><span class="None"> to repeat </span><strong class="inline">s_prev</strong><span class="None"> to be of shape (</span><strong class="inline">m</strong><span class="None">, </span><strong class="inline">Tx</strong><span class="None">, </span><strong class="inline">n_s</strong><span class="None">) so that you can concatenate it with all hidden states "</span><strong class="inline">a</strong><span class="None">"</span><p class="snippet"><span class="None">    s_prev = repeator(s_prev)</span></p></li>
				<li><span class="None"> Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)</span><p class="snippet"><span class="None">    concat = concatenator([h, s_prev])</span></p></li>
				<li><span class="None">Use </span><strong class="inline">densor1</strong><span class="None"> to propagate </span><strong class="inline">concat</strong><span class="None"> through a small fully-connected neural network to compute the "intermediate energies" variable e.</span><p class="snippet"><span class="None">    e = densor1(concat)</span></p></li>
				<li><span class="None">Use </span><strong class="inline">densor2</strong><span class="None"> to propagate e through a small fully-connected neural network to compute the "</span><strong class="inline">energies</strong><span class="None">" variable energies.</span><p class="snippet"><span class="None">    energies = densor2(e)  </span></p></li>
				<li><span class="None">Use "</span><strong class="inline">activator</strong><span class="None">" on "</span><strong class="inline">energies</strong><span class="None">" to compute the attention weights "</span><strong class="inline">alphas</strong><span class="None">"</span><p class="snippet"><span class="None">    alphas = activator(energies)</span></p></li>
				<li><span class="None"> Use </span><strong class="inline">dotor</strong><span class="None"> together with "</span><strong class="inline">alphas</strong><span class="None">" and "</span><strong class="inline">a</strong><span class="None">" to compute the context vector to be given to the next (post-attention) LSTM-cell</span><p class="snippet"><span class="None">    context = dotor([alphas, h])</span></p><p class="snippet"><span class="None">    </span></p><p class="snippet"><span class="None">    return context</span></p><p class="snippet">Define the number of hidden states for decoder and encoder.</p><p class="snippet"><span class="None">n_h = 32</span></p><p class="snippet"><span class="None">n_s = 64</span></p><p class="snippet"><span class="None">post_activation_LSTM_cell = LSTM(n_s, return_state = True)</span></p><p class="snippet"><span class="None">output_layer = Dense(len(machine_vocab), activation=softmax)</span></p><p class="snippet">Define the model architecture and run it to obtain a model.</p><p class="snippet"><span class="None">def model(Tx, Ty, n_h, n_s, human_vocab_size, machine_vocab_size):</span></p><p class="snippet"><span class="None">    """</span></p><p class="snippet"><span class="None">    Arguments:</span></p><p class="snippet"><span class="None">    Tx -- length of the input sequence</span></p><p class="snippet"><span class="None">    Ty -- length of the output sequence</span></p><p class="snippet"><span class="None">    n_h -- hidden state size of the Bi-LSTM</span></p><p class="snippet"><span class="None">    n_s -- hidden state size of the post-attention LSTM</span></p><p class="snippet"><span class="None">    human_vocab_size -- size of the python dictionary "human_vocab"</span></p><p class="snippet"><span class="None">    machine_vocab_size -- size of the python dictionary "machine_vocab"</span></p><p class="snippet"><span class="None">    Returns:</span></p><p class="snippet"><span class="None">    model -- Keras model instance</span></p><p class="snippet"><span class="None">    """</span></p></li>
				<li><span class="None"> Define the inputs of your model with a shape (</span><strong class="inline">Tx</strong><span class="None">,)</span></li>
				<li><span class="None"> Define </span><strong class="inline">s0</strong><span class="None"> and </span><strong class="inline">c0</strong><span class="None">, initial hidden state for the decoder LSTM of shape (</span><strong class="inline">n_s</strong><span class="None">,)</span><p class="snippet"><span class="None">    X = Input(shape=(Tx, human_vocab_size), name="input_first")</span></p><p class="snippet"><span class="None">    s0 = Input(shape=(n_s,), name='s0')</span></p><p class="snippet"><span class="None">    c0 = Input(shape=(n_s,), name='c0')</span></p><p class="snippet"><span class="None">    s = s0</span></p><p class="snippet"><span class="None">    c = c0</span></p></li>
				<li><span class="None">Initialize empty list of outputs</span><p class="snippet"><span class="None">    outputs = []</span></p></li>
				<li><span class="None">Define your pre-attention Bi-LSTM. Remember to use return_sequences=True.</span><p class="snippet"><span class="None">    a = Bidirectional(LSTM(n_h, return_sequences=True))(X)</span></p><p class="snippet"><span class="None">    </span></p><p class="snippet"><span class="None">    # Iterate for Ty steps</span></p><p class="snippet"><span class="None">    for t in range(Ty):</span></p><p class="snippet"><span class="None">    </span></p><p class="snippet"><span class="None">        # Perform one step of the attention mechanism to get back the context vector at step t</span></p><p class="snippet"><span class="None">        context = one_step_attention(h, s)      </span></p></li>
				<li><span class="None">Apply the post-attention LSTM cell to the "</span><strong class="inline">context</strong><span class="None">" vector.</span><p class="snippet"><span class="None">        # Pass: initial_state = [hidden state, cell state]</span></p><p class="snippet"><span class="None">        s, _, c = post_activation_LSTM_cell(context, initial_state = [s,c])  </span></p></li>
				<li><span class="None">Apply </span><strong class="inline">Dense</strong><span class="None"> layer to the hidden state output of the post-attention LSTM</span><p class="snippet"><span class="None">        out = output_layer(s)    </span></p></li>
				<li><span class="None">Append "out" to the "outputs" list</span><p class="snippet"><span class="None">        outputs.append(out)</span></p></li>
				<li><span class="None">Create model instance taking three inputs and returning the list of outputs.</span><p class="snippet"><span class="None">    model = Model(inputs=[X, s0, c0], outputs=outputs)</span></p><p class="snippet"><span class="None">    </span></p><p class="snippet"><span class="None">    return model</span></p><p class="snippet"><span class="None">model = model(Tx, Ty, n_h, n_s, len(human_vocab), len(machine_vocab))</span></p><p class="snippet">#Define model loss functions and other hyperparameters. Also #initialize decoder state vectors.</p><p class="snippet"><span class="None">opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)</span></p><p class="snippet"><span class="None">model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])</span></p><p class="snippet"><span class="None">s0 = np.zeros((10000, n_s))</span></p><p class="snippet"><span class="None">c0 = np.zeros((10000, n_s))</span></p><p class="snippet"><span class="None">outputs = list(Yoh.swapaxes(0,1))</span></p><p class="snippet">Fit the model to our data:</p><p class="snippet"><span class="None">model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)</span></p><p class="snippet">#Run inference step for the <a id="_idTextAnchor256"/>new text.</p><p class="snippet"><span class="None">EXAMPLES = ["Last night a meteorite was seen flying near the earth's moon."]</span></p><p class="snippet"><span class="None">for example in EXAMPLES:</span></p><p class="snippet"><span class="None">    </span></p><p class="snippet"><span class="None">    source = string_to_int(example, Tx, human_vocab)</span></p><p class="snippet"><span class="None">    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))</span></p><p class="snippet"><span class="None">    source = source[np.newaxis, :]</span></p><p class="snippet"><span class="None">    prediction = model.predict([source, s0, c0])</span></p><p class="snippet"><span class="None">    prediction = np.argmax(prediction, axis = -1)</span></p><p class="snippet"><span class="None">    output = [inv_machine_vocab[int(i)] for i in prediction]</span></p><p class="snippet"><span class="None">    </span></p><p class="snippet"><span class="None">    print("source:", example)</span></p><p class="snippet"><span class="None">    print("output:", ''.join(output))</span></p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer324">
					<img alt="Figure 8.18: Text summarization model output&#13;&#10;" src="image/C13783_08_18.jpg"/>
				</div>
			</div>
			<h6>Figure 8.18: Text summarization model output</h6>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor257"/>Chapter 9: A practical NLP project workflow in an organisation</h2>
			<h3 id="_idParaDest-229"><a id="_idTextAnchor258"/>Code for LSTM model</h3>
			<ol>
				<li value="1"><span class="None">Check if GPU is detected</span><p class="snippet"><span class="None">import tensorflow as tf</span></p><p class="snippet"><span class="None">tf.test.gpu_device_name()</span></p></li>
				<li><span class="None">Setting up collar notebook</span><p class="snippet"><span class="None">from google.colab import drive</span></p><p class="snippet"><span class="None">drive.mount('/content/gdrive')</span></p><p class="snippet"><span class="None"># Run the below command in a new cell</span></p><p class="snippet"><span class="None">cd /content/gdrive/My Drive/Lesson-9/</span></p><p class="snippet"><span class="None"># Run the below command in a new cell</span></p><p class="snippet"><span class="None">!unzip data.csv.zip</span></p></li>
				<li><span class="None">Import necessary Python packages and classes.</span><p class="snippet"><span class="None">import os</span></p><p class="snippet"><span class="None">import re</span></p><p class="snippet"><span class="None">import pickle</span></p><p class="snippet"><span class="None">import pandas as pd</span></p><p class="snippet"><span class="None">from keras.preprocessing.text import Tokenizer</span></p><p class="snippet"><span class="None">from keras.preprocessing.sequence import pad_sequences</span></p><p class="snippet"><span class="None">from keras.models import Sequential</span></p><p class="snippet"><span class="None">from keras.layers import Dense, Embedding, LSTM</span></p></li>
				<li><span class="None">Load the data file.</span><p class="snippet"><span class="None">def preprocess_data(data_file_path):</span></p><p class="snippet"><span class="None">    data = pd.read_csv(data_file_path, header=None) # read the csv</span></p><p class="snippet"><span class="None">    data.columns = ['rating', 'title', 'review'] # add column names</span></p><p class="snippet"><span class="None">    data['review'] = data['review'].apply(lambda x: x.lower()) # change all text to lower</span></p><p class="snippet"><span class="None">    data['review'] = data['review'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x))) # remove all numbers</span></p><p class="snippet"><span class="None">    return data</span></p><p class="snippet"><span class="None">df = preprocess_data('data.csv')</span></p></li>
				<li><span class="None">Initialize tokenization.</span><p class="snippet"><span class="None">max_features = 2000</span></p><p class="snippet"><span class="None">maxlength = 250</span></p><p class="snippet"><span class="None">tokenizer = Tokenizer(num_words=max_features, split=' ')</span></p></li>
				<li><span class="None">Fit tokenizer.</span><p class="snippet"><span class="None">tokenizer.fit_on_texts(df['review'].values)</span></p><p class="snippet"><span class="None">X = tokenizer.texts_to_sequences(df['review'].values)</span></p></li>
				<li><span class="None">Pad sequences.</span><p class="snippet"><span class="None">X = pad_sequences(X, maxlen=maxlength)</span></p></li>
				<li><span class="None">Get target variable</span><p class="snippet"><span class="None">y_train = pd.get_dummies(df.rating).values</span></p><p class="snippet"><span class="None">embed_dim = 128</span></p><p class="snippet"><span class="None">hidden_units = 100</span></p><p class="snippet"><span class="None">n_classes = 5</span></p><p class="snippet"><span class="None">model = Sequential()</span></p><p class="snippet"><span class="None">model.add(Embedding(max_features, embed_dim, input_length = X.shape[1]))</span></p><p class="snippet"><span class="None">model.add(LSTM(hidden_units))</span></p><p class="snippet"><span class="None">model.add(Dense(n_classes, activation='softmax'))</span></p><p class="snippet"><span class="None">model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])</span></p><p class="snippet"><span class="None">print(model.summary())</span></p></li>
				<li><span class="None">Fit the model.</span><p class="snippet"><span class="None">model.fit(X[:100000, :], y_train[:100000, :], batch_size = 128, epochs=15, validation_split=0.2)</span></p></li>
				<li><span class="None">Save model and tokenizer.</span><p class="snippet"><span class="None">model.save('trained_model.h5')  # creates a HDF5 file 'trained_model.h5'</span></p><p class="snippet"><span class="None">with open('trained_tokenizer.pkl', 'wb') as f: # creates a pickle file 'trained_tokenizer.pkl'</span></p><p class="snippet"><span class="None">    pickle.dump(tokenizer, f)</span></p><p class="snippet"><span class="None">from google.colab import files</span></p><p class="snippet"><span class="None">files.download('trained_model.h5')</span></p><p class="snippet"><span class="None">files.download('trained_tokenizer.pkl')</span></p></li>
			</ol>
			<h3 id="_idParaDest-230"><a id="_idTextAnchor259"/>Code for Flask</h3>
			<ol>
				<li value="1">Import the necessary Python packages and classes.<p class="snippet"><span class="None">import re</span></p><p class="snippet"><span class="None">import pickle</span></p><p class="snippet"><span class="None">import numpy as np</span></p><p class="snippet"><span class="None">from flask import Flask, request, jsonify</span></p><p class="snippet"><span class="None">from keras.models import load_model</span></p><p class="snippet"><span class="None">from keras.preprocessing.sequence import pad_sequences</span></p></li>
				<li><span class="None">Define the input files and load in dataframe</span><p class="snippet"><span class="None">def load_variables():</span></p><p class="snippet"><span class="None">    global model, tokenizer</span></p><p class="snippet"><span class="None">    model = load_model('trained_model.h5')</span></p><p class="snippet"><span class="None">    model._make_predict_function()  # https://github.com/keras-team/keras/issues/6462</span></p><p class="snippet"><span class="None">    with open('trained_tokenizer.pkl',  'rb') as f:</span></p><p class="snippet"><span class="None">        tokenizer = pickle.load(f)</span></p></li>
				<li>Define preprocessing functions similar to the training code:<p class="snippet"><span class="None">def do_preprocessing(reviews):</span></p><p class="snippet"><span class="None">    processed_reviews = []</span></p><p class="snippet"><span class="None">    for review in reviews:</span></p><p class="snippet"><span class="None">        review = review.lower()</span></p><p class="snippet"><span class="None">        processed_reviews.append(re.sub('[^a-zA-z0-9\s]', '', review))</span></p><p class="snippet"><span class="None">    processed_reviews = tokenizer.texts_to_sequences(np.array(processed_reviews))</span></p><p class="snippet"><span class="None">    processed_reviews = pad_sequences(processed_reviews, maxlen=250)</span></p><p class="snippet"><span class="None">    return processed_reviews</span></p></li>
				<li>Define a Flask app instance:<p class="snippet"><span class="None">app = Flask(__name__)</span></p></li>
				<li>Define an endpoint that displays a fixed message:<p class="snippet"><span class="None">@app.route('/')</span></p><p class="snippet"><span class="None">def home_routine():</span></p><p class="snippet"><span class="None">    return 'Hello World!'</span></p></li>
				<li>We'll have a prediction endpoint, to which we can send our review strings. The kind of HTTP request we will use is a '<strong class="inline">POST</strong>' request:<p class="snippet"><span class="None">@app.route('/prediction', methods=['POST'])</span></p><p class="snippet"><span class="None">def get_prediction():</span></p><p class="snippet"><span class="None">  # get incoming text</span></p><p class="snippet"><span class="None">  # run the model</span></p><p class="snippet"><span class="None">    if request.method == 'POST':</span></p><p class="snippet"><span class="None">        data = request.get_json()</span></p><p class="snippet"><span class="None">    data = do_preprocessing(data)</span></p><p class="snippet"><span class="None">    predicted_sentiment_prob = model.predict(data)</span></p><p class="snippet"><span class="None">    predicted_sentiment = np.argmax(predicted_sentiment_prob, axis=-1)</span></p><p class="snippet"><span class="None">    return str(predicted_sentiment)</span></p></li>
				<li>Start the web server.<p class="snippet"><span class="None">if __name__ == '__main__':</span></p><p class="snippet"><span class="None">  # load model</span></p><p class="snippet"><span class="None">  load_variables()</span></p><p class="snippet"><span class="None">  app.run(debug=True)</span></p></li>
				<li>Save this file as <strong class="inline">app.py</strong> (any name could be used). Run this code from the terminal using <strong class="inline">app.py</strong>:<p class="snippet">python app.py</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer325">
					<img alt="Figure 9.31: Output for flask" src="image/C13783_09_31.jpg"/>
				</div>
			</div>
			<h6>Figure 9.31: Output for flask</h6>
		</div>
		<div>
			<div class="Content" id="_idContainer327">
			</div>
		</div>
		<div>
			<div class="Content" id="_idContainer328">
			</div>
		</div>
	</body></html>