["```py\nactions = [0,1,2,3,4,5,6,7,8,9,10,11] \n```", "```py\n# AI for Logistics - Robots in a warehouse\n# Importing the libraries\nimport numpy as np \n```", "```py\n# Setting the parameters gamma and alpha for the Q-Learning\ngamma = 0.75\nalpha = 0.9 \n```", "```py\n# PART 1 - BUILDING THE ENVIRONMENT\n# Defining the states\nlocation_to_state = {'A': 0,\n                     'B': 1,\n                     'C': 2,\n                     'D': 3,\n                     'E': 4,\n                     'F': 5,\n                     'G': 6,\n                     'H': 7,\n                     'I': 8,\n                     'J': 9,\n                     'K': 10,\n                     'L': 11} \n```", "```py\n# Defining the actions\nactions = [0,1,2,3,4,5,6,7,8,9,10,11] \n```", "```py\n# Defining the rewards\nR = np.array([[0,1,0,0,0,0,0,0,0,0,0,0],\n              [1,0,1,0,0,1,0,0,0,0,0,0],\n              [0,1,0,0,0,0,1,0,0,0,0,0],\n              [0,0,0,0,0,0,0,1,0,0,0,0],\n              [0,0,0,0,0,0,0,0,1,0,0,0],\n              [0,1,0,0,0,0,0,0,0,1,0,0],\n              [0,0,1,0,0,0,1000,1,0,0,0,0],\n              [0,0,0,1,0,0,1,0,0,0,0,1],\n              [0,0,0,0,1,0,0,0,0,1,0,0],\n              [0,0,0,0,0,1,0,0,1,0,1,0],\n              [0,0,0,0,0,0,0,0,0,1,0,1],\n              [0,0,0,0,0,0,0,1,0,0,1,0]]) \n```", "```py\n# PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING\n\n# Initializing the Q-values\nQ = np.array(np.zeros([12,12])) \n```", "```py\n# Implementing the Q-Learning process\nfor i in range(1000):\n    current_state = np.random.randint(0,12)\n    playable_actions = []\n    for j in range(12):\n        if R[current_state, j] > 0:\n            playable_actions.append(j)\n    next_state = np.random.choice(playable_actions)\n    TD = R[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state]\n    Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD \n```", "```py\nprint(\"Q-values:\")\nprint(Q.astype(int)) \n```", "```py\nQ-values:\n[[   0 1661    0    0    0    0    0    0    0    0    0    0]\n [1246    0 2213    0    0 1246    0    0    0    0    0    0]\n [   0 1661    0    0    0    0 2970    0    0    0    0    0]\n [   0    0    0    0    0    0    0 2225    0    0    0    0]\n [   0    0    0    0    0    0    0    0  703    0    0    0]\n [   0 1661    0    0    0    0    0    0    0  931    0    0]\n [   0    0 2213    0    0    0 3968 2225    0    0    0    0]\n [   0    0    0 1661    0    0 2968    0    0    0    0 1670]\n [   0    0    0    0  528    0    0    0    0  936    0    0]\n [   0    0    0    0    0 1246    0    0  703    0 1246    0]\n [   0    0    0    0    0    0    0    0    0  936    0 1661]\n [   0    0    0    0    0    0    0 2225    0    0 1246    0]] \n```", "```py\n# PART 3 - GOING INTO PRODUCTION\n\n# Making a mapping from the states to the locations\nstate_to_location = {state: location for location, state in location_to_state.items()} \n```", "```py\n# Making the final function that will return the optimal route\ndef route(starting_location, ending_location):\n    route = [starting_location]\n    next_location = starting_location\n    while (next_location != ending_location):\n        starting_state = location_to_state[starting_location]\n        next_state = np.argmax(Q[starting_state,])\n        next_location = state_to_location[next_state]\n        route.append(next_location)\n        starting_location = next_location\n    return route \n```", "```py\n# Printing the final route\nprint('Route:')\nroute('E', 'G') \n```", "```py\nRoute:\nOut[1]: ['E', 'I', 'J', 'F', 'B', 'C', 'G']\nOut[2]: ['E', 'I', 'J', 'K', 'L', 'H', 'G'] \n```", "```py\n# PART 1 - BUILDING THE ENVIRONMENT\n\n# Defining the states\nlocation_to_state = {'A': 0,\n                     'B': 1,\n                     'C': 2,\n                     'D': 3,\n                     'E': 4,\n                     'F': 5,\n                     'G': 6,\n                     'H': 7,\n                     'I': 8,\n                     'J': 9,\n                     'K': 10,\n                     'L': 11}\n\n# Defining the actions\nactions = [0,1,2,3,4,5,6,7,8,9,10,11]\n\n# Defining the rewards\nR = np.array([[0,1,0,0,0,0,0,0,0,0,0,0],\n              [1,0,1,0,0,1,0,0,0,0,0,0],\n              [0,1,0,0,0,0,1,0,0,0,0,0],\n              [0,0,0,0,0,0,0,1,0,0,0,0],\n              [0,0,0,0,0,0,0,0,1,0,0,0],\n              [0,1,0,0,0,0,0,0,0,1,0,0],\n              [0,0,1,0,0,0,1,1,0,0,0,0],\n              [0,0,0,1,0,0,1,0,0,0,0,1],\n              [0,0,0,0,1,0,0,0,0,1,0,0],\n              [0,0,0,0,0,1,0,0,1,0,1,0],\n              [0,0,0,0,0,0,0,0,0,1,0,1],\n              [0,0,0,0,0,0,0,1,0,0,1,0]]) \n```", "```py\n# Making a function that returns the shortest route from a starting toÂ ending location\ndef route(starting_location, ending_location):\n    R_new = np.copy(R)\n    ending_state = location_to_state[ending_location]\n    R_new[ending_state, ending_state] = 1000 \n```", "```py\n# Making a function that returns the shortest route from a starting to ending location\ndef route(starting_location, ending_location):\n    R_new = np.copy(R)\n    ending_state = location_to_state[ending_location]\n    R_new[ending_state, ending_state] = 1000\n    Q = np.array(np.zeros([12,12]))\n    for i in range(1000):\n        current_state = np.random.randint(0,12)\n        playable_actions = []\n        for j in range(12):\n            if R_new[current_state, j] > 0:\n                playable_actions.append(j)\n        next_state = np.random.choice(playable_actions)\n        TD = R_new[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state]\n        Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD\n    route = [starting_location]\n    next_location = starting_location\n    while (next_location != ending_location):\n        starting_state = location_to_state[starting_location]\n        next_state = np.argmax(Q[starting_state,])\n        next_location = state_to_location[next_state]\n        route.append(next_location)\n        starting_location = next_location\n    return route \n```", "```py\n# PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING\n\n# Making a mapping from the states to the locations\nstate_to_location = {state: location for location, state in location_to_state.items()}\n\n# Making a function that returns the shortest route from a starting to ending location\ndef route(starting_location, ending_location):\n    R_new = np.copy(R)\n    ending_state = location_to_state[ending_location]\n    R_new[ending_state, ending_state] = 1000\n    Q = np.array(np.zeros([12,12]))\n    for i in range(1000):\n        current_state = np.random.randint(0,12)\n        playable_actions = []\n        for j in range(12):\n            if R_new[current_state, j] > 0:\n                playable_actions.append(j)\n        next_state = np.random.choice(playable_actions)\n        TD = R_new[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state]\n        Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD\n    route = [starting_location]\n    next_location = starting_location\n    while (next_location != ending_location):\n        starting_state = location_to_state[starting_location]\n        next_state = np.argmax(Q[starting_state,])\n        next_location = state_to_location[next_state]\n        route.append(next_location)\n        starting_location = next_location\n    return route \n```", "```py\n# Defining the rewards\nR = np.array([[0,1,0,0,0,0,0,0,0,0,0,0],\n              [1,0,1,0,0,1,0,0,0,0,0,0],\n              [0,1,0,0,0,0,1,0,0,0,0,0],\n              [0,0,0,0,0,0,0,1,0,0,0,0],\n              [0,0,0,0,0,0,0,0,1,0,0,0],\n              [0,1,0,0,0,0,0,0,0,1,0,0],\n              [0,0,1,0,0,0,1,1,0,0,0,0],\n              [0,0,0,1,0,0,1,0,0,0,0,1],\n              [0,0,0,0,1,0,0,0,0,1,0,0],\n              [0,0,0,0,0,1,0,0,1,0,500,0],\n              [0,0,0,0,0,0,0,0,0,1,0,1],\n              [0,0,0,0,0,0,0,1,0,0,1,0]]) \n```", "```py\n# Defining the rewards\nR = np.array([[0,1,0,0,0,0,0,0,0,0,0,0],\n              [1,0,1,0,0,1,0,0,0,0,0,0],\n              [0,1,0,0,0,0,1,0,0,0,0,0],\n              [0,0,0,0,0,0,0,1,0,0,0,0],\n              [0,0,0,0,0,0,0,0,1,0,0,0],\n              [0,1,0,0,0,0,0,0,0,1,0,0],\n              [0,0,1,0,0,0,1,1,0,0,0,0],\n              [0,0,0,1,0,0,1,0,0,0,0,1],\n              [0,0,0,0,1,0,0,0,0,1,0,0],\n              [0,0,0,0,0,-500,0,0,1,0,1,0],\n              [0,0,0,0,0,0,0,0,0,1,0,1],\n              [0,0,0,0,0,0,0,1,0,0,1,0]]) \n```", "```py\n# PART 3 - GOING INTO PRODUCTION\n\n# Making the final function that returns the optimal route\ndef best_route(starting_location, intermediary_location, ending_location):\n    return route(starting_location, intermediary_location) + route(intermediary_location, ending_location)[1:]\n\n# Printing the final route\nprint('Route:')\nbest_route('E', 'K', 'G') \n```", "```py\nRoute:\n['E', 'I', 'J', 'K', 'L', 'H', 'G'] \n```"]