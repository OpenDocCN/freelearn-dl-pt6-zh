<html><head></head><body><div id="book-columns"><div id="book-inner"><div class="chapter" title="Chapter 5. Using Various Generative Models to Generate Images"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/><span class="koboSpan" id="kobo.1.1">Chapter 5. Using Various Generative Models to Generate Images</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Deep learning shines with big data and deeper models. </span><span class="koboSpan" id="kobo.2.2">It has millions of parameters that can take even weeks to train. </span><span class="koboSpan" id="kobo.2.3">Some real-life scenarios may not have sufficient data, hardware, or resources to train bigger networks in order to achieve the desired accuracy. </span><span class="koboSpan" id="kobo.2.4">Is there any alternative approach or do we need to reinvent the training wheel from scratch all the time?</span></p><p><span class="koboSpan" id="kobo.3.1">In this chapter, we will first look at the powerful and widely used training approach in modern deep learning based applications named </span><span class="strong"><strong><span class="koboSpan" id="kobo.4.1">Transfer Learning</span></strong></span><span class="koboSpan" id="kobo.5.1"> through hands-on examples with real datasets (</span><code class="literal"><span class="koboSpan" id="kobo.6.1">MNIST</span></code><span class="koboSpan" id="kobo.7.1">, </span><code class="literal"><span class="koboSpan" id="kobo.8.1">cars vs cats vs dogs vs flower</span></code><span class="koboSpan" id="kobo.9.1">, </span><code class="literal"><span class="koboSpan" id="kobo.10.1">LFW</span></code><span class="koboSpan" id="kobo.11.1">). </span><span class="koboSpan" id="kobo.11.2">Also, you will build deep learning-based network </span><a id="id203" class="indexterm"/><span class="koboSpan" id="kobo.12.1">over large distributed clusters using Apache Spark and BigDL. </span><span class="koboSpan" id="kobo.12.2">Then you will combine both Transfer Learning and GAN to generate high resolution realistic images with facial datasets. </span><span class="koboSpan" id="kobo.12.3">Finally, you will also understand how to create artistic hallucination on images beyond GAN.</span></p><p><span class="koboSpan" id="kobo.13.1">We will cover the following topics in this chapter:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.14.1">What is Transfer Learning?—its benefits and applications</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.15.1">Classifying </span><code class="literal"><span class="koboSpan" id="kobo.16.1">cars vs dog vs flower</span></code><span class="koboSpan" id="kobo.17.1"> with pre-trained VGG model using Keras</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.18.1">Training and deploying a deep network over large distributed clusters with Apache Spark—deep learning pipeline</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.19.1">Identifying handwritten digits through feature extraction and fine tuning using BigDL</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.20.1">High resolution image generation using pre-trained model and SRGAN</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.21.1">Generating artistic hallucinated images with DeepDream and image generation with VAE</span></li></ul></div><p><span class="koboSpan" id="kobo.22.1">Building a deep learning model from scratch requires sophisticated resources and also it is very time consuming. </span><span class="koboSpan" id="kobo.22.2">And</span><a id="id204" class="indexterm"/><span class="koboSpan" id="kobo.23.1"> hence you don't always want to build such deep models from scratch to solve your problem at hand. </span><span class="koboSpan" id="kobo.23.2">Instead of reinventing the same wheel, you will reuse an already existing model built for similar problems to satisfy your use case.</span></p><p><span class="koboSpan" id="kobo.24.1">Let's say you want to build a self-driving car. </span><span class="koboSpan" id="kobo.24.2">You can either to spend years building a decent image recognition algorithm from scratch or you can simply take the pre-trained inception model built by Google from a huge dataset of ImageNet. </span><span class="koboSpan" id="kobo.24.3">A pre-trained model may not reach the desired accuracy level for your application, but it saves huge effort required to reinvent the wheel. </span><span class="koboSpan" id="kobo.24.4">And with some fine tuning and tricks your accuracy level will definitely improve.</span></p><div class="section" title="Introduction to Transfer Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec27"/><span class="koboSpan" id="kobo.25.1">Introduction to Transfer Learning</span></h1></div></div></div><p><span class="koboSpan" id="kobo.26.1">Pre-trained models are not </span><a id="id205" class="indexterm"/><span class="koboSpan" id="kobo.27.1">optimized for tackling user specific datasets, but they are extremely useful for the task at hand that has similarity with the trained model task.</span></p><p><span class="koboSpan" id="kobo.28.1">For example, a popular model, InceptionV3, is optimized for classifying images on a broad set of 1000 categories, but our domain might be to classify some dog breeds. </span><span class="koboSpan" id="kobo.28.2">A well-known technique used in deep learning that adapts an existing trained model for a similar task to the task at hand is known as Transfer Learning.</span></p><p><span class="koboSpan" id="kobo.29.1">And this is why Transfer Learning has gained a lot of popularity among deep learning practitioners and in recent years has become the go-to technique in many real-life use cases. </span><span class="koboSpan" id="kobo.29.2">It is all about transferring knowledge (or features) among related domains.</span></p><div class="section" title="The purpose of Transfer Learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec51"/><span class="koboSpan" id="kobo.30.1">The purpose of Transfer Learning</span></h2></div></div></div><p><span class="koboSpan" id="kobo.31.1">Let's say you have</span><a id="id206" class="indexterm"/><span class="koboSpan" id="kobo.32.1"> trained a deep neural network to differentiate between fresh mango and rotten mango. </span><span class="koboSpan" id="kobo.32.2">During training the network will have required thousands of rotten and fresh mango images and hours of training to learn knowledge such as if any fruit is rotten, liquid will come out of it and it will produce a bad smell. </span><span class="koboSpan" id="kobo.32.3">Now with this training experience the network can be used for different tasks/use-cases to differentiate between rotten apples and fresh apples using the knowledge of the rotten features learned during training of mango images.</span></p><p><span class="koboSpan" id="kobo.33.1">The general approach of Transfer Learning is to train a base network and then copy its first n layers to the first n layers of a target network. </span><span class="koboSpan" id="kobo.33.2">The remaining layers of the target network are initialized randomly and trained toward the targeted use case.</span></p><p><span class="koboSpan" id="kobo.34.1">The main scenarios for using Transfer Learning in your deep learning workflow are as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.35.1">Smaller datasets</span></strong></span><span class="koboSpan" id="kobo.36.1">: When you have a smaller dataset, building a deep learning model from scratch won't work well. </span><span class="koboSpan" id="kobo.36.2">Transfer Learning provides the way to apply a pre-trained </span><a id="id207" class="indexterm"/><span class="koboSpan" id="kobo.37.1">model to new classes of data. </span><span class="koboSpan" id="kobo.37.2">Let's say a pre-trained model built from one million images of ImageNet data will converge to a decent solution (after training on just a fraction of the available smaller training data, for example, CIFAR-10) compared to a deep learning model built with a smaller dataset from scratch.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.38.1">Less resources</span></strong></span><span class="koboSpan" id="kobo.39.1">: Deep learning processes (such as convolution) require a significant amount of</span><a id="id208" class="indexterm"/><span class="koboSpan" id="kobo.40.1"> resource and time. </span><span class="koboSpan" id="kobo.40.2">Deep learning processes are well suited to run on high grade GPU-based machines. </span><span class="koboSpan" id="kobo.40.3">But with pre-trained models, you can easily train across a full training set (let's say 50,000 images) in less than a minute using your laptop/notebook without GPU, since the majority of time a model is modified in the final layer with a simple update of just a classifier or regressor.</span></li></ul></div></div><div class="section" title="Various approaches of using pre-trained models"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec52"/><span class="koboSpan" id="kobo.41.1">Various approaches of using pre-trained models</span></h2></div></div></div><p><span class="koboSpan" id="kobo.42.1">We will discuss how pre-trained model could be used in different ways:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.43.1">Using pre-trained architecture</span></strong></span><span class="koboSpan" id="kobo.44.1">: Instead of transferring weights of the trained model, we</span><a id="id209" class="indexterm"/><span class="koboSpan" id="kobo.45.1"> can only</span><a id="id210" class="indexterm"/><span class="koboSpan" id="kobo.46.1"> use the architecture and initialize our own random weights to our new dataset.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.47.1">Feature extractor</span></strong></span><span class="koboSpan" id="kobo.48.1">: A pre-trained model can be used as a feature extraction mechanism just by simply</span><a id="id211" class="indexterm"/><span class="koboSpan" id="kobo.49.1"> removing the output layer of the network (that gives the probabilities for being in each of the n classes) and then freezing all the previous layers of the network as a fixed feature extractor for the new dataset.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.50.1">Partially freezing the network</span></strong></span><span class="koboSpan" id="kobo.51.1">: Instead of replacing only the final layer and extracting features from all </span><a id="id212" class="indexterm"/><span class="koboSpan" id="kobo.52.1">previous layers, sometimes we might train our new model partially (that is, to keep the weights of initial layers of the network frozen while retraining only the higher layers). </span><span class="koboSpan" id="kobo.52.2">The choice of the number of frozen layers can be considered as one more hyper-parameter.</span><div class="mediaobject"><span class="koboSpan" id="kobo.53.1"><img src="graphics/B08086_05_01.png.jpg" alt="Various approaches of using pre-trained models"/></span><div class="caption"><p><span class="koboSpan" id="kobo.54.1">Figure-1: Transfer Learning with a pre-trained model</span></p></div></div></li></ul></div><p><span class="koboSpan" id="kobo.55.1">Depending mainly on data size and dataset similarity, you might have to decide on how to proceed with Transfer Learning. </span><span class="koboSpan" id="kobo.55.2">The following table discusses these scenarios:</span></p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"><span class="koboSpan" id="kobo.56.1"> </span></th><th style="text-align: left" valign="bottom">
<p><span class="koboSpan" id="kobo.57.1">High data similarity</span></p>
</th><th style="text-align: left" valign="bottom">
<p><span class="koboSpan" id="kobo.58.1">Low data similarity</span></p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong><span class="koboSpan" id="kobo.59.1">Data size small</span></strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p><span class="koboSpan" id="kobo.60.1">In the scenario of small data size but high data similarity, we will modify only the output layers of the pre-trained model and use it as a feature extractor.</span></p>
</td><td style="text-align: left" valign="top">
<p><span class="koboSpan" id="kobo.61.1">When both data size as well as data similarity is low, we can freeze initial </span><span class="emphasis"><em><span class="koboSpan" id="kobo.62.1">k</span></em></span><span class="koboSpan" id="kobo.63.1"> layers of the pre-trained network and train only the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.64.1">(n-k)</span></em></span><span class="koboSpan" id="kobo.65.1"> remaining layers again. </span><span class="koboSpan" id="kobo.65.2">This will help the top layers to customize to the new dataset and the small data size will also get compensated by frozen initial </span><span class="emphasis"><em><span class="koboSpan" id="kobo.66.1">k</span></em></span><span class="koboSpan" id="kobo.67.1"> layers of the network.</span></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong><span class="koboSpan" id="kobo.68.1">Data size large</span></strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p><span class="koboSpan" id="kobo.69.1">In this scenario, we can use the architecture and initial weights of the pre-trained model.</span></p>
</td><td style="text-align: left" valign="top">
<p><span class="koboSpan" id="kobo.70.1">Although we have a large dataset, the data is very different compared to the one used for training the pre-trained model, so using it in this scenario would not be effective. </span><span class="koboSpan" id="kobo.70.2">Instead it is better to train the deep network from scratch.</span></p>
</td></tr></tbody></table></div><p><span class="koboSpan" id="kobo.71.1">In case of image recognition </span><a id="id213" class="indexterm"/><span class="koboSpan" id="kobo.72.1">Transfer Learning utilize the pre-trained convolutional layers to extract features about the new input images, that means only a small part of the original model (mainly the dense layers) are retrained. </span><span class="koboSpan" id="kobo.72.2">The rest of the network remains frozen. </span><span class="koboSpan" id="kobo.72.3">In this way, it saves a lot of time and resource by passing the raw images through the frozen part of the network only once, and then never goes through that part of the network again.</span></p></div><div class="section" title="Classifying car vs cat vs dog vs flower using Keras"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec53"/><span class="koboSpan" id="kobo.73.1">Classifying car vs cat vs dog vs flower using Keras</span></h2></div></div></div><p><span class="koboSpan" id="kobo.74.1">Let us implement the</span><a id="id214" class="indexterm"/><span class="koboSpan" id="kobo.75.1"> concept of Transfer Learning and fine-tuning to identify customizable object categories using a customized dataset consisting of 150 training images and 50 validation images for each category of car, cat, dog, and</span><a id="id215" class="indexterm"/><span class="koboSpan" id="kobo.76.1"> flower. </span></p><p><span class="koboSpan" id="kobo.77.1">Note that the dataset is prepared by taking images from the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.78.1">Kaggle Dogs vs.Cats</span></em></span><span class="koboSpan" id="kobo.79.1"> (</span><a class="ulink" href="https://www.kaggle.com/c/dogs-vs-cats"><span class="koboSpan" id="kobo.80.1">https://www.kaggle.com/c/dogs-vs-cats</span></a><span class="koboSpan" id="kobo.81.1">), Stanford cars (</span><a class="ulink" href="http://ai.stanford.edu/~jkrause/cars/car_dataset.html"><span class="koboSpan" id="kobo.82.1">http://ai.stanford.edu/~jkrause/cars/car_dataset.html</span></a><span class="koboSpan" id="kobo.83.1">), and </span><code class="literal"><span class="koboSpan" id="kobo.84.1">Oxford flower</span></code>
<a id="id216" class="indexterm"/><span class="koboSpan" id="kobo.85.1"> dataset (</span><a class="ulink" href="http://www.robots.ox.ac.uk/~vgg/data/flowers"><span class="koboSpan" id="kobo.86.1">http://www.robots.ox.ac.uk/~vgg/data/flowers</span></a><span class="koboSpan" id="kobo.87.1">).</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.88.1"><img src="graphics/B08086_05_02.jpg" alt="Classifying car vs cat vs dog vs flower using Keras"/></span><div class="caption"><p><span class="koboSpan" id="kobo.89.1">Figure-2: Car vs Cat vs Dog vs Flower dataset structure</span></p></div></div><p><span class="koboSpan" id="kobo.90.1">We need to perform some preprocessing using the </span><code class="literal"><span class="koboSpan" id="kobo.91.1">preprocessing</span></code><span class="koboSpan" id="kobo.92.1"> function and apply various data augmentation transformation through </span><code class="literal"><span class="koboSpan" id="kobo.93.1">rotation</span></code><span class="koboSpan" id="kobo.94.1">, </span><code class="literal"><span class="koboSpan" id="kobo.95.1">shift</span></code><span class="koboSpan" id="kobo.96.1">, </span><code class="literal"><span class="koboSpan" id="kobo.97.1">shear</span></code><span class="koboSpan" id="kobo.98.1">, </span><code class="literal"><span class="koboSpan" id="kobo.99.1">zoom</span></code><span class="koboSpan" id="kobo.100.1">, and </span><code class="literal"><span class="koboSpan" id="kobo.101.1">flip</span></code><span class="koboSpan" id="kobo.102.1"> parameters:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.103.1">train_datagen = ImageDataGenerator(
	preprocessing_function=preprocess_input,
	rotation_range=30,
	width_shift_range=0.2,
	height_shift_range=0.2,
	shear_range=0.2,
	zoom_range=0.2,
	horizontal_flip=True
)
test_datagen = ImageDataGenerator(
	preprocessing_function=preprocess_input,
	rotation_range=30,
	width_shift_range=0.2,
	height_shift_range=0.2,
	shear_range=0.2,
	zoom_range=0.2,
	horizontal_flip=True
)
train_generator = train_datagen.flow_from_directory(
	args.train_dir,
	target_size=(IM_WIDTH, IM_HEIGHT),
	batch_size=batch_size,
)
validation_generator = test_datagen.flow_from_directory(
	args.val_dir,
	target_size=(IM_WIDTH, IM_HEIGHT),
	batch_size=batch_size,
)</span></pre></div><p><span class="koboSpan" id="kobo.104.1">Next, we need to load the InceptionV3 model from the </span><code class="literal"><span class="koboSpan" id="kobo.105.1">keras.applications</span></code><span class="koboSpan" id="kobo.106.1"> module. </span><span class="koboSpan" id="kobo.106.2">The flag </span><code class="literal"><span class="koboSpan" id="kobo.107.1">include_top=False</span></code><span class="koboSpan" id="kobo.108.1"> is used to leave out the weights of the last fully connected layer:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.109.1">base_model = InceptionV3(weights='imagenet', include_top=False)</span></pre></div><p><span class="koboSpan" id="kobo.110.1">Initialize a new last layer</span><a id="id217" class="indexterm"/><span class="koboSpan" id="kobo.111.1"> by adding fully-connected </span><code class="literal"><span class="koboSpan" id="kobo.112.1">Dense</span></code><span class="koboSpan" id="kobo.113.1"> layer of size 1024, followed by a </span><code class="literal"><span class="koboSpan" id="kobo.114.1">softmax</span></code><span class="koboSpan" id="kobo.115.1"> function on the output to squeeze the values between </span><code class="literal"><span class="koboSpan" id="kobo.116.1">[0,1]</span></code><span class="koboSpan" id="kobo.117.1">:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.118.1">
def addNewLastLayer(base_model, nb_classes):
	x = base_model.output
	x = GlobalAveragePooling2D()(x)
	x = Dense(FC_SIZE, activation='relu')(x)
	predictions = Dense(nb_classes, activation='softmax')(x)
	model = Model(input=base_model.input, output=predictions)
	return model
</span></pre></div><p><span class="koboSpan" id="kobo.119.1">Once the last layer of the network is stabilized (Transfer Learning), we can move onto retraining more layers (fine-tuning).</span></p><p><span class="koboSpan" id="kobo.120.1">Use a utility method to freeze all layers and compile the model:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.121.1">
def setupTransferLearn(model, base_model):
	for layer in base_model.layers:
	  layer.trainable = False
model.compile(optimizer='rmsprop',
			  loss='categorical_crossentropy',
              metrics=['accuracy'])
</span></pre></div><p><span class="koboSpan" id="kobo.122.1">This is another utility method to freeze the bottom of the top two inception blocks in the InceptionV3 architecture and retrain the remaining top:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.123.1">
def setupFineTune(model):
   for layer in model.layers[:NB_IV3_LAYERS_TO_FREEZE]:
      layer.trainable = False
   for layer in model.layers[NB_IV3_LAYERS_TO_FREEZE:]:
	  layer.trainable = True
   model.compile(optimizer=SGD(lr=0.0001, momentum=0.9),
loss='categorical_crossentropy')
</span></pre></div><p><span class="koboSpan" id="kobo.124.1">Now we're all ready</span><a id="id218" class="indexterm"/><span class="koboSpan" id="kobo.125.1"> for training using the </span><code class="literal"><span class="koboSpan" id="kobo.126.1">fit_generator</span></code><span class="koboSpan" id="kobo.127.1"> method and finally save our model:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.128.1">
history = model.fit_generator(
	train_generator,
	samples_per_epoch=nb_train_samples,
	nb_epoch=nb_epoch,
	validation_data=validation_generator,
	nb_val_samples=nb_val_samples,
	class_weight='auto')
model.save(args.output_model_file)
</span></pre></div><p><span class="koboSpan" id="kobo.129.1">Run the following command for training and fine tuning:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.130.1">python training-fine-tune.py --train_dir &lt;path to training images&gt; --val_dir &lt;path to validation images&gt;</span></pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.131.1"><img src="graphics/B08086_05_03.png.jpg" alt="Classifying car vs cat vs dog vs flower using Keras"/></span></div><p><span class="koboSpan" id="kobo.132.1">Even with such a small dataset size, we are able to achieve an accuracy of 98.5 percent on the validation set by utilizing the power of the pre-trained model and Transfer Learning:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.133.1"><img src="graphics/B08086_05_04.jpg" alt="Classifying car vs cat vs dog vs flower using Keras"/></span></div><p><span class="koboSpan" id="kobo.134.1">Voila, we can now </span><a id="id219" class="indexterm"/><span class="koboSpan" id="kobo.135.1">use the saved model to predict images (either from the local filesystem or from the URL) with test data:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.136.1">python predict.py --image_url https://goo.gl/DCbuq8 --model inceptionv3-ft.model</span></pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.137.1"><img src="graphics/B08086_05_05.png.jpg" alt="Classifying car vs cat vs dog vs flower using Keras"/></span></div></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Large scale deep learning with Apache Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec28"/><span class="koboSpan" id="kobo.1.1">Large scale deep learning with Apache Spark</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Deep learning is a</span><a id="id220" class="indexterm"/><span class="koboSpan" id="kobo.3.1"> resource hungry and computationally intensive process and you get better result with more data and bigger network, but its speed </span><a id="id221" class="indexterm"/><span class="koboSpan" id="kobo.4.1">gets impacted by the size of the datasets as well. </span><span class="koboSpan" id="kobo.4.2">And in practice, deep learning requires experimenting with different values for training parameters known as hyper-parameter tuning, where you have to run your deep networks on a large dataset iteratively or many times and speed does matter. </span><span class="koboSpan" id="kobo.4.3">Some common ways to tackle this problem is to use faster hardware (usually GPUs), optimized code (with a proper production-ready framework), and scaling out over distributed clusters to achieve some form of parallelism.</span></p><p><span class="koboSpan" id="kobo.5.1">Data parallelism is a concept of sharding large datasets into multiple chunks of data and then processing chunks over neural networks running on separate nodes of distributed clusters.</span></p><p><span class="koboSpan" id="kobo.6.1">Apache Spark is a</span><a id="id222" class="indexterm"/><span class="koboSpan" id="kobo.7.1"> fast, general-purpose, fault-tolerant framework for interactive and iterative computations on large, distributed datasets by doing in-memory processing of RDDs or DataFrames instead of saving data to hard disks. </span><span class="koboSpan" id="kobo.7.2">It</span><a id="id223" class="indexterm"/><span class="koboSpan" id="kobo.8.1"> supports a wide variety of data sources as well as storage layers. </span><span class="koboSpan" id="kobo.8.2">It provides unified data access to combine different data formats, streaming data, and defining complex operations using high-level, composable operators.</span></p><p><span class="koboSpan" id="kobo.9.1">Today Spark is the superpower of big data processing and makes big data accessible to everyone. </span><span class="koboSpan" id="kobo.9.2">But Spark or its core modules alone are not capable of training or running deep networks over the clusters. </span><span class="koboSpan" id="kobo.9.3">In the next few sections, we will develop deep learning applications over Apache Spark cluster with optimized libraries.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note07"/><span class="koboSpan" id="kobo.10.1">Note</span></h3><p><span class="koboSpan" id="kobo.11.1">For coding purposes, we</span><a id="id224" class="indexterm"/><span class="koboSpan" id="kobo.12.1"> will not cover the distributed Spark cluster setup, instead use Apache Spark standalone mode. </span><span class="koboSpan" id="kobo.12.2">More information about Spark cluster mode can be found at: </span><a class="ulink" href="https://spark.apache.org/docs/latest/cluster-overview.html"><span class="koboSpan" id="kobo.13.1">https://spark.apache.org/docs/latest/cluster-overview.html</span></a><span class="koboSpan" id="kobo.14.1">.</span></p></div></div><div class="section" title="Running pre-trained models using Spark deep learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec54"/><span class="koboSpan" id="kobo.15.1">Running pre-trained models using Spark deep learning</span></h2></div></div></div><p><span class="koboSpan" id="kobo.16.1">Deep learning</span><a id="id225" class="indexterm"/><span class="koboSpan" id="kobo.17.1"> pipelines is an open-source library that leverages the power of Apache Spark cluster to easily integrate scalable deep</span><a id="id226" class="indexterm"/><span class="koboSpan" id="kobo.18.1"> learning into machine learning workflows. </span><span class="koboSpan" id="kobo.18.2">It is built on top of Apache Spark's ML Pipelines for training, and uses Spark DataFrames and SQL for deploying models. </span><span class="koboSpan" id="kobo.18.3">It provides high-level APIs for running Transfer Learning in a distributed manner by integrating pre-trained model as transformer in Spark ML Pipeline.</span></p><p><span class="koboSpan" id="kobo.19.1">Deep learning pipelines make Transfer Learning easier with the concept of a featurizer. </span><span class="koboSpan" id="kobo.19.2">The featurizer (or </span><code class="literal"><span class="koboSpan" id="kobo.20.1">DeepImageFeaturizer</span></code><span class="koboSpan" id="kobo.21.1"> in case of image operation) automatically removes the last layer of a pre-trained neural network model and uses all the previous layers output as features for the classification algorithm (for example, logistic regression) specific to the new problem domain.</span></p><p><span class="koboSpan" id="kobo.22.1">Let us implement </span><a id="id227" class="indexterm"/><span class="koboSpan" id="kobo.23.1">deep learning pipelines for predicting sample images of the </span><code class="literal"><span class="koboSpan" id="kobo.24.1">flower</span></code><span class="koboSpan" id="kobo.25.1"> dataset (</span><a class="ulink" href="http://download.tensorflow.org/example_images/flower_photos.tgz"><span class="koboSpan" id="kobo.26.1">http://download.tensorflow.org/example_images/flower_photos.tgz</span></a><span class="koboSpan" id="kobo.27.1">) with a pre-trained model over the Spark cluster:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.28.1">First start the PySpark with the deep learning pipeline package:</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.29.1">pyspark --master local[*] --packages databricks:spark-deep-learning:0.1.0-spark2.1-s_2.11</span></pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip02"/><span class="koboSpan" id="kobo.30.1">Tip</span></h3><p><span class="koboSpan" id="kobo.31.1">Note: If you get the error </span><span class="strong"><strong><span class="koboSpan" id="kobo.32.1">No module named sparkdl</span></strong></span><span class="koboSpan" id="kobo.33.1"> while starting the PySpark with deep learning, please </span><a id="id228" class="indexterm"/><span class="koboSpan" id="kobo.34.1">check the GitHub page for workaround:</span></p><p>
<a class="ulink" href="https://github.com/databricks/spark-deep-learning/issues/18"><span class="koboSpan" id="kobo.35.1">https://github.com/databricks/spark-deep-learning/issues/18</span></a>
</p></div></div></li><li class="listitem"><span class="koboSpan" id="kobo.36.1">First read the</span><a id="id229" class="indexterm"/><span class="koboSpan" id="kobo.37.1"> images and randomly split it into </span><code class="literal"><span class="koboSpan" id="kobo.38.1">train</span></code><span class="koboSpan" id="kobo.39.1">, </span><code class="literal"><span class="koboSpan" id="kobo.40.1">test</span></code><span class="koboSpan" id="kobo.41.1"> set.</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.42.1">img_dir= "path to base image directory"
roses_df = readImages(img_dir + "/roses").withColumn("label", lit(1))
daisy_df = readImages(img_dir + "/daisy").withColumn("label", lit(0))
roses_train, roses_test = roses_df.randomSplit([0.6, 0.4])
daisy_train, daisy_test = daisy_df.randomSplit([0.6, 0.4])
train_df = roses_train.unionAll(daisy_train)</span></pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.43.1">Then create a pipeline with </span><code class="literal"><span class="koboSpan" id="kobo.44.1">DeepImageFeaturizer</span></code><span class="koboSpan" id="kobo.45.1"> using the </span><code class="literal"><span class="koboSpan" id="kobo.46.1">InceptionV3</span></code><span class="koboSpan" id="kobo.47.1"> model:</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.48.1">featurizer = DeepImageFeaturizer(inputCol="image", outputCol="features", modelName="InceptionV3")
lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol="label")
p = Pipeline(stages=[featurizer, lr])</span></pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.49.1">Now fit the</span><a id="id230" class="indexterm"/><span class="koboSpan" id="kobo.50.1"> images with an existing pre-trained model, where </span><code class="literal"><span class="koboSpan" id="kobo.51.1">train_images_df</span></code><span class="koboSpan" id="kobo.52.1"> is a dataset of images and labels:</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.53.1">p_model = p.fit(train_df)   </span></pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.54.1">Finally, we will evaluate the accuracy:</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.55.1">tested_df = p_model.transform(test_df)
evaluator = MulticlassClassificationEvaluator(metricName="accuracy")
print("Test set accuracy = " + str(evaluator.evaluate(tested_df.select("prediction", "label"))))</span></pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.56.1"><img src="graphics/B08086_05_06.png.jpg" alt="Running pre-trained models using Spark deep learning"/></span></div></li></ol></div><p><span class="koboSpan" id="kobo.57.1">In addition to </span><code class="literal"><span class="koboSpan" id="kobo.58.1">DeepImageFeaturizer</span></code><span class="koboSpan" id="kobo.59.1">, we can also utilize the pre-existing model just to do prediction, without</span><a id="id231" class="indexterm"/><span class="koboSpan" id="kobo.60.1"> any retraining or fine tuning using </span><code class="literal"><span class="koboSpan" id="kobo.61.1">DeepImagePredictor</span></code><span class="koboSpan" id="kobo.62.1">:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.63.1">sample_img_dir=&lt;path to your image&gt;

image_df = readImages(sample_img_dir)

predictor = DeepImagePredictor(inputCol="image", outputCol="predicted_labels", modelName="InceptionV3", decodePredictions=True, topK=10)
predictions_df = predictor.transform(image_df)

predictions_df.select("filePath", "predicted_labels").show(10,False)</span></pre></div><p><span class="koboSpan" id="kobo.64.1">The input image and its</span><a id="id232" class="indexterm"/><span class="koboSpan" id="kobo.65.1"> top five predictions are shown as follows:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.66.1"><img src="graphics/B08086_05_07.png.jpg" alt="Running pre-trained models using Spark deep learning"/></span></div><p><span class="koboSpan" id="kobo.67.1">In addition to using the built-in pre-trained models, deep learning pipeline allows users to plug in Keras models or TensorFlow graphs in a Spark prediction pipeline. </span><span class="koboSpan" id="kobo.67.2">This really turns any single-node deep models running on a single-node machine into one that can be trained and</span><a id="id233" class="indexterm"/><span class="koboSpan" id="kobo.68.1"> deployed in a distributed fashion, on a large amount of data.</span></p><p><span class="koboSpan" id="kobo.69.1">First, we will load the</span><a id="id234" class="indexterm"/><span class="koboSpan" id="kobo.70.1"> Keras built-in InceptionV3 model and save it in the file:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.71.1">model = InceptionV3(weights="imagenet")
model.save('model-full.h5')</span></pre></div><p><span class="koboSpan" id="kobo.72.1">During the prediction phase, we will simply load the model and pass images through it to get the desired prediction:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.73.1">def loadAndPreprocessKerasInceptionV3(uri):
    # this is a typical way to load and prep images in keras
    image = img_to_array(load_img(uri, target_size=(299, 299)))
    image = np.expand_dims(image, axis=0)
    return preprocess_input(image)

transformer = KerasImageFileTransformer(inputCol="uri", 
  outputCol="predictions",
                                        modelFile="model-full.h5",
                                    imageLoader=loadAndPreprocessKerasInceptionV3,
                                        outputMode="vector")
dirpath=&lt;path to mix-img&gt;

files = [os.path.abspath(os.path.join(dirpath, f)) for f in os.listdir(dirpath) if f.endswith('.jpg')]
uri_df = sqlContext.createDataFrame(files, StringType()).toDF("uri")

final_df = transformer.transform(uri_df)
final_df.select("uri", "predictions").show()</span></pre></div><p><span class="koboSpan" id="kobo.74.1">Deep learning</span><a id="id235" class="indexterm"/><span class="koboSpan" id="kobo.75.1"> pipeline is a really fast way</span><a id="id236" class="indexterm"/><span class="koboSpan" id="kobo.76.1"> of doing Transfer Learning over distributed Spark clusters. </span><span class="koboSpan" id="kobo.76.2">But you must have noticed that featurizer only allow us to change the final layer of the pre-trained model. </span><span class="koboSpan" id="kobo.76.3">But in some scenarios, you might have to modify more than one layer of the pre-trained network to get the desired result and deep learning pipeline doesn't provide this full capability.</span></p></div><div class="section" title="Handwritten digit recognition at a large scale using BigDL"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec55"/><span class="koboSpan" id="kobo.77.1">Handwritten digit recognition at a large scale using BigDL</span></h2></div></div></div><p><span class="koboSpan" id="kobo.78.1">BigDL is an open-source distributed high</span><a id="id237" class="indexterm"/><span class="koboSpan" id="kobo.79.1"> performance deep learning library that can run directly on top of Apache Spark clusters. </span><span class="koboSpan" id="kobo.79.2">Its high performance is achieved by combing </span><span class="strong"><strong><span class="koboSpan" id="kobo.80.1">Intel® Math Kernel Library</span></strong></span><span class="koboSpan" id="kobo.81.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.82.1">MKL</span></strong></span><span class="koboSpan" id="kobo.83.1">) along with multithreaded programming in each Spark task. </span><span class="koboSpan" id="kobo.83.2">BigDL provides Keras</span><a id="id238" class="indexterm"/><span class="koboSpan" id="kobo.84.1"> style (both sequential and function) high-level APIs to build</span><a id="id239" class="indexterm"/><span class="koboSpan" id="kobo.85.1"> deep learning application and scale out to perform analytics at a large scale. </span><span class="koboSpan" id="kobo.85.2">The main purposes of using BigDL are:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.86.1">Running deep learning model at a large scale and analyzing massive amount of data residing in a Spark or Hadoop cluster (in say Hive, HDFS, or HBase)</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.87.1">Adding deep learning functionality (both training and prediction) to your big data workflow</span><div class="mediaobject"><span class="koboSpan" id="kobo.88.1"><img src="graphics/B08086_05_08.png.jpg" alt="Handwritten digit recognition at a large scale using BigDL"/></span><div class="caption"><p><span class="koboSpan" id="kobo.89.1">Figure-3: BigDL execution over Spark cluster</span></p></div></div></li></ul></div><p><span class="koboSpan" id="kobo.90.1">As you can see from the figure, the BigDL driver program is first launched in the Spark master node of the cluster. </span><span class="koboSpan" id="kobo.90.2">Then with the help of </span><a id="id240" class="indexterm"/>
<span class="strong"><strong><span class="koboSpan" id="kobo.91.1">cluster manager</span></strong></span><span class="koboSpan" id="kobo.92.1"> and the driver program, Spark tasks are distributed across the Spark executors on the worker nodes. </span><span class="koboSpan" id="kobo.92.2">And BigDL interacts with Intel MKL to enable faster execution of those tasks.</span></p><p><span class="koboSpan" id="kobo.93.1">Let us implement a deep neural network at a large scale for identifying hand-written digits with the </span><code class="literal"><span class="koboSpan" id="kobo.94.1">mnist</span></code><span class="koboSpan" id="kobo.95.1"> dataset. </span><span class="koboSpan" id="kobo.95.2">First we will prepare training and validation samples:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.96.1">mnist_path = "datasets/mnist"
(train_data, test_data) = get_mnist(sc, mnist_path)
print train_data.count()
print test_data.count()</span></pre></div><p><span class="koboSpan" id="kobo.97.1">Then we will create the LeNet architecture consisting of two sets of convolutional, activation, and pooling layers, followed by a fully-connected layer, activation, another fully-connected, and finally a </span><code class="literal"><span class="koboSpan" id="kobo.98.1">SoftMax</span></code><span class="koboSpan" id="kobo.99.1"> classifier. </span><span class="koboSpan" id="kobo.99.2">LeNet is small, yet powerful enough to provide interesting results:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.100.1">def build_model(class_num):
    model = Sequential()
    model.add(Reshape([1, 28, 28]))
    model.add(SpatialConvolution(1, 6, 5, 5).set_name('conv1'))
    model.add(Tanh())
    model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool1'))
    model.add(Tanh())
    model.add(SpatialConvolution(6, 12, 5, 5).set_name('conv2'))
    model.add(SpatialMaxPooling(2, 2, 2, 2).set_name('pool2'))
    model.add(Reshape([12 * 4 * 4]))
    model.add(Linear(12 * 4 * 4, 100).set_name('fc1'))
    model.add(Tanh())
    model.add(Linear(100, class_num).set_name('score'))
    model.add(LogSoftMax())
    return model
lenet_model = build_model(10)</span></pre></div><p><span class="koboSpan" id="kobo.101.1">Now we will configure an </span><code class="literal"><span class="koboSpan" id="kobo.102.1">Optimizer</span></code><span class="koboSpan" id="kobo.103.1"> and set the validation logic:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.104.1">optimizer = Optimizer(
    model=lenet_model,
    training_rdd=train_data,
    criterion=ClassNLLCriterion(),
    optim_method=SGD(learningrate=0.4, learningrate_decay=0.0002),
    end_trigger=MaxEpoch(20),
    batch_size=2048)

optimizer.set_validation(
    batch_size=2048,
    val_rdd=test_data,
    trigger=EveryEpoch(),
    val_method=[Top1Accuracy()]
)

trained_model = optimizer.optimize()</span></pre></div><p><span class="koboSpan" id="kobo.105.1">Then we will take a few test samples, and make prediction by checking both the predicted labels and the ground truth labels:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.106.1">predictions = trained_model.predict(test_data)</span></pre></div><p><span class="koboSpan" id="kobo.107.1">Finally, we will train the LeNet model in the Spark cluster using the </span><code class="literal"><span class="koboSpan" id="kobo.108.1">spark-submit</span></code><span class="koboSpan" id="kobo.109.1"> command. </span><span class="koboSpan" id="kobo.109.2">Download the BigDL distribution (</span><a class="ulink" href="https://bigdl-project.github.io/master/#release-download/"><span class="koboSpan" id="kobo.110.1">https://bigdl-project.github.io/master/#release-download/</span></a><span class="koboSpan" id="kobo.111.1">) based on your Apache Spark version and then </span><a id="id241" class="indexterm"/><span class="koboSpan" id="kobo.112.1">execute the file (</span><code class="literal"><span class="koboSpan" id="kobo.113.1">run.sh</span></code><span class="koboSpan" id="kobo.114.1">) provided with the code to submit the job in the Spark cluster:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.115.1">SPARK_HOME= &lt;path to Spark&gt;
BigDL_HOME= &lt;path to BigDL&gt;
PYTHON_API_ZIP_PATH=${BigDL_HOME}/bigdl-python-&lt;version&gt;.zip
BigDL_JAR_PATH=${BigDL_HOME}/bigdl-SPARK-&lt;version&gt;.jar
export PYTHONPATH=${PYTHON_API_ZIP_PATH}:${BigDL_HOME}/conf/spark-bigdl.conf:$PYTHONPATH

${SPARK_HOME}/bin/spark-submit \
      --master &lt;local or spark master url&gt;\
      --driver-cores 5  \
      --driver-memory 5g  \
      --total-executor-cores 16  \
      --executor-cores 8  \
      --executor-memory 10g \
      --py-files ${PYTHON_API_ZIP_PATH},${BigDL_HOME}/BigDL-MNIST.py\
      --properties-file ${BigDL_HOME}/conf/spark-bigdl.conf \
      --jars ${BigDL_JAR_PATH} \
      --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \
      --conf spark.executor.extraClassPath=bigdl-SPARK&lt;version&gt;.jar \
      ${BigDL_HOME}/BigDL-MNIST.py</span></pre></div><p><span class="koboSpan" id="kobo.116.1">More information</span><a id="id242" class="indexterm"/><span class="koboSpan" id="kobo.117.1"> regarding </span><code class="literal"><span class="koboSpan" id="kobo.118.1">spark-submit</span></code><span class="koboSpan" id="kobo.119.1"> can be found at: </span><a class="ulink" href="https://spark.apache.org/docs/latest/submitting-applications.html"><span class="koboSpan" id="kobo.120.1">https://spark.apache.org/docs/latest/submitting-applications.html</span></a><span class="koboSpan" id="kobo.121.1">.</span></p><p><span class="koboSpan" id="kobo.122.1">Once you have submitted the job, you can track the progress on the </span><span class="strong"><strong><span class="koboSpan" id="kobo.123.1">Spark Master</span></strong></span><span class="koboSpan" id="kobo.124.1"> application page as follows:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.125.1"><img src="graphics/B08086_05_09.png.jpg" alt="Handwritten digit recognition at a large scale using BigDL"/></span><div class="caption"><p><span class="koboSpan" id="kobo.126.1">Figure-4: BigDL job of LeNet5 model running on Apache Spark cluster</span></p></div></div><p><span class="koboSpan" id="kobo.127.1">After the job has successfully finished, you can search the logs of the Spark workers to verify the accuracy of your model similar to the one shown as follows:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.128.1">INFO  DistriOptimizer$:536 - Top1Accuracy is Accuracy(correct: 9568, count: 10000, accuracy: 0.9568)</span></pre></div></div><div class="section" title="High resolution image generation using SRGAN"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec56"/><span class="koboSpan" id="kobo.129.1">High resolution image generation using SRGAN</span></h2></div></div></div><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.130.1">Super Resolution Generative Network</span></strong></span><span class="koboSpan" id="kobo.131.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.132.1">SRGAN)</span></strong></span><span class="koboSpan" id="kobo.133.1"> excels in generating high resolution images from its low-resolution</span><a id="id243" class="indexterm"/><span class="koboSpan" id="kobo.134.1"> counterpart. </span><span class="koboSpan" id="kobo.134.2">During the training phase, a high resolution image is transformed to a low resolution image by applying the Gaussian filter to a high resolution image</span><a id="id244" class="indexterm"/><span class="koboSpan" id="kobo.135.1"> followed by the down-sampling operation.</span></p><p><span class="koboSpan" id="kobo.136.1">Let us define some notation before diving into the network architecture:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.137.1">I</span></em></span><sup><span class="emphasis"><em><span class="koboSpan" id="kobo.138.1">LR</span></em></span></sup><span class="koboSpan" id="kobo.139.1">: Low resolution image having the size width(</span><span class="emphasis"><em><span class="koboSpan" id="kobo.140.1">W</span></em></span><span class="koboSpan" id="kobo.141.1">) x height(</span><span class="emphasis"><em><span class="koboSpan" id="kobo.142.1">H</span></em></span><span class="koboSpan" id="kobo.143.1">) x color channels(</span><span class="emphasis"><em><span class="koboSpan" id="kobo.144.1">C</span></em></span><span class="koboSpan" id="kobo.145.1">)</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.146.1">I</span></em></span><sup><span class="emphasis"><em><span class="koboSpan" id="kobo.147.1">HR</span></em></span></sup><span class="koboSpan" id="kobo.148.1">: High resolution image having the size </span><span class="emphasis"><em><span class="koboSpan" id="kobo.149.1">rW × rH × C</span></em></span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.150.1">I</span></em></span><sup><span class="emphasis"><em><span class="koboSpan" id="kobo.151.1">SR</span></em></span></sup><span class="koboSpan" id="kobo.152.1">: Super resolution image having the size </span><span class="emphasis"><em><span class="koboSpan" id="kobo.153.1">rW × rH × C</span></em></span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.154.1">r</span></em></span><span class="koboSpan" id="kobo.155.1">: down sampling factor</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.156.1">G</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.157.1">θG</span></em></span></sub><span class="koboSpan" id="kobo.158.1">: Generator network</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.159.1">D</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.160.1">θD</span></em></span></sub><span class="koboSpan" id="kobo.161.1">: Discriminator network</span></li></ul></div><p><span class="koboSpan" id="kobo.162.1">To achieve the goal of estimating a high resolution input image from its corresponding low resolution counterpart, the generator network is trained as a feed-forward convolution neural network </span><span class="emphasis"><em><span class="koboSpan" id="kobo.163.1">G</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.164.1">θG</span></em></span></sub><span class="koboSpan" id="kobo.165.1"> parametrized by </span><span class="emphasis"><em><span class="koboSpan" id="kobo.166.1">θG</span></em></span><span class="koboSpan" id="kobo.167.1"> where </span><span class="emphasis"><em><span class="koboSpan" id="kobo.168.1">θG</span></em></span><span class="koboSpan" id="kobo.169.1"> is represented by weights (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.170.1">W1:L</span></em></span><span class="koboSpan" id="kobo.171.1">) and biases (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.172.1">b1:L</span></em></span><span class="koboSpan" id="kobo.173.1">) of the L-layer of the deep network and is obtained by optimizing super resolution specific </span><code class="literal"><span class="koboSpan" id="kobo.174.1">loss</span></code><span class="koboSpan" id="kobo.175.1"> function. </span><span class="koboSpan" id="kobo.175.2">For training images having high resolution </span><span class="inlinemediaobject"><span class="koboSpan" id="kobo.176.1"><img src="graphics/B08086_05_23.jpg" alt="High resolution image generation using SRGAN"/></span></span><span class="koboSpan" id="kobo.177.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.178.1">n=1</span></em></span><span class="koboSpan" id="kobo.179.1">; N along with its corresponding low resolution </span><span class="inlinemediaobject"><span class="koboSpan" id="kobo.180.1"><img src="graphics/B08086_05_24.jpg" alt="High resolution image generation using SRGAN"/></span></span><span class="koboSpan" id="kobo.181.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.182.1">n=1</span></em></span><span class="koboSpan" id="kobo.183.1">, N, we can solve for </span><span class="emphasis"><em><span class="koboSpan" id="kobo.184.1">θG</span></em></span><span class="koboSpan" id="kobo.185.1">, as follows:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.186.1"><img src="graphics/B08086_05_10.jpg" alt="High resolution image generation using SRGAN"/></span></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note08"/><span class="koboSpan" id="kobo.187.1">Note</span></h3><p><span class="koboSpan" id="kobo.188.1">The formulation of the perceptual loss</span><a id="id245" class="indexterm"/><span class="koboSpan" id="kobo.189.1"> function </span><span class="emphasis"><em><span class="koboSpan" id="kobo.190.1">lSR</span></em></span><span class="koboSpan" id="kobo.191.1"> is critical for the performance of the generator network. </span><span class="koboSpan" id="kobo.191.2">In general, perceptual loss is commonly modeled based on </span><span class="strong"><strong><span class="koboSpan" id="kobo.192.1">Mean Square Error</span></strong></span><span class="koboSpan" id="kobo.193.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.194.1">MSE</span></strong></span><span class="koboSpan" id="kobo.195.1">), but to avoid unsatisfying solutions with overly smooth textures, a new content loss based on the ReLU activation layers of the pre-trained 19 layer VGG network is formulated.</span></p></div></div><p><span class="koboSpan" id="kobo.196.1">The perceptual loss is the weighted combination of several </span><code class="literal"><span class="koboSpan" id="kobo.197.1">loss</span></code><span class="koboSpan" id="kobo.198.1"> functions that map important characteristics of the super resolution image as follows:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.199.1"><img src="graphics/B08086_05_11.jpg" alt="High resolution image generation using SRGAN"/></span></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.200.1">Content loss</span></strong></span><span class="koboSpan" id="kobo.201.1">: The VGG-based</span><a id="id246" class="indexterm"/><span class="koboSpan" id="kobo.202.1"> content loss is defined as the Euclidean distance between the feature representations of a reconstructed image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.203.1">G</span></em></span><span class="emphasis"><em><sub><span class="koboSpan" id="kobo.204.1">θG</span></sub></em></span><span class="koboSpan" id="kobo.205.1"> (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.206.1">I</span></em></span><sup><span class="emphasis"><em><span class="koboSpan" id="kobo.207.1">LR</span></em></span></sup><span class="koboSpan" id="kobo.208.1">) and the corresponding high resolution image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.209.1">I</span></em></span><span class="emphasis"><em><span class="koboSpan" id="kobo.210.1">HR</span></em></span><span class="koboSpan" id="kobo.211.1">. </span><span class="koboSpan" id="kobo.211.2">Here </span><span class="inlinemediaobject"><span class="koboSpan" id="kobo.212.1"><img src="graphics/B08086_05_25.jpg" alt="High resolution image generation using SRGAN"/></span></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.213.1">i,j</span></em></span></sub><span class="koboSpan" id="kobo.214.1"> indicate the feature map obtained by the jth convolution (after activation) before the ith max-pooling layer within the VGG19 network. </span><span class="koboSpan" id="kobo.214.2">And </span><span class="emphasis"><em><span class="koboSpan" id="kobo.215.1">Wi,j</span></em></span><span class="koboSpan" id="kobo.216.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.217.1">Hi,j</span></em></span><span class="koboSpan" id="kobo.218.1"> describe the dimensions of the respective feature maps within the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.219.1">VGG</span></em></span><span class="koboSpan" id="kobo.220.1"> network:
</span><div class="mediaobject"><span class="koboSpan" id="kobo.221.1"><img src="graphics/B08086_05_12.jpg" alt="High resolution image generation using SRGAN"/></span></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.222.1">Adversarial loss</span></strong></span><span class="koboSpan" id="kobo.223.1">: The generative</span><a id="id247" class="indexterm"/><span class="koboSpan" id="kobo.224.1"> loss is based on the probabilities of the discriminator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.225.1">D</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.226.1">θD</span></em></span></sub><span class="emphasis"><em><span class="koboSpan" id="kobo.227.1">(G</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.228.1">θG</span></em></span></sub><span class="emphasis"><em><span class="koboSpan" id="kobo.229.1">(I</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.230.1">LR</span></em></span></sub><span class="emphasis"><em><span class="koboSpan" id="kobo.231.1">))</span></em></span><span class="koboSpan" id="kobo.232.1"> over all training images and encourages the network to favor solutions residing on the manifold of natural images, in order to fool the discriminator network:</span><div class="mediaobject"><span class="koboSpan" id="kobo.233.1"><img src="graphics/B08086_05_13.jpg" alt="High resolution image generation using SRGAN"/></span></div></li></ul></div><p><span class="koboSpan" id="kobo.234.1">Similar to the concept of adversarial network, the general idea behind the SRGAN approach is to train a generator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.235.1">G</span></em></span><span class="koboSpan" id="kobo.236.1"> with the goal of fooling a discriminator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.237.1">D</span></em></span><span class="koboSpan" id="kobo.238.1"> that is trained to distinguish super-resolution images from real images:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.239.1"><img src="graphics/B08086_05_14.jpg" alt="High resolution image generation using SRGAN"/></span></div><p><span class="koboSpan" id="kobo.240.1">Based on this approach the generator learns to create solutions that are highly similar to real images and thus hard to classify by discriminator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.241.1">D</span></em></span><span class="koboSpan" id="kobo.242.1">. </span><span class="koboSpan" id="kobo.242.2">And this encourages perceptually superior solutions residing in the subspace, the manifold, of natural images.</span></p></div><div class="section" title="Architecture of the SRGAN"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec57"/><span class="koboSpan" id="kobo.243.1">Architecture of the SRGAN</span></h2></div></div></div><p><span class="koboSpan" id="kobo.244.1">As illustrated in the following figure, the generator network </span><span class="strong"><strong><span class="koboSpan" id="kobo.245.1">G</span></strong></span><span class="koboSpan" id="kobo.246.1"> consists of </span><span class="strong"><strong><span class="koboSpan" id="kobo.247.1">B</span></strong></span> <span class="strong"><strong><span class="koboSpan" id="kobo.248.1">residual blocks</span></strong></span><span class="koboSpan" id="kobo.249.1"> with identical layout. </span><span class="koboSpan" id="kobo.249.2">Each</span><a id="id248" class="indexterm"/><span class="koboSpan" id="kobo.250.1"> block has two convolutional layers with small 3×3 kernels and 64 feature maps </span><a id="id249" class="indexterm"/><span class="koboSpan" id="kobo.251.1">followed by batch-normalization layers [32] and ParametricReLU [28] as the </span><code class="literal"><span class="koboSpan" id="kobo.252.1">activation</span></code><span class="koboSpan" id="kobo.253.1"> function. </span><span class="koboSpan" id="kobo.253.2">The resolution of the input image is increased by two trained sub-pixel convolution layers.</span></p><p><span class="koboSpan" id="kobo.254.1">The discriminator network uses Leaky ReLU activation (with </span><span class="emphasis"><em><span class="koboSpan" id="kobo.255.1">α</span></em></span><span class="koboSpan" id="kobo.256.1"> = 0.2) and consists of eight convolutional layers with an increasing number of 3×3 filter kernels, increasing by a factor of 2 from 64 to 512 kernels. </span><span class="koboSpan" id="kobo.256.2">Each time the number of features is doubled, strided convolutions are used to reduce the image resolution. </span></p><p><span class="koboSpan" id="kobo.257.1">The resulting 512 feature maps go through two dense layers followed by a final sigmoid activation layer to obtain a classification probability for a generated image sample:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.258.1"><img src="graphics/B08086_05_15.png.jpg" alt="Architecture of the SRGAN"/></span><div class="caption"><p><span class="koboSpan" id="kobo.259.1">Figure-5: Architecture of generator and discriminator network with corresponding kernel size (k), number of feature maps (n) and stride (s) indicated for each convolutional layer.</span></p><p><span class="koboSpan" id="kobo.260.1">Source: </span><span class="emphasis"><em><span class="koboSpan" id="kobo.261.1">arXiv, 1609.04802, 2017</span></em></span>
</p></div></div><p><span class="koboSpan" id="kobo.262.1">Now it's time to deep dive into the code with TensorFlow and generate high resolution images using an LFW facial dataset.</span></p><p><span class="koboSpan" id="kobo.263.1">The generator network is first built as a single deconvolution layer with 3×3 kernels and 64 feature maps followed by ReLU as an </span><code class="literal"><span class="koboSpan" id="kobo.264.1">activation</span></code><span class="koboSpan" id="kobo.265.1"> function. </span><span class="koboSpan" id="kobo.265.2">Then there are five residual blocks with</span><a id="id250" class="indexterm"/><span class="koboSpan" id="kobo.266.1"> identical layout of each block having two convolutional layers followed by batch-normalization and ReLU. </span><span class="koboSpan" id="kobo.266.2">Finally, the resolution of the input image is increased by two trained pixel-shuffle layers:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.267.1">def generator(self, x, is_training, reuse):
    with tf.variable_scope('generator', reuse=reuse):
      with tf.variable_scope('deconv1'):
          x = deconv_layer(
               x, [3, 3, 64, 3], [self.batch_size, 24, 24, 64], 1)
          x = tf.nn.relu(x)
          shortcut = x
# 5 Residual block with identical layout of deconvolution layers having batch norm and relu as activation function.
    </span><span class="koboSpan" id="kobo.267.2">for i in range(5):
       mid = x
       with tf.variable_scope('block{}a'.format(i+1)):
         x = deconv_layer(x, [3, 3, 64, 64], [self.batch_size, 24, 
24, 64], 1)
         x = batch_normalize(x, is_training)
         x = tf.nn.relu(x)

# 2 deconvolution layers having pixel-suffle and relu as activation function. 
     </span><span class="koboSpan" id="kobo.267.3">with tf.variable_scope('deconv3'):
         x = deconv_layer(x, [3, 3, 256, 64], [self.batch_size, 24, 
24, 256], 1)
         x = pixel_shuffle_layer(x, 2, 64) # n_split = 256 / 2 ** 2
         x = tf.nn.relu(x)
     with tf.variable_scope('deconv4'):
         x = deconv_layer(x, [3, 3, 64, 64], [self.batch_size, 48, 
48, 64], 1)
         x = pixel_shuffle_layer(x, 2, 16)
         x = tf.nn.relu(x)

     . </span><span class="koboSpan" id="kobo.267.4">. </span><span class="koboSpan" id="kobo.267.5">. </span><span class="koboSpan" id="kobo.267.6">. </span><span class="koboSpan" id="kobo.267.7">. </span><span class="koboSpan" id="kobo.267.8">. </span><span class="koboSpan" id="kobo.267.9">. </span><span class="koboSpan" id="kobo.267.10">. </span><span class="koboSpan" id="kobo.267.11">[code omitted for clarity]

return x	</span></pre></div><p><span class="koboSpan" id="kobo.268.1">The </span><code class="literal"><span class="koboSpan" id="kobo.269.1">deconvolution layer</span></code><span class="koboSpan" id="kobo.270.1"> function is defined with a TensorFlow </span><code class="literal"><span class="koboSpan" id="kobo.271.1">conv2d_transpose</span></code><span class="koboSpan" id="kobo.272.1"> method with Xavier initialization as follows:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.273.1">def deconv_layer(x, filter_shape, output_shape, stride, trainable=True):
    filter_ = tf.get_variable(
        name='weight',
        shape=filter_shape,
        dtype=tf.float32,
        initializer=tf.contrib.layers.xavier_initializer(),
        trainable=trainable)
    return tf.nn.conv2d_transpose(
        value=x,
        filter=filter_,
        output_shape=output_shape,
        strides=[1, stride, stride, 1])</span></pre></div><p><span class="koboSpan" id="kobo.274.1">The discriminator network </span><a id="id251" class="indexterm"/><span class="koboSpan" id="kobo.275.1">consists of eight convolutional layers having 3×3 filter kernels that get increased by a factor of 2 from 64 to 512 kernels. </span><span class="koboSpan" id="kobo.275.2">The resulting 512 feature maps are flattened and go through two dense fully connected layers followed by a final softmax layer to obtain a classification probability for a generated image sample:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.276.1">def discriminator(self, x, is_training, reuse):
        with tf.variable_scope('discriminator', reuse=reuse):
            with tf.variable_scope('conv1'):
                x = conv_layer(x, [3, 3, 3, 64], 1)
                x = lrelu(x)
            with tf.variable_scope('conv2'):
                x = conv_layer(x, [3, 3, 64, 64], 2)
                x = lrelu(x)
                x = batch_normalize(x, is_training)

         .  </span><span class="koboSpan" id="kobo.276.2">.   </span><span class="koboSpan" id="kobo.276.3">.   </span><span class="koboSpan" id="kobo.276.4">.   </span><span class="koboSpan" id="kobo.276.5">.   </span><span class="koboSpan" id="kobo.276.6">. </span><span class="koboSpan" id="kobo.276.7">[code omitted for clarity]

            x = flatten_layer(x)
            with tf.variable_scope('fc'):
                x = full_connection_layer(x, 1024)
                x = lrelu(x)
            with tf.variable_scope('softmax'):
                x = full_connection_layer(x, 1)
                
return x</span></pre></div><p><span class="koboSpan" id="kobo.277.1">The network uses LeakyReLU (with </span><span class="emphasis"><em><span class="koboSpan" id="kobo.278.1">α</span></em></span><span class="koboSpan" id="kobo.279.1"> = 0.2) as an </span><code class="literal"><span class="koboSpan" id="kobo.280.1">activation</span></code><span class="koboSpan" id="kobo.281.1"> function with the convolutional layer:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.282.1">def lrelu(x, trainbable=None):
    alpha = 0.2
    return tf.maximum(alpha * x, x)</span></pre></div><p><span class="koboSpan" id="kobo.283.1">The </span><code class="literal"><span class="koboSpan" id="kobo.284.1">convolution layer</span></code><span class="koboSpan" id="kobo.285.1"> function is defined with a TensorFlow </span><code class="literal"><span class="koboSpan" id="kobo.286.1">conv2d</span></code><span class="koboSpan" id="kobo.287.1"> method with Xavier initialization as follows:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.288.1">def conv_layer(x, filter_shape, stride, trainable=True):
    filter_ = tf.get_variable(
        name='weight', 
        shape=filter_shape,
        dtype=tf.float32, 
        initializer=tf.contrib.layers.xavier_initializer(),
        trainable=trainable)
    return tf.nn.conv2d(
        input=x,
        filter=filter_,
        strides=[1, stride, stride, 1],
        padding='SAME')</span></pre></div><p><span class="koboSpan" id="kobo.289.1">Please note that the code</span><a id="id252" class="indexterm"/><span class="koboSpan" id="kobo.290.1"> implementation uses the least squares </span><code class="literal"><span class="koboSpan" id="kobo.291.1">loss</span></code><span class="koboSpan" id="kobo.292.1"> function (to avoid the vanishing gradient problem) for the discriminator, instead of the sigmoid cross entropy </span><code class="literal"><span class="koboSpan" id="kobo.293.1">loss</span></code><span class="koboSpan" id="kobo.294.1"> function as proposed in the original paper of SRGAN (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.295.1">arXiv, 1609.04802, 2017</span></em></span><span class="koboSpan" id="kobo.296.1">):</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.297.1">def inference_adversarial_loss(real_output, fake_output):
alpha = 1e-5
g_loss = tf.reduce_mean(
tf.nn.l2_loss(fake_output - tf.ones_like(fake_output)))
d_loss_real = tf.reduce_mean(
tf.nn.l2_loss(real_output - tf.ones_like(true_output)))
d_loss_fake = tf.reduce_mean(
tf.nn.l2_loss(fake_output + tf.zeros_like(fake_output)))
d_loss = d_loss_real + d_loss_fake
return (g_loss * alpha, d_loss * alpha)

generator_loss, discriminator_loss = (
inference_adversarial_loss(true_output, fake_output))</span></pre></div><p><span class="koboSpan" id="kobo.298.1">More</span><a id="id253" class="indexterm"/><span class="koboSpan" id="kobo.299.1"> information about </span><span class="strong"><strong><span class="koboSpan" id="kobo.300.1">Least Square GAN</span></strong></span><span class="koboSpan" id="kobo.301.1"> can be found at: </span><a class="ulink" href="https://arxiv.org/abs/1611.04076"><span class="koboSpan" id="kobo.302.1">https://arxiv.org/abs/1611.04076</span></a>
</p><p><span class="koboSpan" id="kobo.303.1">The </span><code class="literal"><span class="koboSpan" id="kobo.304.1">code</span></code><span class="koboSpan" id="kobo.305.1"> directory structure for running the</span><a id="id254" class="indexterm"/><span class="koboSpan" id="kobo.306.1"> SRGAN is shown as follows:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.307.1"><img src="graphics/B08086_05_16.png.jpg" alt="Architecture of the SRGAN"/></span></div><p><span class="koboSpan" id="kobo.308.1">First let us download an </span><code class="literal"><span class="koboSpan" id="kobo.309.1">LFW facial</span></code><span class="koboSpan" id="kobo.310.1"> dataset and do some preprocessing (frontal face detection and splitting the dataset as train and test) and store the dataset under the </span><code class="literal"><span class="koboSpan" id="kobo.311.1">data/</span></code><span class="koboSpan" id="kobo.312.1"> directory:</span></p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.313.1">python download-preprocess-lfw.py</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.314.1"><img src="graphics/B08086_05_17.png.jpg" alt="Architecture of the SRGAN"/></span></div><p><span class="koboSpan" id="kobo.315.1">Next download the pre-trained </span><a id="id255" class="indexterm"/><span class="koboSpan" id="kobo.316.1">VGG19 model from the following link, extract it, and save it under the </span><code class="literal"><span class="koboSpan" id="kobo.317.1">backup/</span></code><span class="koboSpan" id="kobo.318.1"> directory:</span></p><p>
<a class="ulink" href="https://drive.google.com/open?id=0B-s6ok7B0V9vcXNfSzdjZ0lCc0k"><span class="koboSpan" id="kobo.319.1">https://drive.google.com/open?id=0B-s6ok7B0V9vcXNfSzdjZ0lCc0k</span></a>
</p><p><span class="koboSpan" id="kobo.320.1">Next execute the </span><code class="literal"><span class="koboSpan" id="kobo.321.1">trainSrgan.py</span></code><span class="koboSpan" id="kobo.322.1"> file to start the SRGAN operation using a VGG19 model:</span></p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.323.1">python trainSrgan.py</span></strong></span>
</pre></div><p><span class="koboSpan" id="kobo.324.1">Once the training is going on the generator network will start generating super resolution images in the </span><code class="literal"><span class="koboSpan" id="kobo.325.1">result/</span></code><span class="koboSpan" id="kobo.326.1"> directory. </span><span class="koboSpan" id="kobo.326.2">Some of the sample images from the </span><code class="literal"><span class="koboSpan" id="kobo.327.1">result/</span></code><span class="koboSpan" id="kobo.328.1"> directory are shown as follows:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.329.1"><img src="graphics/B08086_05_18.png.jpg" alt="Architecture of the SRGAN"/></span></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Generating artistic hallucinated images using DeepDream"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec29"/><span class="koboSpan" id="kobo.1.1">Generating artistic hallucinated images using DeepDream</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">The DeepDream algorithm is a</span><a id="id256" class="indexterm"/><span class="koboSpan" id="kobo.3.1"> modified neural network that has capability of producing impressive surrealistic, dream-like hallucinogenic</span><a id="id257" class="indexterm"/><span class="koboSpan" id="kobo.4.1"> appearances by changing the image in the direction of training data. </span><span class="koboSpan" id="kobo.4.2">It uses backpropagation to change the image instead of changing weights through the network.</span></p><p><span class="koboSpan" id="kobo.5.1">Broadly, the algorithm can be summarized in the following steps: </span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.6.1">Select a layer of the network and a filter that you feel is interesting.</span></li><li class="listitem"><span class="koboSpan" id="kobo.7.1">Then compute the activations of the image up to that layer.</span></li><li class="listitem"><span class="koboSpan" id="kobo.8.1">Back-propagate the activations of the filter back to the input image.</span></li><li class="listitem"><span class="koboSpan" id="kobo.9.1">Multiply the gradients with learning rate and add them to the input image.</span></li><li class="listitem"><span class="koboSpan" id="kobo.10.1">Repeat steps 2 to 4 until you are satisfied with the result.</span></li></ol></div><p><span class="koboSpan" id="kobo.11.1">Applying the algorithm</span><a id="id258" class="indexterm"/><span class="koboSpan" id="kobo.12.1"> iteratively on the output and applying some zooming after each iteration helps the network to generate an endless stream of new impressions by exploring the set of things that it knows about.</span></p><p><span class="koboSpan" id="kobo.13.1">Let's deep dive into the code to</span><a id="id259" class="indexterm"/><span class="koboSpan" id="kobo.14.1"> generate a hallucinogenic dreamy image. </span><span class="koboSpan" id="kobo.14.2">We will apply the following settings to the various layers of the pre-trained VGG16 model available in Keras. </span><span class="koboSpan" id="kobo.14.3">Note that instead of using pre-trained we can apply this setting to a fresh neural network architecture of your choice as well:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.15.1">settings_preset = {
    'dreamy': {
        'features': {
            'block5_conv1': 0.05,
            'block5_conv2': 0.02
        },
        'continuity': 0.1,
        'dream_l2': 0.02,
        'jitter': 0
    }
}

settings = settings_preset['dreamy']</span></pre></div><p><span class="koboSpan" id="kobo.16.1">This utility function basically loads, resizes, and formats images to an appropriate tensors format:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.17.1">def preprocess_image(image_path):
    img = load_img(image_path, target_size=(img_height, img_width))
    img = img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = vgg16.preprocess_input(img)
    return img</span></pre></div><p><span class="koboSpan" id="kobo.18.1">Then we calculate the continuity loss to give the image a local coherence and avoid messy blurs that look like a variant of the total variation loss discussed in the paper (</span><a class="ulink" href="http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf"><span class="koboSpan" id="kobo.19.1">http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf</span></a><span class="koboSpan" id="kobo.20.1">):</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.21.1">def continuity_loss(x):
    assert K.ndim(x) == 4
    a = K.square(x[:, :img_height-1, :img_width-1, :] -
                 x[:, 1:, :img_width-1, :])
    b = K.square(x[:, :img_height-1, :img_width-1, :] -
                 x[:, :img_height-1, 1:, :])

# (a+b) is the squared spatial gradient, 1.25 is a hyperparameter # that should be &gt;1.0 as discussed in the aforementioned paper
    return K.sum(K.pow(a+b, 1.25))</span></pre></div><p><span class="koboSpan" id="kobo.22.1">Next, we will load the VGG16 model with pretrained weights:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.23.1">model = vgg16.VGG16(input_tensor=dream, weights='imagenet', include_top=False)</span></pre></div><p><span class="koboSpan" id="kobo.24.1">After that we will add the </span><code class="literal"><span class="koboSpan" id="kobo.25.1">l2</span></code><span class="koboSpan" id="kobo.26.1"> norm of the features of a layer to the </span><code class="literal"><span class="koboSpan" id="kobo.27.1">loss</span></code><span class="koboSpan" id="kobo.28.1"> and then add continuity loss to</span><a id="id260" class="indexterm"/><span class="koboSpan" id="kobo.29.1"> give the image local coherence</span><a id="id261" class="indexterm"/><span class="koboSpan" id="kobo.30.1"> followed by adding again </span><code class="literal"><span class="koboSpan" id="kobo.31.1">l2</span></code><span class="koboSpan" id="kobo.32.1"> norm to loss in order to prevent pixels from taking very high values and then compute the gradients of the dream with respect to the loss:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.33.1">loss += settings['continuity'] * continuity_loss(dream) / np.prod(img_size)
loss += settings['dream_l2'] * K.sum(K.square(dream)) / np.prod(img_size)
grads = K.gradients(loss, dream)</span></pre></div><p><span class="koboSpan" id="kobo.34.1">Finally, we will add a </span><code class="literal"><span class="koboSpan" id="kobo.35.1">random_jitter</span></code><span class="koboSpan" id="kobo.36.1"> to the input image and run the </span><code class="literal"><span class="koboSpan" id="kobo.37.1">L-BFGS</span></code><span class="koboSpan" id="kobo.38.1"> optimizer over the pixels of the generated image to minimize the loss:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.39.1">random_jitter = (settings['jitter']*2) * (np.random.random(img_size)-0.5)
x += random_jitter

# run L-BFGS 
x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),
                              fprime=evaluator.grads, maxfun=7)</span></pre></div><p><span class="koboSpan" id="kobo.40.1">At the end, we will decode the dream and save it in an output image file:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.41.1">    x = x.reshape(img_size)
    x -= random_jitter
    img = deprocess_image(np.copy(x))
    fn = result_prefix + '_at_iteration_%d.png' % i
    imsave(fn, img)</span></pre></div><p><span class="koboSpan" id="kobo.42.1">The dreamy output generated just after five iterations is shown as follows:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.43.1"><img src="graphics/B08086_05_19.png.jpg" alt="Generating artistic hallucinated images using DeepDream"/></span><div class="caption"><p><span class="koboSpan" id="kobo.44.1">Figure-6: Left showing the original input image and right showing the dreamy artistic image created by deep dream</span></p></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Generating handwritten digits with VAE using TensorFlow"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec30"/><span class="koboSpan" id="kobo.1.1">Generating handwritten digits with VAE using TensorFlow</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">The </span><span class="strong"><strong><span class="koboSpan" id="kobo.3.1">Variational Autoencoder</span></strong></span><span class="koboSpan" id="kobo.4.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.5.1">VAE</span></strong></span><span class="koboSpan" id="kobo.6.1">) nicely synthesizes unsupervised learning with variational Bayesian methods into a</span><a id="id262" class="indexterm"/><span class="koboSpan" id="kobo.7.1"> sleek package. </span><span class="koboSpan" id="kobo.7.2">It applies a probabilistic turn on the basic autoencoder approach by treating inputs, hidden representations, and reconstructed outputs as probabilistic</span><a id="id263" class="indexterm"/><span class="koboSpan" id="kobo.8.1"> random variables within a directed graphical model.</span></p><p><span class="koboSpan" id="kobo.9.1">From Bayesian perspective, the encoder becomes a variational inference network, that maps the observed inputs to posterior distributions over latent space, and the decoder becomes a generative network that maps the arbitrary latent coordinates back to distributions over the original data space.</span></p><p><span class="koboSpan" id="kobo.10.1">VAE is all about adding a constraint on the encoding network that generates latent vectors that roughly follow a</span><a id="id264" class="indexterm"/><span class="koboSpan" id="kobo.11.1"> unit Gaussian distribution (this constraint that separates a VAE from a standard autoencoder) and then reconstructs the image back by passing the latent vector</span><a id="id265" class="indexterm"/><span class="koboSpan" id="kobo.12.1"> through the decoder network:</span></p><p> </p><div class="mediaobject"><span class="koboSpan" id="kobo.13.1"><img src="graphics/B08086_05_22.jpg" alt="Generating handwritten digits with VAE using TensorFlow"/></span></div><p>
</p></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="A real world analogy of VAE"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec31"/><span class="koboSpan" id="kobo.1.1">A real world analogy of VAE</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Let's say we want to</span><a id="id266" class="indexterm"/><span class="koboSpan" id="kobo.3.1"> generate data (an animal) and a good way of doing it is to first decide what kind of data we want to generate, before actually generating the data. </span><span class="koboSpan" id="kobo.3.2">So, we must imagine some criteria about representing the animal, like it should have four legs and be able to swim. </span><span class="koboSpan" id="kobo.3.3">Once we have those criteria, we can then generate the animal by sampling from the animal kingdom. </span><span class="koboSpan" id="kobo.3.4">Our imagination criteria are analogous to latent variables. </span><span class="koboSpan" id="kobo.3.5">Deciding the latent variable in the first place helps to describe the data well, otherwise it is like generating data blindly.</span></p><p><span class="koboSpan" id="kobo.4.1">The basic idea of VAE is to infer </span><span class="emphasis"><em><span class="koboSpan" id="kobo.5.1">p(z)</span></em></span><span class="koboSpan" id="kobo.6.1"> using </span><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">p(z|x)</span></em></span><span class="koboSpan" id="kobo.8.1">. </span><span class="koboSpan" id="kobo.8.2">Let's now expand with some mathematical notation:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.9.1">x</span></em></span><span class="koboSpan" id="kobo.10.1">: Represents the data (that is, animal) we want to generate</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.11.1">z</span></em></span><span class="koboSpan" id="kobo.12.1">: Represents the latent variable (that is, our imagination)</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.13.1">p(x)</span></em></span><span class="koboSpan" id="kobo.14.1">: Represents the distribution of data (that is, animal kingdom)</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.15.1">p(z)</span></em></span><span class="koboSpan" id="kobo.16.1">: Represents the normal probability distribution of the latent variable (that is, the source of imagination—our brain)</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.17.1">p(x|z)</span></em></span><span class="koboSpan" id="kobo.18.1">: Probability distribution of generating data given the latent variable (that is, turning imagination into a realistic animal)</span></li></ul></div><p><span class="koboSpan" id="kobo.19.1">As per the analogous example, we want to restrict our imagination only on the animal kingdom domain, so that we shouldn't imagine about things such as root, leaf, money, glass, GPU, refrigerator, carpet as it's very unlikely that those things have anything in common with the</span><a id="id267" class="indexterm"/><span class="koboSpan" id="kobo.20.1"> animal kingdom.</span></p><p><span class="koboSpan" id="kobo.21.1">The </span><code class="literal"><span class="koboSpan" id="kobo.22.1">loss</span></code><span class="koboSpan" id="kobo.23.1"> function of the VAE is basically the negative log-likelihood with a regularizer as follows:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.24.1"><img src="graphics/B08086_05_20.jpg" alt="A real world analogy of VAE"/></span></div><p><span class="koboSpan" id="kobo.25.1">The first term is the expected negative log-likelihood or reconstruction loss of the ith data point where the expectation is calculated with respect to the encoder's distribution over the representations. </span><span class="koboSpan" id="kobo.25.2">This term helps the decoder to reconstruct the data well and incur large costs if it fails to do so. </span><span class="koboSpan" id="kobo.25.3">The second term represents the Kullback-Leibler divergence between encoder distribution </span><span class="emphasis"><em><span class="koboSpan" id="kobo.26.1">q(z|x)</span></em></span><span class="koboSpan" id="kobo.27.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.28.1">p(z)</span></em></span><span class="koboSpan" id="kobo.29.1"> and acts as a regularizer that adds a penalty to the loss when an encoder's output representation </span><span class="emphasis"><em><span class="koboSpan" id="kobo.30.1">z</span></em></span><span class="koboSpan" id="kobo.31.1"> differs from normal distribution.</span></p><p><span class="koboSpan" id="kobo.32.1">Now let's dive deep into the code for generating handwritten digits from the MNIST dataset with VAE using TensorFlow. </span><span class="koboSpan" id="kobo.32.2">First let us create the encoder network </span><span class="emphasis"><em><span class="koboSpan" id="kobo.33.1">Q(z|X)</span></em></span><span class="koboSpan" id="kobo.34.1"> with a single hidden layer that will take </span><span class="emphasis"><em><span class="koboSpan" id="kobo.35.1">X</span></em></span><span class="koboSpan" id="kobo.36.1"> as input and output </span><span class="emphasis"><em><span class="koboSpan" id="kobo.37.1">μ(X)</span></em></span><span class="koboSpan" id="kobo.38.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.39.1">Σ(X)</span></em></span><span class="koboSpan" id="kobo.40.1"> as part of Gaussian distribution:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.41.1">X = tf.placeholder(tf.float32, shape=[None, X_dim])
z = tf.placeholder(tf.float32, shape=[None, z_dim])

Q_W1 = tf.Variable(xavier_init([X_dim, h_dim]))
Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))

Q_W2_mu = tf.Variable(xavier_init([h_dim, z_dim]))
Q_b2_mu = tf.Variable(tf.zeros(shape=[z_dim]))
Q_W2_sigma = tf.Variable(xavier_init([h_dim, z_dim]))
Q_b2_sigma = tf.Variable(tf.zeros(shape=[z_dim]))


def Q(X):
    h = tf.nn.relu(tf.matmul(X, Q_W1) + Q_b1)
    z_mu = tf.matmul(h, Q_W2_mu) + Q_b2_mu
    z_logvar = tf.matmul(h, Q_W2_sigma) + Q_b2_sigma
    return z_mu, z_logvar</span></pre></div><p><span class="koboSpan" id="kobo.42.1">Now we create the decoder network </span><span class="emphasis"><em><span class="koboSpan" id="kobo.43.1">P(X|z)</span></em></span><span class="koboSpan" id="kobo.44.1">:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.45.1">P_W1 = tf.Variable(xavier_init([z_dim, h_dim]))
P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))

P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))
P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))


def P(z):
    h = tf.nn.relu(tf.matmul(z, P_W1) + P_b1)
    logits = tf.matmul(h, P_W2) + P_b2
    prob = tf.nn.sigmoid(logits)
    return prob, logits</span></pre></div><p><span class="koboSpan" id="kobo.46.1">Then we calculate the</span><a id="id268" class="indexterm"/><span class="koboSpan" id="kobo.47.1"> reconstruction loss and Kullback-Leibler divergence loss and sum them up to get the total loss of the VAE network:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.48.1">recon_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X), 1)
kl_loss = 0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. </span><span class="koboSpan" id="kobo.48.2">- z_logvar, 1)
# VAE loss
vae_loss = tf.reduce_mean(recon_loss + kl_loss)</span></pre></div><p><span class="koboSpan" id="kobo.49.1">And then use an </span><code class="literal"><span class="koboSpan" id="kobo.50.1">AdamOptimizer</span></code><span class="koboSpan" id="kobo.51.1"> to minimize the loss:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.52.1">solver = tf.train.AdamOptimizer().minimize(vae_loss)</span></pre></div><p><span class="koboSpan" id="kobo.53.1">Run the file (</span><code class="literal"><span class="koboSpan" id="kobo.54.1">VAE.py</span></code><span class="koboSpan" id="kobo.55.1"> or </span><code class="literal"><span class="koboSpan" id="kobo.56.1">VAE.ipynb</span></code><span class="koboSpan" id="kobo.57.1">) to start VAE operations on an </span><code class="literal"><span class="koboSpan" id="kobo.58.1">MNIST</span></code><span class="koboSpan" id="kobo.59.1"> dataset and the images will be generated in the output folder. </span><span class="koboSpan" id="kobo.59.2">The sample handwritten digit images are generated after 10,000 iterations:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.60.1"><img src="graphics/B08086_05_21.png.jpg" alt="A real world analogy of VAE"/></span></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="A comparison of two generative models—GAN and VAE"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec32"/><span class="koboSpan" id="kobo.1.1">A comparison of two generative models—GAN and VAE</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Although both are very</span><a id="id269" class="indexterm"/><span class="koboSpan" id="kobo.3.1"> exciting approaches of the generative model and have helped researchers to make inroads into the unsupervised domain along with generating capability, these two models differ in how they are trained. </span><span class="koboSpan" id="kobo.3.2">GAN is rooted in game theory, with an objective to find the nash equilibrium between the discriminator network and generator network. </span><span class="koboSpan" id="kobo.3.3">Whereas VAE is basically a probabilistic graphical model rooted in Bayesian inference, whose goal is latent modeling, that is, it tries to</span><a id="id270" class="indexterm"/><span class="koboSpan" id="kobo.4.1"> model the probability distribution of underlying data, in order to sample new data from that distribution.</span></p><p><span class="koboSpan" id="kobo.5.1">VAE has a clear known way of evaluating the quality of the model (such as log-likelihood, either estimated by importance sampling or lower-bounded) compared to GAN, but the problem with VAE is that it uses direct mean squared error in the calculation of latent loss, instead of an adversarial network, so they over simplify the objective task as they are bound to work in a latent space and as a result they often generate blured images compared to GAN.</span></p></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec33"/><span class="koboSpan" id="kobo.1.1">Summary</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Transfer Learning addresses the problem of dealing with small data effectively without re-inventing the training wheel from scratch. </span><span class="koboSpan" id="kobo.2.2">You have learned to extract and transfer features from a pre-trained model and apply it to your own problem domain. </span><span class="koboSpan" id="kobo.2.3">Also, you have mastered training and running deeper models over a large scale distributed system using Spark and its related components. </span><span class="koboSpan" id="kobo.2.4">Then, you have generated a realistic high resolution image leveraging the power of Transfer Learning within SRGAN. </span><span class="koboSpan" id="kobo.2.5">Also, you have mastered the concepts of other generative model approaches such as VAE and DeepDream for artistic image generation. </span><span class="koboSpan" id="kobo.2.6">In the last chapter, we will shift our focus from training deep models or generative models and learn various approaches of deploying your deep learning-based applications in production.</span></p></div></div></div></body></html>