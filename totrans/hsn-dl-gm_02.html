<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Learning for Games</h1>
                </header>
            
            <article>
                
<p class="mce-root">Welcome to <em>Hands-on Deep Learning for Games</em>. This book is for anyone wanting an extremely practical approach to the complexity of <strong>deep learning</strong> (<strong>DL</strong>) for games. <span>Importantly</span>, the concepts discussed in this book aren't solely limited to games. Much of what we'll learn here will easily carry over to other applications/simulations.</p>
<p class="mce-root"><strong>Reinforcement learning</strong> (<strong>RL</strong>), which will be a core element we talk about in later chapters, is quickly becoming the dominant <strong>machine learning</strong> (<strong>ML</strong>) technology. It has been applied to everything from server optimization to predicting customer activity for retail markets. Our journey in this book will primarily be focused on game development, and our goal will be to build a working adventure game. Keep in the back of your mind how the same principles you discover in this book could be applied to other problems, such as simulations, robotics, and lots more.</p>
<p class="mce-root">In this chapter, we are going to start from the very basics of neural networks and deep learning. We will discuss the background of neural networks, working our way toward building a neural network that can play a simple text game. Specifically, this chapter will cover the following topics: </p>
<ul>
<li>The past, present, a<span>n</span>d future of DL</li>
<li>Neural networks – the foundation</li>
<li>Multilayer perceptron in <strong>TensorFlow</strong> (<strong>TF</strong>)</li>
<li>Understanding TensorFlow</li>
<li>Training neural networks with backpropagation</li>
<li>Building an Autoencoder in Keras</li>
</ul>
<div class="packt_infobox">This book assumes that you have a working knowledge of Python. You should be able to set up and activate a virtual environment. Later chapters will use Unity 3D, which is limited to Windows and macOS (apologies to those hardcore Linux users).</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>You might be inclined to skip this chapter if you've already grasped deep learning. Regardless, this chapter is well worth reading and will establish the terminology we use throughout the book. At the very least, do the hands-on exercise—you will thank yourself later!</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The past, present, and future of DL</h1>
                </header>
            
            <article>
                
<p>While the term <em>de</em><em>ep</em> <em>learning</em> was first associated with neural networks in 2000 by <span>Igor Aizenberg and colleagues, it has only become popular in the last 5 years. Prior to this, we called this type of algorithm an <strong>artificial neural network </strong>(<strong>ANN</strong></span>)<span>. However, deep learning refers to something broader than ANNs and includes many other areas of connected machines. Therefore, to clarify, we will be discussing the ANN form of DL for much of the remainder of this book. However, we will also discuss some other forms of DL that can be used in games, in <a href="6ca7a117-1a8c-49f9-89c0-ee2f2a1e8baf.xhtml">Chapter 5</a>, <em>Introducing DRL</em>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The past</h1>
                </header>
            
            <article>
                
<p>The first form of a <strong>multilayer perceptron</strong> (<strong>MLP</strong>) network, or what we now <span> </span>call an ANN, was introduced by Alexey Ivakhnenko<span> in 1965</span><span>. Ivakhnenko waited several years before writing about the multilayer perceptron in 1971. The concept took a while to percolate and it wasn't until the 1980s that more research began. This time, image classification and speech recognition were attempted, and they failed, but progress was being made. It took another 10 years, and in the late 90s, it became popular again. So much so that ANNs made their way into some games, again, until better methods came along. Things quietened down and another decade or so passed.</span></p>
<p><span>Then, in 2012, Andrew Ng and Jeff Dean used an ANN to recognize cats in videos, and the interest in deep learning exploded. Their stride was one of several trivial (yet entertaining) advancements which made people sit up and take notice of deep learning. Then, in 2015, Google's <strong>DeepMind</strong> team built AlphaGo, and this time the whole world sat up. AlphaGo was shown to solidly beat the best players in the world at the game of Go, and that changed everything. Other techniques soon followed, <strong>Deep Reinforcement Learning</strong> (<strong>DRL</strong>) being one, showing that human performance could be consistently beaten in areas where that was previously not thought of as possible.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="packt_infobox"><span>While teaching their students about neural networks, there is a humorous and pertinent tale professors enjoy sharing: <em>The US Army did early research in the '80s using an ANN to recognize enemy tanks. The algorithm worked 100% of the time, and the army organized a big demonstration to showcase its success. Unfortunately, nothing</em></span><em><span> worked a</span><span>t the demonstration, and every test failed </span><span>miserably</span><span>. After going back and analyzing things, the army realized the ANN wasn't recognizing enemy tanks at all. Instead, it had been trained on images taken on a cloudy day, and all it was doing was recognizing the clouds.</span></em></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The present</h1>
                </header>
            
            <article>
                
<p>At present, at least at the time of writing, we are still in the midst of a deep learning explosion with debris and chaos, and it is our job as developers to make sense of all this. Neural networks are currently the basis for many DL technologies, several of which we will cover in this book. Except, it seems that <span>every day,</span> new and more powerful techniques emerge, and researchers scramble to make sense of them. Now, this rush of ideas can actually stall a technology, as researchers spend more and more time trying to replicate results. It is most certainly a cause for much of the earlier stalling that ANNs (deep learning) previously suffered from. In fact, many skeptics in the industry are predicting yet another hiccup. So, should you be worried, is reading this book worth it? The short answer is <em>yes</em>. The long answer is <em>probably not,</em> this time things are very different and many deep learning concepts are now generating revenue, which is a good thing. The fact that DL technology is now a proven money-earner puts investors at ease and only encourages new investment and growth. Exactly how much growth is yet to be seen, but the machine and DL space is now ripe with opportunity and growth from all sectors.</p>
<p>So, is it still possible that the game industry will again turn its back on games? That is also unlikely, generally because many of the more recent <span>major </span>advances, like reinforcement learning, were built to play classic Atari games, and use games as the problem. This only encourages more research into deep learning using games. Unity 3D, the game platform, has made a major investment into reinforcement learning for playing games. In fact, Unity is developing some of the most cutting-edge technology in reinforcement learning and we will be working with this platform later. Unity does use C# for scripting but uses Python to build and train deep learning models.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The future</h1>
                </header>
            
            <article>
                
<p>Predicting the future of anything is extremely difficult, but if you watch carefully enough, you may gain some insight into what, where, or how things will develop. Of course, having a crystal ball or a well-trained neural network would certainly help, but a lot of what becomes popular often hinges on the next great achievement. Without any ability to predict that, what can we observe about the current trend in deep learning research and commercial development? Well, the current trend is to use ML to generate DL; that is, a machine essentially assembles itself a neural network that is addressed to solve a problem. Google is currently investing considerable resources into building a technology called <strong>AutoML</strong>, which generates a neural network inference model that can recognize objects/activities in images, speech recognition, or handwriting recognition, and more. Geoffery Hinton, who is often cited as the godfather of the ANN, has recently shown that complex deep network systems can be decomposed into reusable layers. Essentially, you can construct a network using layers extracted from various pre-trained models. This will certainly evolve into more interesting tech and plays well into the DL search but also makes way for the next phase in computing.</p>
<p>Now, programming code is going to become too tedious, difficult, and expensive <span>at some point</span>. We can already see this with the explosion of offshore development, with companies seeking the cheapest developers. It is now estimated that code costs an average of $10-$20 per line, yes, per <em>line</em>. So, at what point will the developer start building their code in the form of an ANN or <strong>TensorFlow </strong>(<strong>TF</strong>) inference graph? Well, for most of this book, the DL code we develop will be generated down to a TF inference graph; a brain, if you will. We will then use these brains in the last chapter of the book to build intelligence in our adventure game. The technique of building graph models is quickly becoming mainstream. Many online ML apps now allow users to build models that can recognize things in images, speech, and videos, all by just uploading training content and pressing a button. Does this mean that apps could be developed this way in the future without any programming? The answer is yes, and it is already happening.</p>
<p>Now that we have explored the past, present, and future of deep learning, we can start to dig into more of the nomenclature and how neural networks actually work, in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural networks – the foundation</h1>
                </header>
            
            <article>
                
<p>The inspiration for neural networks or multilayer perceptrons is the human brain and nervous system. At the heart of our nervous system is the neuron pictured above the computer analog, which is a <span>perceptron</span>:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/8455ea99-f7e7-4664-a948-86f3e3fb1811.png" style="width:27.58em;height:15.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example of human neuron beside a perceptron</div>
<p>The neurons in our brain collect input, do something, and then spit out a response much like the computer analog, the <strong>perceptron</strong>. A perceptron takes a set of inputs, sums them all up, and passes them through an activation function. That activation function determines whether to send output, and at what level to send it when activated. Let's take a closer look at the perceptron, as follows:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/9734ac86-d48c-4952-9f17-e790fe5fe746.png" style="width:34.33em;height:17.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Perceptron</div>
<p><span>On the left-hand side of the preceding diagram, you can see the set of inputs getting pushed in, plus a constant bias. We will get more into the bias later. Then the inputs are multiplied by a set of individual weights and passed through an activation function. In Python code, it is as simple as the one in <kbd>Chapter_1_1.py</kbd>:</span></p>
<pre><br/>inputs = [1,2]<br/>weights = [1,1,1]<br/><br/>def perceptron_predict(inputs, weights):<br/>    activation = weights[0]<br/>    for i in range(len(inputs)-1):<br/>        activation += weights[i] * input<br/>    return 1.0 if activation &gt;= 0.0 else 0.0<br/><br/>print(perceptron_predict(inputs,weights))</pre>
<p>Note how the <kbd>weights</kbd> list has one more element than the <kbd>inputs</kbd> list; that is to account for the bias (<kbd>weights[0]</kbd>). Other than that, you can see we just simply loop through the <kbd>inputs</kbd>, multiplying them by the designated weight and adding the bias. Then the <kbd>activation</kbd> is compared to <kbd>0.0</kbd>, and if it is greater than 0, we output. In this very simple example, we are just comparing the value to 0, which is essentially a simple step function. We will spend some time later revisiting various activation functions over and over again; consider this simple model an essential part of carrying out those functions.</p>
<div class="packt_tip">What is the output from the preceding block of sample code? See whether you can figure it out, or take the less challenging route and copy and paste it into your favorite Python editor and run it. The code will run as is and requires no special libraries.</div>
<p>In the previous code example, we are looking at one point of input data, <kbd>[1,2]</kbd>, which is hardly useful when it comes to DL. DL models typically require hundreds, thousands, or even millions of data points or sets of input data to train and learn effectively. Fortunately, with one perceptron, the amount of data we need is less than 10.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's expand on the preceding example and run a training set of 10 points through the <kbd>perceptron_predict</kbd> function by opening up your preferred Python editor and following these steps:</p>
<div class="packt_tip">We will use Visual Studio code for most of the major coding sections later in this book. By all means, use your preferred editor, but if you are relatively new to Python, give the code a try. Code is available for Windows, macOS, and Linux.</div>
<ol>
<li class="mce-root"><span>Enter the following block of code in your preferred Python editor or open</span> <kbd>Chapter_1_2.py</kbd> <span>from the downloaded source code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">train = [[1,2],[2,3],[1,1],[2,2],[3,3],[4,2],[2,5],[5,5],[4,1],[4,4]]<br/>weights = [1,1,1]<br/><br/>def perceptron_predict(inputs, weights):<br/>    activation = weights[0]    <br/>    for i in range(len(inputs)-1):<br/>      activation += weights[i+1] * inputs[i]<br/>      return 1.0 if activation &gt;= 0.0 else 0.0<br/><br/>for inputs in train:<br/>  print(perceptron_predict(inputs,weights))</pre>
<ol start="2">
<li>This code just extends the earlier example we looked at. In this case, we are testing multiple points of data defined in the <kbd>train</kbd> list. Then we just iterate through each item in the list and print out the predicted value.</li>
<li>Run the code and observe the output. If you are unsure of how to run Python code, be sure to take that course first before going any further.</li>
</ol>
<p>You should see an output of repeating 1.0s, which essentially means all input values are recognized as the same. This is not something that is very useful. The reason for this is that we have not trained or adjusted the input weights to match a known output. What we need to do is train the weights to recognize the data, and we will look at how to do that in the next section.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a perceptron in Python</h1>
                </header>
            
            <article>
                
<p>Perfect! We created a simple perceptron that takes input and spits out output but doesn't <span>really </span>do anything. Our perceptron needs to have its weights trained in order to actually do something. Fortunately, there is a well-defined method, known as <strong>gradient descent</strong>, that we can use to adjust each of those weights. Open up your Python editor again and update or enter the following code or open <kbd>Chapter_1_3.py</kbd> from the code download:</p>
<pre class="mce-root"><span>def perceptron_predict(inputs, weights):</span><br/><span> activation = weights[0]</span><br/><span> for i in range(len(inputs)-1):</span><br/><span>  activation += weights[i + 1] * inputs[i]</span><br/><span> return 1.0 if activation &gt;= 0.0 else 0.0</span><br/><br/><span>def train_weights(train, learning_rate, epochs):</span><br/><span> weights = [0.0 for i in range(len(train[0]))]</span><br/><span> for epoch in range(epochs):</span><br/><span>  sum_error = 0.0</span><br/><span>  for inputs in train:</span><br/><span>   prediction = perceptron_predict(inputs, weights)</span><br/><span>   error = inputs[-1] - prediction</span><br/><span>   sum_error += error**2</span><br/><span>   weights[0] = weights[0] + learning_rate * error</span><br/><span>   for i in range(len(inputs)-1):</span><br/><span>    weights[i + 1] = weights[i + 1] + learning_rate * error * inputs[i]</span><br/><span>  print('&gt;epoch=%d, learning_rate=%.3f, error=%.3f' % (epoch, learning_rate, sum_error))</span><br/><span> return weights</span><br/><br/><span>train = [[1.5,2.5,0],[2.5,3.5,0],[1.0,11.0,1],[2.3,2.3,1],[3.6,3.6,1],[4.2,2.4,0],[2.4,5.4,0],[5.1,5.1,1],[4.3,1.3,0],[4.8,4.8,1]]</span><br/><br/><span>learning_rate = 0.1</span><br/><span>epochs = 10</span><br/><span>weights = train_weights(train, learning_rate, epochs)</span><br/><span>print(weights)</span></pre>
<p>The <kbd>train_weights</kbd> function is new and will be used to train the perceptron using iterative error minimization and will be a basis for when we use gradient descent in more complex networks. There is a lot going on here, so we will break it down piece by piece. First, we initialize the <kbd>weights</kbd> list to a value of <kbd>0.0</kbd> with this line:</p>
<pre class="mce-root"><span>weights = [0.0 for i in range(len(train[0]))]</span></pre>
<p>Then we start training each epoch in a <kbd>for</kbd> loop. An <strong>epoch</strong> is essentially one pass through our training data. The reason we make multiple passes is to allow our weights to converge at a global minimum and not a local one. During each epoch, the weights are trained using the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cc2e7fbe-9265-435e-bf04-279555c2ca09.png" style="width:30.25em;height:1.33em;"/></p>
<p>Consider the following:</p>
<p class="mce-root"><img class="fm-editor-equation" src="assets/6b8441cd-52a3-48a8-857d-e9280eb3ad9a.png" style="width:1.00em;height:0.83em;"/> = weight<br/>
<img class="fm-editor-equation" src="assets/e022d5c3-8424-4f64-8a6c-eec5b51c0975.png" style="width:6.08em;height:1.17em;"/> = the rate at which the perceptron learns<br/>
<img class="fm-editor-equation" src="assets/f83850ef-a8ce-4917-8535-66c37dad9d62.png" style="width:4.25em;height:1.17em;"/> = the labeled training value<br/>
<img class="fm-editor-equation" src="assets/c42999b3-45b8-4275-a516-20131ec52ec0.png" style="width:4.67em;height:1.17em;"/> = the value returned from the perceptron<br/>
<img class="fm-editor-equation" src="assets/d9d34cbd-58a9-4f2a-b685-5c0997c5c4fc.png" style="width:2.67em;height:0.75em;"/> = <img class="fm-editor-equation" src="assets/5937c8c0-55a8-459d-aa32-fe722b51b262.png" style="width:3.67em;height:1.00em;"/> - <img class="fm-editor-equation" src="assets/3ec50e08-1a20-4317-b539-4bb85fc26986.png" style="width:4.67em;height:1.17em;"/></p>
<p>The bias is trained in a similar manner, but just recall it is <kbd>weight</kbd>. Note also how we are labeling our data points now in the <kbd>train</kbd> list, with an end value of <kbd>0.0</kbd> or <kbd>1.0</kbd>. A value of <kbd>0.0</kbd> means <em>no match</em>, while a value of <kbd>1.0</kbd> means <em>perfect match</em>, as shown in the following code excerpt:</p>
<pre>train = [[1.5,2.5,0.0],[2.5,3.5,0.0],[1.0,11.0,1.0],[2.3,2.3,1.0],[3.6,3.6,1.0],[4.2,2.4,0.0],[2.4,5.4,0.0],[5.1,5.1,1.0],[4.3,1.3,0.0],[4.8,4.8,1.0]]</pre>
<p>This labeling of data is common in training neural networks and is called <strong>supervised training</strong>. We will explore other unsupervised and semi-supervised training methods in later chapters. If you run the preceding code, you will see the following output:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/2ce90bda-11c8-4219-b997-831957fcfed7.png" style="width:36.75em;height:14.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Example output from sample training run</div>
<p>Now, if you have some previous ML experience, you will immediately recognize the training wobbling going on around some local minima, making our training unable to converge. You will likely come across this type of wobble several more times in your DL career, so it is helpful to understand how to fix it.</p>
<p>In this case, our issue is likely the choice of the <kbd>activation</kbd> function, which, as you may recall, was just a simple step function. We can fix this by entering a new function, called a <strong>Rectified Linear Unit</strong> (<strong>ReLU</strong>). An example of the <kbd>step</kbd> and <kbd>ReLU</kbd> functions, side by side, are shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/d11b04f2-d5cc-4206-8b6c-05ef1427b83c.png" style="width:53.92em;height:16.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Comparison of step and ReLU activation functions</div>
<p class="mce-root"> In order to change the activation function, open up the previous code listing and follow along:</p>
<ol>
<li class="mce-root"><span>Locate the following line of code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">return 1.0 if activation &gt;= 0.0 else 0.0</pre>
<ol start="2">
<li>Modify it, like so:</li>
</ol>
<pre style="padding-left: 60px">return 1.0 if activation * (activation&gt;0) &gt;= 0.0 else 0.0</pre>
<ol start="3">
<li>That subtle difference in multiplying the activation function by itself if its value is greater than 0 is the implementation of the <kbd>ReLU</kbd> function. Yes, it is that deceptively easy.</li>
<li>Run the code and observe the change in output.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p>When you run the code, the values quickly converge and remain stable. This is a tremendous improvement in our training and a cause of changing the activation function to <kbd>ReLU</kbd>. The reason for this is that now our perceptron weights can more slowly converge to a global maximum, whereas before they just wobbled around a local minimum by using the <kbd>step</kbd> function. There are plenty of other activation functions we will test through the course of this book. In the next section, we look at how things get much more complicated when we start to combine our perceptrons into multiple layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multilayer perceptron in TF</h1>
                </header>
            
            <article>
                
<p><span>Thus far, we have been looking at a simple example of a single perceptron and how to train it. This worked well for our small dataset, but as the number of inputs increases, the complexity of our networks increases, and this cascades into the math as well. The following diagram shows a</span> multilayer<span> perceptron, or what we commonly refer to as an ANN:</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/b188eaf0-7904-4c2b-9580-efb1446135d7.png" style="width:26.08em;height:18.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Multilayer perceptron or ANN</div>
<p>In the diagram, we see a network with one input, one hidden, and one output layer. The inputs are now shared across an input layer of neurons. The first layer of neurons processes the inputs, and outputs the results to be processed by the hidden layer and so on, until they finally reach the output layer.</p>
<p>Multilayer networks can get quite complex, and the code for these models is often abstracted away by high-level interfaces such as Keras, PyTorch, and so on. These tools work well for quickly exploring network architecture and understanding DL concepts. However, when it comes to performance, which is key in games, it really requires the models to be built in TensorFlow or an API that supports low-level math operations. In this book, we will swap from Keras, a higher-level SDK, to TensorFlow and back for the introductory DL chapters. This will allow you to see the differences and similarities between working with either interface.</p>
<p>Unity ML-Agents was first prototyped with Keras but has since progressed to TensorFlow.  Most certainly, the team at Unity, as well as others, has done this for reasons of performance and, to some extent, control. Working with TensorFlow is akin to writing your own shaders. While it is quite difficult to write shaders and TF code, the ability to customize your own rendering and now learning will make your game be unique, and it will stand out.</p>
<p>There is a great TensorFlow example of a multilayer perceptron next for your reference, listing <kbd>Chapter_1_4.py</kbd>. In order to run this code using TensorFlow, follow the next steps:</p>
<div class="packt_infobox">We won't cover the basics of TensorFlow until the next section. This is so you can see TF in action first before we bore you with the details.</div>
<ol start="1">
<li>First, install TensorFlow using the following command from a Python 3.5/3.6 window on Windows or macOS. You can also use an Anaconda Prompt, with administrator rights:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>pip install tensorflow</strong> <br/>OR <br/><strong>conda install tensorflow</strong>    //using Anaconda</pre>
<ol start="2">
<li>Make sure you install TensorFlow suited to the default Python environment. We will worry about creating more structured virtual environments later. If you are not sure what a Python virtual environment is, step away from the book and take a course in Python right away.</li>
</ol>
<div class="packt_infobox">In this exercise, we are loading the <strong>MNIST</strong> handwritten digits database. If you have read anything at all about ML and DL, you have most likely seen or heard about this dataset already. If you haven't, just quickly Google <em>MNIST</em> to get a sense of what these digits look like.</div>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>The following Python code is from the <kbd>Chapter_1_4.py</kbd> listing, with each section explained in the following steps:</li>
</ol>
<pre style="padding-left: 60px">from tensorflow.examples.tutorials.mnist import input_data<br/>mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)</pre>
<ol start="4">
<li>We start by loading the <kbd>mnist</kbd> training set. The <kbd>mnist</kbd> dataset is a collection of 28 x 28 pixel images showing hand-drawn representations of the digits 0-9, or what we will refer to as 10 classes:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">import tensorflow as tf<br/># Parameters<br/>learning_rate = 0.001<br/>training_epochs = 15<br/>batch_size = 100<br/>display_step = 1<br/># Network Parameters<br/>n_hidden_1 = 256 # 1st layer number of neurons<br/>n_hidden_2 = 256 # 2nd layer number of neurons<br/>n_input = 784 # MNIST data input (img shape: 28*28)<br/>n_classes = 10 # MNIST total classes (0-9 digits)</pre>
<ol start="5">
<li class="mce-root"><span>Then we import the</span> <kbd>tensorflow</kbd> <span>library as</span> <kbd>tf</kbd><span>. Next, we set a number of parameters we will use later. Note how we are defining the inputs and hidden parameters as well:</span></li>
</ol>
<pre class="mce-root" style="color: black;padding-left: 60px"># tf Graph input<br/>X = tf.placeholder("float", [None, n_input])<br/>Y = tf.placeholder("float", [None, n_classes])<br/><br/># Store layers weight &amp; bias<br/>weights = {<br/> 'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),<br/> 'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),<br/> 'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))<br/>}<br/>biases = {<br/> 'b1': tf.Variable(tf.random_normal([n_hidden_1])),<br/> 'b2': tf.Variable(tf.random_normal([n_hidden_2])),<br/> 'out': tf.Variable(tf.random_normal([n_classes]))<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="6">
<li class="mce-root"><span>Next, we set up a couple of TensorFlow placeholders with</span> <kbd>tf.placeholder</kbd><span>, to hold the number of inputs and classes as type <kbd>'float'</kbd>. Then we create and initialize variables using</span> <kbd>tf.Variable</kbd>, f<span>irst doing the weights and then the biases. Inside the variable declaration, we initialize normally distributed data into a 2D matrix or tensor with dimensions equal to <kbd>n_input</kbd> and <kbd>n_hidden_1</kbd> using <kbd>tf.random_normal</kbd>, which fills a tensor with randomly distributed data:</span></li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># Create model<br/>def multilayer_perceptron(x):<br/> # Hidden fully connected layer with 256 neurons<br/> layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])<br/> # Hidden fully connected layer with 256 neurons<br/> layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])<br/> # Output fully connected layer with a neuron for each class<br/> out_layer = tf.matmul(layer_2, weights['out']) + biases['out']<br/> return out_layer<br/><br/># Construct model<br/>logits = multilayer_perceptron(X)</pre>
<ol start="7">
<li>Then we create the model by multiplying the weights and biases for each layer operation. What we are doing here is essentially converting our activation equation into a matrix/tensor of equations. Now instead of doing a single pass, we perform multiple passes in one operation using matrix/tensor multiplication. This allows us to run multiple training images or sets of data at a time, which is a technique we use to better generalize learning.</li>
</ol>
<p style="padding-left: 60px">For each layer in our neural network, we use <kbd>tf.add</kbd> and <kbd>tf.matmul</kbd> to add matrix multiplication operations to what we commonly call a <strong>TensorFlow inference graph</strong>. You can see by the code we are creating that there are two hidden layers and one output layer for our model:</p>
<pre class="mce-root" style="padding-left: 60px"># Define loss and optimizer<br/>loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))<br/>optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)<br/>train_op = optimizer.minimize(loss_op)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="8">
<li>Next, we define a <kbd>loss</kbd> function and optimizer. <kbd>loss_op</kbd> is used to calculate the total loss of the network. Then <kbd>AdamOptimizer</kbd> is what does the optimizing according to the <kbd>loss</kbd> or <kbd>cost</kbd> function. We will explain these terms in detail later, so don't worry if things are still fuzzy:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># Initializing the variables<br/>init = tf.global_variables_initializer()<br/>with tf.Session() as sess:<br/> sess.run(init)<br/> # Training cycle<br/> for epoch in range(training_epochs):<br/>   avg_cost = 0.<br/>   total_batch = int(mnist.train.num_examples/batch_size)<br/>   # Loop over all batches<br/>   for i in range(total_batch):<br/>     batch_x, batch_y = mnist.train.next_batch(batch_size)<br/>     # Run optimization op (backprop) and cost op (to get loss value)<br/>     _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,Y: batch_y})<br/>     # Compute average loss<br/>     avg_cost += c / total_batch</pre>
<ol start="9">
<li>Then we initialize a new TensorFlow session by creating a new session and running it. We use that epoch iterative training method again to loop over each batch of images. Remember, an entire batch of images goes through the network at the same time, not just one image. Then, we loop through each batch of images in each epoch and optimize (backpropagate and train) the cost, or minimize the cost if you will:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># Display logs per epoch step<br/> if epoch % display_step == 0:<br/> print("Epoch:", '%04d' % (epoch+1), "cost={:.9f}".format(avg_cost))<br/> print("Optimization Finished!")</pre>
<ol start="10">
<li>Then we output the results of each epoch run, showing how the network is minimizing the error:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># Test model<br/> pred = tf.nn.softmax(logits) # Apply softmax to logits<br/> correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))</pre>
<p class="mce-root"/>
<ol start="11">
<li>Next, we actually run the prediction with the preceding code and determine the percentage of correct values using the optimizer we selected before on the <kbd>logits</kbd> model:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># Calculate accuracy<br/> accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))<br/> print("Accuracy:", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))</pre>
<ol start="12">
<li>Finally, we calculate and output the <kbd>accuracy</kbd> of our model. If you run the exercise, don't just go into how accurate the model is but think of ways the accuracy could be improved.</li>
</ol>
<p>There is plenty going on in the preceding reference example, and we will break it down further in the next sections. Hopefully, you can see at this point how complex things can get. This is why for most of the fundamental chapters in this book, we will teach the concepts with Keras first. Keras is a powerful and simple framework that will help us build complex networks in no time and makes it much simpler for us to teach and for you to learn. We will also provide duplicate examples developed in TensorFlow and show some of the key differences as we progress through the book. </p>
<p>In the next section, we explain the basic concepts of TensorFlow, what it is, and how we use it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow Basics</h1>
                </header>
            
            <article>
                
<p><strong>TensorFlow</strong> (<strong>TF</strong>) is quickly becoming the technology that powers many DL applications. There are other APIs, such as Theano, but it is the one that has gathered the greatest interest and mostly applies to us. Overarching frameworks, such as Keras, offer the ability to deploy TF or Theano models, for instance. This is great for prototyping and building a quick proof of concept, but, as a game developer, you know that when it comes to games, the dominant requirements are always performance and control. TF provides better performance and more control than any higher-level framework such as Keras. In other words, to be a serious DL developer, you likely need and want to learn TF.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>TF, as its name suggests, is all about tensors. A tensor is a mathematical concept that describes a set of data organized in <em>n</em> dimensions, where <em>n</em> could be 1, 2 x 2, 4 x 4 x 4, and so on. A one-dimensional tensor would describe a single number, say <img class="fm-editor-equation" src="assets/6fa7dde2-1b00-44af-8f4c-041e252a6be9.png" style="width:1.75em;height:1.00em;"/>, a 2 x 2 tensor would be<img class="fm-editor-equation" src="assets/7077b58d-f2f0-4f9e-843e-fe60c0e44cfe.png" style="width:4.50em;height:2.42em;"/>or what you may refer to as a matrix. A 3 x 3 x 3 tensor would describe a cube shape. Essentially, any operation that you would apply on a matrix can be applied to a tensor and everything in TF is a tensor. It is often helpful when you first start working with tensors, as someone with a game development background, to think of them as a matrix or vector.<br/>
<br/>
<span>Tensors are nothing more than multidimensional arrays, vectors, or matrices, and many examples are shown in the following diagram:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/971d6c2e-1fa2-4e56-8d3b-7410e1ab3118.png" style="width:43.08em;height:28.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Tensor in many forms (placeholder)</div>
<p>Let's go back and open up <kbd>Chapter_1_4.py</kbd> and follow the next steps in order to better understand how the TF example runs:</p>
<ol>
<li class="mce-root"><span>First, examine the top section again and pay special attention to where the placeholder and variable is declared; this is shown again in the following snippet:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">tf.placeholder("float", [None, n_input])<br/>...<br/>tf.Variable(tf.random_normal([n_input, n_hidden_1]))</pre>
<ol start="2">
<li>The <kbd>placeholder</kbd> is used to define the input and output tensors. <kbd>Variable</kbd> sets up a variable tensor that can be manipulated while the TF session or program executes. In the case of the example, a helper method called <kbd>random_normal</kbd> populates the hidden weights with a normally distributed dataset. There are other helper methods such as this that can be used; check the docs for more info.</li>
<li class="mce-root"><span>Next, we construct the</span> <kbd>logits</kbd> model <span>as a function call</span><span>ed</span> <kbd>multilayer_perceptron</kbd>, <span>as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">def multilayer_perceptron(x):<br/>  layer_1 = <strong>tf.add</strong>(<strong>tf.matmul</strong>(x, weights['h1']), biases['b1'])<br/>  layer_2 = <strong>tf.add</strong>(<strong>tf.matmul</strong>(layer_1, weights['h2']), biases['b2'])<br/>  out_layer = <strong>tf.matmul</strong>(layer_2, weights['out']) <strong>+</strong> biases['out']<br/>  return out_layer<br/><br/>logits = multilayer_perceptron(X)</pre>
<ol start="4">
<li>Inside the function, we see the definition of three network layers, two input and one output. Each layer is constructed by using the add or <kbd>+</kbd> function to add the results of the <kbd>matmul (x, weights['h1'])</kbd> and the <kbd>biases['b1']</kbd>. <kbd>Matmul</kbd> does a simple matrix multiplication of each weight times the input <em>x</em>. Think back to our first example perceptron; this is the same as multiplying all our weights by the input and then adding the bias. Note how the resultant tensors <kbd>(layer_1, layer_2)</kbd> are used as inputs into the following layer.</li>
<li>Skip down to around line 50 and note how we grab references to the <kbd>loss</kbd>, <kbd>optimizer</kbd>, and <kbd>initialization</kbd> functions:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>loss_op</strong> = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))<br/><strong>optimizer</strong> = tf.train.AdamOptimizer(learning_rate=learning_rate)<br/><strong>train_op</strong> = optimizer.minimize(loss_op)<br/><strong>init</strong> = tf.global_variables_initializer()</pre>
<p class="mce-root"/>
<ol start="6">
<li>It is important to understand that we are storing references to the functions and not executing them just yet. The loss and optimizer functions have been covered in some depth already, but also pay special attention to the <kbd>global_variables_initalizer()</kbd> function. This function is where all the variables are initialized, and we are required to run this function first.</li>
<li>Next, scroll down to the start of the session initialization and start, as follows:</li>
</ol>
<pre style="padding-left: 60px">with tf.<strong>Session</strong>() as sess:<br/>  sess.<strong>run</strong>(<strong>init</strong>)</pre>
<ol start="8">
<li>We construct <kbd>Session</kbd> in TF as a container of execution or what is called a graph. This is a mathematical graph that describes nodes and connections, not that unlike the networks we are simulating. Everything in TF needs to happen within a session. Then we run the first function, <kbd>(init)</kbd>, with <kbd>run</kbd>.</li>
<li>As we have already covered the training in some detail, the next element we will look at is the next function, <kbd>run</kbd>, executed by the following code:</li>
</ol>
<pre style="color: black;padding-left: 60px">_, c = sess.<strong>run</strong>([<strong>train_op</strong>, <strong>loss_op</strong>], feed_dict={X: batch_x,Y: batch_y})</pre>
<ol start="10">
<li>A lot is going on in the <kbd>run</kbd> function. We input as a set the training and loss functions <kbd>train_op</kbd> and <kbd>loss_op</kbd> using the current <kbd>feed_dict</kbd> dictionary as input. The resultant output value, <kbd>c</kbd>, is equal to the total cost. Note that the input function set is defined as <kbd>train_op</kbd> then <kbd>loss_op</kbd>. In this case, the order is defined as <kbd>train</kbd>/<kbd>loss</kbd>, but it could be also reversed if you choose. You would also need to reverse the output values as well, since the output order matches the input order.</li>
</ol>
<p>The rest of the code has already been defined in some detail, but it is important to understand some of the key differences when building your models with TF. As you can see, it is relatively easy for us to now build complex neural networks quickly. Yet, we are still missing some critical knowledge that will be useful in constructing more complex networks later. What we have been missing is the underlying math used to train a neural network, which we will explore in the next section.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training neural networks with backpropagation</h1>
                </header>
            
            <article>
                
<p>Calculating the activation of a neuron, the forward part, or what we call <strong>feed-forward propagation</strong>, is quite straightforward to process. The complexity we encounter now is training the errors back through the network. When we train the network now, we start at the last output layer and determine the total error, just as we did with a single perceptron, but now we need to sum up all errors across the output layer. Then we need to use this value to backpropagate the error back through the network, updating each of the weights based on their contribution to the total error. Understanding the contribution of a single weight in a network with thousands or millions of weights could be quite complicated, except thankfully for the help of differentiation and the chain rule. Before we get to the complicated math, we first need to discuss the <kbd>Cost</kbd> function and how we calculate errors in the next section.</p>
<div class="packt_tip">While the math of backpropagation is complicated and may be intimidating, at some point, you will want or need to understand it well. However, for the purposes of this book, you can omit or just revisit this section as needed. All the networks we develop in later chapters will automatically handle backpropagation for us. Of course, you can't run away from the math either; it is everywhere in deep learning.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Cost function</h1>
                </header>
            
            <article>
                
<p>A <kbd>Cost</kbd> function describes the average sum of errors for a batch in our entire network and is often defined by this equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/17490b69-202e-4ae1-bdb0-45b96d921c7f.png" style="width:13.08em;height:1.42em;"/></p>
<p>The input is defined as each weight and the output is the total average cost we encountered over the processed batch. Think of this cost as the average sum of errors. Now, our goal here is to minimize this function or the cost of errors to the lowest value possible. In the previous couple of examples, we have seen a technique called <strong>gradient descent</strong> being used to minimize this cost function. Gradient descent works by differentiating the <kbd>Cost</kbd> function and determining the gradient with respect to each weight. Then, for each weight, or dimension if you will, the algorithm alters the weight based on the calculated gradient that minimizes the <kbd>Cost</kbd> function.</p>
<p>Before we get into the heavy math that explains the differentiation, let's see how gradient descent works in two dimensions, with the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/c32e3474-a1e4-4d14-9c54-0c9cb5b6d698.png" style="width:43.17em;height:26.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Example of gradient descent finding a global minimum</div>
<p>In simpler terms, all that the algorithm is doing is just trying to find the minimum in slow gradual steps. We use small steps in order to avoid overshooting the minimum, which as you have seen earlier can happen (remember the wobble). That is where the term <strong>learning rate</strong> also comes in, which determines how fast we want to train. The slower the training, the more confident you will be in your results, but usually at a cost of time. The alternative is to train quicker, using a higher learning rate, but, as you can see now, it may be easy to overshoot any global minimum.</p>
<p>Gradient descent is the simplest form we will talk about, but keep in mind that there are also several advanced variations of other optimization algorithms we will explore. In the TF example, for instance, we used <kbd>AdamOptimizer</kbd> to minimize the <kbd>Cost</kbd> function, but there are several other variations. For now, though, we will focus on how to calculate the gradient of the <kbd>Cost</kbd> function and understand the basics of backpropagation with gradient descent in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Partial differentiation and the chain rule</h1>
                </header>
            
            <article>
                
<p>Before we get into the details of calculating each weight, let's review a little bit of calculus and differentiation. If you recall your favorite math class, calculus, you can determine the slope of change for any point on a function by differentiating. A calculus refresher is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/9c72438b-cc34-4723-9e40-cee35840839a.png" style="width:29.25em;height:29.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">A review of basic calculus equations</div>
<p>In the diagram, we have a nonlinear function, <strong>f</strong>, that describes the equation of the blue line. We can determine the slope (rate of change) on any point by differentiating to <span class="packt_screen">f'</span> and solving. Recall that we can also determine the functions of local and global minimum or maximum using this new function and as shown in the diagram. Simple differentiation allows us to solve for one variable, but we need to solve for multiple weights, so we will use partial derivatives or differentiating with respect to one variable.</p>
<p class="mce-root"/>
<p>As you may recall, partial differentiation allows us to derive for a single variable with respect to the other variables, which we then treat as constants. Let's go back to our <kbd>Cost</kbd> function and see how to differentiate that with respect to a single weight:</p>
<ol>
<li><em><img class="fm-editor-equation" src="assets/540eed90-acc5-4e90-9e5f-5a4a2da7a90f.png" style="width:1.00em;height:1.25em;"/></em> is our cost function described by the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/de02ffdb-0412-45e9-bdc9-d1b72f38f618.png" style="width:15.25em;height:1.33em;"/></p>
<ol start="2">
<li>We can differentiate this function with respect to a single variable weight as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/80566a95-aafa-43b8-ba8a-9bc0d258169e.png" style="width:7.50em;height:2.75em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cc8b62c3-a3dd-4c94-9ff8-fc11d9a2a622.png" style="width:7.08em;height:2.58em;"/></p>
<ol start="3">
<li>If we collect all of these partial derivatives together, we get the vector gradient for our <kbd>Cost</kbd> function, <span><img class="fm-editor-equation" src="assets/0fb9abc8-223f-4fa3-96a3-b025e13774ac.png" style="width:1.00em;height:1.25em;"/></span>,<span> </span>denoted by the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/50074553-fd8d-432d-9d74-20562d73b5d4.png" style="width:8.75em;height:11.00em;"/></p>
<ol start="4">
<li>This gradient defines a vector direction that we want to negate and use to minimize the <kbd>Cost</kbd> function. In the case of our previous example, there are over 13,000 components to this vector. These correspond to over 13,000 weights in the network that we need to optimize. That is a lot of partial derivatives we need to combine in order to calculate the gradient. Fortunately, the chain rule in calculus can come to our rescue and greatly simplify the math. Recall that the chain rule is defined by the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/405bae41-bd8e-43fa-b46c-1be1b4d0657d.png" style="width:8.00em;height:2.83em;"/></p>
<ol start="5">
<li>This now allows us to define the gradient for a single weight using the chain rule as such:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/95d4ef61-83e7-4680-b338-2637ad32a9df.png" style="width:6.92em;height:3.00em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/237b0114-cc6f-4cb2-b9ca-f527d882243a.png" style="width:14.33em;height:2.92em;"/></p>
<ol start="6">
<li><span>Here,<span> <img class="fm-editor-equation" src="assets/53d1990c-21bb-4d13-97a4-d69c40d87558.png" style="width:0.58em;height:1.50em;"/> </span>represents the input number and <img class="fm-editor-equation" src="assets/d7bf0016-5adf-4f73-88c6-a93ebacf1caa.png" style="width:0.58em;height:1.42em;"/><span> </span>the neuron position. Note how we now need to take the partial derivative of the activation function,</span><span> </span><em>a</em>,<span> </span><span>for the given neuron, and that is again summarized by the following:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4ed6a86d-bbe9-435e-8490-8e05e0d56102.png" style="width:10.42em;height:4.00em;"/></p>
<p style="padding-left: 60px"><span>The superscript notation <img class="fm-editor-equation" src="assets/674a1fc1-17c6-4627-a679-e4a020bc511c.png" style="width:1.17em;height:1.25em;"/> </span><span>denotes the current layer and <span><img class="fm-editor-equation" src="assets/7768eb39-9ec9-40af-b61a-bd226e3544ca.png" style="width:2.58em;height:1.08em;"/></span></span><span><span> </span>denotes the previous layer. <span><img class="fm-editor-equation" src="assets/b73cb21f-571d-4923-a76b-f6227090187d.png" style="width:0.75em;height:0.92em;"/></span></span><span><span> </span>denotes either the input or the output from the previous layer. <span><img class="fm-editor-equation" src="assets/5b84bf4a-5db8-43b0-bcb0-d8a9531484ee.png" style="width:0.92em;height:0.92em;"/> </span>denotes the activation function, recall that we previously used the <kbd>Step</kbd> and <kbd>ReLU</kbd> functions for this role.</span></p>
<ol start="7">
<li>Then, we take the partial derivative of this function, like so:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2b7fef57-c426-4338-b802-fe934fdf278a.png" style="width:16.75em;height:3.75em;"/></p>
<p style="padding-left: 60px">For convenience, we define the following<span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e26b112d-5a95-468a-9e28-291bcd88df10.png" style="width:6.33em;height:3.83em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="8">
<li>At this point, things may look a lot more complicated than they are. Try to understand all the subtleties of the notation and remember all we are looking at is essentially the partial derivative of the activation with respect to the <kbd>Cost</kbd> function. All that the extra notation does is allow us to index the individual weight, neuron, and layer. We can then express this as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/700a7c49-24b2-4c81-81ac-1660a520c017.png" style="width:12.67em;height:3.42em;"/></p>
<ol start="9">
<li>Again, all we are doing is defining the gradient (<img class="fm-editor-equation" src="assets/c7be7304-2015-4fb0-bce6-d80275794496.png" style="width:1.25em;height:1.42em;"/>) for the weight at the <img class="fm-editor-equation" src="assets/625ab8a1-2e70-4937-95b8-1ac620c209c5.png" style="width:0.50em;height:1.33em;"/><sup>th</sup> input, <span><img class="fm-editor-equation" src="assets/aa2eb4de-35af-441a-b21a-eedb4d796372.png" style="width:0.50em;height:1.17em;"/></span><sup>th</sup> neuron, and layer <span><img class="fm-editor-equation" src="assets/3fb99529-b043-4057-8400-e4d5924454df.png" style="width:0.58em;height:1.00em;"/></span>. Along with gradient descent, we need to backpropagate the adjustment to the weights using the preceding base formula. For the output layer (last layer), this now can be summarized as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a7244943-32ae-4e8c-8a70-5bdef000f42c.png" style="width:23.75em;height:3.25em;"/></p>
<ol start="10">
<li>For an internal or a hidden layer, the equation comes out to this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/018674a5-8d61-42fd-a036-427bf65bab66.png" style="width:17.42em;height:3.92em;"/></p>
<ol start="11">
<li>And with a few more substitutions and manipulations of the general equation, we end up with this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6318bfc0-51f2-486e-ab6e-c87bc93d5e67.png" style="width:33.33em;height:3.75em;"/></p>
<p style="padding-left: 60px"><span>Here, </span><em>f'</em><span> </span>denotes the derivative of the activation function.</p>
<p>The preceding equation allows us to run the network and backpropagate the errors back through, using the following procedure:</p>
<ol>
<li><span>You first calculate the activations </span><img class="fm-editor-equation" src="assets/12a18170-2428-4904-bbc5-71fcfc00cffd.png" style="width:1.17em;height:1.08em;"/> <span>and </span><img class="fm-editor-equation" src="assets/c475dd78-297d-41ea-99f8-582ae07a5009.png" style="width:1.17em;height:1.17em;"/> <span>for each layer starting with the input layer and propagate forward.</span></li>
<li><span>We then evaluate the term </span><img class="fm-editor-equation" src="assets/4cad7f38-ff20-4529-a65a-046d1ca4dd6d.png" style="width:1.50em;height:1.92em;"/><span>at the output layer using </span><span><img class="fm-editor-equation" src="assets/abb13af1-a57e-435b-95b2-e8bd980a33ab.png" style="width:3.25em;height:1.25em;"/>.</span></li>
</ol>
<ol start="3">
<li><span>We d</span><span>o this by using the remainder to ev</span><span>aluate each lay</span><span>er using</span><span> <img class="fm-editor-equation" src="assets/3a68e2e9-7e67-4bdc-9919-75611c86fe29.png" style="width:12.58em;height:2.50em;"/>,</span><span> </span><span>starting with the output layer and propagating backward.</span></li>
<li><span>Again, we are using the partial derivative </span><span><img class="fm-editor-equation" src="assets/3fcd1213-41e8-4d5d-b7b9-2a737e0b210c.png" style="width:8.00em;height:3.25em;"/></span><span> </span><span>to obtain the required derivatives in each layer.</span></li>
</ol>
<p style="color: black">It may take you a few reads through this section in order to grasp all the concepts. What can also be useful is to run the previous examples and watch the training, trying to imagine how each of the weights is getting updated. We are by no means completely done here, and there are a couple more steps—using automatic differentiation being one of them. Unless you are developing your own low-level networks, just having a basic understanding of that math should give you a better understanding of the needs in training a neural network. In the next section, we get back to some more hands-on basics and put our new knowledge to use by building a neural network agent.</p>
<div class="packt_tip">Learning does not and likely should not all come from the same source. Be sure to diversify your learning to other books, videos, and courses. You will not only be more successful in learning but likely also understand more in the process.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an autoencoder with Keras</h1>
                </header>
            
            <article>
                
<p>While we have covered a lot of important ground we will need for understanding DL, what we haven't done yet is build something that can really do anything. One of the first problems we tackle when starting with DL is to build autoencoders to encode and reform data. Working through this exercise allows us to confirm that what goes into a network can also come back out of a network and essentially reassures us that an ANN is not a complete black box. Building and working with autoencoders also allows us to tweak and test various parameters in order to understand their function. Let's get started by opening up the <kbd>Chapter_1_5.py</kbd> listing and following these steps:</p>
<ol>
<li><span>We will go through the listing section by section. First, we input the base layers </span><kbd>Input</kbd><span> and </span><kbd>Dense</kbd><span>, then </span><kbd>Model</kbd>,<span> all from the <kbd>tensorflow.keras</kbd> module, with the following imports:</span></li>
</ol>
<pre style="padding-left: 60px">from tensorflow.keras.layers import Input, Dense<br/>from tensorflow.keras.models import Model</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Instead of single neurons, we define our DL model in Keras using layers or neurons. The <kbd>Input</kbd> and <kbd>Dense</kbd> layers are the most common ones we use, but we will see others as well. As their name suggests, <kbd>Input</kbd> layers deal with input, while <kbd>Dense</kbd> layers are more or less your typical fully connected neuron layer, which we have already looked at.</li>
</ol>
<div class="packt_infobox"><span>We are using the embedded version of Keras here. The original sample was taken from the Keras blog and converted to TensorFlow.</span></div>
<ol start="3">
<li><span>Next, we set the number of </span><kbd>encoding</kbd><span> dimensions with the following line:</span></li>
</ol>
<pre style="padding-left: 60px">encoding_dim = 32</pre>
<ol start="4">
<li><span>This is the number of dimensions we want to reduce our sample down to. In this case, it is just 32, which is just around 24 times the compression for an image with 784 input dimensions. Remember, we get </span><kbd>784</kbd><span> input dimensions because our input images are 28 x 28, and we flatten them to a vector of length 784, with each pixel representing a single value or dimension. Next, we set up the </span><kbd>Input</kbd><span> layer with the 784 input dimensions with the following:</span></li>
</ol>
<pre style="padding-left: 60px">input_img = Input(shape=(784,))</pre>
<ol start="5">
<li><span>That line creates an <kbd>Input</kbd> layer with a shape of 784 inputs. Then we are going to encode those 784 dimensions into our next </span><kbd>Dense</kbd><span> layer using the following line:</span></li>
</ol>
<pre style="padding-left: 60px">encoded = Dense(encoding_dim, activation='ReLU')(input_img)<br/>encoder = Model(input_img, encoded)</pre>
<ol start="6">
<li>The preceding code simply creates our fully connected hidden (<kbd>Dense</kbd>) layer of 32 (<kbd>encoding_dim</kbd>) neurons and<span> </span>builds the encoder. You can see that the<span> </span><kbd>input_img</kbd>, the<span> </span><kbd>Input</kbd><span> </span>layer, is used as input and our activation function is<span> </span><kbd>ReLU</kbd>. The next line constructs a<span> </span><kbd>Model</kbd><span> </span>using the <kbd>Input</kbd> layer (<kbd>input_img</kbd>) and the <kbd>Dense</kbd> (<kbd>encoded</kbd>) layer. With two layers, we encode the image from 784 dimensions to 32.</li>
</ol>
<ol start="7">
<li>Next, we need to decode the image using more layers with the following code:</li>
</ol>
<pre style="padding-left: 60px">decoded = Dense(784, activation='sigmoid')(encoded)<br/>autoencoder = Model(input_img, decoded)<br/>encoded_input = Input(shape=(encoding_dim,))<br/><br/>decoder_layer = autoencoder.layers[-1]<br/>decoder = Model(encoded_input, decoder_layer(encoded_input))<br/><br/>autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')</pre>
<ol start="8">
<li>The next set of layers and model we build will be used to decode the images back to 784 dimensions. The last line of code at the bottom is where we compile the<span> </span><kbd>autoencoder</kbd><span> </span>model with an<span> </span><kbd>adadelta</kbd><span> optimizer call, </span>using a <kbd>loss</kbd> function of<span> </span><kbd>binary_crossentropy</kbd>. We will spend more time on the types of loss and optimization parameters later, but for now just note that when we compile a model, we are in essence just setting it up to do backpropagation and use an optimization algorithm. Remember, all of this is automatically done for us, and we don't have to deal with any of that nasty math.</li>
</ol>
<ol>
<li style="list-style-type: none">
<ol start="4"/>
</li>
</ol>
<p>That sets up the main parts of our models, the encoder, decoder, and full autoencoder model, which we further compiled for later training. In the next section, we deal with training the model and making predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the model</h1>
                </header>
            
            <article>
                
<p>Next, we need to train our model with a sample set of data. We will again be using the MNIST set of handwritten digits; this is easy, free, and convenient. Get back into the code listing and continue the exercise as follows:</p>
<ol start="1">
<li>Pick up where we left off and locate the following section of code:</li>
</ol>
<pre style="padding-left: 60px">from tensorflow.keras.datasets import mnist<br/>import numpy as np<br/>(x_train, _), (x_test, _) = mnist.load_data()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>We start by importing the <kbd>mnist</kbd> library and <kbd>numpy</kbd> then loads the data into <kbd>x_train</kbd> and <kbd>x_test</kbd> sets of data. As a general rule in data science and machine learning, you typically want a training set for learning and then an evaluation set for testing. These datasets are often generated by randomly splitting the data into <kbd>80</kbd> percent for training and <kbd>20</kbd> percent for testing.</li>
<li class="mce-root"><span>Then we further define our training and testing inputs with the following code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">x_train = x_train.astype('float32') / 255.<br/>x_test = x_test.astype('float32') / 255.<br/>x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))<br/>x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))<br/>print( x_train.shape)<br/>print( x_test.shape)</pre>
<ol start="4">
<li>The first two lines are normalizing our input gray scale pixel color values and a number from <kbd>0</kbd> to <kbd>255</kbd>, by dividing by <kbd>255</kbd>. This gives us a number from <kbd>0</kbd> to <kbd>1</kbd>. We generally want to try to normalize our inputs. Next, we reshape the training and testing sets into an input <kbd>Tensor</kbd>. </li>
<li>With the models all built and compiled, it is time to start training. The next few lines are where the network will learn how to encode and decode the images:</li>
</ol>
<pre style="color: black;padding-left: 60px">autoencoder.fit(x_train, x_train, epochs=50, batch_size=256,<br/> shuffle=True, validation_data=(x_test, x_test))<br/><br/>encoded_imgs = encoder.predict(x_test)<br/>decoded_imgs = decoder.predict(encoded_imgs)</pre>
<ol start="6">
<li>You can see in our code that we are setting up to fit the data using <kbd>x_train</kbd> as input and output. We are using <kbd>50</kbd> <kbd>epochs</kbd> with a <kbd>batch size</kbd> of <kbd>256</kbd> images. Feel free to play with these parameters on your own later to see what effect they have on training. After that, the <kbd>encoder</kbd> and then the <kbd>decoder</kbd> models are used to predict test images.</li>
</ol>
<p>That completes the model and training setup we need for this model, or models if you will. Remember, we are taking a 28 x 28 image, decompressing it to essentially 32 numbers, and then rebuilding the image using a neural network. With our model complete and trained this time, we want to review the output and we will do that in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Examining the output</h1>
                </header>
            
            <article>
                
<p>Our final step this time around will be to see what is actually happening with the images. We will finish this exercise by outputting a small sample of images in order to get our success rate. Follow along in the next exercise in order to finish the code and run the autoencoder:</p>
<ol start="1">
<li>Continuing from the last exercise, locate the following last section of code:</li>
</ol>
<pre style="padding-left: 60px">import matplotlib.pyplot as plt<br/>n = 10 # how many digits we will display<br/>plt.figure(figsize=(20, 4))<br/>for i in range(n):<br/> # display original<br/> ax = plt.subplot(2, n, i + 1)<br/> plt.imshow(x_test[i].reshape(28, 28))<br/> plt.gray()<br/> ax.get_xaxis().set_visible(False)<br/> ax.get_yaxis().set_visible(False)<br/><br/># display reconstruction<br/> ax = plt.subplot(2, n, i + 1 + n)<br/> plt.imshow(decoded_imgs[i].reshape(28, 28))<br/> plt.gray()<br/> ax.get_xaxis().set_visible(False)<br/> ax.get_yaxis().set_visible(False)<br/>plt.show()</pre>
<ol start="2">
<li>In this section of code, we are just outputting the input and resultant auto-encoded images after all the training is done. This section of code starts with importing <kbd>mathplotlib</kbd> for plotting, and then we loop through a number of images to display the results. The rest of the code just outputs the images.</li>
<li>Run the Python code as you normally would, and this time expect the training to take several minutes. After everything is done, you should see an image similar to the following:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/57c7b3de-945e-42fb-af0a-cf5258f7f7f5.png" style="width:25.33em;height:9.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Example of raw input images compared to encoded and decoded output images</span></span></div>
<p>That completes our look into building a simple Keras model that can encode and then decode images. This allows us to see how each small piece of a multilayer neural network is written in Keras functions. In the final section, we invite you, the reader, to undertake some additional exercises for further learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>Use these additional exercises to assist in your learning and test your knowledge <span>further</span>.  </p>
<p>Answer the following questions:</p>
<ol>
<li>Name three different activation functions. Remember, Google is your friend.</li>
<li>What is the purpose of a bias?</li>
<li>What would you expect to happen if you reduced the number of epochs in one of the chapter samples? Did you try it?</li>
<li>What is the purpose of backpropagation?</li>
<li>Explain the purpose of the Cost function.</li>
<li>What happens when you increase or decrease the number of encoding dimensions in the Keras autoencoder example?</li>
<li>What is the name of the layer type that we feed input into?</li>
<li>What happens when you increase or decrease the batch size?</li>
<li>What is the shape of the input <kbd>Tensor</kbd> for the Keras example? Hint: we already have a print statement displaying this.</li>
<li>In the last exercise, how many MNIST samples do we train and test with?</li>
</ol>
<p>As we progress in the book, the additional exercises will certainly become more difficult. For now, though, take some time to answer the questions and test your knowledge.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we explored the foundations of DL from the basics of the simple single perceptron to more complex multilayer perceptron models. We started with the past, present, and future of DL and, from there, we built a basic reference implementation of a single perceptron so that we could understand the raw simplicity of DL. Then we built on our knowledge by adding more perceptrons into a multiple layer implementation using TF. Using TF allowed us to see how a raw internal model is represented and trained with a much more complex dataset, MNIST. Then we took a long journey through the math, and although a lot of the complex math was abstracted away from us with Keras, we took an in-depth look at how gradient descent and backpropagation work. Finally, we finished off the chapter with another reference implementation from Keras that featured an autoencoder. Auto encoding allows us to train a network with multiple purposes and extends our understanding of how network architecture doesn't have to be linear.</p>
<p>For the next chapter, we will build on our current level of knowledge and discover <strong>convolutional</strong> and <strong>recurrent</strong> neural networks. These extensions provide additional capabilities to the base form of a neural network and have played a significant part in our most recent DL advances.</p>
<p>For the next chapter, we will begin our journey into building components for games when we look at another element considered foundational to DL—the GAN. GANs are like a Swiss Army knife in DL and, as we will see in the next chapter, they offer us plenty of uses.</p>


            </article>

            
        </section>
    </body></html>