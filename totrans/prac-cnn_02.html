<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p><span><strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>) are everywhere. In the last five years, we have seen a dramatic rise in the performance of visual recognition systems due to the introduction of deep architectures for feature learning and classification. CNNs have achieved good performance in a variety of areas, such as automatic speech understanding, computer vision, language translation, self-driving cars, and games such as Alpha Go. Thus, the applications of CNNs are almost limitless. DeepMind (from Google) recently published WaveNet, which uses a CNN </span><span>to generate speech that mimics any human voice </span><span>(</span><a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a><span>).</span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li class="mce-root">History of CNNs</li>
<li>Overview of a CNN</li>
<li>Image augmentation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">History of CNNs</h1>
                </header>
            
            <article>
                
<p>There have been numerous attempts to recognize pictures by machines for decades. It is a challenge to mimic the visual recognition system of the human brain in a computer. Human vision is the hardest to mimic and most complex sensory cognitive system of the brain. We will not discuss biological neurons<span> </span><span>here, that is,</span> the primary visual cortex, but rather focus on artificial neurons. Objects in the physical world are three dimensional, whereas pictures of those objects are two dimensional. In this book, we will introduce neural networks without appealing to brain analogies. In 1963, computer scientist Larry Roberts, who is also known as the <strong>father of computer vision</strong>, described the possibility of extracting 3D geometrical information from 2D perspective views of blocks <span>in his research dissertation titled<em> </em></span><strong>BLOCK WORLD</strong>. This was the first breakthrough in the world of computer vision. Many researchers worldwide in machine learning and artificial intelligence followed this work and studied computer vision in the context of BLOCK WORLD. Human beings can recognize blocks regardless of any orientation or lighting changes that may happen. In this dissertation, he said that it is important to understand simple edge-like shapes in images. He extracted these edge-like shapes from blocks in order to make the computer understand that these two blocks are the same irrespective of orientation:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7e8981b5-bdf4-4023-a271-417a9c96d29f.png"/></div>
<p><span>The vision starts with a simple structure. </span>This is the beginning of computer vision as an engineering model. David Mark, an MIT computer vision scientist, gave us the next important concept, that vision is hierarchical. He wrote a very influential book named <em>VISION</em>. This is a simple book. He said that an image consists <span>of</span><span> </span><span>several layers. These two principles form the basis of deep learning architecture, although they do not tell us what kind of mathematical model to use.</span></p>
<p>In the 1970s, the first visual recognition algorithm, known as the <strong>generalized cylinder model</strong>, came from the AI lab at Stanford University. The idea here is that the world is composed of simple shapes and any real-world object is a combination of these simple shapes. At the same time, another model, known as the <strong>pictorial structure model</strong>, was published from SRI Inc. The concept is still the same as the generalized cylinder model, but the parts are connected by springs; thus, it introduced a concept of variability. The first visual recognition algorithm was used in a digital camera by Fujifilm in 2006.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional neural networks</h1>
                </header>
            
            <article>
                
<p>CNNs, or ConvNets, are quite similar to regular neural networks. They are still made up of neurons with weights that can be learned from data. Each neuron receives some inputs and performs a dot product. They still have a loss function on the last fully connected layer. They can still use a nonlinearity function. All of the tips and techniques that we learned from the last chapter are still valid for CNN. <span>As we saw in the previous chapter, a regular neural network receives input data as a single vector and passes through a series of </span>hidden layers<span>. Every hidden layer consists of a set of neurons, wherein every neuron is fully connected to all the other neurons in the previous layer. Within a single layer, each neuron is completely independent and they do not share any connections. The last fully connected layer, also called the <strong>output layer</strong>, contains class scores in the case of an image classification problem. Generally, there are three main layers in a simple ConvNet. They are the <strong>convolution layer</strong>, the <strong>pooling layer</strong>, and the <strong>fully connected layer</strong>. We can see a simple neural network in the following image:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="203" src="assets/22900b9c-4754-4b53-8023-c1f50bfc7eda.png" width="401"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">A regular three-layer neural network</div>
<p>So, what changes? Since a CNN mostly takes images as input, this allows us to encode a few properties into the network, thus reducing the number of parameters.</p>
<p>In the case of real-world image data, CNNs perform better than <strong>Multi-Layer Perceptrons</strong> (<strong>MLPs</strong>). There are two reasons for this:</p>
<ul>
<li>In the last chapter, w<span>e saw </span>that in order to feed an image to an MLP, we convert the input matrix into a simple numeric vector with no spatial structure. It has no knowledge that these numbers are spatially arranged. So, CNNs are built for this very reason; that is, to elucidate the patterns in multidimensional data. Unlike MLPs, CNNs understand the fact that image pixels that are closer in proximity to each other are more heavily related than pixels that are further apart:<br/>
<div class="CDPAlignCenter CDPAlign"><em>CNN = Input layer + hidden layer + fully connected layer</em></div>
</li>
<li>CNNs differ from MLPs in the types of hidden layers that can be included in the model. A<span> ConvNet arranges its neurons in three dimensions: <strong>width</strong>, <strong>height</strong>, and <strong>depth</strong>. Each layer transforms its 3D input volume into a 3D output volume of neurons using activation functions. For example, in the following figure, the red input layer holds the image. Thus its width and height are the dimensions of the image, and the depth is three since there are Red, Green, and Blue channels:</span></li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="167" src="assets/2909a83d-f11b-4e4f-be2b-ed1365c13eae.png" width="466"/></div>
<div class="packt_tip">ConvNets are deep neural networks that share their parameters across space.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How do computers interpret images?</h1>
                </header>
            
            <article>
                
<p><span>Essentially, every image can be represented as a matrix of pixel values. In other words, images can be thought of as a function (<em>f</em>) that maps from <em>R<sup>2</sup></em> to <em>R</em>.</span></p>
<p><em>f(x, y)</em> gives the intensity value at the position <em>(x, y)</em>. In practice, the value of the function ranges <span>only</span><span> </span><span>from <em>0</em> to <em>255</em>. Similarly, a color image can be represented as a stack of three functions. We can write this as a vector of:</span></p>
<p class="CDPAlignCenter CDPAlign"><em> f( x, y) = [ r(x,y) g(x,y) b(x,y)]</em></p>
<p>Or we can write this as a mapping:</p>
<p class="CDPAlignCenter CDPAlign"><em>f: R x R --&gt; R3</em></p>
<p>So, a color image is also a function, but in this case, a value at each <em>(x,y)</em> position is not a single number. Instead it is a vector that has three different light intensities corresponding to three color channels. The following is the code for seeing the details of an image as input to a computer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code for visualizing an image </h1>
                </header>
            
            <article>
                
<p>Let's take a look at how an image can be visualized with the following code:</p>
<pre><span>#import all required lib<br/>import matplotlib.pyplot as plt</span><br/><span>%matplotlib inline</span><br/><span>import numpy as np</span><br/><span>from skimage.io import imread</span><br/><span>from skimage.transform import resize</span></pre>
<pre># Load a color image in grayscale<br/>image = imread('sample_digit.png',as_grey=True)<br/>image = resize(image,(28,28),mode='reflect')<br/>print('This image is: ',type(image), <br/>         'with dimensions:', image.shape)<br/><br/>plt.imshow(image,cmap='gray')<br/><br/></pre>
<p>We obtain the following image as a result:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7becc16a-b4b4-4e07-8f41-30ae0768b2aa.png"/></div>
<pre>def visualize_input(img, ax):<br/><br/>    ax.imshow(img, cmap='gray')<br/>    width, height = img.shape<br/>    thresh = img.max()/2.5<br/>    for x in range(width):<br/>        for y in range(height):<br/>            ax.annotate(str(round(img[x][y],2)), xy=(y,x),<br/>                        horizontalalignment='center',<br/>                        verticalalignment='center',<br/>                        color='white' if img[x][y]&lt;thresh else 'black')<br/><br/>fig = plt.figure(figsize = (12,12)) <br/>ax = fig.add_subplot(111)<br/>visualize_input(image, ax)</pre>
<p>The following result is obtained:</p>
<div class="CDPAlignCenter CDPAlign"><img height="461" src="assets/896620ef-6e56-4fb5-9e79-9d5aa17f9c09.png" width="462"/></div>
<p>In the previous chapter, we used an MLP-based approach to recognize images. There are two issues with that approach:</p>
<ul>
<li>It increases the number of parameters</li>
<li>It only accepts vectors as input, that is, flattening a matrix to a vector</li>
</ul>
<p>This means we must find a new way to process images, in which 2D information is not completely lost. CNNs address this issue. Furthermore, CNNs accept matrices as input. Convolutional layers preserve spatial structures. First, we define a convolution window, also called a <strong>filter</strong>, or <strong>kernel</strong>; then slide this over the image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dropout</h1>
                </header>
            
            <article>
                
<p>A neural network can be thought of as a search problem. Each node in the neural network is searching for correlation between the input data and the correct output data.</p>
<p>Dropout randomly turns nodes off while forward-propagating and thus helps ward off weights from converging to identical positions. After this is done, it turns on all the nodes and back-propagates. Similarly, we can set some of the layer's values to zero at random during forward propagation in order to perform dropout on a layer.</p>
<div class="packt_tip">
<p>Use dropout only during training. Do not use it at runtime or on your testing dataset.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Input layer</h1>
                </header>
            
            <article>
                
<p>The <strong>input layer</strong> holds the image data. In the following figure, the input layer consists of three inputs. In a <strong>fully connected layer</strong>, the neurons between two adjacent layers are fully <span>connected </span>pairwise but do not share any connection within a layer. In other words, the neurons in this layer have full connections to all activations in the previous layer. Therefore, their activations can be computed with a simple matrix multiplication, optionally adding a bias term. The difference between a fully connected and convolutional layer is that neurons in a <span>convolutional </span>layer are connected to a local region in the input, and that they also share parameters:</p>
<div class="CDPAlignCenter CDPAlign"><img height="183" src="assets/d65ee007-a008-4252-88b6-f4ac9d2d13d2.png" width="276"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional layer</h1>
                </header>
            
            <article>
                
<p>The main objective of convolution in relation to ConvNet is to extract features from the input image. This layer does most of the computation in a ConvNet. We will not go into the mathematical details of convolution here but will get an understanding of how it works over images.</p>
<p>The ReLU activation function is extremely useful in CNNs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional layers in Keras</h1>
                </header>
            
            <article>
                
<p>To create a convolutional layer in Keras, you must first import the required modules as follows:</p>
<pre><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Conv2D</pre>
<p>Then, you can create a convolutional layer by using the following format:</p>
<pre>Conv2D(filters, kernel_size, strides, padding, activation=<span class="hljs-string">'relu'</span>, input_shape)</pre>
<p>You must pass the following arguments:</p>
<ul>
<li><kbd>filters</kbd>: The number of filters.</li>
<li><kbd>kernel_size</kbd><span>: A n</span>umber specifying both the height and width of the (square) convolution window. There are <span>also </span>some additional optional arguments that you might like to tune.</li>
<li><kbd>strides</kbd>: The stride of the convolution. If you don't specify anything, this is set to one.</li>
<li><kbd>padding</kbd>: This is either <kbd>valid</kbd> or <kbd>same</kbd>. If you don't specify anything, the padding is set to <kbd>valid</kbd>.</li>
<li><kbd>activation</kbd>: This is typically <kbd>relu</kbd>. If you don't specify anything, no activation is applied. You are strongly encouraged to add a ReLU activation function to every convolutional layer in your networks. </li>
</ul>
<div class="packt_infobox">It is possible to represent both <kbd>kernel_size</kbd> and <kbd>strides</kbd> as either a number or a tuple.</div>
<p>When using your convolutional layer as the first layer (appearing after the input layer) in a model, you must provide an additional <kbd>input_shape</kbd> argument—<kbd>input_shape</kbd><span>. It is a tuple specifying the height, width, and depth (in that order) of the input.</span></p>
<div class="packt_infobox"><span>Please make sure that the </span> <kbd>input_shape</kbd> argument is not included if the convolutional layer is not the first layer in your network.</div>
<p>There are many other tunable arguments that you can set to change the behavior of your convolutional layers:</p>
<ul>
<li><strong>Example 1</strong>: In order to build a CNN with an input layer that accepts <span>images of 200 x 200 pixels in </span>grayscale. In such cases, the next layer would be a convolutional layer of 16 filters with width and height as 2. As we go ahead with the convolution we can set the filter to jump 2 pixels together. Therefore, we can build a convolutional, layer with a filter that doesn't pad the images with zeroes with the following code:</li>
</ul>
<pre style="padding-left: 60px">Conv2D(filters=16, kernel_size=2, strides=2, activation='relu', input_shape=(200, 200, 1))</pre>
<ul>
<li><strong>Example 2</strong>: After we build our CNN model, we can have the next layer in it to be a convolutional layer. This layer will have 32 filters with width and height as 3, which would take the layer that was constructed in the previous example as its input. Here, as we proceed with the convolution, we will set the filter to jump one pixel at a time, such that the convolutional layer will be able to see all the regions of the previous layer too. Such a convolutional layer can be constructed with the help of the following code:</li>
</ul>
<pre style="padding-left: 60px">Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')</pre>
<ul>
<li class="mce-root"><strong>Example 3</strong>: You can also construct convolutional layers in Keras of size 2 x 2, with 64 filters and a ReLU activation function. Here, the convolution utilizes a stride of 1 with padding set to <kbd>valid</kbd> and all other arguments set to their default values. Such a convolutional layer can be built using the following code:</li>
</ul>
<pre style="padding-left: 60px">Conv2D(64, (2,2), activation='relu')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pooling layer</h1>
                </header>
            
            <article>
                
<p>As we have seen, a convolutional layer is a stack of feature maps, with one feature map for each filter. More filters increase the dimensionality of convolution. Higher dimensionality indicates more parameters. So, the pooling layer controls overfitting by progressively reducing the spatial size of the representation to reduce the number of parameters and computation. The pooling layer often takes the convolutional layer as input. The most commonly used pooling approach is <strong>max pooling</strong>. In addition to max pooling, pooling units can also perform other functions such as <strong>average pooling</strong>. In a CNN, we can control the behavior of the convolutional layer by specifying the size of each filter and the number of filters. To increase the number of nodes in a convolutional layer, we can increase the number of filters, and to increase the size of the pattern, we can increase the size of the filter. There are also a few other hyperparameters that can be tuned. One of them is the stride of the convolution. Stride is the amount by which the filter slides over the image. A stride of 1 moves the filter by 1 pixel horizontally and vertically. Here, the convolution becomes the same as the width and depth of the input image. A stride of 2 makes a convolutional layer of half of the width and height of the image. If the filter extends outside of the image, then <span>we can </span>either ignore these unknown values or replace them with zeros. This is known as <strong>padding</strong>. In Keras, we can set <kbd>padding = 'valid'</kbd> if it is acceptable to lose a few values. Otherwise, set <kbd>padding = 'same'</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-220 image-border" height="720" src="assets/ffc13c90-ca46-446d-a5c9-e1c0b2712ba8.png" width="1280"/></div>
<p>A very simple ConvNet looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img height="114" src="assets/f67c1764-58d0-4d31-b204-34f22089b652.png" width="383"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Practical example – image classification</h1>
                </header>
            
            <article>
                
<p>The convolutional layer helps to detect regional patterns in an image. The max pooling layer, present after the <span>convolutional </span>layer, helps reduce dimensionality. Here is an example of image classification using all the principles we studied in the previous sections. One important notion is to first make all the images into a standard size before doing anything else. The first <span>convolution </span>layer requires an additional <kbd>input.shape()</kbd> parameter. In this section, we will train a CNN to classify images from the CIFAR-10 database. CIFAR-10 is a dataset of 60,000 color images of 32 x 32 size. These images are labeled into 10 categories with 6,000 images each. These categories are airplane, automobile, bird, cat, dog, deer, frog, horse, ship, and truck. Let's see how to do this with the following code:</p>
<pre>import keras<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/><br/>fig = plt.figure(figsize=(20,5))<br/>for i in range(36):<br/>    ax = fig.add_subplot(3, 12, i + 1, xticks=[], yticks=[])<br/>    ax.imshow(np.squeeze(x_train[i]))from keras.datasets import cifar10</pre>
<pre class="mce-root"># rescale [0,255] --&gt; [0,1]<br/>x_train = x_train.astype('float32')/255<br/>from keras.utils import np_utils<br/><br/># one-hot encode the labels<br/>num_classes = len(np.unique(y_train))<br/>y_train = keras.utils.to_categorical(y_train, num_classes)<br/>y_test = keras.utils.to_categorical(y_test, num_classes)<br/><br/># break training set into training and validation sets<br/>(x_train, x_valid) = x_train[5000:], x_train[:5000]<br/>(y_train, y_valid) = y_train[5000:], y_train[:5000]<br/><br/># print shape of training set<br/>print('x_train shape:', x_train.shape)<br/><br/># printing number of training, validation, and test images<br/>print(x_train.shape[0], 'train samples')<br/>print(x_test.shape[0], 'test samples')<br/>print(x_valid.shape[0], 'validation samples')x_test = x_test.astype('float32')/255<br/><br/><br/>from keras.models import Sequential<br/>from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout<br/><br/>model = Sequential()<br/>model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', <br/>                        input_shape=(32, 32, 3)))<br/>model.add(MaxPooling2D(pool_size=2))<br/>model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))<br/>model.add(MaxPooling2D(pool_size=2))<br/>model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))<br/>model.add(MaxPooling2D(pool_size=2))<br/>model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))<br/>model.add(MaxPooling2D(pool_size=2))<br/>model.add(Dropout(0.3))<br/>model.add(Flatten())<br/>model.add(Dense(500, activation='relu'))<br/>model.add(Dropout(0.4))<br/>model.add(Dense(10, activation='softmax'))<br/><br/>model.summary()<br/><br/># compile the model<br/>model.compile(loss='categorical_crossentropy', optimizer='rmsprop', <br/>                  metrics=['accuracy'])<br/>from keras.callbacks import ModelCheckpoint <br/><br/># train the model<br/>checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, <br/>                               save_best_only=True)<br/>hist = model.fit(x_train, y_train, batch_size=32, epochs=100,<br/>          validation_data=(x_valid, y_valid), callbacks=[checkpointer], <br/>          verbose=2, shuffle=True)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image augmentation</h1>
                </header>
            
            <article>
                
<p>While training a CNN model, we do not want the model to change any prediction based on the size, angle, and position of the image. The image is represented as a matrix of pixel values, so the size, angle, and position have a huge effect on the pixel values. To make the model more size-invariant, we can add different sizes of the image to the training set. Similarly, in order to make the model more rotation-invariant, we can add images with different angles. This process is known as <strong>image data augmentation</strong>. This also helps to avoid overfitting. Overfitting happens when a model is exposed to very few samples. Image data augmentation is one way to reduce overfitting, but it may not be enough because augmented images are still correlated. Keras provides an image augmentation class called <kbd>ImageDataGenerator</kbd> that defines the configuration for image data augmentation. This also provides other features such as:</p>
<ul>
<li>Sample-wise and feature-wise standardization</li>
<li>Random rotation, shifts, shear, and zoom of the image</li>
<li>Horizontal and vertical flip</li>
<li><span>ZCA whitening</span></li>
<li>Dimension reordering</li>
<li>Saving the changes to disk</li>
</ul>
<p>An augmented image generator object can be created as follows:</p>
<pre>imagedatagen = ImageDataGenerator()</pre>
<p><span>This API generates batches of tensor image data in real-time data augmentation, instead of processing an entire image dataset in memory. This API is designed to create augmented image data during the model fitting process. Thus, it reduces the memory overhead but adds some time cost for model training.</span></p>
<p>After it is created and configured, you must fit your data. <span>This computes any statistics required to perform the transformations to image data.</span> <span>This is done by calling the </span><kbd>fit()</kbd><span> function on the data generator and passing it to the training dataset, as follows:</span></p>
<pre><span class="crayon-sy"><span>imagedatagen</span>.</span><span class="crayon-e">fit</span><span class="crayon-sy">(</span><span class="crayon-v">train_data</span><span class="crayon-sy">)</span></pre>
<p><span>The batch size can be configured, the data generator can be prepared, and batches of images can be received by calling the </span><kbd>flow()</kbd> <span>function:</span></p>
<pre class="CDPAlignLeft CDPAlign"><span>imagedatagen.flow(x_train, y_train, batch_size=</span><span class="hljs-number">32</span><span>)</span></pre>
<p><span>Finally, </span><span>call the </span><kbd>fit_generator()</kbd><span> function instead of calling the <kbd>fit()</kbd> function on the model:</span></p>
<pre>fit_generator(imagedatagen, samples_per_epoch=len(X_train), epochs=200)</pre>
<p><span>Let's look at some examples to understand how</span><span> the image augmentation API in Keras works. </span>We will use the MNIST handwritten digit recognition task in these examples.</p>
<p>Let's begin by taking a look at the first nine images in the training dataset:</p>
<pre>#Plot images <br/>from keras.datasets import mnist<br/>from matplotlib import pyplot<br/>#loading data<br/>(X_train, y_train), (X_test, y_test) = mnist.load_data()<br/>#creating a grid of 3x3 images<br/>for i in range(0, 9):<br/>  pyplot.subplot(330 + 1 + i)<br/>  pyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))<br/>#Displaying the plot<br/>pyplot.show()</pre>
<p>The following code snippet creates augmented images from the CIFAR-10 dataset. We will add these images to the training set of the last example and see how the classification accuracy increases:</p>
<pre class="mce-root">from keras.preprocessing.image import ImageDataGenerator<br/># creating and configuring augmented image generator<br/>datagen_train = ImageDataGenerator(<br/> width_shift_range=0.1, # shifting randomly images horizontally (10% of total width)<br/> height_shift_range=0.1, # shifting randomly images vertically (10% of total height)<br/> horizontal_flip=True) # flipping randomly images horizontally<br/># creating and configuring augmented image generator<br/>datagen_valid = ImageDataGenerator(<br/> width_shift_range=0.1, # shifting randomly images horizontally (10% of total width)<br/> height_shift_range=0.1, # shifting randomly images vertically (10% of total height)<br/> horizontal_flip=True) # flipping randomly images horizontally<br/># fitting augmented image generator on data<br/>datagen_train.fit(x_train)<br/>datagen_valid.fit(x_valid)<br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We began this chapter by briefly looking into the history of CNNs. We introduced you to the implementation of visualizing images. </p>
<p>We studied image classification with the help of a practical example, using all the principles we learned about in the chapter. Finally, we learned how image augmentation helps us avoid overfitting and studied the various other features provided by image augmentation.</p>
<p>In the next chapter, we will learn how to build a simple image classifier CNN model from scratch.</p>


            </article>

            
        </section>
    </body></html>