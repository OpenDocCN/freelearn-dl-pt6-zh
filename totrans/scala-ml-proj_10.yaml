- en: Human Activity Recognition using Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **recurrent neural network** (**RNN**) is a class of artificial neural network
    where connections between units form a directed cycle. RNNs make use of information
    from the past. That way, they can make predictions for data with high temporal
    dependencies. This creates an internal state of the network that allows it to
    exhibit dynamic temporal behavior.
  prefs: []
  type: TYPE_NORMAL
- en: An RNN takes many input vectors to process them and output other vectors. Compared
    to a classical approach, using an RNN with **Long Short-Term Memory** cells (**LSTMs**)
    requires no, or very little, feature engineering. Data can be fed directly into
    the neural network, which acts like a black box, modeling the problem correctly.
    The approach here is rather simple in terms of how much data is preprocessed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will see how to develop a machine learning project using
    RNN implementation, called LSTM for **human activity recognition** (**HAR**),
    using the smartphones dataset. In short, our ML model will be able to classify
    the type of movement from six categories: walking, walking upstairs, walking downstairs,
    sitting, standing, and lying down.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, we will learn the following topics throughout this end-to-end
    project:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with recurrent neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long term dependencies and drawbacks of RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing an LSTM model for human activity recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning LSTM and RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will first provide some contextual information about RNNs.
    Then, we will highlight some potential drawbacks of classical RNNs. Finally, we
    will see an improved variation of RNNs called LSTM to address the drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual information and the architecture of RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Human beings don''t start thinking from scratch; the human mind has so-called
    **persistence of memory**, the ability to associate the past with recent information.
    Traditional neural networks, instead, ignore past events. For example, in a movie
    scenes classifier, it''s not possible for a neural network to use a past scene
    to classify current ones. RNNs were developed to try to solve this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19903995-1d34-4460-af08-79d4dab2a55f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: RNNs have loops'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to conventional neural networks, RNNs are networks with a loop
    that allows the information to be persistent (*Figure 1*). In a neural network
    say, **A**: at some time **t**, input **x[t]** and outputs a value **h[t]**. So
    from *Figure 1*, we can think of an RNN as multiple copies of the same network,
    each passing a message to a successor. Now, if we unroll the previous network,
    what will we receive? Well, the following figure gives you some insight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce3d30f1-828f-4530-932c-9af098e2e36e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An unrolled representation of the same RNN represented in Figure
    1'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the preceding unrolled figure does not provide detailed information
    about RNNs. Rather, an RNN is different from a traditional neural network because
    it introduces a transition weight **W** to transfer information between times.
    RNNs process a sequential input one at a time, updating a kind of vector state
    that contains information about all past elements of the sequence. The following
    figure shows a neural network that takes as input a value of **X(t)**, and then
    outputs a value **Y(t)**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c664553f-02b8-4a9a-a128-625acf326972.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An RNN architecture can use the previous states of the network to
    its advantage'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 1*, the first half of the neural network is characterized
    by the function *Z (t) = X (t) * W[in]*, and the second half of the neural network
    takes the form *Y(t)= Z(t) * W[out]*. If you prefer, the whole neural network
    is just the function *Y (t) = (X (t) * W*[in]*) * W*[out].
  prefs: []
  type: TYPE_NORMAL
- en: 'At each time *t*, calls the learned model, this architecture does not take
    into account knowledge about the previous runs. It''s like predicting stock market
    trends by only looking at data from the current day. A better idea would be to
    exploit overarching patterns from a week''s worth or months worth of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb70b26d-db1e-4a79-9670-e0d38d037b61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: An RNN architecture where all the weights in all the layers have
    to be learned with time'
  prefs: []
  type: TYPE_NORMAL
- en: A more explicit architecture can be found in *Figure 4*, where the temporally
    shared weights **w2** (for the hidden layer) must be learned in addition to **w1**
    (for the input layer) and **w3** (for the output layer).
  prefs: []
  type: TYPE_NORMAL
- en: Incredibly, over the last few years, RNNs have been used for a variety of problems,
    such as speech recognition, language modeling, translation, and image captioning.
  prefs: []
  type: TYPE_NORMAL
- en: RNN and the long-term dependency problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RNNs are very powerful and popular too. However, often, we only need to look
    at recent information to perform the present task rather than information that
    was stored a long time ago. This is frequent in NLP for language modeling. Let''s
    see a common example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/907bff1c-57b0-4927-9a45-0a2922d87fea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: If the gap between the relevant information and the place that its
    needed is small, RNNs can learn to use past information'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose a language model is trying to predict the next word based on the previous
    words. As a human being, if we try to predict the last word in *the sky is blue*,
    without further context, it's most likely the next word that we will predict is
    *blue*. In such cases, the gap between the relevant information and the place
    is small. Thus, RNNs can learn to use past information easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'But consider a longer sentence: *Asif grew up in Bangladesh... He studied in
    Korea... He speaks fluent Bengali* where we need more context. In this sentence,
    most recent information advises us that the next word will probably be the name
    of a language. However, if we want to narrow down which language, we need the
    context of *Bangladesh* from previous words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44e0a9aa-4a4d-48df-87e7-a352f6f8b440.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: If the gap between the relevant information and the place that its
    needed is bigger, RNNs can''t learn to use past information'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the gap is bigger so RNNs become unable to learn the information. This
    is a serious drawback of RNN. However, along comes LSTM to the rescue.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One type of RNN model is an **LSTM**. The precise implementation details of
    LSTM are not within the scope of this book. An LSTM is a special RNN architecture,
    which was originally conceived by Hochreiter and Schmidhuber in 1997\. This type
    of neural network has been recently rediscovered in the context of deep learning,
    because it is free from the problem of vanishing gradients, and offers excellent
    results and performance. LSTM-based networks are ideal for prediction and classification
    of temporal sequences, and are replacing many traditional approaches to deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s a hilarious name, but it means exactly what it sounds. The name signifies
    that short-term patterns aren''t forgotten in the long-term. An LSTM network is
    composed of cells (LSTM blocks) linked to each other. Each LSTM block contains
    three types of gate: input gate, output gate, and forget gate, respectively, that
    implement the functions of writing, reading, and resetting the cell memory. These
    gates are not binary, but analogical (generally managed by a sigmoidal activation
    function mapped in the range (0, 1), where 0 indicates total inhibition, and 1
    shows total activation).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you consider the LSTM cell as a black box, it can be used very much like
    a basic cell, except it will perform much better; training will converge faster,
    and it will detect long-term dependencies in the data. So how does an LSTM cell
    work? The architecture of a basic LSTM cell is shown in *Figure 7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68fcf3fa-5376-443f-94b4-41670eda6887.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Block diagram of an LSTM cell'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see the mathematical notation behind this architecture. If we don''t
    look at what''s inside the LSTM box, the LSTM cell itself looks exactly like a
    regular memory cell, except that its state is split into two vectors, **h(t)**
    and **c(t)**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**c** is a cell'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**h(t)** is the short-term state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**c(t)** is the long-term state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's open the box! The key idea is that the network can learn what to store
    in the long-term state, what to throw away, and what to read from it. As the long-term
    state **c[(t-1)]** traverses the network from left to right, you can see that
    it first goes through a forget gate, dropping some memories, and then it adds
    some new memories via the addition operation (which adds the memories that were
    selected by an input gate). The resulting **c(t)** is sent straight out, without
    any further transformation.
  prefs: []
  type: TYPE_NORMAL
- en: So, at each timestamp, some memories are dropped and some memories are added.
    Moreover, after the addition operation, the long-term state is copied and passed
    through the **tanh** function, and then the result is filtered by the output gate.
    This produces the short-term state **h(t)** (which is equal to the cell's output
    for this time step **y(t)**). Now let's look at where new memories come from and
    how the gates work. First, the current input vector **x(t)** and the previous
    short-term state **h(t-1)** are fed to four different fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The presence of these gates allows LSTM cells to remember information for an
    indefinite time; if the input gate is below the activation threshold, the cell
    will retain the previous state, and if the current state is enabled, it will be
    combined with the input value. As the name suggests, the forget gate resets the
    current state of the cell (when its value is cleared to 0), and the output gate
    decides whether the value of the cell must be carried out or not. The following
    equations are used to do the LSTM computations of a cell''s long-term state, its
    short-term state, and its output at each time step for a single instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72f5567e-1bd5-4ab9-abbd-452789a2b46b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, *W[xi]*, *W[xf]*, *W[xo]*, and *W[xg]* are the weight
    matrices of each of the four layers for their connection to the input vector *x[(t)]*.
    On the other hand, *W[hi]*, *W[hf]*, *W[ho]*, and *W[hg]* are the weight matrices
    of each of the four layers for their connection to the previous short-term state
    *h[(t-1)]*. Finally, *b[i]*, *b[f]*, *b[o]*, and *b[g]* are the bias terms for
    each of the four layers.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know all that, how do both RNN and the LSTM network work? It's time
    to do some hands-on. We will start implementing an MXNet and Scala-based LSTM
    model for HAR.
  prefs: []
  type: TYPE_NORMAL
- en: Human activity recognition using the LSTM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Human Activity Recognition** (**HAR**) database was built from the recordings
    of 30 study participants performing **activities of daily living** (**ADL**) while
    carrying a waist-mounted smartphone with embedded inertial sensors. The objective
    is to classify activities into one of the six activities performed.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset description
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The experiments have been carried out with a group of 30 volunteers within an
    age bracket of 19 - 48 years. Each person accomplished six activities, namely
    walking, walking upstairs, walking downstairs, sitting, standing, and laying by
    wearing a Samsung Galaxy S II smartphone on their waist. Using the accelerometer
    and gyroscope, the author captured 3-axial linear acceleration and 3-axial angular
    velocity at a constant rate of 50 Hz.
  prefs: []
  type: TYPE_NORMAL
- en: Only two sensors, that is, accelerometer and gyroscope, were used. The sensor
    signals were pre-processed by applying noise filters and then sampled in fixed-width
    sliding windows of 2.56 sec and 50% overlap. This gives 128 readings/window. The
    gravitational and body motion components from the sensor acceleration signal were
    separated via a Butterworth low-pass filter into body acceleration and gravity.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, please refer to this paper: Davide Anguita, Alessandro
    Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset
    for *Human Activity Recognition Using Smartphones*. *21st European Symposium on
    Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN*
    2013\. Bruges, Belgium 24-26 April 2013.'
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, the gravitational force is assumed to have only a few but low-frequency
    components. Therefore, a filter of 0.3 Hz cut-off frequency was used. From each
    window, a feature vector was found by calculating variables from the time and
    frequency domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'The experiments have been video recorded to label the data manually. The obtained
    dataset has been randomly partitioned into two sets, where 70% of the volunteers
    were selected for generating the training data and 30% the test data. Now, when
    I explore the dataset, both the training and test set have the following file
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88720740-0311-4da6-b5f1-01ee9d3670a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: HAR dataset file structure'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each record in the dataset, the following is provided:'
  prefs: []
  type: TYPE_NORMAL
- en: Triaxial acceleration from the accelerometer and the estimated body acceleration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triaxial angular velocity from the gyroscope sensor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 561-feature vector with time and frequency domain variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its activity label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An identifier of the subject who carried out the experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we know the problem that needs to be addressed, it''s time to explore the
    technology and related challenges. Well, as I already stated, we will be using
    an MXNet-based LSTM implementation. One question you may ask is: why aren''t we
    using H2O or DeepLearning4j? Well, the answer is that both of them either do not
    have LSTM-based implementation, or cannot be applied to solve this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting and configuring MXNet for Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache MXNet is a flexible and efficient library for deep learning. Building
    a high-performance deep learning library requires many system-level design decisions.
    In this design note, we share the rationale for the specific choices made when
    designing MXNet. We imagine that these insights may be useful to both deep learning
    practitioners and builders of other deep learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this project, we will be needing different packages and libraries: Scala,
    Java, OpenBLAS, ATLAS, OpenCV, and overall, MXNet. Now let''s start configuring
    these tools one by one. For Java and Scala, I am assuming that you already have
    Java and Scala configured. Now the next task is to install build tools and `git`
    since we will be using the MXNet from the GitHub repository. To do this, just
    execute the following commands on Ubuntu:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we need to install OpenBLAS and ATLAS. These are required for linear algebra
    operations performed by MXNet. To install these, just execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to install OpenCV for image processing. Let''s install it by executing
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to generate the prebuilt MXNet binary. To do this, we need
    to clone and build MXNet for Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if the preceding steps went smoothly, a prebuilt-binary for MXNet will
    be generated in `/home/$user_name/mxnet/scala-package/assembly/linux-x86_64-cpu`
    (or `linux-x86_64-gpu` with GPU configured on Linux, and `osx-x86_64-cpu` on macOS).
    Take a look at the following screenshot of the CPU on Ubuntu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/037aaeb6-0dec-465d-8936-594577f7e5d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: MXNet pre-built binary generated'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the next task before you start writing your Scala code on Eclipse (or
    IntelliJ) as a Maven (or SBT) project, is including this JAR in the build path.
    Additionally, we need some extra dependency for Scala plots and `args4j`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Well done! All set and we're ready to go! Let's start coding.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an LSTM model for HAR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The overall algorithm (`HumanAR.scala`) has the following workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the LSTM model using imperative programming and the hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying batch wise training, that is, picking batch size data, feeding it to
    the model, then at some iterations evaluating the model and printing the batch
    loss and the accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output the chart for the training and test errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding steps can be followed and constructed by way of a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9db9b808-6ecd-4cbf-aa7e-6ee6a098d27d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: MXNet pre-built binary generated'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's start the implementation step-by-step. Make sure that you understand
    each line of code then import the given project in Eclipse or SBT.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 - Importing necessary libraries and packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start coding now. We start from the very beginning, that is, by importing
    libraries and packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Step 2 - Creating MXNet context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Then we create an MXNet context for CPU-based computation. Since I am doing
    it by CPU, I instantiated for the CPU. Feel free to use the GPU if you have already
    configured it by providing the device ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Step 3 - Loading and parsing the training and test set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s load the dataset. I am assuming that you copied your dataset to
    the `UCI_HAR_Dataset/` directory. Then, also place the other data files as described
    previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to load the training and test set separately. To do this I wrote
    two methods called `loadData()` and `loadLabels()` that are in the `Utils.scala`
    file. These two methods and their signatures will be provided soon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `loadData()` method loads and maps the data from each `.txt` file based
    on  the input signal type defined by the `INPUT_SIGNAL_TYPES` array in the `Array[Array[Array[Float]]]`
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As stated earlier, the `INPUT_SIGNAL_TYPES` contains some useful constants:
    those are separate, normalized input features for the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, `loadLabels()` is also a user-defined method that is used
    to load only the labels in the training as well as the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The labels are defined in another array as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Step 4 - Exploratory analysis of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s see some statistics about the number of training series (as described
    earlier, this is with 50% overlap between each series), number of test series,
    number of timesteps per series, and number of input parameters per timestep:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Step 5 - Defining internal RNN structure and LSTM hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s define the internal neural network structure and hyperparameters
    for the LSTM network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Step 6 - LSTM network construction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s set up an LSTM model with the preceding parameters and structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding line, `setupModel()` is the method that does the trick. The
    `getSymbol()` method actually constructs the LSTM cell. We will see its signature,
    too, later on. It accepts sequence length, number of input, number of hidden layers,
    number of labels, batch size, number of LSTM layers, dropout MXNet context, and
    constructs an LSTM model of type using the case class `LSTMModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now here''s the signature of the `setupModel()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding method, we obtained a symbolic model for the deep RNN using
    the `getSymbol()` method that can be seen as follows. I have provided detailed
    comments and believe that will be enough to understand the workflow of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In summary, the algorithm uses 128 LSTM cells in parallel, and I concatenated
    all 128 cells and fed them to the output activation layer. Let''s concatenate
    the cells, outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we connect them to an output layer that corresponds to the 6 label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code segment, `LSTMState` and `LSTMParam` are two case classes
    that used to define the state of each LSTM cell and the latter accepts the parameters
    needed to construct an LSTM cell. final case class `LSTMState(c: Symbol, h: Symbol)`
    final case class `LSTMParam(i2hWeight: Symbol, i2hBias: Symbol, h2hWeight: Symbol,
    h2hBias: Symbol)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to discuss the most important step, which is LSTM cell construction.
    We will use some diagrams and legends as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb82303a-6dd0-4ddc-b518-940435467dcb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Legends used to describe LSTM cell in the following'
  prefs: []
  type: TYPE_NORMAL
- en: 'The repeating module in an LSTM contains four interacting layers as shown in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47c687ee-3a27-4670-bc63-8f825d976de1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Inside an LSTM cell, that is the, repeating module in an LSTM contains
    four interacting layers'
  prefs: []
  type: TYPE_NORMAL
- en: 'An LSTM cell is defined by its stats and parameters, as defined by the preceding
    two case classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LSTM state**: **c** is the cell stat (its memory knowledge) to be used during
    the training and **h** is the output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LSTM parameters**: To be optimized by the training algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**i2hWeight**: Input to hidden weight'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**i2hBias**: Input to hidden bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**h2hWeight**: Hidden to hidden weight'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**h2hBias**: Hidden to hidden bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**i2h**: An NN for input data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**h2h**: An NN from the previous **h**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the code, the two fully connected layers have been created, concatenated,
    and transformed to four copies by the following code. Let''s add a hidden layer
    of size `numHidden * 4` (`numHidden` set to 28) that takes as input the `inputdata`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we add a hidden layer of size `numHidden * 4` (`numHidden` set to 28)
    that takes as input the previous output of the cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s concatenate them together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then let''s make four copies of gates before we compute the gates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we compute the gates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the activation for the forget gate is represented by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see this in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11da362b-ef68-4a02-bd87-bf8c8ac8b0c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Forget gate in an LSTM cell'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the activation for the in gate and in transform are represented by the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We can also see this in *Figure 14:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/358d8386-48d9-498a-92e8-e71974a4cd78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: In gate and transform gate in an LSTM cell'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next state is defined by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code can be represented by the following figure too:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed5bd699-4ed1-40d4-9dab-01afb9246576.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Next or transited gate in an LSTM cell'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the output gate can be represented by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code can be represented by the following figure too:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8bb371d3-d4e8-4768-a8c4-4aeecc5494b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Output gate in an LSTM cell'
  prefs: []
  type: TYPE_NORMAL
- en: 'Too much of a mouthful? No worries, here I have provided the full code for
    this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Step 7 - Setting up an optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As suggested by many researchers, the `RMSProp` optimizer helps an LSTM network
    to converge quickly. Therefore, I have decided to use it here too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, the model parameters to be optimized are its parameters, except
    the training data and the label (weights and biases):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Step 8 - Training the LSTM network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will start training the LSTM network. However, before getting started,
    let''s try to define some variables to keep track of the training''s performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we start performing the training steps with `batch_size` iterations at
    each loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t get derailed but take a quick look at *step 6* previously, where we
    have instantiated the LSTM model. Now it''s time to feed the input and labels
    to the RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we do forward and backward passes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we need to update the parameters using the `RMSProp` optimizer
    that we defined in *step 7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'It would also be great to get metrics such as training errors—that is, loss
    and accuracy over the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code segment, `getAccAndLoss()` is a method that computes
    the loss and accuracy and can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, it would be exciting to evaluate only the network at some steps
    for faster training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Step 9 - Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Well done! We have finished the training. How about now evaluating the test
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Yahoo! We have managed to achieve 94% accuracy, which is really outstanding.
    In the previous code, `test()` is the method used for evaluating the performance
    of the model. The signature of the model is given in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'When done, it''s good practice to destroy the model to release resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We saw earlier that we achieved up to 93% accuracy on the test set. How about
    seeing the previous accuracy and errors in a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/60e98ce8-9c88-4c7d-9682-ec49100b8391.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Training and test losses and accuracies per iteration'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding graph, it is clear that with only a few iterations, our LSTM
    converged well and produced very good classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning LSTM hyperparameters and GRU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nevertheless, I still believe it is possible to attain about 100% accuracy
    with more LSTM layers. The following are the hyperparameters that I would still
    try to tune to see the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: There are many other variants of the LSTM cell. One particularly popular variant
    is the **Gated Recurrent Unit** (**GRU**) cell, which is a slightly dramatic variation
    on the LSTM. It also merges the cell state and hidden state and makes some other
    changes. The resulting model is simpler than standard LSTM models and has been
    growing increasingly popular. This cell was proposed by Kyunghyun Cho et al. in
    a 2014 paper that also introduced the encoder-decoder network we mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this type of LSTM, interested readers should refer to the following publications:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Learning Phrase Representations using RNN Encoder-Decoder for Statistical
    Machine Translation*, K. Cho et al. (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A 2015 paper by Klaus Greff et al., *LSTM: A Search Space Odyssey*, seems to
    show that all LSTM variants perform roughly the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Technically, a GRU cell is a simplified version of an LSTM cell, where both
    the state vectors are merged into a single vector called **h(t)**. A single gate
    controller controls both the forget gate and the input gate. If the gate controller
    outputs a 1, the input gate is open and the forget gate is closed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30895ce6-726f-4424-b456-f5c2ef15f40f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Internal structure of a GRU cell'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if it outputs a 0, the opposite happens. Whenever a memory
    must be stored, the location where it will be stored is erased first, which is
    actually a frequent variant to the LSTM cell in and of itself. The second simplification
    is that since the full state vector is output at every time step, there is no
    output gate. However, there is a new gate controller introduced that controls
    which part of the previous state will be shown to the main layer. The following
    equations are used to do the GRU computations of a cell''s long-term state, its
    short-term state, and its output at each time step for a single instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e585549b-3e79-453e-a74b-3e1c044ce67c.png)'
  prefs: []
  type: TYPE_IMG
- en: The LSTM and GRU cells are one of the main reasons for the success of RNNs in
    recent years, in particular for applications in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have seen how to develop an ML project using the RNN implementation,
    and called LSTM for HAR using the smartphones dataset. Our LSTM model has been
    able to classify the type of movement from six categories: walking, walking upstairs,
    walking downstairs, sitting, standing, and lying. In particular, we have achieved
    up to 94% accuracy. Later on, we discussed some possible ways to improve the accuracy
    further using GRU cell.'
  prefs: []
  type: TYPE_NORMAL
- en: A **convolutional neural network** (**CNN**) is a type of feedforward neural
    network in which the connectivity pattern between its neurons is inspired by the
    animal visual cortex. Over the last few years, CNNs have demonstrated superhuman
    performance in complex visual tasks such as image search services, self-driving
    cars, automatic video classification, voice recognition, and **natural language
    processing** (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: Considering these, in the next chapter we will see how to develop an end-to-end
    project for handling a multi-label (that is, each entity can belong to multiple
    classes) image classification problem using CNN based on the Scala and Deeplearning4j
    framework on real Yelp image datasets. We will also discuss some theoretical aspects
    of CNNs before getting started. Furthermore, we will discuss how to tune hyperparameters
    for better classification results.
  prefs: []
  type: TYPE_NORMAL
