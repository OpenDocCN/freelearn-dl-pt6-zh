<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Implementing GANs to Recognize Handwritten Digits</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will build an Android application that detects handwritten numbers and works out what the number is by using adversarial learning. We will use the <strong>Modified National Institute of Standards and Technology</strong><span> (</span><strong>MNIST</strong>) dataset for digit classification. We will also look into the basics of <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>).</p>
<p>In this chapter, we will take a closer look at the following topics:</p>
<ul>
<li>Introduction to GANs</li>
<li>Understanding the MNIST database</li>
<li>Building the TensorFlow model</li>
<li>Building the Android application</li>
</ul>
<div class="mce-root packt_infobox">The code for this application can be found at <a href="https://github.com/intrepidkarthi/AImobileapps" target="_blank">https://github.com/intrepidkarthi/AImobileapps</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to GANs</h1>
                </header>
            
            <article>
                
<p class="mce-root">GANs are a class of <strong>machine learning</strong> (<strong>ML</strong>) algorithm that's used in unsupervised ML. They are comprised of two deep neural networks that are competing against each other (so it is termed as adversarial). GANs were introduced <span>at the University of Montreal in 2014 </span>by Ian Goodfellow and other researchers, including Yoshua Bengio.</p>
<div class="mce-root packt_infobox">Ian Goodfellow's paper on GANs can be found at <a href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a>.</div>
<p>GANs have the potential to mimic any data. This means that GANs can be trained to create similar versions of any data, such as images, audio, or text. A simple workflow of a GAN is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-842 image-border" src="assets/155fcb59-cb75-46e4-bc93-4aa7fd25d6e7.png" style="width:37.75em;height:17.25em;"/></p>
<p>The workflow of the GAN will be explained in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generative versus discriminative algorithms</h1>
                </header>
            
            <article>
                
<p>To understand GANs, we must know how discriminative and generative algorithms work. Discriminative algorithms try to predict a label and classify the input data, or categorize them to where the data belongs. On the other hand, generative algorithms make an attempt to predict features to give a certain label.</p>
<p class="mce-root">For example, a discriminative algorithm can predict whether an email is spam or not. Here, spam is one of the labels, and the text that's captured from the message is considered the input data. If you consider the label as <em>y</em> and the input as <em>x</em>, we can formulate this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c4a4b7b1-c903-4e1a-b3ef-9845baa9935e.png" style="width:3.92em;height:1.75em;"/></p>
<p class="CDPAlignLeft CDPAlign">On the other hand, generative algorithms try to guess how likely these input features (<em>x</em>, in the previous equation) are. Generative models care about how you get <em>x</em>, while discriminative models care about the relation between <em>x</em> and <em>y</em>.</p>
<p class="mce-root">Using the MNIST database as an example, the generator will generate images and pass them on to the discriminator. The discriminator will authenticate the image if it is truly from the MNIST dataset. The generator generates images with the hope that it will pass through the discriminator and be authenticated, even though it is fake (as shown in the preceding diagram).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How GANs work</h1>
                </header>
            
            <article>
                
<p class="mce-root">Based on our example, we will assume that we are passing numbers as inputs:</p>
<ol>
<li class="mce-root">The generator takes random numbers as inputs and returns an image as the output</li>
<li class="mce-root">The output image is passed into the discriminator, and, at the same time, the discriminator receives input from the dataset</li>
<li class="mce-root">The discriminator takes in both real and fake input images, and returns probabilities between zero and one (with one representing a prediction of authenticity and zero representing a fake)</li>
</ol>
<p>Using the example application we discussed in this chapter, we can use the same steps to pass the user's hand-drawn image as one of the fake images and try to find the probability value of it being correct.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the MNIST database</h1>
                </header>
            
            <article>
                
<p><span>The MNIST dataset consists of 60,000 handwritten digits. It also consists of a test dataset made up of 10,000 digits. While it is a subset of the NIST dataset, all the digits in this dataset are size normalized and have been centered on a 28 x 28 pixels sized image. Here, every pixel contains a value of 0-255 with its grayscale value.</span></p>
<div class="mce-root packt_infobox">The MNIST dataset can be found at <a href="http://yann.lecun.com/exdb/mnist/" target="_blank">http://yann.lecun.com/exdb/mnist/</a>. The NIST dataset can be found at <a href="https://www.nist.gov/srd/nist-special-database-19" target="_blank">https://www.nist.gov/srd/nist-special-database-19</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the TensorFlow model</h1>
                </header>
            
            <article>
                
<p>In this application, we will build an MNIST dataset based TensorFlow model that we will use in our Android application. Once we have the TensorFlow model, we will convert it into a TensorFlow Lite model. The step-by-step procedure of downloading the model and building the TensorFlow model is as follows.</p>
<p>Here is the architecture diagram on how our model works. The way to achieve this is explained as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-843 image-border" src="assets/191be622-af24-4350-abaa-b854ef81eb00.png" style="width:24.58em;height:23.17em;"/></div>
<p class="CDPAlignLeft CDPAlign">Using TensorFlow, we can download the MNIST data with one line of Python code, as follows:</p>
<pre class="CDPAlignLeft CDPAlign">import tensorflow as tf <br/>from tensorflow.examples.tutorials.mnist import input_data <br/>#Reading data <br/>mnist = input_data.read_data_sets("./data/", one_hot-True) </pre>
<p class="CDPAlignLeft CDPAlign">Now, we have the MNIST dataset downloaded. After that, we will read the data, as shown in the previous code.</p>
<p class="CDPAlignLeft CDPAlign">Now, we can run the script to download the dataset. We will run the script from the console, as follows:</p>
<pre class="CDPAlignLeft CDPAlign">&gt; python mnist.py <br/>Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes. <br/>Extracting MNIST_data/train-images-idx3-ubyte.gz Successfully downloaded train-labels-idxl-ubyte.gz 28881 bytes. <br/>Extracting MNIST_data/train -labels -idxl -ubyte.gz <br/>Successfully downloaded tlOk -images -idx3 -ubyte.gz 1648877 bytes. Extracting MNIST_data/t1Ok -images -idx3 -ubyte.gz <br/>Successfully downloaded tlOk -labels -idxl -ubyte.gz 4542 bytes. Extracting MNIST_data/t1Ok -labels -idxl -ubyte.gz </pre>
<p>Once we have the dataset ready, we will add a few variables that we will use in our application, as follows: </p>
<pre>image_size = 28 <br/>labels_size = 10 <br/>learning_rate = 0.05 <br/>steps_number = 1000 <br/>batch size = 100 </pre>
<p><span>We need to define these variables to control the parameters on building the model as required by the TensorFlow framework. </span>This classification process is simple. The number of pixels that exist in a 28 x 28 image is 784. So, we have a corresponding number of input layers. Once we have the architecture set up, we will train the network and evaluate the results, obtained to understand the effectiveness and accuracy of the model.</p>
<p>Now, let's define the variables that we added in the preceding code block. Depending on whether the model is in the training phase or the testing phase, different data will be passed through the classifier. The training process needs labels to be able to match them to current predictions. This is defined in the following variable:</p>
<pre> #Define placeholders <br/> training_data = tf.placeholder(tf.float32, [None, image_size*image_size]) <br/> labels = tf.placeholder(tf.float32, [None, labels_size]) </pre>
<p>As the computation-graph evaluation occurs, placeholders will be filled. In the training process, we adjust the values of biases and weights toward increasing the accuracy of our results. To achieve this, we will define the weight and bias parameters, as follows:</p>
<pre>#Variables to be tuned <br/>W = tf.Variable(tf.truncated_normal([image_size*image_size, labels_size], stddev=0.1)) <br/>b = tf.Variable(tf.constant(0.1, shape-[labels_size]))</pre>
<p>Once we have variables that can be tuned, we can move on to building the output layer in just one step:</p>
<pre class="mce-root">#Build the network <br/>output = tf.matmul(training_data, W) + b</pre>
<p class="mce-root">We have successfully built the output layer of the network with the training data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the neural network</h1>
                </header>
            
            <article>
                
<p>By optimizing loss, we can get the training process to work. We need to reduce the difference between the actual label value and the network prediction. The term to define this loss is <strong>cross entropy</strong>.</p>
<p>In TensorFlow, cross entropy is provided by the following method:</p>
<pre>tf.nn.softmax_cross_entropy_with_logits</pre>
<p>This method applies softmax to the model's prediction. Softmax is similar to logistic regression, and produces a decimal between 0 and 1.0. For example, a logistic regression output of 0.9 from an email classifier suggests a 90% chance of an email being spam and a 10% chance of it not being spam. The sum of all the probabilities is 1.0, as shown with an example in the following table:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b80b1630-1bd3-4b50-ab7c-da1d0d4e7ce8.png" style="width:18.92em;height:10.50em;"/></p>
<p>Softmax is implemented through a neural network layer, just before the output layer. The softmax layer must have the same number of nodes as the output layer.</p>
<p>Loss is defined using the <kbd>tf.reduce_mean</kbd> method, and the <kbd>GradientDescentOptimizer()</kbd> method is used in training steps to minimize the loss. This is shown in the following code:</p>
<pre>#Defining the loss <br/>loss =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels-labels, logits-output)) <br/># Training step with gradient descent <br/>train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) </pre>
<p>The <kbd>GradientDescentOptimizer</kbd> method will take several steps by adjusting the values of <em>w</em> and <em>b</em> (the weight and bias parameters) in the output. The values will be adjusted until we reduce loss and are closer to a more accurate prediction, as follows:</p>
<pre># Accuracy calculation <br/>correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1)) <br/>accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</pre>
<p class="mce-root">We start the training by initializing the session and the variables, as follows:</p>
<pre class="mce-root"># Run the training <br/>sess = tf.InteractiveSession() sess.run(tf.global_variables_initializer())</pre>
<p class="mce-root">Based on the parameters of the number of steps (<kbd>steps_number</kbd>) defined previously, the algorithm will run in a loop. We will then run the optimizer, as follows:</p>
<pre class="mce-root">for i in range(steps_number): <br/>    # Get the next batch input_batch, <br/>    labels_batch = mnist.train.next_batch(batch_size) <br/>    feed_dict = {training_data: input_batch, labels: labels_batch}<br/>    # Run the training step <br/>    train_step.run(feed_dict=feed_dict) </pre>
<p class="mce-root">With TensorFlow, we can measure the accuracy of our algorithm and print the accuracy value. We can keep it improving as long as the accuracy level increases and finds the threshold value on where to stop, as follows:</p>
<pre class="mce-root"># Print the accuracy progress on the batch every 100 steps <br/>if i%100 == 0: <br/>    train_accuracy = accuracy.eval(feed_dict=feed_dict) <br/>    print("Step %d, batch accuracy %g %%"%(i, train_accuracy*100))</pre>
<p class="mce-root"><span>Once the training is done, we can evaluate the network's performance. We can use the training data to measure performance, as follows:</span></p>
<pre class="mce-root"># Evaluate on the test set <br/>test_accuracy = accuracy.eval(feed_dict=ftraining_data: mnist.test.images, labels: mnist.test.labels}) <br/>print("Test accuracy: %g %%"%(test_accuracy*100))</pre>
<p class="mce-root">When we run the Python script, the output on the console is as follows:</p>
<pre class="mce-root">Step 0, training batch accuracy 13 % <br/>Step 100, training batch accuracy 80 % <br/>Step 200, training batch accuracy 87 % <br/>Step 300, training batch accuracy 81 % <br/>Step 400, training batch accuracy 86 % <br/>Step 500, training batch accuracy 85 % <br/>Step 600, training batch accuracy 89 % <br/>Step 700, training batch accuracy 90 % <br/>Step 800, training batch accuracy 94 % <br/>Step 900, training batch accuracy: 89.49 % <br/>Test accuracy 91 %</pre>
<p class="mce-root">Now, we have arrived at an accuracy rate of 89.2%. When we try to optimize our results more, the accuracy level reduces; this is where we set have our threshold value to stop the training.</p>
<p class="mce-root">Let's build the TensorFlow model for the MNIST dataset. Inside the TensorFlow framework, the scripts that are provided save the MNIST dataset into a TensorFlow (<kbd>.pb</kbd>) model. The same script is attached to this application's repository. </p>
<div class="mce-root packt_infobox">The code for this application can be found at <a href="https://github.com/intrepidkarthi/AImobileapps" target="_blank">https://github.com/intrepidkarthi/AImobileapps</a>.</div>
<p class="mce-root">We will begin by training the model using the following Python code line:</p>
<pre class="mce-root"><strong>$:python mnist.py</strong> </pre>
<p class="mce-root">We will now run the script to generate our model.</p>
<p class="mce-root">The following script helps us export the model by adding some additional parameters:</p>
<pre class="mce-root"><strong>python mnist.py --export_dir /./mnist_model</strong> </pre>
<p class="mce-root">The saved model can be found in the time stamped directory under <kbd>/./mnist_mode1/</kbd> (for example, <kbd>/./mnist_model/1536628294/</kbd>).</p>
<p class="mce-root">The obtained TensorFlow model will be converted into a TensorFlow Lite model using <kbd>toco</kbd>, as follows:</p>
<pre class="mce-root">toco \ <br/>--input_format=TENSORFLOW_GRAPHDEF <br/>--output_format=TFLITE \ <br/>--output_file=./mnist.tflite \<br/>--inference_type=FLOAT \ <br/>--input_type=FLOAT <br/>--input_arrays=x \ <br/>--output_arrays=output <br/>--input_shapes=1,28,28,1 \ <br/>--graph_def_file=./mnist.pb </pre>
<p>Toco is a command-line tool that's used to run the <strong>TensorFlow Lite Optimizing Converter</strong> (<strong>TOCO</strong>), which converts a TensorFlow model into a TensorFlow Lite model. The preceding <kbd>toco</kbd> command produces <kbd>mnist.tflite</kbd> as its output, which we will use in our application in the next section. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the Android application</h1>
                </header>
            
            <article>
                
<p>Let's create the Android application step-by-step with the model that we have built. We will start by creating a new project in Android Studio:</p>
<ol>
<li>Create a new application in Android Studio:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-844 image-border" src="assets/04420888-dc76-451a-b2c2-5d0dc5fed348.png" style="width:42.33em;height:28.75em;"/></p>
<ol start="2">
<li class="CDPAlignLeft CDPAlign">Drag the created TensorFlow Lite model to the <kbd>assets</kbd> folder, along with the <kbd>labels.txt</kbd> file. We will read the model and label from the assets folder:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8a178f84-13a2-418e-a617-a881a3e56cae.png" style="width:23.17em;height:35.67em;"/></p>
<p>The preceding screenshot shows the file structure in the project. If necessary, we can store the model file inside the secondary memory storage as well.</p>
<p>One of the advantages of <span class="packt_screen">FreeHandView</span> is that we can create a simple view where users can draw any number of digits. In addition to this, the bar chart on the screen will show the classification of the detected number.</p>
<p>We will use a step-by-step procedure to create the classifier.</p>
<p class="mce-root"/>
<p>Here is the <kbd>FreeHandView</kbd> constructor method that we will use to draw the digits. We initialize the <kbd>Paint</kbd> object with the necessary parameters, as follows:</p>
<pre>public FreeHandView(Context context, AttributeSet attrs) { <br/> super(context, attrs); <br/> mPaint = new Paint(); <br/> mPaint.setAntiAlias(true); <br/> mPaint.setDither(true); <br/> mPaint.setColor(DEFAULT_COLOR); <br/> mPaint.setStyle(Paint.Style.STROKE); <br/> mPaint.setStrokeJoin(Paint.Join.ROUND); <br/> mPaint.setStrokeCap(Paint.Cap.ROUND); <br/> mPaint.setXfermode(null); mPaint.setAlpha(Oxff); <br/> mEmboss = new EmbossMaskFilter(new float[] I1, 1, 1}, 0.4f, 6, 3.5f); <br/> mBlur = new BlurMaskFilter(5, BlurMaskFilter.Blur.NORMAL); <br/>}</pre>
<p>The functions of each parameter that was used in the preceding code block are explained as follows:</p>
<ul>
<li class="mce-root"><kbd>mPaint.setAntiAlias(true)</kbd>: A helper for <kbd>setFlags()</kbd>, setting or clearing the <kbd>ANTI_ALIAS_FLAG</kbd> bit. Antialiasing smooths out the edges of what is being drawn, but it has no impact on the interior of the shape.</li>
<li class="mce-root"><kbd>mPaint.setDither(true)</kbd>: A helper for <kbd>setFlags()</kbd>, setting or clearing the <kbd>DITHER_FLAG</kbd> bit. Dithering affects how colors that are higher precision than the device are down-sampled.</li>
<li class="mce-root"><kbd>mPaint.setColor(DEFAULT_COLOR)</kbd>: Sets the paint's color.</li>
<li class="mce-root"><kbd>mPaint.setStyle(Paint.Style.STROKE)</kbd>: Sets the paint's style, used for controlling how primitives' geometries are interpreted (except for <kbd>drawBitmap</kbd>, which always assumes <kbd>Fill</kbd>).</li>
<li class="mce-root"><kbd>mPaint.setStrokeJoin(Paint.Join.ROUND)</kbd>: Sets the paint's <kbd>Join</kbd>.</li>
<li class="mce-root"><kbd>mPaint.setStrokeCap(Paint.Cap.ROUND)</kbd>: Sets the paint's <kbd>Cap</kbd>.</li>
<li class="mce-root"><kbd>mPaint.setXfermode(null)</kbd>: Sets or clears the transfer mode object.</li>
<li class="mce-root"><kbd>mPaint.setAlpha(Oxff)</kbd>: A helper to <kbd>setColor()</kbd>, that only assigns the color's <kbd>alpha</kbd> value, leaving its <kbd>r</kbd>, <kbd>g</kbd>, and <kbd>b</kbd> values unchanged.</li>
</ul>
<p class="mce-root">Inside the <kbd>init()</kbd> method of the view life cycle, we will initialize the <kbd>ImageClassifier</kbd>, and pass on the <kbd>BarChart</kbd> object:</p>
<pre class="mce-root">public void init(DisplayMetrics metrics, ImageClassifier classifier, BarChart barChart) { <br/> int height = 1000; <br/> int width = 1000; <br/> mBitmap = Bitmap.createBitmap(width, height, Bitmap.Config.ARGB_8888); <br/> mCanvas = new Canvas(mBitmap); <br/> currentColor = DEFAULT_COLOR; strokeWidth = BRUSH_SIZE;<br/> mClassifier = classifier; <br/> this.predictionBar = predictionBar; <br/> this.barChart = barChart; addValuesToBarEntryLabels(); <br/>}</pre>
<div class="mce-root packt_infobox">We will use the chart from the following library: <a href="https://github.com/PhilJay/MPAndroidChart">https://github.com/PhilJay/MPAndroidChart</a>.</div>
<p class="mce-root">We will initialize the <kbd>BarChart</kbd> view, with the <em>x</em> axis containing numbers from zero to nine and the <em>y</em> axis containing the probability value from 0 to 1.0:</p>
<pre class="mce-root">BarChart barChart = (BarChart) findViewByld(R.id.barChart); <br/> barChart.animateY(3000); <br/> barChart.getXAxis().setEnabled(true); <br/> barChart.getAxisRight().setEnabled(false); <br/> barChart.getAxisLeft().setAxisMinimum(0.0f); // start at zero <br/> barChart.getAxisLeft().setAxisMaximum(1.0f); // the axis maximum is 100 <br/> barChart.getDescription().setEnabled(false); <br/> barChart.getLegend().setEnabled(false);<br/>// the labels that should be drawn on the X-Axis final String[] barLabels = new String[]{"0", "1", "2", "3", "4", "5", "6", n7,1, 118n, n9,1}; <br/><br/>//To format the value as integers<br/>IAxisValueFormatter formatter = new IAxisValueFormatter() {<br/>@Override public String getFormattedValue(float value, AxisBase axis) { <br/> return barLabels[(int) value); }<br/>};<br/> barChart.getXAxis().setGranularity(0f); // minimum axis-step (interval) is 1 <br/> barChart.getXAxis().setValueFormatter(formatter); <br/> barChart.getXAxis().setPosition(XAxis.XAxisPosition.BOTTOM); <br/> barChart.getXAxis().setTextSize(5f);</pre>
<p class="mce-root">Once we have initialized the <kbd>BarChart</kbd> view, we will call the <kbd>OnDraw()</kbd> method of the view life cycle, which applies strokes in accordance with the path of the user's finger movements. The <kbd>OnDraw()</kbd> method is called as part of the view life cycle method once the <kbd>BarChart</kbd> view is initialized.</p>
<p class="mce-root">Inside the <kbd>OnDraw</kbd> method, we will track the finger movement of the user, and the same movements will be drawn on the canvas, as follows:</p>
<pre>@Override protected void onDraw(Canvas canvas) {<br/> canvas.save(); <br/> mCanvas.drawColor(backgroundColor); <br/> for (FingerPath fp : paths) { <br/> mPaint.setColor(fp.color); <br/> mPaint.setStrokeWidth(fp.strokeWidth); <br/> mPaint.setMaskFilter(null);<br/>    if (fp.emboss) <br/>     mPaint.setMaskFilter(mEmboss); <br/>    else if (fp.blur) <br/>     mPaint.setMaskFilter(mBlur);<br/>    mCanvas.drawPath(fp.path, mPaint); <br/> }<br/> canvas.drawBitmap(mBitmap, 0, 0, mBitmapPaint); canvas. restore(); <br/>}</pre>
<p class="mce-root">Inside the <kbd>onTouchEvent()</kbd> method, we can track the user's finger position using the move, up, and down events and initiate actions based upon that. This is one of the methods in the view's life cycle that's used to track events. There are three events that will be triggered when you touch your mobile based on finger movements. In the case of <kbd>action_down</kbd> and <kbd>action_move</kbd>, we will handle events to draw the on-hand movement on the view with the initial paint object attributes. When the <kbd>action_up</kbd> event is triggered, we will save the view into a file, as well as pass the file image to the classifier to identify the digit. After that, we will represent the probability values using the <kbd>BarChart</kbd> view. These steps are as follows:</p>
<pre class="mce-root">@Override public boolean onTouchEvent(MotionEvent event) { <br/> float x = event.getX(); <br/> float y = event.getY(); <br/> BarData exampleData; <br/> switch(event.getAction()) { <br/>     case MotionEvent.ACTION_DOWN : <br/>     touchStart(x, y); <br/>     invalidate(); <br/>     break; <br/>     case MotionEvent.ACTION_MOVE : <br/>     touchMove(x, y); <br/>     invalidate(); <br/>     break; <br/>     case MotionEvent.ACTION_UP : <br/>     touchUp();<br/>    Bitmap scaledBitmap = Bitmap.createScaledBitmap(mBitmap,         mClassifier.getImageSizeX(), mClassifier.getImageSizeY(), true); <br/>     Random rng = new Random(); <br/>     try { <br/>     File mFile; <br/>     mFile = this.getContext().getExternalFilesDir(String.valueOf (rng.nextLong() + ".png")); <br/>     FileOutputStream pngFile = new FileOutputStream(mFile); <br/>     } <br/>    catch (Exception e){ } <br/>     //scaledBitmap.compress(Bitmap.CompressFormat.PNG, 90, pngFile); <br/>     Float prediction = mClassifier.classifyFrame(scaledBitmap); <br/>     exampleData = updateBarEntry(); <br/>     barChart.animateY(1000, Easing.EasingOption.EaseOutQuad); <br/>     XAxis xAxis = barChart.getXAxis(); <br/>     xAxis.setValueFormatter(new IAxisValueFormatter() {<br/>     @Override public String getFormattedValue(float value, AxisBase axis) {<br/>         return xAxisLabel.get((int) value); <br/>     });<br/>    barChart.setData(exampleData); <br/>     exampleData.notifyDataChanged(); // let the data know a // dataset changed <br/>     barChart.notifyDataSetChanged(); // let the chart know it's // data changed <br/>     break; <br/>     } <br/> return true; <br/>}</pre>
<p class="mce-root">Inside the <kbd>ACTION_UP</kbd> action, there is a <kbd>updateBarEntry()</kbd> <span>method call.</span> This is where we call the classifier to get the probability of the result. This method also updates the <kbd>BarChart</kbd> view based on the results from the classifier, as follows:</p>
<pre class="mce-root"><span> public BarData updateBarEntry() { <br/>     ArrayList&lt;BarEntry&gt; mBarEntry = new ArrayList&lt;&gt;(); <br/>     for (int j = 0; j &lt; 10; ++j) { <br/>         mBarEntry.add(new BarEntry(j, mClassifier.getProbability(j))); <br/>     } <br/>     BarDataSet mBarDataSet = new BarDataSet(mBarEntry, "Projects"); <br/>     mBarDataSet.setColors(ColorTemplate.COLORFUL_COLORS); <br/>     BarData mBardData = new BarData(mBarDataSet); <br/>     return mBardData; <br/> }</span></pre>
<p class="mce-root">FreeHandView looks like this, along with an empty bar chart:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-845 image-border" src="assets/ad30f114-27ff-4b55-87b6-1fb2e54beb58.png" style="width:29.58em;height:52.83em;"/></p>
<p>With this, we will add the module to recognize the handwritten digits and then classify them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Digit classifier</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now, let's write the classifier.</p>
<ol>
<li class="mce-root">First, we will load the model file. This method reads the model from the assets folder and loads it into the memory:</li>
</ol>
<pre style="padding-left: 60px">/** Memory-map the model file in Assets. */ <br/>private MappedByteBuffer loadModelFile(Activity activity) throws <br/>I0Exception <br/>{ <br/>    AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(getModelPath()); <br/>    FilelnputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor()); <br/>    FileChannel fileChannel = inputStream.getChannel(); <br/>    long startOffset = fileDescriptor.getStartOffset(); <br/>    long declaredLength = fileDescriptor.getDeclaredLength();         return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength); <br/>}</pre>
<ol start="2">
<li class="mce-root">Now, let's write the TensorFlow Lite classifier, frame-by-frame. This is the place where we get the results from the digit classifier. Once we have received the saved file image as the user input, the bitmap will be converted into a byte buffer to run the inference on top of the model. Once we have received the output, the time taken to get the results are noted using the <kbd>SystemClock</kbd> time:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">/** Classifies a frame from the preview stream. */ <br/>public float classifyFrame(Bitmap bitmap) <br/>{ <br/>    if (tflite == null){ <br/>    Log.e(TAG, "classifier has not been initialized; Skipped.");     return 0.5f; <br/>    } <br/>convertBitmapToByteBuffer(bitmap); // Here's where the classification happens!!! <br/>long startTime = SystemClock.uptimeMillis(); <br/>runlnference(); <br/>long endTime = SystemClock.uptimeMillis(); <br/>Log.d(TAG, "Timecost to run model inference: " + Long.toString(endTime - startTime)); <br/>return getProbability(0); <br/>}</pre>
<p class="mce-root"/>
<ol start="3">
<li class="mce-root">The <kbd>runlnference()</kbd> method calls the <kbd>run</kbd> method from<kbd>tflite</kbd>, as follows:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">@Override <br/>protected void runlnference() <br/>{ <br/>    tflite.run(imgData, labelProbArray); <br/>}</pre>
<ol start="4">
<li class="mce-root">Next, let's start the application from <kbd>MainActivity</kbd>, where the <kbd>barChart</kbd> view is initialized. Initialize the <kbd>barChart</kbd> view on the <em>x</em> and <em>y</em> axis, along with the following values:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">BARENTRY = new ArrayList&lt;&gt;(); initializeBARENTRY();<br/>Bardataset = new BarDataSet(BARENTRY, "project"); <br/>BARDATA = new BarData(Bardataset); <br/>barChart.setData(BARDATA); </pre>
<ol start="5">
<li>Initialize FreeHandView to start classifying inside the <kbd>OnCreate()</kbd> method of <kbd>MainActivity</kbd>:</li>
</ol>
<pre style="padding-left: 60px">paintView.init(metrics, classifier, barChart);</pre>
<ol start="6">
<li>When you reach the probability value of 1.00, the algorithm identifies the digit with 100% accuracy. An example of this is shown here:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/21170abb-5da1-4041-ad38-d5ea86814376.png" style="width:28.92em;height:50.50em;"/></p>
<ol start="7">
<li>There are instances in which the classification decreases the probability with partial matches, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c7b2ae6e-4285-4a5d-94da-05abb224eee7.png" style="width:25.00em;height:43.00em;"/></p>
<ol start="8">
<li class="mce-root">There are also other instances where the probability ends up with multiple partial matches. An example of this is shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/60708fa3-3c98-4444-b041-9d322a25909a.png" style="width:28.08em;height:49.00em;"/></p>
<p style="padding-left: 60px"><span>Any such situation requires more rigorous training of the model.</span></p>
<ol start="9">
<li>Clicking on the <span class="packt_screen">RESET</span> button will clear up the view so that you can draw again. We will implement it using the following lines of code:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">resetButton.setOnClickListener(new View.OnClickListener() {<br/> public void onClick(View v) { <br/> paintView.clear(); <br/> }<br/>}) ;</pre>
<p>Once you click on the <span class="packt_screen">RESET</span> button, the preceding code clears up the FreeHandView area, as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/557caf09-525b-4553-8684-893d6776e15f.png" style="width:18.83em;height:33.17em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>You can also check that the application works properly by writing characters other than digits, and checking the performance of the output on the bar chart.</span></p>
<p>In this section, we learned how the application classifies the different digits that are hand-drawn, and also provides the probability of those digits being correct.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">Using this Android application, we can learn how to write a freehand writing classifier using TensorFlow Lite. With more data on handwritten alphabet datasets, we should be able to identify alphabets in any language using GANs.</p>
<p class="mce-root">In the next chapter, we will build a model for sentiment analysis and build an app on top of it.</p>


            </article>

            
        </section>
    </body></html>