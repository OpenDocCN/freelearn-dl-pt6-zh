["```py\n$SPARK_HOME/bin/spark-shell.sh\n```", "```py\nSpark context Web UI available at http://10.72.0.2:4040\nSpark context available as 'sc' (master = local[*], app id = local-1518131682342).\nSpark session available as 'spark'.\nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 2.2.1\n /_/\n\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala>\n```", "```py\nscala> spark.read.textFile(\"/usr/spark-2.2.1/examples/src/main/resources/people.txt\")\nres5: org.apache.spark.sql.Dataset[String] = [value: string]\n```", "```py\nscala> res5.count()\nres6: Long = 3\n```", "```py\nscala> res5.first()\nres7: String = Michael, 29\n```", "```py\n:quit\n```", "```py\nscala> :help\n```", "```py\n$SPARK_HOME/bin/pyspark.sh\n```", "```py\n>>> textFileDf = spark.read.text(\"/usr/spark-2.2.1/examples/src/main/resources/people.txt\")\n>>> textFileDf.count()\n3\n>>> textFileDf.first()\nRow(value='Michael, 29')\n```", "```py\nquit()\n```", "```py\nimport org.apache.spark.sql.SparkSession\n\nobject SimpleApp {\n  def main(args: Array[String]) {\n    val logFile = \"/usr/spark-2.2.1/examples/src/main/resources/people.txt\"\n    val spark = SparkSession.builder.master(\"local\").appName(\"Simple Application\").getOrCreate()\n    val logData = spark.read.textFile(logFile).cache()\n    val numAs = logData.filter(line => line.contains(\"a\")).count()\n    val numBs = logData.filter(line => line.contains(\"b\")).count()\n    println(s\"Lines with a: $numAs, Lines with b: $numBs\")\n    spark.stop()\n  }\n}\n```", "```py\nval spark = SparkSession.builder.master(\"local\").appName(\"Simple Application\").getOrCreate()\n```", "```py\nspark.stop()\n```", "```py\nfrom pyspark.sql import SparkSession\n\nlogFile = \"YOUR_SPARK_HOME/README.md\"  # Should be some file on your system\nspark = SparkSession.builder().appName(appName).master(master).getOrCreate()\nlogData = spark.read.text(logFile).cache()\n\nnumAs = logData.filter(logData.value.contains('a')).count()\nnumBs = logData.filter(logData.value.contains('b')).count()\n\nprint(\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))\n\nspark.stop()\n```", "```py\nscala> val spiderman = sc.textFile(\"/usr/spark-2.2.1/tests/spiderman.txt\")\nspiderman: org.apache.spark.rdd.RDD[String] = /usr/spark-2.2.1/tests/spiderman.txt MapPartitionsRDD[1] at textFile at <console>:24\n```", "```py\nscala> val topWordCount = spiderman.flatMap(str=>str.split(\" \")).filter(!_.isEmpty).map(word=>(word,1)).reduceByKey(_+_).map{case(word, count) => (count, word)}.sortByKey(false)\ntopWordCount: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[9] at sortByKey at <console>:26\n```", "```py\nscala> topWordCount.take(5).foreach(x=>println(x))\n(34,the)\n(28,and)\n(19,of)\n(19,in)\n(16,Spider-Man)\n```", "```py\nfrom operator import add\nspiderman = spark.read.text(\"/usr/spark-2.2.1/tests/spiderman.txt\")\nlines = spiderman.rdd.map(lambda r: r[0])\ncounts = lines.flatMap(lambda x: x.split(' ')) \\\n                  .map(lambda x: (x, 1)) \\\n                  .reduceByKey(add) \\\n                  .map(lambda x: (x[1],x[0])) \\\n                  .sortByKey(False)\n```", "```py\n>> counts.take(5)\n[(34, 'the'), (28, 'and'), (19, 'in'), (19, 'of'), (16, 'Spider-Man')]\n```", "```py\nscala> val broadcastVar = sc.broadcast(Array(1, 2, 3))\nbroadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)\n\nscala> broadcastVar.value\nres0: Array[Int] = Array(1, 2, 3)\n```", "```py\n>>> broadcastVar = sc.broadcast([1, 2, 3])\n <pyspark.broadcast.Broadcast object at 0x102789f10>\n\n >>> broadcastVar.value\n [1, 2, 3]\n```", "```py\nscala> val accum = sc.longAccumulator(\"First Long Accumulator\")\naccum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some\n(First Long Accumulator), value: 0)\n\nscala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum.add(x))\n[Stage 0:>                                                          (0 + 0) / 8]\n\nscala> accum.value\nres1: Long = 10\n```", "```py\n>>> accum = sc.accumulator(0)\n>>> accum\nAccumulator<id=0, value=0>\n\n>>> sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))\n>>> accum.value\n10\n```", "```py\nval df = spark.read.json(\"/opt/spark/spark-2.2.1-bin-hadoop2.7/examples/src/main/resources/people.json\")\n```", "```py\nscala> df.show()\n+----+-------+\n| age| name|\n+----+-------+\n|null|Michael|\n| 30| Andy|\n| 19| Justin|\n+----+-------+\n```", "```py\nimport spark.implicits._\n```", "```py\nscala> df.printSchema()\nroot\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n```", "```py\nscala> df.select(\"name\").show()\n+-------+\n|   name|\n+-------+\n|Michael|\n|   Andy|\n| Justin|\n+-------+\n```", "```py\nscala> df.filter($\"age\" > 27).show()\n+---+----+\n|age|name|\n+---+----+\n| 30|Andy|\n+---+----+\n```", "```py\nscala> df.groupBy(\"age\").count().show()\n+----+-----+\n| age|count|\n+----+-----+\n|  19|    1|\n|null|    1|\n|  30|    1|\n+----+-----+\n```", "```py\nscala> df.select($\"name\", $\"age\" + 1).show()\n+-------+---------+\n| name|(age + 1)|\n+-------+---------+\n|Michael| null|\n| Andy| 31|\n| Justin| 20|\n+-------+---------+\n```", "```py\nval df = spark.read.json(\"/opt/spark/spark-2.2.1-bin-hadoop2.7/examples/src/main/resources/people.json\")\n```", "```py\ndf.createOrReplaceTempView(\"people\")\n```", "```py\nscala> val sqlDF = spark.sql(\"SELECT * FROM people\")\nsqlDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]\n\nscala> sqlDF.show()\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n```", "```py\n>>> df = spark.read.json(\"/opt/spark/spark-2.2.1-bin-hadoop2.7/examples/src/main/resources/people.json\")\n```", "```py\n>> df.show()\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n>>> df.printSchema()\nroot\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n>>> df.select(\"name\").show()\n+-------+\n|   name|\n+-------+\n|Michael|\n|   Andy|\n| Justin|\n+-------+\n\n>>> df.filter(df['age'] > 21).show()\n+---+----+\n|age|name|\n+---+----+\n| 30|Andy|\n+---+----+\n\n>>> df.groupBy(\"age\").count().show()\n+----+-----+\n| age|count|\n+----+-----+\n|  19|    1|\n|null|    1|\n|  30|    1|\n+----+-----+\n\n>>> df.select(df['name'], df['age'] + 1).show()\n+-------+---------+\n|   name|(age + 1)|\n+-------+---------+\n|Michael|     null|\n|   Andy|       31|\n| Justin|       20|\n+-------+---------+\n\n>>> df.createOrReplaceTempView(\"people\")\n>>> sqlDF = spark.sql(\"SELECT * FROM people\")\n>>> sqlDF.show()\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n```", "```py\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nval sparkConf = new SparkConf().setAppName(\"NetworkWordCount\").setMaster(\"local[*]\")\n  val ssc = new StreamingContext(sparkConf, Seconds(1))\n\n```", "```py\nval lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)\n```", "```py\nval words = lines.flatMap(_.split(\" \"))\n```", "```py\nval words = lines.flatMap(_.split(\" \"))\n val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)\nwordCounts.print()\n```", "```py\nssc.start()\n ssc.awaitTermination()\n```", "```py\nnc -lk 9999\n```", "```py\nlocalhost 9999\n```", "```py\nimport java.io.DataOutputStream\nimport java.net.{ServerSocket, Socket}\nimport java.util.Scanner\n\nobject SocketWriter {\n  def main(args: Array[String]) {\n    val listener = new ServerSocket(9999)\n    val socket = listener.accept()\n\n    val outputStream = new DataOutputStream(socket.getOutputStream())\n    System.out.println(\"Start writing data. Enter close when finish\");\n    val sc = new Scanner(System.in)\n    var str = \"\"\n    /**\n     * Read content from scanner and write to socket.\n     */\n    while (!(str = sc.nextLine()).equals(\"close\")) {\n        outputStream.writeUTF(str);\n    }\n    //close connection now.\n    outputStream.close()\n    listener.close()\n  }\n}\n```", "```py\nfrom __future__ import print_function\n\nimport sys\n\nfrom pyspark import SparkContext\nfrom pyspark.streaming import StreamingContext\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: network_wordcount.py <hostname> <port>\", file=sys.stderr)\n        exit(-1)\n    sc = SparkContext(appName=\"PythonStreamingNetworkWordCount\")\n    ssc = StreamingContext(sc, 1)\n\n    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))\n    counts = lines.flatMap(lambda line: line.split(\" \"))\\\n                  .map(lambda word: (word, 1))\\\n                  .reduceByKey(lambda a, b: a+b)\n    counts.pprint()\n\n    ssc.start()\n    ssc.awaitTermination()\n```", "```py\nval windowedWordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) => (a + b), Seconds(60), Seconds(10))\n```", "```py\nwindowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 60, 10)\n```", "```py\nspark.master                     spark://<master_hostname_or_IP>:7077\n```", "```py\nSPARK_MASTER_HOST,               <master_hostname_or_IP>\n```", "```py\n$SPARK_HOME/sbin/start-master.sh\n```", "```py\n$SPARK_HOME/sbin/start-slave.sh <master-spark-URL>\n```", "```py\n$SPARK_HOME/bin/spark-shell --master <master-spark-URL>\n```", "```py\n$SPARK_HOME/bin/spark-shell --master mesos://127.0.0.1:5050 -c spark.mesos.executor.home=`pwd`\n```", "```py\n$SPARK_HOME/bin/spark-submit --master mesos://127.0.0.1:5050 --total-executor-cores 2 --executor-memory 3G  $SPARK_HOME/examples/src/main/python/pi.py 100\n```", "```py\n$SPARK_HOME/bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] <app jar> [app options]\n```", "```py\n$SPARK_HOME/bin/docker-image-tool.sh -r <repo> -t my-tag build\n```", "```py\n$SPARK_HOME/bin/docker-image-tool.sh -r <repo> -t my-tag push\n```", "```py\n$SPARK_HOME/bin/spark-submit \\\n --master k8s://https://<k8s_hostname>:<k8s_port> \\\n --deploy-mode cluster \\\n --name <application-name> \\\n --class <package>.<ClassName> \\\n --conf spark.executor.instances=<instance_count> \\\n --conf spark.kubernetes.container.image=<spark-image> \\\n local:///path/to/<sparkjob>.jar\n```"]