["```py\n// construct RBM\nRestrictedBoltzmannMachines nn = new RestrictedBoltzmannMachines(nVisible, nHidden, null, null, null, rng);\n```", "```py\n   if (W == null) {\n\n       W = new double[nHidden][nVisible];\n       double w_ = 1\\. / nVisible;\n\n       for (int j = 0; j < nHidden; j++) {\n           for (int i = 0; i < nVisible; i++) {\n               W[j][i] = uniform(-w_, w_, rng);\n           }\n       }\n   }\n\n   if (hbias == null) {\n       hbias = new double[nHidden];\n\n       for (int j = 0; j < nHidden; j++) {\n           hbias[j] = 0.;\n       }\n   }\n\n   if (vbias == null) {\n       vbias = new double[nVisible];\n\n       for (int i = 0; i < nVisible; i++) {\n           vbias[i] = 0.;\n       }\n   }\n```", "```py\n// train with contrastive divergence\nfor (int epoch = 0; epoch < epochs; epoch++) {\n   for (int batch = 0; batch < minibatch_N; batch++) {\n       nn.contrastiveDivergence(train_X_minibatch[batch], minibatchSize, learningRate, 1);\n   }\n   learningRate *= 0.995;\n}\n```", "```py\n// CD-k : CD-1 is enough for sampling (i.e. k = 1)\nsampleHgivenV(X[n], phMean_, phSample_);\n\nfor (int step = 0; step < k; step++) {\n\n   // Gibbs sampling\n   if (step == 0) {\n       gibbsHVH(phSample_, nvMeans_, nvSamples_, nhMeans_, nhSamples_);\n   } else {\n       gibbsHVH(nhSamples_, nvMeans_, nvSamples_, nhMeans_, nhSamples_);\n   }\n\n}\n```", "```py\npublic void gibbsHVH(int[] h0Sample, double[] nvMeans, int[] nvSamples, double[] nhMeans, int[] nhSamples) {\n   sampleVgivenH(h0Sample, nvMeans, nvSamples);\n   sampleHgivenV(nvSamples, nhMeans, nhSamples);\n}\n```", "```py\npublic void sampleHgivenV(int[] v0Sample, double[] mean, int[] sample) {\n\n   for (int j = 0; j < nHidden; j++) {\n       mean[j] = propup(v0Sample, W[j], hbias[j]);\n       sample[j] = binomial(1, mean[j], rng);\n   }\n\n}\n\npublic void sampleVgivenH(int[] h0Sample, double[] mean, int[] sample) {\n\n   for(int i = 0; i < nVisible; i++) {\n       mean[i] = propdown(h0Sample, i, vbias[i]);\n       sample[i] = binomial(1, mean[i], rng);\n   }\n}\n```", "```py\npublic double propup(int[] v, double[] w, double bias) {\n\n   double preActivation = 0.;\n\n   for (int i = 0; i < nVisible; i++) {\n       preActivation += w[i] * v[i];\n   }\n   preActivation += bias;\n\n   return sigmoid(preActivation);\n}\n\npublic double propdown(int[] h, int i, double bias) {\n\n   double preActivation = 0.;\n\n   for (int j = 0; j < nHidden; j++) {\n       preActivation += W[j][i] * h[j];\n   }\n   preActivation += bias;\n\n   return sigmoid(preActivation);\n}\n```", "```py\npublic static int binomial(int n, double p, Random rng) {\n   if(p < 0 || p > 1) return 0;\n\n   int c = 0;\n   double r;\n\n   for(int i=0; i<n; i++) {\n       r = rng.nextDouble();\n       if (r < p) c++;\n   }\n\n   return c;\n}\n```", "```py\n// calculate gradients\nfor (int j = 0; j < nHidden; j++) {\n   for (int i = 0; i < nVisible; i++) {\n       grad_W[j][i] += phMean_[j] * X[n][i] - nhMeans_[j] * nvSamples_[i];\n   }\n\n   grad_hbias[j] += phMean_[j] - nhMeans_[j];\n}\n\nfor (int i = 0; i < nVisible; i++) {\n   grad_vbias[i] += X[n][i] - nvSamples_[i];\n}\n\n// update params\nfor (int j = 0; j < nHidden; j++) {\n   for (int i = 0; i < nVisible; i++) {\n       W[j][i] += learningRate * grad_W[j][i] / minibatchSize;\n   }\n\n   hbias[j] += learningRate * grad_hbias[j] / minibatchSize;\n}\n\nfor (int i = 0; i < nVisible; i++) {\n   vbias[i] += learningRate * grad_vbias[i] / minibatchSize;\n}\n```", "```py\npublic double[] reconstruct(int[] v) {\n\n   double[] x = new double[nVisible];\n   double[] h = new double[nHidden];\n\n   for (int j = 0; j < nHidden; j++) {\n       h[j] = propup(v, W[j], hbias[j]);\n   }\n\n   for (int i = 0; i < nVisible; i++) {\n       double preActivation_ = 0.;\n\n       for (int j = 0; j < nHidden; j++) {\n           preActivation_ += W[j][i] * h[j];\n       }\n       preActivation_ += vbias[i];\n\n       x[i] = sigmoid(preActivation_);\n   }\n\n   return x;\n}\n```", "```py\n// construct DBN\nSystem.out.print(\"Building the model...\");\nDeepBeliefNets classifier = new DeepBeliefNets(nIn, hiddenLayerSizes, nOut, rng);\nSy\nstem.out.println(\"done.\");\n```", "```py\n// construct multi-layer\nfor (int i = 0; i < nLayers; i++) {\n   int nIn_;\n   if (i == 0) nIn_ = nIn;\n   else nIn_ = hiddenLayerSizes[i-1];\n\n   // construct hidden layers with sigmoid function\n   //   weight matrices and bias vectors will be shared with RBM layers\n   sigmoidLayers[i] = new HiddenLayer(nIn_, hiddenLayerSizes[i], null, null, rng, \"sigmoid\");\n\n   // construct RBM layers\n   rbmLayers[i] = new RestrictedBoltzmannMachines(nIn_, hiddenLayerSizes[i], sigmoidLayers[i].W, sigmoidLayers[i].b, null, rng);\n}\n\n// logistic regression layer for output\nlogisticLayer = new LogisticRegression(hiddenLayerSizes[nLayers-1], nOut);\n```", "```py\n// pre-training the model\nSystem.out.print(\"Pre-training the model...\");\nclassifier.pretrain(train_X_minibatch, minibatchSize, train_minibatch_N, pretrainEpochs, pretrainLearningRate, k);\nSystem.out.println(\"done.\");\n```", "```py\npublic void pretrain(int[][][] X, int minibatchSize, int minibatch_N, int epochs, double learningRate, int k) {\n\n   for (int layer = 0; layer < nLayers; layer++) {  // pre-train layer-wise\n       for (int epoch = 0; epoch < epochs; epoch++) {\n           for (int batch = 0; batch < minibatch_N; batch++) {\n\n               int[][] X_ = new int[minibatchSize][nIn];\n               int[][] prevLayerX_;\n\n               // Set input data for current layer\n               if (layer == 0) {\n                   X_ = X[batch];\n               } else {\n\n                   prevLayerX_ = X_;\n                   X_ = new int[minibatchSize][hiddenLayerSizes[layer-1]];\n\n                   for (int i = 0; i < minibatchSize; i++) {\n                       X_[i] = sigmoidLayers[layer-1].outputBinomial(prevLayerX_[i], rng);\n                   }\n               }\n\n               rbmLayers[layer].contrastiveDivergence(X_, minibatchSize, learningRate, k);\n           }\n       }\n   }\n\n}\n```", "```py\n// fine-tuning the model\nSystem.out.print(\"Fine-tuning the model...\");\nfor (int epoch = 0; epoch < finetuneEpochs; epoch++) {\n   for (int batch = 0; batch < validation_minibatch_N; batch++) {\n       classifier.finetune(validation_X_minibatch[batch], validation_T_minibatch[batch], minibatchSize, finetuneLearningRate);\n   }\n   finetuneLearningRate *= 0.98;\n}\nSystem.out.println(\"done.\");\n```", "```py\npublic void finetune(double[][] X, int[][] T, int minibatchSize, double learningRate) {\n\n   List<double[][]> layerInputs = new ArrayList<>(nLayers + 1);\n   layerInputs.add(X);\n\n   double[][] Z = new double[0][0];\n   double[][] dY;\n\n   // forward hidden layers\n   for (int layer = 0; layer < nLayers; layer++) {\n\n       double[] x_;  // layer input\n       double[][] Z_ = new double[minibatchSize][hiddenLayerSizes[layer]];\n\n       for (int n = 0; n < minibatchSize; n++) {\n\n           if (layer == 0) {\n               x_ = X[n];\n           } else {\n               x_ = Z[n];\n           }\n\n           Z_[n] = sigmoidLayers[layer].forward(x_);\n       }\n\n       Z = Z_.clone();\n       layerInputs.add(Z.clone());\n   }\n\n   // forward & backward output layer\n   dY = logisticLayer.train(Z, T, minibatchSize, learningRate);\n\n   // backward hidden layers\n   double[][] Wprev;\n   double[][] dZ = new double[0][0];\n\n   for (int layer = nLayers - 1; layer >= 0; layer--) {\n\n       if (layer == nLayers - 1) {\n           Wprev = logisticLayer.W;\n       } else {\n           Wprev = sigmoidLayers[layer+1].W;\n           dY = dZ.clone();\n       }\n\n       dZ = sigmoidLayers[layer].backward(layerInputs.get(layer), layerInputs.get(layer+1), dY, Wprev, minibatchSize, learningRate);\n   }\n}\n```", "```py\npublic Integer[] predict(double[] x) {\n\n   double[] z = new double[0];\n\n   for (int layer = 0; layer < nLayers; layer++) {\n\n       double[] x_;\n\n       if (layer == 0) {\n           x_ = x;\n       } else {\n           x_ = z.clone();\n       }\n\n       z = sigmoidLayers[layer].forward(x_);\n   }\n\n   return logisticLayer.predict(z);\n}\n```", "```py\ndouble corruptionLevel = 0.3;\n```", "```py\n// construct DA\nDenoisingAutoencoders nn = new DenoisingAutoencoders(nVisible, nHidden, null, null, null, rng);\n\n// train\nfor (int epoch = 0; epoch < epochs; epoch++) {\n   for (int batch = 0; batch < minibatch_N; batch++) {\n       nn.train(train_X_minibatch[batch], minibatchSize, learningRate, corruptionLevel);\n   }\n}\n```", "```py\n// add noise to original inputs\ndouble[] corruptedInput = getCorruptedInput(X[n], corruptionLevel);\n\n// encode\ndouble[] z = getHiddenValues(corruptedInput);\n\n// decode\ndouble[] y = getReconstructedInput(z);\n```", "```py\npublic double[] getCorruptedInput(double[] x, double corruptionLevel) {\n\n   double[] corruptedInput = new double[x.length];\n\n   // add masking noise\n   for (int i = 0; i < x.length; i++) {\n       double rand_ = rng.nextDouble();\n\n       if (rand_ < corruptionLevel) {\n           corruptedInput[i] = 0.;\n       } else {\n           corruptedInput[i] = x[i];\n       }\n   }\n\n   return corruptedInput;\n}\n```", "```py\n// calculate gradients\n\n// vbias\ndouble[] v_ = new double[nVisible];\n\nfor (int i = 0; i < nVisible; i++) {\n   v_[i] = X[n][i] - y[i];\n   grad_vbias[i] += v_[i];\n}\n\n// hbias\ndouble[] h_ = new double[nHidden];\n\nfor (int j = 0; j < nHidden; j++) {\n\n   for (int i = 0; i < nVisible; i++) {\n       h_[j] = W[j][i] * (X[n][i] - y[i]);\n   }\n\n   h_[j] *= z[j] * (1 - z[j]);\n   grad_hbias[j] += h_[j];\n}\n\n// W\nfor (int j = 0; j < nHidden; j++) {\n   for (int i = 0; i < nVisible; i++) {\n       grad_W[j][i] += h_[j] * corruptedInput[i] + v_[i] * z[j];\n   }\n}\n```", "```py\npublic double[] reconstruct(double[] x) {\n\n   double[] z = getHiddenValues(x);\n   double[] y = getReconstructedInput(z);\n\n   return y;\n}\n```", "```py\npublic void pretrain(double[][][] X, int minibatchSize, int minibatch_N, int epochs, double learningRate, double corruptionLevel) {\n\n   for (int layer = 0; layer < nLayers; layer++) {\n       for (int epoch = 0; epoch < epochs; epoch++) {\n           for (int batch = 0; batch < minibatch_N; batch++) {\n\n               double[][] X_ = new double[minibatchSize][nIn];\n               double[][] prevLayerX_;\n\n               // Set input data for current layer\n               if (layer == 0) {\n                   X_ = X[batch];\n               } else {\n\n                 prevLayerX_ = X_;\n                 X_ = new double[minibatchSize][hiddenLayerSizes[layer-1]];\n\n                   for (int i = 0; i < minibatchSize; i++) {\n                       X_[i] = sigmoidLayers[layer-1].output(prevLayerX_[i]);\n                   }\n               }\n\n               daLayers[layer].train(X_, minibatchSize, learningRate, corruptionLevel);\n           }\n       }\n   }\n\n}\n```"]