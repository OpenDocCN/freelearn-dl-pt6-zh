["```py\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Dropout\nfrom keras.layers.core import Activation\nfrom keras import backend as K\n```", "```py\nfrom keras.datasets import mnist\n(x_train, y_train),(x_test, y_test)= fashion_mnist.load_data()\n```", "```py\ntype(x_train[0]),x_train.shape,y_train.shape\n```", "```py\n(numpy.ndarray, (60000, 28, 28), (60000,))\n\n```", "```py\nimport matplotlib.-pyplot as plt\n%matplotlib inline\nplt.show(x_train[0], cmap= plt.cm.binary)\n<matplotlib.image.AxesImage at 0x24b7f0fa3c8>\n```", "```py\nx_train=keras.utils.normalize(x_train, axis=1)\nx_test=keras.utils.normalize(x_test, axis=1)\nplt.imshow(x_train[0], cmap=plt.cm.binary)\n```", "```py\n<matplotlib.image.AxesImage at 0x24b00003e48>\n```", "```py\n#Simple Feedforward Neural Network\nmodel = Sequential()\n\n#feeds in the image composed of 28  28 a pixel matrix as one sequence   \n of 784\nmodel.add(Flatten(input_shape=(28,28)))\nmodel.add(Dense(24, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n```", "```py\n#feeds in the image composed of 2828 as one sequence of 784\nmodel.add(Flatten(input_shape=(28,28)))\nmodel.add(Dense(64, activation='relu',   \n          kernel_initializer='glorot_uniform',   \n          bias_initializer='zeros'))\nmodel.add(Dense(18, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n```", "```py\nkeras.utils.print_summary(model, line_length=None, positions=None,     \n                          print_fn=None)\n```", "```py\nmodel.summary()\n```", "```py\n_________________________________________________________________\nLayer (type) Output Shape Param # \n=================================================================\nflatten_2 (Flatten) (None, 784) 0 \n_________________________________________________________________\ndense_4 (Dense) (None, 1024) 803840 \n_________________________________________________________________\ndense_5 (Dense) (None, 28) 28700 \n_________________________________________________________________\ndense_6 (Dense) (None, 10) 290 \n=================================================================\nTotal params: 832,830\nTrainable params: 832,830\nNon-trainable params: 0\n_________________________________________________________________\n```", "```py\nmodel.compile(optimizer='resprop', #'sgd'\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n```", "```py\nmodel.fit(x_train, y_train, epochs=5, batch_size = 2) #other arguments   \n                            validation split=0.33, batch_size=10\n```", "```py\nEpoch 1/5\n60000/60000 [==========] - 12s 192us/step - loss: 0.3596 - acc: 0.9177\nEpoch 2/5\n60000/60000 [==========] - 10s 172us/step - loss: 0.1822 - acc: 0.9664\nEpoch 3/5\n60000/60000 [==========] - 10s 173us/step - loss: 0.1505 - acc: 0.9759\nEpoch 4/5\n60000/60000 [==========] - 11s 177us/step - loss: 0.1369 - acc:  \n                           0.97841s - loss: \nEpoch 5/5\n60000/60000 [==========] - 11s 175us/step - loss: 0.1245 - acc: 0.9822\n```", "```py\nmodel.evaluation(x_test, y_test)\n\n10000/10000 [==============================] - 1s 98us/step\n[0.1425468367099762, 0.9759]\n```", "```py\npredictions= load_model.predict([x_test])\n\n#predict use the inference graph generated in the model to predict class labels on our test set\n#print maximum value for prediction of x_test subject no. 110)\n\nimport numpy as np\nprint(np.argmax(predictions[110]))\n-------------------------------------------\n8\n------------------------------------------\nplt.imshow(x_test[110]))\n<matplotlib.image.AxesImage at 0x174dd374240>\n\n```", "```py\nmodel.save('mnist_nn.model')\nload_model=kera.models.load_model('mnist_nn.model')\n```", "```py\nmodel.add(Dense(512,  activation=’softmax’))\n```", "```py\nimport keras.regularizers\nmodel=Sequential()\nmodel.add(Flatten(input_shape=(28, 28)))\nmodel.add(Dense(1024, kernel_regularizer=  \n                      regularizers.12(0.0001),activation ='relu'))\nmodel.add(Dense(28, kernel_regularizer=regularizers.12(0.0001), \n          activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n```", "```py\nimport keras.regularizers\nmodel=Sequential()\nmodel.add(Flatten(input_shape=(28,28)))\nmodel.add(Dense(1024, kernel_regularizer=regularizers.12(0.0001), \n          activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n```", "```py\n#Simple feed forward neural network\nmodel=Sequential()\n\n#feeds in the image composed of 28  28 a pixel matrix as one sequence of 784\nmodel.add(Flatten(input_shape=(28,28)))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.3)\nmodel.add(Dense(28, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n```", "```py\nimport keras\nfrom keras.datasets import imdb\n(x_train,y_train), (x_test,y_test)=imdb.load_data(num_words=12000)\n```", "```py\nx_train.shape, x_test.shape, type(x_train)\n((25000,), (25000,), numpy.ndarray)\n```", "```py\nx_train[1]\n\n[1,\n 194,\n 1153,\n 194,\n 8255,\n 78,\n 228,\n 5,\n 6,\n 1463,\n 4369,\n 5012,\n 134,\n 26,\n 4,\n 715,\n 8,\n 118,\n 1634,\n 14,\n 394,\n 20,\n 13,\n 119,\n 954,\n```", "```py\n#backup labels, so we can verify our networks prediction after vectorization\nxtrain = x_train\nxtest = x_test\n```", "```py\nword_index =imdb.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n```", "```py\ndef decode_review(n, split= 'train'):\nif split=='train':\n    decoded_review=' '.join([reverse_word_index.get(i-3,'?')for i in \n                   ctrain[n]])\nelif split=='test':\n    decoded_review=' '.join([reverse_word_index.get(i-3,'?')for i in   \n                   xtest[n]])\nreturn decoded_review\n```", "```py\nprint('Training label:',y_train[5])\ndecode_review(5, split='train'),\nTraining label: 0.0\n```", "```py\nimport numpy as np\ndef vectorize_features(features):\n\n#Define the number of total words in our corpus \n#make an empty 2D tensor of shape (25000, 12000)\ndimension=12000\nreview_vectors=np.zeros((len(features), dimension))\n\n#interate over each review \n#set the indices of our empty tensor to 1s\nfor location, feature in enumerate(features):\n    review_vectors[location, feature]=1\nreturn review_vectors\n\nx_train = vectorize_features(x_train)\nx_test = vectorize_features(x_test)\n```", "```py\ntype(x_train),x_train.shape, y_train.shape\n(numpy.ndarray, (25000, 12000), (25000,))\n\nx_train[0].shape, x_train[0]\n((12000,), array([0., 1., 1., ..., 0., 0., 0.]), 12000)\n```", "```py\ny_train= np.asarray(y_train).astype('float32')\ny_test = np.asarray(y_test).astype('float32')\n```", "```py\nfrom keras.models import sequential \nfrom keras.layers import Dense\nmodel=Sequential()\nmodel.add(Dense(6, activation='relu', input_shape=(12000)))\nmodel.add(Dense(6, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n```", "```py\nfrom keras import optimizers\nmodel.compile(optimizer=optimizers.RMSprop(1r=0.001),\n    loss='binary_crossentropy',\n    metrics=['accuracy']) \n```", "```py\nnetwork_metadata=model.fit(x_train, y_train,\n                           validation_data=(x_test, y_test),\n                           epochs=20,\n                           batch_size=100)\n```", "```py\nearly_stopping= keras.callbacks.EarlyStopping(monitor='loss')\nnetwork_metadata=model.fit(x_train, y_train, validation_data=(x_test,  \n                           y_test), epochs=20, batch_size=100,  \n                           callbacks=[early_stopping]) \n```", "```py\nhistory_dict = network_metadata.history\nhistory_dict.keys()\ndict_keys(['val_loss','val_acc','loss','acc'])\n```", "```py\nimport matplotlib.pyplot as plt\n\nacc=history_dict['acc']\nloss_values=history_dict['loss']\nval_loss_values=history_dict['loss']\nval_loss_values=history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, loss_values,'r',label='Training loss')\nplt.plot(epochs, val_loss_valuesm, 'rD', label-'Validation loss')\nplt.title('Training and validation loss')plt.xlabel('Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n```", "```py\nplt.clf()\nacc_values=history_dict['acc']\nval_acc_values=history_dict['val_acct']\nplt.plot(epochs, history_dict.get('acc'),'g',label='Training acc')\nplt.plot(epochs, history_dict.get('val_acc'),'gD',label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n```", "```py\npredictions=model.predict([x_test])\npredictions[5]\n```", "```py\ny_test[5], decode_review(5, split='test')\n```", "```py\ndef gauge_predictions(n):\nif (predictions[n]<=0.4) and (y_test[n]==0):\n    print('Network correctly predicts that review %d is negative' %(n))\nelif (predictions[n] <=0.7) and (y_test[n]==1);\nelif (predictions[n]>-0.7) and (y_test[n]==0):\nelse:\n    print('Network is not so sure. Review mp. %d has a probability score of %(n),   \n           predictions[n])\ndef verify_predictions(n):\n    return gauge_predictions(n), predictions[n], decode_review(n, split='test')\n```", "```py\nverify-predictions(22)\nnetwork falsely predicts that review 22 is negative\n```", "```py\nverify_predictions(19999)\nNetwork is not so sure. Review no. 19999 has a probability score of [0.5916141]\n```", "```py\nverify_predictions(4)\nNetwork correctly predicts that review 4 is positive\n```", "```py\nfrom random import randint\ndef random_predict(n_reviews):\nfor i in range(n_reviews):\nprint(verify_predictions(randint(0, 24000)))\nrandom_predict(2)\nNetwork correctly predicts that review 20092 is positive\n```", "```py\nimport keras\nfrom keras.datasets import boston_housing.load_data()\n(x_train, y_train),(x_test,y_test)=boston_housing.load_data()\nx_train[1], y_train[1]\n```", "```py\nprint(type(x_train),'training data:',x_train.shape,'test data:',x_test.shape)\n<class 'numpy.ndarray'>training data:(403, 13) test data: (102, 13)\n```", "```py\ncolumn_names=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LST  \n  AT']\n\nkey= ['Per capita crime rate.',\n    'The proportion of residential land zoned for lots over 25,000   \n     square feet.',\n    'The proportion of non-retail business acres per town.',\n    'Charles River dummy variable (=1 if tract bounds river; 0 \n     otherwise).',\n    'Nitric oxides concentration (parts per 10 million).',\n    'The average number of rooms per dwelling.',\n    'The porportion of owner-occupied units built before 1940.',\n    'Weighted distances to five Boston employment centers.',\n    'Index of accessibility to radial highways.',\n    'Full-value property tax rate per $10,000.',\n    'Pupil-Teacher ratio by town.',\n    '1000*(Bk-0.63)**2 where Bk is the proportion of Black people by \n     town.',\n    'Percentage lower status of the population.'}\n```", "```py\nimport pandas as pd\ndf= pd.DataFrame(x_train, columns=column_names)\ndf.head()\n```", "```py\nmean=x_train.mean(axis=0)\nstd=x_train.std(axis=0)\nx_train=(x_train-mean)/std\nx_test=(x_test-mean)/std\nprint(x_train[0]) #First Training sample, normalized\n```", "```py\nfrom keras.layers import Dense, Dropout\nfrom keras.models import Sequential\nmodel= Sequential()\nmodel.add(Dense(26, activation='relu',input_shape=(13,)))\nmodel.add(Dense(26, activation='relu'))\nmodel.add(Dense(12, activation='relu'))\nmodel.add(Dense(1))\n```", "```py\nfrom keras import optimizers\nmodel.compile(optimizer= opimizers.RMSprop(lr=0.001),\n              loss-'mse',\n              metrics=['mae'])\nmodel.summary()\n__________________________________________________________\nLayer (type)                 Output Shape              Param #   \n==========================================================\ndense_1 (Dense)              (None, 6)                 72006     \n__________________________________________________________\ndense_2 (Dense)              (None, 6)                 42        \n__________________________________________________________\ndense_3 (Dense)              (None, 1)                 7         \n==========================================================\nTotal params: 72,055\nTrainable params: 72,055\nNon-trainable params: 0\n__________________________________________________________\n```", "```py\nimport numpy as np\nimport pandas as pd\n​\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n​\n​\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n​\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom keras.datasets import boston_housing\n(x_train,y_train),(x_test,y_test) = boston_housing.load_data()\n\nx_train.shape, x_test.shape\n\n---------------------------------------------------------------\n((404, 13), (102, 13)) ----------------------------------------------------------------\n\nimport numpy as np\n\nx_train = np.concatenate((x_train,x_test), axis=0)\ny_train = np.concatenate((y_train,y_test), axis=0)\n\nx_train.shape, y_train.shape\n\n-----------------------------------------------------------------\n((506, 13), (506,))\n-----------------------------------------------------------------\n```", "```py\ndef baseline_model():\n    model = Sequential()\n    model.add(Dense(13, input_dim=13, kernel_initializer='normal', \n              activation='relu'))\n model.add(Dense(1, kernel_initializer='normal'))\n model.compile(loss='mean_squared_error', optimizer='adam')\n return model\n```", "```py\n#set seed for reproducability \nseed = 7\nnumpy.random.seed(seed)\n\n# Add a data Scaler and the keras regressor containing our model function to a list of estimators\n\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasRegressor(build_fn=baseline_model,   \n                    epochs=100, batch_size=5, verbose=0)))\n\n#add our estimator list to a Sklearn pipeline\n\npipeline = Pipeline(estimators)\n\n#initialize instance of k-fold validation from sklearn api\n\nkfold = KFold(n_splits=5, random_state=seed)\n\n#pass pipeline instance, training data and labels, and k-fold crossvalidator instance to evaluate score\n\nresults = cross_val_score(pipeline, x_train, y_train, cv=kfold)\n\n#The results variable contains the mean squared errors for each of our     \n 5 cross validation runs.\nprint(\"Average MSE of all 5 runs: %.2f, with standard dev: (%.2f)\" %   \n      (-1*(results.mean()), results.std()))\n\n------------------------------------------------------------------\nModel Type: <function larger_model at 0x000001454959CB70>\nMSE per fold:\n[-11.07775911 -12.70752338 -17.85225084 -14.55760158 -17.3656806 ]\nAverage MSE of all 5 runs: 14.71, with standard dev: (2.61) \n```"]