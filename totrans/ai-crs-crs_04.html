<html><head></head><body>
		<div id="_idContainer034">
			<h1 class="chapterNumber sigil_not_in_toc">4</h1>
			<h1 class="chapterTitle" xml:lang="en-GB" id="sigil_toc_id_55" lang="en-GB"><a id="_idTextAnchor077"/>AI Foundation Techniques</h1>
</div>

			<p class="normal">In this chapter, you'll begin your study of AI 
theory in earnest. You'll start with an introduction to a major branch 
of AI, called Reinforcement Learning, and the five principles that 
underpin every Reinforcement Learning model. Those principles will give 
you the theoretical understanding to make sense of every forthcoming AI 
model in this book.</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_56" lang="en-GB"><a id="_idTextAnchor078"/>What is Reinforcement Learning?</h2>
			<p class="normal">When people refer to <a id="_idIndexMarker084"/>AI
 today, some of them think of Machine Learning, while others think of 
Reinforcement Learning. I fall into the second category. I always saw 
Machine Learning as statistical models that have the ability to learn 
some correlations, from which they make predictions without 
being explicitly programmed.</p>
			<p class="normal">While this is, in some way, a form of AI, Machine 
Learning does not include the process of taking actions and interacting 
with an environment like we humans do. Indeed, as intelligent human 
beings, what we constantly keep doing is the following:</p>
			<ol>
				<li class="list">We observe some input, whether it's what we see with our eyes, what we hear with our ears, or what we remember in our memory.</li>
				<li class="list">These inputs are then processed in our brain.</li>
				<li class="list">Eventually, we make decisions and take actions.</li>
			</ol>
			<p class="normal">This process of interacting with an environment is 
what we are trying to reproduce in terms of Artificial Intelligence. And
 to that extent, the branch of AI that works on this is Reinforcement 
Learning. This is the closest match to the way we think; the most 
advanced form of Artificial Intelligence, if we see AI as the science 
that tries to mimic (or surpass) human intelligence.</p>
			<p class="normal">Reinforcement Learning also has the most impressive results in business applications of AI. For example, Alibaba leveraged <a id="_idIndexMarker085"/>Reinforcement Learning to increase its<a id="_idIndexMarker086"/> ROI in online advertising by 240% without increasing their advertising budget (see <a href="https://arxiv.org/pdf/1802.09756.pdf"><span class="url">https://arxiv.org/pdf/1802.09756.pdf</span></a>, page 9, Table 1 last row (DCMAB)). We'll tackle the same industry application in this book!</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_57" lang="en-GB"><a id="_idTextAnchor079"/>The five principles of Reinforcement Learning</h2>
			<p class="normal">Let's begin building the first<a id="_idIndexMarker087"/>
 pillars of your intuition into how Reinforcement Learning works. These 
are the fundamental principles of Reinforcement Learning, which will <a id="_idIndexMarker088"/>get you started with the right, solid basics in AI.</p>
			<p class="normal">Here are the five principles:</p>
			<ol>
				<li class="list" value="1"><strong class="bold">Principle #1</strong>: The input and output system</li>
				<li class="list"><strong class="bold">Principle #2</strong>: The reward</li>
				<li class="list"><strong class="bold">Principle #3</strong>: The AI environment</li>
				<li class="list"><strong class="bold">Principle #4</strong>: The Markov decision process</li>
				<li class="list"><strong class="bold">Principle #5</strong>: Training and inference</li>
			</ol>
			<p class="normal">In the following sections, you can read about each one in t<a id="_idTextAnchor080"/>urn.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_58" lang="en-GB"><a id="_idTextAnchor081"/>Principle #1 – The input and output system</h3>
			<p class="normal">The first step is to <a id="_idIndexMarker089"/>understand
 that today, all AI models are based on the common principle of inputs 
and outputs. Every single form of Artificial Intelligence, including 
Machine Learning models, ChatBots, recommender systems, robots, and of 
course Reinforcement Learning models, will take something as input, and 
will return another thing as output.</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_04_01.png" alt=""/></figure>
			<p class="packt_figref">Figure 1: The input and output system</p>
			<p class="normal">In Reinforcement Learning, these inputs and outputs have a specific name: the input is called the state, or input state. The <a id="_idIndexMarker090"/>output
 is the action performed by the AI. And in the middle, we have nothing 
other than a function that takes a state as input and returns an action 
as output. That function is called a policy. Remember the name, 
"policy," because you will often see it in AI literature.</p>
			<p class="normal">As an example, consider a self-driving car. Try to imagine what the input and output would be in that case. </p>
			<p class="normal">The input would be what the embedded computer 
vision system sees, and the output would be the next move of the car: 
accelerate, slow down, turn left, turn right, or brake. Note that the 
output at any time (<em class="italics">t</em>) could very well be 
several actions performed at the same time. For instance, the 
self-driving car can accelerate while at the same time turning left. In 
the same way, the input at each time (<em class="italics">t</em>) can be
 composed of several elements: mainly the image observed by the computer
 vision system, but also some parameters of the car such as the current 
speed, the amount of gas remaining in the tank, and so on.</p>
			<p class="normal">That's the very first important principle in 
Artificial Intelligence: it is an intelligent system (a policy) that 
takes some elements as input, does its magic in the middle, and returns 
some actions to perform<a id="_idIndexMarker091"/> as output. Remember that the inputs are also called the <strong class="bold">states</strong>.</p>
			<p class="normal">The next important principle is the r<a id="_idTextAnchor082"/>eward.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_59" lang="en-GB"><a id="_idTextAnchor083"/>Principle #2 – The reward</h3>
			<p class="normal">Every AI has its performance measured <a id="_idIndexMarker092"/>by
 a reward system. There's nothing confusing about this; the reward is 
simply a metric that will tell the AI how well it does over time.</p>
			<p class="normal">The simplest example is a binary reward: 0 or 1. 
Imagine an AI that has to guess an outcome. If the guess is right, the 
reward will be 1, and if the guess is wrong, the reward will be 0. This 
could very well be the reward system defined for an AI; it really can be
 as simple as that!</p>
			<p class="normal">A reward doesn't have to be binary, however. It can be continuous. Consider the famous game of <em class="italics">Breakout</em>:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_04_02.png" alt=""/></figure>
			<p class="packt_figref">Figure 2: The Breakout game</p>
			<p class="normal">Imagine an AI playing this game. Try to work out 
what the reward would be in that case. It could simply be the score; 
more precisely, the score would be the accumulated reward over time in 
one game, and<a id="_idIndexMarker093"/> the rewards could be defined as the derivative of that score.</p>
			<p class="normal">This is one of the many ways we could define a 
reward system for that game. Different AIs will have different reward 
structures; we will build five rewards systems for five different 
real-world applications in this book.</p>
			<p class="normal">With that in mind, remember this as well: the 
ultimate goal of the AI will always be to maximize the accumulated 
reward over time.</p>
			<p class="normal">Those are the first two basic, but fundamental, 
principles of Artificial Intelligence as it exists today; the input and 
output system, and the reward. The next thing to consider is the AI 
envir<a id="_idTextAnchor084"/>onment.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_60" lang="en-GB"><a id="_idTextAnchor085"/>Principle #3 – The AI environment</h3>
			<p class="normal">The third principle is what we call <a id="_idIndexMarker094"/>an "AI environment." It is a very simple framework where you define three things at each time (<em class="italics">t</em>):</p>
			<ul>
				<li class="list">The input (the state)</li>
				<li class="list">The output (the action)</li>
				<li class="list">The reward (the performance metric)</li>
			</ul>
			<p class="normal">For each and every single AI based on Reinforcement
 Learning that is built today, we always define an environment composed 
of the preceding elements. It is, however, important to understand that 
there are more than these three elements in a given AI environment.</p>
			<p class="normal">For example, if you are building an AI to beat a 
car racing game, the environment will also contain the map and the 
gameplay of that game. Or, in the example of a self-driving car, the 
environment will also <a id="_idIndexMarker095"/>contain all the 
roads along which the AI is driving and the objects that surround those 
roads. But what you will always find in common when building any AI, are
 the three elements of state, action, and reward. The next principle, 
the Markov decision process, covers how they work in <a id="_idTextAnchor086"/>practice.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_61" lang="en-GB"><a id="_idTextAnchor087"/>Principle #4 – The Markov decision process</h3>
			<p class="normal">The Markov decision process, or MDP, is simply a <a id="_idIndexMarker096"/>process that models how the AI interacts with the environment over time. The process starts at <em class="italics">t</em> = 0, and then, at each next iteration, meaning at <em class="italics">t</em> = 1, <em class="italics">t</em> = 2, … <em class="italics">t</em> = <em class="italics">n</em> units of time (where the unit can be anything, for example, 1 second), the AI follows the same format of transition:</p>
			<ol>
				<li class="list" value="1">The AI observes the current state, <span class="mediaobject"><img src="../Images/B14110_04_001.png" alt=""/></span>.</li>
				<li class="list">The AI performs the action, <span class="mediaobject"><img src="../Images/B14110_04_002.png" alt=""/></span>.</li>
				<li class="list">The AI receives the reward, <span class="mediaobject"><img src="../Images/B14110_04_003.png" alt=""/></span>.</li>
				<li class="list" value="4">The AI enters the following state, <span class="mediaobject"><img src="../Images/B14110_04_004.png" alt=""/></span>.</li>
			</ol>
			<p class="normal">The goal of the AI is always the same in 
Reinforcement Learning: it is to maximize the accumulated rewards over 
time, that is, the sum of all the <img src="../Images/B14110_04_005.png" alt=""/> received at each transition.</p>
			<p class="normal">The following graphic will help you visualize and remember an MDP better, the basis of Reinforcement Learning models:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_04_03.png" alt=""/></figure>
			<p class="packt_figref">Figure 3: The Markov Deci<a id="_idTextAnchor088"/>sion process</p>
			<p class="normal">Now four essential pillars are <a id="_idIndexMarker097"/>already shaping your<a id="_idIndexMarker098"/>
 intuition of AI. Adding a last important one completes the foundation 
of your understanding of AI. The last principle is training and 
inference; in training, the AI learns, and in inference, it pr<a id="_idTextAnchor089"/>edicts.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_62" lang="en-GB"><a id="_idTextAnchor090"/>Principle #5 – Training and inference</h3>
			<p class="normal">The final principle you have to <a id="_idIndexMarker099"/>understand
 is the difference between training and inference. When building an AI, 
there is a time for the training mode, and a separate time for inference
 mode. I'll explain what that means starting with the traini<a id="_idTextAnchor091"/>ng mode.</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_63" lang="en-GB"><a id="_idTextAnchor092"/>Training mode</h4>
			<p class="normal">Now you <a id="_idIndexMarker100"/>understand, 
from the three first principles, that the very first step of building an
 AI is to build an environment in which the input states, the output 
actions, and a system of rewards are clearly defined. From the fourth 
principle, you also understand that inside this environment we will 
build an AI to interact with it, trying to maximize the total reward 
accumulated over time.</p>
			<p class="normal">To put it simply, there will be a preliminary (and 
long) period of time during which the AI will be trained to do that. 
That period of time is called the training; we can also say that the AI 
is in training mode. During that time, the AI tries to accomplish 
a certain goal over and over again until it succeeds. After each 
attempt, the parameters of the AI model are modified in order to do 
better at the next attempt.</p>
			<p class="normal">For example, let's say you're building<a id="_idIndexMarker101"/> a self-driving car and you want it to go from point <em class="italics">A</em> to point <em class="italics">B</em>.
 Let's also imagine that there are some obstacles that you want your 
self-driving car to avoid. Here is how the training process happens:</p>
			<ol>
				<li class="list" value="1">You choose an AI model, which can be Thompson Sampling (<em class="italics">Chapters 5</em> and <em class="italics">6</em>), Q-learning (<em class="italics">Chapters 7</em> and <em class="italics">8</em>), deep Q-learning (<em class="italics">Chapters 9</em>, <em class="italics">10,</em> and <em class="italics">11</em>) or even deep convolutional Q-learning (<em class="italics">Chapters</em> <em class="italics">12</em> and <em class="italics">13</em>).</li>
				<li class="list">You initialize the parameters of the model.</li>
				<li class="list">Your AI tries to go from <em class="italics">A</em> to <em class="italics">B</em> (by observing the states and performing its actions). During this first attempt, the closer it gets to <em class="italics">B</em>, the higher reward you give to the AI. If it fails reaching <em class="italics">B</em> or hits an obstacle, you give the AI a very bad reward. If it manages to reach <em class="italics">B</em>
 without hitting any obstacle, you give the AI an extremely good reward.
 It's just like you would train a dog to sit: you give the dog a treat 
or say "good boy" (positive reward) if the dog sits. And you give the 
dog whatever small punishment you need to if the dog disobeys (negative 
reward). That process is training, and it works the same way in 
Reinforcement Learning.</li>
				<li class="list">At the end of the attempt (also called an episode),
 you modify the parameters of the model in order to do better next time.
 The parameters are modified intelligently, either iteratively through 
equations (Q-Learning), or by using Machine Learning and Deep Learning 
techniques such as stochastic gradient descent or backpropagation. All 
these techniques will be covered in this book.</li>
				<li class="list">You repeat steps 3 and 4 again, and again, until 
you reach the desired performance; that is, until you have your fully 
non-dangerous auton<a id="_idTextAnchor093"/>omous car!</li>
			</ol>
			<p class="normal">So, that's training. Now, how about inference?</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_64" lang="en-GB"><a id="_idTextAnchor094"/>Inference mode</h4>
			<p class="normal">Inference mode simply comes after your AI is fully 
trained and ready to perform well. It will simply consist of interacting
 with the environment by performing the actions to accomplish the goal 
the AI was trained to achieve before in training mode. In inference 
mode, no parameters are modified at the end of each episode. </p>
			<p class="normal">For example, imagine you have an AI company that 
builds customized AI solutions for businesses, and one of your clients 
asked you to build an AI to optimize the flows in a smart grid. First, 
you'd enter an R&amp;D phase during which you would train your AI to 
optimize these flows (training mode), and as soon as you reached a good 
level of performance, you'd deliver your AI to your client and <a id="_idIndexMarker102"/>go
 into production. Your AI would regulate the flows in the smart grid 
only by observing the current states of the grid and performing the 
actions it has been trained to do. That's inference mode.</p>
			<p class="normal">Sometimes, the environment is subject to change, in
 which case you have to alternate fast between training and inference 
modes so that your AI can adapt to the new changes in the environment. 
An even better solution is to train your AI model every day, and go into
 inference mode with the most recently trained model.</p>
			<p class="normal">That was the last fundamental principle common to 
every AI. Congratulations – now you already have a solid basic 
understanding of Artificial Intelligence! Since you have that, you are 
ready to tackle your very first AI model in the next chapter: a simple 
yet very powerful one, still widely used today in business and 
marketing, to solve a problem that has the delightful name of the 
multi-armed bandit problem.</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_65" lang="en-GB"><a id="_idTextAnchor095"/>Summary</h2>
			<p class="normal">In this chapter, you learned the five fundamental 
principles of Artificial Intelligence from a Reinforcement Learning 
perspective. Firstly, an AI is a system that takes an observation 
(values, images, or any data) as input, and returns an action to perform
 as output (principle #1). Then, there is a reward system that helps it 
measure its performance. The AI will learn through trial and error based
 on the reward it gets over time (principle #2). The input (state), the 
output (action), and the reward system define the AI environment 
(principle #3). The AI interacts with this environment through the 
Markov decision process (principle #4). Finally, in training mode, the 
AI learns how to maximize its total reward by updating its parameters 
through the iterations, and in inference mode, the AI simply performs 
its actions over full episodes without updating any of its parameters – 
that is to say, without learning (principle #5).</p>
			<p class="normal">In the next chapter, you will learn about Thompson 
Sampling, a simple Reinforcement Learning model, and use it to solve the
 multi-armed bandit problem.</p>
</body></html>