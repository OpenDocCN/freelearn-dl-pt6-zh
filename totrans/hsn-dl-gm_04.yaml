- en: GAN for Games
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far, in our deep learning exploration, we have trained all our networks
    using a technique called **supervised training**. This training technique works
    well for when you have taken the time to identify and label your data. All of
    our previous example exercises used supervised training, because it is the simplest
    form of teaching. However, supervised learning tends to be the most cumbersome
    and tedious method, largely because it requires some amount of data labeling or
    identification before training. There have been attempts to use this form of training
    for machine learning or deep learning in gaming and simulation, but they have
    proven to be unsuccessful.
  prefs: []
  type: TYPE_NORMAL
- en: This is why, for most of this book, we will look at other forms of training,
    starting with a form of unsupervised training called a **generative adversarial
    network** (**GAN**). GANs are able to train themselves using, in essence, a two-player
    game. This makes them an ideal next step in our learning and a perfect way to
    actually start generating content for games.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we explore GANs and their use in developing game content.
    Along the way, we will learn more fundamentals of deep learning techniques. In
    this chapter, we will cover the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding a GAN in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wasserstein GAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GAN for creating textures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating music with a GAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs are notoriously hard to train and build successfully. Therefore, it is
    recommended you take your time with this chapter and go through the exercises
    a couple of times if you need to. The techniques we learn to make effective GANs
    will provide you with a better overall understanding of training networks and
    the many other options available. We also still need to cover many fundamental
    concepts about training networks, so please work through this chapter thoroughly.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The concept of GANs is typically introduced using the analogy of a two-player
    game. In this game, there is typically an art expert and an art forger. The goal
    of the art forger or counterfeiter is to make a convincing-enough fake to fool
    the art expert and thus win the game. An example of how this was first portrayed
    as a neural network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5736d34f-71e3-4723-acce-24916b356499.png)'
  prefs: []
  type: TYPE_IMG
- en: GAN by Ian and others
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the Generator takes the place of the art forger, the
    one trying to best the art expert, shown as the Discriminator. The Generator uses
    random noise as a source to generate an image, with a goal that the image is convincing
    enough to fool the Discriminator. The Discriminator is trained on both real and
    fake images, and all it does is classify the image as real or fake. The Generator
    is then trained to build a convincing-enough fake that will fool the Discriminator.
    While this concept seems simple enough as a way of self-training a network, in
    the last few years, the implementation of this adversarial technique has proven
    exceptional in many areas.
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs were first developed by Ian Goodfellow and others at the University of
    Montreal in 2014\. In only a few short years, this technique has exploded into
    many wide and varied applications, from generating images and text to animating
    static images, all in a very short time. The following is a short summary of some
    of the more impressive GAN improvements/implementations currently turning heads
    in the deep learning community:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep convolutional GANs** (**DCGANs**): These were the first major improvement
    to the standard architecture we just covered. We will explore this as our first
    form of GAN in the next section of this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial Autoencoder GAN**: This variation of an autoencoder uses the
    adversarial GAN technique to isolate attributes or properties of your data. It
    has interesting applications for determining latent relationships in data, such
    as being able to tell the difference in style versus content for a set of handwritten
    digits, for instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auxiliary Classifier GAN**: This is another enhanced GAN that relates to
    conditioned or conditional GANs. It has been shown to synthesize higher-resolution
    images and is certainly worth exploring more in gaming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CycleGAN**: This is a variation that is impressive in that it allows the
    translation of style from one image to another. There are plenty of examples of
    this form of GAN being used to style a picture as if Van Gogh painted it, to swapping
    celebrity faces. If this chapter piques your interest in GANs and you want to
    explore this form, check out this post: [https://hardikbansal.github.io/CycleGANBlog/](https://hardikbansal.github.io/CycleGANBlog/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditional GANS**: These use a form of semi-supervised learning. This means
    that the training data is labeled but with meta data or attributes. So, instead
    of labeling a handwritten digit from the MNIST data set as a 9, you may instead
    label the writing style (cursive or print). Then, this new form of conditioned
    GAN can learn not only the digits, but also whether they are cursive or print.
    This form of GAN has shown some interesting applications and it is one we will
    explore further when we speak to specific applications in gaming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DiscoGAN**: This is yet another form of GAN showing fun results, from swapping
    celebrity hairstyles to genders. This GAN extracts features or domains and allows
    you to transfer them to other images or data spaces. This GAN has numerous applications
    in gaming and is certainly worth exploring further for the interested reader.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DualGAN**: This uses dual GANs to train two generators against two discriminators
    in order to transfer images or data to other styles. This would be very useful
    as a way of restyling multiple assets and would work nicely for generating different
    forms of art content for games.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Least squares GAN** (**LSGAN**): This uses a different form of calculating
    loss and has been shown to be more effective than the DCGAN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pix2pixGAN**: This is an extension to conditional GANs that allows it to
    transfer or generate multiple features from one image to another. This allows
    for images of the sketch of an object to return an actual 3D-rendered image of
    the same object or vice versa. While this is a very powerful GAN, it still is
    very much research-driven and may not be ready for use in games. Perhaps you will
    just have to wait six months or a year.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**InfoGANs**: These types of GANs are, as of yet, used extensively to explore
    features or information about the training data. They can be used to identify
    the rotation of a digit in the MNIST dataset, for instance. Also, they are often
    used as a way of identifying attributes for conditioned GAN training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stacked or SGAN**: This is a form of GAN that breaks itself into layers where
    each layer is a generator and discriminator battling it out. This makes the overall
    GAN easier to train but also requires you to understand each stage or layer in
    some detail. If you are just starting, this is not the GAN for you, but as you
    build more complex networks, revisit this model again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wasserstein GANs**: This is a state-of-the-art GAN, and it will also get
    attention in its own section in this chapter. The calculation of loss is the improvement
    in this form of GAN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WassGANs**: This uses the Wasserstein distance to determine loss, which dramatically
    helps with model convergence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will explore further instances of specific GAN implementations as we work
    through this chapter. Here, we will look at how to generate game textures and
    music with a GAN. For now, though, let's move on to the next section and learn
    how to code a GAN in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Coding a GAN in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Of course, the best way to learn is by doing, so let''s jump in and start coding
    our first GAN. In this example, we will be building the basic DCGAN and then modifying
    it later for our purposes. Open up `Chapter_3_2.py` and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: This code was originally pulled from [https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN),
    which is the best representation of GANs in Keras anywhere, and is all thanks
    to Erik Linder-Norén. Great job, and thanks for the hard work, Erik.
  prefs: []
  type: TYPE_NORMAL
- en: An alternate listing a vanilla GAN has been added as `Chapter_3_1.py` for your
    learning pleasure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There are a few highlighted new types introduced in the preceding code: `Reshape`,
    `BatchNormalization`, `ZeroPadding2D`, `LeakyReLU`, `Model`, and `Adam`. We will
    explore each of these types in more detail next.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Most of our previous examples worked with basic scripts. We are now at a point
    where we want types (classes) of our own built for further use later. That means
    we now start by defining our class like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So, we create a new class (type) called `DCGAN` for our implementation of a
    deep convolutional GAN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we would normally define our `init` function by Python convention. However,
    for our purposes, let''s first look at the `generator` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `build_generator` function builds the art-forger model, which means it takes
    that sample set of noise and tries to convert it into an image the discriminator
    will believe is real. In this form, it uses the principle of convolution to make
    it more efficient, except, in this case, it generates a feature map of noise that
    it then turns into a real image. Essentially, the generator is doing the opposite
    of recognizing an image, but instead trying to generate an image based on feature
    maps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the preceding block of code, note how the input starts with `128, 7x7` feature
    maps of noise then uses a `Reshape` layer to turn it into the proper image layout
    we want to create. It then up-samples (the reverse of pooling or down-sampling)
    the feature map into 2x size (14 x 14), training another layer of convolution
    followed by more up-sampling (2x to 28 x 28) until the correct image size (28x28
    for the MNIST) is generated. We also see the use of a new layer type called `BatchNormalization`,
    which we will cover in more detail shortly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will build the `build_discriminator` function like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This time, the discriminator is testing the image inputs and determining whether
    they are fake. It uses convolution to identify features, but in this example it
    uses `ZeroPadding2D` to place a buffer of zeros around the images in order to
    help identification. The opposite form of this layer would be `Cropping2D`, which
    crops an image. Note how this model does not use down-sampling or pooling with
    the convolution. We will explore the other new special layers `LeakyReLU` and
    `BatchNormalization` in the coming sections. Note how we have not used any pooling
    layers in our convolution. This is done to increase the spatial dimensionality
    through the fractionally strided convolutions. See how inside the convolution
    layers we are using an odd kernel and stride size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will now circle back and define the `init` function like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This initialization code sets up the sizes for our input images (28 x 28 x 1,
    one channel for grayscale). It then sets up an `Adam` optimizer, something else
    we will review in another section on optimizers. After this, it builds the `discriminator`
    and then the `generator`. Then it combines the two models or sub networks (`generator`
    and `discriminator`) together. This allows the networks to work in tandem and
    optimize training across an entire network. Again, this is a concept we will look
    at more closely under optimizers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before we get too deep, take some time to run this example. This sample can
    take an extensive amount of time to run, so return to the book after it starts
    and keep it running.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As the sample runs, you will be able to see the generated output get placed
    into a folder called `images` within the same folder as your running Python file.
    Go ahead and watch as every 50 epochs a new image is saved, which is shown in
    the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/dfa6688e-16d8-4d0f-b177-6f2ea058dd59.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of output generated from a GAN
  prefs: []
  type: TYPE_NORMAL
- en: The preceding shows the results after 3,900 epochs or so. When you start training,
    it will take a while to get results this good.
  prefs: []
  type: TYPE_NORMAL
- en: That covers the basics of setting up the models, except all the work that is
    in the training, which we will cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training a GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training a GAN requires a fair bit more attention to detail and an understanding
    of more advanced optimization techniques. We will walk through each section of
    this function in detail in order to understand the intricacies of training. Let''s
    open up `Chapter_3_1.py` and look at the `train` function and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the start of the `train` function, you will see the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The data is first loaded from the MNIST training set and then rescaled to the
    range of `-1` to `1`. We do this in order to better center that data around 0
    and to accommodate our activation function, `tanh`. If you go back to the generator
    function, you will see that the bottom activation is `tanh`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we build a `for` loop to iterate through the epochs like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we randomly select half of the *real* training images, using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we sample `noise` and generate a set of forged images with the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, half of the images are real and the other half are faked by our `generator`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, the `discriminator` is trained against the images generating a loss for
    incorrectly predicted fakes and correctly identified real images as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Remember, this block of code is running across a set or batch. This is why we
    use the `numpy np.add` function to add the `d_loss_real`, and `d_loss_fake`. `numpy`
    is a library we will often use to work on sets or tensors of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we train the generator using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how the `g_loss` is calculated based on training the combined model. As
    you may recall, the combined model takes the input from real and fake images and
    backpropagates the training back through the entire model. This allows us to train
    both the `generator` and `discriminator` together as a combined model. An example
    of how this looks is shown next, but just note that the image sizes are a little
    different than ours:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c85f204f-7196-444a-be1c-53b285b79cd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Layer architecture diagram of DCGAN
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a better understanding of the architecture, we need to go back
    and understand some details about the new layer types and the optimization of
    the combined model. We will look at how we can optimize a joined model such as
    our GAN in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An **optimizer** is really nothing more than another way to train the backpropagation
    of error through a network. As we learned back in [Chapter 1](108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml)*,
    Deep Learning for Games*, the base algorithm we use for backpropagation is the
    gradient descent and the more advanced **stochastic gradient descent** (**SGD**).
  prefs: []
  type: TYPE_NORMAL
- en: 'SGD works by altering the evaluation of the gradient by randomly picking the
    batch order during each training iteration. While SGD works well for most cases,
    it does not perform well in a GAN, due to a problem known as the **vanishing **/ **exploding
    gradient**, which happens when trying to train multiple, but combined, networks.
    Remember, we are directly feeding the results of our generator into the discriminator.
    Instead, we look to more advanced optimizers. A graph showing the performance
    of the typical best optimizers is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce2b1eaf-c1e9-4018-ac13-b8208344a68e.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance comparison of various optimizers
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the methods in the graph have their origin in SGD, but you can clearly
    see the winner in this instance is **Adam**. There are cases where this is not
    the case, but the current favorite optimizer is Adam. It is something we have
    used extensively before, as you may have noticed, and you will likely continue
    using it in the future. However, let''s take a look at each of the optimizers
    in a little more detail, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SGD**: This is one of the first models we looked at and it will often be
    our baseline to train against.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SGD with Nesterov**: The problem SGD often faces is that wobble effect we
    saw in network loss, in one of the earlier training examples. Remember, during
    training, our network loss would fluctuate between two values, almost as if it
    was a ball going up and down a hill. In essence, that is exactly what is happening,
    but we can correct that by introducing a term we call **momentum**. An example
    of the effect momentum has on training is shown in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9ab302ca-8dd7-4e12-b02e-da2befc259a0.png)'
  prefs: []
  type: TYPE_IMG
- en: SGD with and without momentum
  prefs: []
  type: TYPE_NORMAL
- en: So, now, instead of just letting the ball blindly roll around, we control its
    speed. We give it a push to get over some of those annoying bumps or wobbles,
    and more efficiently get to the lowest point.
  prefs: []
  type: TYPE_NORMAL
- en: As you may recall from studying the math of backpropagation, we control the
    gradient in SGD to train the network to minimize error or loss. By introducing
    momentum, we try to control the gradient to be more efficient by approximating
    what the values should be. The **Nesterov technique**, or it may just be referred
    to as **Momentum**, uses an accelerated momentum term to further optimize the
    gradient.
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaGrad**: This method optimizes the individual training parameters based
    on the frequency of the updates, which makes it ideal for working with smaller
    datasets. The other main benefit is that it allows you to not have to tune the
    learning rate. However, a big weakness with this method is squared gradients causing
    the learning rate to become so small that the network stops learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AdaDelta**: This method is an extension to AdaGrad, which deals with the
    squared gradients and vanishing learning rate. It does this by fixing the learning
    rate window to a particular minimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RMSProp**: Developed by Geoff Hinton, the grandfather of deep learning, this
    is a technique to manage the vanishing learning rate problem in AdaGrad. As you
    can see in the graph, it is on par with AdaDelta for the sample shown.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptive Moment Estimation** (**Adam**): This is another technique that attempts
    to control that gradient using a more controlled version of Momentum. It is often
    described as Momentum plus RMSProp, since it combines the best of both techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AdaMax**: This method is not shown on the performance graph but is worth
    mentioning. It is an extension to Adam that generalizes each iteration of an update
    applied to the momentum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nadam**: This is another method not on the graph; it is a combination of
    Nesterov-accelerated Momentum and Adam. The vanilla Adam just uses a Momentum
    term that is not accelerated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AMSGrad**: This is a variation of Adam that works best when Adam is shown
    to be unable to converge or wobble. This is caused by the algorithm failing to
    adapt learning rates and is fixed by taking a maximum rather than an average of
    previously squared gradients. The difference is subtle and tends to prefer smaller
    datasets. Keep this option in the back of your mind as a possible future tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That completes our short overview of optimizers; be sure to refer to the exercises
    at the end of the chapter for ways you can explore them further. In the next section,
    we build our own GAN that can generate textures we can use in games.
  prefs: []
  type: TYPE_NORMAL
- en: Wasserstein GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can most certainly appreciate by now, GANs have wide and varied applications,
    several of which apply very well to games. One such application is the generation
    of textures or texture variations. We often want slight variations in textures
    to give our game worlds a more convincing look. This is and can be done with **shaders**,
    but for performance reasons, it is often best to create **static assets**.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this section, we will build a GAN project that allows us to generate
    textures or height maps. You could also extend this concept using any of the other
    cool GANs we briefly touched on earlier. We will be using a default implementation
    of the Wasserstein GAN by Erik Linder-Norén and converting it for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the major hurdles you will face when first approaching deep learning
    problems is shaping data to the form you need. In the original sample, Erik used
    the MNIST dataset, but we will convert the sample to use the CIFAR100 dataset.
    The CIFAR100 dataset is a set of color images classified by type, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87786072-1218-45e0-868c-b71599064740.png)'
  prefs: []
  type: TYPE_IMG
- en: CIFAR 100 dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, though, let''s open up `Chapter_3_wgan.py` and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the Python file and review the code. Most of the code will look the same
    as the DCGAN we already looked at. However, there are a few key differences we
    want to look at, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The Wasserstein GAN uses a distance function in order to determine the cost
    or loss for each training iteration. Along with this, this form of GAN uses multiple
    critics rather than a single discriminator to determine cost or loss. Training
    multiple critics together improves performance and handles the vanishing gradient
    problem we often see plaguing GANs. An example of a different form of GAN training
    is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b1e15842-e119-44ed-9f30-a52a0aab0e32.png)'
  prefs: []
  type: TYPE_IMG
- en: Training performance across GAN implementations ([https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: A WGAN overcomes the gradient problem by managing cost through a distance function
    that determines the cost of moving, rather than a difference in error values.
    A linear cost function could be as simple as the number of moves a character needs
    to take in order to spell a word correctly. For example, the word *SOPT* would
    have a cost of 2, since the *T* character needs to move two places to spell *STOP*
    correctly. The word *OTPS* has a distance cost of *3 (S) + 1 (T) = 4* to spell
    *STOP* correctly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Wasserstein distance function essentially determines the cost of transforming
    one probability distribution to another. As you can imagine, the math to understand
    this can be quite complex, so we will defer that to the more interested reader.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the example. This sample can take a significant time to run, so be patient.
    Also, this sample has been shown to have trouble training on some GPU hardware.
    If you find this to be the case, just disable the use of GPU.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the sample runs, open the `images` folder from the same folder as the Python
    file and watch the training images generate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the sample for as long as you feel the need to in order to understand how
    it works. This sample can take several hours even on advanced hardware. When you
    are done, move on to the next section, and we will see how to modify this sample
    for generating textures.
  prefs: []
  type: TYPE_NORMAL
- en: Generating textures with a GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the things so rarely covered in advanced deep learning books is the
    specifics of shaping data to input into a network. Along with shaping data is
    the need to alter the internals of a network to accommodate the new data. The
    final version of this example is `Chapter_3_3.py`, but for this exercise, start
    with the `Chapter_3_wgan.py` file and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by changing the training set of data from MNIST to CIFAR by swapping
    out the imports like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'At the start of the class, we will change the image size parameters from 28
    x 28 grayscale to 32 x 32 color like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, move down to the `train` function and alter the code as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This code loads the images from the CIFAR100 dataset and sorts through them
    by label. Labels are stored in the `y` variable, and the code loops through all
    the downloaded images and isolates those to one specific set. In this case, we
    are using the label `33`, which corresponds to forest images. There are 100 categories
    in the CIFAR100, and we are selecting one category that holds 500 images. Feel
    free to try to generate other textures from other categories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rest of the code is fairly straightforward, except for the `np.reshape`
    call where we reshape the data into a list of 500 images `32x32` pixels by three
    channels. You may also want to note that we do not need to expand the axis to
    three as we did before. This is because our image is already scaled to three channels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now need to go back to the generator and critic models and alter that code
    slightly. First, we will change the generator like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The boldface code denotes the changes. All we are doing for this model is converting
    the `7x7` original feature map to `8x8`. Recall that the original full image size
    is `28x28`. Our convolution starts with a `7x7` feature map, doubled twice, which
    equals `28x28`. Since our new image size is `32x32`, we need to convert our network
    to start with `8x8` feature maps, which doubled twice equals `32x32`, the same
    size as the CIFAR100 images. Fortunately, we can leave the critic model as it
    is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we add a new function to save samples of the original CIFAR images, and
    this is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `save_images` function outputs a sampling of the original images and is
    called by the following code in the `train` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The new code is in boldface and just outputs what a sampling of the originals
    looks like, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ecd433e1-66ac-49a0-817c-0eb87d7531ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of the original images
  prefs: []
  type: TYPE_NORMAL
- en: Run the sample and observe the output in the `images` folder again labeled `cifar`,
    showing the result of training. Again, this sample can take some time to run,
    so read on to the next section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the sample runs, you can observe how the GAN is training to match the images.
    The benefit here is that you can generate various textures easily using a variety
    of techniques. You can use these as textures or height maps in Unity or another
    game engine. Before we finish up this section, let's jump into some normalization
    and other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Batch normalization**, as its name suggests, normalizes the distribution
    of weights in a layer around some mean of 0\. This allows for the network to use
    a higher learning while still avoiding a vanishing or exploding gradient problem.
    It is due to the weights being normalized, which allows for fewer shifts or training
    wobble, as we have seen before.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By normalizing the weights in a layer, we allow for the network to use a higher
    learning rate and thus train faster. Also, we can avoid or reduce the need to
    use `DropOut`. You will see that we use the standard term, shown here, to normalize
    the layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Recall from our discussions of optimizers that momentum controls how quickly
    or slowly we want to decrease the training gradient. In this case, momentum refers
    to the amount of change of the mean or center of the normalized distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at another special layer called LeakyReLU.
  prefs: []
  type: TYPE_NORMAL
- en: Leaky and other ReLUs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LeakyReLU** adds an activation layer that allows for negative values to have
    a small slope, rather than just 0, as in the case of the standard ReLU activation
    function. The standard ReLU encourages sparsity in the network by only allowing
    neurons with positive activation to fire. However, this also creates a dead neuron
    state, where parts of the network essentially die off or become untrainable. To
    overcome this issue, we introduce a leaky form of ReLU activation called LeakyReLU.
    An example of how this activation works is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0377b7b9-e422-4d78-9c9e-c8dc7c37d9b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a leaky and parametric ReLU
  prefs: []
  type: TYPE_NORMAL
- en: Pictured in the preceding diagram is **Parametric ReLU**, which is similar to
    Leaky, but it allows the network to train the parameter itself. This allows the
    network to adjust on its own, but it will take longer to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other ReLU variants you can use are summarized here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exponential Linear** (**ELU, SELU**): These forms of ReLU activate as shown
    in the diagram as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/cea2fa2e-b196-457a-9603-d4cf88bcff72.png)'
  prefs: []
  type: TYPE_IMG
- en: ELU and SELU
  prefs: []
  type: TYPE_NORMAL
- en: '**Concatenated ReLU** (**CReLU**): This joins the regular and leaky form together
    to provide a new function that produces two output values. For positive values,
    it generates *[0,x],* while for negative values, it returns *[x,0]*. One thing
    to note about this layer is the doubling of output, since two values are generated
    per neuron.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ReLU-6**: The value of 6 is arbitrary but allows for the network to train
    sparse neurons. Sparsity is of value because it encourages the network to learn
    or build stronger weights or bonds. The human brain has been shown to function
    in a sparse state, with only a few activated neurons at a time. You will often
    hear the myth that we only use 10% of our brain at a time at most. This may very
    well be true, but the reasons for this are more mathematical than us being able
    to use our entire brain. We do use our entire brain, just not all of it at the
    same time. Stronger individual weights, encouraged by sparsity, allow for the
    network to make better/stronger decisions. Fewer weights also encourage less overfitting
    or memorization of data. This can often happen in deep networks with thousands
    of neurons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization is another technique we will often use to trim or reduce unneeded
    or weights and create sparse networks. We will have a few opportunities to look
    at regularization and sparsity later in the coming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we use what we have learned to build a working music GAN
    that can generate game music.
  prefs: []
  type: TYPE_NORMAL
- en: A GAN for creating music
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our final grand example of this chapter, we are going to look at generating
    music with GANs for games. Music generation is not especially difficult, but it
    does allow us to see a whole variation of a GAN that uses LSTM layers to identify
    sequences and patterns in music. Then it attempts to build that music back from
    random noise to a passable sequence of notes and melodies. This sample becomes
    ethereal when you listen to those generated notes and realize the tune originates
    from a computer brain.
  prefs: []
  type: TYPE_NORMAL
- en: The origins of this sample are pulled from GitHub, [https://github.com/megis7/musegen](https://github.com/megis7/musegen),
    and developed by Michalis Megisoglou. The reason we look at these code examples
    is so that we can see the best of what others have produced and learn from those.
    In some cases, these samples are close to the original, and others not so much.
    We did have to tweak a few things. Michalis also produced a nice GitHub README
    on the code he built for his implementation of **museGAN**, music generation with
    GAN. If you are interested in building on this example further, be sure to check
    out the GitHub site as well. There are a few implementations of museGAN available
    using various libraries; one of them is TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: We use Keras in this example in order to make this example easier to understand.
    If you are serious about using TensorFlow, then be sure to take a look at the
    TensorFlow version of museGAN as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example trains the discriminator and generator separately, which means
    it needs to have the discriminator trained first. For our first run, we will run
    this example with the author''s previously generated models, but we still need
    some setup; let''s follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to install a couple of dependencies. Open an Anaconda or Python
    window as an admin and run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`Music21` is a Python library for loading MIDI files. **MIDI** is a music interchange
    format used to describe, as you might have guessed, music/notes. The original
    models were trained on a collection of MIDI files that describe 300 chorales of
    Bach''s music. You can locate the project by navigating to the `musegen` folder
    and running the script.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the project folder and execute the script that runs the previously
    trained models like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This will load the previously saved modelsand use those models to train the
    generator and generate music. You could, of course, train this GAN on other MIDI
    files of your choosing later as needed. There are plenty of free sources for MIDI
    files from classical music, to TV theme music, games, and modern pop. We use the
    author's original models in this example, but the possibilities are endless.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Loading the music files and training can take a really long time, as training
    typically does. So, take this opportunity to look at the code. Open up the `musegen.py`
    file located in the project folder. Take a look at around line 39, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This section of code loads the previously trained model from an `hdf5` or hierarchical
    data file. The preceding code sets up a number of variables that define the notes
    to a vocabulary we will use to generate new notes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate the `notegenerator.py` file located in the same project folder. Take
    a look at the creation of the model code, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note how we have changed from using `Conv2D` layers to `LSTM` layers, since
    we have gone from image recognition to sequence or note pattern recognition. We
    have also gone from using more straightforward layers to a complex time-distributed
    architecture. Also, the author used a concept known as **variational auto encoding**
    in order to determine the distribution of notes in a sequence. This network is
    the most complex we have looked at so far, and there is a lot going on here. Don't
    fret too much about this example, except to see how the code flows. We will take
    a closer look at more of these type of advanced time- distributed networks in
    [Chapter 4](a8e699ff-c668-4601-842d-4c6e06c47a61.xhtml)*, Building a Deep Learning
    Gaming Chatbot*[.](http://Chapter_4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let the sample run and generate some music samples into the `samples/note-generator`
    folder. As we get into more complex problems, our training time will go from hours
    to days for very complex problems or more. It is possible that you could easily
    generate a network that you would not have the computing power to train in a reasonable
    time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the folder and double-click on one of the sample files to listen to the
    generated MIDI file. Remember, this music was just generated by a computer brain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is a lot of code that we did not cover in this example. So, be sure to
    go back and go through the `musegen.py` file to get a better understanding of
    the flow and types of layers used to build the network generator. In the next
    section, we explore how to train this GAN.
  prefs: []
  type: TYPE_NORMAL
- en: Training the music GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we get into training this network, we will look at the overall architecture
    as depicted in the author''s original GitHub source:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e630248f-63d9-43d6-9d64-616a450543f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of museGAN network architecture
  prefs: []
  type: TYPE_NORMAL
- en: The networks are almost identical until you look closer and see the subtle differences
    in the LSTM layers. Note how one set uses double the units as the other model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can generate music models by running the following command at the Python
    or Anaconda prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This script loads the sample data and generates the models we use in the `musegen.py`
    file later when we create original music. Open up the `note-generator.py` file
    with the main parts shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: The code was modified from the original to make it more Windows-compatible and
    cross-platform. Again, this is certainly not a criticism of the author's excellent
    work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This code uses the Music21 library to read the MIDI notes and other music forms
    from the corpus of music you can use for your own testing. This training dataset
    is an excellent way to generate other sources of music and is composed of the
    following: [http://web.mit.edu/music21/doc/moduleReference/moduleCorpus.html](http://web.mit.edu/music21/doc/moduleReference/moduleCorpus.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can further modify this example by modifying the contents or adding additional
    configuration options in the `config.py` file as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The previous sample is great for exploring the generation of music. A more practical
    and potentially useful example will be introduced in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Generating music via an alternative GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another example of music generation is also included in the `Chapter_3` source
    folder, called **Classical-Piano-Composer**, with the source located at [https://github.com/Skuldur/Classical-Piano-Composer](https://github.com/Skuldur/Classical-Piano-Composer),
    developed by Sigurður Skúli. This example uses a full set of Final Fantasy MIDI
    files as source inspiration for the music generation and is a great practical
    example for generating your own music.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to run this sample, you need to run the `lstm.py` first using the
    following command from the `Classical-Piano-Composer` project folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This sample can take a substantial time to train, so be sure to open the file
    and read through what it does.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the models are trained, you can run the generator by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This script loads the trained model and generates the music. It does this by
    encoding the MIDI notes into network input in terms of sequences or sets of notes.
    What we are doing here is breaking up the music files into short sequences, or
    a music snapshot if you will. You can control the length of these sequences by
    adjusting the `sequences_length` property in the code file.
  prefs: []
  type: TYPE_NORMAL
- en: The great thing about this second example is the ability to download your own
    MIDI files and put them in the appropriate input folder for training. It is also
    interesting to see how both projects use a similar three-layer LSTM structure
    but vary quite widely in other forms of execution.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about audio or music development for games and especially
    for Unity, check out the book *Game Audio Development with Unity 5.x*, by Micheal
    Lanham. This book can show you many more techniques for working with audio and
    music in games.
  prefs: []
  type: TYPE_NORMAL
- en: Both music samples can take some time to train and then generate music, but
    it is certainly worth the effort to run through both examples and understand how
    they work. GANs have innovated the way we think of training neural networks and
    what type of output they are able to produce. As such, they certainly have a place
    in generating content for games.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Take some time to reinforce your learning by undertaking the following exercises:'
  prefs: []
  type: TYPE_NORMAL
- en: What type of GAN would you use to transfer styles on an image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What type of GAN would you use to isolate or extract the style?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the number of critics used in the Wasserstein GAN example and see the
    effect it has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the first GAN, the DCGAN, to improve training performance using any technique
    you learned in this chapter. How did you increase training performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the BatchNormalization momentum parameter and see what effect it has
    on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify a few of the samples by changing the activation from LeakyReLU to another
    advanced form of activation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify the Wasserstein GAN example to use your own textures. There is a sample
    data loader available in the downloaded code sample for the chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download one of the other reference GANs from [https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)
    and modify that to use your own dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alter the first music generation GAN to use a different corpus.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your own MIDI files to train the second music generation GAN example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (BONUS) Which music GAN generated better music? Is it what you expected?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You certainly don't have to work through all these exercises, but give a few
    a try. Putting this knowledge to practice right away can substantially improve
    your understanding of the material. Practice does make perfect, after all.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at generative adversarial networks, or GANs, as a
    way to build DNNs that can generate unique content based on copying or extracting
    features from other content. This also allowed us to explore unsupervised training,
    a method of training that requires no previous data classification or labeling.
    In the previous chapter, we used supervised training. We started with looking
    at the many variations of GANs currently making an impression in the DL community.
    Then we coded up a deep convolutional GAN in Keras, followed by the state-of-the-art
    Wasserstein GAN. From there, we looked at how to generate game textures or height
    maps using sample images. We finished the chapter off by looking at two music-generating
    GANs that can generate original MIDI music from sampled music.
  prefs: []
  type: TYPE_NORMAL
- en: For the final sample, we looked at music generation with GANs that relied heavily
    on RNNs (LSTM). We will continue our exploration of RNNs when we look at how to
    build DL chatbots for games.
  prefs: []
  type: TYPE_NORMAL
