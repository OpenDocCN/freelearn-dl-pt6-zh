- en: A Deeper Dive into Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will encounter more in-depth details of neural networks.
    We will start from building a perceptron. Moving on, we will learn about activation
    functions. And we will also be training our first perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: From the biological to the artificial neuron – the perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning through errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling the perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A single layered network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the biological to the artificial neuron – the perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have briefly familiarized ourselves with some insights on the nature
    of data processing, it''s about time we see how the artificial cousins of our
    own biological neurons work themselves. We start with a creation of Frank Rosenblatt,
    dating back to the 1950s. He called this invention of his the **Perceptron** ([http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)).
    Essentially, you can think of the perceptron as a single neuron in an **artificial
    neural network** (**ANN**). Understanding how a single perceptron propagates information
    forward will serve as an excellent stepping stone to understanding the more state-of-the-art
    networks that we will face in later chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70ac79de-d77e-4e4d-8cc4-516a38d34757.png)'
  prefs: []
  type: TYPE_IMG
- en: Building a perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For now, we will define a perceptron using six specific mathematical representations
    that demonstrate its learning mechanism. These representations are the inputs,
    weights, bias term, summation, and the activation function. The output will be
    functionally elaborate upon here under.
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember how a biological neuron takes in electrical impulses from its dendrites?
    Well, the perceptron behaves in a similar fashion, yet it prefers to ingest numbers
    in lieu of electricity. Essentially, it takes in feature inputs, as shown in the
    preceding diagram. This particular perceptron only has three input channels, these
    being *x[1]*, *x[2]*, and *x[3]*. These feature inputs (*x[1]*, *x[2]*, and *x[3]*)
    can be any independent variable that you choose to represent your observation
    by. Simply speaking, if we want to predict whether it will be sunny or rainy on
    any given day, we can record independent variables such as temperature and air
    pressure per day, along with the appropriate output class of that day (whether
    the day itself was sunny or rainy). We will then feed these independent variables
    that we have, one day at a time, as input features into our perceptron model.
  prefs: []
  type: TYPE_NORMAL
- en: Weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, we know how data flows into our simple neuron, but how do we transform this
    data into actionable knowledge? How do we build a model that takes these input
    features, and represents them in a manner that helps us predict the weather on
    any given day?
  prefs: []
  type: TYPE_NORMAL
- en: Giving us two features that we can use as input in our model, for the binary
    classification task of determining *rainy* or *sunny* days.
  prefs: []
  type: TYPE_NORMAL
- en: Well, the first step will be to pair up each input feature with a respective
    weight. You can think of this weight as the relative importance this specific
    input feature has with respect to the output class that we are trying to predict.
    In other words, the weight for our input feature temperature should ideally reflect
    exactly how much this input feature is related to the output classes. These weights
    are randomly initialized at first, and are learned as our models see more and
    more data. Our hope here in doing this is that after enough iterations through
    our data, these weights will be nudged in the right direction, and learn the ideal
    configuration of temperature and pressure values that correspond to rainy and
    sunny days. We actually know, from domain knowledge, that temperature is highly
    correlated to weather, and hence will expect our model to, ideally, learn heavier
    weights for this feature, as data propagates through it. This can be somewhat
    comparable to the myelin sheath that covers axons in a biological neuron. If a
    specific neuron fires frequently, its myelin sheath is said to thicken, which
    insulates the axon, and allows the neuron to communicate faster next time.
  prefs: []
  type: TYPE_NORMAL
- en: Summation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, now we have our input features flowing into our perceptron, with each input
    feature paired up with a randomly initialized weight. The next step is fairly
    easy. First, we represent all three of our features and their weights as two different
    3 x 1 matrices. We want to use these two matrices to represent the combined effect
    of our input features and their weights. As you will recall from high school mathematics,
    you cannot actually multiply two 3 x 1 matrices together. So, we have to perform
    a little mathematical trick to reductively represent our two matrices as one value.
    We simply transpose our feature matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fc54096-4dd9-4917-84e3-b125cae8fde0.png)'
  prefs: []
  type: TYPE_IMG
- en: We can use this new transposed feature matrix (of dimension 3 x 1) and multiply
    it with the weight matrix (of dimension 1 x 3). When we perform a matrix-by-matrix
    multiplication, the result we obtain is referred to as the **dot product** of
    these two matrices. In our case, we compute the dot product of our transposed
    feature matrix and our weights matrix. Doing so, we are able to reduce our two
    matrices to one single scalar value, which represents the collective influence
    of all of our input features and their respective weights. Now, we will see how
    we can use this collective representation and gauge it against a certain threshold
    to assess the quality of this representation. In other words, we will use a function
    to assess whether this scalar representation encodes a useful pattern to remember.
    A useful pattern will ideally be one that helps our model distinguish between
    the different classes in our data, and thereby output correct predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing non-linearity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So now, we know how data enters a perceptron unit, and how associated weights
    are paired up with each input feature. We also know how to represent our input
    features, and their respective weights, as *n* x 1 matrices, where *n* is the
    number of input features. Lastly, we saw how we can transpose our feature matrix
    to be able to compute its dot product with the matrix containing its weights.
    This operation left us with one single scalar value. So, what's next? This is
    not a bad time to take a step back and ponder over what we are trying to achieve,
    as this will help us to understand the idea behind why we want to employ something
    like an activation function.
  prefs: []
  type: TYPE_NORMAL
- en: Well, you see, real-word data is often non-linear. What we mean by this is that
    whenever we attempt to model an observation as a function of different inputs,
    this function itself cannot be represented linearly, or on a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: 'If all patterns in data only constituted straight lines, we would probably
    not be discussing neural networks at all. Techniques such as **Support Vector
    Machines** (**SVMs**) or even linear regression already excel at this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2d549a7-a668-4df6-bc93-beabd654adb4.png)'
  prefs: []
  type: TYPE_IMG
- en: Modeling sunny and rainy days with temperature, for example, will produce a
    non-linear curve. In effect, this just means that we cannot possibly separate
    our decision boundary using a straight line. In other words, on some days, it
    may rain despite high temperatures, and on other days, it may remain sunny despite
    low temperatures.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because temperature is not linearly related to the weather. The weather
    outcome on any given day is very likely to be a complex function, involving interactive
    variables such as wind speed, air pressure, and more. So, on any given day, a
    temperature of 13 degrees could mean a sunny day in Berlin, Germany, but a rainy
    day in London, UK:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59bf7c16-f222-4ddf-b179-f6e2ef347415.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are some cases, of course, where a phenomenon may be linearly represented.
    In physics, for example, the relationship between the mass of an object and its
    volume can be linearly defined, as shown in the following screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56356469-bfba-4e93-a959-43701ceade75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is an example of a non-linear function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13255c57-c769-455a-bd7a-69e9929d72b0.png)'
  prefs: []
  type: TYPE_IMG
- en: '| **A linear function** | **A non-linear function** |'
  prefs: []
  type: TYPE_TB
- en: '| *Y = mx + b* | *Y = mx² + b* |'
  prefs: []
  type: TYPE_TB
- en: Here, *m *is the slope of the line, *x* is any point (an input or an *x*-value)
    on the line, and *b* is where the line crosses the *y* axis.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, linearity is often not guaranteed with real-world data, as we
    model observations using multiple features, each of which could have a varied
    and disproportional contribution towards determining our output classes. In fact,
    our world is extremely non-linear, and hence, to capture this non-linearity in
    our perceptron model, we need it to incorporate non-linear functions that are
    capable of representing such phenomena. By doing so, we increase the capacity
    of our neuron to model more complex patterns that actually exist in the real world,
    and draw decision boundaries that would not be possible, were we to only use linear
    functions. These types of functions, used to model non-linear relationships in
    our data, are known as **activation functions**.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, basically what we have done so far is represent our different input features
    and their weights in a lower dimension, as a scalar representation. We can use
    this reduced representation and pass it through a simple non-linear function that
    tells us whether our representation is above or below a certain threshold value.
    Similar to the weights we initialized before, this threshold value can be thought
    of as a learnable parameter of our perceptron model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, we want our perceptron to figure out the ideal combinations
    of weights and a threshold, allowing it to reliably match our inputs to the correct
    output class. Hence, we compare our reduced feature representation with a threshold
    value, and then activate our perceptron unit if we are above this threshold value,
    or do nothing otherwise. This very function that compares our reduced feature
    value against a threshold, is known as an **activation function**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e39b47b9-f526-41a4-9bb0-c9d9193c74e9.png)'
  prefs: []
  type: TYPE_IMG
- en: These non-linear functions come in different forms, and will be explored in
    further detail in subsequent chapters. For now, we present two different activation
    functions; the **heavy set step** and the **logistic sigmoid** activation functions.
    The perceptron unit that we previously showed you was originally implemented with
    such a heavy step function, leading to binary outputs of 1 (active) or 0 (inactive).
    Using the step function in our perceptron unit, we observe that a value above
    the curve will lead to activation (1), whereas a value below or on the curve will
    not lead to the activation unit firing (0). This process may be summarized in
    an algebraic manner as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the heavy step function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/960b3966-48d8-4dd9-8ce8-4c83dc1a62a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output threshold formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33960858-cb2e-4c20-9ada-1370f805a9bb.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In essence, a step function is not really a non-linear function, as it can
    be rewritten as two finite linear combinations. Hence, this piece-wise constant
    function is not very flexible in modeling real-world data, which is often more
    probabilistic than binary. The logistic sigmoid, on the other hand, is indeed
    a non-linear function, and may model data with more flexibility. This function
    is known for **squishing** its input to an output value between 0 and 1, which
    makes it a popular function for representing probabilities, and is a commonly
    employed activation function for neurons in modern neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6b9bb4c-33a7-4d97-a7ce-4b4c89f9e714.png)'
  prefs: []
  type: TYPE_IMG
- en: Each type of activation function comes with its own set of advantages and disadvantages
    that we will also delve into in later chapters. For now, you can intuitively think
    about the choice of different activation functions as a consideration based on
    your specific type of data. In other words, we ideally try to experiment and pick
    a function that best captures the underlining trends that may be present in your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we will employ such activation functions to threshold the incoming inputs
    of a neuron. Inputs are consequentially transformed and gauged against this activation
    threshold, in turn causing a neuron to fire, or abstain therefrom. In the following
    illustration, we can visualize the decision boundary produced by an activation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b89a9e7b-9b18-4410-af79-11b3f253cb6e.png)'
  prefs: []
  type: TYPE_IMG
- en: Understanding the role of the bias term
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, now we have a good idea of how data enters our perceptron; it is paired
    up with weights and reduced through a dot product, only to be compared to an activation
    threshold. Many of you may ask at this point, *what if we wanted our threshold
    to adapt to different patterns in data?* In other words, what if the boundaries
    of the activation function were not ideal to separately identify the specific
    patterns we want our model to learn? We need to be able to play with the form
    of our activation curve, so as to guarantee some flexibility in the sort of patterns
    each neuron may locally capture.
  prefs: []
  type: TYPE_NORMAL
- en: 'And how exactly will we shape our activation function? Well, one way to do
    this is by introducing a **bias term** into our model. This is depicted by the
    arrow leaving the first input node (marked with the number ''1'') in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a0db2c1-49f6-4a50-ae58-30be9a9a2dbb.png)'
  prefs: []
  type: TYPE_IMG
- en: Representatively, we can think of this bias as the weight of a **fictional**
    input*.* This fictional input is said to be always present, allowing our activation
    unit to fire at will, without requiring any input features to be explicitly present
    (as shown in the green circle previously). The motivation behind this term is
    to be able to manipulate the shape of our activation function, which in turn impacts
    the learning of our model. We want our shape to flexibly fit different patterns
    in our data. The weight of the bias term is updated in the same manner as all
    the other weights are. What makes it different is that it is not disturbed by
    its input neuron, which simply always holds a constant value (as shown previously).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how do we actually influence our activation threshold using this bias term?
    Well, lets consider a simplified example. Suppose we have some outputs generated
    by a stepped activation function, which produces either a ''0''or a ''1''for every
    output, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7c86ad9-1663-40a0-bdcd-866715c87e7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then rewrite this formula to include the bias term, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/330ee91e-aa4e-411f-a4e7-add0eb9cb6f6.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, we are using yet another mathematical trick and redefining the
    threshold value as the negative of our bias term (*Threshold = -(bias)*). This
    bias term is randomly initialized at the beginning of our training session, and
    is iteratively updated as the model sees more examples, and learns from these
    examples. Hence, it is important to understand that although we randomly initialize
    model parameters, such as the weights and biases, our hope is to actually show
    the model enough input examples and their corresponding output classes. In doing
    so, we want our model to learn from its errors, searching for the ideal parametric
    combinations of weights and bias corresponding to the correct output classes.
    Do note that when we initialize different weights, what we are actually doing
    is modifying the steepness of our activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows how different weights impact the steepness of a sigmoid
    activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9682b24-892c-4c74-8889-5664bfba37d4.png)'
  prefs: []
  type: TYPE_IMG
- en: We essentially hope that by tinkering with the steepness of our activation function,
    we are able to ideally capture a certain underlying pattern in our data. Similarly,
    when we initialize different bias terms, what we are actually trying to do is
    shift the activation function in an optimal manner (to the left or to the right),
    so as to trigger activation corresponding to specific configurations of input
    and output features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows how different bias terms impact the position of a
    sigmoid activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ebaca6c-ca47-458c-8054-cc4482fc514a.png)'
  prefs: []
  type: TYPE_IMG
- en: Output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our simple perceptron model, we denote the actual output class as *y*, and
    the predicted output classes as ![](img/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png).
    The output classes simply refer to the different classes in our data that we are
    trying to predict. To elaborate, we use the input features (*x[n]*), such as temperature
    (*x[1]*) and air pressure (*x[2]*) on a given day, to predict whether that specific
    day is a sunny or rainy one (![](img/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png)).
    We can then compare our model''s predictions with the actual output class of that
    day, denoting whether that day was indeed rainy or sunny. We can denote this simple
    comparison as (![](img/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png) - *y*), which
    allows us to observe by how much our perceptron missed the mark, on average. But
    more on that later. For now, we can represent our entire prediction model using
    all that we have learned so far, in a mathematical manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4f8f3d6-b751-40c5-b4e1-4bd2a691bdab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram displays an example of the preceding formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dae36234-c075-4bd3-b38a-5522521562b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we graphically plot our prediction line (![](img/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png))
    shown precedingly, we will get to visualize the decision boundary separating our
    entire feature space into two subspaces. In essence, plotting a prediction line
    simply gives us an idea of what the model has learned, or how the model chooses
    to separate the hyperplane containing all our data points into the various output
    classes that interest us. Actually, by plotting out this line, we are able to
    visualize how well our model does by simply placing observations of sunny and
    rainy days on this feature space, and then checking whether our decision boundary
    ideally separates the output classes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79bf9bfc-0a83-49bc-9c33-867ffd4ff6bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Learning through errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All we essentially do to our input data is compute a dot product, add a bias
    term, pass it through a non-linear equation, and then compare our prediction to
    the real output value, taking a step in the direction of the actual output. This
    is the general architecture of an artificial neuron. You will soon see how this
    structure, configured repetitively, gives rise to some of the more complex neural
    networks around.
  prefs: []
  type: TYPE_NORMAL
- en: Exactly how we converge to ideal parametric values by taking a step in the right
    direction is through a method known as the **backward propagation of errors**,
    or **backpropagation** for short. But to propagate errors backwards, we need a
    metric to assess how well we are doing with respect to our goal. We define this
    metric as a loss, and calculate it using a loss function. This function attempts
    to incorporate the residual difference between what our model thinks it sees,
    and the actual ground reality. Mathematically speaking, this is shown as (*y*
    - ![](img/7a282e6b-02ad-450b-bfc8-37467f25c131.png)). It is important to understand
    here that the loss values can actually be defined as a function of our model parameters.
    Thus, tweaking these parameters permits us to reduce our loss and get our predictions
    closer to actual output values. You will see exactly what we mean by this when
    we review the full training process of our perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: The mean squared error loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A prominently used loss function is the **mean squared error** (**MSE**) function,
    represented algebraically in the following formula. As you will notice, this function,
    at its core, simply compares the actual model output (*y*) with the predicted
    model output (![](img/1a12876d-d701-4c8b-9986-b147145150ab.png)). This function
    is particularly helpful for us to asses our predictive power, as this function
    models the loss quadratically. That is to say, if our model performs poorly, and
    our predicted and actual output values become more and more divergent, the loss
    increases by an exponent of two, allowing us to penalize higher errors more severely:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97716134-e976-4bdc-a971-85dc25d9b587.png)'
  prefs: []
  type: TYPE_IMG
- en: The average MSE between output values *y[i]*, and predicted values ![](img/a8625ea3-68b5-432c-94d1-e0da34ae2337.png).
  prefs: []
  type: TYPE_NORMAL
- en: We will revisit this notion to understand how we reduce the difference between
    what our model predicts versus the actual output using various types of loss functions.
    For now, it suffices to know that our model's loss may be minimized through a
    process known as **gradient descent**. As we will soon see, gradient descent is
    simply grounded in calculus and implemented through backpropagation-based algorithms.
    The process of mathematically reducing the difference between predicted and actual
    output, by tuning the parameters of a network, is actually what makes the network
    learn. This tuning occurs as we train our model, by showing it new examples of
    inputs and associated outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Training a perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have a clear grasp of how data actually propagates through our perceptron.
    We also briefly saw how the errors of our model can be propagated backwards. We
    use a loss function to compute a loss value at each training iteration. This loss
    value tells us how far our model's predictions lie from the actual ground truth.
    But what then?
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the loss value gives us an indication of the difference between our predicted
    and actual outputs, it stands to reason that if our loss value is high, then there
    is a big difference between our model's predictions and the actual output. Conversely,
    a low loss value indicates that our model is closing the distance between the
    predicted and actual output. Ideally, we want our loss to converge to zero, which
    means that there is in effect not much difference between what our model thinks
    it sees, and what it is actually shown. We make our loss converge to zero by simply
    using another mathematical trick, grounded in calculus. How, you ask?
  prefs: []
  type: TYPE_NORMAL
- en: Loss as a function of model weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, remember when we said that we can also think of our loss value as a *function*
    of the model parameters? Consider this. Our loss value tells us how far our model
    is from the actual prediction. This same loss can also be redefined as a function
    of our model's weight (θ). Recall that these weights are what actually lead to
    our model's prediction at each training iteration. Thinking about this intuitively,
    we want to be able to change our model weights with respect to the loss, so as
    to reduce our prediction errors as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'More mathematically put, we want to minimize our loss function so as to iteratively
    update the weights for our model, and ideally converge to the best possible weights.
    These will be the best weights in the sense that they will best be able to represent
    features that are predictive of our output classes. This process is known as **loss
    optimization**, and can be mathematically illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bd1ef52-965a-4e97-9f0d-89a1f67b0f66.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient descent
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we represent our ideal model weights (θ^(***)) as the minima of our
    loss function over the entire training set. In other words, for all the feature
    inputs and labeled outputs we show our model, we want it to find a place in our
    feature space where the overall difference between the actual (*y*) and predicted
    (![](img/0577a5df-72ab-422b-9fc6-41d0b83225a7.png)) values are the smallest. The
    feature space we refer to is all the different possible combinations of weights
    that the model may initialize. For the sake of having a simplified representation
    of our loss function, we denote it as *J*(θ). We can now iteratively solve for
    the minimum of our loss function, *J*(θ), and descend the hyperplane to converge
    to a global minimum. This process is what we call **gradient descent**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34fc1e60-e821-4528-9982-7f694c6bfdae.png)'
  prefs: []
  type: TYPE_IMG
- en: Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the more mathematically oriented, you must be wondering how exactly we
    descend our gradient iteratively. Well, as you know, we start by initializing
    random weights to our model, feed in some data, compute dot products, and pass
    it through our activation function along with our bias to get a predicted output.
    We use this predicted output and the actual output to estimate the errors in our
    model''s representations, using the loss function. Now here comes the calculus.
    What we can do now is differentiate our loss function, *J*(θ), with respect to
    the weights of our model (θ). This process essentially lets us compare how changes
    in our model''s weights affect the changes in our model''s loss. The result of
    this differentiation gives us the gradient of our J(θ) function at the current
    model weight (θ) along with the direction of the highest ascent. By highest ascent,
    we meant the direction in which the difference between prediction and output values
    seem higher. Hence, we simply take a step in the opposite direction, and descend
    the gradient of our loss function, *J*(θ), with respect to our model weights (θ).
    We present an algorithmic representation of this concept, in form of pseudo-code,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fcde7a14-32f2-406f-b062-ac487bd7a92b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following graph is a visualization of the gradient descent algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a23c2d4c-88b7-44d0-85a8-7513668a9f6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we see, the gradient descent algorithm allows us to take steps down the
    loss hyperplane, until our model converges to some optimal parameters. At this
    point, the difference between our model''s predictions and reality will be quite
    negligible, and we can consider our model trained!:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8975d35f-d47b-4475-99f6-4efeaf0f526d.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we compute the changes in our network weights with respect to the changes
    of the values generated by the loss function (i.e. the gradients of the network
    weights). Then, we proportionally update the network weights, in the opposite
    direction of the computed gradients, so as to adjust for the errors.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are familiar with the backpropagation algorithm as well as the notion
    of gradient decscent, we can address more technical questions. Questions like, *how
    do we actually compute this gradient?* As you know, our model does not have the
    liberty of visualizing the loss landscape, and picking out a nice path of descent.
    In fact, our model cannot tell what is up, or what is down. All it knows, and
    will ever know, is numbers. However, as it turns out, numbers can tell a lot!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s reconsider our simple perceptron model to see how we can backpropagate
    its errors by computing the gradient of our loss function, *J*(θ), iteratively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d983710e-6717-49bc-83c6-45c5464dc97d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What if we wanted to see how changes in the weights of the second layer impact
    the changes in our loss? Obeying the rules of calculus, we can simply differentiate
    our loss function, *J*(θ), with respect to the weights of the second layer (θ[*2*]).
    Mathematically, we can actually represent this in a different manner as well.
    Using the chain rule, we can show how changes in our loss with respect to the
    second layer weights are actually a product of two different gradients themselves.
    One represents the changes in our losses with respect to the model''s prediction,
    and the other shows the changes in our model''s prediction with respect to the
    weights in the second layer. This may be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93d30d82-fee8-4614-b9ea-962b1b3ed8aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As if this wasn''t complicated enough, we can even take this recursion further.
    Let''s say that instead of modeling the impact of the changing weights of the
    second layer (θ*[2]*), we wanted to propagate all the way back and see how our
    loss changes with respect to the weights of our first layer. We then simply redefine
    this equation using the chain rule, as we did earlier. Again, we are interested
    in the change in our model''s loss with respect to the model weights for our first
    layer, (θ[*1*]). We define this using the product of three different gradients;
    the changes in our loss with respect to output, changes in our output with respect
    to our hidden layer value, and finally, the changes in our hidden layer value
    with respect to our first layer weights. We can summarize this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58df4217-ce8f-438f-97bb-50e3dcb16802.png)'
  prefs: []
  type: TYPE_IMG
- en: And so, this is how we use the loss function, and backpropagate the errors by
    computing the gradient of our loss function with respect to every single weight
    in our model. Doing so, we are able to adjust the course of our model in the right
    direction, being the direction of the highest descent, as we saw before. We do
    this for our entire dataset, denoted as an epoch. And what about the size of our
    step? Well, that is determined by the learning rate we set.
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While somewhat intuitive, the learning rate of a model simply determines how
    fast it can learn. Mathematically put, the learning rate determines the exact
    size of the step we take at each iteration, as we descend the loss landscape to
    converge to ideal weights. Setting the right learning rate for your problem can
    be challanging, specially when the loss landscape is complex and full of surprises,
    as can be seen in the illustration here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/334bbf3f-b157-4b18-b044-7d05e9a7d981.png)'
  prefs: []
  type: TYPE_IMG
- en: This is quite an important notion. If we set a learning rate too small, then
    naturally, our model learns less than it potentially could per any given training
    iteration. Even worse with low learning rates is when our model gets stuck in
    a local minimum, thinking that it has reached a global minimum. Conversely, a
    high learning rate could, on the other hand, deter our model from capturing patterns
    of predictive value.
  prefs: []
  type: TYPE_NORMAL
- en: If our steps are too large, we may simply keep overshooting over any global
    minima present in our feature space of weights, and hence, never converge on our
    ideal model weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'One solution to this problem is to set an adaptive learning rate, responsive
    to the specific loss landscape it may encounter during training. We will explore
    various implementations of adaptive learning rates (such as Momentum, Adadelta,
    Adagrad, RMSProp, and more) in subsequent chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3074f96-b948-4089-903d-e4d549c8b997.png)'
  prefs: []
  type: TYPE_IMG
- en: Scaling the perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, we have seen so far how a single neuron may learn to represent a pattern,
    as it is trained. Now, let''s say we want to leverage the learning mechanism of
    an additional neuron, in parallel. With two perceptron units in our model, each
    unit may learn to represent a different pattern in our data. Hence, if we wanted
    to scale the previous perceptron just a little bit by adding another neuron, we
    may get a structure with two fully connected layers of neurons, as shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a4de24d-5e18-41a8-80b6-f87143eb4f6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Note here that the feature weights, as well as the additional fictional input
    we will have per neuron to represent our bias, have both disappeared. To simplify
    our representation, we have instead denoted both the scalar dot product, and our
    bias term, as a single symbol.
  prefs: []
  type: TYPE_NORMAL
- en: We choose to represent this mathematical function as the letter *z*. The value
    of *z* is then fed to the activation function, just as we previously did, thus
    *y* = *g*(*z*). As you can see in the preceding diagram, our input features connect
    to two different neurons, each of which may adjust its weights and biases to learn
    a specific and distinct representation from the data it is fed. These representations
    are then used to predict our output classes, and updated as we train our model.
  prefs: []
  type: TYPE_NORMAL
- en: A single layered network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Right, now we have seen how to leverage two versions of our perception unit,
    in parallel, enabling each individual unit to learn a different underlying pattern
    that is possibly present in the data we feed it. We naturally want to connect
    these neurons to output neurons, which fire to indicate the presence of a specific
    output class. In our sunny-rainy day classification example, we have two output
    classes (sunny or rainy), hence a predictive network tasked to solve this problem
    will have two output neurons. These neurons will be supported by the learning
    of neurons from the previous layer, and ideally will represent features that are
    informative for predicting either a rainy or a sunny day. Mathematically speaking,
    all that is simply happening here is the forward propagation of our transformed
    input features, followed by the backward propagation of the errors in our prediction.
    One way of thinking about this is to visualize each node in the following diagram
    as the holder of a specific number. Similarly, each arrow can be seen as picking
    up a number from a node, performing a weighted computation on it, and carrying
    it forward to the next layer of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/147f084b-24fe-4d7a-a103-fffb516e8af7.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we have a neural network with one hidden layer. We call this a hidden layer
    as the state of this layer is not directly enforced, as opposed to the input and
    output layers. Their representations are not hardcoded by the designer of the
    network. Rather, they are inferred by the network, as data propagates through
    it.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw, the input layer holds our input values. The set of arrows connecting
    the input layer to the hidden layer simply compute the bias adjusted dot product
    (*z*), of our input features (*x*) and their respective weights (θ[*1*]). The
    (*z*) values then reside in the hidden layer neurons, until we apply our non-linear
    function, *g*(*x*), to these values. After this, the arrows leading away from
    the hidden layer compute the dot product of *g*(*z*) and the weights corresponding
    to *the* hidden layer, (θ*[2]*), before carrying the result forward to the two
    output neurons, ![](img/25c7dba2-7da4-41b4-af22-9d3bdbd503a2.png) and ![](img/1af86b7f-9ad1-45e2-8dbc-8ab4b4dcb6a7.png).
    Notice that with each layer comes a respective weights matrix, which is iteratively
    updated by differentiating our loss function with respect to the weight matrices
    from the previous training iteration. Hence, we train a neural network by descending
    the gradient of our loss function relative to our model weights, converging to
    a global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with TensorFlow playground
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how different neurons can actually capture different patterns in
    our data using a dummy example. Suppose that we have two output classes in our
    data, as plotted in the following diagram. The task of our neural network is to
    learn the decision boundaries separating our two output classes. Plotting this
    two-dimensional dataset, we get something similar to the following diagram, where
    we see several decision boundaries that classify the different possible outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0c7325b-b6d8-4f85-9fa0-0f4d098ab377.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will employ a phenomenal open-source tool to visualize our model''s learnings,
    known as TensorFlow playground. This tool simply allows to simulate a neural network
    with some synthetic data, and lets us actually *see* what patterns our neurons
    are picking up on*.* It lets you tinker with all the concepts we have overviewed
    so far, including different types and forms of input features, activation functions,
    learning rate, and many more. We highly encourage you to experiment with the different
    synthetic datasets they provide, play with the input features and progressively
    add neurons, as well as hidden layers to see how this affects learning. Do also
    experiment with different activation functions to see how your model can capture
    various complex patterns from data. Seeing, is indeed believing!  (Or more scientifically
    put, nullius in verba).  As we can see in our diagram below, both neurons in the
    hidden layer are actually capturing different curvatures in our feature space,
    learning a specific pattern in the data. You can visualize the weights of our
    model by observing the thickness of the lines connecting the layers. You can also
    visualize the output of each neuron (the shaded blue and white areas shown within
    the neurons) to see what underlining pattern that specific neuron is capturing
    in our data. This representation, as you will see by experimenting on the playground,
    is iteratively updated and converges to an ideal value, given the form and type
    of the data, activation functions, and learning rates used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5da2a3e3-fe20-4d99-9b70-366fc12aeb3c.png) ![](img/6256fd9d-ef64-4cc7-9e5c-f83691beb5af.png)'
  prefs: []
  type: TYPE_IMG
- en: A model with one hidden layer, two neurons, and the sigmoid activation function,
    trained for 1,000 epochs
  prefs: []
  type: TYPE_NORMAL
- en: Capturing patterns heirarchically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We previously saw how a specific model configuration with two neurons, each
    equipped with a sigmoid activation function, manages to capture two different
    curvatures in our feature space, which is then combined to plot our decision boundary,
    represented by the aforementioned output. However, this is just one possible configuration,
    leading to one possible decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a model with two hidden layers with the sigmoid
    activation function, trained for 1,000 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6b339b1-a220-4cf6-8ff9-3be17075ea68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram shows a model with one hidden layer, composed of two
    neurons, with a rectified linear unit activation function, trained for 1,000 epochs,
    on the same dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c033539-6573-48a8-b4c7-3dd577cfb9cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram shows a model with one hidden layer, composed of three
    neurons, with a rectified linear unit activation function, again on the same dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/651adca8-95d9-4f8c-bbd6-f7cb212cbb23.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that by using different activation functions, and manipulating the number
    of hidden layers and their neurons, we can achieve very different decision boundaries.
    It is up to us to asses which of them is ideally predictive, and is suitable for
    our use case. Mostly, this is done through experimentation, although domain knowledge
    about the data you are modelling may go a long way.
  prefs: []
  type: TYPE_NORMAL
- en: Steps ahead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! In just a few pages, we have already come a long way. Now you
    know how neural networks learn, and have an idea of the higher-level mathematical
    constructs that permit it to learn from data. We saw how a single neuron, namely
    the perceptron, is configured. We saw how this neural unit transforms its input
    features as data propagates forward through it. We also understood the notion
    of representing non-linearity through activation functions, and how multiple neurons
    may be organized in a layer, allowing each individual neuron in the layer to represent
    different patterns from our data. These learned patterns are updated at each training
    iteration, for each neuron. We know now that this is done by computing the loss
    between our predictions and actual output values, and adjusting the weights of
    each neuron in the model, until we find an ideal configuration.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, modern neural networks employ various types of neurons, configured
    in diverse ways, for different predictive tasks. While the underlining learning
    architecture of neural networks always remains the same, the specific configuration
    of neurons, in terms of their number, inter-connectivity, activation functions
    used, etc. are elements which define the different types of neural network architectures
    you may come across. For the time being, we leave you with a comprehensive illustration
    generously provided by the Asimov institute.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see some prominent types of neurons, or *cells*,
    along with their configurations that form some of the most commonly used state
    of the art neural networks, which you will also throughout the course of this
    book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64f53973-778e-40c8-b812-05381d4e291a.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have achieved a comprehensive understanding of neural learning systems,
    we can start getting our hands dirty. We will soon implement our first neural
    network, test it out for a classic classification task, and practically face many
    of the concepts we have covered here. In doing so, we will cover a detailed overview
    of the exact nature of loss optimization, and the evaluation metrics of neural
    networks.
  prefs: []
  type: TYPE_NORMAL
