["```py\n$ sudo pip3 install tensorflow\n\n```", "```py\n$ sudo pip3 install tensorflow-gpu\n\n```", "```py\n$ sudo pip3 install keras\n\n```", "```py\n$ python3\n>>> import tensorflow as tf\n>>> message = tf.constant('Hello world!')\n>>> session = tf.Session()\n>>> session.run(message)\nb'Hello world!'\n>>> import keras.backend as K\nUsing TensorFlow backend.\n>>> print(K.epsilon())\n1e-07\n\n```", "```py\ntensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\n\n```", "```py\nimport numpy as np\nfrom keras.datasets import mnist\nimport matplotlib.pyplot as plt\n\n# load dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# count the number of unique train labels\nunique, counts = np.unique(y_train, return_counts=True)\nprint(\"Train labels: \", dict(zip(unique, counts)))\n\n# count the number of unique test labels\nunique, counts = np.unique(y_test, return_counts=True)\nprint(\"Test labels: \", dict(zip(unique, counts)))\n\n# sample 25 mnist digits from train dataset\nindexes = np.random.randint(0, x_train.shape[0], size=25)\nimages = x_train[indexes]\nlabels = y_train[indexes]\n\n# plot the 25 mnist digits\nplt.figure(figsize=(5,5))\nfor i in range(len(indexes)):\n    plt.subplot(5, 5, i + 1)\n    image = images[i]\n    plt.imshow(image, cmap='gray')\n    plt.axis('off')\n\nplt.show()\nplt.savefig(\"mnist-samples.png\")\nplt.close('all')\n```", "```py\nTrain labels:  {0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\nTest labels:  {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}\n\n```", "```py\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.utils import to_categorical, plot_model\nfrom keras.datasets import mnist\n\n# load mnist dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# compute the number of labels\nnum_labels = len(np.unique(y_train))\n\n# convert to one-hot vector\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n# image dimensions (assumed square)\nimage_size = x_train.shape[1]\ninput_size = image_size * image_size\n\n# resize and normalize\nx_train = np.reshape(x_train, [-1, input_size])\nx_train = x_train.astype('float32') / 255\nx_test = np.reshape(x_test, [-1, input_size])\nx_test = x_test.astype('float32') / 255\n\n# network parameters\nbatch_size = 128\nhidden_units = 256\ndropout = 0.45\n\n# model is a 3-layer MLP with ReLU and dropout after each layer\nmodel = Sequential()\nmodel.add(Dense(hidden_units, input_dim=input_size))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout))\nmodel.add(Dense(hidden_units))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout))\nmodel.add(Dense(num_labels))\n# this is the output for one-hot vector\nmodel.add(Activation('softmax'))\nmodel.summary()\nplot_model(model, to_file='mlp-mnist.png', show_shapes=True)\n\n# loss function for one-hot vector\n# use of adam optimizer\n# accuracy is a good metric for classification tasks\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n# train the network\nmodel.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n\n# validate the model on test dataset to determine generalization\nloss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n```", "```py\n# compute the number of labels\nnum_labels = len(np.unique(y_train))\n```", "```py\n# convert to one-hot vector\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n```", "```py\n# image dimensions (assumed square)\nimage_size = x_train.shape[1]\ninput_size = image_size * image_size\n\n# resize and normalize\nx_train = np.reshape(x_train, [-1, input_size])\nx_train = x_train.astype('float32') / 255\nx_test = np.reshape(x_test, [-1, input_size])\nx_test = x_test.astype('float32') / 255\n```", "```py\n# model is a 3-layer MLP with ReLU and dropout after each layer\nmodel = Sequential()\nmodel.add(Dense(hidden_units, input_dim=input_size))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout))\nmodel.add(Dense(hidden_units))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout))\nmodel.add(Dense(num_labels))\n# this is the output for one-hot vector\nmodel.add(Activation('softmax'))\n```", "```py\nfrom keras.regularizers import l2\nmodel.add(Dense(hidden_units,\n          kernel_regularizer=l2(0.001),\n          input_dim=input_size))\n```", "```py\n[  3.57351579e-11   7.08998016e-08   2.30154569e-07   6.35787558e-07\n   5.57471187e-11   4.15353840e-09   3.55973775e-16   9.99995947e-01\n   1.29531730e-09   3.06023480e-06]\n\n```", "```py\n# loss function for one-hot vector\n# use of adam optimizer\n# accuracy is a good metric for classification tasks\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n# train the network\nmodel.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n```", "```py\nmodel.summary()\n```", "```py\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 256)               200960    \n_________________________________________________________________\nactivation_1 (Activation)    (None, 256)               0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 256)               65792     \n_________________________________________________________________\nactivation_2 (Activation)    (None, 256)               0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                2570      \n_________________________________________________________________\nactivation_3 (Activation)    (None, 10)                0         \n=================================================================\nTotal params: 269,322\nTrainable params: 269,322\nNon-trainable params: 0\n\n```", "```py\nplot_model(model, to_file='mlp-mnist.png', show_shapes=True)\n\n```", "```py\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten\nfrom keras.utils import to_categorical, plot_model\nfrom keras.datasets import mnist\n\n# load mnist dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# compute the number of labels\nnum_labels = len(np.unique(y_train))\n\n# convert to one-hot vector\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n# input image dimensions\nimage_size = x_train.shape[1]\n# resize and normalize\nx_train = np.reshape(x_train,[-1, image_size, image_size, 1])\nx_test = np.reshape(x_test,[-1, image_size, image_size, 1])\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# network parameters\n# image is processed as is (square grayscale)\ninput_shape = (image_size, image_size, 1)\nbatch_size = 128\nkernel_size = 3\npool_size = 2\nfilters = 64\ndropout = 0.2\n\n# model is a stack of CNN-ReLU-MaxPooling\nmodel = Sequential()\nmodel.add(Conv2D(filters=filters,\n                 kernel_size=kernel_size,\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size))\nmodel.add(Conv2D(filters=filters,\n                 kernel_size=kernel_size,\n                 activation='relu'))\nmodel.add(MaxPooling2D(pool_size))\nmodel.add(Conv2D(filters=filters,\n                 kernel_size=kernel_size,\n                 activation='relu'))\nmodel.add(Flatten())\n# dropout added as regularizer\nmodel.add(Dropout(dropout))\n# output layer is 10-dim one-hot vector\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))\nmodel.summary()\nplot_model(model, to_file='cnn-mnist.png', show_shapes=True)\n\n# loss function for one-hot vector\n# use of adam optimizer\n# accuracy is good metric for classification tasks\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n# train the network\nmodel.fit(x_train, y_train, epochs=10, batch_size=batch_size)\n\nloss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n```", "```py\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 26, 26, 64)        640       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 13, 13, 64)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 11, 11, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 576)               0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 576)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                5770      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 10)                0         \n=================================================================\nTotal params: 80,266\nTrainable params: 80,266\nNon-trainable params: 0\n\n```", "```py\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, SimpleRNN\nfrom keras.utils import to_categorical, plot_model\nfrom keras.datasets import mnist\n\n# load mnist dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# compute the number of labels\nnum_labels = len(np.unique(y_train))\n\n# convert to one-hot vector\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n# resize and normalize\nimage_size = x_train.shape[1]\nx_train = np.reshape(x_train,[-1, image_size, image_size])\nx_test = np.reshape(x_test,[-1, image_size, image_size])\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n\n# network parameters\ninput_shape = (image_size, image_size)\nbatch_size = 128\nunits = 256\ndropout = 0.2 \n\n# model is RNN with 256 units, input is 28-dim vector 28 timesteps\nmodel = Sequential()\nmodel.add(SimpleRNN(units=units,\n                    dropout=dropout,\n                    input_shape=input_shape))\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))\nmodel.summary()\nplot_model(model, to_file='rnn-mnist.png', show_shapes=True)\n\n# loss function for one-hot vector\n# use of sgd optimizer\n# accuracy is good metric for classification tasks\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n# train the network\nmodel.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n\nloss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\nprint(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n```", "```py\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nsimple_rnn_1 (SimpleRNN)     (None, 256)               72960     \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                2570      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 10)                0         \n=================================================================\nTotal params: 75,530\nTrainable params: 75,530\nNon-trainable params: 0\n\n```"]