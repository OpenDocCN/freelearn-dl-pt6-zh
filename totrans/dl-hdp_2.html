<html><head></head><body><div id="book-columns"><div id="book-inner"><div class="chapter" title="Chapter 2.  Distributed Deep Learning for Large-Scale Data"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/><span class="koboSpan" id="kobo.1.1">Chapter 2.  Distributed Deep Learning for Large-Scale Data </span></h1></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"><span class="koboSpan" id="kobo.2.1"> </span></td><td valign="top"><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.3.1">"In God we trust, all others must bring data"</span></em></span>
</p></td><td valign="top"><span class="koboSpan" id="kobo.4.1"> </span></td></tr><tr><td valign="top"><span class="koboSpan" id="kobo.5.1"> </span></td><td colspan="2" align="right" valign="top" style="text-align: center"><span class="koboSpan" id="kobo.6.1">--</span><span class="attribution"><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">W. </span><span class="koboSpan" id="kobo.7.2">Edwards Deming</span></em></span></span></td></tr></table></div><p><span class="koboSpan" id="kobo.8.1">In this exponentially growing digital world, big data and deep learning are the two hottest technical trends. </span><span class="koboSpan" id="kobo.8.2">Deep learning and big data are two interrelated topics in the world of data science, and in terms of technological growth, both are critically interconnected and equally significant.</span></p><p><span class="koboSpan" id="kobo.9.1">Digital data and cloud storage follow a generic law, termed as Moore's law [50], which roughly states that the world's data are doubling every two years; however, the cost of storing that data decreases at approximately the same rate. </span><span class="koboSpan" id="kobo.9.2">This profusion of data generates more features and verities, hence, to extract all the valuable information out of it, better deep learning models should be built.</span></p><p><span class="koboSpan" id="kobo.10.1">This voluminous availability of data helps to bring huge opportunities for multiple sectors. </span><span class="koboSpan" id="kobo.10.2">Moreover, big data, with its analytic part, has produced lots of challenges in the field of data mining, harnessing the data, and retrieving the hidden information out of it. </span><span class="koboSpan" id="kobo.10.3">In the field of Artificial Intelligence, deep learning algorithms provide their best output with large-scale data during the learning process. </span><span class="koboSpan" id="kobo.10.4">Therefore, as data are growing faster than ever before, deep learning also plays a crucial part in delivering all the big data analytic solutions.</span></p><p><span class="koboSpan" id="kobo.11.1">This chapter will give an insight into how deep learning models behave with big data, and reveal the associated challenges. </span><span class="koboSpan" id="kobo.11.2">The later part of the chapter will introduce Deeplearning4j, an open source distributed framework, with a provision for integration with Hadoop and Spark, used to deploy deep learning for large-scale data. </span><span class="koboSpan" id="kobo.11.3">The chapter will provide examples to show how basic deep neural networks can be implemented with Deeplearning4j, and its integration to Apache Spark and Hadoop YARN.</span></p><p><span class="koboSpan" id="kobo.12.1">The following are the important topics that will be covered in this chapter:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.13.1">Deep learning for massive amounts of data</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.14.1">Challenges of deep learning for big data</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.15.1">Distributed deep learning and Hadoop</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.16.1">Deeplearning4j: An open source distributed framework for deep learning</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.17.1">Setting up Deeplearning4j on Hadoop YARN</span></li></ul></div><div class="section" title="Deep learning for massive amounts of data"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec12"/><span class="koboSpan" id="kobo.18.1">Deep learning for massive amounts of data</span></h1></div></div></div><p><span class="koboSpan" id="kobo.19.1">In this Exa-Byte scale era, the data are increasing at an exponential rate. </span><span class="koboSpan" id="kobo.19.2">This growth of data are analyzed by many organizations and researchers in various ways, and also for so many different purposes. </span><span class="koboSpan" id="kobo.19.3">According to the survey of </span><span class="strong"><strong><span class="koboSpan" id="kobo.20.1">International Data Corporation</span></strong></span><span class="koboSpan" id="kobo.21.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.22.1">IDC</span></strong></span><span class="koboSpan" id="kobo.23.1">), the Internet is processing approximately 2 Petabytes of data every day [51]. </span><span class="koboSpan" id="kobo.23.2">In 2006, the size of digital data was around 0.18 ZB, whereas this volume has increased to 1.8 ZB in 2011. </span><span class="koboSpan" id="kobo.23.3">Up to 2015, it was expected to reach up to 10 ZB in size, and by 2020, its volume in the world will reach up to approximately 30 ZB to 35 ZB. </span><span class="koboSpan" id="kobo.23.4">The timeline of this data mountain is shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.24.1">Figure 2.1</span></em></span><span class="koboSpan" id="kobo.25.1">. </span><span class="koboSpan" id="kobo.25.2">These immense amounts of data in the digital world are formally termed as big data.</span></p><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"><span class="koboSpan" id="kobo.26.1"> </span></td><td valign="top"><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.27.1">"The world of Big Data is on fire"</span></em></span>
</p></td><td valign="top"><span class="koboSpan" id="kobo.28.1"> </span></td></tr><tr><td valign="top"><span class="koboSpan" id="kobo.29.1"> </span></td><td colspan="2" align="right" valign="top" style="text-align: center"><span class="koboSpan" id="kobo.30.1">--</span><span class="attribution"><span class="emphasis"><em><span class="koboSpan" id="kobo.31.1">The Economist, Sept 2011</span></em></span></span></td></tr></table></div><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.32.1"><img src="graphics/image_02_001-1.jpg" alt="Deep learning for massive amounts of data"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.33.1">Figure 2.1: Figure shows the increasing trend of data for a time span of around 20 years</span></p><p><span class="koboSpan" id="kobo.34.1">Facebook has almost 21 PB in 200M objects [52], whereas Jaguar ORNL has more than 5 PB data. </span><span class="koboSpan" id="kobo.34.2">These stored data are growing so rapidly that Exa-Byte scale storage systems are likely to be used by 2018 to 2020.</span></p><p><span class="koboSpan" id="kobo.35.1">This explosion of data certainly poses an immediate threat to the traditional data-intensive computations, and points towards the need for some distributed and scalable storage architecture for querying and analysis of the large-scale data. </span><span class="koboSpan" id="kobo.35.2">A generic line of thought for big data is that raw data is extremely complex, sundry, and increasingly growing. </span><span class="koboSpan" id="kobo.35.3">An ideal Big dataset consists of a vast amount of unsupervised raw data, and with some negligible amount of structured/categorized data. </span><span class="koboSpan" id="kobo.35.4">Therefore, while processing these amounts of non-stationary structured data, the conventional data-intensive computations often fail. </span><span class="koboSpan" id="kobo.35.5">As a result, big data, having unrestricted diversity, requires sophisticated methods and tools, which could be implemented to extract patterns and analyze the large-scale data. </span><span class="koboSpan" id="kobo.35.6">The growth of big data has mostly been caused by an increasing computational processing power and the capability of the modern systems to store data at lower cost.</span></p><p><span class="koboSpan" id="kobo.36.1">Considering all these features of big data, it can be broken into four distinct dimensions, often referred to as the four Vs: </span><span class="strong"><strong><span class="koboSpan" id="kobo.37.1">Volume</span></strong></span><span class="koboSpan" id="kobo.38.1">, </span><span class="strong"><strong><span class="koboSpan" id="kobo.39.1">Variety</span></strong></span><span class="koboSpan" id="kobo.40.1">, </span><span class="strong"><strong><span class="koboSpan" id="kobo.41.1">Velocity</span></strong></span><span class="koboSpan" id="kobo.42.1">, and </span><span class="strong"><strong><span class="koboSpan" id="kobo.43.1">Veracity</span></strong></span><span class="koboSpan" id="kobo.44.1">. </span><span class="koboSpan" id="kobo.44.2">Following </span><span class="emphasis"><em><span class="koboSpan" id="kobo.45.1">figure 2.2</span></em></span><span class="koboSpan" id="kobo.46.1"> shows the different characteristics of big data by providing all the 4Vs of data:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.47.1"><img src="graphics/B05883_02_02-1.jpg" alt="Deep learning for massive amounts of data"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.48.1">Figure 2.2: Figure depicts the visual representation of 4Vs of big data</span></p><p><span class="koboSpan" id="kobo.49.1">In this current data-intensive technology era, the velocity of the data, the escalating rate at which the data are collected and obtained is as significant as the other parameters of the big data, that is, </span><span class="strong"><strong><span class="koboSpan" id="kobo.50.1">Volume</span></strong></span><span class="koboSpan" id="kobo.51.1"> and </span><span class="strong"><strong><span class="koboSpan" id="kobo.52.1">Variety</span></strong></span><span class="koboSpan" id="kobo.53.1">. </span><span class="koboSpan" id="kobo.53.2">With the given pace, at which this data is getting generated, if it is not collected and analyzed sensibly, there is a huge risk of important data loss. </span><span class="koboSpan" id="kobo.53.3">Although, there is an option to retain this rapid-moving data into bulk storage for batch processing at a later period, the genuine importance in tackling this high velocity data lies in how quickly an organization can convert the raw data to a structured and usable format. </span><span class="koboSpan" id="kobo.53.4">Specifically, time-sensitive information such as flight fare, hotel fare, or some e-commerce product's price, and so on would become obsolete if the data is not immediately retained and processed in a systemic manner. </span><span class="koboSpan" id="kobo.53.5">The parameter veracity in big data is concerned with the accuracy of the results obtained after the data analysis. </span><span class="koboSpan" id="kobo.53.6">As data turns more complex each day, sustaining trust in the hidden information of big data throws a significant challenge.</span></p><p><span class="koboSpan" id="kobo.54.1">To extract and analyze such critically complex data, a better, well-planned model is desired. </span><span class="koboSpan" id="kobo.54.2">In ideal cases, a model should perform better dealing with big data compared to data with small sizes. </span><span class="koboSpan" id="kobo.54.3">However, this is not always the case. </span><span class="koboSpan" id="kobo.54.4">Here, we will show one example to discuss more on this point.</span></p><p><span class="koboSpan" id="kobo.55.1">As illustrated in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.56.1">Figure 2.3</span></em></span><span class="koboSpan" id="kobo.57.1">, with a small size dataset, the performance of the best algorithm is </span><span class="emphasis"><em><span class="koboSpan" id="kobo.58.1">n%</span></em></span><span class="koboSpan" id="kobo.59.1"> better than the worst one. </span><span class="koboSpan" id="kobo.59.2">However, as the size of the dataset increases (big data), the performance also enhances exponentially to some </span><span class="emphasis"><em><span class="koboSpan" id="kobo.60.1">k % &gt;&gt; n %</span></em></span><span class="koboSpan" id="kobo.61.1">. </span><span class="koboSpan" id="kobo.61.2">Such kind of traces can well be found from [53], which clearly shows the effect of a large-scale training dataset in the performance of the model. </span><span class="koboSpan" id="kobo.61.3">However, it would be completely misleading that with any of the simplest models, one can achieve the best performance only using Big dataset.</span></p><p><span class="koboSpan" id="kobo.62.1">From [53] we can see that algorithm 1 is basically a Naive Bayes model, algorithm 2 belongs to a memory-based model, and algorithm 3 corresponds to Winnow. </span><span class="koboSpan" id="kobo.62.2">The following graph shows, with a small dataset, that the performance of Winnow is less that the memory-based one. </span><span class="koboSpan" id="kobo.62.3">Whereas when dealing with Big dataset, both the Naive Bayes and Winnow show better performance than the memory-based model. </span><span class="koboSpan" id="kobo.62.4">So, looking at the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.63.1">Figure 2.3</span></em></span><span class="koboSpan" id="kobo.64.1">, it would be really difficult to infer on what basis any one of these simple models work better in an environment of large dataset. </span><span class="koboSpan" id="kobo.64.2">An intuitive explanation for the relatively poor performance of the memory-based method with large datasets is that the algorithm suffered due to the latency of loading a huge amount of data to its memory. </span><span class="koboSpan" id="kobo.64.3">Hence, it is purely a memory related issue, and only using big data would not resolve that. </span><span class="koboSpan" id="kobo.64.4">Therefore, a primary reason for the performance should be how sophisticated the models are. </span><span class="koboSpan" id="kobo.64.5">Hence, the importance of deep learning model comes into play.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note9"/><span class="koboSpan" id="kobo.65.1">Note</span></h3><p><span class="koboSpan" id="kobo.66.1">Big data. </span><span class="koboSpan" id="kobo.66.2">Small Minds. </span><span class="koboSpan" id="kobo.66.3">No Progress! </span><span class="koboSpan" id="kobo.66.4">Big data. </span><span class="koboSpan" id="kobo.66.5">Big Brains. </span><span class="koboSpan" id="kobo.66.6">Breakthrough! </span><span class="koboSpan" id="kobo.66.7">[54]</span></p></div></div><p><span class="koboSpan" id="kobo.67.1">Deep learning stands in contrast to big data. </span><span class="koboSpan" id="kobo.67.2">Deep learning has triumphantly been implemented in various industry products and widely practiced by various researchers by taking advantage of this large-scale digital data. </span><span class="koboSpan" id="kobo.67.3">Famous technological companies such as Facebook, Apple, and Google collect and analyze this voluminous amount of data on a daily basis, and have been bellicosely going forward with various deep learning related projects over the last few years.</span></p><p><span class="koboSpan" id="kobo.68.1">Google deploys deep learning algorithms on the massive unstructured data collected from various sources including Google's Street view, image search engine, Google's translator, and Android's voice recognition.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.69.1"><img src="graphics/image_02_003.jpg" alt="Deep learning for massive amounts of data"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.70.1">Figure 2.3: Variation of percentage of accuracy of different types of algorithms with increasing size of datasets</span></p><p><span class="koboSpan" id="kobo.71.1">Apple's Siri, a virtual personal assistant for iPhones, provides a bulk of different services, such as sport news, weather reports, answers to users' questions, and so on. </span><span class="koboSpan" id="kobo.71.2">The entire application of Siri is based on deep learning, which collects data from different Apple services and obtains its intelligence. </span><span class="koboSpan" id="kobo.71.3">Other industries, mainly Microsoft and IBM, are also using deep learning as their major domain to deal with this massive amount of unstructured data. </span><span class="koboSpan" id="kobo.71.4">IBM's brain-like computer, Watson, and Microsoft's Bing search engine primarily use deep learning techniques to leverage the big data.</span></p><p><span class="koboSpan" id="kobo.72.1">Current deep learning architectures comprise of millions or even billions of data points. </span><span class="koboSpan" id="kobo.72.2">Moreover, the scale at which the data is growing prevents the model from the risk of overfitting. </span><span class="koboSpan" id="kobo.72.3">The rapid increase in computation power too has made the training of advanced models much easier.</span></p><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.73.1">Table 2.1</span></em></span><span class="koboSpan" id="kobo.74.1"> shows how big data is practiced with popular deep learning models in recent research to get maximum information out of data:</span></p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/></colgroup><tbody><tr><td>
<p><span class="strong"><strong><span class="koboSpan" id="kobo.75.1">Models</span></strong></span></p>
</td><td>
<p><span class="strong"><strong><span class="koboSpan" id="kobo.76.1">Computing power</span></strong></span></p>
</td><td>
<p><span class="strong"><strong><span class="koboSpan" id="kobo.77.1">Datasets</span></strong></span></p>
</td><td>
<p><span class="strong"><strong><span class="koboSpan" id="kobo.78.1">Average running time</span></strong></span></p>
</td></tr><tr><td>
<p><span class="koboSpan" id="kobo.79.1">Convolutional Neural Network[55]</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.80.1">Two NVIDIA GTX 580 3 GB GPUs.</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.81.1">Roughly 90 cycles through the training set of 1.2 million high resolution images.</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.82.1">Five to six days.</span></p>
</td></tr><tr><td>
<p><span class="koboSpan" id="kobo.83.1">Deep Belief Network [41]</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.84.1">NVIDIA GTX 280 1 GB GPU.</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.85.1">1 million images.</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.86.1">Approximately one day.</span></p>
</td></tr><tr><td>
<p><span class="koboSpan" id="kobo.87.1">Sparse autoencoder[ 66]</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.88.1">1000 CPU having 16000 cores each.</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.89.1">10 million 200*200 pixel images.</span></p>
</td><td>
<p><span class="koboSpan" id="kobo.90.1">Approximately three days.</span></p>
</td></tr></tbody></table></div><p><span class="koboSpan" id="kobo.91.1">Table 2.1: Recent research progress of large-scale deep learning models. </span><span class="koboSpan" id="kobo.91.2">Partial information taken from [55]</span></p><p><span class="koboSpan" id="kobo.92.1">Deep learning algorithms, with the help of a hierarchical learning approach, are basically used to extract meaningful generic representations from the input raw data. </span><span class="koboSpan" id="kobo.92.2">Basically, at a higher level, more complex and abstract representations of the data are learnt from the previous layers and the less abstracted data of the multi-level learning model. </span><span class="koboSpan" id="kobo.92.3">Although deep learning can also learn from massive amounts of labelled (categorized) data, the models generally look attractive when they can learn from unlabeled/uncategorized data [56], and hence, help in generating some meaningful patterns and representation of the big unstructured data.</span></p><p><span class="koboSpan" id="kobo.93.1">While dealing with large-scale unsupervised data, deep learning algorithms can extract the generic patterns and relationships among the data points in a much better way than the shallow learning architectures. </span><span class="koboSpan" id="kobo.93.2">The following are a few of the major characteristics of deep learning algorithms, when trained with large-scale unlabeled data:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.94.1">From the higher level of abstractions and representation, semantics and relational knowledge of the big data can be obtained from the deep learning models</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.95.1">Even a simple linear model can perform effectively with the knowledge obtained from excessively complex and more abstract representations of the huge dataset</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.96.1">This huge variety of data representation from the unsupervised data opens its door for learning other data types such as textual, audio, video, image, and the like</span></li></ul></div><p><span class="koboSpan" id="kobo.97.1">Therefore, it can be surely concluded that deep learning will become an essential ingredient for providing big data sentiment analysis, predictive analysis, and so on, particularly with the enhanced processing power and advancement in the </span><span class="strong"><strong><span class="koboSpan" id="kobo.98.1">graphics processing unit</span></strong></span><span class="koboSpan" id="kobo.99.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.100.1">GPU</span></strong></span><span class="koboSpan" id="kobo.101.1">) capacity. </span><span class="koboSpan" id="kobo.101.2">The aim of this chapter is not to extensively cover big data, but to represent the relationship between big data and deep learning. </span><span class="koboSpan" id="kobo.101.3">The subsequent sections will introduce the key concepts, applications, and challenges of deep learning while working with large-scale uncategorized data.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Challenges of deep learning for big data"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec13"/><span class="koboSpan" id="kobo.1.1">Challenges of deep learning for big data</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">The potential of big data is certainly noteworthy. </span><span class="koboSpan" id="kobo.2.2">However, to fully extract valuable information at this scale, we would require new innovations and promising algorithms to address many of these related technical problems. </span><span class="koboSpan" id="kobo.2.3">For example, to train the models, most of the traditional machine learning algorithms load the data in memory. </span><span class="koboSpan" id="kobo.2.4">But with a massive amount of data, this approach will surely not be feasible, as the system might run out of memory. </span><span class="koboSpan" id="kobo.2.5">To overcome all these gritty problems, and get the most out of the big data with the deep learning techniques, we will require brain storming.</span></p><p><span class="koboSpan" id="kobo.3.1">Although, as discussed in the earlier section, large-scale deep learning has achieved many accomplishments in the past decade, this field is still in a growing phase. </span><span class="koboSpan" id="kobo.3.2">Big data is constantly raising limitations with its 4Vs. </span><span class="koboSpan" id="kobo.3.3">Therefore, to tackle all of those, many more advancements in the models need to take place.</span></p><div class="section" title="Challenges of deep learning due to massive volumes of data (first V)"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec11"/><span class="koboSpan" id="kobo.4.1">Challenges of deep learning due to massive volumes of data (first V)</span></h2></div></div></div><p><span class="koboSpan" id="kobo.5.1">The volume of large-scale data imposes a great challenge to deep learning. </span><span class="koboSpan" id="kobo.5.2">With very high dimensionality (attributes), a large number of examples (input) and large varieties of classifications (outputs), big data often increases the complexity of the model, as well as the running-time complexity of the algorithm. </span><span class="koboSpan" id="kobo.5.3">The mountain of data makes the training of deep learning algorithms almost impossible using centralized storage and its limited processing ability. </span><span class="koboSpan" id="kobo.5.4">To provide a cushion to this challenge, pushed by the huge volume of data, distributed frameworks with parallelized servers should be used. </span><span class="koboSpan" id="kobo.5.5">The upgraded deep network models have started to use clusters of CPUs and GPUs to enhance the training speed, without compromising the algorithm's accuracy. </span><span class="koboSpan" id="kobo.5.6">Various new strategies have been evolved for model parallelism and data parallelism.</span></p><p><span class="koboSpan" id="kobo.6.1">In these types, the models or data are split into blocks, which can fit with the in-memory data, and then be distributed to various nodes with forward and backward propagations [57]. </span><span class="koboSpan" id="kobo.6.2">Deeplearning4j, a Java-based distributed tool for deep learning, uses data parallelism for this purpose, and will be explained in the next section.</span></p><p><span class="koboSpan" id="kobo.7.1">High volumes of data are always associated with noisy labels and data incompleteness. </span><span class="koboSpan" id="kobo.7.2">This poses a major challenge during the training of large-scale deep learning. </span><span class="koboSpan" id="kobo.7.3">A huge proportion of the big data is contained by the unlabeled or unstructured data, where the noisy labels predominantly exist. </span><span class="koboSpan" id="kobo.7.4">To overcome this issue, some manual curation of the datasets is required to a significant extent. </span><span class="koboSpan" id="kobo.7.5">For example, all the search engines are used to collect the data over the last one year span. </span><span class="koboSpan" id="kobo.7.6">For this data, we need some sort of filtering, particularly to remove redundancy and the low-value data. </span><span class="koboSpan" id="kobo.7.7">Advanced deep learning methods are essential to handle such noisy, redundant data. </span><span class="koboSpan" id="kobo.7.8">Also, the associated algorithms should be able to tolerate these disarray datasets. </span><span class="koboSpan" id="kobo.7.9">One can also implement some more efficient cost function and updated training strategy to fully overcome the effect of noisy labels. </span><span class="koboSpan" id="kobo.7.10">Moreover, the use of semi-supervised learning [58] [59] could help to enhance the solution associated with this noisy data.</span></p></div><div class="section" title="Challenges of deep learning from a high variety of data (second V)"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec12"/><span class="koboSpan" id="kobo.8.1">Challenges of deep learning from a high variety of data (second V)</span></h2></div></div></div><p><span class="koboSpan" id="kobo.9.1">This is the second dimension of big data, which represents all types of formats, with different distributions and from numerous sources. </span><span class="koboSpan" id="kobo.9.2">The exponentially growing data come from heterogeneous sources, which include a mammoth collection of audio streams, images, videos, animations, graphics, and unstructured texts from various log files. </span><span class="koboSpan" id="kobo.9.3">These varieties of data possess different characteristics and behavior. </span><span class="koboSpan" id="kobo.9.4">Data integration could be the only way to deal with such situations. </span><span class="koboSpan" id="kobo.9.5">As stated in </span><a class="link" href="ch01.html" title="Chapter 1. Introduction to Deep Learning"><span class="koboSpan" id="kobo.10.1">
Chapter 1
</span></a><span class="koboSpan" id="kobo.11.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.12.1">Introduction to Deep Learning</span></em></span><span class="koboSpan" id="kobo.13.1">, deep learning has the ability to represent learning from structured/unstructured data. </span><span class="koboSpan" id="kobo.13.2">Deep learning can carry out unsupervised learning in a hierarchical fashion, which is training performed one level at a time, and the higher level features are defined by the immediate lower levels. </span><span class="koboSpan" id="kobo.13.3">This property of deep learning can be used to address the data integration problem. </span><span class="koboSpan" id="kobo.13.4">The natural solution of this could be to learn the data representation from each individual data sources, and then integrate the learned features at the subsequent levels.</span></p><p><span class="koboSpan" id="kobo.14.1">There have already been a few experiments [60] [61], which have successfully demonstrated that deep learning can easily be used for the heterogeneous data sources for its significant gains in system performance. </span><span class="koboSpan" id="kobo.14.2">However, there are still many unanswered questions which deep learning has to address in the upcoming years. </span><span class="koboSpan" id="kobo.14.3">Currently, most of the deep learning models are mainly tested on bi-modalities (data from only two sources), but will the system performance be enhanced while dealing with multiple modalities? </span><span class="koboSpan" id="kobo.14.4">It might happen that multiple sources of data will offer conflicting information; in those cases, how will the model be able to nullify such conflicts and integrate the data in a constructive and fruitful way? </span><span class="koboSpan" id="kobo.14.5">Deep learning seems perfectly appropriate for the integration of various sources of data with multiple modalities, on account of its capability of learning intermediate representations and the underlying factors associated with a variety of data.</span></p></div><div class="section" title="Challenges of deep learning from a high velocity of data (third V)"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec13"/><span class="koboSpan" id="kobo.15.1">Challenges of deep learning from a high velocity of data (third V)</span></h2></div></div></div><p><span class="koboSpan" id="kobo.16.1">The extreme velocity at which data is growing poses an enormous challenge to the deep learning technique. </span><span class="koboSpan" id="kobo.16.2">For data analytics, data created at this speed should also be processed in a timely manner. </span><span class="koboSpan" id="kobo.16.3">Online learning is one of the solutions to learning from this high velocity data [62-65]. </span><span class="koboSpan" id="kobo.16.4">However, online learning uses a sequential learning strategy, where the entire dataset should be kept in-memory, which becomes extremely difficult for traditional machines. </span><span class="koboSpan" id="kobo.16.5">Although the conventional neural network has been modified for online learning [67-71], there is still so much scope for progress in this field for deep learning. </span><span class="koboSpan" id="kobo.16.6">As an alternate approach to online learning, the stochastic gradient descent approach [72], [73] is also applied for deep learning. </span><span class="koboSpan" id="kobo.16.7">In this type, one training example with the known label is fed to the next label to update the model parameters. </span><span class="koboSpan" id="kobo.16.8">Further, to speed up learning, the updates can also be performed on a small batch basis [74]. </span><span class="koboSpan" id="kobo.16.9">This mini batch can provide a good balance between running time and the computer memory. </span><span class="koboSpan" id="kobo.16.10">In the next section, we will explain why mini batch data is most important for distributed deep learning.</span></p><p><span class="koboSpan" id="kobo.17.1">One more big challenge related to this high velocity of data is that this data is extremely changeable in nature. </span><span class="koboSpan" id="kobo.17.2">The distribution of data happens too frequently over time. </span><span class="koboSpan" id="kobo.17.3">Ideally, the data that changes over time is split into chunks taken from small time durations. </span><span class="koboSpan" id="kobo.17.4">The basic idea is that the data remains stationary for some time, and also possesses some major degree of correlation [75] [76]. </span><span class="koboSpan" id="kobo.17.5">Therefore, the deep learning algorithms of big data should have the feature of learning the data as a stream. </span><span class="koboSpan" id="kobo.17.6">Algorithms which can learn from those non-stationary data are really crucial for deep learning.</span></p></div><div class="section" title="Challenges of deep learning to maintain the veracity of data (fourth V)"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec14"/><span class="koboSpan" id="kobo.18.1">Challenges of deep learning to maintain the veracity of data (fourth V)</span></h2></div></div></div><p><span class="koboSpan" id="kobo.19.1">Data veracity, imprecise, or uncertain data, is sometime overlooked, though it is equally consequential as the other 3Vs of big data. </span><span class="koboSpan" id="kobo.19.2">With the immense variety and velocity of big data, an organization can no longer rely on the traditional models to measure the accuracy of data. </span><span class="koboSpan" id="kobo.19.3">Unstructured data, by definition, contains a huge amount of imprecise and uncertain data. </span><span class="koboSpan" id="kobo.19.4">For example, social media data is excessively uncertain in nature. </span><span class="koboSpan" id="kobo.19.5">Although there are tools that can automate the normalization and cleansing of data, they are mostly in the pre-industrial stage.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Distributed deep learning and Hadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec14"/><span class="koboSpan" id="kobo.1.1">Distributed deep learning and Hadoop</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">From the earlier sections of this chapter, we already have enough insights on why and how the relationship of deep learning and big data can bring major changes to the research community. </span><span class="koboSpan" id="kobo.2.2">Also, a centralized system is not going to help this relationship substantially with the course of time. </span><span class="koboSpan" id="kobo.2.3">Hence, distribution of the deep learning network across multiple servers has become the primary goal of the current deep learning practitioners. </span><span class="koboSpan" id="kobo.2.4">However, dealing with big data in a distributed environment is always associated with several challenges. </span><span class="koboSpan" id="kobo.2.5">Most of those are explained in-depth in the previous section. </span><span class="koboSpan" id="kobo.2.6">These include dealing with higher dimensional data, data with too many features, amount of memory available to store, processing the massive Big datasets, and so on. </span><span class="koboSpan" id="kobo.2.7">Moreover, Big datasets have a high computational resource demand on CPU and memory time. </span><span class="koboSpan" id="kobo.2.8">So, the reduction of processing time has become an extremely significant criterion. </span><span class="koboSpan" id="kobo.2.9">The following are the central and primary challenges in distributed deep learning:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.3.1">How can we keep chunks of dataset in the primary memory of the nodes?</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.4.1">How can we maintain coordination among the chunks of data, so that later they can be moved together to result in the final outcome?</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.5.1">How can we make distributed and parallel processing extremely scheduled and coordinated?</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.6.1">How can we achieve an orchestral search process across the dataset to achieve high performance?</span></li></ul></div><p><span class="koboSpan" id="kobo.7.1">There are multiple ways of using distributed deep learning with big datasets. </span><span class="koboSpan" id="kobo.7.2">However, when we talk about big data, the framework that is performing tremendously well in defending most of the challenges from the past half decade is the Hadoop framework [77-80]. </span><span class="koboSpan" id="kobo.7.3">Hadoop allows for parallel and distributed processing. </span><span class="koboSpan" id="kobo.7.4">It is undoubtedly the most popular and widely used framework, and it can store and process the data mountain more efficiently compared to the other traditional frameworks. </span><span class="koboSpan" id="kobo.7.5">Almost all the major technology companies, such as Google, Facebook, and so on use Hadoop to deploy and process their data in a sophisticated fashion. </span><span class="koboSpan" id="kobo.7.6">Most of the software designed at Google, which requires the use of an ocean of data, uses Hadoop. </span><span class="koboSpan" id="kobo.7.7">The primary advantage of Hadoop is the way it stores and processes enormous amount of data across thousands of commodity servers, bringing some well-organized results [81]. </span><span class="koboSpan" id="kobo.7.8">From our general understanding of deep learning, we can relate that deep learning surely needs that sort of distributed computing power to produce some wondrous outcomes from the input data. </span><span class="koboSpan" id="kobo.7.9">The Big dataset can be split into chunks and distributed across multiple commodity hardware for parallel training. </span><span class="koboSpan" id="kobo.7.10">Further more, the complete stage of a deep neural network can be split into subtasks, and then those subtasks can be processed in parallel.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/><span class="koboSpan" id="kobo.8.1">Note</span></h3><p><span class="koboSpan" id="kobo.9.1">Hadoop has turned out to be the point of convergence for all the data lakes. </span><span class="koboSpan" id="kobo.9.2">The need to shif deep learning to the data, which is already residing in Hadoop, has become quintessential.</span></p></div></div><p><span class="koboSpan" id="kobo.10.1">Hadoop operates on the concept that </span><span class="emphasis"><em><span class="koboSpan" id="kobo.11.1">moving computation is cheaper than moving data</span></em></span><span class="koboSpan" id="kobo.12.1"> [86] [87]. </span><span class="koboSpan" id="kobo.12.2">Hadoop allows for the distributed processing of large-scale datasets across clusters of commodity servers. </span><span class="koboSpan" id="kobo.12.3">It also provides efficient load balancing, has a very high degree of fault tolerance, and is highly horizontally scalable with minimal effort. </span><span class="koboSpan" id="kobo.12.4">It can detect and tolerate failures in the application layers, and hence, is suitable for running on commodity hardware. </span><span class="koboSpan" id="kobo.12.5">To achieve the high availability of data, Hadoop, by default, keeps a replication factor of three, with a copy of each block placed on two other separate machines. </span><span class="koboSpan" id="kobo.12.6">So, if a node fails, the recovery can be done instantly from the other two nodes. </span><span class="koboSpan" id="kobo.12.7">The replication factor of Hadoop can be easily increased based on how valuable the data is and other associated requirements on the data.</span></p><p><span class="koboSpan" id="kobo.13.1">Hadoop was initially built mainly for processing the batch tasks, so it is mostly suitable for deep learning networks, where the main task is to find the classification of large-scale data. </span><span class="koboSpan" id="kobo.13.2">The selection of features to learn how to classify the data is mainly done on a large batch of datasets.</span></p><p><span class="koboSpan" id="kobo.14.1">Hadoop is extremely configurable, and can easily be optimized as per the user's requirements. </span><span class="koboSpan" id="kobo.14.2">For example, if a user wants to keep more replicas of the data for better reliability, he can increase the replication factor. </span><span class="koboSpan" id="kobo.14.3">However, an increase in the number of replicas will eventually increase the storage requirements. </span><span class="koboSpan" id="kobo.14.4">Here we will not be explaining more about the features and configuration of data, rather we will mostly discuss the part of Hadoop which will be used extensively in distributed deep neural networks.</span></p><p><span class="koboSpan" id="kobo.15.1">In the new version of Hadoop, the parts which we will mainly use in this book are HDFS, Map-Reduce, and </span><span class="strong"><strong><span class="koboSpan" id="kobo.16.1">Yet Another Resource Negotiator</span></strong></span><span class="koboSpan" id="kobo.17.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.18.1">YARN</span></strong></span><span class="koboSpan" id="kobo.19.1">). </span><span class="koboSpan" id="kobo.19.2">YARN has already dominated Hadoop's Map-Reduce (explained in the next part) in a large manner. </span><span class="koboSpan" id="kobo.19.3">YARN currently has the responsibility to assign the works to the Data nodes (data server) of Hadoop. </span><span class="strong"><strong><span class="koboSpan" id="kobo.20.1">Hadoop Distributed File System</span></strong></span><span class="koboSpan" id="kobo.21.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.22.1">HDFS</span></strong></span><span class="koboSpan" id="kobo.23.1">), on the other hand, is a distributed file system, which is distributed across all the Data nodes under a centralized meta-data server called NameNode. </span><span class="koboSpan" id="kobo.23.2">To achieve high-availability, in the later version, a secondary NameNode was integrated to Hadoop framework, the purpose of which is to have a copy of the metadata from primary NameNode after certain checkpoints.</span></p><div class="section" title="Map-Reduce"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec15"/><span class="koboSpan" id="kobo.24.1">Map-Reduce</span></h2></div></div></div><p><span class="koboSpan" id="kobo.25.1">The Map-Reduce paradigm [83] is a distributed programming model developed by Google in 2004, and is associated with processing huge datasets with a parallel and distributed algorithm on a cluster of machines. </span><span class="koboSpan" id="kobo.25.2">The entire Map-Reduce application is useful with large-scale datasets. </span><span class="koboSpan" id="kobo.25.3">Basically, it has two primary components, one is called Map and the other is called Reduce, along with a few intermediate stages like shuffling, sorting and partitioning. </span><span class="koboSpan" id="kobo.25.4">In the map phase, the large input job is broken down into smaller ones, and each of the jobs is distributed to different cores. </span><span class="koboSpan" id="kobo.25.5">The operation(s) are then carried out on every small job placed on those machines. </span><span class="koboSpan" id="kobo.25.6">The Reduce phase accommodates all the scattered and transformed output into one single dataset.</span></p><p><span class="koboSpan" id="kobo.26.1">Explaining the concept of Map-Reduce in detail is beyond the scope of this chapter; interested readers can go through </span><span class="emphasis"><em><span class="koboSpan" id="kobo.27.1">"Map-Reduce: Simplified data processing on large clusters</span></em></span><span class="koboSpan" id="kobo.28.1">" [83] to get an in-depth knowledge of this.</span></p></div><div class="section" title="Iterative Map-Reduce"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec16"/><span class="koboSpan" id="kobo.29.1">Iterative Map-Reduce</span></h2></div></div></div><p><span class="koboSpan" id="kobo.30.1">The deep learning algorithms are iterative in nature - the models learn from the optimization algorithms, which go through multiple steps so that it leads to a point of minimal error. </span><span class="koboSpan" id="kobo.30.2">For these kinds of models the Map-Reduce application does not seem to work as efficiently as it does for other use-cases.</span></p><p><span class="koboSpan" id="kobo.31.1">Iterative Map-Reduce, a next generation YARN framework (unlike the traditional Map-Reduce) does multiple iterations on the data, which passes through only once. </span><span class="koboSpan" id="kobo.31.2">Although the architecture of Iterative Map-Reduce and Map-Reduce is dissimilar in design, the high level of understanding of both the architectures is simple. </span><span class="koboSpan" id="kobo.31.3">Iterative Map-Reduce is nothing but a sequence of Map-Reduce operations, where the output of the first Map-Reduce operation becomes the input to the next operation and so on. </span><span class="koboSpan" id="kobo.31.4">In the case of deep learning models, the map phase places all the operations of a particular iteration on each node of the distributed systems. </span><span class="koboSpan" id="kobo.31.5">It then distributes that massive input dataset to all the machines in the cluster. </span><span class="koboSpan" id="kobo.31.6">The training of the models is performed on each node of the cluster.</span></p><p><span class="koboSpan" id="kobo.32.1">Before sending the aggregated new model back to each of the machines, the reduce phase takes all the outputs collected from the map phase and calculates the average of the parameters. </span><span class="koboSpan" id="kobo.32.2">The same operations are iterated over and over again by the Iterative Reduce algorithm until the learning process completes and the errors minimize to almost zero.</span></p><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.33.1">Figure 2.4</span></em></span><span class="koboSpan" id="kobo.34.1"> compares the high-level functionalities of the two methods. </span><span class="koboSpan" id="kobo.34.2">The left image shows the block diagram of Map-Reduce, while on the right, we have the close-up of Iterative Map-Reduce. </span><span class="koboSpan" id="kobo.34.3">Each 'Processor' is a working deep network, which is learning on small chunks of the larger dataset. </span><span class="koboSpan" id="kobo.34.4">In the 'Superstep' phase, the averaging of the parameters is done before the entire model is redistributed to the whole cluster as shown in the following diagram:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.35.1"><img src="graphics/B05883_02_04.jpg" alt="Iterative Map-Reduce"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.36.1">Figure 2.4: Difference of functionalities in Map-Reduce and parallel iterative reduce</span></p></div><div class="section" title="Yet Another Resource Negotiator (YARN)"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec17"/><span class="koboSpan" id="kobo.37.1">Yet Another Resource Negotiator (YARN)</span></h2></div></div></div><p><span class="koboSpan" id="kobo.38.1">The primary idea of YARN is to dissociate job scheduling and resource management from the data processing. </span><span class="koboSpan" id="kobo.38.2">So the data can continue to process in the system in parallel with the Map-Reduce batch jobs. </span><span class="koboSpan" id="kobo.38.3">YARN possesses a central resource manager, which mostly manages the Hadoop system resources according to the need. </span><span class="koboSpan" id="kobo.38.4">The node manager (specific to nodes) is responsible for managing and monitoring the processing of individual nodes of the cluster. </span><span class="koboSpan" id="kobo.38.5">This processing is dedicatedly controlled by an ApplicationMaster, which monitors the resources from the central resource manager, and works with the node manager to monitor and execute the tasks. </span><span class="koboSpan" id="kobo.38.6">The following figure gives an overview of the architecture of YARN:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.39.1"><img src="graphics/B05883_02_05-1.jpg" alt="Yet Another Resource Negotiator (YARN)"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.40.1">Figure 2.5: An overview of the high-level architecture of YARN</span></p><p><span class="koboSpan" id="kobo.41.1">All these components of Hadoop are primarily used in distributed deep learning to overcome all the challenges stated earlier. </span><span class="koboSpan" id="kobo.41.2">The following subsection shows the criteria that need to be satisfied for better performance of distributed deep learning.</span></p></div><div class="section" title="Important characteristics for distributed deep learning design"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec18"/><span class="koboSpan" id="kobo.42.1">Important characteristics for distributed deep learning design</span></h2></div></div></div><p><span class="koboSpan" id="kobo.43.1">the following are the important characteristics of distributed deep learning design:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong><span class="koboSpan" id="kobo.44.1">Small batch processing</span></strong></span><span class="koboSpan" id="kobo.45.1">: In distributed deep learning, the network must intake and process data quickly in parallel. </span><span class="koboSpan" id="kobo.45.2">To process and provide results more accurately, every node of the cluster should receive small chunks of data of approximately 10 elements at a time.</span><p><span class="koboSpan" id="kobo.46.1">For example, say the master node of YARN is coordinating 20 worker nodes for a Big dataset of 200 GB. </span><span class="koboSpan" id="kobo.46.2">The master node will split the dataset into 10 GB of 20 small batches of data, allocating one small batch to each worker. </span><span class="koboSpan" id="kobo.46.3">The workers will process the data in parallel, and send the results back to the master as soon as it finishes the computing. </span><span class="koboSpan" id="kobo.46.4">All these outcomes will be aggregated by the master node, and the average of the results will be finally redistributed to the individual workers.</span></p><p><span class="koboSpan" id="kobo.47.1">Deep learning networks perform well with small batches of nearly 10, rather than working with 100 or 200 large batches of data. </span><span class="koboSpan" id="kobo.47.2">Small batches of data empower the networks to learn from different orientations of the data in-depth, which later on recompiles to give a broader knowledge to the model.</span></p><p><span class="koboSpan" id="kobo.48.1">On the other hand, if the batch size is too large, the network tries to learn quickly, which maximizes the errors. </span><span class="koboSpan" id="kobo.48.2">Conversly, smaller batch size slows down the speed of learning, and results in the possibility of divergence as the network approaches towards the minimum error rate.</span></p></li><li class="listitem"><span class="strong"><strong><span class="koboSpan" id="kobo.49.1">Parameter Averaging</span></strong></span><span class="koboSpan" id="kobo.50.1">: Parameter averaging is a crucial operation for the training of distributed deep network. </span><span class="koboSpan" id="kobo.50.2">In a network, parameters are generally the weight and biases of the node layers. </span><span class="koboSpan" id="kobo.50.3">As mentioned in the small batch processing section, once training is completed for several workers, they will pass different sets of parameters back to the master. </span><span class="koboSpan" id="kobo.50.4">With every iteration, the parameters are averaged, updated, and sent back to the master for further operations.</span><p><span class="koboSpan" id="kobo.51.1">The sequential process of parameter averaging can be outlined as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.52.1">The master configures the initial network and sets the different hyperparameters</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.53.1">Based on the configuration of the training master, the Big dataset is split into chunks of several smaller datasets</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.54.1">For each split of the training dataset, until the error rate approaches towards zero, perform the following:</span><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.55.1">The master distributes the parameter from the master to each individual worker</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.56.1">Each worker starts the training of the model with its dedicated chunk of dataset</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.57.1">The average of the parameters is calculated and returned back to the master.</span></li></ul></div></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.58.1">The training completes, and the master will have one copy of the training network</span></li></ul></div></li><li class="listitem"><span class="koboSpan" id="kobo.59.1">Parameter averaging offers the following two important advantages in case of distributed training:</span><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.60.1">It enables parallelism by generating simultaneous results.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.61.1">It helps to prevent over-fitting by distributing the given dataset into multiple datasets of smaller sizes. </span><span class="koboSpan" id="kobo.61.2">The network then learns the average result, rather than just aggregating the results from different smaller batches.</span></li></ul></div></li></ol></div><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.62.1">Figure 2.6</span></em></span><span class="koboSpan" id="kobo.63.1"> shows a combined diagrammatic overview of the small batch processing and parameter averaging operation:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.64.1"><img src="graphics/B05883_02_06-1.jpg" alt="Important characteristics for distributed deep learning design"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.65.1">Figure 2.6: Figure shows the high level architecture of a distributed deep learning architecture</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Deeplearning4j - an open source distributed framework for deep learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec15"/><span class="koboSpan" id="kobo.1.1">Deeplearning4j - an open source distributed framework for deep learning</span></h1></div></div></div><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.2.1">Deeplearning4j</span></strong></span><span class="koboSpan" id="kobo.3.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.4.1">DL4J</span></strong></span><span class="koboSpan" id="kobo.5.1">) [82] is an open source deep learning framework which is written for JVM, and mainly used for commercial grade. </span><span class="koboSpan" id="kobo.5.2">The framework is written entirely in Java, and thus, the name '4j' is included. </span><span class="koboSpan" id="kobo.5.3">Because of its use with Java, Deeplearning4j has started to earn popularity with a much wider audience and range of practitioners.</span></p><p><span class="koboSpan" id="kobo.6.1">This framework is basically composed of a distributed deep learning library that is integrated with Hadoop and Spark. </span><span class="koboSpan" id="kobo.6.2">With the help of Hadoop and Spark, we can very easily distribute the model and Big datasets, and run multiple GPUs and CPUs to perform parallel operations. </span><span class="koboSpan" id="kobo.6.3">Deeplearning4j has primarily shown substantial success in performing pattern recognition in images, sound, text, time series data, and so on. </span><span class="koboSpan" id="kobo.6.4">Apart from that, it can also be applied for various customer use cases such as facial recognition, fraud detection, business analytics, recommendation engines, image and voice search, and predictive maintenance with the sensor data.</span></p><p><span class="koboSpan" id="kobo.7.1">The following</span><span class="emphasis"><em><span class="koboSpan" id="kobo.8.1"> Figure 2.7</span></em></span><span class="koboSpan" id="kobo.9.1"> shows a generic high-level architectural block diagram of Deeplearning4j:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.10.1"><img src="graphics/B05883_02_07-1.jpg" alt="Deeplearning4j - an open source distributed framework for deep learning"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.11.1">Figure 2.7: High level architectural block diagram of Deeplearning4j [82]</span></p><div class="section" title="Major features of Deeplearning4j"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec19"/><span class="koboSpan" id="kobo.12.1">Major features of Deeplearning4j</span></h2></div></div></div><p><span class="koboSpan" id="kobo.13.1">Deeplearning4j comes with various attractive features, which completely distinguishes it from other existing deep learning tools like Theano, Torch, and so on.</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.14.1">Distributed architecture</span></strong></span><span class="koboSpan" id="kobo.15.1">: Training in Deeplearning4j can be performed in two ways - with distributed, multi-threaded deep-learning, or with traditional, normal single-threaded deep-learning techniques. </span><span class="koboSpan" id="kobo.15.2">The training is carried out in clusters of commodity nodes. </span><span class="koboSpan" id="kobo.15.3">Therefore, Deeplearning4j is able to process any amount of data quickly. </span><span class="koboSpan" id="kobo.15.4">The neural networks are trained in parallel using the iterative reduce method, which works on Hadoop YARN and Spark. </span><span class="koboSpan" id="kobo.15.5">It also integrates with Cuda kernels to conduct pure GPU operations, and works with distributed GPUs.</span></li></ul></div><p><span class="koboSpan" id="kobo.16.1">Deeplearning4j operations can be run on Hadoop YARN or Spark as a job. </span><span class="koboSpan" id="kobo.16.2">In Hadoop, Iterative Reduce workers work on every block of HDFS, and synchronously process the data in parallel. </span><span class="koboSpan" id="kobo.16.3">As the processing completes, they push the transformed parameters back to their master, where the average of the parameters are taken and the model of each worker's node is updated.</span></p><p><span class="koboSpan" id="kobo.17.1">In Deeplearning4j, the distributed runtimes are interchangeable, where they act like a directory in a huge modular architecture, which can be swapped in or out.</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.18.1">Data parallelism</span></strong></span><span class="koboSpan" id="kobo.19.1">: There are two ways in which the neural networks can be trained in a distributed manner: one is data parallelism, and the other is model parallelism. </span><span class="koboSpan" id="kobo.19.2">Deeplearning4j follows data parallelism for training. </span><span class="koboSpan" id="kobo.19.3">In data parallelism, we can split the large dataset into chunks of smaller datasets, and distribute those to parallel models running on different servers to train in parallel.</span></li></ul></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.20.1">Scientific computing capability for the JVM</span></strong></span><span class="koboSpan" id="kobo.21.1">: For scientific computing in Java and Scala, Deeplearning4j includes an N-dimensional array class using </span><span class="strong"><strong><span class="koboSpan" id="kobo.22.1">N-Dimensional Arrays for Java</span></strong></span><span class="koboSpan" id="kobo.23.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.24.1">ND4J</span></strong></span><span class="koboSpan" id="kobo.25.1">). </span><span class="koboSpan" id="kobo.25.2">The functionality of ND4J is much faster than what Numpy provides to Python, and its mostly written in C++. </span><span class="koboSpan" id="kobo.25.3">It's effectively based on a library for matrix manipulation and linear algebra in a production environment. </span><span class="koboSpan" id="kobo.25.4">Most of the routines of ND4J are designed to run fast with minimum RAM requirements.</span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong><span class="koboSpan" id="kobo.26.1">Vectorization tool for machine learning</span></strong></span><span class="koboSpan" id="kobo.27.1">: For vectorization of various file formats and data types, Canova has been merged with Deeplearning4j. </span><span class="koboSpan" id="kobo.27.2">Canova performs vectorization using an input/output system similar to how Hadoop uses Map-Reduce. </span><span class="koboSpan" id="kobo.27.3">Canova is primarily designed to vectorize text, CSVs, images, sounds, videos, and so on from the </span><span class="strong"><strong><span class="koboSpan" id="kobo.28.1">command line interface</span></strong></span><span class="koboSpan" id="kobo.29.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.30.1">CLI</span></strong></span><span class="koboSpan" id="kobo.31.1">).</span></li></ul></div></div><div class="section" title="Summary of functionalities of Deeplearning4j"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec20"/><span class="koboSpan" id="kobo.32.1">Summary of functionalities of Deeplearning4j</span></h2></div></div></div><p><span class="koboSpan" id="kobo.33.1">The following are summary of functionalities of Deeplearning4j:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.34.1">Deeplearning4j can be claimed as the most complete, production-ready, open source deep learning library ever built</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.35.1">Compared to Theano-based tools, it has many more features specially designed for deep networks</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.36.1">Deeplearning4j is very easy to use; even non-specialists can apply its conventions to solve computationally intensive problems</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.37.1">The tools provide a wide range of applicability, hence, the networks work equally well with image, sound, text, and time-series</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.38.1">It is completely distributed and can run multiple GPUs in parallel, unlike Theano [84], which is not distributed, and Torch7 [85], which has not automated its distribution like DL4J</span></li></ul></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Setting up Deeplearning4j on Hadoop YARN"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec16"/><span class="koboSpan" id="kobo.1.1">Setting up Deeplearning4j on Hadoop YARN</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Deeplearning4j primarily works on networks having multiple layers. </span><span class="koboSpan" id="kobo.2.2">To get started working with Deeplearning4j, one needs to get accustomed with the prerequisites, and how to install all the dependent software. </span><span class="koboSpan" id="kobo.2.3">Most of the documentation can be easily found on the official website of Deeplearning4j at </span><a class="ulink" href="https://deeplearning4j.org/"><span class="koboSpan" id="kobo.3.1">https://deeplearning4j.org/</span></a><span class="koboSpan" id="kobo.4.1"> [88].</span></p><p><span class="koboSpan" id="kobo.5.1">In this section of the chapter, we will help you to get familiar with the code of Deeplearning4j. </span><span class="koboSpan" id="kobo.5.2">Initially, we will show the implementation of a simple operation of a multilayer neural network with Deeplearning4j. </span><span class="koboSpan" id="kobo.5.3">The later part of the section will discuss distributed deep learning with Deeplearning4j library. </span><span class="koboSpan" id="kobo.5.4">Deeplearning4j trains distributed deep neural network on multiple distributed GPUs using Apache Spark. </span><span class="koboSpan" id="kobo.5.5">The later part of this section will also introduce the setup of Apache Spark for Deeplearning4j.</span></p><div class="section" title="Getting familiar with Deeplearning4j"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec21"/><span class="koboSpan" id="kobo.6.1">Getting familiar with Deeplearning4j</span></h2></div></div></div><p><span class="koboSpan" id="kobo.7.1">This part will mainly introduce the 'Hello World' programs of deep learning with deeplearning4j. </span><span class="koboSpan" id="kobo.7.2">We will explain the basic functions of the library with the help of two simple deep learning problems.</span></p><p><span class="koboSpan" id="kobo.8.1">In Deeplearning4j, </span><code class="literal"><span class="koboSpan" id="kobo.9.1">MultiLayerConfiguration</span></code><span class="koboSpan" id="kobo.10.1">, a class of the library can be considered as the base of the building block, which is responsible for organizing the layers and the corresponding hyperparameters of a neural network. </span><span class="koboSpan" id="kobo.10.2">This class can be considered as the core building block of Deeplearning4j for neural networks. </span><span class="koboSpan" id="kobo.10.3">Throughout the book, we will use this class to configure different multilayer neural networks.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/><span class="koboSpan" id="kobo.11.1">Note</span></h3><p><span class="koboSpan" id="kobo.12.1">Hyperparameters are the main backbone to determine the learning process of a neural network. </span><span class="koboSpan" id="kobo.12.2">They mostly include how to initialize the weights of the models, how many times they should be updated, the learning rate of the model, which optimization algorithms to use, and so on.</span></p></div></div><p><span class="koboSpan" id="kobo.13.1">In the first example, we will show how to classify data patterns for the multilayer perceptron classifier with the help of Deeplearning4j.</span></p><p><span class="koboSpan" id="kobo.14.1">The following is the sample training dataset that will be used in this program:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.15.1">  0, -0.500568579838,  0.687106471955 
  1,  0.190067977988, -0.341116711905 
  0,  0.995019651532,  0.663292952846 
  0, -1.03053733564,   0.342392729177 
  1,  0.0376749555484,-0.836548188848 
  0, -0.113745482508,  0.740204108847 
  1,  0.56769119889,  -0.375810486522 
</span></pre><p><span class="koboSpan" id="kobo.16.1">Initially, we need to initialize the various hyperparameters of the networks. </span><span class="koboSpan" id="kobo.16.2">The following piece of code will set the ND4J environment for the program:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.17.1">Nd4j.ENFORCE_NUMERICAL_STABILITY = true; 
int batchSize = 50; 
int seed = 123; 
double learningRate = 0.005; 
</span></pre><p><span class="koboSpan" id="kobo.18.1">Number of epochs is set to </span><code class="literal"><span class="koboSpan" id="kobo.19.1">30</span></code><span class="koboSpan" id="kobo.20.1">:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.21.1">int nEpochs = 30;  
int numInputs = 2; 
int numOutputs = 2; 
int numHiddenNodes = 20; 
</span></pre><p><span class="koboSpan" id="kobo.22.1">The following piece of code will load the training data to the network:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.23.1">RecordReader rr = new CSVRecordReader(); 
rr.initialize(new FileSplit(new File("saturn_data_train.csv"))); 
DataSetIterator trainIter = new RecordReaderDataSetIterator      
                            (rr,batchSize,0,2); 
</span></pre><p><span class="koboSpan" id="kobo.24.1">As the training data is loaded next we load the test data into the model with the following code:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.25.1">RecordReader rrTest = new CSVRecordReader(); 
rrTest.initialize(new FileSplit(new File("saturn_data_eval.csv"))); 
DataSetIterator trainIter = new RecordReaderDataSetIterator
                            (rrTest,batchSize,0,2); 
</span></pre><p><span class="koboSpan" id="kobo.26.1">Organization of all the layers of the network model as well as setting up the hyperparameters can be done with the following piece of code:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.27.1">MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder() 
.seed(seed)
.iterations(1)                          
.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) 
.learningRate(learningRate) 
.updater(Updater.NESTEROVS).momentum(0.9)
.list() 
.layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(numHiddenNodes) 
   .weightInit(WeightInit.XAVIER) 
   .activation("relu")
   .build()) 
 .layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD) 
   .weightInit(WeightInit.XAVIER)                         
   .activation("softmax")
   .nIn(numHiddenNodes).nOut(numOutputs).build())
 .pretrain(false)
 .backprop(true)
 .build(); 
</span></pre><p><span class="koboSpan" id="kobo.28.1">Now, we have loaded the training and test dataset, the initialization of the model can be done by calling the </span><code class="literal"><span class="koboSpan" id="kobo.29.1">init()</span></code><span class="koboSpan" id="kobo.30.1"> method. </span><span class="koboSpan" id="kobo.30.2">This will also start the training of the model from the given inputs:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.31.1">MultiLayerNetwork model = new MultiLayerNetwork(conf); 
</span><span class="strong"><strong><span class="koboSpan" id="kobo.32.1">model.init(); 
</span></strong></span>
</pre><p><span class="koboSpan" id="kobo.33.1">To check the output after a certain internal, let's print the scores every </span><code class="literal"><span class="koboSpan" id="kobo.34.1">5</span></code><span class="koboSpan" id="kobo.35.1"> parameter updates:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.36.1">model.setListeners(new ScoreIterationListener(5));      
for ( int n = 0; n &lt; nEpochs; n++) 
{ 
</span></pre><p><span class="koboSpan" id="kobo.37.1">Finally, the network is trained by calling the </span><code class="literal"><span class="koboSpan" id="kobo.38.1">.fit()</span></code><span class="koboSpan" id="kobo.39.1"> method:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.40.1">  model.fit( trainIter );</span></strong></span><span class="koboSpan" id="kobo.41.1"> 
}  
System.out.println("Evaluating the model....");  
Evaluation eval = new Evaluation(numOutputs); 
while(testIter.hasNext())
  { 
    DataSet t = testIter.next(); 
    INDArray features = t.getFeatureMatrix(); 
    INDArray lables = t.getLabels(); 
    INDArray predicted = model.output(features,false); 
    eval.eval(lables, predicted); 
  } 
System.out.println(eval.stats()); 
</span></pre><p><span class="koboSpan" id="kobo.42.1">So the training of the model is done. </span><span class="koboSpan" id="kobo.42.2">In the next part, the data points will be plotted and the corresponding accuracy of the data will be calculated as shown in the following code:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.43.1">double xMin = -15;
double xMax = 15; 
double yMin = -15; 
double yMax = 15; 
 
   
int nPointsPerAxis = 100; 
double[][] evalPoints = new double[nPointsPerAxis*nPointsPerAxis][2]; 
int count = 0; 
for( int i=0; i&lt;nPointsPerAxis; i++ )
{ 
 for( int j=0; j&lt;nPointsPerAxis; j++ )
 { 
   double x = i * (xMax-xMin)/(nPointsPerAxis-1) + xMin; 
   double y = j * (yMax-yMin)/(nPointsPerAxis-1) + yMin; 
 
   evalPoints[count][0] = x; 
   evalPoints[count][1] = y; 
 
   count++; 
 } 
} 
 
INDArray allXYPoints = Nd4j.create(evalPoints); 
 
INDArray predictionsAtXYPoints = model.output(allXYPoints); 
</span></pre><p><span class="koboSpan" id="kobo.44.1">The following code will store all the training data in an array before plotting those in the graph:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.45.1">         
rr.initialize(new FileSplit(new File("saturn_data_train.csv"))); 
rr.reset(); 
int nTrainPoints = 500; 
trainIter = new RecordReaderDataSetIterator(rr,nTrainPoints,0,2); 
DataSet ds = trainIter.next(); 
PlotUtil.plotTrainingData(ds.getFeatures(), ds.getLabels(),allXYPoints, predictionsAtXYPoints, nPointsPerAxis); 
</span></pre><p><span class="koboSpan" id="kobo.46.1">Running the test data through the network and generating the prediction can be done with the following code:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.47.1">rrTest.initialize(new FileSplit(new File("saturn_data_eval.csv"))); 
rrTest.reset(); 
int nTestPoints = 100; 
testIter = new RecordReaderDataSetIterator(rrTest,nTestPoints,0,2); 
ds = testIter.next(); 
INDArray testPredicted = model.output(ds.getFeatures()); 
PlotUtil.plotTestData(ds.getFeatures(), ds.getLabels(), testPredicted, allXYPoints, predictionsAtXYPoints, nPointsPerAxis); 
</span></pre><p><span class="koboSpan" id="kobo.48.1">When the preceding code is executed, it will run for approximately 5-10 seconds, depending upon your system configuration. </span><span class="koboSpan" id="kobo.48.2">During that time, you can check the console, which will display the updated score of training for your model.</span></p><p><span class="koboSpan" id="kobo.49.1">A piece of evaluation is displayed as follows:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.50.1">o.d.o.l.ScoreIterationListener - Score at iteration 0 is    
                                 0.6313823699951172 
o.d.o.l.ScoreIterationListener - Score at iteration 5 is 
                                 0.6154170989990234 
o.d.o.l.ScoreIterationListener - Score at iteration 10 is     
                                 0.4763660430908203 
o.d.o.l.ScoreIterationListener - Score at iteration 15 is 
                                 0.52469970703125 
o.d.o.l.ScoreIterationListener - Score at iteration 20 is    
                                 0.4296367645263672 
o.d.o.l.ScoreIterationListener - Score at iteration 25 is 
                                 0.4755714416503906 
o.d.o.l.ScoreIterationListener - Score at iteration 30 is 
                                 0.3985047912597656 
o.d.o.l.ScoreIterationListener - Score at iteration 35 is 
                                 0.4304619598388672 
o.d.o.l.ScoreIterationListener - Score at iteration 40 is   
                                 0.3672477722167969 
o.d.o.l.ScoreIterationListener - Score at iteration 45 is 
                                 0.39150180816650393 
o.d.o.l.ScoreIterationListener - Score at iteration 50 is 
                                 0.3353725051879883 
o.d.o.l.ScoreIterationListener - Score at iteration 55 is 
                                 0.3596681213378906 
</span></pre><p><span class="koboSpan" id="kobo.51.1">Finally, the program will output the different statistics of the training for the model using Deeplearning4j as follows:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.52.1">Evaluating the model.... 
</span><span class="koboSpan" id="kobo.52.2">Examples labeled as 0 classified by model as 0: 48 times 
Examples labeled as 1 classified by model as 1: 52 times 
</span></pre><p><span class="koboSpan" id="kobo.53.1">In the background, we can visualize the plotting of the data, which will give an impression of what the planet Saturn looks like. </span><span class="koboSpan" id="kobo.53.2">In the next part, we will show how to integrate Hadoop YARN and Spark with Deeplearning4j. </span><span class="koboSpan" id="kobo.53.3">The following </span><span class="emphasis"><em><span class="koboSpan" id="kobo.54.1">Figure 2.8</span></em></span><span class="koboSpan" id="kobo.55.1"> shows the output of the program in graphical representation:</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.56.1"><img src="graphics/image_02_008.jpg" alt="Getting familiar with Deeplearning4j"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.57.1">Figure 2.8: The scattered data points are plotted when the preceding program is executed. </span><span class="koboSpan" id="kobo.57.2">The data points give an impression of the planet Saturn</span></p></div><div class="section" title="Integration of Hadoop YARN and Spark for distributed deep learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec22"/><span class="koboSpan" id="kobo.58.1">Integration of Hadoop YARN and Spark for distributed deep learning</span></h2></div></div></div><p><span class="koboSpan" id="kobo.59.1">To use Deeplearning4j on Hadoop, we need to include the </span><code class="literal"><span class="koboSpan" id="kobo.60.1">deeplearning-hadoop</span></code><span class="koboSpan" id="kobo.61.1"> dependency as shown in the following code:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.62.1">&lt;!-- https://mvnrepository.com/artifact/org.Deeplearning4j/Deeplearning4j-hadoop --&gt; 
&lt;dependency&gt; 
    &lt;groupId&gt;org.Deeplearning4j&lt;/groupId&gt; 
    &lt;artifactId&gt;Deeplearning4j-hadoop&lt;/artifactId&gt; 
    &lt;version&gt;0.0.3.2.7&lt;/version&gt; 
&lt;/dependency&gt; 
</span></pre><p><span class="koboSpan" id="kobo.63.1">Similarly, for Spark, we have to include the </span><code class="literal"><span class="koboSpan" id="kobo.64.1">deeplearning-spark</span></code><span class="koboSpan" id="kobo.65.1"> dependency as shown in the following code:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.66.1">&lt;!-- https://mvnrepository.com/artifact/org.Deeplearning4j/dl4j-spark-nlp_2.11 --&gt; 
&lt;dependency&gt; 
    &lt;groupId&gt;org.Deeplearning4j&lt;/groupId&gt; 
    &lt;artifactId&gt;dl4j-spark-nlp_2.11&lt;/artifactId&gt; 
    &lt;version&gt;0.5.0&lt;/version&gt; 
&lt;/dependency&gt; 
</span></pre><p><span class="koboSpan" id="kobo.67.1">Explaining the detailed functionalities of Apache Spark is beyond the scope of this book. </span><span class="koboSpan" id="kobo.67.2">Interested readers can catch up on the same at </span><a class="ulink" href="http://spark.apache.org/"><span class="koboSpan" id="kobo.68.1">
http://spark.apache.org/
</span></a><span class="koboSpan" id="kobo.69.1">.</span></p></div><div class="section" title="Rules to configure memory allocation for Spark on Hadoop YARN"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec23"/><span class="koboSpan" id="kobo.70.1">Rules to configure memory allocation for Spark on Hadoop YARN</span></h2></div></div></div><p><span class="koboSpan" id="kobo.71.1">As already stated in the previous section, Apache Hadoop YARN is a cluster resource manager. </span><span class="koboSpan" id="kobo.71.2">When Deeplearning4j submits a training job to a YARN cluster via Spark, it is the responsibility of YARN to manage the allocation of resources such as CPU cores, amount of memory consumed by each executor, and so on. </span><span class="koboSpan" id="kobo.71.3">However, to extract the best performance from Deeplearning4j on YARN, some careful memory configuration is desired. </span><span class="koboSpan" id="kobo.71.4">This is done as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.72.1">The executer JVM memory amount needs to be specified using </span><code class="literal"><span class="koboSpan" id="kobo.73.1">spark.executor.memory</span></code><span class="koboSpan" id="kobo.74.1">.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.75.1">The YARN container memory overhead needs to be specified using </span><code class="literal"><span class="koboSpan" id="kobo.76.1">spark.yarn.executor.memoryOverhead</span></code><span class="koboSpan" id="kobo.77.1">.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.78.1">The sum of </span><code class="literal"><span class="koboSpan" id="kobo.79.1">spark.executor.memory</span></code><span class="koboSpan" id="kobo.80.1"> and </span><code class="literal"><span class="koboSpan" id="kobo.81.1">spark.yarn.executor.memoryOverhead</span></code><span class="koboSpan" id="kobo.82.1"> must always be less than the amount of memory allocated to a container by the YARN.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.83.1">ND4j and JavaCPP should know the allotment of the off-heap memory; this can be done using the </span><code class="literal"><span class="koboSpan" id="kobo.84.1">org.bytedeco.javacpp.maxbytes</span></code><span class="koboSpan" id="kobo.85.1"> system property.</span></li><li class="listitem" style="list-style-type: disc"><code class="literal"><span class="koboSpan" id="kobo.86.1">org.bytedeco.javacpp.maxbytes</span></code><span class="koboSpan" id="kobo.87.1"> must be less than </span><code class="literal"><span class="koboSpan" id="kobo.88.1">spark.yarn.executor.memoryOverhead</span></code><span class="koboSpan" id="kobo.89.1">.</span></li></ul></div><p><span class="koboSpan" id="kobo.90.1">The current version of Deeplearning4j uses parameter averaging to perform distributed training of the neural network. </span><span class="koboSpan" id="kobo.90.2">The following operation is performed exactly the way it is described in the parameter averaging part of the earlier section:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.91.1">SparkDl4jMultiLayer sparkNet = new SparkDl4jMultiLayer(sc,conf, 
                               new ParameterAveragingTrainingMaster
                              .Builder(numExecutors(),dataSetObjSize 
                              .batchSizePerWorker(batchSizePerExecutor) 
                              .averagingFrequency(1) 
                              .repartionData(Repartition.Always) 
                              .build()); 
sparkNet.setCollectTrainingStats(true); 
</span></pre><p><span class="koboSpan" id="kobo.92.1">To list all the files from HDFS so as to run the code on different nodes, run the following code:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.93.1">Configuration config = new Configuration(); 
FileSystem hdfs = FileSystem.get(tempDir.toUri(), config); 
RemoteIterator&lt;LocatedFileStatus&gt; fileIter = hdfs.listFiles
  (new org.apache.hadoop.fs.Path(tempDir.toString()),false); 
 
List&lt;String&gt; paths = new ArrayList&lt;&gt;(); 
while(fileIter.hasNext())
  { 
   String path = fileIter.next().getPath().toString(); 
   paths.add(path); 
  } 
</span></pre><p><span class="koboSpan" id="kobo.94.1">A complete code for how to set up Spark with YARN and HDFS will be provided along with the code bundle. </span><span class="koboSpan" id="kobo.94.2">For simplicity, only part of the code is shown here for the purpose of understanding.</span></p><p><span class="koboSpan" id="kobo.95.1">Now, we will show an example to demonstrate how to use Spark and load the data into memory with Deeplearning4j. </span><span class="koboSpan" id="kobo.95.2">We will use a basic DataVec example to show some pre-processing operation on some CSV data.</span></p><p><span class="koboSpan" id="kobo.96.1">The sample dataset will look as like the following:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.97.1">2016-01-01 17:00:00.000,830a7u3,u323fy8902,1,USA,100.00,Legit 
2016-01-01 18:03:01.256,830a7u3,9732498oeu,3,FR,73.20,Legit 
2016-01-03 02:53:32.231,78ueoau32,w234e989,1,USA,1621.00,Fraud 
2016-01-03 09:30:16.832,t842uocd,9732498oeu,4,USA,43.19,Legit 
2016-01-04 23:01:52.920,t842uocd,cza8873bm,10,MX,159.65,Legit 
2016-01-05 02:28:10.648,t842uocd,fgcq9803,6,CAN,26.33,Fraud 
2016-01-05 10:15:36.483,rgc707ke3,tn342v7,2,USA,-0.90,Legit 
</span></pre><p><span class="koboSpan" id="kobo.98.1">The problem statement of the program is as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.99.1">Remove some unnecessary columns</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.100.1">Filter out data, and keep only examples with values </span><code class="literal"><span class="koboSpan" id="kobo.101.1">USA</span></code><span class="koboSpan" id="kobo.102.1"> and </span><code class="literal"><span class="koboSpan" id="kobo.103.1">MX</span></code><span class="koboSpan" id="kobo.104.1"> for the </span><code class="literal"><span class="koboSpan" id="kobo.105.1">MerchantCountryCode</span></code><span class="koboSpan" id="kobo.106.1"> column</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.107.1">Replace the invalid entries in the </span><code class="literal"><span class="koboSpan" id="kobo.108.1">TransactionAmountUSD</span></code><span class="koboSpan" id="kobo.109.1"> column</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.110.1">Parse the data string, and collect the hour of day from it to create a new </span><code class="literal"><span class="koboSpan" id="kobo.111.1">HourOfDay</span></code><span class="koboSpan" id="kobo.112.1"> column</span></li></ul></div><pre class="programlisting"><span class="koboSpan" id="kobo.113.1">Schema inputDataSchema = new Schema.Builder() 
     .addColumnString("DateTimeString") 
     .addColumnsString("CustomerID", "MerchantID")  
     .addColumnInteger("NumItemsInTransaction") 
     .addColumnCategorical("MerchantCountryCode",  
      Arrays.asList("USA","CAN","FR","MX")) 
     .addColumnDouble("TransactionAmountUSD",0.0,null,false,false)  
     .addColumnCategorical("FraudLabel",Arrays.asList("Fraud","Legit"))
     .build(); 
  
System.out.println("\n\nOther information obtainable from schema:"); 
System.out.println("Number of columns: " + 
                   inputDataSchema.numColumns()); 
System.out.println("Column names: " +              
                   inputDataSchema.getColumnNames()); 
System.out.println("Column types: " +  
                   inputDataSchema.getColumnTypes()); 
</span></pre><p><span class="koboSpan" id="kobo.114.1">The following part will define the operations that we want to perform on the dataset:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.115.1">TransformProcess tp = new TransformProcess.Builder(inputDataSchema) 
.removeColumns("CustomerID","MerchantID") 
.filter(new ConditionFilter(
 new CategoricalColumnCondition("MerchantCountryCode", 
 ConditionOp.NotInSet, new HashSet&lt;&gt;(Arrays.asList("USA","MX"))))) 
</span></pre><p><span class="koboSpan" id="kobo.116.1">In unstructured data, the datasets are generally noisy, and so we need to take care of some of the invalid data. </span><span class="koboSpan" id="kobo.116.2">In case of negative dollar value, the program will replace those to </span><code class="literal"><span class="koboSpan" id="kobo.117.1">0.0</span></code><span class="koboSpan" id="kobo.118.1">. </span><span class="koboSpan" id="kobo.118.2">We will keep the positive dollar amounts intact.</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.119.1">.conditionalReplaceValueTransform( 
  "TransactionAmountUSD",       
  new DoubleWritable(0.0),      
  new DoubleColumnCondition("TransactionAmountUSD",ConditionOp.LessThan
  , 0.0))   
</span></pre><p><span class="koboSpan" id="kobo.120.1">Now, to format the </span><code class="literal"><span class="koboSpan" id="kobo.121.1">DateTime</span></code><span class="koboSpan" id="kobo.122.1"> format as per the problem statement, the following piece of code is used:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.123.1">.stringToTimeTransform("DateTimeString","YYYY-MM-DD HH:mm:ss.SSS",            
 DateTimeZone.UTC) 
.renameColumn("DateTimeString", "DateTime") 
.transform(new DeriveColumnsFromTimeTransform.Builder("DateTime") 
   .addIntegerDerivedColumn("HourOfDay", DateTimeFieldType.hourOfDay()) 
   .build())
.removeColumns("DateTime")
.build(); 
</span></pre><p><span class="koboSpan" id="kobo.124.1">A different schema is created after execution of the all these operations as follows:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.125.1">Schema outputSchema = tp.getFinalSchema(); 
 
System.out.println("\nSchema after transforming data:"); 
System.out.println(outputSchema); 
</span></pre><p><span class="koboSpan" id="kobo.126.1">The following piece of code will set Spark to perform all the operations:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.127.1">SparkConf conf = new SparkConf(); 
conf.setMaster("local[*]"); 
conf.setAppName("DataVec Example"); 
 
JavaSparkContext sc = new JavaSparkContext(conf); 
 
String directory = new  ClassPathResource("exampledata.csv").getFile()
.getParent(); 
</span></pre><p><span class="koboSpan" id="kobo.128.1">To take the data directly from HDFS, one has to pass </span><code class="literal"><span class="koboSpan" id="kobo.129.1">hdfs://{the filepath name}</span></code><span class="koboSpan" id="kobo.130.1">:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.131.1">JavaRDD&lt;String&gt; stringData = sc.textFile(directory); 
</span></pre><p><span class="koboSpan" id="kobo.132.1">The input data are parsed using </span><code class="literal"><span class="koboSpan" id="kobo.133.1">CSVRecordReader()</span></code><span class="koboSpan" id="kobo.134.1"> method as follows:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.135.1">RecordReader rr = new CSVRecordReader(); 
JavaRDD&lt;List&lt;Writable&gt;&gt; parsedInputData = stringData.map(new  StringToWritablesFunction(rr)); 
</span></pre><p><span class="koboSpan" id="kobo.136.1">The pre-defined transformation of Spark is performed as follows:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.137.1">SparkTransformExecutor exec = new SparkTransformExecutor(); 
JavaRDD&lt;List&lt;Writable&gt;&gt; processedData = exec.execute(parsedInputData, 
tp); 
  
JavaRDD&lt;String&gt; processedAsString = processedData.map(new 
WritablesToStringFunction(","));  
</span></pre><p><span class="koboSpan" id="kobo.138.1">As mentioned, to save the data back to HDFS, just putting the file path after </span><code class="literal"><span class="koboSpan" id="kobo.139.1">hdfs://</span></code><span class="koboSpan" id="kobo.140.1"> will do:</span></p><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.141.1">processedAsString.saveAsTextFile("hdfs://your/hdfs/save/path/here")</span></strong></span><span class="koboSpan" id="kobo.142.1"> 
 
List&lt;String&gt; processedCollected = processedAsString.collect(); 
List&lt;String&gt; inputDataCollected = stringData.collect(); 
 
 
System.out.println("\n ---- Original Data ----"); 
for(String s : inputDataCollected) System.out.println(s); 
 
System.out.println("\n ---- Processed Data ----"); 
for(String s : processedCollected) System.out.println(s); 
</span></pre><p><span class="koboSpan" id="kobo.143.1">When the program is executed with Spark using Deeplearning4j, we will get the following output:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.144.1">14:20:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.0 KB, free 1390.9 MB) 
16/08/27 14:20:12 INFO MemoryStore: ensureFreeSpace(10065) called with curMem=106480, maxMem=1458611159 
16/08/27 14:20:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.8 KB, free 1390.9 MB) 
16/08/27 14:20:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:46336 (size: 9.8 KB, free: 1391.0 MB) 
16/08/27 14:20:12 INFO SparkContext: Created broadcast 0 from textFile at BasicDataVecExample.java:144 
16/08/27 14:20:13 INFO SparkTransformExecutor: Starting execution of stage 1 of 7 
16/08/27 14:20:13 INFO SparkTransformExecutor: Starting execution of stage 2 of 7 
16/08/27 14:20:13 INFO SparkTransformExecutor: Starting execution of stage 3 of 7 
16/08/27 14:20:13 INFO SparkTransformExecutor: Starting execution of stage 4 of 7 
16/08/27 14:20:13 INFO SparkTransformExecutor: Starting execution of stage 5 of 7 
</span></pre><p><span class="koboSpan" id="kobo.145.1">The following is the output:</span></p><pre class="programlisting"><span class="koboSpan" id="kobo.146.1">---- Processed Data ---- 
17,1,USA,100.00,Legit 
2,1,USA,1621.00,Fraud 
9,4,USA,43.19,Legit 
23,10,MX,159.65,Legit 
10,2,USA,0.0,Legit 
</span></pre><p><span class="koboSpan" id="kobo.147.1">Similar to this example, lots of other datasets can be processed in a customized way in Spark. </span><span class="koboSpan" id="kobo.147.2">From the next chapter, we will show the Deeplearning4j codes for specific deep neural networks. </span><span class="koboSpan" id="kobo.147.3">The implementation of Apache Spark and Hadoop YARN is a generic procedure, and will not change according to neural network. </span><span class="koboSpan" id="kobo.147.4">Readers can use that code to deploy the deep network code in cluster or locally, based on their requirements.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec17"/><span class="koboSpan" id="kobo.1.1">Summary</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">In contrast to the traditional machine learning algorithms, deep learning models have the capability to address the challenges imposed by a massive amount of input data. </span><span class="koboSpan" id="kobo.2.2">Deep learning networks are designed to automatically extract complex representation of data from the unstructured data. </span><span class="koboSpan" id="kobo.2.3">This property makes deep learning a precious tool to learn the hidden information from the big data. </span><span class="koboSpan" id="kobo.2.4">However, due to the velocity at which the volume and varieties of data are increasing day by day, deep learning networks need to be stored and processed in a distributed manner. </span><span class="koboSpan" id="kobo.2.5">Hadoop, being the most widely used big data framework for such requirements, is extremely convenient in this situation. </span><span class="koboSpan" id="kobo.2.6">We explained the primary components of Hadoop that are essential for distributed deep learning architecture. </span><span class="koboSpan" id="kobo.2.7">The crucial characteristics of distributed deep learning networks were also explained in depth. </span><span class="koboSpan" id="kobo.2.8">Deeplearning4j, an open source distributed deep learning framework, integrates with Hadoop to achieve the mentioned indispensable requirement. </span><span class="koboSpan" id="kobo.2.9">Deeplearning4j is entirely written in Java, can process data faster in a distributed manner with iterative Map-Reduce, and can address many problems imposed by the large-scale data. </span><span class="koboSpan" id="kobo.2.10">We have provided two sample examples to let you know about basic Deeplearning4j codes and syntax. </span><span class="koboSpan" id="kobo.2.11">We have also provided some code snippets for Spark configuration with integration with Hadoop YARN and Hadoop Distributed File System.</span></p><p><span class="koboSpan" id="kobo.3.1">The next chapter of this book will introduce convolutional neural network, a popular deep learning network. </span><span class="koboSpan" id="kobo.3.2">The chapter will discuss the method convolution and how it can be used to build an advanced neural network mainly for image processing and image recognition. </span><span class="koboSpan" id="kobo.3.3">The chapter will then provide information on how a convolutional neural network can be implemented using Deeplearning4j.</span></p></div></div></div></body></html>