- en: Image Classification for Small Data Using Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we developed deep learning networks and explored various
    application examples related to image data. One major difference compared to what
    we will be discussing in this chapter is that, in the previous chapters, we developed
    models from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning can be defined as an approach where we reuse what a trained
    deep network has learned to solve a new but related problem. For example, we may
    be able to reuse a deep learning network that's been developed to classify thousands
    of different fashion items to develop a deep network to classify three different
    types of dresses. This approach is similar to what we can observe in real life,
    where a teacher transfers knowledge or learning gained over the years to students
    or a coach passes on learning or experience to new players. Another example is
    where learning to ride a bicycle is transferred to learning to ride a motorbike
    and this, in turn, becomes useful for learning how to drive a car.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will make use of pretrained deep networks while developing
    models for image classification. Pretrained models allow us to transfer useful
    features that we've learned from a much larger dataset to models we are interested
    in developing with a somewhat similar, but new and relatively smaller dataset.
    The use of pretrained models not only allows us to overcome issues as a result
    of the dataset being small, but also helps reduce the time and cost of developing
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the use of pretrained image classification models, in this chapter,
    we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a pretrained model to identify an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with the CIFAR10 dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image classification with CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying images using the pretrained RESNET50 model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation and prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a pretrained model to identify an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we proceed, let''s load three packages that we''ll need in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The Keras and TensorFlow libraries will be used for developing the pretrained
    image classification model, while the EBImage library will be used for processing
    and visualizing image data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, the following pretrained image classification models are available:'
  prefs: []
  type: TYPE_NORMAL
- en: Xception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VGG16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VGG19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: InceptionV3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: InceptionResNetV2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MobileNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MobileNetV2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DenseNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NASNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These pretrained models are trained on images from ImageNet ([http://www.image-net.org/](http://www.image-net.org/)).
    ImageNet is a huge image database that contains several million images.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by using a pretrained model known as `resnet50` to identify an
    image. The following is the code we can use to utilize this pretrained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have specified `weights` as `"imagenet"`. This allows us to reuse the
    pretrained weights of the RESNET50 network. RESNET50 is a deep residual network
    that has a depth of 50 layers and includes convolutional neural network layers.
    Note that in case we only want to use the model architecture without the pretrained
    weights and we would like to train from scratch, then we can specify `weights`
    as `null`. By using `summary`, we can obtain the architecture of the RESNET50
    network. However, to conserve space, we do not provide any output from the summary.
    The total number of parameters in this network is 25,636,712\. The RESNET50 network
    is trained in using over a million images from ImageNet and has the capability
    to classify images into 1,000 different categories.
  prefs: []
  type: TYPE_NORMAL
- en: Reading an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by reading an image of a dog in RStudio. The following code loads
    an image file and then obtains the respective output:'
  prefs: []
  type: TYPE_NORMAL
- en: When using the RESNET50 network, the maximum target size that's allowed is 224
    x 224 and the minimum target size that's allowed is 32 x 32.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A picture of a Norwich terrier dog is loaded from the computer desktop that's
    224 x 224 in size using the `image_load()` function from Keras.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the original image may not be 224 x 224 in size. However, specifying
    this dimension at the time of loading the image allows us to easily resize the
    original image so that it has new dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This image is converted into an array of numbers using the `image_to_array()`
    function. The structure of this array shows a dimension of 224 x 224 x 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The summary of the array shows that it contains numbers between zero and 255.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the 224 x 224 color picture of a Norwich terrier dog. This
    can be obtained using a plot command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7f94f76-0669-4b3b-94fd-923433580bf8.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding image is a picture of a Norwich terrier dog sitting and looking
    forward. We will make use of this picture and check whether the RESNET50 model
    can accurately predict the type of dog in the picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'A histogram that was developed from the values in the array is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a64050d-273a-438d-b893-acc6f8172087.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding histogram of values in the array shows that the intensity values
    range from zero to 255, with most of the values concentrated around 200\. Next,
    we will preprocess the image data. This histogram can be used to compare the resulting
    changes to the image data.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the input
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now preprocess the input to prepare it so that it can be used with the
    pretrained RESNET50 model. The codes to preprocess the data are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: After applying the `array_reshape()` function, the dimensions of the array will
    change to 1 x 224 x 224 x 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used the `imagnet_preprocess_input()` function to prepare the data in the
    required format using the pretrained model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A plot of the data in the form of a histogram after preprocessing is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e135198d-2aef-4e2d-8f41-9b507ea26fdf.png)'
  prefs: []
  type: TYPE_IMG
- en: The histogram of values after preprocessing shows a shift in location. Most
    of the values are now concentrated between 50 and 100\. However, there is no major
    change in the overall pattern of the histogram.
  prefs: []
  type: TYPE_NORMAL
- en: Top five categories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we can use the pretrained model to make predictions by providing preprocessed
    image data as input. The code to achieve this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The predictions are made using the `predict` function and contain probabilities
    for 1,000 different categories, out of which the top five categories with the
    highest probabilities are obtained using the `imagenet_decode_predictions()` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The highest score of about 0.7699 correctly identifies that the picture is of
    a Norwich terrier dog.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second highest score is for the Norfolk terrier dog, which looks very similar
    to the Norwich terrier dog.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predictions also suggest that the picture could be of another type of terrier
    dog; however, those probabilities are relatively small or negligible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will look at a larger image dataset instead of a single
    image and use a pretrained network to develop an image classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the CIFAR10 dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For illustrating the use of pretrained models with new data, we will make use
    of the CIFAR10 dataset. CIFAR stands for *Canadian Institute For Advanced Research*,
    and 10 refers to the 10 categories of images that are contained in the data. The
    CIFAR10 dataset is part of the Keras library and the code for obtaining it is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We can read the dataset using the `dataset_cifar10()` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The structure of the data shows that there are 50,000 training images available
    with labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also contains 10,000 test images with labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we will extract the train and test data from CIFAR10 using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We saved the training image data in `trainx` and the test image data in `testx`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also carry out one-hot encoding of the train and test data labels using the `to_categorical()` 
    function and save the results in `trainy` and `testy`, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The table for the training data indicates that the images are classified in
    10 different categories, with each category containing exactly 5,000 images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, the test data contains exactly 1,000 images for each of the 10 categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, the labels for the first 64 images in the training data can
    be obtained using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, each picture is labeled using a number between 0 and 9\. A description
    of the 10 different categories of images can be seen in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Label | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Airplane |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Automobile |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Bird |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Cat |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Deer |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Dog |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Frog |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Horse |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Ship |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Truck |'
  prefs: []
  type: TYPE_TB
- en: Note that there is no overlap between these 10 categories. For example, the
    automobile category refers to cars and SUVs, whereas the truck category only refers
    to large trucks.
  prefs: []
  type: TYPE_NORMAL
- en: Sample images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first 64 images from the training data of CIFAR10 can be plotted using
    the following code. Doing this, we can get a glimpse of the type of images contained
    in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The images from CIFAR10 are all 32 x 32 color images. The following plot shows
    64 images in an 8 x 8 grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1db8a05c-8153-45ea-8868-1d191620f6b8.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding images, we can see that these images come with various backgrounds
    and are of low resolution. In addition, sometimes, these images aren't completely
    visible, which makes image classification a challenging task.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing and prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use the pretrained RESNET50 model to identify the second image in the
    training data. Note that, since this second image in the training data is 32 x
    32 in size, whereas RESNET50 is trained on images that are 224 x 224 in size,
    we need to resize the image before applying the code that we have used earlier.
    The following code is used for identifying the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can observe that the top category with a score of
    0.9988 is for a moving van. The scores for the other four categories are comparatively
    negligible.
  prefs: []
  type: TYPE_NORMAL
- en: Image classification with CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use a subset of the CIFAR10 dataset to develop a convolutional
    neural network-based image classification model and assess its classification
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will keep the data size smaller by using only the first 2,000 images in
    the training and test data from CIFAR10\. This will allow the image classification
    model to be run on a regular computer or laptop. We will also resize the training
    and test images from 32 x 32 dimensions to 224 x 224 dimensions to be able to
    compare classification performance with the pretrained model. The following code
    includes the necessary preprocessing that we went over earlier in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, while resizing dimensions from 32 x 32 to 224 x 224,
    we use bilinear interpolation, which is included as part of the EBImage package.
    Bilinear interpolation extends linear interpolation to two variables, which in
    this case is the height and width of an image. The effect of bilinear interpolation
    can be observed from the before and after images of the truck shown in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b87b0400-79fc-43ed-b4d3-bfc2b466229a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that the after image (second image) looks smoother as it contains
    more pixels compared to the original image (first image).
  prefs: []
  type: TYPE_NORMAL
- en: CNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start by using a not-so-deep convolutional neural network to develop
    an image classification model. We will use the following code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The total number of parameters in this network is 99,136,170.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When compiling the model, we use `categorical_crossentropy` as the loss function
    since the response has 10 categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the optimizer, we specify `rmsprop`, which is a gradient-based optimization
    method and is a popular choice that provides reasonably good performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We train the model with 10 epochs and with a batch size of 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of 2,000 images in the training data, 20% (or 400 images) is used for assessing
    validation errors and the remaining 80% (or 1,600 images) is used for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A plot of the accuracy and loss values after training the model is as follows
    for `model_one`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a382f094-6b43-4c2c-9725-6168925657f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, the following observations can be made:'
  prefs: []
  type: TYPE_NORMAL
- en: The plot of the accuracy and loss values shows that, after about 4 epochs, the
    loss and accuracy values for both training and validation data remain more or
    less constant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the accuracy for the training data reaches high values closer to 100%,
    there seems to be no impact on the accuracy based on the images in the validation
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, the gap between the accuracy of the training and validation data
    seems to be high, suggesting the presence of overfitting. When assessing the model's
    performance, we expect to see low accuracy in terms of image classification by
    the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that developing a decent image classification model using CNN requires
    a large number of images for training, and therefore more time and resources.
    Later in this chapter, we will learn how to use pretrained networks to help us
    overcome this problem. For now, though, let's proceed by assessing the image classification
    model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For assessing the model's performance, we will carry out calculations for the
    loss, accuracy, and confusion matrix for the training and test data.
  prefs: []
  type: TYPE_NORMAL
- en: Performance assessment with training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for obtaining the loss, accuracy, and confusion matrix based on the
    training data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the loss and accuracy values for the training data are
    3.335 and 0.846, respectively. The confusion matrix shows decent results based
    on the training data. However, for some types of images, misclassifications are
    high. For example, 12 images from category 7 (horse) are misclassified as category-9
    (truck). Similarly, 11 images, each belonging to category-6 (frog) and category-8
    (ship), are also misclassified as category-9 (truck).
  prefs: []
  type: TYPE_NORMAL
- en: Performance assessment with test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for obtaining the loss, accuracy, and confusion matrix based on the
    test data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding output, the following observations can be made:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy values for the test data are 16.456 and 0.232, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These results are not as impressive as what we observed for the training data
    due to the overfitting problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although we can try and develop a deeper network in an effort to improve image
    classification results or to try and increase training data to provide more samples
    to learn from, here, we will make use of pretrained networks to obtain better
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying images using the pretrained RESNET50 model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will make use of the pretrained RESNET50 model to develop
    an image classification model. We will use the same training and test data that
    we used in the previous section to make comparing classification performance easier.
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will upload the RESNET50 model without including the top layer. This will
    help us customize the pretrained model for use with CIFAR10 data. Since the RESNET50
    model is trained with the help of over 1 million images, it captures useful features
    and representations of images that can be reused with new but similar and smaller
    data. This reusability aspect of pretrained models not only helps to reduce the
    time and cost of developing an image classification model from scratch, but is
    especially useful when the training data is relatively small.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code that''s used for developing the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: When uploading the RESNET50 model, the input dimensions for the data based on
    color images are specified as 224 x 224 x 3\. Although smaller dimensions will
    work too, image dimensions cannot be less than 32 x 32 x 3\. Images in the CIFAR10
    dataset have dimensions of 32 x 32 x 3, but we have resized them to 224 x 224
    x 3 as it gives us better image classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding summary, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The output dimensions from the RESNET50 network are 7 x 7 x 2,048.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use a flattened layer to change the output shape to a single column with
    7 x 7 x 2,048 = 100,352 elements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dense layer with 256 units and a `relu` activation function is added.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This dense layer leads to (100,353 x 256) + 256 = 25,690,368 parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last dense layer has 10 units for images with 10 categories and a `softmax`
    activation function. This network has a total of 49,280,650 parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of the total parameters in the network, 49,227,530 are trainable parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although we can train the network with all of these parameters, this is not
    advisable. Training and updating parameters related to the RESNET50 network will
    cause us to lose the benefits that we would get as a result of the features that
    have been learned from over 1 million images. We are only using data from 2,000
    images for training and have 10 different categories. So, for each category, we
    only have approximately 200 images. Therefore, it is important to freeze the weights
    in the RESNET50 network, which will allow us to obtain the benefits of using a
    pretrained network.
  prefs: []
  type: TYPE_NORMAL
- en: Freezing pretrained network weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for freezing the weights of the RESNET50 network and then compiling
    the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: To freeze the weights in the RESNET50 network, we use the `freeze_weights()`
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that after freezing the pretrained network weights, the model needs to
    be compiled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After freezing the weights of the RESNET50 network, we observe that the number
    of trainable parameters goes down from 49,227,530 to a lower value of 25,692,938.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These parameters belong to the two dense layers that we added and will help
    us customize the results from the RESNET50 network so that we can apply them to
    the images from the CIFAR10 data that we are using.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for fitting the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We train the network with 10 epochs and with a batch size of 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We specify 20% (or 400 images) to be used for assessing the validation loss
    and validation accuracy, and the remaining 80% (or 1,600 images) for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The plot of the accuracy and loss values after training the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5deac824-d6ec-4195-917a-6a696101b72d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the plot for the loss and accuracy values, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: There is an important difference compared to the previous plot, where the pretrained
    model wasn't used. This plot shows us that the model reaches an accuracy of over
    60% by the second epoch itself compared to the previous plot, where it remained
    below 25%. Thus, we can see that the use of a pretrained model has an immediate
    impact on image classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The improvements based on validation data are slow compared to those for the
    training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the accuracy values based on the validation data show gradual improvement,
    the loss values for the validation data show more variability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will evaluate the model and assess its prediction performance.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation and prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will evaluate the performance of this model for the training and test
    data. Calculations relating to the loss, accuracy, and confusion matrix will be
    carried out so that we can evaluate the model image's classification performance.
    We will also obtain the accuracy for each of the 10 categories.
  prefs: []
  type: TYPE_NORMAL
- en: Loss, accuracy, and confusion matrix with the training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for obtaining the loss, accuracy, and confusion matrix for the training
    data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy based on the training data are 1.954 and 0.879, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both of these numbers are an improvement over the corresponding results based
    on the previous model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The confusion matrix shows a decent image classification performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best image classification performance is seen for category 9 (truck), where
    only one image is misclassified as category-0 (airplane) and provides an accuracy
    of 99.5%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model is the most confused regarding category-3 (cat), which is mostly
    classified as category-5 (dog) or category-7 (horse) and provides an accuracy
    of only 68.2% for this category.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among the misclassifications, the highest case (35 images) is when category-1
    (automobile) is misclassified as category-9 (truck).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will assess the model's performance using the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Loss, accuracy, and confusion matrix with the test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for obtaining the loss, accuracy, and confusion matrix for the test
    data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy based on the test data are 4.437 and 0.768, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although this performance based on the test data is inferior to the results
    based on the training data, it is a significant improvement over the results from
    the first model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The confusion matrix provides further insights into the model's performance.
    The best performance is for category 9 (truck), with 199 correct classifications
    and an accuracy of 98%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the test data, the model seems to be the most confused regarding category-3
    (cat), which has the most misclassifications. The accuracy of this category can
    be as low as 43.2%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The highest misclassification for a single category (54 images) is for category-1
    (automobile), which is misclassified as category-9 (truck).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With 76.8% accuracy, we can say that this image classification performance is
    decent. The use of a pretrained model has allowed us to transfer our learning
    of a model trained on data involving over 1 million images to new data containing
    2,000 images from the CIFAR10 dataset. This is a huge advantage compared to building
    an image classification model totally from scratch, which would involve more time
    and computing costs. Now that we've achieved a decent performance from the model,
    we can explore how to improve this even further.
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To explore further image classification improvement, in this section, we will
    try three experiments. In the first experiment, we will mainly use the `adam`
    optimizer when compiling the model. In the second experiment, we will carry out
    hyperparameter tuning by varying the number of units in the dense layer, the dropout
    percentage in the dropout layer, and the batch size when fitting the model. Finally,
    in the third experiment, we will work with another pretrained network called VGG16.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with the adam optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this first experiment, we will use the `adam` optimizer when compiling the
    model. At the time of training the model, we will also increase the number of
    epochs to 20.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot of the accuracy and loss values after training the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0987325-56b5-4aa2-ad2a-d7ea843fb6f4.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding loss and accuracy plot for this model shows that the values related
    to the training data are flat after about six epochs. For the validation data,
    the loss values show a gradual increase, whereas the accuracy values are flat
    after the third epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for obtaining the loss, accuracy, and confusion matrix for the test
    data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy of the test data are 4.005 and 0.772, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These results are marginally better than they are for `model_two`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The confusion matrix shows a somewhat different image classification pattern
    compared to the previous model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best classification results are obtained for category 8 (ship), with 205
    correct image classifications out of 217 (94.5% accuracy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lowest classification performance is for category 3 (cat), with 80 correct
    predictions out of 199 (40.2% accuracy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The worst misclassification is of 58 images from category 3 (cat) when they
    are misclassified as category-5 (dog).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will carry out an experiment with hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this experiment, we will vary the units in the dense layer, the dropout
    rate, and the batch size to obtain values that help us improve classification
    performance. This also illustrates an efficient way of obtaining suitable parameter
    values through experimentation. We will start by creating a `TransferLearning.R` file
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, after reading the pretrained model, we declare three
    flags for the parameters that we want to experiment with. Now, we can use these
    flags in the model architecture (dense units and dropout rate) and in the code
    for fitting the model (batch size). We have reduced the number of epochs to five
    and for the optimizer, while compiling the model, we retain `adam`. We will save
    this R file, which we'll call `TransferLearning.R`, on the desktop of our computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for running this experiment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we can see that the working directory is set at the location
    of the `TransferLearning.R` file. Note that the output from this experiment will
    be saved in this directory too. For running the hyperparameter tuning experiment,
    we will use the `tfruns` library. For the number of units in the dense layer,
    we will try 256 and 512 as the values. For the dropout rate, we will experiment
    with 0.1 and 0.3\. Fnally, for the batch size, we will try 10 and 30\. With three
    parameters, each being tried at two values, the total number of experimental runs
    will be 2³ = 8.
  prefs: []
  type: TYPE_NORMAL
- en: 'An extract from the results that were obtained from this experiment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding output shows the loss and accuracy values based on the validation
    data for all eight experimental runs. For easy reference, it also includes parameter
    values. We can make the following observations from the preceding output:'
  prefs: []
  type: TYPE_NORMAL
- en: The highest accuracy value (row 3) is obtained when the number of dense units
    is 512, the dropout rate is 0.1, and the batch size is 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the lowest accuracy value (row 6) is obtained when the number
    of dense units is 256, the dropout rate is 0.3, and the batch size is 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for obtaining the loss, accuracy, and confusion matrix using the test
    data for row 3 of the experiment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding results, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Both the loss and accuracy values for the test data are better than the results
    we've obtained so far.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best classification results are obtained for category 8 (ship), with 203
    correct image classifications out of 217 (93.5% accuracy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lowest classification performance is for category 3 (cat), with 92 correct
    predictions out of 199 (46.2% accuracy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The worst misclassification is of 50 images from category 3 (cat) when they
    are misclassified as category-5 (dog).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next experiment, we will use another pretrained network: VGG16.'
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with VGG16 as a pretrained network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this experiment, we will use a pretrained network called VGG16\. VGG16 is
    a convolutional neural network that is 16 layers deep and can classify images
    into thousands of categories. This network is also trained using over 1 million
    images from the ImageNet database. The code for the model''s architecture and
    compiling and then fitting the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding summary, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: This model has 21,140,042 parameters, which, after freezing the weights of VGG16,
    goes down to a total of 6,425,354 trainable parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When compiling the model, we retain the use of the `adam` optimizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, we run 10 epochs to train the model. All the other settings are
    the same ones that we used for the previous models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A plot of the accuracy and loss values after training the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57485ef8-cf6f-45da-be86-19fd0123cf60.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding loss and accuracy plot for the training and validation data indicates
    that, after about four epochs, the model performance remains flat. This is in
    contrast to the previous model, where the loss values for the validation data
    showed a gradual increase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for obtaining the loss, accuracy, and confusion matrix for the test
    data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding output, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy of the test data are 1.674 and 0.757, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The confusion matrix provides further insights. This model has the best classification
    accuracy of 88.9% when classifying category 6 (frog).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the accuracy when classifying category 4 (deer) images is
    only about 59.6%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we experimented with three situations:'
  prefs: []
  type: TYPE_NORMAL
- en: The use of the `adam` optimizer improved the results a little bit and provided
    test data accuracy of about 77.2%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second experiment, hyperparameter tuning provided the best results for
    the number of dense units at 512, a dropout rate at 0.1, and a batch size at 30\.
    This combination of parameters helped us obtain a test data accuracy of about
    79.8%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third experiment, where we used the VGG16 pretrained network, also provided
    decent results. However, it provided test data accuracy of slightly lower than
    75.7%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another approach when working with smaller datasets is to use data augmentation.
    In this approach, the existing images are modified (by flipping, rotation, shifting,
    and so on) to create new samples. Since images in image datasets aren't always
    centered, such artificially created new samples help us to learn about useful
    features that, in turn, improve image classification performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we illustrated the use of pretrained deep neural networks for
    developing image classification models. Such pretrained networks, which are trained
    using over 1 million images, capture reusable features that can be applied to
    similar but new data. This aspect becomes valuable when developing image classification
    models with relatively smaller datasets. In addition, they provide savings in
    terms of the use of computational resources and time. We started by making use
    of the RESNET50 pretrained network to identify an image of a Norwich terrier dog.
    Subsequently, we made use of 2,000 images from the CIFAR10 dataset to illustrate
    the usefulness of applying pretrained networks to a relatively smaller dataset.
    The initial convolutional neural networks model that we built from scratch suffered
    from overfitting and did not yield useful results.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we used the pretrained RESNET50 network and customized it to suit our
    needs by adding two dense layers on top of the pretrained network. We obtained
    decent results, with a test data accuracy of about 76.8%. Although pretrained
    models can provide faster results that require fewer epochs, we need to explore
    improvements that we can make to the model's performance with the help of some
    experimentation. In an effort to explore better results, we experimented with
    the `adam` optimizer, which yielded test data accuracy of about 77.2%. We also
    carried out hyperparameter tuning, which yielded the best levels in terms of the
    number of units in the dense layer, which was 512, the dropout rate in the dropout
    layer, which was 0.1, and the batch size at the time of fitting the model, which
    was 30\. The image classification accuracy with this combination yielded a test
    data accuracy of about 79.8%. Finally, we experimented with the pretrained VGG16
    network, where we obtained test data accuracy of about 75.6%. These experiments
    illustrated how we can explore and improve model performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore another interesting and popular class of
    deep networks, called **generative adversarial networks** (**GANs**). We will
    make use of GANs to create new images.
  prefs: []
  type: TYPE_NORMAL
