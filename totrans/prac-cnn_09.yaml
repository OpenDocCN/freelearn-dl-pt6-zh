- en: Attention Mechanism for CNN and Visual Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not everything in an image or text—or in general, any data—is equally relevant
    from the perspective of insights that we need to draw from it. For example, consider
    a task where we are trying to predict the next word in a sequence of a verbose
    statement like *Alice and Alya are friends. Alice lives in France and works in
    Paris. Alya is British and works in London. Alice prefers to buy books written
    in French, whereas Alya prefers books in _____.*
  prefs: []
  type: TYPE_NORMAL
- en: 'When this example is given to a human, even a child with decent language proficiency
    can very well predict the next word will most probably be *English*. Mathematically,
    and in the context of deep learning, this can similarly be ascertained by creating
    a vector embedding of these words and then computing the results using vector
    mathematics, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7834171b-7edd-4d0f-84b3-cc116bcd796a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *V(Word)* is the vector embedding for the required word; similarly, *V(French)*,
    *V(Paris)*, and *V(London)* are the required vector embeddings for the words *French*,
    *Paris*, and *London*, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings are (often) lower dimensional and dense (numerical) vector representations
    of inputs or indexes of inputs (for non-numerical data); in this case, text.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms such as `Word2Vec` and `glove` can be used to get word embeddings.
    Pretrained variants of these models for general texts are available in popular
    Python-based NLP libraries, such as SpaCy, Gensim and others can also be trained
    using most deep learning libraries, such as Keras, TensorFlow, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of embeddings is as much relevant to vision and images as it is
    to text.
  prefs: []
  type: TYPE_NORMAL
- en: There may not be an existing vector exactly matching the vector we obtained
    just now in the form of ![](img/c3f164b3-d260-4473-9ec8-892bf9703ee6.png); but
    if we try to find the one closest to the so obtained ![](img/a8e0ee9f-386b-48a2-a6df-61e883dde986.png)
    that exists and find the representative word using reverse indexing, that word
    would most likely be the same as what we as humans thought of earlier, that is,
    *English*.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms such as cosine similarity can be used to get the vector closest to
    the computed one.
  prefs: []
  type: TYPE_NORMAL
- en: For implementation, a computationally more efficient way of finding the closest
    vector would be **approximate nearest neighbor** (**ANN**), as available in Python's
    `annoy` library.
  prefs: []
  type: TYPE_NORMAL
- en: Though we have helped get the same results, both cognitively and through deep
    learning approaches, the input in both the cases was not the same. To humans,
    we had given the exact sentence as to the computer, but for deep learning applications,
    we had carefully picked the correct words (*French*, *Paris*, and *London*) and
    their right position in the equation to get the results. Imagine how we can very
    easily realize the right words to pay attention to in order to understand the
    correct context, and hence we have the results; but in the current form, it was
    not possible for our deep learning approach to do the same.
  prefs: []
  type: TYPE_NORMAL
- en: Now there are quite sophisticated algorithms in language modeling using different
    variants and architectures of RNN, such as LSTM and Seq2Seq, respectively. These
    could have solved this problem and got the right solution, but they are most effective
    in shorter and more direct sentences, such as *Paris is to French what London
    is to _____*. In order to correctly understand a long sentence and generate the
    correct result, it is important to have a mechanism to teach the architecture
    whereby specific words need to be paid more attention to in a long sequence of
    words. This is called the **attention mechanism** in deep learning, and it is
    applicable to many types of deep learning applications but in slightly different
    ways.
  prefs: []
  type: TYPE_NORMAL
- en: '**RNN** stands for **recurrent neural networks** and is used to depict a temporal
    sequence of data in deep learning. Due to the vanishing gradient problem, RNN
    is seldom used directly; instead, its variants, such as **LSTM** (**Long-Short
    Term Memory**) and **GRU** (**Gated Recurrent Unit**) are more popular in actual
    implementations.**Seq2Seq** stands for **Sequence-to-Sequence** models and comprises
    two RNN (or variant) networks (hence it is called **Seq2Seq**, where each RNN
    network represents a sequence); one acts as an encoder and the other as a decoder.
    The two RNN networks can be multi-layer or stacked RNN networks, and they are
    connected via a thought or context vector. Additionally, Seq2Seq models may use
    the attention mechanism to improve performance, especially for longer sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, to be more precise, even we had to process the preceding information
    in layers, first understanding that the last sentence is about Alya. Then we can
    identify and extract Alya's city, then that for Alice, and so on. Such a layered
    way of human thinking is analogous to stacking in deep learning, and hence in
    similar applications, stacked architectures are quite common.
  prefs: []
  type: TYPE_NORMAL
- en: To know more about how stacking works in deep learning, especially with sequence-based
    architectures, explore topics such as stacked RNN and stacked attention networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanism for image captioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of attention (Hard, and Soft Attentions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using attention to improve visual models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent models of visual attention
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention mechanism for image captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the introduction, so far, it must be clear to you that the attention mechanism
    works on a sequence of objects, assigning each element in the sequence a weight
    for a specific iteration of a required output. With every next step, not only
    the sequence but also the weights in the attention mechanism can change. So, attention-based
    architectures are essentially sequence networks, best implemented in deep learning
    using RNNs (or their variants).
  prefs: []
  type: TYPE_NORMAL
- en: 'The question now is: how do we implement a sequence-based attention on a static
    image, especially the one represented in a **convolutional neural network** (**CNN**)?
    Well, let''s take an example that sits right in between a text and image to understand
    this. Assume that we need to caption an image with respect to its contents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have some images with captions provided by humans as training data and using
    this, we need to create a system that can provide a decent caption for any new
    image not seen earlier by the model. As seen earlier, let''s take an example and
    see how we, as humans, will perceive this task and the analogous process to it
    that needs to be implemented in deep learning and CNN. Let''s consider the following
    image and conceive some plausible captions for it. We''ll also rank them heuristically
    using human judgment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4769fd23-01c6-49b2-a5bb-003b7d071e8b.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Some probable captions (in order of most likely to least likely) are:'
  prefs: []
  type: TYPE_NORMAL
- en: Woman seeing dog in snow forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown dog in snow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A person wearing cap in woods and white land
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dog, tree, snow, person, and sunshine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An important thing to note here is that, despite the fact that the woman is
    central to the image and the dog is not the biggest object in the image, the caption
    we sought probable focused on them and then their surroundings here. This is because
    we consider them as important entities here (given no prior context). So as humans,
    how we reached this conclusion is as follows: we first glanced the whole image,
    and then we focused towards the woman, in high resolution, while putting everything
    in the background (assume a **Bokeh** effect in a dual-camera phone). We identified
    the caption part for that, and then the dog in high resolution while putting everything
    else in low resolution; and we appended the caption part. Finally, we did the
    same for the surroundings and caption part for that.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So essentially, we saw it in this sequence to reach to the first caption:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f25f7ff-82b8-48db-b230-bbb01e70cc2e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 1: Glance the image first'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c27a5da0-d904-4809-87fd-45f43765afcd.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 2: Focus on woman'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75e4944d-5be6-42ca-b241-941152165171.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Image 3: Focus on dog
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c6594a7-aeca-4723-b445-37b172ec0a1c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Image 4: Focus on snow'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cdf6808c-9cef-4710-bcf3-a959a2ab1810.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Image 5: Focus on forest
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of weight of attention or focus, after glancing the image, we focus
    on the first most important object: the woman here. This is analogous to creating
    a mental frame in which we put the part of the image with the woman in high-resolution
    and the remaining part of the image in low-resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: In a deep learning reference, the attention sequence will have the highest weight
    for the vector (embedding) representing the concept of the woman for this part
    of the sequence. In the next step of the output/sequence, the weight will shift
    more towards the vector representation for the dog and so on.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this intuitively, we convert the image represented in the form
    of CNN into a flattened vector or some other similar structure; then we create
    different splices of the image or sequences with different parts in varying resolutions.
    Also, as we understand now from our discussion in [Chapter 7](20952d99-3977-420f-a5c7-a3320b96bed6.xhtml),
    *Object-Detection & Instance-Segmentation with CNN*, we must have the relevant
    portions that we need to detect in varying scales as well for effective detection.
    The same concept applies here too, and besides resolution, we also vary the scale;
    but for now, we will keep it simple and ignore the scale part for intuitive understanding.
  prefs: []
  type: TYPE_NORMAL
- en: These splices or sequences of images now act as a sequence of words, as in our
    earlier example, and hence they can be treated inside an RNN/LSTM or similar sequence-based
    architecture for the purpose of attention. This is done to get the best-suited
    word as the output in every iteration. So the first iteration of the sequence
    leads to woman (from the weights of a sequence representing an object represented
    as a *Woman* in *Image 2*) → then the next iteration as → *seeing* (from a sequence
    identifying the back of the *Woman* as in *Image 2*) → *Dog* (sequence as in *Image
    3*) → *in* (from a sequence where everything is blurred generating *filler* words
    transitioning from entities to surroundings) → *Snow* (sequence as in *Image 4*) →
    *Forest* (sequence as in *Image 5*).
  prefs: []
  type: TYPE_NORMAL
- en: Filler words such as *in* and action words such as *seeing* can also be automatically
    learned when the best image splice/sequence mapping to human-generated captions
    is done across several images. But for the simpler version, a caption such as *Woman*,
    *Dog*, *Snow*, and *Forest* can also be a good depiction of entities and surroundings
    in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two types attention mechanisms. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Hard attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soft attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now take a look at each one in detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Hard Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In reality, in our recent image caption example, several more pictures would
    be selected, but due to our training with the handwritten captions, those would
    never be weighted higher. However, the essential thing to understand is how the
    system would understand what all pixels (or more precisely, the CNN representations
    of them) the system focuses on to draw these high-resolution images of different
    aspects and then how to choose the next pixel to repeat the process.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, the points are chosen at random from a distribution
    and the process is repeated. Also, which pixels around this point get a higher
    resolution is decided inside the attention network. This type of attention is
    known as **hard attention**.
  prefs: []
  type: TYPE_NORMAL
- en: Hard attention has something called the **differentiability problem**. Let's
    spend some time understanding this. We know that in deep learning the networks
    have to be trained and to train them we iterate across training batches in order
    to minimize the loss function. We can minimize the loss function by changing the
    weights in the direction of the gradient of the minima, which in turn is arrived
    at after differentiating the loss function*.*
  prefs: []
  type: TYPE_NORMAL
- en: This process of minimizing losses across layers of a deep network, starting
    from the last layer to the first, is known as **back-propagation**.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of some differentiable loss functions used in deep learning and machine
    learning are the log-likelihood loss function, squared-error loss function, binomial
    and multinominal cross-entropy, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: However, since the points are chosen randomly in each iteration in hard attention—and
    since such a random pixel choosing mechanism is not a differentiable function—we
    essentially cannot train this attention mechanism, as explained. This problem
    is overcome either by using **Reinforcement Learning** (**RL**) or by switching
    to soft attention.
  prefs: []
  type: TYPE_NORMAL
- en: RL involves mechanisms of solving two problems, either separately or in combination.
    The first is called the **control problem**, which determines the most optimal
    action that the agent should take in each step given its state, and the second
    is the **prediction problem**, which determines the optimal *value* of the state.
  prefs: []
  type: TYPE_NORMAL
- en: Soft Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As introduced in the preceding sub-section on hard attention, soft attention
    uses RL to progressively train and determine where to seek next (control problem).
  prefs: []
  type: TYPE_NORMAL
- en: 'There exist two major problems with using the combination of hard attention
    and RL to achieve the required objective:'
  prefs: []
  type: TYPE_NORMAL
- en: It becomes slightly complicated to involve RL and train an RL agent and an RNN/deep
    network based on it separately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variance in the gradient of the policy function is not only high (as in
    **A3C** model), but also has a computational complexity of *O(N)*, where *N* is
    the number of units in the network. This increases the computation load for such
    approaches massively. Also, given that the attention mechanism adds more value
    in overly long sequences (of words or image embedding splices)—and to train networks
    involving longer sequences requires larger memory, and hence much deeper networks—this
    approach is computationally not very efficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Policy Function **in RL, determined as *Q(a,s)*, is the function used
    to determine the optimal policy or the action *(a)* that should be taken in any
    given state *(s)* to maximize the rewards.
  prefs: []
  type: TYPE_NORMAL
- en: So what is the alternative? As we discussed, the problem arose because the mechanism
    that we were choosing for attention led to a non-differentiable function, because
    of which we had to go with RL. So let's take a different approach here. Taking
    an analogy of our language modeling problem example (as in the A*ttention Mechanism
    - Intuition* section) earlier, we assume that we have the vector of the tokens
    for the objects/ words present in the attention network. Also, in same vector
    space (say in the embedding hyperspace) we bring the tokens for the object/ words
    in the required query of the particular sequence step. On taking this approach,
    finding the right attention weights for the tokens in the attention network with
    the respect to the tokens in query space is as easy as computing the vector similarity
    between them; for example, a cosine distance. Fortunately, most vector distance
    and similarity functions are differentiable; hence the loss function derived by
    using such vector distance/similarity functions in such space is also differentiable,
    and our back-propagation can work in this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: The cosine distance between two vectors, say ![](img/a1f3eff3-207d-4afa-89ea-46df50ec41d7.png),
    and ![](img/4d0ee39a-0f58-4d78-9d0c-9ac88bd3089f.png), in a multi-dimensional
    (three in this example) vector space is given as:![](img/4ae1832d-60bb-4a85-b664-748a097ef5c4.png)
  prefs: []
  type: TYPE_NORMAL
- en: This approach of using a differentiable loss function for training an attention
    network is known as **soft attention**.
  prefs: []
  type: TYPE_NORMAL
- en: Using attention to improve visual models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discovered in the NLP example covered in the earlier section on Attention
    Mechanism - Intuition, Attention did help us a lot in both achieving new use-cases,
    not optimally feasible with conventional NLP, and vastly improving the performance
    of the existing NLP mechanism. Similar is the usage of Attention in CNN and Visual
    Models as well
  prefs: []
  type: TYPE_NORMAL
- en: In the earlier chapter [Chapter 7](20952d99-3977-420f-a5c7-a3320b96bed6.xhtml),
    *Object-Detection & Instance-Segmentation with CNN*, we discovered how Attention
    (like) mechanism are used as Region Proposal Networks for networks like Faster
    R-CNN and Mask R-CNN, to greatly enhance and optimize the proposed regions, and
    enable the generation of segment masks. This corresponds to the first part of
    the discussion. In this section, we will cover the second part of the discussion,
    where we will use 'Attention' mechanism to improve the performance of our CNNs,
    even under extreme conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Reasons for sub-optimal performance of visual CNN models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The performance of a CNN network can be improved to a certain extent by adopting
    proper tuning and setup mechanisms such as: data pre-processing, batch normalization,
    optimal pre-initialization of weights; choosing the correct activation function;
    using techniques such as regularization to avoid overfitting; using an optimal
    optimization function; and training with plenty of (quality) data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond these training and architecture-related decisions, there are image-related
    nuances because of which the performance of visual models may be impacted. Even
    after controlling the aforementioned training and architectural factors, the conventional
    CNN-based image classifier does not work well under some of the following conditions
    related to the underlying images:'
  prefs: []
  type: TYPE_NORMAL
- en: Very big images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly cluttered images with a number of classification entities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very noisy images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's try to understand the reasons behind the sub-optimal performance under
    these conditions, and then we will logically understand what may fix the problem.
  prefs: []
  type: TYPE_NORMAL
- en: In conventional CNN-based models, even after a downsizing across layers, the
    computational complexity is quite high. In fact, the complexity is of the order
    of ![](img/9576a375-e104-439d-8c29-614bc5121d3a.png), where *L* and *W* are the length
    and width of the image in inches, and *PPI* is pixels per inch (pixel density).
    This translates into a linear complexity with respect to the total number of pixels
    (*P*) in the image, or *O(P)*. This directly answers the first point of the challenge;
    for higher *L*, *W*, or *PPI*, we need much higher computational power and time
    to train the network.
  prefs: []
  type: TYPE_NORMAL
- en: Operations such as max-pooling, average-pooling, and so on help downsize the
    computational load drastically vis-a-vis all the computations across all the layers
    performed on the actual image.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we visualize the patterns formed in each of the layers of our CNN, we would
    understand the intuition behind the working of the CNN and why it needs to be
    deep. In each subsequent layer, the CNN trains higher conceptual features, which
    may progressively better help understand the objects in the image layer after
    layer. So, in the case of MNIST, the first layer may only identify boundaries,
    the second the diagonals and straight-line-based shapes of the boundaries, and
    so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84b09c81-669a-48a9-a78a-a9dc717454a5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Illustrative conceptual features formed in different (initial) layers of CNN
    for MNIST
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7917f980-20eb-4bb3-9d7b-8c5add0c914a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'MNIST is a simple dataset, whereas real-life images are quite complex; this
    requires higher conceptual features to distinguish them, and hence more complex
    and much deeper networks. Moreover, in MNIST, we are trying to distinguish between
    similar types of objects (all handwritten numbers). Whereas in real life, the
    objects might differ widely, and hence the different types of features that may
    be required to model all such objects will be very high:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/902073a5-cb0b-4ff7-ac0a-7710d216b9de.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This brings us to our second challenge. A cluttered image with too many objects
    would require a very complex network to model all these objects. Also, since there
    are too many objects to identify, the image resolution needs to be good to correctly
    extract and map the features for each object, which in turn means that the image
    size and the number of pixels need to be high for an effective classification.
    This, in turn, increases the complexity exponentially by combining the first two
    challenges.
  prefs: []
  type: TYPE_NORMAL
- en: The number of layers, and hence the complexity of popular CNN architectures
    used in ImageNet challenges, have been increasing over the years. Some examples
    are VGG16 – Oxford (2014) with 16 layers, GoogLeNet (2014) with 19 layers, and
    ResNet (2015) with 152 layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Not all images are perfect SLR quality. Often, because of low light, image
    processing, low resolution, lack of stabilization, and so on, there may be a lot
    of noise introduced in the image. This is just one form of noise, one that is
    easier to understand. From the perspective of CNN, another form of noise can be
    image transition, rotation, or transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a998d97-d806-4d65-ade6-9cb2aeab0b4e.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Image without noise
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/843cf705-b4bc-4751-b10e-77f64284478f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Same image with added noise
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding images, try reading the newspaper title *Business* in the image
    without and with noise, or identify the mobile in both the images. Difficult to
    do that in the image with noise, right? Similar is the detection/classification
    challenge with our CNN in the case of noisy images.
  prefs: []
  type: TYPE_NORMAL
- en: Even with exhaustive training, perfect hyperparameter adjustment, and techniques
    such as dropouts and others, these real-life challenges continue to diminish the
    image recognition accuracy of CNN networks. Now that we've understood the causes
    and intuition behind the lack of accuracy and performance in our CNNs, let's explore
    some ways and architectures to alleviate these challenges using visual attention.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent models of visual attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Recurrent models of visual attention* can be used to answer some of the challenges
    we covered in the earlier section. These models use the hard attention method,
    as covered in an earlier (*Types of attention*) section. Here we use one of the
    popular variants of recurrent models of visual attention, the **Recurrent Attention
    Model** (**RAM**).'
  prefs: []
  type: TYPE_NORMAL
- en: As covered earlier, hard attention problems are non-differentiable and have
    to use RL for the control problem. The RAM thus uses RL for this optimization.
  prefs: []
  type: TYPE_NORMAL
- en: A recurrent model of visual attention does not process the entire image, or
    even a sliding-window-based bounding box, at once. It mimics the human eye and
    works on the concept of *Fixation* of *Gaze* at different locations of an image;
    with each *Fixation*, it incrementally combines information of importance to dynamically
    build up an internal representation of scenes in the image. It uses an RNN to
    do this in a sequential manner.
  prefs: []
  type: TYPE_NORMAL
- en: The model selects the next location to Fixate to based on the RL agents control
    policy to maximize the reward based on the current state. The current state, in
    turn, is a function of all the past information and the demands of the task. Thus,
    it finds the next coordinate for fixation so that it can maximize the reward (demands
    of the task), given the information collected until now across the previous gazes
    in the memory snapshot of the RNN and the previously visited coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: Most RL mechanisms use the **Markov Decision Process** (**MDP**), in which the
    next action is determined only by the current state, irrespective of the states
    visited earlier. By using RNN here, important information from previous *Fixations*
    can be combined in the present state itself.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding mechanism solves the last two problems highlighted in CNN in the
    earlier section. Also, in the RAM, the number of parameters and amount of computation
    it performs can be controlled independently of the size of the input image, thus
    solving the first problem as well.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the RAM on a noisy MNIST sample
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand the working of the RAM in greater detail, let''s try to create
    an MNIST sample incorporating some of the problems as highlighted in the earlier
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2c083ac-a490-449a-9c63-bf7c251244d2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Larger image of noisy and distorted MNIST
  prefs: []
  type: TYPE_NORMAL
- en: The preceding image represents a larger image/collage using an actual and slightly
    noisy sample of an MNIST image (of number **2**), and a lot of other distortions
    and snippets of other partial samples. Also, the actual digit **2** here is not
    centered. This example represents all the previously stated problems, yet it is
    simple enough to understand the working of the RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RAM uses the concept of a **Glimpse Sensor**. The RL agent fixes its gaze
    at a particular coordinate (*l*) and particular time (*t-1*). The coordinate at
    time t-1, *l[t-1]* of the image *x[t ]*and uses the **Glimpse Sensor** to extract
    retina-like multiple-resolution patches of the image with *l[t-1]* as the center.
    These representations, extracted at time *t-1*, are collectively called *p(x[t]*,
    *l[t-1])*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/619b6dcc-4967-458b-8628-3becc0bafd72.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The concept of the Glimpse Sensor
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a545ac17-e9dd-4116-b679-169f3fb2ec46.jpg)  ;![](img/c44ab12e-cd94-4a50-ae69-1d5876ba0978.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These images show the representations of our image across two fixations using
    the **Glimpse Sensor**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The representations obtained from the **Glimpse Sensor** are passes through
    the ''Glimpse Network, which flattens the representation at two stages. In the
    first stage, the representations from the **Glimpse Sensor** and the **Glimpse
    Network** are flattened separately (![](img/7963a240-8377-44d3-ae47-0ddb6b5cd3e3.png)),
    and then they are combined into a single flattened layer (![](img/f6d21a4f-d1e9-4fbf-a645-7f785778784e.png))
    to generate the output representation *g[t] *for time *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c09406b3-37cb-4929-a9c1-42e08c8742e7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The concept of the Glimpse Network
  prefs: []
  type: TYPE_NORMAL
- en: 'These output representations are then passed through the RNN model architecture.
    The fixation for the next step in the iteration is determined by the RL agent
    to maximize the reward from this architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41cb0399-2275-462d-b820-135b3243e91d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Model architecture (RNN)
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be intuitively understood, the Glimpse Sensor captures important information
    across fixations, which can help identify important concepts. For example, the
    multiple resolution (here 3) representations at the Fixation represented by our
    second sample image have three resolutions as marked (red, green, and blue in
    order of decreasing resolution). As can be seen, even if these are used directly,
    we have got a varying capability to detect the right digit represented by this
    noisy collage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c44ab12e-cd94-4a50-ae69-1d5876ba0978.jpg)![](img/a15e4752-6c1d-4d40-b966-cacf871b6701.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Glimpse Sensor in code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the earlier section, the Glimpse Sensor is a powerful concept.
    Combined with other concepts, such as RNN and RL, as discussed earlier, it is
    at the heart of improving the performance of visual models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see this in greater detail here. The code is commented at every line
    for easy understanding and is self-explanatory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov,
    Richard S. Zemel, Yoshua Bengio, Show, Attend and Tell: *Neural Image Caption
    Generation with Visual Attention*, CoRR, arXiv:1502.03044, 2015.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Karl Moritz Hermann, Tom's Kocisk, Edward Grefenstette, Lasse Espeholt, Will
    Kay, Mustafa Suleyman, Phil Blunsom, *Teaching Machines to Read and Comprehend*,
    CoRR, arXiv:1506.03340, 2015.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu, *Recurrent Models
    of Visual Attention*, CoRR, arXiv:1406.6247, 2014.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Tat-Seng Chua, SCA-CNN: *Spatial
    and Channel-wise Attention in Convolutional Networks for Image Captioning*, CoRR,
    arXiv:1611.05594, 2016.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, Ram Nevatia, ABC-CNN: *An
    Attention Based Convolutional Neural Network for Visual Question Answering*, CoRR,
    arXiv:1511.05960, 2015.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wenpeng Yin, Sebastian Ebert, Hinrich Schutze, *Attention-Based Convolutional
    Neural Network for Machine Comprehension*, CoRR, arXiv:1602.04341, 2016.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wenpeng Yin, Hinrich Schutze, Bing Xiang, Bowen Zhou, ABCNN: *Attention-Based
    Convolutional Neural Network for Modeling Sentence Pairs*, CoRR, arXiv:1512.05193,
    2015.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alexander J. Smola, *Stacked
    Attention Networks for Image Question Answering*, CoRR, arXiv:1511.02274, 2015.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Y. Chen, D. Zhao, L. Lv and C. Li, *A visual attention based convolutional neural
    network for image classification*, *2016 12th World Congress on Intelligent Control
    and Automation (WCICA)*, Guilin, 2016, pp. 764-769.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: H. Zheng, J. Fu, T. Mei and J. Luo, *Learning Multi-attention Convolutional
    Neural Network for Fine-Grained Image Recognition*, *2017 IEEE International Conference
    on Computer Vision (ICCV)*, Venice, 2017, pp. 5219-5227.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, Zheng Zhang, *The
    Application of Two-level Attention Models in Deep Convolutional Neural Network
    for Fine-grained Image Classification*, CoRR, arXiv:1411.6447, 2014.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Jlindsey15, *A TensorFlow implementation of the recurrent attention model*,
    GitHub, [https://github.com/jlindsey15/RAM](https://github.com/jlindsey15/RAM),
    Feb 2018.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: QihongL, *A TensorFlow implementation of the recurrent attention model*, GitHub, [https://github.com/QihongL/RAM](https://github.com/QihongL/RAM),
    Feb 2018.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amasky, *Recurrent Attention Model*, GitHub, [https://github.com/amasky/ram](https://github.com/amasky/ram),
    Feb 2018.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The attention mechanism is the hottest topic in deep learning today and is conceived
    to be in the center of most of the cutting-edge algorithms under current research,
    and in probable future applications. Problems such as image captioning, visual
    question answering, and many more have gotten great solutions by using this approach.
    In fact, attention is not limited to visual tasks and was conceived earlier for
    problems such as neural machine translations and other sophisticated NLP problems.
    Thus, understanding the attention mechanism is vital to mastering many advanced
    deep learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are used not only for vision but also for many good applications with attention
    for solving complex NLP problems, such as **modeling sentence pairs and machine
    translation**. This chapter covered the attention mechanism and its application
    to some NLP problems, along with image captioning and recurrent vision models.
    In RAMs, we did not use CNN; instead, we applied RNN and attention to reduced-size
    representations of an image from the Glimpse Sensor. But there are recent works
    to apply attention to CNN-based visual models as well.
  prefs: []
  type: TYPE_NORMAL
- en: Readers are highly encouraged to go through the original papers in the references
    and also explore advanced concepts in using attention, such as multi-level attention,
    stacked attention models, and the use of RL models (such as the **Asynchronous
    Advantage Actor-Critic** (**A3C**) model for the hard attention control problem).
  prefs: []
  type: TYPE_NORMAL
