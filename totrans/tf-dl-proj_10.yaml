- en: Video Games by Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Contrary to supervised learning, where an algorithm has to associate an input
    with an output, in reinforcement learning you have another kind of maximization
    task. You are given an environment (that is, a situation) and you are required
    to find a solution that will act (something that may require to interact with
    or even change the environment itself) with the clear purpose of maximizing a
    resulting reward. Reinforcement learning algorithms, then, are not given any clear,
    explicit goal but to get the maximum result possible in the end. They are free
    to find the way to achieve the result by trial and error. This resembles the experience
    of a toddler who experiments freely in a new environment and analyzes the feedback
    in order to find out how to get the best from their experience. It also resembles
    the experience we have with a new video game: first, we look for the best winning
    strategy; we try a lot of different things and then we decide how to act in the
    game.'
  prefs: []
  type: TYPE_NORMAL
- en: At the present time, no reinforcement learning algorithm has the general learning
    capabilities of a human being. A human being learns more quickly from several
    inputs, and a human can learn how to behave in very complex, varied, structured,
    unstructured and multiple environments. However, reinforcement learning algorithms
    have proved able to achieve super-human capabilities (yes, they can be better
    than a human) in very specific tasks. A reinforcement learning algorithm can achieve
    brilliant results if specialized on a specific game and if given enough time to
    learn (an example is AlphaGo [https://deepmind.com/research/alphago/](https://deepmind.com/research/alphago/)—
    the first computer program to defeat a world champion at Go, a complex game requiring
    long-term strategy and intuition).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to provide you with the challenging project of
    getting a reinforcement learning algorithm to learn how to successfully manage
    the commands of the Atari game Lunar Lander, backed up by deep learning. Lunar
    Lander is the ideal game for this project because reinforcement learning algorithm
    can work successfully on it, the game has few commands and it can be successfully
    completed just by looking at few values describing the situation in the game (there
    is no need even to look at the screen in order to understand what to do, in fact,
    the first version of the game dates back to the 1960s and it was textual).
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks and reinforcement learning are not new to each other;  in the
    early 1990s, at IBM, Gerry Tesauro programmed the famous TD-Gammon, combining
    feedforward neural networks with temporal-difference learning (a combination of
    Monte Carlo and dynamic programming) in order to train TD-Gammon to play world-class
    backgammon, which a board game for two players to be played using a couple of
    dices. If curious about the game,  you can read everything about the rules from
    the US Backgammon Federation: [http://usbgf.org/learn-backgammon/backgammon-rules-and-terms/rules-of-backgammon/](http://usbgf.org/learn-backgammon/backgammon-rules-and-terms/rules-of-backgammon/).
    At the time, the approach worked well with backgammon, due to the role of dices
    in the game that made it a non-deterministic game. Yet, it failed with every other
    game problem which was more deterministic. The last few years, thanks to the Google
    deep learning team of researchers, proved that neural networks can help solve
    problems other than backgammon, and that problem solving can be achieved on anyone''s
    computer. Now, reinforcement learning is at the top of the list of next big things
    in deep learning and machine learning as you can read from Ian Goodfellow, an
    AI research scientist at Google Brain, who is putting it top of the list: [https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#6a8f8cd81002](https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#6a8f8cd81002).'
  prefs: []
  type: TYPE_NORMAL
- en: The game legacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lunar Lander is an arcade game developed by Atari that first appeared in video
    game arcades around 1979\. Developed in black and white vector graphics and distributed
    in specially devised cabinets, the game showed, as a lateral view, a lunar landing
    pod approaching the moon, where there were special areas for landing. The landing
    areas varied in width and accessibility because of the terrain around them, which
    gave the user different scores when the lander landed. The player was provided
    with information about altitude, speed, amount of fuel available, score, and time
    taken so far. Given the force of gravity attracting the landing pod to the ground,
    the player could rotate or thrust (there were also inertial forces to be considered)
    the landing pod at the expense of some fuel. The fuel was the key to the game.
  prefs: []
  type: TYPE_NORMAL
- en: The game ended when the landing pod touched the moon after running out of fuel.
    Until the fuel ran out, you kept on playing, even if you crashed. The commands
    available to the player were just four buttons, two for rotating left and right;
    one for thrusting from the base of the landing pod, pushing the module in the
    direction it is orientated; and the last button was for aborting the landing by
    rotating the landing pod upright and using a powerful (and fuel consuming) thrust
    in order to prevent the landing pod from crashing.
  prefs: []
  type: TYPE_NORMAL
- en: The interesting aspect of such a game is that there are clearly costs and rewards,
    but some are immediately apparent (like the quantity of fuel you are spending
    in your attempt) and others that they are all delayed until the time the landing
    pod touches the soil (you will know if the landing was a successful one only once
    it comes to a full stop). Maneuvering to land costs fuel, and that requires an
    economic approach to the game, trying not to waste too much. Landing provides
    a score. The more difficult and the safer the landing, the higher the score.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI version
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As stated in the documentation available at its website ([https://gym.openai.com/](https://gym.openai.com/)),
    OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.
    The toolkit actually consists of a Python package that runs with both Python 2
    and Python 3, and the website API, which is useful for uploading your own algorithm's
    performance results and comparing them with others (an aspect of the toolkit that
    we won't be exploring, actually).
  prefs: []
  type: TYPE_NORMAL
- en: 'The toolkit embodies the principles of reinforcement learning, where you have
    an environment and an agent: the agent can perform actions or inaction in the
    environment, and the environment will reply with a new state (representing the
    situation in the environment) and a reward, which is a score telling the agent
    if it is doing well or not. The Gym toolkit provides everything with the environment,
    therefore it is you that has to code the agent with an algorithm that helps the
    agent to face the environment. The environment is dealt by `env`, a class with
    methods for reinforcement learning which is instantiated when you issue the command
    to create it for a specific game: `gym.make(''environment'')`. Let''s examine
    an example from the official documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the run environment is `CartPole-v0`. Mainly a control problem,
    in the `CartPole-v0` game, a pendulum is attached to a cart that moves along a
    friction less track. The purpose of the game is to keep the pendulum upright as
    long as possible by applying forward or backward forces to the cart, and you can
    look at the dynamics of the game by watching this sequence on YouTube, which is
    part of a real-life experiment held at the Dynamics and Control Lab, IIT Madras
    and based on Neuron-like adaptive elements that can solve difficult control problems: [https://www.youtube.com/watch?v=qMlcsc43-lg](https://www.youtube.com/watch?v=qMlcsc43-lg).
  prefs: []
  type: TYPE_NORMAL
- en: The Cartpole problem is described in *Neuron like adaptive elements that can
    solve difficult learning control problems* ([http://ieeexplore.ieee.org/document/6313077/](http://ieeexplore.ieee.org/document/6313077/))
    by BARTO, Andrew G.; SUTTON, Richard S.; ANDERSON, Charles W. in IEEE transactions
    on systems, man, and Cybernetics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a brief explanation of the env methods, as applied in the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`reset()`: This resets the environment''s state to the initial default conditions.
    It actually returns the start observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`step(action)`: This moves the environment by a single time step. It returns
    a four-valued vector made of variables: `observations`, `reward`, `done`, and
    `info`. Observations are a representation of the state of the environment and
    it is represented in each game by a different vector of values. For instance,
    in a game involving physics such as `CartPole-v0`, the returned vector is composed
    of the cart''s position, the cart''s velocity, the pole''s angle, and the pole''s
    velocity. The reward is simply the score achieved by the previous action (you
    need to total the rewards in order to figure out the total score at each point).
    The variable `done` is a Boolean flag telling you whether you are at a terminal
    state in the game (game over). `info` will provide diagnostic information, something
    that you are expected not to use for your algorithm, but just for debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`render( mode=''human'', close=False)`: This renders one time frame of the
    environment. The default mode will do something human-friendly, such as popping
    up a window. Passing the `close` flag signals the rendering engine to close any
    such windows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The resulting effect of the commands is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up the `CartPole-v0` environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run it for 1,000 steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly choose whether to apply a positive or negative force to the cart
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The interesting aspect of this approach is that you can change the game easily,
    just by providing a different string to the `gym.make` method (try for instance
    `MsPacman-v0` or `Breakout-v0` or choose any from the list you can obtain by `gym.print(envs.registry.all())`)
    and test your approach to solving different environments without changing anything
    in your code. OpenAI Gym makes it easy to test the generalization of your algorithm
    to different problems by using a common interface for all its environments. Moreover,
    it provides a framework for your reasoning, understanding and solving of agent-environment
    problems according to the schema. At time *t-1* a state and reward are pushed
    to an agent, and the agent reacts with an action, producing a new state and a
    new reward at time *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15d89b8b-a5c6-482b-8d00-d4e2f6b8b81e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: How the environment and agent interact by means of state, action,
    and reward'
  prefs: []
  type: TYPE_NORMAL
- en: 'In every distinct game in OpenAI Gym, both the action space (the commands the
    agent responds to) and the `observation_space` (the representation state) change.
    You can see how they have changed by using some `print` commands, just after you
    set up an environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Installing OpenAI on Linux (Ubuntu 14.04 or 16.04)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We suggest installing the environment on an Ubuntu system. OpenGym AI has been
    created for Linux systems and there is little support for Windows. Depending on
    the previous settings of your system, you may need to install some additional
    things first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We suggest of working with Anaconda, so install Anaconda 3, too. You can find
    everything about installing this Python distribution at [https://www.anaconda.com/download/](https://www.anaconda.com/download/).
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting the system requirements, installing OpenGym AI with all its modules
    is quite straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For this project, we are actually interested in working with the Box2D module,
    which is a 2D physics engine providing a rendering of real-world physics in a
    2D environment, as commonly seen in pseudo-realistic video games. You can test
    that the Box2D module works by running these commands in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If the provided code runs with no problem, you can proceed with the project.
    In some situations, Box2D may become difficult to run and, for instance, there
    could be problems such as those reported in [https://github.com/cbfinn/gps/issues/34](https://github.com/cbfinn/gps/issues/34),
    though there are many other examples around. We have found that installing the
    Gym in a `conda` environment based on Python 3.4 makes things much easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This installation sequence should allow you to create a `conda` environment
    that's appropriate for the project we are going to present in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Lunar Lander in OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LunarLander-v2 is a scenario developed by Oleg Klimov, an engineer at OpenAI,
    inspired by the original Atari Lunar Lander ([https://github.com/olegklimov](https://github.com/olegklimov)).
    In the implementation, you have to take your landing pod to a lunar pad that is
    always located at coordinates *x=0* and *y=0*. In addition, your actual *x* and
    *y* position is known since their values are stored in the first two elements
    of the state vector, the vector that contains all the information for the reinforcement
    learning algorithm to decide the best action to take at a certain moment.
  prefs: []
  type: TYPE_NORMAL
- en: This renders the task accessible because you won't have to deal with fuzzy or
    uncertain localization of your position with respect to the objective (a common
    problem in robotics).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a99d1857-625c-476b-8829-5605e0eded24.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2: LunarLander-v2 in action
  prefs: []
  type: TYPE_NORMAL
- en: 'At each moment, the landing pod has four possible actions to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: Do nothing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotate left
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotate right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thrust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is then a complex system of reward to make things interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: Reward for moving from the top of the screen to the landing pad and reaching
    zero speed ranges from 100 to 140 points (landing outside the landing pad is possible)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the landing pod moves away from landing pad without coming to a stop, it
    loses some of the previous rewards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each episode (the term used to point out a game session) completes when the
    landing pod crashes or it comes to rest, respectively providing additional -100
    or +100 points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each leg in contact with the ground is +10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Firing the main engine is -0.3 points per frame (but fuel is infinite)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving the episode grants 200 points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The game works perfectly with discrete commands (they are practically binary:
    full thrust or no thrust) because, as the author of the simulation says, according
    to Pontryagin''s maximum principle it''s optimal to fire the engine at full throttle
    or completely turn it off.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The game is also solvable using some simple heuristics based on the distance
    to the target and using a **proportional integral derivative** (**PID**) controller
    to manage the descending speed and angle. A PID is an engineering solution to
    control systems where you have feedback. At the following URL, you can get a more
    detailed explanation of how they work: [https://www.csimn.com/CSI_pages/PIDforDummies.html](https://www.csimn.com/CSI_pages/PIDforDummies.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring reinforcement learning through deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this project, we are not interested in developing a heuristic (a still valid
    approach to solving many problems in artificial intelligence) or constructing
    a working PID. We intend instead to use deep learning to provide an agent with
    the necessary intelligence to operate a Lunar Lander video game session successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reinforcement learning theory offers a few frameworks to solve such problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Value-based learning**: This works by figuring out the reward or outcome
    from being in a certain state. By comparing the reward of different possible states,
    the action leading to the best state is chosen. Q-learning is an example of this
    approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy-based learning**: Different control policies are evaluated based on
    the reward from the environment. It is decided upon the policy achieving the best
    results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model-based learning**: Here, the idea is to replicate a model of the environment
    inside the agent, thus allowing the agent to simulate different actions and their
    consequent reward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our project, we will use a value-based learning framework; specifically,
    we will use the now classical approach in reinforcement learning based on Q-learning,
    which has been successfully controlled games where an agent has to decide on a
    series of moves that will lead to a delayed reward later in the game. Devised
    by C.J.C.H. Watkins in 1989 in his Ph.D. thesis, the method, also called **Q-learning**,
    is based on the idea that an agent operates in an environment, taking into account
    the present state, in order to define a sequence of actions that will lead to
    an ultimate reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99587134-1e20-4285-b204-3cff85be23f7.png)'
  prefs: []
  type: TYPE_IMG
- en: In the above formula, it is described how a state *s*, after an action *a*,
    leads to a reward, *r*, and a new state *s'*. Starting from the initial state
    of a game, the formula applies a series of actions that, one after the other,
    transforms each subsequent state until the end of the game. You can then imagine
    a game as a series of chained states by a sequence of actions. You can then also
    interpret the above formula how an initial state *s* is transformed into a final
    state *s'* and a final reward *r* by a sequence of actions *a*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In reinforcement terms, a **policy** is how to best choose our sequence of
    actions, *a*.  A policy can be approximated by a function, which is called *Q*,
    so that given the present state, *s*, and a possible action, *a*, as inputs, it
    will provide an estimate of the maximum reward, *r*, that will derive from that
    action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7fd4657-9ec3-4acf-94ae-c06421c76173.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This approach is clearly greedy, meaning that we just choose the best action
    at a precise state because we expect that always choosing the best action at each
    step will lead us to the best outcome. Thus, in the greedy approach, we do not
    consider the possible chain of actions leading to the reward, but just the next
    action, *a*. However, it can be easily proved that we can confidently adopt a
    greedy approach and obtain the maximum reward using such a policy if such conditions
    are met:'
  prefs: []
  type: TYPE_NORMAL
- en: we find the perfect policy oracle, *Q**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we operate in an environment where information is perfect (meaning we can know
    everything about the environment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the environment adheres to the **Markov principle (see the tip box)**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the Markov principle states that the future (states, rewards) only depends on
    the present and not the past, therefore we can simply derive the best to be done
    by looking at the present state and ignoring what has previously happened.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, if we build the *Q* function as a recursive function, we just need
    to explore (using a breadth-first search approach) the ramifications to the present
    state of our action to be tested, and the recursive function will return the maximum
    reward possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such an approach works perfectly in a computer simulation, but it makes little
    sense in the real world:'
  prefs: []
  type: TYPE_NORMAL
- en: Environments are mostly probabilistic. Even if you perform an action, you don't
    have the certainty of the exact reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environments are tied to the past, the present alone cannot describe what could
    be the future because the past can have hidden or long-term consequences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environments are not exactly predictable, so you cannot know in advance the
    rewards from an action, but you can know them afterward (this is called an **a
    posteriori** condition).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environments are very complex. You cannot figure out in a reasonable time all
    the possible consequences of an action, hence you cannot figure out with certainty
    the maximum reward deriving from an action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution is then to adopt an approximate *Q* function, one that can take
    into account probabilistic outcomes and that doesn't need to explore all the future
    states by prediction. Clearly, it should be a real approximation function, because
    building a search table of values is unpractical in complex environments (some
    state spaces could take continuous values, making the possible combinations infinite).
    Moreover, the function can be learned offline, which implies leveraging the experience
    of the agent (the ability to memorize becomes then quite important).
  prefs: []
  type: TYPE_NORMAL
- en: There have been previous attempts to approximate a *Q* function by a neural
    network, but the only successful application has been `TD_Gammon`, a backgammon
    program that learned to play by reinforcement learning powered by a multi-layer
    perceptron only. `TD_Gammon` achieved a superhuman level of play, but at the time
    its success couldn't be replicated in different games, such as chess or go.
  prefs: []
  type: TYPE_NORMAL
- en: That led to the belief that neural networks were not really suitable for figuring
    out a *Q* function unless the game was somehow stochastic (you have to throw a
    dice in backgammon). In 2013, a paper on deep reinforcement learning, *Playing
    Atari with deep reinforcement learning(*[https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)*)*
    Volodymyr Minh, et al, applied to old Atari games demonstrated the contrary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such paper demonstrates how a *Q* function could be learned using neural networks
    to play a range of Atari arcade games (such as Beam Rider, Breakout, Enduro, Pong,
    Q*bert, Seaquest, and Space Invaders) just by processing video inputs (by sampling
    frames from a 210 x 160 RGB video at 60 Hz) and outputting joystick and fire button
    commands. The paper names the method a **Deep Q-Network** (**DQN**), and it also introduces
    the concepts of experience replay and exploration versus exploitation, which we
    are going to discuss in the next section. These concepts help to overcome some
    critical problems when trying to apply deep learning to reinforcement learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Lack of plenty of examples to learn from—something necessary in reinforcement
    learning and even more indispensable when using deep learning for it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extended delay between an action and the effective reward, which requires dealing
    with sequences of further actions of variable length before getting a reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Series of highly correlated sequences of actions (because an action often influences
    the following ones), which may cause any stochastic gradient descent algorithm
    to overfit to the most recent examples or simply converge non-optimally (stochastic
    gradient descent expects random examples, not correlated ones)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper, *Human-level control through deep reinforcement learning* ([http://www.davidqiu.com:8888/research/nature14236.pdf](http://www.davidqiu.com:8888/research/nature14236.pdf)),
    by Mnih and other researchers just confirms DQN efficacy where more games are
    explored by using it and performances of DQN are compared to human players and
    classical algorithms in reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: In many games, DQN proved better than human skills, though the long-term strategy
    is still a problem for the algorithm. In certain games, such as *Breakout*, the
    agent discovers cunning strategies such as digging a tunnel through the wall in
    order to send the ball through and destroy the wall in an effortless manner. In
    other games, such as *Montezuma's Revenge*, the agent remains clueless.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, the authors discuss at length how the agent understands the nuts
    and bolts of winning a Breakout game and they provide a chart of the response
    of the DQN function demonstrating how high reward scores are assigned to behaviors that
    first dig a hole in the wall and then let the ball pass through it.
  prefs: []
  type: TYPE_NORMAL
- en: Tricks and tips for deep Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Q-learning obtained by neural networks was deemed unstable until some tricks
    made it possible and feasible. There are two power-horses in deep Q-learning,
    though other variants of the algorithm have been developed recently in order to
    solve problems with performance and convergence in the original solution. Such
    new variants are not discussed in our project: double Q-learning, delayed Q-learning,
    greedy GQ, and speedy Q-learning. The two main DQN power-horses that we are going
    to explore are **experience replay** and the decreasing trade-off between **exploration
    and exploitation**.'
  prefs: []
  type: TYPE_NORMAL
- en: With experience replay, we simply store away the observed states of the game
    in a queue of a prefixed size since we discard older sequences when the queue
    is full. Contained in the stored data, we expect to have a number of tuples consisting
    of the present state, the applied action, the consequently obtained state, and
    the reward gained. If we consider a simpler tuple made of just the present state
    and the action, we have the observation of the agent operating in the environment,
    which we can consider the root cause of the consequent state and of the reward.
    We can consider now the tuple (present state and action) as our predictor ( *x*
    vector) with respect to the reward. Consequently, we can use the reward directly
    connected to the action and the reward that will be achieved at the end of the
    game.
  prefs: []
  type: TYPE_NORMAL
- en: Given such stored data (which we can figure out as the memory of our agent),
    we sample a few of them in order to create a batch and use the obtained batch
    to train our neural network. However, before passing the data to the network,
    we need to define our target variable, our *y* vector. Since the sampled states
    mostly won't be the final ones, we will probably have a zero reward or simply
    a partial reward to match against the known inputs (the present state and the
    chosen action). A partial reward is not very useful because it just tells part
    of the story we need to know. Our objective is, in fact, to know the total reward
    we will get at the end of the game, after having taken the action from the present
    state we are evaluating (our *x* value).
  prefs: []
  type: TYPE_NORMAL
- en: In this case, since we don't have such information, we simply try to approximate
    the value by using our existing *Q* function in order to estimate the residual
    reward that will be the maximum consequence of the (state, action) tuple we are
    considering. After obtaining it, we discount its value using the Bellman equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read an explanation of this now classic approach in reinforcement learning
    in this excellent tutorial by Dr. Sal Candido, a software engineer at Google:
    [http://robotics.ai.uiuc.edu/~scandido/?Developing_Reinforcement_Learning_from_the_Bellman_Equation](http://robotics.ai.uiuc.edu/~scandido/?Developing_Reinforcement_Learning_from_the_Bellman_Equation)),
    where the present reward is added to the discounted future reward.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a small value (approaching zero) for discounting makes the *Q* function
    more geared toward short-term rewards, whereas using a high discount value (approaching
    one) renders the *Q* function more oriented to future gains.
  prefs: []
  type: TYPE_NORMAL
- en: The second very effective trick is using a coefficient for trading between exploration
    and exploitation. In exploration, the agent is expected to try different actions
    in order to find the best course of action given a certain state. In exploitation,
    the agent leverages what it learned in the previous explorations and simply decides
    for what it believes the best action to be taken in that situation.
  prefs: []
  type: TYPE_NORMAL
- en: Finding a good balance between exploration and exploitation is strictly connected
    to the usage of the experience replay we discussed earlier. At the start of the
    DQN algorithm optimization, we just have to rely on a random set of network parameters.
    This is just like sampling random actions, as we did in our simple introductory
    example to this chapter. The agent in such a situation will explore different
    states and actions, and help to shape the initial *Q* function. For complex games
    such as *Lunar Lander* using random choices won't take the agent far, and it could
    even turn unproductive in the long run because it will prevent the agent from
    learning the expected reward for tuples of state and action that can only be accessed
    if the agent has done the correct things before. In fact, in such a situation
    the DQN algorithm will have a hard time figuring out how to appropriately assign
    the right reward to an action because it will never have seen a completed game. Since the
    game is complex, it is unlikely that it could be solved by random sequences of
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: The correct approach, then, is to balance learning by chance and using what
    has been learned to take the agent further in the game to where problems are yet
    to be solved. This resembles finding a solution by a series of successive approximations,
    by taking the agent each time a bit nearer to the correct sequence of actions
    for a safe and successful landing. Consequently, the agent should first learn
    by chance, find the best things to be done in a certain set of situations, then
    apply what has been learned and get access to new situations that, by random choice,
    will be also solved, learned, and applied successively.
  prefs: []
  type: TYPE_NORMAL
- en: This is done using a decreasing value as the threshold for the agent to decide
    whether, at a certain point in the game, to take a random choice and see what
    happens or leverage what it has learned so far and use it to make the best possible
    action at that point, given its actual capabilities. Picking a random number from
    a uniform distribution [*0*,*1*], the agent compares it with an epsilon value,
    and if the random number is larger than the epsilon it will use its approximate
    neural *Q* function. Otherwise, it will pick a random action from the options
    available. After that, it will decrease the epsilon number. Initially, epsilon
    is set at the maximum value, *1.0*, but depending on a decaying factor, it will
    decrease with time more or less rapidly, arriving at a minimum value that should
    never be zero (no chance of taking a random move) in order for there to always
    be the possibility of learning something new and unexpected (a minimal openness
    factor) by serendipity.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the limitations of deep Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even with deep Q-learning, there are some limitations, no matter whether you
    approximate your *Q* function by deriving it from visual images or other observations
    about the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The approximation takes a long time to converge, and sometimes it doesn''t
    achieve it smoothly: you may even witness the learning indicators of the neural
    network worsening instead of getting better for many epochs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Being based on a greedy approach, the approach offered by Q-learning is not
    dissimilar from a heuristic: it points out the best direction but it cannot provide
    detailed planning. When dealing with long-term goals or goals that have to be
    articulated into sub-goals, Q-learning performs badly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another consequence of how Q-learning works is that it really doesn't understand
    the game dynamics from a general point of view but from a specific one (it replicates
    what it experienced as effective during training). As a consequence, any novelty
    introduced into the game (and never actually experienced during training) can
    break down the algorithm and render it completely ineffective. The same goes when
    introducing a new game to the algorithm; it simply won't perform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting the project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After this long detour into reinforcement learning and the DQN approach, we
    are finally ready to start coding, having all the basic understanding of how to
    operate an OpenAI Gym environment and how to set a DQN approximation of a *Q*
    function. We simply start importing all the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `tempfile` module generates temporary files and directories that can be
    used as a temporary storage area for data files. The `deque `command, from the
    `collections` module, creates a double-ended queue, practically a list where you
    can append items at the start or at the end. Interestingly, it can be set to a
    predefined size. When full, older items are discarded in order to make the place
    for new entries.
  prefs: []
  type: TYPE_NORMAL
- en: We will structure this project using a series of classes representing the agent,
    the agent's brain (our DQN), the agent's memory, and the environment, which is
    provided by OpenAI Gym but it needs to be correctly connected to the agent. It
    is necessary to code a class for this.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the AI brain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in the project is to create a `Brain` class containing all the
    neural network code in order to compute a Q-value approximation. The class will
    contain the necessary initialization, the code for creating a suitable TensorFlow
    graph for the purpose, a simple neural network (not a complex deep learning architecture
    but a simple, working network for our project—you can replace it with more complex
    architectures), and finally, methods for fit and predict operations.
  prefs: []
  type: TYPE_NORMAL
- en: We start from initialization. As inputs, first, we really need to know the size
    of the state inputs (`nS`) corresponding to the information we receive from the
    game, and the size of the action output (`nA`) corresponding to the buttons we
    can press to perform actions in the game. Optionally, but strongly recommended,
    we also have to set the scope. In order to define the scope a string will help
    us to keep separate networks created for different purposes, and in our project,
    we have two, one for processing the next reward and one for guessing the final
    reward.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we have to define the learning rate for the optimizer, which is an Adam.
  prefs: []
  type: TYPE_NORMAL
- en: The Adam optimizer is described in the following paper: [https://arxiv.org/abs/1412.6980.](https://arxiv.org/abs/1412.6980)It
    is a very efficient gradient-based optimization method that requires very little
    to be tuned in order to work properly. The Adam optimization is a stochastic gradient
    descent algorithm similar to RMSprop with Momentum. This post, [https://theberkeleyview.wordpress.com/2015/11/19/berkeleyview-for-adam-a-method-for-stochastic-optimization/](https://theberkeleyview.wordpress.com/2015/11/19/berkeleyview-for-adam-a-method-for-stochastic-optimization/),
    from the UC Berkeley Computer Vision Review Letters, provides more information.
    From our experience, it is one of the most effective solutions when training a
    deep learning algorithm in batches, and it requires some tuning for the learning
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we also provide:'
  prefs: []
  type: TYPE_NORMAL
- en: A neural architecture (if we prefer to change the basic one provided with the
    class)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input the `global_step`, a global variable that will keep track of the number
    of training batches of examples that have been feed to the DQN network  up to
    that moment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The directory in which to store the logs for TensorBoard, the standard visualization
    tool for TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The command  `tf.summary.FileWriter` initializes an event file in a target directory
    (`summary_dir`) where we store the key measures of the learning process. The handle
    is kept in `self.summary_writer`, which we will be using later for storing the
    measures we are interested in representing during and after the training for monitoring
    and debugging what has been learned.
  prefs: []
  type: TYPE_NORMAL
- en: The next method to be defined is the default neural network that we will be
    using for this project. As input, it takes the input layer and the respective
    size of the hidden layers that we will be using. The input layer  is defined by
    the state that we are using, which could be a vector of measurements, as in our
    case, or an image, as in the original DQN paper)
  prefs: []
  type: TYPE_NORMAL
- en: Such layers are simply defined using the higher level ops offered by the `Layers`
    module of TensorFlow ([https://www.tensorflow.org/api_guides/python/contrib.layers](https://www.tensorflow.org/api_guides/python/contrib.layers)).
    Our choice goes for the vanilla `fully_connected`, using the `ReLU` (rectifier)
    `activation` function for the two hidden layers and the linear activation of the
    output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The predefined size of 32 is perfectly fine for our purposes, but you may increment
    it if you like. Also, there is no dropout in this network. Clearly, the problem
    here is not overfitting, but the quality of what is being learned, which could
    only be improved by providing useful sequences of unrelated states and a good
    estimate of the final reward to be associated. It is in the useful sequences of
    states, especially under the light of the trade-off between exploration and exploitation,
    that the key to not having the network overfit resides. In a reinforcement learning
    problem, you have overfitted if you fall into one of these two situations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sub-optimality: the algorithm suggests sub-optimal solutions, that it is, our
    lander learned a rough way to land and it sticks to it because at least it lands'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Helplessness: the algorithm has fallen into a learned helplessness; that is,
    it has not found a way to land correctly, so it just accepts that it is going
    to crash in the least bad way possible'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two situations can prove really difficult to overcome for a reinforcement
    learning algorithm such as DQN unless the algorithm can have the chance to explore
    alternative solutions during the game. Taking random moves from time to time is
    not simply a way to mess up things, as you may think at first sight, but a strategy
    to avoid pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: With larger networks than this one, on the other hand, you may instead have
    a problem with a dying neuron requiring you to use a different activation, `tf.nn.leaky_relu` ([https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu](https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu)),
    in order to obtain a working network.
  prefs: []
  type: TYPE_NORMAL
- en: A dead `ReLU` ends up always outputting the same value, usually a zero value,
    and it becomes resistant to backpropagation updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The activation `leaky_relu` has been available since TensorFlow 1.4\. If you
    are using any previous version of TensorFlow, you can create an `ad hoc` function
    to be used in your custom network:'
  prefs: []
  type: TYPE_NORMAL
- en: '`def leaky_relu(x, alpha=0.2):       return tf.nn.relu(x) - alpha * tf.nn.relu(-x)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now proceed to code our `Brain` class, adding some more functions to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The method`create_network` combines input, neural network, loss, and optimization.
    The loss is simply created by taking the difference between the original reward
    and the estimated result, squaring it, and taking the average through all the
    examples present in the batch being learned. The loss is minimized using an Adam
    optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, a few summaries are recorded for TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: The average loss of the batch, in order to keep track of the fit during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum predicted reward in the batch, in order to keep track of extreme
    positive predictions, pointing out the best-winning moves
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average predicted reward in the batch, in order to keep track of the general
    tendency of predicting good moves
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for `create_network`, the TensorFlow engine of our project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The class is completed by a `predict` and a `fit` method. The `fit` method takes
    as input the state matrix, `s`, as the input batch and the vector of reward `r`
    as the outcome. It also takes into account how many epochs you want to train (in
    the original papers it is suggested using just a single epoch per batch in order
    to avoid overfitting too much to each batch of observations). Then, in the present
    session, the input is fit with respect to the outcome and summaries (previously
    defined as we created the network).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As a result,  `global step` is returned, which is a counter that helps to keep
    track of the number of examples used in training up so far, and then recorded
    for later use.
  prefs: []
  type: TYPE_NORMAL
- en: Creating memory for experience replay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After defining the brain (the TensorFlow neural network), our next step is to
    define the memory, that is the storage for data that will power the learning process
    of the DQN network. At each training episode each step, made of a state and an
    action, is recorded together with the consequent state and the final reward of
    the episode (something that will be known only when the episode completes).
  prefs: []
  type: TYPE_NORMAL
- en: Adding a flag telling if the observation is a terminal one or not completes
    the set of recorded information. The idea is to connect certain moves not just
    to the immediate reward (which could be null or modest) but the ending reward,
    thus associating every move in that session to it.
  prefs: []
  type: TYPE_NORMAL
- en: The class memory is simply a queue of a certain size, which is then filled with information on
    the previous game experiences, and it is easy to sample and extract from it. Given
    its fixed size, it is important that older examples are pushed out of the queue,
    thus allowing the available examples to always be among the last ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class comprises an initialization, where the data structure takes origin
    and its size is fixed, the `len` method (so we know whether the memory is full
    or not, which is useful, for instance, in order to wait for any training at least
    until we have plenty of them for better randomization and variety for learning),
    `add_memory` for recording in the queue, and `recall_memory` for recovering all
    the data from it in a list format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Creating the agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next class is the agent, which has the role of initializing and maintaining
    the brain (providing the *Q-value* function approximation) and the memory. It
    is the agent, moreover, that acts in the environment. Its initialization sets
    a series of parameters that are mostly fixed given our experience in optimizing
    the learning for the Lunar Lander game. They can be explicitly changed, though,
    when the agent is first initialized:'
  prefs: []
  type: TYPE_NORMAL
- en: '`epsilon = 1.0` is the initial value in the exploration-exploitation parameter.
    The `1.0` value forces the agent to completely rely on exploration, that is, random
    moving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon_min = 0.01` sets the minimum value of the exploration-exploitation
    parameter: a value of `0.01` means that there is a 1% chance that the landing
    pod will move randomly and not based on *Q* function feedback. This always provides
    a minimum chance to find another optimal way of completing the game, without compromising
    it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon_decay = 0.9994` is the decay that regulates the speed the `epsilon`
    diminishes toward the minimum. In this setting, it is tuned to reach a minimum
    value after about 5,000 episodes, which on average should provide the algorithm
    at least 2 million examples to learn from.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma = 0.99` is the reward discount factor with which the Q-value estimation
    weights the future reward with respect to the present reward, thus allowing the
    algorithm to be short- or long-sighted, according to what is best in the kind
    of game being played (in Lunar Lander it is better to be long-sighted because
    the actual reward will be experienced only when the landing pod lands on the Moon).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate = 0.0001` is the learning rate for the Adam optimizer to learn
    the batch of examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs = 1` is the training epochs used by the neural network in order to
    fit the batch set of examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size = 32` is the size of the batch examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory = Memory(memory_size=250000)` is the size of the memory queue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the preset parameters, you are assured that the present project will work.
    For different OpenAI environments, you may need to find different optimal parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The initialization will also provide the commands required to define where the
    TensorBoard logs will be placed (by default, the `experiment` directory), the
    model for learning how to estimate the immediate next reward, and another model
    to store the weights for the final reward. In addition, a saver (`tf.train.Saver`)
    will be initialized, allowing the serialization of the entire session to disk
    in order to restore it later and use it for playing the real game, not just learning
    how to play it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two mentioned models are initialized in the same session, using different
    scope names (one will be `q`, the next reward model monitored by the TensorBoard;
    the other one will be `target_q`). Using two different scope names will allow
    easy handling of the neuron''s coefficients, making it possible to swap them with
    another method present in the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `epsilon` dealing with the share of time devoted exploring new solutions
    compared to exploiting the knowledge of the network is constantly updated with
    the `epsilon_update` method, which simply modifies the actual `epsilon` by multiplying
    it by `epsilon_decay` unless it has already reached its allowed minimum value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `save_weights` and `load_weights` methods simply allow the session to be saved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `set_weights` and  `target_model_update` methods work together to update
    the target Q network with the weights of the Q network (`set_weights` is a general-purpose,
    reusable function you can use in your solutions, too). Since we named the two
    scopes differently, it is easy to enumerate the variables of each network from
    the list of trainable variables. Once enumerated, the variables are joined in
    an assignment to be executed by the running session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `act` method is the core of the policy implementation because it will decide,
    based on `epsilon`, whether to take a random move or go for the best possible
    one. If it is going for the best possible move, it will ask the trained Q network
    to provide a reward estimate for each of the possible next moves (represented
    in a binary way by pushing one of four buttons in the Lunar Lander game) and it
    will return the move characterized by the maximum predicted reward (a greedy approach
    to the solution):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `replay` method completes the class. It is a crucial method because it makes
    learning for the DQN algorithm possible. We are going, therefore, to discuss how
    it works thoroughly. The first thing that the `replay` method does is to sample
    a batch (we defined the batch size at initialization) from the memories of previous
    game episodes (such memories are just the variables containing values about status,
    action, reward, next status, and a flag variable noticing if the observation is
    a final status or not). The random sampling allows the model to find the best
    coefficients in order to learn the *Q* function by a slow adjustment of the network's
    weights, batch after batch.
  prefs: []
  type: TYPE_NORMAL
- en: Then the method finds out whether the sampling recalled statuses are final or
    not. Non-final rewards need to be updated in order to represent the reward that
    you get at the end of the game. This is done by using the target network, which
    represents a snapshot of the *Q* function network as fixed at the end of the previous
    learning. The target network is fed with the following status, and the resulting
    reward is summed, after being discounted by a gamma factor, with the present reward.
  prefs: []
  type: TYPE_NORMAL
- en: Using the present *Q* function may lead to instabilities in the learning process
    and it may not result in a satisfying *Q* function network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: When the rewards of non-terminal states have been updated, the batch data is
    fed into the neural network for training.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last class to be implemented is the `Environment` class. Actually, the
    environment is provided by the `gym` command, though you need a good wrapper around
    it in order to have it work with the previous `agent` class. That''s exactly what
    this class does. At initialization, it starts the Lunar Lander game and sets key
    variables such as `nS`, `nA` (dimensions of state and action), `agent`, and the
    cumulative reward (useful for testing the solution by providing an average of
    the last 100 episodes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Then, we prepare the code for methods for `test`, `train`, and `incremental`
    (incremental training), which are defined as wrappers of the comprehensive `learning`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using incremental training is a bit tricky and it requires some attention if
    you do not want to spoil the results you have obtained with your training so far.
    The trouble is that when we restart the brain has pre-trained coefficients but
    memory is actually empty (we can call this as a cold restart). Being the memory
    of the agent empty, it cannot support good learning because of too few and limited
    examples. Consequently, the quality of the examples being fed is really not perfect
    for learning (the examples are mostly correlated with each other and very specific
    to the few newly experienced episodes). The risk of ruining the training can be
    mitigated using a very low `epsilon` (we suggest set at the minimum, `0.01` ):
    in this way, the network  will most of the time simply re-learn its own weights
    because it will suggest for each state the actions it already knows, and its performance
    shouldn''t worsen but oscillate in a stable way until there are enough examples
    in memory and it will start improving again.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for issuing the correct methods for training and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The final method is `learn`, arranging all the steps for the agent to interact
    with and learn from the environment. The method takes the `epsilon` value (thus
    overriding any previous `epsilon` value the agent had), the number of episodes
    to run in the environment, whether it is being trained or not (a Boolean flag),
    and whether the training is continuing from the training of a previous model (another
    Boolean flag).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first block of code, the method loads the previously trained weights
    of the network for Q value approximation if we want:'
  prefs: []
  type: TYPE_NORMAL
- en: to test the network and see how it works;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: to carry on some previous training using further examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the method delves into a nested iteration. The outside iteration is running
    through the required number of episodes (each episode a Lunar Lander game has
    taken to its conclusion). Whereas the inner iteration is instead running through
    a maximum of 1,000 steps making up an episode.
  prefs: []
  type: TYPE_NORMAL
- en: At each time step in the iteration, the neural network is interrogated on the
    next move. If it is under test, it will always simply provide the answer about
    the next best move. If it is under training, there is some chance, depending on
    the value of `epsilon`, that it won't suggest the best move but it will instead
    propose making a random move.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: After the move, all the information is gathered (initial state, chosen action,
    obtained reward, and consequent state) and saved into memory. At this time frame,
    if the memory is large enough to create a batch for the neural network approximating
    the *Q* function, then a training session is run. When all the time frames of
    the episode have been consumed, the weights of the DQN get stored into another
    network to be used as a stable reference as the DQN network is learning from a
    new episode.
  prefs: []
  type: TYPE_NORMAL
- en: Running the reinforcement learning process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, after all the digression on reinforcement learning and DQN and writing
    down the complete code for the project, you can run it using a script or a Jupyter
    Notebook, leveraging the `Environment` class that puts all the code functionalities
    together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After instantiating it, you just have to run the `train`, starting from `epsilon=1.0`
    and setting the goal to `5000` episodes (which corresponds to about 2.2 million
    examples of chained variables of state, action and reward). The actual code we
    provided is set to successfully accomplish a fully trained DQN model, though it
    may take some time, given your GPU''s availability and its computing capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the end, the class will complete the required training, leaving a saved
    model on disk (which could be run or even reprised anytime). You can even inspect
    the TensorBoard using a simple command that can be run from a shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The plots will appear on your browser, and they will be available for inspection
    at the local address `localhost:6006`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3007e975-dc2b-4ce9-98b9-b9607a870578.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4: The loss trend along the training, the peaks represent break-thoughts
    in learning such as at 800k examples
  prefs: []
  type: TYPE_NORMAL
- en: when it started landing safely on the ground.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss plot will reveal that, contrary to other projects, the optimization
    is still characterized by a decreasing loss, but with many spikes and problems
    along the way:'
  prefs: []
  type: TYPE_NORMAL
- en: The plots represented here are the result of running the project once. Since
    there is a random component in the process, you may obtain slightly different
    plots when running the project on your own computer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29069c1b-c954-4c9b-8c2c-baae261a3d4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The trend of maximum q values obtained in a batch session of learning'
  prefs: []
  type: TYPE_NORMAL
- en: 'The same story is told by the maximum predicted *q* value and the average predicted
    *q* value.  The network improves at the end, though it can slightly retrace its
    steps and linger on plateaus for a long time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a361ba7-4c97-4b4e-b03f-c64b3a51345a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The trend of average q values obtained in a batch session of learning'
  prefs: []
  type: TYPE_NORMAL
- en: 'Only if you take the average of the last 100 final rewards do you see an incremental
    path, hinting at a persistent and steady improvement of the DQN network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ce2623c-3338-47c5-8059-45e78cc942fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The trend of actually obtained scores at the end of each learning
    episode, it more clearly depicts the growing capabilities of the DQN'
  prefs: []
  type: TYPE_NORMAL
- en: Using the same information, from the output, not from the TensorBoard, you'll
    also figure out that the number of actions changes on average depending on the
    `epsilon` value. At the beginning, the number of actions required to finish an
    episode was under 200\. Suddenly, when `epsilon` is `0.5`, the average number
    of actions tends to grow steadily and reach a peak at about 750 (the landing pod
    has learned to counteract gravity by using its rockets).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, the network discovers this is a sub-optimal strategy and when `epsilon`
    turns below `0.3`, the average number of actions for completing an episode drops
    as well. The DQN in this phase is discovering how to successfully land the pod
    in a more efficient way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9948d955-1382-4c2c-a97d-472505d1ca43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The relationship between the epsilon (the exploration/exploitation
    rate) and the efficiency of the DQN network,'
  prefs: []
  type: TYPE_NORMAL
- en: expressed as a number of moves used to complete an episode
  prefs: []
  type: TYPE_NORMAL
- en: 'If for any reason, you believe that the network needs more examples and learning,
    you can reprise the learning using the incremental `method`, keeping in mind that
    `epsilon` should be very low in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After the training, if you need to see the results and know, on average every
    100 episodes, how much the DQN can score (the ideal target is a `score >=200`), 
    you can just run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Acknowledgements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the conclusion of this project, we would like to indeed thank Peter Skvarenina,
    whose project "Lunar Lander II" ([https://www.youtube.com/watch?v=yiAmrZuBaYU](https://www.youtube.com/watch?v=yiAmrZuBaYU))
    has been the key inspiration for our own project, and for all his suggestions
    and hints during the making of our own version of the Deep Q-Network.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this project, we have explored what a reinforcement algorithm can manage
    to achieve in an OpenAI environment, and we have programmed a TensorFlow graph
    capable of learning how to estimate a final reward in an environment characterized
    by an agent, states, actions, and consequent rewards. This approach, called DQN,
    aims to approximate the result from a Bellman equation using a neural network
    approach. The result is a Lunar Lander game that the software can play successfully
    at the end of training by reading the game status and deciding on the right actions
    to be taken at any time.
  prefs: []
  type: TYPE_NORMAL
