- en: Video Games by Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习与视频游戏
- en: 'Contrary to supervised learning, where an algorithm has to associate an input
    with an output, in reinforcement learning you have another kind of maximization
    task. You are given an environment (that is, a situation) and you are required
    to find a solution that will act (something that may require to interact with
    or even change the environment itself) with the clear purpose of maximizing a
    resulting reward. Reinforcement learning algorithms, then, are not given any clear,
    explicit goal but to get the maximum result possible in the end. They are free
    to find the way to achieve the result by trial and error. This resembles the experience
    of a toddler who experiments freely in a new environment and analyzes the feedback
    in order to find out how to get the best from their experience. It also resembles
    the experience we have with a new video game: first, we look for the best winning
    strategy; we try a lot of different things and then we decide how to act in the
    game.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，监督学习要求算法将输入与输出关联起来，而强化学习则是另一种最大化任务。在强化学习中，你会被给定一个环境（即一个情境），并需要找到一个解决方案，进行行动（这可能需要与环境进行交互，甚至改变环境本身），其明确目标是最大化最终的奖励。因此，强化学习算法没有明确的目标，而是致力于获得最终可能的最大结果。它们可以通过反复试验和错误的方式自由地找到实现结果的路径。这类似于幼儿在新环境中自由实验并分析反馈，以便找出如何从经验中获得最佳效果的过程。它也类似于我们玩新视频游戏时的体验：首先，我们寻找最佳的获胜策略；尝试许多不同的方式，然后决定如何在游戏中行动。
- en: At the present time, no reinforcement learning algorithm has the general learning
    capabilities of a human being. A human being learns more quickly from several
    inputs, and a human can learn how to behave in very complex, varied, structured,
    unstructured and multiple environments. However, reinforcement learning algorithms
    have proved able to achieve super-human capabilities (yes, they can be better
    than a human) in very specific tasks. A reinforcement learning algorithm can achieve
    brilliant results if specialized on a specific game and if given enough time to
    learn (an example is AlphaGo [https://deepmind.com/research/alphago/](https://deepmind.com/research/alphago/)—
    the first computer program to defeat a world champion at Go, a complex game requiring
    long-term strategy and intuition).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，没有任何强化学习算法具有人类的通用学习能力。人类能从多种输入中更快速地学习，并且能够在非常复杂、多变、有结构、无结构和多重环境中学习如何行为。然而，强化学习算法已经证明能够在非常具体的任务中达到超越人类的能力（是的，它们可以比人类做得更好）。如果强化学习算法专注于某个特定游戏，并且有足够的时间进行学习，它们可以取得出色的成果（例如
    AlphaGo [https://deepmind.com/research/alphago/](https://deepmind.com/research/alphago/)
    —— 第一个击败围棋世界冠军的计算机程序，围棋是一项复杂的游戏，需要长期的策略和直觉）。
- en: In this chapter, we are going to provide you with the challenging project of
    getting a reinforcement learning algorithm to learn how to successfully manage
    the commands of the Atari game Lunar Lander, backed up by deep learning. Lunar
    Lander is the ideal game for this project because reinforcement learning algorithm
    can work successfully on it, the game has few commands and it can be successfully
    completed just by looking at few values describing the situation in the game (there
    is no need even to look at the screen in order to understand what to do, in fact,
    the first version of the game dates back to the 1960s and it was textual).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将为你提供一个具有挑战性的项目，要求你让强化学习算法学习如何成功地控制 Atari 游戏《月球着陆者》的指令，该算法由深度学习提供支持。《月球着陆者》是这个项目的理想游戏，因为强化学习算法可以成功地在其中工作，游戏命令较少，并且仅通过查看描述游戏情境的几个数值，就能成功完成游戏（实际上，甚至不需要看屏幕来理解该怎么做，事实上，游戏的第一个版本可以追溯到1960年代，它是文字版的）。
- en: 'Neural networks and reinforcement learning are not new to each other;  in the
    early 1990s, at IBM, Gerry Tesauro programmed the famous TD-Gammon, combining
    feedforward neural networks with temporal-difference learning (a combination of
    Monte Carlo and dynamic programming) in order to train TD-Gammon to play world-class
    backgammon, which a board game for two players to be played using a couple of
    dices. If curious about the game,  you can read everything about the rules from
    the US Backgammon Federation: [http://usbgf.org/learn-backgammon/backgammon-rules-and-terms/rules-of-backgammon/](http://usbgf.org/learn-backgammon/backgammon-rules-and-terms/rules-of-backgammon/).
    At the time, the approach worked well with backgammon, due to the role of dices
    in the game that made it a non-deterministic game. Yet, it failed with every other
    game problem which was more deterministic. The last few years, thanks to the Google
    deep learning team of researchers, proved that neural networks can help solve
    problems other than backgammon, and that problem solving can be achieved on anyone''s
    computer. Now, reinforcement learning is at the top of the list of next big things
    in deep learning and machine learning as you can read from Ian Goodfellow, an
    AI research scientist at Google Brain, who is putting it top of the list: [https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#6a8f8cd81002](https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#6a8f8cd81002).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和强化学习彼此并不陌生；在1990年代初期，IBM的Gerry Tesauro编程了著名的TD-Gammon，将前馈神经网络与时间差学习（蒙特卡洛方法和动态规划的结合）结合，训练TD-Gammon玩世界级的西洋双陆棋（一种使用骰子的两人棋盘游戏）。如果你对这款游戏感兴趣，可以通过美国双陆棋协会阅读规则：[http://usbgf.org/learn-backgammon/backgammon-rules-and-terms/rules-of-backgammon/](http://usbgf.org/learn-backgammon/backgammon-rules-and-terms/rules-of-backgammon/)。当时，这种方法在西洋双陆棋中效果很好，因为骰子在游戏中起着非确定性作用。然而，它在其他更具确定性的问题游戏中却失败了。近年来，感谢谷歌深度学习团队的研究人员证明，神经网络可以帮助解决除西洋双陆棋外的其他问题，而且问题解决可以在任何人的计算机上实现。现在，强化学习已成为深度学习和机器学习领域的下一个重要趋势，正如你可以从谷歌大脑的AI研究科学家Ian
    Goodfellow的文章中看到，他把它列为首要事项：[https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#6a8f8cd81002](https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#6a8f8cd81002)。
- en: The game legacy
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏的遗产
- en: Lunar Lander is an arcade game developed by Atari that first appeared in video
    game arcades around 1979\. Developed in black and white vector graphics and distributed
    in specially devised cabinets, the game showed, as a lateral view, a lunar landing
    pod approaching the moon, where there were special areas for landing. The landing
    areas varied in width and accessibility because of the terrain around them, which
    gave the user different scores when the lander landed. The player was provided
    with information about altitude, speed, amount of fuel available, score, and time
    taken so far. Given the force of gravity attracting the landing pod to the ground,
    the player could rotate or thrust (there were also inertial forces to be considered)
    the landing pod at the expense of some fuel. The fuel was the key to the game.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 《月球着陆者》是由Atari公司开发的一款街机游戏，首次出现在大约1979年的电子游戏街机中。该游戏采用黑白矢量图形开发，并通过特制的机柜分发，展示了一个从侧面视角看到的月球着陆舱接近月球的场景，月球上有专门的着陆区域。由于周围地形的原因，着陆区域的宽度和可达性有所不同，因此着陆时会给玩家不同的分数。玩家会得到关于高度、速度、剩余燃料、得分和至今用时的信息。由于重力的作用将着陆舱吸引到地面，玩家可以旋转或推动着陆舱（同时需要考虑惯性力），以消耗部分燃料。燃料是游戏的关键。
- en: The game ended when the landing pod touched the moon after running out of fuel.
    Until the fuel ran out, you kept on playing, even if you crashed. The commands
    available to the player were just four buttons, two for rotating left and right;
    one for thrusting from the base of the landing pod, pushing the module in the
    direction it is orientated; and the last button was for aborting the landing by
    rotating the landing pod upright and using a powerful (and fuel consuming) thrust
    in order to prevent the landing pod from crashing.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏在着陆舱用尽燃料后触碰到月球时结束。直到燃料耗尽，你一直在玩游戏，即使你发生了碰撞。玩家可以使用的指令只有四个按钮：两个用于向左和向右旋转；一个用于从着陆舱底部推力，推动模块朝着其朝向的方向；最后一个按钮用于中止着陆，通过将着陆舱旋转至竖直状态并使用强力（且耗费燃料的）推力，以防止着陆舱坠毁。
- en: The interesting aspect of such a game is that there are clearly costs and rewards,
    but some are immediately apparent (like the quantity of fuel you are spending
    in your attempt) and others that they are all delayed until the time the landing
    pod touches the soil (you will know if the landing was a successful one only once
    it comes to a full stop). Maneuvering to land costs fuel, and that requires an
    economic approach to the game, trying not to waste too much. Landing provides
    a score. The more difficult and the safer the landing, the higher the score.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个游戏的有趣之处在于，尽管存在明显的成本和奖励，但有些是立刻显现的（比如你在尝试过程中消耗的燃料量），而其他则是直到着陆舱触碰到地面时才会出现（只有当着陆舱完全停止后，你才能知道着陆是否成功）。为了着陆而进行的操控消耗燃料，因此需要一种经济的游戏策略，尽量避免浪费过多燃料。着陆会提供一个分数，着陆越困难且越安全，分数就越高。
- en: The OpenAI version
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI 版本
- en: As stated in the documentation available at its website ([https://gym.openai.com/](https://gym.openai.com/)),
    OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.
    The toolkit actually consists of a Python package that runs with both Python 2
    and Python 3, and the website API, which is useful for uploading your own algorithm's
    performance results and comparing them with others (an aspect of the toolkit that
    we won't be exploring, actually).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其官网上的文档所述（[https://gym.openai.com/](https://gym.openai.com/)），OpenAI Gym 是一个用于开发和比较强化学习算法的工具包。这个工具包实际上是一个
    Python 包，支持 Python 2 和 Python 3 运行，还有网站 API，可以上传你自己算法的性能结果，并与其他结果进行比较（这个工具包的一个方面我们并不会探讨）。
- en: 'The toolkit embodies the principles of reinforcement learning, where you have
    an environment and an agent: the agent can perform actions or inaction in the
    environment, and the environment will reply with a new state (representing the
    situation in the environment) and a reward, which is a score telling the agent
    if it is doing well or not. The Gym toolkit provides everything with the environment,
    therefore it is you that has to code the agent with an algorithm that helps the
    agent to face the environment. The environment is dealt by `env`, a class with
    methods for reinforcement learning which is instantiated when you issue the command
    to create it for a specific game: `gym.make(''environment'')`. Let''s examine
    an example from the official documentation:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具包体现了强化学习的基本原理，其中包括一个环境和一个代理：代理可以在环境中执行动作或不动作，环境会返回一个新的状态（表示环境中的情况）和奖励，奖励是一个分数，用来告诉代理它是否做得好。Gym
    工具包提供了环境的一切，因此你需要编写代理的算法，帮助代理应对环境。环境通过 `env` 类来处理，这个类包含强化学习方法，实例化后可用于特定游戏，通过命令
    `gym.make('environment')` 创建。让我们来看一个来自官方文档的示例：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, the run environment is `CartPole-v0`. Mainly a control problem,
    in the `CartPole-v0` game, a pendulum is attached to a cart that moves along a
    friction less track. The purpose of the game is to keep the pendulum upright as
    long as possible by applying forward or backward forces to the cart, and you can
    look at the dynamics of the game by watching this sequence on YouTube, which is
    part of a real-life experiment held at the Dynamics and Control Lab, IIT Madras
    and based on Neuron-like adaptive elements that can solve difficult control problems: [https://www.youtube.com/watch?v=qMlcsc43-lg](https://www.youtube.com/watch?v=qMlcsc43-lg).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，运行环境是 `CartPole-v0`。主要是一个控制问题，在 `CartPole-v0` 游戏中，一个摆锤被固定在一个沿无摩擦轨道移动的小车上。游戏的目的是通过对小车施加前进或后退的力量，使摆锤尽可能保持直立。你可以通过观看这个
    YouTube 视频，了解游戏的动态，该视频是 IIT Madras 动力学与控制实验室的一项实际实验的一部分，并基于“类似神经元的自适应元素能够解决困难的控制问题”：[https://www.youtube.com/watch?v=qMlcsc43-lg](https://www.youtube.com/watch?v=qMlcsc43-lg)。
- en: The Cartpole problem is described in *Neuron like adaptive elements that can
    solve difficult learning control problems* ([http://ieeexplore.ieee.org/document/6313077/](http://ieeexplore.ieee.org/document/6313077/))
    by BARTO, Andrew G.; SUTTON, Richard S.; ANDERSON, Charles W. in IEEE transactions
    on systems, man, and Cybernetics.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Cartpole 问题在*类似神经元的自适应元素能够解决困难的学习控制问题*（[http://ieeexplore.ieee.org/document/6313077/](http://ieeexplore.ieee.org/document/6313077/)）一文中由BARTO,
    Andrew G.; SUTTON, Richard S.; ANDERSON, Charles W. 在IEEE Transactions on Systems,
    Man, and Cybernetics 中描述。
- en: 'Here is a brief explanation of the env methods, as applied in the example:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是应用于示例中的 env 方法的简要说明：
- en: '`reset()`: This resets the environment''s state to the initial default conditions.
    It actually returns the start observations.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`step(action)`: This moves the environment by a single time step. It returns
    a four-valued vector made of variables: `observations`, `reward`, `done`, and
    `info`. Observations are a representation of the state of the environment and
    it is represented in each game by a different vector of values. For instance,
    in a game involving physics such as `CartPole-v0`, the returned vector is composed
    of the cart''s position, the cart''s velocity, the pole''s angle, and the pole''s
    velocity. The reward is simply the score achieved by the previous action (you
    need to total the rewards in order to figure out the total score at each point).
    The variable `done` is a Boolean flag telling you whether you are at a terminal
    state in the game (game over). `info` will provide diagnostic information, something
    that you are expected not to use for your algorithm, but just for debugging.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`render( mode=''human'', close=False)`: This renders one time frame of the
    environment. The default mode will do something human-friendly, such as popping
    up a window. Passing the `close` flag signals the rendering engine to close any
    such windows.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The resulting effect of the commands is as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Set up the `CartPole-v0` environment
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run it for 1,000 steps
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly choose whether to apply a positive or negative force to the cart
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize the results
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The interesting aspect of this approach is that you can change the game easily,
    just by providing a different string to the `gym.make` method (try for instance
    `MsPacman-v0` or `Breakout-v0` or choose any from the list you can obtain by `gym.print(envs.registry.all())`)
    and test your approach to solving different environments without changing anything
    in your code. OpenAI Gym makes it easy to test the generalization of your algorithm
    to different problems by using a common interface for all its environments. Moreover,
    it provides a framework for your reasoning, understanding and solving of agent-environment
    problems according to the schema. At time *t-1* a state and reward are pushed
    to an agent, and the agent reacts with an action, producing a new state and a
    new reward at time *t*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15d89b8b-a5c6-482b-8d00-d4e2f6b8b81e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: How the environment and agent interact by means of state, action,
    and reward'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'In every distinct game in OpenAI Gym, both the action space (the commands the
    agent responds to) and the `observation_space` (the representation state) change.
    You can see how they have changed by using some `print` commands, just after you
    set up an environment:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Installing OpenAI on Linux (Ubuntu 14.04 or 16.04)
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We suggest installing the environment on an Ubuntu system. OpenGym AI has been
    created for Linux systems and there is little support for Windows. Depending on
    the previous settings of your system, you may need to install some additional
    things first:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We suggest of working with Anaconda, so install Anaconda 3, too. You can find
    everything about installing this Python distribution at [https://www.anaconda.com/download/](https://www.anaconda.com/download/).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用 Anaconda，所以也安装 Anaconda 3。你可以在[https://www.anaconda.com/download/](https://www.anaconda.com/download/)找到关于安装这个
    Python 发行版的所有信息。
- en: 'After setting the system requirements, installing OpenGym AI with all its modules
    is quite straightforward:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 设置系统要求后，安装 OpenGym AI 及其所有模块非常简单：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For this project, we are actually interested in working with the Box2D module,
    which is a 2D physics engine providing a rendering of real-world physics in a
    2D environment, as commonly seen in pseudo-realistic video games. You can test
    that the Box2D module works by running these commands in Python:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们实际上是想使用 Box2D 模块，它是一个 2D 物理引擎，提供在 2D 环境中模拟真实世界物理的渲染效果，通常在伪现实的电子游戏中能看到。你可以通过在
    Python 中运行这些命令来测试 Box2D 模块是否正常工作：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If the provided code runs with no problem, you can proceed with the project.
    In some situations, Box2D may become difficult to run and, for instance, there
    could be problems such as those reported in [https://github.com/cbfinn/gps/issues/34](https://github.com/cbfinn/gps/issues/34),
    though there are many other examples around. We have found that installing the
    Gym in a `conda` environment based on Python 3.4 makes things much easier:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提供的代码运行没有问题，你可以继续进行项目。在某些情况下，Box2D 可能变得难以运行，例如可能会遇到[https://github.com/cbfinn/gps/issues/34](https://github.com/cbfinn/gps/issues/34)中报告的问题，虽然周围还有许多其他类似的例子。我们发现，将
    Gym 安装在基于 Python 3.4 的 `conda` 环境中会使事情变得更加简单：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This installation sequence should allow you to create a `conda` environment
    that's appropriate for the project we are going to present in this chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个安装顺序应该能让你创建一个适合本章我们要介绍的项目的 `conda` 环境。
- en: Lunar Lander in OpenAI Gym
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym 中的 Lunar Lander
- en: LunarLander-v2 is a scenario developed by Oleg Klimov, an engineer at OpenAI,
    inspired by the original Atari Lunar Lander ([https://github.com/olegklimov](https://github.com/olegklimov)).
    In the implementation, you have to take your landing pod to a lunar pad that is
    always located at coordinates *x=0* and *y=0*. In addition, your actual *x* and
    *y* position is known since their values are stored in the first two elements
    of the state vector, the vector that contains all the information for the reinforcement
    learning algorithm to decide the best action to take at a certain moment.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LunarLander-v2 是由 OpenAI 工程师 Oleg Klimov 开发的一个场景，灵感来源于原始的 Atari Lunar Lander
    ([https://github.com/olegklimov](https://github.com/olegklimov))。在实现中，你需要将着陆舱带到一个始终位于坐标
    *x=0* 和 *y=0* 的月球平台。此外，你的实际 *x* 和 *y* 位置是已知的，因为它们的值存储在状态向量的前两个元素中，状态向量包含所有信息，供强化学习算法在某一时刻决定采取最佳行动。
- en: This renders the task accessible because you won't have to deal with fuzzy or
    uncertain localization of your position with respect to the objective (a common
    problem in robotics).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得任务更加可接近，因为你不需要处理与目标位置相关的模糊或不确定的位置定位问题（这是机器人技术中的常见问题）。
- en: '![](img/a99d1857-625c-476b-8829-5605e0eded24.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a99d1857-625c-476b-8829-5605e0eded24.png)'
- en: Figure 2: LunarLander-v2 in action
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LunarLander-v2 的运行情况
- en: 'At each moment, the landing pod has four possible actions to choose from:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每一刻，着陆舱都有四个可能的行动可供选择：
- en: Do nothing
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么也不做
- en: Rotate left
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向左旋转
- en: Rotate right
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向右旋转
- en: Thrust
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推力
- en: 'There is then a complex system of reward to make things interesting:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后有一个复杂的奖励系统来使事情变得有趣：
- en: Reward for moving from the top of the screen to the landing pad and reaching
    zero speed ranges from 100 to 140 points (landing outside the landing pad is possible)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从屏幕顶部移动到着陆平台并达到零速度的奖励范围从 100 到 140 分（着陆在着陆平台外是可能的）
- en: If the landing pod moves away from landing pad without coming to a stop, it
    loses some of the previous rewards
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果着陆舱在没有停下来的情况下离开着陆平台，它会失去一些之前的奖励
- en: Each episode (the term used to point out a game session) completes when the
    landing pod crashes or it comes to rest, respectively providing additional -100
    or +100 points
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一回合（指游戏会话的术语）会在着陆舱坠毁或停下来时结束，分别提供额外的 -100 或 +100 分
- en: Each leg in contact with the ground is +10
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与地面接触的每条腿加 10 分
- en: Firing the main engine is -0.3 points per frame (but fuel is infinite)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动主引擎每帧扣除 -0.3 分（但燃料是无限的）
- en: Solving the episode grants 200 points
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决这一回合会获得 200 分
- en: 'The game works perfectly with discrete commands (they are practically binary:
    full thrust or no thrust) because, as the author of the simulation says, according
    to Pontryagin''s maximum principle it''s optimal to fire the engine at full throttle
    or completely turn it off.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个游戏非常适合离散命令（它们实际上是二进制的：全推力或无推力），因为正如模拟的作者所说，根据庞特里亚金最大值原理，最优的做法是以全推力开火引擎或完全关闭引擎。
- en: 'The game is also solvable using some simple heuristics based on the distance
    to the target and using a **proportional integral derivative** (**PID**) controller
    to manage the descending speed and angle. A PID is an engineering solution to
    control systems where you have feedback. At the following URL, you can get a more
    detailed explanation of how they work: [https://www.csimn.com/CSI_pages/PIDforDummies.html](https://www.csimn.com/CSI_pages/PIDforDummies.html).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个游戏也可以通过一些简单的启发式方法解决，这些方法基于与目标的距离，并使用**比例积分微分**（**PID**）控制器来管理下降的速度和角度。PID是用于控制系统的工程解决方案，在这些系统中你有反馈。你可以通过以下网址获取更详细的解释：[https://www.csimn.com/CSI_pages/PIDforDummies.html](https://www.csimn.com/CSI_pages/PIDforDummies.html)。
- en: Exploring reinforcement learning through deep learning
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过深度学习探索强化学习
- en: In this project, we are not interested in developing a heuristic (a still valid
    approach to solving many problems in artificial intelligence) or constructing
    a working PID. We intend instead to use deep learning to provide an agent with
    the necessary intelligence to operate a Lunar Lander video game session successfully.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们并不打算开发启发式方法（虽然它仍然是解决许多人工智能问题的有效方法）或构建一个有效的PID控制器。相反，我们打算使用深度学习为智能体提供必要的智能，以成功操作月球着陆器视频游戏。
- en: 'Reinforcement learning theory offers a few frameworks to solve such problems:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习理论提供了一些框架来解决此类问题：
- en: '**Value-based learning**: This works by figuring out the reward or outcome
    from being in a certain state. By comparing the reward of different possible states,
    the action leading to the best state is chosen. Q-learning is an example of this
    approach.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于值的学习**：通过计算处于某个状态时的奖励或结果来工作。通过比较不同可能状态的奖励，选择导致最佳状态的动作。Q-learning就是这种方法的一个例子。'
- en: '**Policy-based learning**: Different control policies are evaluated based on
    the reward from the environment. It is decided upon the policy achieving the best
    results.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于策略的学习**：不同的控制策略基于来自环境的奖励进行评估，并决定哪种策略能够获得最佳结果。'
- en: '**Model-based learning**: Here, the idea is to replicate a model of the environment
    inside the agent, thus allowing the agent to simulate different actions and their
    consequent reward.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的学习**：这里的理念是复制一个环境模型到智能体中，从而允许智能体模拟不同的动作及其相应的奖励。'
- en: 'In our project, we will use a value-based learning framework; specifically,
    we will use the now classical approach in reinforcement learning based on Q-learning,
    which has been successfully controlled games where an agent has to decide on a
    series of moves that will lead to a delayed reward later in the game. Devised
    by C.J.C.H. Watkins in 1989 in his Ph.D. thesis, the method, also called **Q-learning**,
    is based on the idea that an agent operates in an environment, taking into account
    the present state, in order to define a sequence of actions that will lead to
    an ultimate reward:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的项目中，我们将使用基于值的学习框架；具体而言，我们将采用现在在强化学习中已经成为经典的Q-learning方法，这个方法成功地应用于控制游戏，其中一个智能体需要决定一系列动作，最终导致游戏后期的延迟奖励。这个方法由C.J.C.H.
    Watkins在1989年在他的博士论文中提出，也被称为**Q-learning**，它基于这样的理念：智能体在一个环境中操作，考虑当前状态，以定义一系列动作，最终获得奖励：
- en: '![](img/99587134-1e20-4285-b204-3cff85be23f7.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99587134-1e20-4285-b204-3cff85be23f7.png)'
- en: In the above formula, it is described how a state *s*, after an action *a*,
    leads to a reward, *r*, and a new state *s'*. Starting from the initial state
    of a game, the formula applies a series of actions that, one after the other,
    transforms each subsequent state until the end of the game. You can then imagine
    a game as a series of chained states by a sequence of actions. You can then also
    interpret the above formula how an initial state *s* is transformed into a final
    state *s'* and a final reward *r* by a sequence of actions *a*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，描述了一个状态*s*在经过一个动作*a*后，如何导致奖励*r*和一个新状态*s'*。从游戏的初始状态开始，该公式应用一系列动作，依次转化每个后续状态，直到游戏结束。你可以将游戏想象成通过一系列动作连接起来的状态链。你还可以解读上述公式，如何通过一系列动作*a*将初始状态*s*转变为最终状态*s'*和最终奖励*r*。
- en: 'In reinforcement terms, a **policy** is how to best choose our sequence of
    actions, *a*.  A policy can be approximated by a function, which is called *Q*,
    so that given the present state, *s*, and a possible action, *a*, as inputs, it
    will provide an estimate of the maximum reward, *r*, that will derive from that
    action:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，**策略**是如何选择最佳的动作序列，*a*。策略可以通过一个函数来逼近，称为*Q*，它以当前状态*s*和可能的动作*a*为输入，提供最大奖励*r*的估计，该奖励将从这个动作中得到：
- en: '![](img/d7fd4657-9ec3-4acf-94ae-c06421c76173.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7fd4657-9ec3-4acf-94ae-c06421c76173.png)'
- en: 'This approach is clearly greedy, meaning that we just choose the best action
    at a precise state because we expect that always choosing the best action at each
    step will lead us to the best outcome. Thus, in the greedy approach, we do not
    consider the possible chain of actions leading to the reward, but just the next
    action, *a*. However, it can be easily proved that we can confidently adopt a
    greedy approach and obtain the maximum reward using such a policy if such conditions
    are met:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法显然是贪心的，意味着我们仅仅选择在某个精确状态下的最佳动作，因为我们预期始终选择最佳动作将导致最佳结果。因此，在贪心方法中，我们并不考虑可能的动作链条，而只是关注下一个动作*a*。然而，可以很容易证明，只要满足以下条件，我们就可以自信地采用贪心方法，并通过这种策略获得最大奖励：
- en: we find the perfect policy oracle, *Q**
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们找到了完美的策略预言机，*Q*
- en: we operate in an environment where information is perfect (meaning we can know
    everything about the environment)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们操作的环境信息是完备的（即我们可以知道环境的所有信息）
- en: the environment adheres to the **Markov principle (see the tip box)**
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境遵循**马尔科夫原理**（见提示框）
- en: the Markov principle states that the future (states, rewards) only depends on
    the present and not the past, therefore we can simply derive the best to be done
    by looking at the present state and ignoring what has previously happened.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔科夫原理指出，未来（状态、奖励）仅依赖于当前状态，而与过去无关，因此我们可以通过仅查看当前状态并忽略过去的发生来推导出应该做什么。
- en: In fact, if we build the *Q* function as a recursive function, we just need
    to explore (using a breadth-first search approach) the ramifications to the present
    state of our action to be tested, and the recursive function will return the maximum
    reward possible.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如果我们将*Q*函数构建为递归函数，我们只需要使用广度优先搜索方法，探索当前状态下我们测试的动作的影响，递归函数将返回可能的最大奖励。
- en: 'Such an approach works perfectly in a computer simulation, but it makes little
    sense in the real world:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在计算机模拟中效果很好，但在现实世界中意义不大：
- en: Environments are mostly probabilistic. Even if you perform an action, you don't
    have the certainty of the exact reward.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境大多是概率性的。即使你执行了某个动作，也不能确定精确的奖励。
- en: Environments are tied to the past, the present alone cannot describe what could
    be the future because the past can have hidden or long-term consequences.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境与过去紧密相关，单独的当前状态无法描述未来的可能性，因为过去可能会带来隐性或长期的后果。
- en: Environments are not exactly predictable, so you cannot know in advance the
    rewards from an action, but you can know them afterward (this is called an **a
    posteriori** condition).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境并不完全可预测，因此你无法预先知道某个动作的奖励，但你可以在事后知道它们（这被称为**后验**条件）。
- en: Environments are very complex. You cannot figure out in a reasonable time all
    the possible consequences of an action, hence you cannot figure out with certainty
    the maximum reward deriving from an action.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境非常复杂。你无法在合理的时间内弄清楚一个动作可能带来的所有后果，因此你无法确定某个动作产生的最大奖励。
- en: The solution is then to adopt an approximate *Q* function, one that can take
    into account probabilistic outcomes and that doesn't need to explore all the future
    states by prediction. Clearly, it should be a real approximation function, because
    building a search table of values is unpractical in complex environments (some
    state spaces could take continuous values, making the possible combinations infinite).
    Moreover, the function can be learned offline, which implies leveraging the experience
    of the agent (the ability to memorize becomes then quite important).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是采用近似的*Q*函数，它能够考虑概率性结果，并且不需要通过预测来探索所有未来状态。显然，这应该是一个真正的逼近函数，因为在复杂环境中构建值的查找表是不切实际的（一些状态空间可能是连续值，使得可能的组合数是无限的）。此外，函数可以离线学习，这意味着可以利用智能体的经验（记忆能力变得非常重要）。
- en: There have been previous attempts to approximate a *Q* function by a neural
    network, but the only successful application has been `TD_Gammon`, a backgammon
    program that learned to play by reinforcement learning powered by a multi-layer
    perceptron only. `TD_Gammon` achieved a superhuman level of play, but at the time
    its success couldn't be replicated in different games, such as chess or go.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 之前曾有尝试通过神经网络来逼近*Q*函数，但唯一成功的应用是`TD_Gammon`，一个仅通过多层感知器的强化学习来学习玩跳棋的程序。`TD_Gammon`达到了超人类的水平，但当时它的成功无法在其他游戏中复制，比如国际象棋或围棋。
- en: That led to the belief that neural networks were not really suitable for figuring
    out a *Q* function unless the game was somehow stochastic (you have to throw a
    dice in backgammon). In 2013, a paper on deep reinforcement learning, *Playing
    Atari with deep reinforcement learning(*[https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)*)*
    Volodymyr Minh, et al, applied to old Atari games demonstrated the contrary.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致人们认为神经网络不适合计算*Q*函数，除非游戏本身是随机的（例如你必须在跳棋中掷骰子）。然而，2013年，Volodymyr Minh等人发表了一篇关于深度强化学习的论文，*Playing
    Atari with deep reinforcement learning*（[https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)），证明了相反的观点。
- en: 'Such paper demonstrates how a *Q* function could be learned using neural networks
    to play a range of Atari arcade games (such as Beam Rider, Breakout, Enduro, Pong,
    Q*bert, Seaquest, and Space Invaders) just by processing video inputs (by sampling
    frames from a 210 x 160 RGB video at 60 Hz) and outputting joystick and fire button
    commands. The paper names the method a **Deep Q-Network** (**DQN**), and it also introduces
    the concepts of experience replay and exploration versus exploitation, which we
    are going to discuss in the next section. These concepts help to overcome some
    critical problems when trying to apply deep learning to reinforcement learning:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文展示了如何使用神经网络学习一个*Q*函数，以便通过处理视频输入（通过以60Hz的频率从210 x 160 RGB视频中采样帧）并输出摇杆和开火按钮命令，来玩一系列的Atari街机游戏（如Beam
    Rider、Breakout、Enduro、Pong、Q*bert、Seaquest和Space Invaders）。论文将这种方法命名为**深度Q网络**（**DQN**），并且它还介绍了经验重放和探索与利用的概念，我们将在下一节讨论这些概念。这些概念有助于克服在将深度学习应用于强化学习时的一些关键问题：
- en: Lack of plenty of examples to learn from—something necessary in reinforcement
    learning and even more indispensable when using deep learning for it
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏足够的例子供学习——这是强化学习所必须的，特别是在使用深度学习时更加不可或缺。
- en: Extended delay between an action and the effective reward, which requires dealing
    with sequences of further actions of variable length before getting a reward
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作与有效奖励之间的延迟较长，这需要处理进一步行动的序列，这些序列长度不定，直到获得奖励为止。
- en: Series of highly correlated sequences of actions (because an action often influences
    the following ones), which may cause any stochastic gradient descent algorithm
    to overfit to the most recent examples or simply converge non-optimally (stochastic
    gradient descent expects random examples, not correlated ones)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一系列高度相关的行动序列（因为一个动作通常会影响后续的动作），这可能导致任何随机梯度下降算法过拟合最近的例子，或者根本没有以最优方式收敛（随机梯度下降期望的是随机样本，而不是相关样本）。
- en: The paper, *Human-level control through deep reinforcement learning* ([http://www.davidqiu.com:8888/research/nature14236.pdf](http://www.davidqiu.com:8888/research/nature14236.pdf)),
    by Mnih and other researchers just confirms DQN efficacy where more games are
    explored by using it and performances of DQN are compared to human players and
    classical algorithms in reinforcement learning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 论文*Human-level control through deep reinforcement learning*（[http://www.davidqiu.com:8888/research/nature14236.pdf](http://www.davidqiu.com:8888/research/nature14236.pdf)）由Mnih和其他研究人员撰写，进一步确认了DQN的有效性，使用该方法探索了更多的游戏，并将DQN的表现与人类玩家及经典强化学习算法进行了比较。
- en: In many games, DQN proved better than human skills, though the long-term strategy
    is still a problem for the algorithm. In certain games, such as *Breakout*, the
    agent discovers cunning strategies such as digging a tunnel through the wall in
    order to send the ball through and destroy the wall in an effortless manner. In
    other games, such as *Montezuma's Revenge*, the agent remains clueless.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多游戏中，DQN表现得比人类更优秀，尽管长期策略仍然是该算法的一个问题。在某些游戏中，例如*Breakout*，代理发现了巧妙的策略，如通过墙壁挖隧道，将球送过墙壁并轻松摧毁它。在其他游戏中，如*Montezuma's
    Revenge*，代理依然一无所知。
- en: In the paper, the authors discuss at length how the agent understands the nuts
    and bolts of winning a Breakout game and they provide a chart of the response
    of the DQN function demonstrating how high reward scores are assigned to behaviors that
    first dig a hole in the wall and then let the ball pass through it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Tricks and tips for deep Q-learning
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Q-learning obtained by neural networks was deemed unstable until some tricks
    made it possible and feasible. There are two power-horses in deep Q-learning,
    though other variants of the algorithm have been developed recently in order to
    solve problems with performance and convergence in the original solution. Such
    new variants are not discussed in our project: double Q-learning, delayed Q-learning,
    greedy GQ, and speedy Q-learning. The two main DQN power-horses that we are going
    to explore are **experience replay** and the decreasing trade-off between **exploration
    and exploitation**.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: With experience replay, we simply store away the observed states of the game
    in a queue of a prefixed size since we discard older sequences when the queue
    is full. Contained in the stored data, we expect to have a number of tuples consisting
    of the present state, the applied action, the consequently obtained state, and
    the reward gained. If we consider a simpler tuple made of just the present state
    and the action, we have the observation of the agent operating in the environment,
    which we can consider the root cause of the consequent state and of the reward.
    We can consider now the tuple (present state and action) as our predictor ( *x*
    vector) with respect to the reward. Consequently, we can use the reward directly
    connected to the action and the reward that will be achieved at the end of the
    game.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Given such stored data (which we can figure out as the memory of our agent),
    we sample a few of them in order to create a batch and use the obtained batch
    to train our neural network. However, before passing the data to the network,
    we need to define our target variable, our *y* vector. Since the sampled states
    mostly won't be the final ones, we will probably have a zero reward or simply
    a partial reward to match against the known inputs (the present state and the
    chosen action). A partial reward is not very useful because it just tells part
    of the story we need to know. Our objective is, in fact, to know the total reward
    we will get at the end of the game, after having taken the action from the present
    state we are evaluating (our *x* value).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: In this case, since we don't have such information, we simply try to approximate
    the value by using our existing *Q* function in order to estimate the residual
    reward that will be the maximum consequence of the (state, action) tuple we are
    considering. After obtaining it, we discount its value using the Bellman equation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read an explanation of this now classic approach in reinforcement learning
    in this excellent tutorial by Dr. Sal Candido, a software engineer at Google:
    [http://robotics.ai.uiuc.edu/~scandido/?Developing_Reinforcement_Learning_from_the_Bellman_Equation](http://robotics.ai.uiuc.edu/~scandido/?Developing_Reinforcement_Learning_from_the_Bellman_Equation)),
    where the present reward is added to the discounted future reward.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Using a small value (approaching zero) for discounting makes the *Q* function
    more geared toward short-term rewards, whereas using a high discount value (approaching
    one) renders the *Q* function more oriented to future gains.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: The second very effective trick is using a coefficient for trading between exploration
    and exploitation. In exploration, the agent is expected to try different actions
    in order to find the best course of action given a certain state. In exploitation,
    the agent leverages what it learned in the previous explorations and simply decides
    for what it believes the best action to be taken in that situation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Finding a good balance between exploration and exploitation is strictly connected
    to the usage of the experience replay we discussed earlier. At the start of the
    DQN algorithm optimization, we just have to rely on a random set of network parameters.
    This is just like sampling random actions, as we did in our simple introductory
    example to this chapter. The agent in such a situation will explore different
    states and actions, and help to shape the initial *Q* function. For complex games
    such as *Lunar Lander* using random choices won't take the agent far, and it could
    even turn unproductive in the long run because it will prevent the agent from
    learning the expected reward for tuples of state and action that can only be accessed
    if the agent has done the correct things before. In fact, in such a situation
    the DQN algorithm will have a hard time figuring out how to appropriately assign
    the right reward to an action because it will never have seen a completed game. Since the
    game is complex, it is unlikely that it could be solved by random sequences of
    actions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The correct approach, then, is to balance learning by chance and using what
    has been learned to take the agent further in the game to where problems are yet
    to be solved. This resembles finding a solution by a series of successive approximations,
    by taking the agent each time a bit nearer to the correct sequence of actions
    for a safe and successful landing. Consequently, the agent should first learn
    by chance, find the best things to be done in a certain set of situations, then
    apply what has been learned and get access to new situations that, by random choice,
    will be also solved, learned, and applied successively.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: This is done using a decreasing value as the threshold for the agent to decide
    whether, at a certain point in the game, to take a random choice and see what
    happens or leverage what it has learned so far and use it to make the best possible
    action at that point, given its actual capabilities. Picking a random number from
    a uniform distribution [*0*,*1*], the agent compares it with an epsilon value,
    and if the random number is larger than the epsilon it will use its approximate
    neural *Q* function. Otherwise, it will pick a random action from the options
    available. After that, it will decrease the epsilon number. Initially, epsilon
    is set at the maximum value, *1.0*, but depending on a decaying factor, it will
    decrease with time more or less rapidly, arriving at a minimum value that should
    never be zero (no chance of taking a random move) in order for there to always
    be the possibility of learning something new and unexpected (a minimal openness
    factor) by serendipity.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the limitations of deep Q-learning
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even with deep Q-learning, there are some limitations, no matter whether you
    approximate your *Q* function by deriving it from visual images or other observations
    about the environment:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'The approximation takes a long time to converge, and sometimes it doesn''t
    achieve it smoothly: you may even witness the learning indicators of the neural
    network worsening instead of getting better for many epochs.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Being based on a greedy approach, the approach offered by Q-learning is not
    dissimilar from a heuristic: it points out the best direction but it cannot provide
    detailed planning. When dealing with long-term goals or goals that have to be
    articulated into sub-goals, Q-learning performs badly.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another consequence of how Q-learning works is that it really doesn't understand
    the game dynamics from a general point of view but from a specific one (it replicates
    what it experienced as effective during training). As a consequence, any novelty
    introduced into the game (and never actually experienced during training) can
    break down the algorithm and render it completely ineffective. The same goes when
    introducing a new game to the algorithm; it simply won't perform.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting the project
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After this long detour into reinforcement learning and the DQN approach, we
    are finally ready to start coding, having all the basic understanding of how to
    operate an OpenAI Gym environment and how to set a DQN approximation of a *Q*
    function. We simply start importing all the necessary packages:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `tempfile` module generates temporary files and directories that can be
    used as a temporary storage area for data files. The `deque `command, from the
    `collections` module, creates a double-ended queue, practically a list where you
    can append items at the start or at the end. Interestingly, it can be set to a
    predefined size. When full, older items are discarded in order to make the place
    for new entries.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: We will structure this project using a series of classes representing the agent,
    the agent's brain (our DQN), the agent's memory, and the environment, which is
    provided by OpenAI Gym but it needs to be correctly connected to the agent. It
    is necessary to code a class for this.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Defining the AI brain
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in the project is to create a `Brain` class containing all the
    neural network code in order to compute a Q-value approximation. The class will
    contain the necessary initialization, the code for creating a suitable TensorFlow
    graph for the purpose, a simple neural network (not a complex deep learning architecture
    but a simple, working network for our project—you can replace it with more complex
    architectures), and finally, methods for fit and predict operations.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: We start from initialization. As inputs, first, we really need to know the size
    of the state inputs (`nS`) corresponding to the information we receive from the
    game, and the size of the action output (`nA`) corresponding to the buttons we
    can press to perform actions in the game. Optionally, but strongly recommended,
    we also have to set the scope. In order to define the scope a string will help
    us to keep separate networks created for different purposes, and in our project,
    we have two, one for processing the next reward and one for guessing the final
    reward.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Then, we have to define the learning rate for the optimizer, which is an Adam.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: The Adam optimizer is described in the following paper: [https://arxiv.org/abs/1412.6980.](https://arxiv.org/abs/1412.6980)It
    is a very efficient gradient-based optimization method that requires very little
    to be tuned in order to work properly. The Adam optimization is a stochastic gradient
    descent algorithm similar to RMSprop with Momentum. This post, [https://theberkeleyview.wordpress.com/2015/11/19/berkeleyview-for-adam-a-method-for-stochastic-optimization/](https://theberkeleyview.wordpress.com/2015/11/19/berkeleyview-for-adam-a-method-for-stochastic-optimization/),
    from the UC Berkeley Computer Vision Review Letters, provides more information.
    From our experience, it is one of the most effective solutions when training a
    deep learning algorithm in batches, and it requires some tuning for the learning
    rate.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we also provide:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: A neural architecture (if we prefer to change the basic one provided with the
    class)
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input the `global_step`, a global variable that will keep track of the number
    of training batches of examples that have been feed to the DQN network  up to
    that moment
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The directory in which to store the logs for TensorBoard, the standard visualization
    tool for TensorFlow
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The command  `tf.summary.FileWriter` initializes an event file in a target directory
    (`summary_dir`) where we store the key measures of the learning process. The handle
    is kept in `self.summary_writer`, which we will be using later for storing the
    measures we are interested in representing during and after the training for monitoring
    and debugging what has been learned.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: The next method to be defined is the default neural network that we will be
    using for this project. As input, it takes the input layer and the respective
    size of the hidden layers that we will be using. The input layer  is defined by
    the state that we are using, which could be a vector of measurements, as in our
    case, or an image, as in the original DQN paper)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Such layers are simply defined using the higher level ops offered by the `Layers`
    module of TensorFlow ([https://www.tensorflow.org/api_guides/python/contrib.layers](https://www.tensorflow.org/api_guides/python/contrib.layers)).
    Our choice goes for the vanilla `fully_connected`, using the `ReLU` (rectifier)
    `activation` function for the two hidden layers and the linear activation of the
    output layer.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'The predefined size of 32 is perfectly fine for our purposes, but you may increment
    it if you like. Also, there is no dropout in this network. Clearly, the problem
    here is not overfitting, but the quality of what is being learned, which could
    only be improved by providing useful sequences of unrelated states and a good
    estimate of the final reward to be associated. It is in the useful sequences of
    states, especially under the light of the trade-off between exploration and exploitation,
    that the key to not having the network overfit resides. In a reinforcement learning
    problem, you have overfitted if you fall into one of these two situations:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Sub-optimality: the algorithm suggests sub-optimal solutions, that it is, our
    lander learned a rough way to land and it sticks to it because at least it lands'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Helplessness: the algorithm has fallen into a learned helplessness; that is,
    it has not found a way to land correctly, so it just accepts that it is going
    to crash in the least bad way possible'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two situations can prove really difficult to overcome for a reinforcement
    learning algorithm such as DQN unless the algorithm can have the chance to explore
    alternative solutions during the game. Taking random moves from time to time is
    not simply a way to mess up things, as you may think at first sight, but a strategy
    to avoid pitfalls.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: With larger networks than this one, on the other hand, you may instead have
    a problem with a dying neuron requiring you to use a different activation, `tf.nn.leaky_relu` ([https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu](https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu)),
    in order to obtain a working network.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: A dead `ReLU` ends up always outputting the same value, usually a zero value,
    and it becomes resistant to backpropagation updates.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'The activation `leaky_relu` has been available since TensorFlow 1.4\. If you
    are using any previous version of TensorFlow, you can create an `ad hoc` function
    to be used in your custom network:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '`def leaky_relu(x, alpha=0.2):       return tf.nn.relu(x) - alpha * tf.nn.relu(-x)`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'We now proceed to code our `Brain` class, adding some more functions to it:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The method`create_network` combines input, neural network, loss, and optimization.
    The loss is simply created by taking the difference between the original reward
    and the estimated result, squaring it, and taking the average through all the
    examples present in the batch being learned. The loss is minimized using an Adam
    optimizer.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, a few summaries are recorded for TensorBoard:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: The average loss of the batch, in order to keep track of the fit during training
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum predicted reward in the batch, in order to keep track of extreme
    positive predictions, pointing out the best-winning moves
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average predicted reward in the batch, in order to keep track of the general
    tendency of predicting good moves
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code for `create_network`, the TensorFlow engine of our project:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The class is completed by a `predict` and a `fit` method. The `fit` method takes
    as input the state matrix, `s`, as the input batch and the vector of reward `r`
    as the outcome. It also takes into account how many epochs you want to train (in
    the original papers it is suggested using just a single epoch per batch in order
    to avoid overfitting too much to each batch of observations). Then, in the present
    session, the input is fit with respect to the outcome and summaries (previously
    defined as we created the network).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As a result,  `global step` is returned, which is a counter that helps to keep
    track of the number of examples used in training up so far, and then recorded
    for later use.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Creating memory for experience replay
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After defining the brain (the TensorFlow neural network), our next step is to
    define the memory, that is the storage for data that will power the learning process
    of the DQN network. At each training episode each step, made of a state and an
    action, is recorded together with the consequent state and the final reward of
    the episode (something that will be known only when the episode completes).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Adding a flag telling if the observation is a terminal one or not completes
    the set of recorded information. The idea is to connect certain moves not just
    to the immediate reward (which could be null or modest) but the ending reward,
    thus associating every move in that session to it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The class memory is simply a queue of a certain size, which is then filled with information on
    the previous game experiences, and it is easy to sample and extract from it. Given
    its fixed size, it is important that older examples are pushed out of the queue,
    thus allowing the available examples to always be among the last ones.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'The class comprises an initialization, where the data structure takes origin
    and its size is fixed, the `len` method (so we know whether the memory is full
    or not, which is useful, for instance, in order to wait for any training at least
    until we have plenty of them for better randomization and variety for learning),
    `add_memory` for recording in the queue, and `recall_memory` for recovering all
    the data from it in a list format:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Creating the agent
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next class is the agent, which has the role of initializing and maintaining
    the brain (providing the *Q-value* function approximation) and the memory. It
    is the agent, moreover, that acts in the environment. Its initialization sets
    a series of parameters that are mostly fixed given our experience in optimizing
    the learning for the Lunar Lander game. They can be explicitly changed, though,
    when the agent is first initialized:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '`epsilon = 1.0` is the initial value in the exploration-exploitation parameter.
    The `1.0` value forces the agent to completely rely on exploration, that is, random
    moving.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon_min = 0.01` sets the minimum value of the exploration-exploitation
    parameter: a value of `0.01` means that there is a 1% chance that the landing
    pod will move randomly and not based on *Q* function feedback. This always provides
    a minimum chance to find another optimal way of completing the game, without compromising
    it.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon_decay = 0.9994` is the decay that regulates the speed the `epsilon`
    diminishes toward the minimum. In this setting, it is tuned to reach a minimum
    value after about 5,000 episodes, which on average should provide the algorithm
    at least 2 million examples to learn from.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma = 0.99` is the reward discount factor with which the Q-value estimation
    weights the future reward with respect to the present reward, thus allowing the
    algorithm to be short- or long-sighted, according to what is best in the kind
    of game being played (in Lunar Lander it is better to be long-sighted because
    the actual reward will be experienced only when the landing pod lands on the Moon).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate = 0.0001` is the learning rate for the Adam optimizer to learn
    the batch of examples.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs = 1` is the training epochs used by the neural network in order to
    fit the batch set of examples.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size = 32` is the size of the batch examples.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`memory = Memory(memory_size=250000)` is the size of the memory queue.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the preset parameters, you are assured that the present project will work.
    For different OpenAI environments, you may need to find different optimal parameters.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The initialization will also provide the commands required to define where the
    TensorBoard logs will be placed (by default, the `experiment` directory), the
    model for learning how to estimate the immediate next reward, and another model
    to store the weights for the final reward. In addition, a saver (`tf.train.Saver`)
    will be initialized, allowing the serialization of the entire session to disk
    in order to restore it later and use it for playing the real game, not just learning
    how to play it.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'The two mentioned models are initialized in the same session, using different
    scope names (one will be `q`, the next reward model monitored by the TensorBoard;
    the other one will be `target_q`). Using two different scope names will allow
    easy handling of the neuron''s coefficients, making it possible to swap them with
    another method present in the class:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `epsilon` dealing with the share of time devoted exploring new solutions
    compared to exploiting the knowledge of the network is constantly updated with
    the `epsilon_update` method, which simply modifies the actual `epsilon` by multiplying
    it by `epsilon_decay` unless it has already reached its allowed minimum value:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `save_weights` and `load_weights` methods simply allow the session to be saved:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `set_weights` and  `target_model_update` methods work together to update
    the target Q network with the weights of the Q network (`set_weights` is a general-purpose,
    reusable function you can use in your solutions, too). Since we named the two
    scopes differently, it is easy to enumerate the variables of each network from
    the list of trainable variables. Once enumerated, the variables are joined in
    an assignment to be executed by the running session:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `act` method is the core of the policy implementation because it will decide,
    based on `epsilon`, whether to take a random move or go for the best possible
    one. If it is going for the best possible move, it will ask the trained Q network
    to provide a reward estimate for each of the possible next moves (represented
    in a binary way by pushing one of four buttons in the Lunar Lander game) and it
    will return the move characterized by the maximum predicted reward (a greedy approach
    to the solution):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `replay` method completes the class. It is a crucial method because it makes
    learning for the DQN algorithm possible. We are going, therefore, to discuss how
    it works thoroughly. The first thing that the `replay` method does is to sample
    a batch (we defined the batch size at initialization) from the memories of previous
    game episodes (such memories are just the variables containing values about status,
    action, reward, next status, and a flag variable noticing if the observation is
    a final status or not). The random sampling allows the model to find the best
    coefficients in order to learn the *Q* function by a slow adjustment of the network's
    weights, batch after batch.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Then the method finds out whether the sampling recalled statuses are final or
    not. Non-final rewards need to be updated in order to represent the reward that
    you get at the end of the game. This is done by using the target network, which
    represents a snapshot of the *Q* function network as fixed at the end of the previous
    learning. The target network is fed with the following status, and the resulting
    reward is summed, after being discounted by a gamma factor, with the present reward.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Using the present *Q* function may lead to instabilities in the learning process
    and it may not result in a satisfying *Q* function network.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: When the rewards of non-terminal states have been updated, the batch data is
    fed into the neural network for training.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Specifying the environment
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last class to be implemented is the `Environment` class. Actually, the
    environment is provided by the `gym` command, though you need a good wrapper around
    it in order to have it work with the previous `agent` class. That''s exactly what
    this class does. At initialization, it starts the Lunar Lander game and sets key
    variables such as `nS`, `nA` (dimensions of state and action), `agent`, and the
    cumulative reward (useful for testing the solution by providing an average of
    the last 100 episodes):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Then, we prepare the code for methods for `test`, `train`, and `incremental`
    (incremental training), which are defined as wrappers of the comprehensive `learning`
    method.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Using incremental training is a bit tricky and it requires some attention if
    you do not want to spoil the results you have obtained with your training so far.
    The trouble is that when we restart the brain has pre-trained coefficients but
    memory is actually empty (we can call this as a cold restart). Being the memory
    of the agent empty, it cannot support good learning because of too few and limited
    examples. Consequently, the quality of the examples being fed is really not perfect
    for learning (the examples are mostly correlated with each other and very specific
    to the few newly experienced episodes). The risk of ruining the training can be
    mitigated using a very low `epsilon` (we suggest set at the minimum, `0.01` ):
    in this way, the network  will most of the time simply re-learn its own weights
    because it will suggest for each state the actions it already knows, and its performance
    shouldn''t worsen but oscillate in a stable way until there are enough examples
    in memory and it will start improving again.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for issuing the correct methods for training and testing:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The final method is `learn`, arranging all the steps for the agent to interact
    with and learn from the environment. The method takes the `epsilon` value (thus
    overriding any previous `epsilon` value the agent had), the number of episodes
    to run in the environment, whether it is being trained or not (a Boolean flag),
    and whether the training is continuing from the training of a previous model (another
    Boolean flag).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first block of code, the method loads the previously trained weights
    of the network for Q value approximation if we want:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: to test the network and see how it works;
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: to carry on some previous training using further examples.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the method delves into a nested iteration. The outside iteration is running
    through the required number of episodes (each episode a Lunar Lander game has
    taken to its conclusion). Whereas the inner iteration is instead running through
    a maximum of 1,000 steps making up an episode.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: At each time step in the iteration, the neural network is interrogated on the
    next move. If it is under test, it will always simply provide the answer about
    the next best move. If it is under training, there is some chance, depending on
    the value of `epsilon`, that it won't suggest the best move but it will instead
    propose making a random move.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: After the move, all the information is gathered (initial state, chosen action,
    obtained reward, and consequent state) and saved into memory. At this time frame,
    if the memory is large enough to create a batch for the neural network approximating
    the *Q* function, then a training session is run. When all the time frames of
    the episode have been consumed, the weights of the DQN get stored into another
    network to be used as a stable reference as the DQN network is learning from a
    new episode.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Running the reinforcement learning process
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, after all the digression on reinforcement learning and DQN and writing
    down the complete code for the project, you can run it using a script or a Jupyter
    Notebook, leveraging the `Environment` class that puts all the code functionalities
    together:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After instantiating it, you just have to run the `train`, starting from `epsilon=1.0`
    and setting the goal to `5000` episodes (which corresponds to about 2.2 million
    examples of chained variables of state, action and reward). The actual code we
    provided is set to successfully accomplish a fully trained DQN model, though it
    may take some time, given your GPU''s availability and its computing capabilities:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the end, the class will complete the required training, leaving a saved
    model on disk (which could be run or even reprised anytime). You can even inspect
    the TensorBoard using a simple command that can be run from a shell:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The plots will appear on your browser, and they will be available for inspection
    at the local address `localhost:6006`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3007e975-dc2b-4ce9-98b9-b9607a870578.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Figure 4: The loss trend along the training, the peaks represent break-thoughts
    in learning such as at 800k examples
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: when it started landing safely on the ground.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss plot will reveal that, contrary to other projects, the optimization
    is still characterized by a decreasing loss, but with many spikes and problems
    along the way:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The plots represented here are the result of running the project once. Since
    there is a random component in the process, you may obtain slightly different
    plots when running the project on your own computer.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29069c1b-c954-4c9b-8c2c-baae261a3d4a.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The trend of maximum q values obtained in a batch session of learning'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'The same story is told by the maximum predicted *q* value and the average predicted
    *q* value.  The network improves at the end, though it can slightly retrace its
    steps and linger on plateaus for a long time:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a361ba7-4c97-4b4e-b03f-c64b3a51345a.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The trend of average q values obtained in a batch session of learning'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Only if you take the average of the last 100 final rewards do you see an incremental
    path, hinting at a persistent and steady improvement of the DQN network:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ce2623c-3338-47c5-8059-45e78cc942fa.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The trend of actually obtained scores at the end of each learning
    episode, it more clearly depicts the growing capabilities of the DQN'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Using the same information, from the output, not from the TensorBoard, you'll
    also figure out that the number of actions changes on average depending on the
    `epsilon` value. At the beginning, the number of actions required to finish an
    episode was under 200\. Suddenly, when `epsilon` is `0.5`, the average number
    of actions tends to grow steadily and reach a peak at about 750 (the landing pod
    has learned to counteract gravity by using its rockets).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, the network discovers this is a sub-optimal strategy and when `epsilon`
    turns below `0.3`, the average number of actions for completing an episode drops
    as well. The DQN in this phase is discovering how to successfully land the pod
    in a more efficient way:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9948d955-1382-4c2c-a97d-472505d1ca43.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The relationship between the epsilon (the exploration/exploitation
    rate) and the efficiency of the DQN network,'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: expressed as a number of moves used to complete an episode
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'If for any reason, you believe that the network needs more examples and learning,
    you can reprise the learning using the incremental `method`, keeping in mind that
    `epsilon` should be very low in this case:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'After the training, if you need to see the results and know, on average every
    100 episodes, how much the DQN can score (the ideal target is a `score >=200`), 
    you can just run the following command:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Acknowledgements
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the conclusion of this project, we would like to indeed thank Peter Skvarenina,
    whose project "Lunar Lander II" ([https://www.youtube.com/watch?v=yiAmrZuBaYU](https://www.youtube.com/watch?v=yiAmrZuBaYU))
    has been the key inspiration for our own project, and for all his suggestions
    and hints during the making of our own version of the Deep Q-Network.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this project, we have explored what a reinforcement algorithm can manage
    to achieve in an OpenAI environment, and we have programmed a TensorFlow graph
    capable of learning how to estimate a final reward in an environment characterized
    by an agent, states, actions, and consequent rewards. This approach, called DQN,
    aims to approximate the result from a Bellman equation using a neural network
    approach. The result is a Lunar Lander game that the software can play successfully
    at the end of training by reading the game status and deciding on the right actions
    to be taken at any time.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
