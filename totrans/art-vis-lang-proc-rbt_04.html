<html><head></head><body>
		<div id="_idContainer108" class="Content">
			<h1 id="_idParaDest-76"><em class="italics"><a id="_idTextAnchor095"/>Chapter 4</em></h1>
		</div>
		<div id="_idContainer109" class="Content">
			<h1 id="_idParaDest-77"><a id="_idTextAnchor096"/>Neural Networks with NLP</h1>
		</div>
		<div id="_idContainer110" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Explain what a Recurrent Neural Network is</li>
				<li class="bullets">Design and build a Recurrent Neural Network</li>
				<li class="bullets">Evaluate non-numeric data</li>
				<li class="bullets">Evaluate the different state-of-the-art language models with RNNs</li>
				<li class="bullets">Predict a value with a temporal sequence of data</li>
			</ul>
			<p>This chapter covers various aspects of RNNs. it deals with <a id="_idTextAnchor097"/>explaining, designing, and building the various RNN models.</p>
		</div>
		<div id="_idContainer145" class="Content">
			<h2 id="_idParaDest-78"><a id="_idTextAnchor098"/>Introduction</h2>
			<p>As mentioned in the previous chapter, Natural Language Processing (NLP) is an area of Artificial Intelligence (AI) that covers how computers can understand and manipulate human language in order to perform useful tasks. Now, with the growth of deep learning techniques, deep NLP has become a new area of research.</p>
			<p>So, what is deep NLP? It is a combination of NLP techniques and deep learning. The result of the combination of these techniques are advances in the following areas:</p>
			<ul>
				<li>Linguistics: Speech to text</li>
				<li>Tools: POS tagging, entity recognition, and sentence parsing</li>
				<li>Applications: Sentiment analysis, question answering, dialogue agents, and machine translation</li>
			</ul>
			<p>One of the most important approaches of deep NLP is the representation of words and sentences. Words can be represented as a vector located in a plane full of other words. Depending on the similarity of each word to another word, its distance in the plane would be accordingly set as greater or smaller.</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/C13550_04_01.jpg" alt="Figure 4.1: Representation of words in multiple dimensions"/>
				</div>
			</div>
			<h6>Figure 4.1: Representation of words in multiple dimensions</h6>
			<p>The previous figure shows an example of word embedding. <strong class="keyword">Word embedding</strong> is a collection of techniques and methods that map words and sentences from a corpus into vectors or real numbers. It generates a representation of each word in terms of the context in which a word appears. Then, word embedding can find the similarities between words. For example, the nearest words to dog are as follows:</p>
			<ol>
				<li>Dogs</li>
				<li>Cat</li>
				<li>Cow</li>
				<li>Rat</li>
				<li>Bird</li>
			</ol>
			<p>There are different ways to generate embeddings, such as Word2Vec, which will be covered in <em class="italics">Chapter 7</em>, <em class="italics">Build a Conversational Agent to Manage the Robot</em>.</p>
			<p>This is not the only big change deep learning brings to NLP on a morphological level. With deep learning, a word can be represented as a combination of vectors. </p>
			<p>Each morpheme is a vector, and a word is the result of combining several morpheme vectors.</p>
			<p>This technique of combining vectors is also used on a semantic level, but for the creation of words and for the creation of a sentence. Each phrase is formed by a combination of many word vectors, so a sentence can be represented as one vector.</p>
			<p>Another improvement is in parsing sentences. This task is hard because it is ambiguous. Neural networks can accurately determine the grammatical structure of a sentence.</p>
			<p>In full application terms, the areas are as follows:</p>
			<ul>
				<li><strong class="keyword">Sentiment analysis</strong>: Traditionally, this consists of a bag of words labeled with positive or negative sentiments. Then, combining these words returns the sentiment of the whole sentence. Now, using deep learning and word representation models, the results are better.</li>
				<li><strong class="keyword">Question answering</strong>: To find the answer to a question, vector representations can match a document, a paragraph, or a sentence with an input question.</li>
				<li><strong class="keyword">Dialogue agents</strong>: With neural language models, a model can understand a query and create a response.</li>
				<li><strong class="keyword">Machine translation</strong>: Machine translation is one of the hardest tasks in NLP. A lot of approaches and models have been tried. Traditional models are very large and complex, but deep learning neural machine translation has solved that problem. Sentences are encoded with vectors, and the output is decoded.</li>
			</ul>
			<p>The vector representation of words is fundamental to deep NLP. Creating a plane, many tasks can be completed. Before analyzing deep NLP techniques, we are going to review what a recurrent neural network (RNN) is, what its applications are within deep learning, and how to create our first RNN.</p>
			<p>Our future conversational agent will detect the intention of a conversation and respond with a predefined answer. But with a good dataset of conversations, we could create a Recurrent Neural Network to train a language model (LM) capable of generating a response to a given topic in a conversation. This task can be performed by other neural network architectures, such as seq2seq models. </p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor099"/>Recurrent Neural Networks</h2>
			<p>In this section, we are going to review <strong class="keyword">Recurrent Neural Networks</strong> (<strong class="keyword">RNNs</strong>). This topic will first look at the theory of RNNs. It will review many architectures within this model and help you to work out which model to use to solve a certain problem, and it will also look at several types of RNN and their pros and cons. Also, we will look at how to create a simple RNN, train it, and make predictions.</p>
			<h3 id="_idParaDest-80"><a id="_idTextAnchor100"/>Introduction to Recurrent Neural Networks (RNN)</h3>
			<p>Human behavior shows a variety of serially ordered action sequences. A human is capable of learning dynamic paths based on a set of previous actions or sequences. This means that people do not start learning from scratch; we have some previous knowledge, which helps us. For example you could not understand a word if you did not understand the previous word in a sentence!</p>
			<p>Traditionally, neural networks cannot solve these types of problem because they cannot learn previous information. But what happens with problems that cannot be solved with just current information?</p>
			<p>In 1986, Michael I. Jordan proposed a model that deals with the classical problem of temporal organization. This model is capable of learning the trajectories of a dynamic object by studying its previous movements. Jordan created the first RNN.</p>
			<h6>   </h6>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/C13550_04_02.jpg" alt="Figure 4.2: Example of non-previous information versus temporal sequences"/>
				</div>
			</div>
			<h6>Figure 4.2: Example of non-previous information versus temporal sequences</h6>
			<p>In the previous figure, the image on the left shows us that, without any information, we cannot know what the next action of the black point will be, but if we suppose its previous movements are recorded as the red line on the right-hand side of the graph we can predict what its next action will be.</p>
			<h3 id="_idParaDest-81"><a id="_idTextAnchor101"/>Inside Recurrent Neural Networks</h3>
			<p>So far, we have seen that RNNs are different to neural networks (NNs). RNN neurons are like normal neurons, but with loops within them, allowing them to store a time state. Storing the state of a certain moment in time, they can make predictions based on previous state of time.</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/C13550_04_03.jpg" alt="Figure 4.3: Traditional neuron"/>
				</div>
			</div>
			<h6>Figure 4.3: Traditional neuron</h6>
			<p>The preceding figure shows a traditional neuron, used in an NN. <em class="italics">X</em><em class="italics">n</em> are the inputs of the neuron, and after the activation function, it generates a response. The schema of an RNN neuron is different:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/C13550_04_04_(1).jpg" alt="Figure 4.4: Recurrent neuron"/>
				</div>
			</div>
			<h6>Figure 4.4: Recurrent neuron</h6>
			<p>The loop in the previous figure allows the neuron to store the time state. <em class="italics">h</em><em class="italics">n</em> is the output of the input, <em class="italics">X</em><em class="italics">n</em>, and the previous state. The neuron changes and evolves over time.</p>
			<p>If the input of the neuron is a sequence, an unrolled RNN would be like this:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/C13550_04_05.jpg" alt="Figure 4.5: Unrolled recurrent neuron"/>
				</div>
			</div>
			<h6>Figure 4.5: Unrolled recurrent neuron</h6>
			<p>The chain-like schema in figure 4.5 shows that RNNs are closely related to sequences and lists. So, we have as many neurons as inputs, and each neuron passes its state to the next.</p>
			<h3 id="_idParaDest-82"><a id="_idTextAnchor102"/>RNN architectures</h3>
			<p>Depending on the quantity of inputs and outputs in the RNN, there are many architectures with different numbers of neurons. Each architecture is specialized for a certain task. So far, there are many types of network:</p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/C13550_04_06.jpg" alt="Figure 4.6: Structures of RNNs"/>
				</div>
			</div>
			<h6>Figure 4.6: Structures of RNNs</h6>
			<p>The previous figure shows the various classifications of RNNs. Earlier in this book, we reviewed the one-to-one architecture. In this chapter, we will learn about the many-to-one architecture.</p>
			<ul>
				<li><strong class="keyword">One-to-one</strong>: Classification or regression tasks from one input (image classification).</li>
				<li><strong class="keyword">One-to-many</strong>: Image captioning tasks. These are hard tasks in deep learning. For example, a model that passes an image as an input could describe the elements that are in the picture.</li>
				<li><strong class="keyword">Many-to-one</strong>: Temporal series, sentiment analysis… every task with just one output but based in a sequence of different inputs.</li>
				<li><strong class="keyword">Many-to-many</strong>: Machine automated translation systems.</li>
				<li><strong class="keyword">Synchronized many-to-many</strong>: Video classification.</li>
			</ul>
			<h3 id="_idParaDest-83"><a id="_idTextAnchor103"/>Long-Dependency Problem</h3>
			<p>In some tasks, it is only necessary to use the most recent information to predict the next step of a model. With a temporal series, it is necessary to check older elements to learn or predict the next element or word in a sentence. For example, take a look at this sentence:</p>
			<ul>
				<li>The clouds are in the sky.</li>
			</ul>
			<p>Now imagine this sentence:</p>
			<ul>
				<li>The clouds are in the [?]</li>
			</ul>
			<p>You would assume that the required word would be sky, and you know this because of the previous information:</p>
			<ul>
				<li>The clouds are in the</li>
			</ul>
			<p>But there are other tasks in which the model would need previous information to obtain a better prediction. For example, have a look at this sentence:</p>
			<ul>
				<li>I was born in Italy, but when I was 3, I moved to France… that's the reason why I speak [?]</li>
			</ul>
			<p>To predict the word, the model needs to take the information from the beginning of the sentence, and that could be a problem. This is a problem with RNNs: when the distance to the information is large, it is more difficult to learn. This problem is called the <strong class="keyword">vanishing gradient</strong>.</p>
			<p><strong class="bold">The vanishing gradient problem</strong></p>
			<p>Information travels through time in an RNN so that information from previous steps is used as input in the next step. At each step, the model calculates the cost function, so each time, the model may obtain an error measure. While propagating the error calculated through the network, and trying to minimize that error when updating the weights, the result of that operation is a number closer to zero (if you multiply two small numbers, the result is a smaller number). This means the gradient of the model becomes less and less with each multiplication. The problem here is that the network will not train properly. A solution to this problem with RNNs is to use Long Short-Term Memory (LSTM).</p>
			<h3 id="_idParaDest-84"><a id="_idTextAnchor104"/>Exercise 14: Predict House Prices with an RNN</h3>
			<p>We are going to create our first RNN using Keras. This exercise is not a time-series problem. We are going to use a regression dataset to introduce RNNs. </p>
			<p>We can use several methods included in the Keras library as a model or a type of layer:</p>
			<ul>
				<li>Keras models: These let us use the different available models in Keras. We are going to use the Sequential model.</li>
				<li>Keras layers: We can add different types of layers to our neural network. In this exercise, we are going to use LSTM and a Dense layer. A dense layer is a regular layer of neurons in a neural network. Each neuron receives input from all the neurons in the previous layer, but they are densely connected.</li>
			</ul>
			<p>The main objective of this exercise is to predict the value of a house in Boston, so our dataset will contain information on each house, such as the total area of the property or the number of rooms:</p>
			<ol>
				<li value="1">Import the dataset of Boston house prices from <strong class="inline">sklearn</strong> and take a look at the data:<p class="snippet">from sklearn.datasets import load_boston</p><p class="snippet">boston = load_boston()</p><p class="snippet">boston.data</p><div id="_idContainer117" class="IMG---Figure"><img src="image/C13550_04_07.jpg" alt="Figure 4.7: Boston house-prices data"/></div><h6>Figure 4.7: Boston house prices data</h6></li>
				<li>You can see the data has high values, so the best thing to do is to normalize the data. With the <strong class="inline">MinMaxScaler</strong> function of <strong class="inline">sklearn</strong>, we are going to transform our data into values between 0 and 1:<p class="snippet">from sklearn.preprocessing import MinMaxScaler</p><p class="snippet">import numpy as np</p><p class="snippet"> </p><p class="snippet">scaler = MinMaxScaler()</p><p class="snippet">x = scaler.fit_transform(boston.data)</p><p class="snippet"> </p><p class="snippet">aux = boston.target.reshape(boston.target.shape[0], 1)</p><p class="snippet">y = scaler.fit_transform(aux)</p></li>
				<li>Divide the data into train and test sets. A good percentage for the test set is 20% of the data:<p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet"> </p><p class="snippet">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)</p><p class="snippet">print('Shape of x_train {}'.format(x_train.shape))</p><p class="snippet">print('Shape of y_train {}'.format(y_train.shape))</p><p class="snippet">print('Shape of x_test {}'.format(x_test.shape))</p><p class="snippet">print('Shape of y_test {}'.format(y_test.shape))</p><div id="_idContainer118" class="IMG---Figure"><img src="image/C13550_04_08.jpg" alt="Figure 4.8: Shape of the train and test data"/></div><h6>Figure 4.8: Shape of the train and test data</h6></li>
				<li>Import the Keras libraries and set a seed to initialize the weights:<p class="snippet">import tensorflow as tf</p><p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.layers import Dense</p><p class="snippet">tf.set_random_seed(1)</p></li>
				<li>Create a simple model. The dense layer is just a set of neurons. The last dense layer has only one neuron to return the output:<p class="snippet">model = Sequential()</p><p class="snippet"> </p><p class="snippet">model.add(Dense(64, activation='relu'))</p><p class="snippet">model.add(Dense(32, activation='relu'))</p><p class="snippet">model.add(Dense(1))</p><p class="snippet"> </p><p class="snippet">model.compile(loss='mean_squared_error', optimizer='adam')</p></li>
				<li>Train the network:<p class="snippet">history = model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)</p><div id="_idContainer119" class="IMG---Figure"><img src="image/C13550_04_09.jpg" alt="Figure 4.9: Training the network"/></div><h6>Figure 4.9: Training the network</h6></li>
				<li>Compute the error of the model:<p class="snippet">error = model.evaluate(x_test, y_test)</p><p class="snippet">print('MSE: {:.5f}'.format(error))</p><div id="_idContainer120" class="IMG---Figure"><img src="image/C13550_04_10.jpg" alt="Figure 4.10: Computing the error in the model"/></div><h6>Figure 4.10: Computing the error of the model</h6></li>
				<li>Plot the predictions:<p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet"> </p><p class="snippet">prediction = model.predict(x_test)</p><p class="snippet">print('Prediction shape: {}'.format(prediction.shape))</p><p class="snippet"> </p><p class="snippet">plt.plot(range(len(x_test)), prediction.reshape(prediction.shape[0]), '--r')</p><p class="snippet">plt.plot(range(len(y_test)), y_test)</p><p class="snippet">plt.show()</p></li>
			</ol>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/C13550_04_11.jpg" alt="Figure 4.11: Predictions of our model"/>
				</div>
			</div>
			<h6>Figure 4.11: Predictions of our model</h6>
			<p>Now you have an RNN for a regression problem! You can try to modify the parameters, add more layers, or change the number of neurons to see what happens. In the next exercise, we will solve time-series problems with LSTM layers.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor105"/>Long Short-Term Memory</h2>
			<p><strong class="keyword">LSTM </strong>is a type of RNN that's designed to solve the long-dependency problem. It can remember values for long or short time periods. The principal way it differs from traditional RNNs is that they include a cell or a loop to store the memory internally.</p>
			<p>This type of neural network was created in 1997 by Hochreiter and Schmidhuber. This is the basic schema of an LSTM neuron:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/C13550_04_12.jpg" alt="Figure 4.12: LSTM neuron structure"/>
				</div>
			</div>
			<h6>Figure 4.12: LSTM neuron structure</h6>
			<p>As you can see in the previous figure, the schema of an LSTM neuron is complex. It has three types of gate:</p>
			<ul>
				<li>Input gate: Allows us to control the input values to update the state of the memory cell.</li>
				<li>Forget gate: Allows us to erase the content of the memory cell. </li>
				<li>Output gate: Allows us to control the returned values of the input and cell memory content.</li>
			</ul>
			<p>An LSTM model in Keras has a three-dimensional input:</p>
			<ul>
				<li>Sample: Is the amount of data you have (quantity of sequences).</li>
				<li>Time step: Is the memory of your network. In other words, it stores previous information in order to make better predictions.</li>
				<li>Features: Is the number of features in every time step. For example, if you are processing pictures, the features are the number of pixels.<h4>Note</h4><p class="callout">This complex design causes another type of network to be formed. This new type of neural network is a <strong class="keyword">Gated Recurrent Unit (GRU)</strong>, and it solves the vanishing gradient problem.</p></li>
			</ul>
			<h3 id="_idParaDest-86"><a id="_idTextAnchor106"/>Exercise 15: Predict the Next Solution of a Mathematical Function </h3>
			<p>In this exercise, we are going to build an LSTM to predict the values of a sine function. In this exercise, you will learn how to train and predict a model with Keras, using the LSTM model. Also, this exercise will cover data generation and how to split data into training samples and test samples:</p>
			<ol>
				<li value="1">With Keras, we can create an RNN using the Sequential class, and we can create an LSTM to add new recurrent neurons. Import the Keras libraries for LSTM models, NumPy for setting up the data, and matplotlib to print the graphs:<p class="snippet">import tensorflow as tf</p><p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.layers import LSTM, Dense</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib.pyplot as plt</p></li>
				<li>Create the dataset to train and evaluate the model. We are going to generate an array of 1,000 values as a result of the sine function:<p class="snippet">serie = 1000</p><p class="snippet">x_aux = [] #Natural numbers until serie</p><p class="snippet">x_aux = np.arange(serie)</p><p class="snippet">serie = (np.sin(2 * np.pi * 4 * x_aux / serie) + 1) / 2</p></li>
				<li>To see if the data is good, let's plot it:<p class="snippet">plt.plot(x_aux, serie)</p><p class="snippet">plt.show()</p><div id="_idContainer123" class="IMG---Figure"><img src="image/C13550_04_13.jpg" alt="Figure 4.13: Output with the plotted data"/></div><h6>Figure 4.13: Output with the plotted data</h6></li>
				<li>As this chapter explains, RNN works with sequences of data, so we need to split our data into sequences. In our case, the maximum length of the sequences will be 5. This is necessary because the RNNs need sequences as input.<p>This model will be <strong class="bold">many-to-one</strong> because the input is a sequence and the output is just a value. To see why we are going to create an RNN using the many-to-one structure, we just need to know the dimensions of our input and output data:</p><p class="snippet">#Prepare input data</p><p class="snippet">maxlen = 5</p><p class="snippet">seq = []</p><p class="snippet">res = []</p><p class="snippet">for i in range(0, len(serie) - maxlen):</p><p class="snippet">    seq.append(serie[i:maxlen+i])</p><p class="snippet">    res.append(serie[maxlen+i])</p><p class="snippet">print(seq[:5])</p><p class="snippet">print(res[:5])</p></li>
				<li>Prepare the data to introduce it to the LSTM model. Pay attention to the shape of the <strong class="inline">x</strong> and <strong class="inline">y</strong> variables. RNNs need a three-dimensional vector as input and a two-dimensional vector as output. That's why we will reshape the variables:<p class="snippet">x = np.array(seq)</p><p class="snippet">y = np.array(res)</p><p class="snippet">x = x.reshape(x.shape[0], x.shape[1], 1)</p><p class="snippet">y = y.reshape(y.shape[0], 1)</p><p class="snippet">print('Shape of x {}'.format(x.shape))</p><p class="snippet">print('Shape of y {}'.format(y.shape))</p><div id="_idContainer124" class="IMG---Figure"><img src="image/C13550_04_14.jpg" alt="Figure 4.14: Reshaping the variables"/></div><h6>Figure 4.14: Reshaping the variables</h6><h4>Note</h4><p class="callout">The input dimension of an LSTM is 3.</p></li>
				<li>Split the data into train and test sets:<p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet"> </p><p class="snippet">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)</p><p class="snippet">print('Shape of x_train {}'.format(x_train.shape))</p><p class="snippet">print('Shape of y_train {}'.format(y_train.shape))</p><p class="snippet">print('Shape of x_test {}'.format(x_test.shape))</p><p class="snippet">print('Shape of y_test {}'.format(y_test.shape))</p><div id="_idContainer125" class="IMG---Figure"><img src="image/C13550_04_15.jpg" alt="Figure 4.15: Splitting data as train and test"/></div><h6>Figure 4.15: Splitting data into train and test sets</h6></li>
				<li>Build a simple model with one LSTM unit and one dense layer with one neuron and linear activation. The dense layer is just a regular layer of neurons receiving the input from the previous layer and generating many neurons as output. Because of that, our dense layer has only one neuron because we need a scalar value as the output:<p class="snippet">tf.set_random_seed(1)</p><p class="snippet">model = Sequential()</p><p class="snippet">model.add(LSTM(1, input_shape=(maxlen, 1)))   </p><p class="snippet">model.add(Dense(1, activation='linear'))      </p><p class="snippet">model.compile(loss='mse', optimizer='rmsprop')</p></li>
				<li>Train the model for 5 epochs (one epoch is when the entire dataset is processed by the neural network) and a batch size of 32 and evaluate it:<p class="snippet">history = model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)</p><p class="snippet">error = model.evaluate(x_test, y_test)</p><p class="snippet">print('MSE: {:.5f}'.format(error))</p><div id="_idContainer126" class="IMG---Figure"><img src="image/C13550_04_16.jpg" alt="Figure 4.16: Training with 5 epochs with batch size 32"/></div><h6>Figure 4.16: Training with 5 epochs with a batch size of 32</h6></li>
				<li>Plot the test predictions to see if it works well:<p class="snippet">prediction = model.predict(x_test)</p><p class="snippet">print('Prediction shape: {}'.format(prediction.shape))</p><p class="snippet">plt.plot(range(len(x_test)), prediction.reshape(prediction.shape[0]), '--r')</p><p class="snippet">plt.plot(range(len(y_test)), y_test)</p><p class="snippet">plt.show()</p><div id="_idContainer127" class="IMG---Figure"><img src="image/C13550_04_17.jpg" alt="Figure 4.17: Plotting the predicted shape"/></div><h6>Figure 4.17: Plotting the predicted shape</h6></li>
				<li>Let's improve our model. Create a new one with four units in the LSTM layer and one dense layer with one neuron, but with the sigmoid activation:<p class="snippet">model2 = Sequential()</p><p class="snippet">model2.add(LSTM(4,input_shape=(maxlen,1)))</p><p class="snippet">model2.add(Dense(1, activation='sigmoid'))</p><p class="snippet">model2.compile(loss='mse', optimizer='rmsprop')</p></li>
				<li>Train and evaluate it for 25 epochs and with a batch size of 8:<p class="snippet">history = model2.fit(x_train, y_train,</p><p class="snippet">                     batch_size=8,</p><p class="snippet">                     epochs=25, </p><p class="snippet">                     verbose=1)</p><p class="snippet">error = model2.evaluate(x_test, y_test)</p><p class="snippet">print('MSE: {:.5f}'.format(error))</p><div id="_idContainer128" class="IMG---Figure"><img src="image/C13550_04_18.jpg" alt="Figure 4.18: Training for 25 epochs with batch size 8"/></div><h6>Figure 4.18: Training for 25 epochs with a batch size of 8</h6></li>
				<li>Plot the predictions of the model:<p class="snippet">predict_2 = model2.predict(x_test)</p><p class="snippet">predict_2 = predict_2.reshape(predict_2.shape[0]) </p><p class="snippet">print(x_test.shape)</p><p class="snippet">plt.plot(range(len(x_test)),predict_2, '--r')</p><p class="snippet">plt.plot(range(len(y_test)), y_test)</p><p class="snippet">plt.show()</p></li>
			</ol>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/C13550_04_19.jpg" alt="Figure 4.19: Prediction of our neural network"/>
				</div>
			</div>
			<h6>Figure 4.19: Predictions of our neural network</h6>
			<p>You can now compare the plots of each model, and we can see that the second model is better. With this exercise, you have learned the basics of LSTM, how to train and evaluate the model you have created, and also how to determine whether it is good or not.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor107"/>Neural Language Models</h2>
			<p><em class="italics">Chapter 3</em>, <em class="italics">Fundamentals of Natural Language Processing</em> introduced us to statistical language models (LMs), which are the probability distribution for a sequence of words. We know LMs can be used to predict the next word in a sentence, or to compute the probability distribution of the next word.</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/C13550_04_20.jpg" alt="Figure 4.20: LM formula to compute the probability distribution of an upcoming word"/>
				</div>
			</div>
			<h6>Figure 4.20: LM formula to compute the probability distribution of an upcoming word</h6>
			<p>The sequence of words is <em class="italics">x1</em> , <em class="italics">x2</em> … and the next word is <em class="italics">x</em><em class="italics">t+1</em>. <em class="italics">w</em><em class="italics">j</em> is a word in the vocabulary. <em class="italics">V</em> is the vocabulary and <em class="italics">j</em> is a position of a word in that vocabulary. <em class="italics">w</em><em class="italics">j</em> is the word located in position <em class="italics">j</em> within <em class="italics">V</em>.</p>
			<p>You use LMs every day. The keyboards on cell phones use this technology to predict the next word of a sentence, and search engines such as Google use it to predict what you want to search in their search for engine.</p>
			<p>We talked about the n-gram model and bigrams counting the words in a corpus, but that solution has some limitations, such as long dependencies. Deep NLP and neural LMs will help to get around these limitations. </p>
			<h3 id="_idParaDest-88"><a id="_idTextAnchor108"/>Introduction to Neural Language Models</h3>
			<p>Neural LMs follow the same structure as statistical LMs. They aim to predict the next word in a sentence, but in a different way. A neural LM is motivated by an RNN because of the use of sequences as inputs.</p>
			<p><em class="italics">Exercise 15</em>, <em class="italics">Predict the Next Solution of a Mathematical Function</em> predicts the next result of the sine function from a sequence of five previous steps. In this case, instead of sequences of sine function results, the data is words, and the model will predict the next word.</p>
			<p>These neural LMs emerged from the necessity to improve the statistical approach. Newer models can work around some of the limitations and problems of traditional LMs. </p>
			<p><strong class="bold">Problems of statistical LMs</strong></p>
			<p>In the previous chapter, we reviewed LMs and the concepts of N-grams, bigrams, and the Markov model. These methods are executed by counting occurrences in the text. That's why these methods are called statistical LMs.</p>
			<p>The main problem with LMs is data limitation. What can we do if the probability distribution of the sentence we want to compute does not exist in the data? A partial solution here is the smoothing method, but that is insufficient.</p>
			<p>Another solution is to use the Markov Assumption (each probability only depends on the previous step, simplifying the Chain Rule) to simplify the sentence, but that will not give a good prediction. What this means is, we could simplify our model using 3-grams.</p>
			<p>A solution to this problem is to increase the size of the corpus, but the corpus will end up being to large. These limitations in n-gram models are called <strong class="bold">sparsity problems</strong>.</p>
			<p><strong class="bold">Window-Based Neural Model</strong></p>
			<p>A first approximation of this new model was the use of a sliding window to compute the probabilities of the next word. The concept of this solution comes from window classification.</p>
			<p>In terms of words, it is hard to understand the meaning of a single word without any context. There are many problems if that word is not in a sentence or in a paragraph, for example, ambiguity between two similar words or auto-antonyms. Auto-antonyms are words with multiple meanings. The word handicap, depending on its context, can mean an advantage (for example, in sport) or a disadvantage (sometimes offensive, a physical problem).</p>
			<p>Window classification classifies a word in the context (created by the window) of its neighboring words. The approach of a sliding window can be used to generate an LM. Here is a graphical example: </p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/C13550_04_21.jpg" alt="Figure 4.21: Window-based neural LM"/>
				</div>
			</div>
			<h6>Figure 4.21: Window-based neural LM</h6>
			<p>In the previous figure, there is an example of how a window-based neural model works. The window size is 5 (word1 to word5). It creates a vector joining the embedding vector of each word, and computes this in a hidden layer:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/C13550_04_22.jpg" alt="Figure 4.22: Hidden layer formula"/>
				</div>
			</div>
			<h6>Figure 4.22: Hidden layer formula</h6>
			<p>And finally, to predict a word, the model returns a value that can be used to classify the probability of the word:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/C13550_04_23.jpg" alt="Figure 4.23: Softmax function"/>
				</div>
			</div>
			<h6>Figure 4.23: Softmax function</h6>
			<p>Then, the word with the highest value will be the predicted word.</p>
			<h4>Note</h4>
			<p class="callout">We are not going to go deeper into these terms because we will use an LSTM to create the LM.</p>
			<p>The benefits of this approach over the traditional one are as follows:</p>
			<ul>
				<li>Less computational work. Window-based neural models need less computational resources because they don't need to iterate through the corpus computing probabilities.</li>
				<li>It avoids the problem of changing the dimension of the N-gram to find a good probability distribution.</li>
				<li>The generated text will have more sense in terms of meaning because this approach solves the sparsity problem.</li>
			</ul>
			<p>But there are some problems:</p>
			<ul>
				<li>Window limitations: The size of the window cannot be large, so the meaning of some words could be wrong.</li>
				<li>Each window has its own weight value, so it can cause ambiguity.</li>
				<li>If the window grows in size, the model grows too.</li>
			</ul>
			<p>Analyzing the problems with the window model, an RNN can improve its performance. </p>
			<h3 id="_idParaDest-89"><a id="_idTextAnchor109"/>RNN Language Model</h3>
			<p>An RNN is able to compute the probabilities of an upcoming word in a sequence of previous steps. The core idea of this approach is to apply the same weights repeatedly throughout the process of training.</p>
			<p>There are some advantages of using an RNN LM over a window-based model:</p>
			<ul>
				<li>This architecture can process any length sentence; it does not have a fixed size, unlike the window-based approach.</li>
				<li>The model is the same for every input size. It will not grow if the input is larger.</li>
				<li>Depending on the NN architecture, it can use information from the previous steps and from the steps ahead.</li>
				<li>The weights are shared across the timesteps.</li>
			</ul>
			<p>So far, we have talked about different ways to improve the statistical LM and the pros and cons of each one. Before developing an RNN LM, we need to know how to introduce a sentence as input in the NN.</p>
			<p><strong class="bold">One-hot encoding</strong></p>
			<p>Neural networks and machine learning are about numbers. As we have seen throughout this book, input elements are numbers and outputs are codified labels. But if a neural network has a sentence or a set of characters as input, how can it transform this into numerical values? </p>
			<p>One-hot encoding is a numerical representation of discrete variables. It assumes a feature vector with the same size for different values within a discrete set of variables. This means that if there is a corpus of size 10, each word will be codified as a vector of length 10. So, each dimension corresponds to a unique element of the set.</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/C13550_04_24.jpg" alt="Figure 4.24: RNN pre-processing data flow"/>
				</div>
			</div>
			<h6>Figure 4.24: RNN pre-processing data flow</h6>
			<p>The previous figure shows how one-hot encoding works. It is important to understand the shapes of each vector because the neural network needs to understand what input data we have and what output we want to obtain. Next, <em class="italics">Exercise 16, Encoding a small Corpus</em> will help you examine the fundamentals of one-hot encoding in more detail.</p>
			<h3 id="_idParaDest-90"><a id="_idTextAnchor110"/>Exercise 16: Encoding a Small Corpus</h3>
			<p>In this exercise, we are going to learn how to encode a set of words using one-hot encoding. It is the most basic encoding method, and it gives us a representation of discrete variables.</p>
			<p>This exercise will cover different ways of performing this task. One way is to manually perform encoding, while another way is to use libraries. After finishing the exercise, we will obtain a vector representation of each word, ready to use as the input for a neural network:</p>
			<ol>
				<li value="1">Define a corpus. This corpus is the same one that we used in <em class="italics">Chapter 3</em>, <em class="italics">Fundamentals of Natural Language Processing</em>:<p class="snippet">corpus = [</p><p class="snippet">     'My cat is white',</p><p class="snippet">     'I am the major of this city',</p><p class="snippet">     'I love eating toasted cheese',</p><p class="snippet">     'The lazy cat is sleeping',</p><p class="snippet">]</p></li>
				<li>Tokenize it using <strong class="inline">spaCy</strong>. We are not going to use the stop-words (erasing useless words, such as articles) method because we have a small corpus. We want all the tokens:<p class="snippet">import spacy</p><p class="snippet">import en_core_web_sm</p><p class="snippet">nlp = en_core_web_sm.load()</p><p class="snippet"> </p><p class="snippet">corpus_tokens = []</p><p class="snippet">for c in corpus:</p><p class="snippet">    doc = nlp(c)</p><p class="snippet">    tokens = []</p><p class="snippet">    for t in doc:</p><p class="snippet">        tokens.append(t.text)</p><p class="snippet">    corpus_tokens.append(tokens)</p><p class="snippet">corpus_tokens</p></li>
				<li>Create a list with every unique token in the corpus:<p class="snippet">processed_corpus = [t for sentence in corpus_tokens for t in sentence]</p><p class="snippet">processed_corpus = set(processed_corpus)</p><p class="snippet">processed_corpus</p><div id="_idContainer135" class="IMG---Figure"><img src="image/C13550_04_25.jpg" alt="Figure 4.25: List with each unique token in the corpus"/></div><h6>Figure 4.25: List with each unique token in the corpus</h6></li>
				<li>Create a dictionary with each word in the corpus as the key and a unique number as the value. This dictionary will look like {word:value}, and this value will have the index of 1 in the one-hot encoded vector:<p class="snippet">word2int = dict([(tok, pos) for pos, tok in enumerate(processed_corpus)])</p><p class="snippet">word2int</p><div id="_idContainer136" class="IMG---Figure"><img src="image/C13550_04_26.jpg" alt="Figure 4.26: Each word as a key and a unique number as value"/></div><h6>Figure 4.26: Each word as a key and a unique number as a value</h6></li>
				<li>Encode a sentence. This way of performing encoding is manual. There are some libraries, such as sklearn, that provide automatic encoding methods:<p class="snippet">Import numpy as np</p><p class="snippet">sentence = 'My cat is lazy'</p><p class="snippet">tokenized_sentence = sentence.split()</p><p class="snippet">encoded_sentence = np.zeros([len(tokenized_sentence),len(processed_corpus)])</p><p class="snippet">encoded_sentence</p><p class="snippet">for i,c in enumerate(sentence.split()):</p><p class="snippet">    encoded_sentence[i][ word2int[c] ] = 1</p><p class="snippet">encoded_sentence</p><div id="_idContainer137" class="IMG---Figure"><img src="image/C13550_04_27.jpg" alt=""/></div><h6>Figure 4.27: Manual one-hot encoded vectors.</h6><p class="snippet">print("Shape of the encoded sentence:", encoded_sentence.shape)</p></li>
				<li>Import the <strong class="inline">sklearn</strong> methods. sklearn first encodes each unique token in the corpus with <strong class="inline">LabelEncoder</strong>, and then uses <strong class="inline">OneHotEncoder</strong> to create the vectors:<p class="snippet">from sklearn.preprocessing import LabelEncoder</p><p class="snippet">from sklearn.preprocessing import OneHotEncoder</p><p class="snippet">Declare the LabelEncoder() class.</p><p class="snippet">le = LabelEncoder()</p><p class="snippet">Encode the corpus with this class.</p><p class="snippet">labeled_corpus = le.fit_transform(list(processed_corpus))</p><p class="snippet">labeled_corpus</p><div id="_idContainer138" class="IMG---Figure"><img src="image/C13550_04_28.jpg" alt=""/></div><h6>Figure 4.28: Vectors created with OneHotEncoder</h6></li>
				<li>Now, take the same sentence that we encoded before and apply the <strong class="inline">LabelEncoder</strong> transform method we created:<p class="snippet">sentence = 'My cat is lazy'</p><p class="snippet">tokenized_sentence = sentence.split()</p><p class="snippet">integer_encoded = le.transform(tokenized_sentence)</p><p class="snippet">integer_encoded</p><div id="_idContainer139" class="IMG---Figure"><img src="image/C13550_04_29.jpg" alt="Figure 4.29: LabelEncoder transform applied"/></div><h6>Figure 4.29: LabelEncoder transform applied</h6></li>
				<li>We can decode <strong class="inline">LabelEncoder</strong> in the initial sentence:<p class="snippet">le.inverse_transform(integer_encoded)</p><div id="_idContainer140" class="IMG---Figure"><img src="image/C13550_04_30.jpg" alt="Figure 4.30: Decoded LabelEncoder"/></div><h6>Figure 4.30: Decoded LabelEncoder</h6></li>
				<li>Declare <strong class="inline">OneHotEncoder</strong> with <strong class="inline">sparse=False</strong> (if you do not specify this, it will return a sparse matrix):<p class="snippet">onehot_encoder = OneHotEncoder(sparse=False)</p></li>
				<li>To encode our sentence with the label encoder that we have created, we need to reshape our labeled corpus to fit it into the <strong class="inline">onehot_encoder</strong> method:<p class="snippet">labeled_corpus = labeled_corpus.reshape(len(labeled_corpus), 1)</p><p class="snippet">onehot_encoded = onehot_encoder.fit(labeled_corpus)</p></li>
				<li>Finally, we can transform our sentence (encoded with LabelEncoder) into a one-hot vector. The results of this way of encoding and manual encoding will not be the same, but they will have the same shape:<p class="snippet">sentence_encoded = onehot_encoded.transform(integer_encoded.reshape(len(integer_encoded), 1))</p><p class="snippet">print(sentence_encoded)</p></li>
			</ol>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/C13550_04_31.jpg" alt="Figure 4.31: One-hot encoded vectors using Sklearn methods"/>
				</div>
			</div>
			<h6>Figure 4.31: One-hot encoded vectors using Sklearn methods</h6>
			<h4>Note</h4>
			<p class="callout">This exercise is really important. If you do not understand the shapes of the matrices, it will be very hard to understand the inputs of RNNs.</p>
			<p>Good job! You have finished <em class="italics">Exercise 16</em>. Now you can encode discrete variables into vectors. This is part of pre-processing data to train and evaluate a neural network. Next, we have the activity of the chapter, the objective of which is to create an LM using RNNs and one-hot encoding.</p>
			<h4>Note</h4>
			<p class="callout">For larger corpuses, one-hot encoding is not very useful because it would create huge vectors for the words. Instead, it is normal to use an embedding vector. This concept will be covered later in this chapter.</p>
			<h3 id="_idParaDest-91"><a id="_idTextAnchor111"/>The Input Dimensions of RNNs</h3>
			<p>Before getting started with the RNN activity, you may not understand input dimensions. In this section, we will focus on understanding the shape of the n-dimensional arrays, and how we can add a new dimension or erase one.</p>
			<p><strong class="bold">Sequence data format</strong></p>
			<p>We've mentioned the many-to-one architecture, where each sample consists of a fixed sequence and a label. That label corresponds with the upcoming value in the sequence. It is something like this:</p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/C13550_04_32.jpg" alt="Figure 4.32: Format of sequence data"/>
				</div>
			</div>
			<h6>Figure 4.32: Format of sequence data</h6>
			<p>In this example, we have two sequences in matrix X, and the two output labels in Y. So, the shapes are as follows:</p>
			<p>X = (2, 4)</p>
			<p>Y = (2)</p>
			<p>But if you tried to insert this data into an RNN, it wouldn't work because it does not have the correct dimensions.</p>
			<p><strong class="bold">RNN data format</strong></p>
			<p>To implement an RNN with temporal sequences in Keras, the model will need an input vector with three dimensions and, as output, one vector with two dimensions.</p>
			<p>So, for the X matrix, we will have the following:</p>
			<ul>
				<li>Number of samples</li>
				<li>Sequence length</li>
				<li>Value length</li>
			</ul>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/C13550_04_33.jpg" alt="Figure 4.33: RNN data format"/>
				</div>
			</div>
			<h6>Figure 4.33: RNN data format</h6>
			<p>The shapes here are as follows:</p>
			<p>X = (2, 4, 1)</p>
			<p>Y = (2, 1)</p>
			<p><strong class="bold">One-hot format</strong></p>
			<p>With one-hot encoding, we have the same dimensions as input, but the value length changes. In the preceding figure, we can see the values ([1], [2], …) with one-dimensionality. But with one-hot encoding, these values will change to vectors, so the shape would be as follows:</p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/C13550_04_34.jpg" alt="Figure 4.34: One-hot format"/>
				</div>
			</div>
			<h6>Figure 4.34: One-hot format</h6>
			<p>X = (2, 4, 3)</p>
			<p>Y = (2, 3)</p>
			<p>To perform all these changes to the dimensions, the <strong class="bold">reshape</strong> method from the NumPy library will be used.</p>
			<h4>Note</h4>
			<p class="callout">With this knowledge of dimensions, you can start the activity, and remember, the input dimension of an LSTM is three and the output dimension is two. So, if you create two LSTM layers continuously, how can you add the third dimension to the output of the first layer? Change the return state to True.</p>
			<h3 id="_idParaDest-92"><a id="_idTextAnchor112"/>Activity 4: Predict the Next Character in a Sequence</h3>
			<p>In this activity, we will predict the upcoming character in a long sequence. The activity has to be performed using one-hot encoding to create the input and output vectors. The architecture of the model will be an LSTM, as we saw in <em class="italics">Exercise 14</em>, <em class="italics">Predict Houses Prices with an RNN</em>.</p>
			<p>Scenario: You work in a global company as the security manager. One morning, you notice a hacker has discovered and changed all the passwords for the company's databases. You and your team of engineers start trying to decode the hacker's passwords to enter the system and fix everything. After analyzing all the new passwords, you see a common structure. </p>
			<p>You only need to decode one more character in the password, but you don't know what the character is and you only have one more opportunity to get the correct password.</p>
			<p>Then, you decide to create a program that analyzes long sequences of data and the five characters of the password you already know. With this information, it can predict the last character of the password.</p>
			<p>The first five characters of the password are: tyuio. What will the last character be? </p>
			<h4>Note</h4>
			<p class="callout">You have to use one-hot encoding and LSTM. You will train your model with one-hot encoded vectors.</p>
			<ol>
				<li value="1">This is the sequence of data: qwertyuiopasdfghjklñzxcvbnm<h4>Note</h4><p class="callout">This sequence is repeated 100 times, so do this: sequence = 'qwertyuiopasdfghjklñzxcvbnm' * 100.</p></li>
				<li>Divide the data into sequences of five characters and prepare the output data.</li>
				<li>Encode the input and the output sequences as one-hot encoded vectors.</li>
				<li>Set the train and test data.</li>
				<li>Design the model.<h4>Note</h4><p class="callout">The output has many zeroes, so it is hard to achieve an exact result. Use the LeakyRelu activation function with an alpha of 0.01, and when you do the prediction, round off the value of that vector.</p></li>
				<li>Train and evaluate it.</li>
				<li>Create a function that, when given five characters, predicts the next one in order to work out the last character of the password.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 308.</p></li>
			</ol>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor113"/>Summary</h2>
			<p>AI and deep learning are making huge advances in terms of images and artificial vision thanks to convolutional networks. But RNNs also have a lot of power. </p>
			<p>In this chapter, we reviewed how a neural network would can to predict the values of a sine function using temporal sequences. If you change the training data, this architecture can learn about stock movements for each distribution. Also, there are many architectures for RNNs, each of which is optimized for a certain task. But RNNs have a problem with vanishing gradients. A solution to this problem is a new model, called LSTM, which changes the structure of a neuron to memorize timesteps. </p>
			<p>Focusing on linguistics, statistical LMs have many problems related with computational load and distribution probabilities. To solve the sparsity problem, the size of the n-gram model was lowered to 4 or 3 grams, but that was an insufficient number of steps back to predict an upcoming word. If we use this approach, the sparsity problem appears. A neural LM with a fixed window size can prevent the sparsity problem, but there are still problems with the limited size of the window and the weights. With RNNs, these problems do not arise, and depending on the architecture, it can obtain better results, looking many steps back and forward. But deep learning is about vectors and numbers. When you want to predict words, you need to encode the data to train the model. There are various different methods, such as the one-hot encoder or the label encoder. You can now generate text from a trained corpus and an RNN. </p>
			<p>In the next chapter, we will talk about Convolutional Neural Networks (CNNs). We will review the fundamental techniques and architectures of CNNs, and also look at more complex implementations, such as transfer learning.</p>
		</div>
	</body></html>