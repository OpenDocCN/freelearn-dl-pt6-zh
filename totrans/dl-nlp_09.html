<html><head></head><body>
		<div class="Content" id="_idContainer260">
			<h1 id="_idParaDest-190"><em class="italics"><a id="_idTextAnchor217"/>Chapter 9</em></h1>
		</div>
		<div class="Content" id="_idContainer261">
			<h1 id="_idParaDest-191"><a id="_idTextAnchor218"/>A Practical NLP Project Workflow in an Organization</h1>
		</div>
		<div class="Content" id="_idContainer262">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Identify the requirements of a natural language processing project</li>
				<li class="bullets">Understand how different teams in an organization might be involved</li>
				<li class="bullets">Use Google Colab notebooks to leverage a GPU to train Deep Learning models</li>
				<li class="bullets">Deploy a model on AWS to be used as Software as a Service (SaaS)</li>
				<li class="bullets">Get acquainted with a simple tech stack for deployment</li>
			</ul>
			<p>In this chapter, we will be looking at a real-time NLP project and its flow in an organization,right till the final stage through the entire chapter.</p>
		</div>
		<div class="Content" id="_idContainer293">
			<h2 id="_idParaDest-192"><a id="_idTextAnchor219"/>Introduction</h2>
			<p>Up to this point in the book, we have studied several deep learning techniques that can be applied to solve specific problems in the NLP domain. Having knowledge of these techniques has empowered us to build good models and deliver high-quality performance. However, when it comes to delivering a working machine learning product in an organization, several other aspects need to be considered.</p>
			<p>In this chapter, we will go through a practical project workflow when delivering a working deep learning system in an organization. Specifically, you will be introduced to the possible roles of various teams within your organization, building a deep learning pipeline and, finally, delivering your product in the form of SaaS.</p>
			<h3 id="_idParaDest-193"><a id="_idTextAnchor220"/>General Workflow for the Development of a Machine Learning Product</h3>
			<p>Today, there are several ways of working with data science in an organization. Most organizations have a workflow that is specific to their environment. Some example workflows are as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer263">
					<img alt="Figure 9.1: General workflow for the development of a machine learning product" src="image/C13783_9_01.jpg"/>
				</div>
			</div>
			<h6>Figure 9.1: General workflow for the development of a machine learning product</h6>
			<h3 id="_idParaDest-194"><a id="_idTextAnchor221"/>The Presentation Workflow:</h3>
			<div>
				<div class="IMG---Figure" id="_idContainer264">
					<img alt="" src="image/C13783_9_02.jpg"/>
				</div>
			</div>
			<h6>Figure 9.2: General presentation workflow</h6>
			<p>The presentation workflow can be elaborated as follows:</p>
			<ol>
				<li>The data science team receives a request to solve a problem using machine learning. The requester could be some other team within the organization or some other company that has hired you as consultants.</li>
				<li>You obtain the relevant data and apply specific machine learning techniques.</li>
				<li>You showcase the results and insights in the form of a report/presentation to the stakeholders. This could also be a potential way to approach the <em class="italics">Proof of Concept </em>(<em class="italics">PoC</em>) phase of a project.</li>
			</ol>
			<h3 id="_idParaDest-195"><a id="_idTextAnchor222"/>The Research Workflow:</h3>
			<div>
				<div class="IMG---Figure" id="_idContainer265">
					<img alt="" src="image/C13783_9_03.jpg"/>
				</div>
			</div>
			<h6>Figure 9.3: Research workflow</h6>
			<p>The main focus of this approach is to conduct research to solve a particular problem that caters to a use case. The solution can be leveraged both by the organization as well as the community in general. Other factors that distinguish this workflow from the presentation workflow are as follows:</p>
			<ul>
				<li>The timelines for such projects are typically longer than those imposed on presentation workflows.</li>
				<li>The deliverable is in the form of research papers and/or toolboxes.</li>
			</ul>
			<p>The workflow can be broken down as follows:</p>
			<ol>
				<li value="1">Your organization has a research wing that wishes to enhance the existing machine learning state in the community, while also allowing your company to leverage the results.</li>
				<li>Your team goes through the existing research that caters to the problem you are being asked to solve. This involves reading research papers in detail and implementing them to establish the baseline performance on some datasets suggested in the research papers.</li>
				<li>You then either try to tailor the existing research to solve your problem or come up with novel ways to solve it yourself.</li>
				<li>The end product could be research papers and/or toolboxes.</li>
			</ol>
			<h3 id="_idParaDest-196"><a id="_idTextAnchor223"/>The Production-Oriented Workflow</h3>
			<div>
				<div class="IMG---Figure" id="_idContainer266">
					<img alt="Figure 9.4: Production-oriented workflow&#13;&#10;" src="image/C13783_9_04.jpg"/>
				</div>
			</div>
			<h6>Figure 9.4: Production-oriented workflow</h6>
			<p>The workflow can be elaborated on as follows:</p>
			<ol>
				<li value="1">The data science team receives a request to solve a problem using machine learning. The requester could be some other team within the organization or another company that has hired you as consultants. It could also be that the data science team wishes to build a product that they think will bring value to the organization.</li>
				<li>You obtain the data, do the necessary research, and build the machine learning model. The data could be obtained either from within the organization or, if the problem is general enough (for example: language translation), it could also be an open source dataset. The model built could, hence, qualify as <em class="italics">PoC</em> to be shown to the stakeholders.</li>
				<li>You define a Minimum Viable Product (MVP): for example, a machine learning model in the form of SaaS.</li>
			</ol>
			<p>Once MVP is achieved, you iteratively add other aspects, such as <em class="italics">Data Acquisition Pipelines</em>, <em class="italics">Continuous Integration</em>, <em class="italics">Monitoring </em>and so on.</p>
			<p>You will notice that even the sample workflows share components. In this chapter, our focus will be on part of <em class="italics">The Production Workflow</em>. We will build a Minimum Viable Product for a specific problem.</p>
			<h2 id="_idParaDest-197">Prob<a id="_idTextAnchor224"/>lem Definition</h2>
			<p>Let's say that you work for an e-commerce platform, through which your customers can purchase a variety of products. The merchandising department of your company comes up with a request to add a feature to the website – '<strong class="bold">Addition of a slider that contains the 5 items that received the most positive reviews in a given calendar week</strong>'.</p>
			<p>This request is first made to the web development department since, ultimately, they are the ones responsible for displaying the website contents. The web development department realizes that, to get a review rating, the data science team needs to be involved. The data science team receives the request from the web development team – '<strong class="bold">We need a web service that takes a string of text as input and returns a score that indicates the degree to which the text represents a positive sentiment</strong>'.</p>
			<p>The data science team then refines the requirements and agrees upon the definition of a Minimum Viable Product (MVP) with the web development team:</p>
			<ol>
				<li value="1">The deliverable will be a web service deployed on an AWS EC2 instance.</li>
				<li>The input to the web service will be a post request containing four reviews (that is, a single post request to the service will contain four reviews).</li>
				<li>The output of the web service will be a set of four scores that correspond to each input text.</li>
				<li>The output score will be on a scale from 1 to 5, with 1 being the least and 5 being the most positive review.</li>
			</ol>
			<h2 id="_idParaDest-198">Data<a id="_idTextAnchor225"/> Acquisition</h2>
			<p>A big contribution toward determining the performance of any machine learning model is the quality and quantity of the data.</p>
			<p>Usually, a data warehousing team/infrastructure team (DWH) is responsible for maintaining the data-related infrastructure at a company. The team takes care that data is never lost, that the underlying infrastructure is stable, and that data is always available for any team that might be interested in using it. The data science team, being one of the consumers of the data, contacts the DWH team, which grants them access to a database that contains all the reviews for various items in the product catalog of the company.</p>
			<p>Typically, there are multiple data fields/tables in the database, some of which may not be important for the machine learning model development.</p>
			<p>A data engineer (a part of the DWH team/member of another team/member of your team) then connects to the database, processes the data into a tabular format, and generates a flat file in the <strong class="bold">csv</strong> format. A discussion between the data scientist and the data engineer at this point results in the retention of only three columns from the database table:</p>
			<ul>
				<li>'Rating': A score on the scale of 1 to 5 that indicates the degree to which a positive sentiment is represented</li>
				<li>'Review Title': A simple title for the review</li>
				<li>'Review': Actual review text</li>
			</ul>
			<p>Notice that all three fields are inputs from customers (users of your e-commerce platform). Additionally, fields such as '<em class="italics">item id</em>' are not retained since they are not required to build this machine learning model for sentiment classification. The removal and retention of such information is also a product of discussions between the DS team, data engineers, and the DWH team.</p>
			<p>It might have been the case that the current data is devoid of sentiment ratings. In such a case, one common solution is to manually go through each review and assign it a sentiment score for the purpose of obtaining training data for the model. However, as you can imagine, doing so for millions of reviews is a daunting task. Thus, crowdsourcing services such as <em class="italics">Amazon Mechanical Turk</em><span class="None"> can be utilized to annotate the data and get training labels for it.</span></p>
			<h4>Note</h4>
			<p class="callout">For more information on Amazon Mechanical Turk, refer to <a href="">https://www.mturk.com/</a>.</p>
			<h2 id="_idParaDest-199">Goog<a id="_idTextAnchor226"/>le Colab</h2>
			<p><span class="None">You are familiar with the intense computational requirements of deep learning models. On a CPU, it would take a remarkably long time to train a deep learning model with lots of training data. Hence, to keep training times practical, it is common practice to use cloud-based services that offer Graphics Processing Units (GPU) to speed up computations. You can expect a speedup of 10-30 times when compared to running the training session on a CPU. The exact amount of speedup, of course, depends upon the power of the GPU, the amount of data involved, and the processing steps.</span></p>
			<p>There are many vendors offering such cloud services, such as <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), <strong class="bold">Microsoft Azure</strong> and others. Google offers an environment/IDE called <strong class="bold">Google Colab</strong>, which offers up to 12 hours of free GPU usage per day for anyone looking to train deep learning models. Additionally, the code is run on a <strong class="bold">Jupyter</strong>-like notebook. In this chapter, we will leverage the power of Google Colab to develop our deep learning-based sentiment classifier.</p>
			<p>In order to familiarize yourself with Google Colab, you are urged to go through a tutorial for it.</p>
			<h4>Note</h4>
			<p class="callout">Before proceeding further, refer to the tutorial at https://colab.research.google.com/notebooks/welcome.ipynb#recent=true</p>
			<p>The following steps should acquaint you well with Google Colab:</p>
			<ol>
				<li value="1">To<span class="None"> open a new blank </span><strong class="bold">colab</strong><span class="None"> notebook, go to </span><a href="">https://colab.research.google.com/notebooks/welcome.ipynb</a><span class="None">, select </span>'<strong class="bold">File</strong>' <span class="None">from the menu, and then select the </span>'<strong class="bold">New Python 3 notebook</strong>' <span class="None">option, as shown in the screenshot:</span><div class="IMG---Figure" id="_idContainer267"><img alt="" src="image/C13783_9_05.jpg"/></div><h6>Figure 9.5: A new Python notebook on Google Colab</h6></li>
				<li><span class="None">Next, rename the notebook any name of your choice. Then, to use a </span><strong class="bold">GPU</strong><span class="None"> for training, we need to select a </span><strong class="bold">GPU</strong><span class="None"> as the runtime. To do so, choose the </span>'<strong class="bold">Edit</strong>' <span class="None">option from the menu and select </span>'<strong class="bold">Notebook Settings</strong><span class="None">'.</span><div class="IMG---Figure" id="_idContainer268"><img alt="Figure 9.6: Edit dropdown in Google Colab&#13;&#10;" src="image/C13783_9_06.jpg"/></div><h6>Figure 9.6: Edit dropdown in Google Colab</h6></li>
				<li><span class="None">A menu pops up with a </span>'<strong class="bold">Hardware Accelerator</strong>' <span class="None">field</span>, <span class="None">which is set to </span>'<strong class="bold">None</strong>' <span class="None">by default:</span><div class="IMG---Figure" id="_idContainer269"><img alt="Figure 9.7: Notebook settings for Google Colab&#13;&#10;" src="image/C13783_9_07.jpg"/></div><h6>Figure 9.7: Notebook settings for Google Colab</h6></li>
				<li><span class="None">A dropdown can be used at this point to select </span>'<strong class="bold">GPU</strong>' <span class="None">as the option:</span><div class="IMG---Figure" id="_idContainer270"><img alt="Figure 9.8: GPU hardware accelerator &#13;&#10;" src="image/C13783_9_08.jpg"/></div><h6>Figure 9.8: GPU hardware accelerator </h6></li>
				<li><span class="None">To check whether the GPU has, in fact, been allotted to your notebook, run the following snippet:</span><p class="snippet"><span class="None"># Check if GPU is detected</span></p><p class="snippet"><span class="None">import tensorflow as tf</span></p><p class="snippet"><span class="None">tf.test.gpu_device_name()</span></p><p><span class="None">The output of running this snippet should indicate the GPU's availability:</span></p><div class="IMG---Figure" id="_idContainer271"><img alt="Figure 9.9: Screenshot for GPU device name&#13;&#10;" src="image/C13783_9_09.jpg"/></div><h6>Figure 9.9: Screenshot for GPU device name</h6><p><span class="None">The output is the GPU device name.</span></p></li>
				<li><span class="None">Next, the data needs to be made accessible within the notebook. There are a number of ways to do this. One way to accomplish this task is by moving the data to a personal Google Drive location. It</span>'<span class="None">s better to move the data in a zipped format to avoid using up too much space on the drive. Go ahead and create a new folder on Google Drive and move the zipped CSV data file within the folder.  Next, we mount the Google Drive onto the Colab notebook machine to make the drive data available for use within the Colab notebook:</span><p class="snippet"><span class="None">from google.colab import drive</span></p><p class="snippet"><span class="None">drive.mount('/content/gdrive')</span></p><p><span class="None">The snippet we just mentioned would return a weblink for authorization. Upon clicking on that link, a new browser tab opens up containing an authorization code that should be copied and pasted onto the notebook prompt:</span></p><div class="IMG---Figure" id="_idContainer272"><img alt="Figure 9.10: Screenshot for importing data from Google Drive&#13;&#10;" src="image/C13783_9_10.jpg"/></div><h6>Figure 9.10: Screenshot for importing data from Google Drive</h6><p><span class="None">At this point, all the data within your Google Drive is available for use within the Colab notebook.</span></p></li>
				<li><span class="None">Next, navigate to the folder location where the zipped data is present:</span><p class="snippet"><span class="None">cd "/content/gdrive/My Drive/Lesson-9/"</span></p></li>
				<li><span class="None">Confirm that you have navigated to the desired location by issuing a </span>'<strong class="inline">pwd</strong>' <span class="None">command in the notebook cell:</span><div class="IMG---Figure" id="_idContainer273"><img alt="Figure 9.11: Data imported on the Colab notebook from Google Drive&#13;&#10;" src="image/C13783_9_11.jpg"/></div><h6>Figure 9.11: Data imported on the Colab notebook from Google Drive</h6></li>
				<li><span class="None">Next, unzip the zipped data file using the </span><strong class="inline">unzip</strong><span class="None"> command:</span><p class="snippet"><span class="None">!unzip data.csv.zip</span></p><p><span class="None">This will result in the following output:</span></p><div class="IMG---Figure" id="_idContainer274"><img alt="Figure 9.12: Unzipping a data file on a Colab notebook&#13;&#10;" src="image/C13783_9_12.jpg"/></div><h6>Figure 9.12: Unzipping a data file on a Colab notebook</h6><p><span class="None">The </span>'<strong class="bold">MACOSX</strong>' <span class="None">output lines are operating system-specific and may not be the same for everyone. Anyhow, an unzipped data file, </span>'<strong class="bold">data.csv</strong>' <span class="None">is now available for use within the Colab notebook.</span></p></li>
				<li><span class="None">Now that we have the data available and the environment to use the GPU is set, we can start coding up the model. We will import the required packages first</span>:<p class="snippet"><span class="None">import os</span></p><p class="snippet"><span class="None">import re</span></p><p class="snippet"><span class="None">import pandas as pd</span></p><p class="snippet"><span class="None">from keras.preprocessing.text import Tokenizer</span></p><p class="snippet"><span class="None">from keras.preprocessing.sequence import pad_sequences</span></p><p class="snippet"><span class="None">from keras.models import Sequential</span></p><p class="snippet"><span class="None">from keras.layers import Dense, Embedding, LSTM</span></p></li>
				<li><span class="None">Next, we will write a preprocessing function that turns all the text to lowercase and removes any numbers:</span><p class="snippet"><span class="None">def preprocess_data(data_file_path):</span></p><p class="snippet"><span class="None">    data = pd.read_csv(data_file_path, header=None) # read the csv</span></p><p class="snippet"><span class="None">    data.columns = ['rating', 'title', 'review'] # add column names</span></p><p class="snippet"><span class="None">    data['review'] = data['review'].apply(lambda x: x.lower()) # change all text to lower</span></p><p class="snippet"><span class="None">    data['review'] = data['review'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x))) # remove all numbers</span></p><p class="snippet"><span class="None">    return data</span></p></li>
				<li><span class="None">Note that we are using pandas for reading and processing texts. Let</span>'<span class="None">s run this function with the path of our CSV file:</span><p class="snippet"><span class="None">df = preprocess_data('data.csv')</span></p></li>
				<li><span class="None">We can now examine the contents of the dataframe:</span><div class="IMG---Figure" id="_idContainer275"><img alt="Figure 9.13: Screenshot of dataframe contents&#13;&#10;" src="image/C13783_9_13.jpg"/></div><h6>Figure 9.13: Screenshot of dataframe contents</h6></li>
				<li><span class="None">As expected, we have three fields. Also, we see that the </span>'<strong class="inline">review</strong>' <span class="None">column has much more text than the </span>'<strong class="inline">title</strong>' <span class="None">column. So, we choose to use only the </span>'<strong class="inline">review</strong>' <span class="None">column for developing the model. We'll now proceed with tokenizing the text:</span><p class="snippet"><span class="None"># initialize tokenization</span></p><p class="snippet"><span class="None">max_features = 2000</span></p><p class="snippet"><span class="None">maxlength = 250</span></p><p class="snippet"><span class="None">tokenizer = Tokenizer(num_words=max_features, split=' ')</span></p><p class="snippet"><span class="None"># fit tokenizer</span></p><p class="snippet"><span class="None">tokenizer.fit_on_texts(df['review'].values)</span></p><p class="snippet"><span class="None">X = tokenizer.texts_to_sequences(df['review'].values)</span></p><p class="snippet"><span class="None"># pad sequences</span></p><p class="snippet"><span class="None">X = pad_sequences(X, maxlen=maxlength)</span></p><p><span class="None">Here, we have restricted the feature count to 2,000 words. We then apply the tokenizer with the maximum features to the </span>'<span class="None">review</span>' <span class="None">column of the data. We also pad the sequence length to 250 words.</span></p><p><span class="None">The </span><strong class="inline">X</strong><span class="None"> variable looks as follows:</span></p><div class="IMG---Figure" id="_idContainer276"><img alt="Figure 9.14: Screenshot of the X variable array&#13;&#10;" src="image/C13783_9_14.jpg"/></div><h6>Figure 9.14: Screenshot of the X variable array</h6><p><span class="None">The X variable is a </span><strong class="inline">NumPy</strong><span class="None"> array with 3,000,000 rows and 250 columns. This is because there are 3,000,000 reviews available and each review has a fixed length of 250 words after padding.</span></p></li>
				<li><span class="None">We'll now prepare the target variable for training. We define the problem as a five-class classification problem where each class corresponds to a rating. Since the rating (sentiment score) is on a scale of 1-5, there are 5 outputs of the classifier. (You could also model this as a regression problem). We use the </span><strong class="inline">get_dummies</strong><span class="None"> function from pandas to get the five outputs:</span><p class="snippet"># get target variable</p><p class="snippet">y_train = pd.get_dummies(df.rating).values</p><p><span class="None">The </span><strong class="inline">y_train</strong><span class="None"> variable is a </span><strong class="inline">NumPy</strong><span class="None"> array with 3,000,000 rows and 5 columns with values, as shown:</span></p><div class="IMG---Figure" id="_idContainer277"><img alt="Figure 9.15: y_train output&#13;&#10;" src="image/C13783_9_15.jpg"/></div><h6>Figure 9.15: y_train output</h6></li>
				<li><span class="None">We have now preprocessed the text and prepared the target variable. Let</span>'<span class="None">s now define the model:</span><p class="snippet"><span class="None">embed_dim = 128</span></p><p class="snippet"><span class="None">hidden_units = 100</span></p><p class="snippet"><span class="None">n_classes = 5</span></p><p class="snippet"><span class="None">model = Sequential()</span></p><p class="snippet"><span class="None">model.add(Embedding(max_features, embed_dim, input_length = X.shape[1]))</span></p><p class="snippet"><span class="None">model.add(LSTM(hidden_units))</span></p><p class="snippet"><span class="None">model.add(Dense(n_classes, activation='softmax'))</span></p><p class="snippet"><span class="None">model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])</span></p><p class="snippet"><span class="None">print(model.summary())</span></p><p><span class="None">We choose 128 embedding dimensions for input. We also choose an LSTM as the RNN unit with 100 hidden dimensions. The model summary is printed as follows:</span></p><div class="IMG---Figure" id="_idContainer278"><img alt="Figure 9.16: Screenshot of the model summary&#13;&#10;" src="image/C13783_9_16.jpg"/></div><h6>Figure 9.16: Screenshot of the model summary</h6></li>
				<li><span class="None">We can now fit the model:</span><p class="snippet"><span class="None"># fit the model</span></p><p class="snippet"><span class="None">model.fit(X[:100000, :], y_train[:100000, :], batch_size = 128, epochs=15, validation_split=0.2)</span></p><p><span class="None">Note that we fit 100,000 reviews instead of 3,000,000. Running the training session with this configuration takes around 90 minutes. It would take much longer with a complete amount of data:</span></p><div class="IMG---Figure" id="_idContainer279"><img alt="Figure 9.17: Screenshot of the training session&#13;&#10;" src="image/C13783_9_17.jpg"/></div><h6>Figure 9.17: Screenshot of the training session</h6><p><span class="None">The validation accuracy for this 5-class problem is 48%. This isn</span>'<span class="None">t a good result, but for the purpose of demonstration, we can go ahead and deploy it.</span></p></li>
				<li><span class="None">We now have the model that we wish to deploy. Now, we need to save the model file and the tokenizer that will be used in the production environment to get predictions on the new reviews:</span><p class="snippet"><span class="None"># save model and tokenizer</span></p><p class="snippet"><span class="None">model.save('trained_model.h5')  # creates a HDF5 file 'trained_model.h5'</span></p><p class="snippet"><span class="None">with open('trained_tokenizer.pkl', 'wb') as f: # creates a pickle file 'trained_tokenizer.pkl'</span></p><p class="snippet"><span class="None">    pickle.dump(tokenizer, f)</span></p></li>
				<li><span class="None">These files now need to be downloaded from the Google Colab environment to the local drive:</span><p class="snippet"><span class="None">from google.colab import files</span></p><p class="snippet"><span class="None">files.download('trained_model.h5')</span></p><p class="snippet"><span class="None">files.download('trained_tokenizer.pkl')</span></p><p><span class="None">This snippet will download the tokenizer and model files to the local computer. We are now ready to use the model for predictions.</span></p></li>
			</ol>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor227"/>Flask</h2>
			<p><span class="None">In this section, we will use the Flask microserver framework provided by Python to make a web application that provides predictions. We will get a RESTful API that we can query to get our results. Before commencing, we need to install Flask (use </span><strong class="inline">pip</strong><span class="None">):</span></p>
			<ol>
				<li value="1">Let'<span class="None">s begin by importing the packages</span>:<p class="snippet"><span class="None">import re</span></p><p class="snippet"><span class="None">import pickle</span></p><p class="snippet"><span class="None">import numpy as np</span></p><p class="snippet"><span class="None">from flask import Flask, request, jsonify</span></p><p class="snippet"><span class="None">from keras.models import load_model</span></p><p class="snippet"><span class="None">from keras.preprocessing.sequence import pad_sequences</span></p></li>
				<li><span class="None">Now, let</span>'<span class="None">s write a function that loads the trained model and </span><strong class="inline">tokenizer</strong><span class="None">:</span><p class="snippet"><span class="None">def load_variables():</span></p><p class="snippet"><span class="None">    global model, tokenizer</span></p><p class="snippet"><span class="None">    model = load_model('trained_model.h5')</span></p><p class="snippet"><span class="None">    model._make_predict_function()  #https://github.com/keras-team/keras/issues/6462</span></p><p class="snippet"><span class="None">    with open('trained_tokenizer.pkl',  'rb') as f:</span></p><p class="snippet"><span class="None">        tokenizer = pickle.load(f)</span></p><p><span class="None">The </span><strong class="inline">make_predict_function()</strong><span class="None"> is a hack that allows using </span><strong class="inline">keras</strong><span class="None"> models with Flask.</span></p></li>
				<li><span class="None">Now, we</span>'<span class="None">ll define preprocessing functions similar to the training code:</span><p class="snippet"><span class="None">def do_preprocessing(reviews):</span></p><p class="snippet"><span class="None">    processed_reviews = []</span></p><p class="snippet"><span class="None">    for review in reviews:</span></p><p class="snippet"><span class="None">        review = review.lower()</span></p><p class="snippet"><span class="None">        processed_reviews.append(re.sub('[^a-zA-z0-9\s]', '', review))</span></p><p class="snippet"><span class="None">    processed_reviews = tokenizer.texts_to_sequences(np.array(processed_reviews))</span></p><p class="snippet"><span class="None">    processed_reviews = pad_sequences(processed_reviews, maxlen=250)</span></p><p class="snippet"><span class="None">    return processed_reviews</span></p><p><span class="None">Similar to the training phase, the reviews are first lowercased. Then, numbers are replaced with blanks. Next, the loaded tokenizer is applied and the sequences are padded to have a fixed length of 250 to make them consistent with the training input.</span></p></li>
				<li><span class="None">We will now define a Flask app instance:</span><p class="snippet"><span class="None">app = Flask(__name__)</span></p></li>
				<li><span class="None">We now define an endpoint that displays a fixed message:</span><p class="snippet"><span class="None">@app.route('/')</span></p><p class="snippet"><span class="None">def home_routine():</span></p><p class="snippet"><span class="None">    return 'Hello World!'</span></p><p><span class="None">It is good practice to have a root endpoint to check whether the web service is up.</span></p></li>
				<li><span class="None">Next, we</span>'<span class="None">ll have a prediction endpoint, to which we can send our review strings. The kind of HTTP request we will use is a </span>'<strong class="inline">POST</strong>' <span class="None">request:</span><p class="snippet"><span class="None">@app.route('/prediction', methods=['POST'])</span></p><p class="snippet"><span class="None">def get_prediction():</span></p><p class="snippet"><span class="None">  # get incoming text</span></p><p class="snippet"><span class="None">  # run the model</span></p><p class="snippet"><span class="None">    if request.method == 'POST':</span></p><p class="snippet"><span class="None">        data = request.get_json()</span></p><p class="snippet"><span class="None">    data = do_preprocessing(data)</span></p><p class="snippet"><span class="None">    predicted_sentiment_prob = model.predict(data)</span></p><p class="snippet"><span class="None">    predicted_sentiment = np.argmax(predicted_sentiment_prob, axis=-1)</span></p><p class="snippet"><span class="None">    return str(predicted_sentiment)</span></p></li>
				<li><span class="None">We can now start the web server:</span><p class="snippet"><span class="None">if __name__ == '__main__':</span></p><p class="snippet"><span class="None">  # load model</span></p><p class="snippet"><span class="None">  load_variables()</span></p><p class="snippet"><span class="None">  app.run(debug=True)</span></p></li>
				<li><span class="None">We could save this file as </span><strong class="inline">app.py</strong> <span class="None">(any name could be used). Run this code from the terminal using </span><strong class="inline">app.py</strong>:<p class="snippet"><span class="None">python app.py</span></p><p><span class="None">An output such as the one shown here will be produced in the terminal window:</span></p><div class="IMG---Figure" id="_idContainer280"><img alt="Figure 9.18: Output for Flask&#13;&#10;" src="image/C13783_9_18.jpg"/></div><h6>Figure 9.18: Output for Flask</h6></li>
				<li><span class="None">At this point, go to your browser window and enter the </span><strong class="inline">http://127.0.0.1:5000/</strong> <span class="None">address. The </span>'<span class="None">Hello World!</span>' <span class="None">message</span> <span class="None">will be displayed on the screen. The output produced corresponds to the root endpoint we set in the code. Now, we send our review texts to the </span>'<span class="None">prediction</span>' <span class="None">endpoint of our Flask web service. Let</span>'<span class="None">s send the following four reviews:</span></li>
				<li><span class="None">"The book was very poor"</span></li>
				<li><span class="None">"Very nice!"</span></li>
				<li><span class="None">"The author could have done more"</span></li>
				<li><span class="None">"Amazing product!"</span></li>
				<li><span class="None">We can send post requests to a web service using </span><strong class="inline">curl</strong><span class="None"> requests. For the four reviews mentioned, the </span><strong class="inline">curl</strong><span class="None"> request can be sent through the terminal, as follows:</span><p class="snippet"><span class="None">curl -X POST \</span></p><p class="snippet"><span class="None">127.0.0.1:5000/prediction \</span></p><p class="snippet"><span class="None">-H 'Content-Type: application/json' \</span></p><p class="snippet"><span class="None">-d '["The book was very poor", "Very nice!", "The author could have done more", "Amazing product!"]'</span></p><p><span class="None">The list of four reviews is posted to the prediction endpoint of the web service.</span></p><p><span class="None">The web service replies with a list of four ratings:</span></p><p class="snippet"><span class="None">[0 4 2 4]</span></p><p><span class="None">So, the sentiment ratings are as follows:</span></p></li>
				<li><span class="None">"The book was very poor"- 0</span></li>
				<li>"Very nice!"- 4</li>
				<li><span class="None">"The author could have done more" - 2</span></li>
				<li><span class="None">"Amazing product!" - 4</span><p><span class="None">The ratings actually make sense!</span></p></li>
			</ol>
			<h2 id="_idParaDest-201"><span class="None"><a id="_idTextAnchor228"/>Deployment</span></h2>
			<p><span class="None">Up to this point, the data science team has a Flask web service that works on a local system. However, the web development team is still not in a position to use the service, since it only runs on a local system. So, we need to host this web service somewhere on a cloud platform so that it is also available for the web development team to use. This section provides a basic pipeline for the deployment to work, which can be broken down into the following steps:</span></p>
			<ol>
				<li value="1"><span class="None">Make changes to the Flask web app so that it can be deployed.</span></li>
				<li><span class="None">Use Docker to wrap the flask web application into a container.</span></li>
				<li><span class="None">Host the container on an Amazon Web Services (AWS) EC2 instance.</span></li>
			</ol>
			<p><span class="None">Let's look at each of these steps in detail.</span></p>
			<h3 id="_idParaDest-202"><a id="_idTextAnchor229"/>Making Changes to a Flask Web App</h3>
			<p><span class="None">The flask application that was coded in the FLASK section ran on a local web address: </span><strong class="inline">http://127.0.0.1:5000</strong><span class="None">. Since our intention is to host it on the internet, this address needs to be changed to: 0.0.0.0. Additionally, since the default HTTP port is 80, the port also needs to be changed from 5000 to 80. So, the address that needs to be queried now becomes: 0.0.0.0:80.</span></p>
			<p><span class="None">In the code snippet, this change can be accomplished simply by modifying the call to the </span><strong class="inline">app.run</strong><span class="None"> function, as shown here:</span></p>
			<p class="snippet"><span class="None">app.run(host=0.0.0.0, port=80)</span></p>
			<p><span class="None">Notice that the '</span><strong class="bold">debug</strong><span class="None">' flag has also vanished (the default value of '</span><strong class="bold">debug</strong><span class="None">' flag is '</span><em class="italics">False</em><span class="None">'). This is because the application is past the debugging phase and is ready to be deployed to production.</span></p>
			<h4><span class="None">Note</span></h4>
			<p class="callout"><span class="None">The rest of the code remains exactly the same as before.</span></p>
			<p><span class="None">The application should be run again using the same command as earlier, and it should be verified that the same responses as earlier are received. The address in the curl request needs to be changed to reflect the updated web address:</span></p>
			<p class="snippet"><span class="None">curl -X POST \</span></p>
			<p class="snippet"><span class="None">0.0.0.0:80/prediction \</span></p>
			<p class="snippet"><span class="None">-H 'Content-Type: application/json' \</span></p>
			<p class="snippet"><span class="None">-d '["The book was very poor", "Very nice!", "The author could have done more", "Amazing product!"]' </span></p>
			<h4><span class="None">Note</span></h4>
			<p class="callout"><span class="None">If a permission error is received at this point, change the port number to 5000 in the </span><strong class="inline">app.run()</strong><span class="None"> command in app.py. (Port 80 is a privileged port, so change it to a  port that isn't, for example, 5000). However, be sure to change the port back to 80 once it is verified that the code works.</span></p>
			<h3 id="_idParaDest-203"><a id="_idTextAnchor230"/>Use Docker to Wrap the Flask Web Application into a Container</h3>
			<p><span class="None">The DS team intends to run the web service on a virtual machine hosted on a cloud platform (that is, AWS EC2). To isolate the EC2 operating system from the code environment, Docker offers containerization as a solution. We'll be using that here.</span></p>
			<h4><span class="None">Note</span></h4>
			<p class="callout"><span class="None"> For a quick tutorial on the basics of Docker and how to install and use it, refer to </span><a href="">https://docker-curriculum.com/</a>.</p>
			<p><span class="None">Follow these steps to deploy the application onto the container:</span></p>
			<ol>
				<li value="1"><span class="None">We first need a </span><em class="italics">requirements.txt</em><span class="None"> file that lists the specific packages that are needed to run the Python code:</span><p class="snippet"><span class="None">Flask==1.0.2</span></p><p class="snippet"><span class="None">numpy==1.14.1</span></p><p class="snippet"><span class="None">keras==2.2.4</span></p><p class="snippet"><span class="None">tensorflow==1.10.0</span></p></li>
				<li><span class="None">We need a </span><strong class="inline">Dockerfile</strong><span class="None"> containing instructions so that the Docker daemon can build the docker image:</span><p class="snippet"><span class="None">FROM python:3.6-slim</span></p><p class="snippet"><span class="None">COPY ./app.py /deploy/</span></p><p class="snippet"><span class="None">COPY ./requirements.txt /deploy/</span></p><p class="snippet"><span class="None">COPY ./trained_model.h5 /deploy/</span></p><p class="snippet"><span class="None">COPY ./trained_tokenizer.pkl /deploy/</span></p><p class="snippet"><span class="None">WORKDIR /deploy/</span></p><p class="snippet"><span class="None">RUN pip install -r requirements.txt</span></p><p class="snippet"><span class="None">EXPOSE 80</span></p><p class="snippet"><span class="None">ENTRYPOINT ["python", "app.py"]</span></p><p><span class="None">The Docker image is pulled from the Python dockerhub </span>repository<span class="None">. Here, the Dockerfile is executed. The </span><em class="italics">app.py</em><span class="None">, </span><em class="italics">requirements.txt</em><span class="None">, </span><em class="italics">tokenizer pickle</em><span class="None"> file, and </span><em class="italics">trained model</em><span class="None"> are copied over to the Docker image using the COPY command. To change the working directory to the 'deploy' directory (in which the files were copied), the WORKDIR command is used. The </span><strong class="inline">RUN</strong><span class="None"> command then installs the Python packages mentioned in the Dockerfile. Since port 80 is required to be accessed outside the container, the </span><strong class="inline">EXPOSE</strong><span class="None"> command is used.</span></p><h4><span class="None">Note</span></h4><p class="callout"><span class="None">The Docker Hub link can be found at </span><a href="">https://hub.docker.com/_/python</a>.</p></li>
				<li><span class="None">The Docker image should next be made using the </span><strong class="inline">docker build</strong><span class="None"> command:</span><p class="snippet"><span class="None">docker build -f Dockerfile -t app-packt .</span></p><p><span class="None">Don't forget the period in this command. The output of the command is as follows:</span></p><div class="IMG---Figure" id="_idContainer281"><img alt="Figure 9.19: Output screenshot for docker build&#13;&#10;" src="image/C13783_9_19.jpg"/></div><h6>Figure 9.19: Output screenshot for docker build</h6><p><span class="None">'</span><strong class="inline">app-packt</strong><span class="None">' is the name of the Docker image generated.</span></p></li>
				<li><span class="None">The Docker image can now be run as a container by issuing the </span><strong class="inline">docker run</strong><span class="None"> command:</span><p class="snippet"><span class="None">docker run -p 80:80 app-packt</span></p><p><span class="None">The </span><strong class="inline">p flag</strong><span class="None"> is used to do port mapping between port 80 of the local system to port 80 of the Docker container. (Change the port mapping part of the command to 5000:80 if 5000 is used locally. Please change the mapping back to 80:80 after verifying that the Docker container works, as explained.)</span></p><p><span class="None">The following screenshot depicts the output of the </span><strong class="inline">docker run</strong><span class="None"> command:</span></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer282">
					<img alt="Figure 9.20: Output screenshot for the docker run command&#13;&#10;" src="image/C13783_9_20.jpg"/>
				</div>
			</div>
			<h6>Figure 9.20: Output screenshot for the docker run command</h6>
			<p><span class="None">The exact same curl request from the last section can now be issued to verify that the application works.</span></p>
			<p><span class="None">The application code is now ready to be deployed onto AWS EC2.</span></p>
			<h3 id="_idParaDest-204"><a id="_idTextAnchor231"/>Host the Container on an Amazon Web Services (AWS) EC2 instance</h3>
			<p><span class="None">The DS team now has a containerized application that works on their local system. The web development team is still not in a position to use it, as it is still local. As per the initial MVP definition, the DS team now goes on to use the </span>AWS EC2 instance<span class="None"> to deploy the application. The deployment will ensure that the web service is available for the web development team to use.</span></p>
			<p><span class="None">As a prerequisite, you need to have an </span><a href="">AWS account</a><span class="None"> to use the EC2 instance. For the purpose of demonstration, we will be using a '</span><em class="italics">t2.small</em><span class="None">' EC2 instance type. This instance costs around 2 cents (USD) per hour at the time of writing. Note that this instance is not free-tier eligible. By default, this instance will not be available in your AWS region and a request needs to be raised for this instance to be added to your account. This usually takes a couple of hours. Alternatively, check the instance limits for your AWS region and select another instance with a minimum of 2GB RAM. A simple '</span><em class="italics">t2.micro</em><span class="None">' instance will not work for us here, as it has only 1GB of memory.</span></p>
			<h4><span class="None">Note</span></h4>
			<p class="callout"><span class="None">The link for the AWS account can be found at </span><a href="">https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/</a></p>
			<p class="callout">To add i<a id="_idTextAnchor232"/>nstances and check instance limits, refer to <a href="">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html</a>.</p>
			<p><span class="None">Let's start with the deployment process:</span></p>
			<ol>
				<li value="1">After l<span class="None">ogging into the AWS Management Console, search for '</span><strong class="bold">ec2</strong><span class="None">' in the search bar. This takes you to the EC2 dashboard, as shown here:</span><div class="IMG---Figure" id="_idContainer283"><img alt="Figure 9.21: AWS services in the AWS Management Console&#13;&#10;" src="image/C13783_9_21.jpg"/></div><h6>Figure 9.21: AWS services in the AWS Management Console</h6></li>
				<li>A key pair needs to be created to access AWS resources. To create one<span class="None">, </span><span class="None">look for the following pane and select '</span><strong class="bold">Key Pairs</strong><span class="None">'. This allows you to create a new key pair:</span><div class="IMG---Figure" id="_idContainer284"><img alt="Figure 9.22: Network and security on the AWS console&#13;&#10;" src="image/C13783_9_22.jpg"/></div><h6>Figure 9.22: Network and security on the AWS console</h6></li>
				<li><span class="None">A '</span><strong class="inline">.pem</strong><span class="None">' file is downloaded, which is the key file. Be sure to save the </span><strong class="inline">pem</strong><span class="None"> file safely and change its mode using the following command:</span><p class="snippet"><span class="None">chmod 400 key-file-name.pem</span></p><p><span class="None">This is required to change file permissions to </span>private.</p></li>
				<li><span class="None">To configure the instance, select '</span><strong class="bold">Launch Instance</strong><span class="None">' on the EC2 dashboard:</span><div class="IMG---Figure" id="_idContainer285"><img alt="Figure 9.23: Resources on the AWS console&#13;&#10;" src="image/C13783_9_23.jpg"/></div><h6>Figure 9.23: Resources on the AWS console</h6></li>
				<li>Next, select t<span class="None">he </span><strong class="bold">Amazon Machine Instance</strong><span class="None"> (</span><strong class="bold">AMI</strong><span class="None">), which selects the OS that EC2 instance runs. We will work with '</span><a href=""><strong class="bold">Amazon Linux 2 AMI</strong></a>':<h4>Note</h4><p class="callout">For more information on Amazon Linux 2 AMI, refer to <a href="">https://aws.amazon.com/amazon-linux-2/</a>.</p><div class="IMG---Figure" id="_idContainer286"><img alt="Figure 9.24: Amazon Machine Instance (AMI)&#13;&#10;" src="image/C13783_9_24.jpg"/></div><h6>Figure 9.24: Amazon Machine Instance (AMI)</h6></li>
				<li><span class="None">Now, we select the hardware part of EC2, which is the '</span><strong class="bold">t2.small</strong><span class="None">' instance:</span><div class="IMG---Figure" id="_idContainer287"><img alt="Figure 9.25: Choosing the instance type on AMI&#13;&#10;" src="image/C13783_9_25.jpg"/></div><h6>Figure 9.25: Choosing the instance type on AMI</h6></li>
				<li><span class="None">Clicking on '</span><strong class="bold">Review and Launch</strong><span class="None">' gets you to step 7 – the </span><strong class="bold">Review Instance Launch</strong><span class="None"> screen:</span><div class="IMG---Figure" id="_idContainer288"><img alt="Figure 9.26: The review instance launch screen&#13;&#10;" src="image/C13783_9_26.jpg"/></div><h6>Figure 9.26: The review instance launch screen</h6></li>
				<li>Now, to make<span class="None"> the web service reachable,</span> <span class="None">the security group needs to be modified. To this end, a rule needs to be created. At the end, you should see the following screen:</span><div class="IMG---Figure" id="_idContainer289"><img alt="Figure 9.27: Configure the security group&#13;&#10;" src="image/C13783_9_27.jpg"/></div><h6>Figure 9.27: Configure the security group</h6><h4>Note</h4><p class="callout">M<span class="None">ore can be learned about security groups and configuration using the </span><a href="">AWS documentation</a><span class="None"> at </span><a href="">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html</a>.</p></li>
				<li><span class="None">Next, clicking on the 'Launch' icon will trigger a redirection to a </span><strong class="bold">Launch</strong><span class="None"> screen:</span><div class="IMG---Figure" id="_idContainer290"><img alt="Figure 9.28: Launch status on the AWS instance&#13;&#10;" src="image/C13783_9_28.jpg"/></div><h6>Figure 9.28: Launch status on the AWS instance</h6><p>T<span class="None">he '</span><strong class="bold">View Instance</strong><span class="None">' button is to be used to navigate to a screen that displays the EC2 instance being launched, which is ready to be used when the instance state turns to 'running.'</span></p></li>
				<li><span class="None">Next, access the EC2 using the following command from the local system terminal with the '</span><strong class="inline">public-dns-name</strong>' <span class="None">field replaced with your EC2 instance name (of the form: </span>ec2–x–x–x–x.compute-1.amazonaws.com<span class="None">) and the path of the key pair </span><strong class="inline">pem</strong><span class="None"> file that was saved earlier:</span><p class="snippet"><span class="None">ssh -i /path/my-key-pair.pem ec2-user@public-dns-name</span></p><p><span class="None">This command will take you to the prompt of the EC2 instance where Docker needs to be installed first. Docker installation is required for the workflow since the Docker image will be built within the EC2 instance.</span></p></li>
				<li><span class="None">For Amazon Linux 2 AMI, the following commands should be used to accomplish this:</span><p class="snippet"><span class="None">sudo amazon-linux-extras install docker</span></p><p class="snippet"><span class="None">sudo yum install docker</span></p><p class="snippet"><span class="None">sudo service docker start</span></p><p class="snippet"><span class="None">sudo usermod -a -G docker ec2-user</span></p><h4><span class="None">Note</span></h4><p class="callout"><span class="None">For an explanation of the commands, check out the </span><a href="">documentation</a> at <a href="">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html</a>.</p></li>
				<li><span class="None">The '</span><strong class="inline">exit</strong><span class="None">' command should be used to log out of the instance. Next, log back in using the </span><strong class="inline">ssh</strong><span class="None"> command that was used earlier. Verify that Docker is working by issuing the '</span><strong class="inline">docker info</strong><span class="None">' command. Open another local terminal window for the next steps.</span></li>
				<li><span class="None">Now, copy the files that are needed to build the Docker image within the EC2 instance. Issue the command from the local terminal (not from within EC2!):</span><p class="snippet"><span class="None">scp -i /path/my-key-pair.pem file-to-copy </span><a href=""><span class="Hyperlink-4">ec2-user@</span></a><span class="None">public-dns-name:/home/ec2-user</span></p></li>
				<li><span class="None">The following files should be copied to build the Docker image, as was done earlier: </span><em class="italics">requirements.txt</em><span class="None">, </span><em class="italics">app.py</em><span class="None">, </span><em class="italics">trained_model.h5</em>, <em class="italics">trained_tokenizer.pkl</em><span class="None">,</span> and <em class="italics">Dockerfile</em><span class="None">.</span></li>
				<li>Next, l<span class="None">og in to the EC2 instance, issue the '</span><strong class="inline">ls</strong><span class="None">' command to see whether the copied files exist, and build and run the Docker image using the same commands that were used in the local system (ensure that you use port 80 at all locations in the code/commands).</span></li>
				<li>Enter<span class="None"> the home endpoint from the local browser using the public DNS name to see the '</span><strong class="bold">Hello World!</strong><span class="None">' message:</span><div class="IMG---Figure" id="_idContainer291"><img alt="Figure 9.29: Screenshot for the home endpoint&#13;&#10;" src="image/C13783_9_29.jpg"/></div><h6>Figure 9.29: Screenshot for the home endpoint</h6></li>
				<li><span class="None">Now you can send a curl request to the web service from a local terminal with the test sample data after replacing the </span><strong class="inline">public-dns-name</strong><span class="None"> with yours:</span><p class="snippet"><span class="None">curl -X POST \</span></p><p class="snippet"><span class="None">public-dns-name:80/predict \</span></p><p class="snippet"><span class="None">-H 'Content-Type: application/json' \</span></p><p class="snippet"><span class="None">-d '["The book was very poor", "Very nice!", "The author could have done more", "Amazing product!"]'</span></p></li>
				<li><span class="None">This should return the same review ratings as the ones obtained locally.</span></li>
			</ol>
			<p><span class="None">This concludes the simple deployment process.</span></p>
			<p><span class="None">The DS team now shares this </span><strong class="inline">curl</strong><span class="None"> request with the web development team, which can consume the web service with their test samples.</span></p>
			<h4><span class="None">Note</span></h4>
			<p class="callout"><span class="None">When the web service is not required, stop or terminate the EC2 instance to avoid getting charged.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer292">
					<img alt="Figure 9.30: Stopping the AWS EC2 instance&#13;&#10;" src="image/C13783_9_30.jpg"/>
				</div>
			</div>
			<h6><span class="None">Figure 9.30: Stopping the AWS EC2 instance</span></h6>
			<p><span class="None">From an MVP point of view, the deliverables are now complete!</span></p>
			<h3 id="_idParaDest-205"><a id="_idTextAnchor233"/>I<span class="None">mprovements</span></h3>
			<p><span class="None">The workflow described in this chapter is only meant to introduce a basic workflow using</span><span class="None"> certain tools (Flask, Colab, Docker, and AWS EC2) and inspire an example plan for a deep learning project in an organization. This is, however, only</span> <span class="None">an MVP, which could be improved in many ways for future iterations</span>.</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor234"/>S<span class="None">ummary</span></h2>
			<p><span class="None">In this chapter, we saw the journey of a deep learning project as it flows through an organization. We also learned about Google Colab notebooks to leverage GPUs for faster training. Additionally, we developed a Flask-based web service using Docker and deployed it to a cloud environment, hence enabling the stakeholders to obtain predictions for a given </span>input.</p>
			<p><span class="None">This chapter concludes our efforts toward learning how to leverage deep learning techniques to solve problems in the domain of natural language processing. Almost every aspect discussed in this chapter and the previous ones is a topic of research and is being improved upon continuously. The only way to stay informed is to keep learning about the new and exciting ways to tackle problems. Some common ways to do so are by following discussions on social media, following the work of top researchers/deep learning practitioners, and being on the constant lookout for organizations that are doing cutting-edge work when it comes to this domain.</span></p>
		</div>
	</body></html>