<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Deep Learning</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we'll cover some of the basics of deep learning. Deep learning refers to neural networks with lots of layers. It's kind of a buzzword, but the technology behind it is real and quite sophisticated.</p>
<p class="calibre2">The term has been rising in popularity along with machine learning and artificial intelligence, as shown in this Google trend chart:</p>
<div class="cdpaligncenter"><img src="../images/00161.jpeg" class="calibre20"/></div>
<p class="calibre2">As stated by some of the inventors of deep learning methods, the primary advantage of deep learning is that adding more data and more computing power often produces more accurate results, without the significant effort required for engineering.</p>
<p class="calibre2">In this chapter, we are going to be looking at the following:</p>
<ul class="calibre10">
<li class="calibre11">Deep learning methods</li>
<li class="calibre11">Identifying handwritten mathematical symbols with CNNs</li>
<li class="calibre11">Revisiting the bird species identifier to use images</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Deep learning methods</h1>
                
            
            <article>
                
<p class="calibre2">Deep learning refers to several methods which may be used in a particular application. These methods include convolutional layers and pooling. Simpler and faster activation functions, such as ReLU, return the neuron's weighted sum if it's positive and zero if negative. Regularization techniques, such as dropout, randomly ignore weights during the weight update base to prevent overfitting. GPUs are used for faster training with the order that is 50 times faster. This is because they're optimized for matrix calculations that are used extensively in neural networks and memory units for applications such as speech recognition.</p>
<p class="calibre2">Several factors have contributed to deep learning's dramatic growth in the last five years. Large public datasets, such as ImageNet, that holds millions of labeled images covering a thousand categories and Mozilla's Common Voice Project, that contain speech samples are now available. Such datasets have satisfied the basic requirement for deep learning-lot of training data. GPUs have transitioned to deep learning and clusters while also focusing on gaming. This helps make large-scale deep learning possible.</p>
<p class="calibre2">Advanced software frameworks that were released open source and are undergoing rapid improvement are also available to everyone. These include TensorFlow, Keras, Torch, and Caffe. Deep architectures that achieve state-of-the-art results, such as Inception-v3 are being used for the ImageNet dataset. This network actually has an approximate of 24 million parameters, and a large community of researchers and software engineers quickly translating research prototypes into open source software that anyone can download, evaluate, and extend.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Convolutions and pooling</h1>
                
            
            <article>
                
<p class="calibre2">This sections takes a closer look at two fundamental deep learning technologies, namely, convolution and pooling. Throughout this section, we will be using images to understand these concepts. Nevertheless, what we'll be studying can also be applied to other data, such as, audio signals. Let's take a look at the following photo and begin by zooming in to observe the pixels:</p>
<div class="cdpaligncenter"><img src="../images/00162.jpeg" class="calibre22"/></div>
<p class="calibre2">Convolutions occur per channel. An input image would generally consist of three channels; red, green, and blue. The next step would be to separate these three colors. The following diagram depicts this: </p>
<div class="cdpaligncenter"><img src="../images/00163.jpeg" class="calibre23"/></div>
<p class="calibre2">A convolution is a kernel. In this image, we apply a 3 x 3 kernel. Every kernel contains a number of weights. The kernel slides around the image and computes the weighted sum of the pixels on the kernel, each multiplied by their corresponding kernel weights:</p>
<div class="cdpaligncenter"><img src="../images/00164.gif" class="calibre24"/></div>
<p class="calibre2">A bias term is also added. A single number, the weighted sum, is produced for each position that the kernel slides over. The kernel's weights start off with any random value and change during the training phase. The following diagram shows three examples of kernels with different weights:</p>
<div class="cdpaligncenter"><img src="../images/00165.jpeg" class="calibre25"/></div>
<p class="calibre2"/>
<p class="calibre2">You can see how the image transforms differently depending on the weights. The rightmost image highlights the edges, which is often useful for identifying objects. The stride helps us understand how the kernel slides across the image. The following diagram is an example of a 1 x 1 stride:</p>
<div class="cdpaligncenter"><img src="../images/00166.jpeg" class="calibre26"/></div>
<p class="calibre2"/>
<p class="cdpalignleft1">The kernel moves by one pixel to the right and then down. Throughout this process, the center of the kernel will hit every pixel of the image whilst overlapping the other kernels. It is also observed that <span class="calibre5">some pixels are missed by the center of the kernel. </span>The following image depicts a 2 x 2 stride:</p>
<div class="cdpaligncenter"><img src="../images/00167.jpeg" class="calibre27"/></div>
<p class="calibre2"/>
<p class="calibre2">In certain cases, it is observed that no overlapping takes place. To prove this, the following diagram contains a 3 x 3 stride: </p>
<div class="cdpaligncenter"><img src="../images/00168.jpeg" class="calibre28"/></div>
<p class="calibre2">In such cases, no overlap takes place because the kernel is the same size as the stride.</p>
<p class="calibre2">However, the borders of the image need to be handled differently. To affect this, we can use padding. This helps avoid extending the kernel across the border. P<span class="calibre5">adding consists of extra pixels, which are always zero. They don't contribute to the weighted sum. The padding allows the kernel's weights to cover every region of the image while still letting the kernels assume the stride is 1. The kernel produces one output for every region it covers. Hence, if we have a stride that is greater than 1, we'll have fewer outputs than there were original pixels. In other words, the convolution helped reduce the image's dimensions. The formula shown here tells us the dimensions of the output of a convolution:</span></p>
<div class="cdpaligncenter"><img src="../images/00169.jpeg" class="calibre29"/></div>
<p class="calibre2">It is a general practice to use square images. Kernels and strides are used for simplicity. This helps us focus on only one dimension, which will be the same for the width and height. In the following diagram, a 3 x 3 kernel with a (3, 3) stride is depicted: </p>
<div class="cdpaligncenter"><img src="../images/00170.jpeg" class="calibre30"/></div>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">The preceding calculation gives the result of 85 width and 85 height. The image's width and height have effectively been reduced by a factor of three from the original 256. Rather than use a large stride, we shall let the convolution hit every pixel by using a stride of 1. This will help us attain a more practical result. We also need to make sure that there is sufficient padding. However, it is beneficial to reduce the image dimensions as we move through the network. This helps the network train faster as there will be fewer parameters. Fewer parameters imply a smaller chance of over-fitting.</p>
<p class="calibre2">We often use max or average pooling between convolution dimensionality instead of varying the stride length. Pooling looks at a region, which, let us assume, is 2 x 2, and keeps only the largest or average value. The following image depicts a 2 x 2 matrix that depicts pooling: </p>
<div class="cdpaligncenter"><img src="../images/00171.gif" class="calibre31"/></div>
<p class="calibre2">A pooling region always has the same-sized stride as the pool size. This helps avoid overlapping. </p>
<div class="packt_infobox">Pooling doesn't use any weights, which means there is nothing to train.</div>
<p class="calibre2"/>
<p class="calibre2">Here's a relatively shallow <strong class="calibre4">convolutional neural networks</strong> (<strong class="calibre4">CNNs</strong>) representation:</p>
<div class="cdpaligncenter"><img src="../images/00172.jpeg" class="calibre32"/></div>
<div class="cdpaligncenter2">Source: cs231.github.io, MIT License</div>
<p class="calibre2">We observe that the input image is subjected to various convolutions and pooling layers with ReLU activations between them before finally arriving at a traditionally fully connected network. The fully connected network, though not depicted in the diagram, is ultimately predicting the class. In this example, as in most CNNs, we will have multiple convolutions at each layer. Here, we will observe 10, which are depicted as rows. Each of these 10 convolutions have their own kernels in each column so that different convolutions can be learned at each resolution. The fully connected layers on the right will determine which convolutions best identify the car or the truck, and so forth. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Identifying handwritten mathematical symbols with CNNs</h1>
                
            
            <article>
                
<p class="calibre2">This sections deals with building a CNN to identify handwritten mathematical symbols. We're going to use the <kbd class="calibre12">HASYv2</kbd> dataset. This contains 168,000 images from 369 different classes where each represents a different symbol. This dataset is a more complex analog compared to the popular MNIST dataset, which contains handwritten numbers.</p>
<p class="calibre2">The following diagram depicts the kind of images that are available in this dataset:</p>
<div class="cdpaligncenter"><img src="../images/00173.jpeg" class="calibre33"/></div>
<p class="calibre2"/>
<p class="calibre2">And here, we can see a graph showing how many symbols have different numbers of images:</p>
<div class="cdpaligncenter"><img src="../images/00174.jpeg" class="calibre20"/></div>
<p class="calibre2">It is observed that many symbols have few images and there are a few that have lots of images. The code to import any image is as follows:</p>
<div class="cdpaligncenter"><img src="../images/00175.jpeg" class="calibre34"/></div>
<p class="calibre2"/>
<p class="calibre2">We begin by importing the <kbd class="calibre12">Image</kbd> class from the <kbd class="calibre12">IPython</kbd> library. This allows us to show images inside Jupyter Notebook. Here's one image from the dataset:</p>
<div class="cdpaligncenter"><img src="../images/00176.jpeg" class="calibre35"/></div>
<p class="calibre2">This is an image of the alphabet <strong class="calibre4">A</strong>. Each image is 30 x 30 pixels. This image is in the RGB format even though it doesn't really need to be RGB. The different channels are predominately black and white or grayscale. We're going to use these three channels. We then proceed to import CSV, which allows us to load the dataset:</p>
<div class="cdpaligncenter"> <img src="../images/00177.jpeg" class="calibre36"/></div>
<p class="calibre2">This CSV file states all the different filenames and the class names. We import the image class from <kbd class="calibre12">pil</kbd>, which allows us to load the image. We import <kbd class="calibre12">preprocessing.image</kbd>, which then allows us to convert the images into <kbd class="calibre12">numpy</kbd> arrays. Let's us then go through the data file, taking a closer look at every filename and loading it, while recording which class it belongs to:</p>
<div class="cdpaligncenter"><img src="../images/00178.jpeg" class="calibre20"/></div>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">The immediate next step would be to save the images and the classes and use the CSV reader. We need to set a counter to make sure we skip the first row, which is the header of the CSV file. Only after this, we proceed to open the image, which is in the first column of each row. This is converted into an array. The achieved result will have dimensions of 30 x 30 x 3, which is interpreted as 30 width, 30 height, and 3 channels (RGB).</p>
<p class="calibre2">These three channels will have numbers between 0 and 255. These are typical pixel values, which are not good for a neural network. We need values that lie between 0 and 1 or -1 and 1. To do this, we divide each pixel value by 255. To make things easier, we're going to collect the filename, the class name, and the image matrix and put them into our images list. We will also make a note of the name of the class. The following snippet will make us understand the concept to a greater depth:</p>
<div class="cdpaligncenter"><img src="../images/00179.jpeg" class="calibre37"/></div>
<p class="calibre2"/>
<p class="calibre2">The file is named <kbd class="calibre12">hasy-data/v2-00000.png</kbd>. <kbd class="calibre12">A</kbd> is the name of the class followed by the array. The array has dimensions 30 x 30 x 3. The innermost and last dimension, is 3. Each <span class="calibre5">1.0</span> depicts the color white. We understand this because we divided everything by 255 as mentioned earlier. </p>
<p class="calibre2">We have 168,000 images in the <span class="calibre5"><kbd class="calibre12">HASYv2</kbd> dataset</span>:</p>
<div class="cdpaligncenter"><img src="../images/00180.jpeg" class="calibre38"/></div>
<p class="calibre2">We then proceed to shuffle and then split the data on an 80% train, 20% test basis. As seen in the following codeblock, we first shuffle, then proceed to split the image: </p>
<div class="cdpaligncenter"><img src="../images/00181.jpeg" class="calibre39"/></div>
<p class="calibre2">Because we use these tuples with three different values, we're going to need to ultimately collect all that into a matrix:</p>
<div class="cdpaligncenter"><img src="../images/00182.jpeg" class="calibre40"/></div>
<p class="calibre2">We need to collect the images as well as the labels. To collect the images, we go through each row and take each third element. This element is the image matrix. We stick it all together into a <kbd class="calibre12">numpy</kbd> array. The same is done for the train and test datasets.</p>
<p class="calibre2">For the outputs, we need to go and pick out the second value. These are still strings, such as <kbd class="calibre12">a</kbd> and <kbd class="calibre12">=</kbd>. We need to convert the second value into one-hot encoding before it can be used for a neural network.</p>
<p class="calibre2"/>
<p class="calibre2">We proceed to use scikit-learn's preprocessing label encoder and one-hot encoder:</p>
<div class="cdpaligncenter"><img src="../images/00183.jpeg" class="calibre20"/></div>
<p class="calibre2">We're going to make a <kbd class="calibre12">LabelEncoder</kbd> object and we're going to both fit and transform on the classes:</p>
<div class="cdpaligncenter"><img src="../images/00184.jpeg" class="calibre20"/></div>
<p class="calibre2">The <kbd class="calibre12">fit</kbd> function learns which classes exist. It learns that there are 369 different class names. The <kbd class="calibre12">tranform</kbd> function turns them into integers. This is done by sorting the classes and giving each class an integer ID. <kbd class="calibre12"><span>integer_encoded</span></kbd> helps to reproduce the list of classes as integer IDs. The one-hot encoder takes these integers and fits on them; this too learns how many different integers are represented. Just as <kbd class="calibre12">LabelEncoder</kbd> learned about the class names, <kbd class="calibre12">onehot_encoder</kbd> is going to learn that there are 369 different integers.</p>
<p class="calibre2">The code then moves to <kbd class="calibre12">LabelEncoder</kbd> which transforms <kbd class="calibre12">train_output</kbd> into integers. These integers are then transformed into one-hot encoding. The one-hot encoding returns a 369-dimension with the first dimension of 369 values and a vector of 369 values. All values are zeros except for a single 1. The position of this 1 depends on which class it is. <kbd class="calibre12">test_output</kbd> undergoes the same process. When the training data for input and output is ready, we proceed to build a neural network.</p>
<p class="calibre2">To do this, we are going to use <kbd class="calibre12">Sequential</kbd> again:</p>
<div class="cdpaligncenter"><img src="../images/00185.jpeg" class="calibre41"/></div>
<p class="calibre2">Sequential is a feed-forward network. Even though there are convolutions that still feed forward and are not recurrent, there are no cycles. Dense layers are used at the end of the network. We also use <kbd class="calibre12">Dropout</kbd> to try to prevent overfitting. When we switch from convolutions to dense layers, we need to use the <kbd class="calibre12">flatten</kbd> command, since convolutions are two-dimensional and dense layers are not. We also need to use <kbd class="calibre12">Conv2D</kbd> and <kbd class="calibre12">MaxPooling2D</kbd>.</p>
<p class="calibre2">The following code block is our network design:</p>
<div class="cdpaligncenter"><img src="../images/00186.jpeg" class="calibre42"/></div>
<p class="calibre2">This is modeled after MNIST design, which handles handwritten numbers. We start by making a sequential model. We need to add a convolution layer that has 32 different convolutions. The kernel size will be 3 x 3 and the activation will be ReLU. Since this is the first layer, we need to mention the input shape. If you recall, the dimensions were 30 x 30 x 3.</p>
<p class="calibre2">We use the kernel size of 3 x 3 and the stride as 1 as it is the default value. Having the stride as 1 will require padding. This is going to produce a 30 x 30 x 32 shape because there are 32 convolutions. The 30 x 30 dimensions remain constant. WE now observe that we haven't really reduced dimensions just by doing this convolution.</p>
<p class="calibre2"><kbd class="calibre12">MaxPooling</kbd> is used to reduce the dimensions by half. This is possible because it has a 2 x 2 pool size. We then follow with another convolution layer, which is another dimensionality reduction.</p>
<p class="calibre2">After all the convolutions have taken place, we flatten everything. This converts a two-dimensional representation into a one-dimensional representation. This is then fed into a dense layer with more than 1,000 neurons.</p>
<p class="calibre2">This dense layer will then have a <kbd class="calibre12">tanh</kbd> activation. This is then fed into another dense layer of neurons. This time around, there are 369 of them for the class outputs. This is the <kbd class="calibre12">onehot_encoding</kbd> output. We're not going to do any particular activation except for softmax. So, the original values will be rescaled to be between 0 and 1. This means that the sum of all the values across the 369 different neurons is 1.0. Softmax basically turns the output into a probability.</p>
<p class="calibre2">Proceeding to compiling <kbd class="calibre12">categorical _crossentropy</kbd> again helps us predict one of multiple classes. You would want to do this on the <kbd class="calibre12">adam</kbd> optimizer and observe it's accuracy. Here's the model's summary:</p>
<div class="cdpaligncenter"><img src="../images/00187.jpeg" class="calibre43"/></div>
<p class="calibre2">It is observed that the convolution layer doesn't change the dimensions, but the pooling does. It reduces it by half because of the odd dimension size, that is, 15. The next layer is at 13 output, which also gets reduced by half. The <kbd class="calibre12">conv2d_1 (Conv2D)</kbd> parameters are used for learning the convolutions. The <kbd class="calibre12">dense_1 (Dense)</kbd> parameters are used for learning the weights connected to the prior layer. In a similiar fashion, the <kbd class="calibre12">dense_2 (Dense)</kbd> parameters are for the weights for the prior layer. Ultimately, we have about 1.6 million parameters.</p>
<p class="calibre2">We're going to visualize the performance's accuracy and validation's accuracy with TensorBoard. We're going to save all the results into a directory called <kbd class="calibre12">mnist-style</kbd> because that's the style of the network we built earlier. The following is a callback:</p>
<div class="cdpaligncenter"><img src="../images/00188.jpeg" class="calibre44"/></div>
<p class="calibre2">Keras supports callbacks of various types. The callback is used in the <kbd class="calibre12">fit</kbd> method, so after every epoch, it calls the callback. It passes information to the callback, such as the validation loss and the training loss. We use 10 epochs and a batch size of 32, with a 0.2, 20%, validation split.</p>
<p class="calibre2">Here's the result of the training:</p>
<div class="cdpaligncenter"><img src="../images/00189.jpeg" class="calibre45"/></div>
<p class="calibre2">Now, there are a lot of choices, but ultimately we need to check them. We got about 76% validation accuracy, and when we test this out on the test set, we get the same 76% accuracy. Now, there were a lot of decisions in this design, including how many convolution layers to have and what size they should be, what kernel should be used or what size of kernel, what kind of stride, what the activation was for the convolutions, where the max pooling showed up, if it ever did, what the pooling size was, how many dense layers we have, when do they appear, what is the activation, and so on and so forth. A lot of decisions. It's quite difficult to know how to choose these different designs. These are actually called <strong class="calibre4">hyperparameters</strong>.</p>
<p class="calibre2">The weights that can be learned during the fit procedure are just called parameters, but the decisions you have to make about how to design the network and the activation functions and so forth we call hyperparameters, because they can't be learned by the network. In order to try different parameters, we can just do some loops:</p>
<div class="cdpaligncenter"><img src="../images/00190.jpeg" class="calibre20"/></div>
<p class="calibre2">We will time how long it takes to train each of these. We will collect the results, which would be the accuracy numbers. Then, we will try a convolution 2D, which will have one or two such layers. We're going to try a dense layer with 128 neurons. We will try a dropout as <kbd class="calibre12">for dropout in [0.0, 0.25, 0.50, 0.7</kbd>, which will be either yes or no, and means 0-25%, 50%, 75%. So, for each of these combinations, we make a model depending on how many convolutions we're going to have, with convolution layers either one or two. We're going to add a convolution layer.</p>
<p class="calibre2"/>
<p class="calibre2">If it's the first layer, we need to put in the input shape, otherwise we'll just add the layer. Then, after adding the convolution layer, we're going to do the same with max pooling. Then, we're going to flatten and add a dense layer of whatever size that comes from <kbd class="calibre12">for dense_size in [128, 256, 512, 1024, 2048]: loop</kbd>. It will always be <kbd class="calibre12">tanh</kbd>, though.</p>
<p class="calibre2">If <kbd class="calibre12">Dropout</kbd> is used, we're going to add a dropout layer. Calling this dropout means, say it's 50%, that every time it goes to update the weights after each batch, there's a 50% chance for each weight that it won't be updated, but we put this between the two dense layers to kind of protect it from overfitting. The last layer will always be the number of classes because it has to be, and we'll use softmax. It gets compiled in the same way.</p>
<p class="calibre2">Set up a different log directory for TensorBoard so that we can distinguish the different configurations. Start the timer and run fit. Do the evaluation and get the score, stop the timer, and print the results. So, here it is running on all of these different configurations:</p>
<div class="cdpaligncenter"><img src="../images/00191.jpeg" class="calibre20"/></div>
<p class="calibre2"/>
<p class="calibre2">0.74 is the actual test set accuracy. So, you can see that there are a lot of different numbers for accuracy. They go down to low point sevens up to the high point sevens, and the time differs depending on how many parameters there are in the network. We can visualize these results because we are using the callback function.</p>
<p class="calibre2">Here's the accuracy and loss, which are from the training set:</p>
<div class="cdpaligncenter"><img src="../images/00192.jpeg" class="calibre46"/></div>
<p class="calibre2">And here's the validation accuracy and validation loss:</p>
<div class="cdpaligncenter"><img src="../images/00193.jpeg" class="calibre47"/></div>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Zoom out a bit so that we can see the configurations on the side, and then we can turn them all off. Turn <kbd class="calibre12">mnist-style</kbd> back on. This was the first one we tried:</p>
<div class="cdpaligncenter"><img src="../images/00194.jpeg" class="calibre20"/></div>
<p class="calibre2">You can see that the accuracy goes up and the loss goes down. That's pretty normal. Validation accuracy goes up and loss goes down, and it mostly stays consistent. What we don't want to see is validation loss skyrocketing after a while, even though the accuracy is going way up. That's pretty much by-definition overfitting. It's learning the training examples really well, but it's getting much worse on the examples it didn't see. We really don't want that to happen. So, let's compare a few things. First, we'll compare different dropouts. Let's go to <kbd class="calibre12">conv2d_1</kbd>-<kbd class="calibre12">dense_128</kbd> but with different dropouts.</p>
<p class="calibre2">As far as loss goes:</p>
<div class="cdpaligncenter"><img src="../images/00195.jpeg" class="calibre20"/></div>
<p class="calibre2">We can see that with a very low dropout, such as 0 or 0.25, the loss is minimized. That's because if you want to really learn that training set, don't refuse to update weights. Instead, update all of them all the time. With that same run, by looking at the dark blue line, we can see that it definitely overfit after just two epochs because the validation loss, the examples it did not see, started to get much worse. So, that's where the overfitting started. It's pretty clear that dropout reduces overfitting. Look at the 0.75 dropout. That's where the validation loss just got better and better, which means lower and lower.</p>
<p class="calibre2"/>
<p class="calibre2">It doesn't make it the most accurate, though, because we can see that the accuracy is not necessarily the best for our training set or the validation set:</p>
<div class="cdpaligncenter"><img src="../images/00196.jpeg" class="calibre20"/></div>
<p class="calibre2">Actually, about 0.5 seems pretty good for a validation set. Now, let's just make sure it's the same for other layers. Again, with no dropouts (0.0), we get the lowest training loss but the highest validation loss. Likewise, we get a 0.75 dropout for the lowest validation loss but not necessarily the best training.</p>
<p class="calibre2">Now, let's compare how many dense layers they have. We're just going to stick with dropout 0.5, so we'll use <kbd class="calibre12">conv2d_1</kbd>. So, we have one convolution layer, <kbd class="calibre12">dense_*</kbd>, and a dropout of 0.50:</p>
<div class="cdpaligncenter"><img src="../images/00197.jpeg" class="calibre20"/></div>
<p class="calibre2">So the choice here is, does the dense layer have 128, 256, 512, 1,024, or 2,048? In the previous graph, we can see that there are some clear cases of overfitting. Pretty much anything that's not the 128 starts to suffer from overfitting. So, a dense layer of 128 is probably the best choice. Now, let's compare one convolution layer to two convolution layers:</p>
<div class="cdpaligncenter"><img src="../images/00198.jpeg" class="calibre20"/></div>
<p class="calibre2"/>
<p class="calibre2">Not a big difference, actually. For validation, we get two convolution layers and receive the lowest loss, which is usually the same as the highest accuracy. This means that we've narrowed down. This is called model selection, which is all about figuring out what the best model is, as well as the best hyperparameters. We've narrowed this down to the two-dimensional convolution, two layers of that, 128 dense in the first dense layer, and 50% dropout. Given that, let's retrain on all the data so that we have the best trained model we could possibly have:</p>
<div class="cdpaligncenter"><img src="../images/00199.jpeg" class="calibre48"/></div>
<p class="calibre2">We get our two convolution layers, we do our dense 128 dropout 0.5, and in this case we take all the data we have, the entire dataset trained and tested, and stick it all together. Now, we can't really evaluate this model because we just lost our testing set, so what we're going to do instead is use this model to predict other images. Actually, we're going to save the model after it's fit and we're going to show how to load in a minute. If you're going to load this in another file, you're also going to want to know what those labels were called because all we know is the one-hot encoding. From the one-hot encoding, we can get back the integer number, but still that's not the same as the actual name of the symbol. So, we have to save the classes from <kbd class="calibre12">LabelEncoder</kbd> and we're just going to use a <kbd class="calibre12">numpy</kbd> file to save that.</p>
<p class="calibre2">Let's train the model:</p>
<div class="cdpaligncenter"><img src="../images/00200.jpeg" class="calibre49"/></div>
<p class="calibre2">This could actually be all in a separate file. You can load everything again:</p>
<div class="cdpaligncenter"><img src="../images/00201.jpeg" class="calibre20"/></div>
<p class="calibre2">Import <kbd class="calibre12">keras.models</kbd> and you can use the <kbd class="calibre12">load _model</kbd> feature. The model file there actually saves the structure as well as the weights. That's all you need to do to recover the network. You can print the summary again. For <kbd class="calibre12">LabelEncoder</kbd>, we need to call the constructor again and give it the classes that we saved ahead of time.</p>
<p class="calibre2">Now, we can make a function called predict takes an image. We do a little bit of preprocessing to turn the image into an array, we divide it by 255, and we predict. If you have a whole set of images, you won't need to do this reshape, but since we just have one, we can put it in an array that has a single row. We will get the prediction out of this, and using <kbd class="calibre12">LabelEncoder</kbd>, we can reverse the prediction to the actual name of the class, the name of the symbol, and which prediction? Well, it's one-hot encoding, so you can figure out the position of the highest number. This takes all the neuron outputs, the 369, figures out what the largest confidence number is, and says that's the one that was predicted. Therefore, one-hot encoding would tell you this particular symbol, and then we can print it:</p>
<div class="cdpaligncenter"><img src="../images/00202.jpeg" class="calibre50"/></div>
<p class="calibre2">Here's how we can use that function:</p>
<div class="cdpaligncenter"><img src="../images/00203.jpeg" class="calibre20"/></div>
<p class="calibre2">We're actually using the training images for this purpose instead of making new ones, but you get the idea. You take an image that says that's an <kbd class="calibre12">A</kbd>, and I'm 87% confident about it. For pi prediction, we're 58% confident and for alpha prediction, we're 88% confident. Next, we'll look at the bird species example we used previously, and instead of using all of the attributes that humans created, we're going to use the images themselves.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Revisiting the bird species identifier to use images</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we're going to revisit the bird species identifier from before. This time, we're going to update it to use neural networks and deep learning. Can you recall the birds dataset? It has 200 different species of birds across 12,000 images. Unlike last time, we won't be using the human-labeled attributes, and instead we'll use the actual images without any pre-processing. In our first attempt, we're going to build a custom convolutional neural network, just like we did for the mathematical symbols classifier.</p>
<p class="calibre2">Let's go to the code. We will start with the typical imports:</p>
<div class="cdpaligncenter"><img src="../images/00204.jpeg" class="calibre20"/></div>
<p class="calibre2">We'll make some convenience variables, the rows and columns of the image, the width and height, and the number of channels, RGB, though every bird image will be equal. Even though they're not all necessarily the same size, we're going to resize them to this size so that they're all consistent:</p>
<div class="cdpaligncenter"><img src="../images/00205.jpeg" class="calibre20"/></div>
<p class="calibre2">Now, this project introduces an interesting feature on Keras called an <strong class="calibre4">image data generator</strong>:</p>
<div class="cdpaligncenter"><img src="../images/00206.jpeg" class="calibre20"/></div>
<p class="calibre2">The data generator can produce new images from the existing training set and these new images can have various differences; for example, they can be rotated, they can be flipped  horizontally or vertically, and so forth. Then, we can generate more examples than we actually started with. This is a great thing to do when you have a small number of training examples. We have, in our case, about 6,000 training sets. That's relatively small in deep learning, so we want to be able to generate more; the data generator will just keep generating them as long as we keep asking for them. For the training images, we want to also generate versions with the horizontal flip. We don't want to do a vertical flip because I don't expect any bird images to be upside down. We also want to support rotations of up to 45 degrees, and we want to rescale all the pixel values to divide by 255. Actually, <kbd class="calibre12">ImageDataGenerator</kbd> just calls the constructor, so nothing's actually happened yet. What you want to do next is use <kbd class="calibre12">flow_from_directory</kbd>, so that your images can be organized into directories or subdirectories.</p>
<p class="calibre2">We have a <kbd class="calibre12">train</kbd> directory, and inside that there's going to be a folder for each bird class. So, there's 200 different folders inside train and inside those folders are the images for that particular bird. We want all the images to be resized to 256 x 256 and we can indicate that instead of using binary, we want to use categorical classes, meaning that we will have lots of different classes (200, in this case). We're going to use the data generator for the test set too, just because <kbd class="calibre12">flow_from_directory</kbd> is a convenient function. We don't want to do any flips, though, or rotations. We just want to use the testing set as is so we can compare it with other people. The other really convenient thing about <kbd class="calibre12">flow_from _directory</kbd> is that it's automatically going to produce a <kbd class="calibre12">numpy</kbd> matrix with the image data, and it's also going to give the class values in one-hot encoding.</p>
<p class="calibre2">So, what was several steps before is now being done all at once.</p>
<p class="calibre2"/>
<p class="calibre2">Now, I don't really need to do a reset, but since these are technically iterators, if you're constantly fixing the model and trying to retrain, then you might want to do a reset so that you get all the same images in the same order. In any event, it's an iterator, so you can call next, reset, and so forth:</p>
<div class="cdpaligncenter"><img src="../images/00207.jpeg" class="calibre51"/></div>
<p class="calibre2">Now, we will build a sequential model, which is going to be a convolutional model. We have a convolution kernel of 3 x 3, 64 of this. We also have a <kbd class="calibre12">relu</kbd> and another convolution built by <kbd class="calibre12">relu</kbd>, which we can do a max pooling with, and just from experimentation, I discovered that this works relatively well: 3 x 3 followed by 3 x 3, each 64. By having a pretty dramatic max point of 4 x 4, so we repeat this process and then we flatten. We have a dropout of 50% just to reduce overfitting, a dense of 400 neurons, another dropout, and then 200 for the output because there are 200 different classes, and because it's categorical one-hot encoding, we want to use softmax so that only one of those 200 has the highest value. We also want to ensure that they all add up to 1.0.</p>
<p class="calibre2">Here's the summary of the model. Ultimately, we have about 5 million parameters:</p>
<div class="cdpaligncenter"><img src="../images/00208.jpeg" class="calibre52"/></div>
<div class="cdpaligncenter"><img src="../images/00209.jpeg" class="calibre53"/></div>
<p class="calibre2">The different variations I did that had far more parameters, such as, say, 100 million performed worse because there were just too many parameters. There's either too many parameters, meaning it's really hard to train it to learn anything because obviously all the parameters start random, so it's really hard to make those parameters trend toward the right values, or there are so few that it's not going to learn anything either. There's kind of a balance that you have to find, and 5 million, I think, is somewhere near that balance.</p>
<p class="calibre2">Now, if you use a generator, you don't have all the data for the training prepared ahead of time; it's going to produce those images as it goes:</p>
<div class="cdpaligncenter"><img src="../images/00210.jpeg" class="calibre20"/></div>
<p class="calibre2">That makes it actually quite memory-efficient. You don't have to load the whole dataset ahead of time. It'll just make it as needed, but you have to call <kbd class="calibre12">fit _generator</kbd> instead of just using fit. What you give instead of the train input and train output is the generator. The generator knows how to produce the image matrices and it knows how to produce one-hot encoding. So, again, that's extremely convenient when you have images. There's other kinds of generators, too. Look at the Keras documentation for these. <kbd class="calibre12">steps_per_epoch</kbd> shows how many images to produce per epoch, or how many batches to produce. The generator, by default, produces batches of 32 images. Regarding the number of epochs, and if you want to do some statistics on TensorBoard, you can set up a callback and verbose 2 so that we can see some output here.</p>
<p class="calibre2">There are 10 epochs:</p>
<div class="cdpaligncenter"><img src="../images/00211.jpeg" class="calibre54"/></div>
<p class="calibre2">We can see that the training accuracy is on the images that is training on. It's not very accurate for what the accuracy is going to be on the test set, so we do this separately. The test images are also in a generator. You don't just evaluate<span class="calibre5">—y</span>ou use <kbd class="calibre12">evaluate_generator</kbd> and you say, <em class="calibre16">how many images do you want to evaluate?</em> We'll just do 1,000, and we'll get 22% accuracy.</p>
<p class="calibre2">That's not so bad. Random guessing would yield 0.5%, so 22% is pretty good, and that's just from a handcrafted model starting from scratch that had to learn everything from those bird images. The reason I'm saying things like this is because the next thing we're going to do is extend a pre-trained model to get a good boost in accuracy.</p>
<p class="calibre2">This model was built by hand, but it would be even better to extend something such as <kbd class="calibre12">Inceptionv3</kbd>, which is shown here:</p>
<div class="cdpaligncenter"><img src="../images/00212.jpeg" class="calibre20"/></div>
<p class="calibre2">It's quite deep; it has a lot of convolutional layers and, like most CNNs, it ends with a fully-connected layer or perhaps multiple fully connected layers. The <kbd class="calibre12">Inceptionv3</kbd> model was designed for ImageNet. Well, it's the dataset, and there's competitions associated with it where there are millions of images and 1,000 different classes, such as insects, houses, cars, and so on. The <kbd class="calibre12">Inceptionv3</kbd> model is state-of-the-art, or it was at one point. It was ImageNet's competition to combat other databases. We're going to use most of this network all the way up until the fully-connected layers. We don't want the final fully-connected or dense layers because those are designed for ImageNet. Specifically, there are 1,000 outputs and that's not good for us. We don't need to recognize the ImageNet images. We do need to recognize our bird images however, and there's only 200 different classes.</p>
<p class="calibre2">So, we just chop off the front of that and replace it with our own fully-connected layer, or multiple layers. We're going to use all the convolutions that it learned, and all of the kernels that it learned based on those ImageNet images. Let's go to the code. To do this, import <kbd class="calibre12">Inceptionv3</kbd> from Keras's applications. There's other models that you can choose from that Keras has available as well:</p>
<div class="cdpaligncenter"><img src="../images/00213.jpeg" class="calibre55"/></div>
<p class="calibre2">We're going to use the data generator just like we did previously.</p>
<p class="calibre2">This is where it starts to become different:</p>
<div class="cdpaligncenter"><img src="../images/00214.jpeg" class="calibre56"/></div>
<p class="calibre2">First, load the <kbd class="calibre12">InceptionV3</kbd> model using the ImageNet weights. <kbd class="calibre12">include_top =False</kbd> means to drop off the dense fully connected layers at the top. That's what they call the top. That's where it finally produces 1,000 different outputs. We don't want that. We want just the convolutions. This would be called the <kbd class="calibre12">base_model</kbd>. Call <kbd class="calibre12">x</kbd>, which is the output of the base model, add a <kbd class="calibre12">GlobalAveragePooling</kbd>, which means that it's computing the average across the whole convolution, and then put in some dense layers, with 1,024 dense neurons and another layer of 200. Of course, the 200 is because we have 200 different bird species, and the 1,024 is just to learn how the convolutions can match the bird species and then produce a model with those layers. The input of the model is the input of <kbd class="calibre12">Inceptionv3</kbd> and the output is <kbd class="calibre12">out_layer = Dense(200, activation='softmax')(x)</kbd>.</p>
<p class="calibre2">At this point, you can call regular model functions such as compile, but before we compile, we want to mark all of the base model layers and all of the convolutions as not trainable.</p>
<p class="calibre2">We're going to perform two steps here. When we attached our new two dense layers, the 1,024 dense and the 200 dense, those have random weights, so they're pretty much useless so far. The convolutions have been learned on ImageNet, so they're good. We don't want to change the convolutions below all those kernels by training on our bird images until we get that new pair of dense layers in the right order. So, we're first going to mark those layers from the inception model as not trainable; just keep those numbers as they are<span class="calibre5">—</span>we're only going to train our two new layers:</p>
<div class="cdpaligncenter"><img src="../images/00215.jpeg" class="calibre57"/></div>
<p class="calibre2">That happens next on the fit generator, just like before.</p>
<p class="calibre2">We will do 100 epochs to start off:</p>
<div class="cdpaligncenter"><img src="../images/00216.jpeg" class="calibre58"/></div>
<p class="calibre2"/>
<p class="calibre2">And we'll do an evaluation:</p>
<div class="cdpaligncenter"><img src="../images/00217.jpeg" class="calibre59"/></div>
<p class="calibre2">So, now, we're up to 44% accuracy. So just by using the inception v3 weights and structure or ImageNet but replacing the top two layers with our own fully-connected network, we get a 20% boost from what we had with our own custom convolutional neural network. But we can do even better.</p>
<p class="calibre2">We can use what we just got so that the model has now trained the top two layers and marked everything as trainable:</p>
<div class="cdpaligncenter"><img src="../images/00218.jpeg" class="calibre20"/></div>
<p class="calibre2">So, now that the top two layers are kind of massaged into a form that is reasonable, with 44% accuracy, we're going to let the entire network update all of our bird images. We're going to do it very slowly using stochastic gradient descent with a very slow learning rate and a high momentum. Going through 100 epochs, we now have 64%:</p>
<div class="cdpaligncenter"><img src="../images/00219.jpeg" class="calibre60"/></div>
<p class="calibre2"/>
<p class="calibre2">So, we basically did a 20% boost each time. With the custom CNN, we got 22% accuracy just by starting from scratch. Now, of course, this is not as big of a network as the inception model, but it kind of shows what happens if you just start from scratch. Then, we started with inception, all the kernels, but then added our own random 2 layers on top, with random weights, trained those weights but did not change the kernels, and we got 44% accuracy. Finally, we went through and updated all the weights, kernels, and the top layer, and we got 64% accuracy.</p>
<p class="calibre2">So, this is far far better than what random guessing would be, which is 0.5%, and it's been an increasing gain in accuracy each time we've improved the model. You can save the result and then you can load it into a separate file, perhaps by loading the model:</p>
<div class="cdpaligncenter"><img src="../images/00220.jpeg" class="calibre61"/></div>
<p class="calibre2">You also want to know what the class names are if you want to print the name of the bird to the user:</p>
<div class="cdpaligncenter"><img src="../images/00221.jpeg" class="calibre62"/></div>
<p class="calibre2"/>
<p class="calibre2">In this case, we can just list the subdirectories in a sorted form because that's going to match the one -hot encoding, and we can define a function called <kbd class="calibre12">predict</kbd> where you give it a filename with an image in it and it loads that image. Make sure it resizes it and converts it into an array, divides it by 255, and then runs the predictor. All this was done for us before with the image generator:</p>
<div class="cdpaligncenter"><img src="../images/00222.jpeg" class="calibre20"/></div>
<p class="calibre2">But now, because we're doing this one at a time, we're just going to do it by hand instead. Run the prediction, find out what the best score was, the position, and retrieve the class name and then print it, plus the confidence. There's a couple of examples of just birds that I found on the internet:</p>
<div class="cdpaligncenter"><img src="../images/00223.jpeg" class="calibre63"/></div>
<p class="calibre2">I can't guarantee that these were not part of the training set, but who knows. In the case of the hummingbird, they got it right. The house wren was also predicted correctly. However, the goose was not predicted correctly. This is an example of letting the user type in filenames. So if you have your own images that are relatively close to photography type images, you should consider using a pre-trained model like <kbd class="calibre12">InceptionV3</kbd> to get a major gain in accuracy.</p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we discussed deep learning and CNNs. We practiced with convolutional neural networks and deep learning with two projects. First, we built a system that can read handwritten mathematical symbols and then revisited the bird species identifier form and changed the implementation to use a deep convolutional neural network that is significantly more accurate. This concludes the Python AI projects for beginners.</p>


            </article>

            
        </section>
    </body></html>