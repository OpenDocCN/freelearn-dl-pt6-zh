- en: Training Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](270a3617-74cd-4e64-98f7-eb0c4e3cbcf6.xhtml), *Composing Networks*,
    we learned how to create Caffe2 operators and how we can compose networks from
    them. In this chapter, the focus is on training neural networks. We will learn
    how to create a network that is intended for training and how to train it using
    Caffe2\. We will continue to use the MNIST dataset as an example. However, instead
    of the MLP network we built in the previous chapter, we will create a popular
    network named LeNet.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to training a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the training network for LeNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and monitoring the LeNet network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we provide a brief overview of how a neural network is trained.
    This will help us to understand the later sections where we use Caffe2 to actually
    train a network.
  prefs: []
  type: TYPE_NORMAL
- en: Components of a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We employ neural networks to solve a particular type of problem for which devising
    a computer algorithm would be onerous or difficult. For example, in the MNIST
    problem (introduced in [Chapter 2](270a3617-74cd-4e64-98f7-eb0c4e3cbcf6.xhtml),
    *Composing Networks*), handcrafting a complicated algorithm to detect the common
    stroke patterns for each digit, and thereby determining each digit, would be tedious.
    Instead, it is easier to design a neural network suited to this problem and then
    train it (as shown later in this chapter) using a lot of data to do the same.
    If the training data is diverse and the training is done carefully, such a network
    would also be far more robust to variations in the input data than any deterministic
    handcrafted algorithm would.
  prefs: []
  type: TYPE_NORMAL
- en: 'A neural network has two main components: its structure and its weights. We
    typically design the network structure and then use a training algorithm and training
    data to determine the weights. After it is trained, the network structure, with
    its embedded weights, can be used for inference on new unseen data, as shown in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/990caef5-f3af-4b7d-871a-f26df03aeb66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Structure and weights of a network used for inference'
  prefs: []
  type: TYPE_NORMAL
- en: Structure of a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The structure of a network is the series of its layers, their types, and their
    configurations. The structure is typically devised by a researcher or a practitioner
    familiar with the problem that the neural network is being designed to solve.
    For example, to solve image classification problems, computer vision researchers
    might typically use a series of convolution layers in the network. (We will learn
    about the convolution layer later in this chapter.) Various configuration parameters
    of each layer also need to be determined beforehand, such as the size and number
    of the convolution filters in a convolution layer. There is a huge amount of interest
    in using deep learning itself to ascertain the structure of a network suited to
    a particular problem. However, discussion of this meta-learning topic is beyond
    the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Weights of a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second component of a network is its weights and biases. We generally refer
    to them together as **weights**, or sometimes as **parameters**. These are the
    floating point values that are the parameters of every layer in the network. How
    the weights of a layer are used is determined by the type of layer. For example,
    in a fully connected layer, a bigger weight value might signify a stronger correlation
    between an input signal and the network's output. In a convolution layer, the
    weights of a convolution filter might signify what type of pattern or shape in
    the input it is looking for.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we sit down and devise the structure of a network to solve a particular
    problem. Our choices in this process will be limited by our understanding of the
    problem space, the types of layers available in the DL framework, the hardware
    constraints of the accelerator we are using, and how much training time we are
    prepared to put up with. For example, the memory available in a GPU or CPU might
    limit the number of weights we might use in a layer or the number of layers we
    might use in the network. The amount of training time we are willing to spend
    also limits the number of weights and layers we can use in a network, because
    the more of these we employ, the longer it may take for the network to converge
    and train.
  prefs: []
  type: TYPE_NORMAL
- en: Training process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have a network structure fleshed out, we can then use a DL framework
    such as Caffe2 to describe that structure. We then apply one of the many training
    algorithms available in the framework on our training data. This trains the network
    and learns the weights of the layers that would best amplify the signal and dampen
    the noise. This process is depicted in Figure 3.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1286ef22-d6f7-41aa-b550-43778aff6980.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Training is the process of learning the weights of a neural network
    using a training algorithm and data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks are typically trained using a gradient-based optimization algorithm.
    To do this, we first define an **objective function** or **loss function** for
    the network. This function computes a loss or error value by comparing the output
    of the network on a given input to the ground truth result of that input. The
    training process iteratively picks training data and computes its loss, and then
    uses the optimization algorithm to update the weights so that the error is reduced.
    This process is repeated until we see no further improvement in the accuracy of
    the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d3af34c-bebe-4b12-910d-1a905be35517.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Three stages of an iteration in training'
  prefs: []
  type: TYPE_NORMAL
- en: A single iteration of the training process is depicted in *Figure 3.3*. We can
    see that it has three distinct stages. The first stage is a **Forward pass**,
    where we essentially perform inference of the network with its current weights
    to obtain the result or hypothesis of the network. In the second stage, we compute
    the loss of the network using a loss function. The third stage is a **Backward
    pass**, where we use an algorithm called backpropagation to update the weights
    of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are commonly three variants of gradient descent we can employ to sample
    the training data used in every iteration of training. If we use the entire training
    dataset in each iteration, this process is called **batch gradient descent**.
    If we use one randomly chosen sample of the training data in each iteration, then
    the process is called **stochastic gradient descent**. The variant that is most
    commonly used is **mini-batch gradient descent**, where we use a randomly chosen
    subset of the training data in each iteration. For best results, this is done
    by shuffling the training data, and then dividing it into mini-batches used in
    each iteration. After we are finished with one run through the training data,
    called an **epoch**, we shuffle and divide again into batches and continue.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of this chapter, we will learn about the LeNet network that
    can be used for MNIST, how to build it, and how to use it for training using Caffe2.
  prefs: []
  type: TYPE_NORMAL
- en: LeNet network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter](270a3617-74cd-4e64-98f7-eb0c4e3cbcf6.xhtml) 2, *Composing Networks*,
    we built an MLP network that was composed of multiple pairs of fully connected
    layers and activation layers. In this chapter, we will build and train a **convolutional
    neural network** (**CNN**). This type of network is so named because it primarily
    uses convolution layers (introduced in the next section). For computer vision
    problems, CNNs have been shown to deliver better results with fewer numbers of
    parameters compared to MLPs. One of the first successful CNNs was used to solve
    the MNIST problem that we looked at earlier. This network, named **LeNet-5**,
    was created by Yann LeCun and his colleagues:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8d74f04-007f-4288-a8c2-7f2c537fd2a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Structure of our LeNet model'
  prefs: []
  type: TYPE_NORMAL
- en: We will construct a network similar in spirit to the LeNet. We will refer to
    this as the LeNet model in the remainder of this book. From *Figure 3.4*, we can
    see that our LeNet network has eight layers. After the input layer, there are
    two pairs of convolution layers and pooling layers. They are followed by a pair
    of fully connected and ReLU activation layers and another fully connected layer.
    A final SoftMax layer is used to obtain the MNIST classification result.
  prefs: []
  type: TYPE_NORMAL
- en: 'We next look at two new layers that are important in CNNs and are part of LeNet:
    convolution and pooling.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *convolution layer* is the most important layer in neural networks that
    are used to solve computer vision problems, involving images and video. The input
    tensor to a convolution layer has at least three dimensions in its size: ![](img/0cfc418c-1293-4fde-b975-479b21fcecb4.png).
    That is, the input has ![](img/c945c225-fbf5-4062-92f5-6224033d66a8.png) channels,
    each channel being a 2D matrix of height ![](img/6c535cec-90bc-4514-a283-f594567d32b5.png)
    and width ![](img/85b6c19e-b58c-4b0b-8e2f-ee6cc2ef4aa4.png). This follows naturally
    from the layout of images. For example, an RGB image has three channels, each
    channel of a certain height and width.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we refer to **convolution**, we generally mean **2-dimensional** (**2D**)
    convolution. A 2D convolution layer has two sets of parameters that are learned
    during training: filter parameters and bias parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: The first set of parameters associated with a 2D convolution layer is ![](img/b00de931-2ab9-49a4-9a24-ef12fc601b8c.png)
    filters. Each **filter** or **kernel** is a **3-dimensional** (**3D**) tensor
    of shape ![](img/68dde4f0-a566-4cc2-8023-f7cb583b9038.png) holding floating point
    values that were learned during training. So, the total number of filter parameters
    that need to be learned during training for a 2D convolution layer is ![](img/a73b1f10-003e-47d0-bea6-7577d4814c77.png).
    Note how a kernel of a 2D convolution layer has the same number of channels, ![](img/35e85920-9ccd-4bf8-8464-8685609f987d.png),
    as the input to the layer.
  prefs: []
  type: TYPE_NORMAL
- en: The second set of parameters associated with a 2D convolution layer are ![](img/a845ffeb-7a17-457a-a671-414b043747aa.png)
    bias values, each value being associated with each of the ![](img/55ff6eb5-7ff4-456d-9e9e-bd2ababe2676.png)
    filters described previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'During convolution, each of the ![](img/0b647453-d81b-4a4d-bcd3-c3d3555d652a.png)
    kernels is slid across the width and height of the input. At every location where
    a kernel stops, a dot product is computed between the kernel values and the input
    values that overlap with the kernel, in order to obtain one output value for that
    location. Finally, the bias value associated with that kernel is added to each
    output value. This process is illustrated in Figure 3.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f7525d1-6f20-4e70-acef-c0ba0404c8e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: 2D convolution of ![](img/4cfefe2f-2b21-4f5f-bba1-96e80fb0b76c.png)
    input with two filters of shape ![](img/d9bfaa13-634c-4df9-a999-46af255782ad.png).
    Output is of the shape ![](img/40b5a8c5-6887-4478-9d8a-e5e815cee36d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Note how convolving with each kernel results in an output tensor of size ![](img/568b94bd-6728-4846-8c1e-05723ed034d7.png).
    Thus, when an input of size ![](img/3074dce5-23d2-4c38-af4b-28a556b54d5b.png)
    is fed to a 2D convolution layer, the resulting output is of size ![](img/55440054-fd94-4cd0-90a9-2671ce0b9e6b.png).
    If we feed a batch of ![](img/164214f4-a9f3-47f0-aabb-631e66695444.png) inputs
    to a 2D convolution layer, the resulting output is of size ![](img/3a91706e-03ab-4e94-b07b-e41dc7c7462b.png).
  prefs: []
  type: TYPE_NORMAL
- en: A 2D convolution layer has a few other arguments. A couple of important arguments
    are the stride and padding. **Stride** indicates how many values along the height
    and width a kernel moves before stopping to perform a convolution. For example,
    if the stride is ![](img/b20bf2d0-23dc-420a-a6f9-e9b75663ce10.png), kernels only
    visit every alternate location in the input. **Padding** indicates how much the
    height and width of input can be assumed to be expanded with **padding values**
    for performing convolution. Zero values are commonly used as padding values, and
    this is called **zero padding**.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another popular type of layer used in CNNs is called the **pooling layer**.
    It is typically used to reduce the width and height of the outputs of a previous
    layer. It operates by subsampling its input to produce the output. Unlike a convolution
    layer, a pooling layer does not have any pretrained parameters. It has two arguments
    associated with it: a **window size** and a **reduction function**. Similar to
    the convolution layer, a pooling layer has arguments such as stride and padding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What the pooling layer does is to slide the window of specified width and height
    across the input. At each location where it stops, it applies its reduction function
    to the input values in the window to produce a single output value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ba01ee9-2a27-4fc2-aaa9-43f129b9ffc8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Pooling layer producing ![](img/88ded9de-47b1-41ff-83bc-d224e781cc30.png)
    output after pooling ![](img/95426f58-5746-4bcf-bb2c-23f19c438f96.png) input with
    a ![](img/bd09d1ca-b52e-4e3c-884d-026fe55ead2c.png) pooling window'
  prefs: []
  type: TYPE_NORMAL
- en: Common reduction functions are max and average. In a max-pooling layer, the
    maximum of the values in the input window becomes the output value. In an **average-pooling
    layer**, the average of the values in the input window becomes the output value.
    *Figure 3.6* illustrates an operation in a max-pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: The pooling window is 2D and moves along the width and height of the input.
    So, it only reduces the width and height of the input. The number of channels
    remains the same in the input and in the output.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to look at a code example that trains a LeNet network. The
    complete source code for this is available as `ch3/mnist_lenet.py`. We begin by
    reading the MNIST training data in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use brew in this chapter to simplify the process of building our LeNet network.
    We begin by first initializing the model using `ModelHelper`, which was introduced
    in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We then add inputs to the training network using our `add_model_inputs` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Training data is usually stored in a **database** (**DB**) so that it can be
    accessed efficiently. Reading from a DB is usually faster than reading from thousands
    of individual files on the filesystem. For every training image in the MNIST dataset,
    the DB stores the ![](img/794e7fbf-e3ba-4db8-85c5-2896e17aed95.png) grayscale
    pixel values of the image and the digit that is in the image. Each grayscale pixel
    value is an 8-bit unsigned integer, with values in the range ![](img/30176a5d-e2f0-4dae-a677-971f411d6f79.png).
    The actual digit that is in each image is called a **label** and is usually annotated
    by a human by inspecting the image. For example, if the handwritten digit in the
    image is a 9, then a human annotator would have looked at the image and given
    it a label of 9.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our `add_model_inputs` method, we use a convenient brew helper function
    named `db_input` to connect the DB to our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the names of the blobs in our workspace to which the image and label
    data should be stored: `input_images_uint8` and `input_labels`. We also specify
    the batch size and information required to access the DB, such as its name and
    type.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks almost always work with float values, ideally normalized to
    the range ![](img/12c6eec1-e08e-476d-99b7-f59c6ff39426.png). So, we indicate that
    our input image data, which is an 8-bit unsigned integer data type, should be
    cast to the float data type and normalized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how the Caffe2 `ModelHelper` provides helpful methods to perform both
    these operations with ease: `Cast` and `Scale`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we add a `StopGradient` operator to the image data blob to indicate
    to the backward pass algorithm not to compute gradients for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We do this because the input layer is not a real layer of the neural network.
    It has no learnable parameters and does not have anything to be trained. So, the
    backward pass can stop there and does not need to move past it. `StopGradient`
    is a pseudo operator in Caffe2 that achieves this effect.
  prefs: []
  type: TYPE_NORMAL
- en: Building LeNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We build the LeNet layers required for inference by calling the `build_mnist_lenet`
    method in our script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note how we only pass in the image pixel data input to this network and not
    the labels. The labels are not required for inference; they are required for training
    or testing to use as ground truth to compare against the prediction of the network’s
    final layer.
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of the following subsections describe how we add pairs of convolution
    and pooling layers, the fully connected and ReLU layers, and the final SoftMax
    layer, to create the LeNet network.
  prefs: []
  type: TYPE_NORMAL
- en: Layer 1 – Convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first layer in LeNet is a convolution layer, which we introduced earlier
    in this chapter. We build it from a Caffe2 2D convolution operator, `Conv2D`,
    available in the operators' catalog. This can be added to the model using the
    handy `brew.conv` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating the operator, we specify that the input is a single-channel matrix
    of grayscale values. We also indicate that the output should have `20` channels,
    each channel holding a matrix. Finally, we specify that each convolution kernel
    used should have a width and height of `5` pixels. In Caffe2, we can provide minimal
    information like this and the API figures out the rest of the necessary arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let's expand these values to get a better idea of the sizes of the input, output,
    and kernels of this layer. Since the MNIST dataset is a ![](img/164e8dc6-95e9-4536-9238-878e3fac1a99.png)
    grid of values of a single grayscale channel, the input to this first layer of
    the network is a 3D array of size ![](img/50a726c1-8bfb-4082-bd09-9da816d48385.png).
    We are performing 2D convolution here, where each kernel has the same number of
    channels as the input to the layer. Furthermore, we indicated that the kernel
    width and height should be 5\. So, the size of each kernel is ![](img/1c43255e-5f55-41d0-8d2d-91a990fb4818.png).
    Since we indicated that we want 20 channels of output from this layer, we need
    20 such kernels. Hence, the actual size of kernel parameters of this layer is
    ![](img/4302d892-80e0-402a-885f-3261315691a4.png). Convolution layers also use
    a bias value, one for each output channel, so the size of bias values is ![](img/ec2c64ef-8cb2-4b64-9289-f339d9d492ae.png).
  prefs: []
  type: TYPE_NORMAL
- en: If a ![](img/94d82477-647b-4887-9b0c-d527cd3e3bf0.png) kernel is convolved on
    a ![](img/0096a185-07f2-4eb9-a84f-e8b185bf58d4.png) input with a stride of 1,
    the result is ![](img/c55d39ae-efd3-42d1-9911-152d98acfcdc.png). When 20 such
    kernels are used, the result is ![](img/eabb0e24-f509-4813-bcf3-f5ab7c4e7884.png).
    This is the size of the output of this layer.
  prefs: []
  type: TYPE_NORMAL
- en: Layer 2 – Max-pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The output of the first convolution layer is connected to a max-pooling layer,
    introduced earlier in this chapter. We build it from a Caffe2 max-pooling operator,
    `MaxPool`, available in the operators'' catalog. This can be added to the model
    using the handy `brew.max_pool` method. When creating this operator, we specify
    that its kernels are 2 x 2 in size, and that the stride is 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output of the previous convolution layer was of the size ![](img/53b05c6d-a7ac-4662-8ec4-3fe0b3823426.png).
    When max-pooling using window size ![](img/a0bd1dfd-87bb-4d09-a39c-cf5d3930d410.png)
    and stride 2 is performed, the output is of the size ![](img/41f92570-45d4-4f54-b0a0-df93ccd522de.png).
  prefs: []
  type: TYPE_NORMAL
- en: Layers 3 and 4 – Convolution and max-pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first pair of convolution and pooling layers is followed by another pair
    of convolution and pooling layers in LeNet, to further reduce the width and height
    and increase the channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The second pair of convolution and pooling layers is similar to the first pair,
    in that convolution kernels have a size of 5 x 5 and the stride is 2, while the
    max-pooling window size is ![](img/5ce98050-ac22-45cc-bc9f-42ce41850c58.png) and
    the stride is 2\. What is different is that the second convolution layer uses
    50 kernels to produce an output having 50 channels. After the second convolution
    layer, the output is of the size ![](img/e333ca6a-e958-4d28-be29-7fc833f6bb1d.png).
    After the second max-pooling layer, the output is ![](img/b024ade5-42d5-43a5-8142-03fc5b8dc05e.png).
    Note how the width and height of the inputs have gone down dramatically.
  prefs: []
  type: TYPE_NORMAL
- en: Layers 5 and 6 – Fully connected and ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The convolution and pooling layers are followed by a pair of fully connected
    and ReLU layers, added using the handy methods, `brew.fc` and `brew.relu`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The input to the fully connected layer is of size ![](img/d2047bd9-1ae3-4748-86a4-cbdbec659ae1.png).
    This 3D input is flattened to a vector of size 800 when fed to the fully connected
    layer. We have specified the output size of the layer as `500`. So this layer
    needs to learn ![](img/6e72624a-259d-4f1c-b667-1f78d84e2b3b.png) values, plus
    a bias value, during training, so that they can be used during inference. The
    output of the fully connected layer is fed to a ReLu layer, which acts as an activation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Layer 7 and 8 – Fully connected and Softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LeNet-5 uses a second fully connected layer to reduce the output down to the
    `10` values required to predict probabilities for the 10 digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'A final SoftMax layer converts the 10 output values of the fully connected
    layer to a probability distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Training layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In earlier sections, we built the layers of a LeNet network required for inference
    and added inputs of image pixels and the label corresponding to each image. In
    this section, we are adding a few layers at the end of the network required to
    compute the loss function and for backpropagation. These layers are only required
    during training and can be discarded when using the trained network for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Loss layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we noted in the *Introduction to training* section, we need a `loss` function
    at the end of the network to determine the error of the network. Caffe2 provides
    implementations of many common loss functions as operators in its operators' catalog.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we compute the loss value using **categorical cross-entropy
    loss**. This loss is typically used to measure the performance of a classification
    model whose output is between `0` and `1`. In Caffe2, this loss can be implemented
    as a composition of two operators, `LabelCrossEntropy` and `AveragedLoss`, shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Optimization layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Introduction to training* section, we noted how a gradient-based optimization
    algorithm lies at the heart of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first indicate to Caffe2 to use the output of the loss layer we added earlier
    to start the computation of gradients during the backward pass during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `AddGradientOperators` method takes away the pain of specifying these operators
    explicitly and adds them to the network for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we specify the gradient-based optimization algorithm **Stochastic
    Gradient Descent** (**SGD**) to be used for our training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We specify important SGD parameters, such as the learning rate to use, the policy
    to use in order to change the learning rate, the step size, and gamma.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization algorithms are implemented as `Optimizer` in Caffe2\. The DL framework
    has implementations of many common optimization algorithms, including SGD, Adam,
    AdaGrad, RMSProp, and AdaDelta. In our preceding call, we used a helpful wrapper,
    `build_sgd`, provided by the `optimizer` module that configures the SGD optimizer
    for us.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we indicate that the accuracy of the model should be tracked with
    a call to our `add_accuracy_op` method, which has this statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note the second argument to the function call. This indicates to Caffe2 that
    the output of the SoftMax layer should be compared against the ground truth labels
    to determine the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The `accuracy` layer is helpful for human supervision of the training process.
    We can perform inference at any point in the training process and, using the output
    of the accuracy layer, get a sense of how accurate the network is at that point.
  prefs: []
  type: TYPE_NORMAL
- en: Training and monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We begin the training process by creating the network in the workspace and
    initializing all the parameter blobs of the network in the workspace. This is
    done by calling the workspace `RunNetOnce` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we ask Caffe2 to create the network in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We are finally ready to train. We iterate a predetermined number of times and,
    in each iteration, we use the workspace `RunNet` method to run a forward pass
    and a backward pass.
  prefs: []
  type: TYPE_NORMAL
- en: Training a small network such as our LeNet model is fast both on CPU and GPU.
    However, many of the real models you train might take several hours or days to
    train. For this reason, it is a good idea to constantly monitor the training process
    by extracting the loss and accuracy after every training iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our LeNet model, we use the following code to extract the loss and accuracy
    values after each training iteration from the output blobs of the loss and accuracy
    layers we added earlier to the training network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can monitor the health of the training by looking at the raw values, or
    importing them into a spreadsheet, or plotting them in a graph. Figure 3.7 shows
    a graph plotted from the values of one such training session:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d9b338b-8f4d-49f9-a229-0509f7ec5df7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Loss and accuracy of training our model'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the loss is high at the beginning. This is expected, as we typically
    initialize a network with zero or random weights. As the training proceeds, we
    see that the loss decreases and, correspondingly, the accuracy of the network
    increases. If you do not see the loss decreasing or accuracy increasing, then
    that indicates a problem with our training parameters or training data. If the
    training pace is slow or is causing values to blow off, then you might need to
    tweak the learning rate and such parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We can typically stop training at any iteration where the loss curve is leveling
    off and the accuracy is suitable. To aid the export of a model at a particular
    iteration, it is a good idea to export the model to disk (demonstrated in [Chapter
    5](4481e225-7882-4625-9d42-63ba41e74b4f.xhtml), *Working with Other Frameworks*)
    after each iteration. That way, you can pick the model at the best iteration after
    the training is complete.
  prefs: []
  type: TYPE_NORMAL
- en: Another useful practice is to measure the accuracy on a validation dataset after
    every epoch or so. **Validation data** is typically a portion of the training
    data that is separated out for this purpose prior to the training. We did not
    use validation data in our example, in order to keep it simple.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the general training process for a neural
    network using a gradient-based optimization algorithm. We learned about CNNs and
    the classic LeNet CNN to solve the MNIST problem. We built this network, and learned
    how to add training and test layers to it, so that we could use it for training.
    We finally used this network to train and learned how to monitor the network during
    training using Caffe2\. In the following chapters, we will learn how to work with
    models trained using other frameworks, such as Caffe, TensorFlow, and PyTorch.
  prefs: []
  type: TYPE_NORMAL
