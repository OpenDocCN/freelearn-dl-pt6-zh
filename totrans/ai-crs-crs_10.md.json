["```py\n self.signal1 = int(np.sum(sand[int(self.sensor1_x)-10:int(self.sensor1_x)+10, int(self.sensor1_y)-10:int(self.sensor1_y)+10]))/400\\.   #81\n        self.signal2 = int(np.sum(sand[int(self.sensor2_x)-10:int(self.sensor2_x)+10, int(self.sensor2_y)-10:int(self.sensor2_y)+10]))/400\\.   #82\n        self.signal3 = int(np.sum(sand[int(self.sensor3_x)-10:int(self.sensor3_x)+10, int(self.sensor3_y)-10:int(self.sensor3_y)+10]))/400\\.   #83 \n```", "```py\n xx = goal_x - self.car.x   #126\n        yy = goal_y - self.car.y   #127\n        orientation = Vector(*self.car.velocity).angle((xx,yy))/180\\.   #128 \n```", "```py\n state = [orientation, self.car.signal1, self.car.signal2, self.car.signal3]   #129 \n```", "```py\naction2rotation = [0,20,-20]   #34\n\n        rotation = action2rotation[action]   #131 \n```", "```py\n if sand[int(self.car.x),int(self.car.y)] > 0:   #138\n            self.car.velocity = Vector(1, 0).rotate(self.car.angle)   #139\n            reward = -1   #140 \n```", "```py\n if sand[int(self.car.x),int(self.car.y)] > 0:   #138\n            self.car.velocity = Vector(1, 0).rotate(self.car.angle)   #139\n            reward = -1   #140\n        else:   #141\n            self.car.velocity = Vector(6, 0).rotate(self.car.angle)   #142\n            reward = -0.2   #143\n            if distance < last_distance:   #144\n                reward = 0.1   #145 \n```", "```py\n# AI for Autonomous Vehicles - Build a Self-Driving Car   #1\n#2\n# Importing the libraries   #3\n#4\nimport os   #5\nimport random   #6\nimport torch   #7\nimport torch.nn as nn   #8\nimport torch.nn.functional as F   #9\nimport torch.optim as optim   #10\nfrom torch.autograd import Variable   #11 \n```", "```py\n# Creating the architecture of the Neural Network   #13\n#14\nclass Network(nn.Module):   #15\n    #16\n    def __init__(self, input_size, nb_action):   #17\n        super(Network, self).__init__()   #18\n        self.input_size = input_size   #19\n        self.nb_action = nb_action   #20\n        self.fc1 = nn.Linear(input_size, 30)   #21\n        self.fc2 = nn.Linear(30, nb_action)   #22\n    #23\n    def forward(self, state):   #24\n        x = F.relu(self.fc1(state))   #25\n        q_values = self.fc2(x)   #26\n        return q_values   #27 \n```", "```py\n# Implementing Experience Replay   #29\n#30\nclass ReplayMemory(object):   #31\n    #32\n    def __init__(self, capacity):   #33\n        self.capacity = capacity   #34\n        self.memory = []   #35\n    #36\n    def push(self, event):   #37\n        self.memory.append(event)   #38\n        if len(self.memory) > self.capacity:   #39\n            del self.memory[0]   #40\n    #41\n    def sample(self, batch_size):   #42\n        samples = zip(*random.sample(self.memory, batch_size))   #43\n        return map(lambda x: Variable(torch.cat(x, 0)), samples)   #44 \n```", "```py\n# Implementing Deep Q-Learning   #46\n#47\nclass Dqn(object):   #48\n    #49\n    def __init__(self, input_size, nb_action, gamma):   #50\n        self.gamma = gamma   #51\n        self.model = Network(input_size, nb_action)   #52\n        self.memory = ReplayMemory(capacity = 100000)   #53\n        self.optimizer = optim.Adam(params = self.model.parameters())   #54\n        self.last_state = torch.Tensor(input_size).unsqueeze(0)   #55\n        self.last_action = 0   #56\n        self.last_reward = 0   #57 \n```", "```py\n def select_action(self, state):   #59\n        probs = F.softmax(self.model(Variable(state))*100)   #60\n        action = probs.multinomial(len(probs))   #61\n        return action.data[0,0]   #62 \n```", "```py\n def learn(self, batch_states, batch_actions, batch_rewards, batch_next_states):   #64\n        batch_outputs = self.model(batch_states).gather(1, batch_actions.unsqueeze(1)).squeeze(1)   #65\n        batch_next_outputs = self.model(batch_next_states).detach().max(1)[0]   #66\n        batch_targets = batch_rewards + self.gamma * batch_next_outputs   #67\n        td_loss = F.smooth_l1_loss(batch_outputs, batch_targets)   #68\n        self.optimizer.zero_grad()   #69\n        td_loss.backward()   #70\n        self.optimizer.step()   #71 \n```", "```py\n def update(self, new_state, new_reward):   #73\n        new_state = torch.Tensor(new_state).float().unsqueeze(0)   #74\n        self.memory.push((self.last_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward]), new_state))   #75\n        new_action = self.select_action(new_state)   #76\n        if len(self.memory.memory) > 100:   #77\n            batch_states, batch_actions, batch_rewards, batch_next_states = self.memory.sample(100)   #78\n            self.learn(batch_states, batch_actions, batch_rewards, batch_next_states)   #79\n        self.last_state = new_state   #80\n        self.last_action = new_action   #81\n        self.last_reward = new_reward   #82\n        return new_action   #83 \n```", "```py\nbrain = Dqn(4,3,0.9)   #33 \n```", "```py\n state = [orientation, self.car.signal1, self.car.signal2, self.car.signal3]   #129\n        action = brain.update(state, reward)   #130 \n```", "```py\n def save(self):   #85\n        torch.save({'state_dict': self.model.state_dict(),   #86\n                    'optimizer' : self.optimizer.state_dict(),   #87\n                   }, 'last_brain.pth')   #88\n    #89\n    def load(self):   #90\n        if os.path.isfile('last_brain.pth'):   #91\n            print(\"=> loading checkpoint... \")   #92\n            checkpoint = torch.load('last_brain.pth')   #93\n            self.model.load_state_dict(checkpoint['state_dict'])   #94\n            self.optimizer.load_state_dict(checkpoint['optimizer'])   #95\n            print(\"done !\")   #96\n        else:   #97\n            print(\"no checkpoint found...\")   #98 \n```", "```py\nconda create -n selfdrivingcar python=3.6 \n```", "```py\nconda activate selfdrivingcar \n```", "```py\nconda install pytorch==0.3.1 -c pytorch \n```", "```py\nconda install -c conda-forge/label/cf201901 kivy \n```", "```py\ncd Desktop \n```", "```py\ncd AI-Crash-Course-master \n```", "```py\npython map.py \n```", "```py\n if distance < last_distance:   #144\n                reward = 0.1   #145 \n```"]