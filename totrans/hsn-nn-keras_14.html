<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Contemplating Present and Future Developments</h1>
                </header>
            
            <article>
                
<p>Throughout the course of this book, we had the good fortune to explore together an intriguing idea that populates, and currently dominates, the realm of <strong>Artificial Intelligence</strong> (<strong>AI</strong>): <strong>Artificial Neural Networks</strong> (<strong>ANNs</strong>). On our journey, we had the opportunity to get detailed insight into the functioning of neural models, including the feed-forward, convolutional, and recurrent networks, and thereby <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>). We continued our journey by subsequently exploring self-supervised methods, including <strong>Reinforcement Learning </strong>(<strong>RL</strong>) with deep Q-networks, as well as autoencoders. We finalized our excursion by going over the intuition behind generative models.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Sharing representations with transfer learning</li>
<li>Transfer learning on Keras</li>
<li>Concluding our experiments</li>
<li>Learning representations</li>
<li>Limits of current neural networks</li>
<li>Encouraging sparse representation learning</li>
<li>Tuning hyperparameters</li>
<li>Automatic optimization and evolutionary algorithms</li>
<li>Multi-network predictions and ensemble models</li>
<li>The future of AI and neural networks</li>
<li>The road ahead</li>
<li>The problems with classical computing</li>
<li>The advent of quantum computing</li>
<li>Quantum neural networks</li>
<li>Technology and society</li>
<li>Contemplating the future</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Sharing representations with transfer learning</h1>
                </header>
            
            <article>
                
<p>One powerful paradigm that we have not yet had the pleasure of discussing is the notion of <strong>transfer learning</strong>. In our excursions, we saw various methods and techniques that allow neural networks to induct powerful and accurate representations from the data they see.</p>
<p>Yet, what if we wanted to transfer these learned representations to other networks? This can be quite useful if we are tackling a task where not a lot of training data is available beforehand. Essentially, transfer learning seeks to leverage commonalities among different learning tasks that may share similar statistical features. Consider the following case: you are a radiologist who wants to use a <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>) to classify different pulmonary diseases, using images of chest X-rays. The only problem is you only have about a hundred labeled images of chest X-rays. Since you can't go about ordering X-rays for any unsuspecting patient to augment your dataset, you are required to get creative. Maybe you have different images of the same phenomenon (such as MRIs and CT scans), or perhaps you have a lot of X-ray images from different body parts. So, why not use these?</p>
<p>Since we know that earlier layers in CNNs learn the same low-level features (such as edges, line segments, and curvatures), why not simply reuse these learned features from a different task and fine tune that model to our new learning task ? In many cases, transfer learning can save a lot of time, when compared to training a network from scratch, and is a very useful tool to have in your deep learning repertoire. In that spirit, let's explore one very last hands-on example: implementing a simple transfer learning workflow on Keras.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Transfer learning on Keras</h1>
                </header>
            
            <article>
                
<p>In this section, we will explore a very simplistic transfer learning methodology in Keras. The idea behind this is simple: why waste precious computation resources on learning the repetitive low-level features common to almost all images?</p>
<p>We will use the famous <kbd>CIFAR10</kbd> dataset to illustrate our implementation, by making it our task to classify images pertaining to any of the 10 image categories present in the dataset. However, we will augment our learning experience by using layers from pretrained networks and adding them to a network of our own. To do this, we will import a very deep CNN, that has already been trained on expensive <strong>Graphics Processing Units</strong> (<strong>GPUs</strong>) for hundreds of hours and simply fine-tune it to our use case. The model in question that we will use is the same <strong>VGG net</strong> we used back in <a href="https://cdp.packtpub.com/hands_on_neural_networks_with_keras/wp-admin/post.php?post=655&amp;action=edit">Chapter 4</a>, <em>Convolutional Neural Networks,</em> to visualize how a neural network sees a cheetah.</p>
<p>This time, however, we will be slicing it open and picking out some of its intermediate layers to splice into our own model, thereby transferring what it has learnt to a new task. We will begin by making some imports:</p>
<pre> "import numpy as np\n",<br/> "import matplotlib.pyplot as plt\n",<br/> "% matplotlib inline\n",<br/> "\n",<br/> "\n",<br/> "from keras import applications\n",<br/> "from keras import optimizers\n",<br/> "from keras.models import Sequential, Model \n",<br/> "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, Input\n",<br/> "from keras import backend as k \n",<br/> "from keras.datasets import cifar10\n",<br/> "from keras import utils"</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading a pretrained model</h1>
                </header>
            
            <article>
                
<p>We define our image dimensions and load in the VGG16 model with the training weights it achieved on the <strong>ImageNet</strong> classification task (ILSVRC), excluding its input layer. We do this since the images we will train the network on differ in dimension from the original ones it was trained on. In the following code block, we can visually summarize the model object that we loaded in:</p>
<pre>img_width, img_height = 32, 32<br/>model = applications.VGG19(weights = "imagenet", include_top=False, input_shape = (img_width, img_height, 3))<br/>model.summary()</pre>
<p>The output will be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-784 image-border" src="Images/c97cb657-860e-4ae4-91fb-61494c2a7237.png" style="width:31.50em;height:46.42em;" width="495" height="730"/></p>
<p>This is quite a big model. In fact, it has about 20 million trainable parameters!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Obtaining intermediate layers from a model</h1>
                </header>
            
            <article>
                
<p>Thanks to Keras's Lego-like modular interface, we can do some really cool things, such as break apart the aforementioned model and reuse its layers as part of another network. This will be our next step, and it can be easily achieved using the functional API:</p>
<pre>model2= Model(inputs=model.input, outputs=   <br/>              model.get_layer('block3_pool').output) <br/>model2.summary()</pre>
<p>The result obtained will be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-783 image-border" src="Images/50903a31-7d23-45ac-b8e4-443bbea2bd26.png" style="width:32.00em;height:31.00em;" width="486" height="470"/></p>
<p>Notice that all we had to do is initiate a model object using the functional API and pass it the first 12 layers of the VGG net. This is achieved by using the <kbd>.get_layer()</kbd> method on the VGG model object and passing it a layer name. Recall that the name of an individual layer can be verified by using the <kbd>.summary()</kbd> method on a given model object.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Adding layers to a model</h1>
                </header>
            
            <article>
                
<p>Now we have retrieved the pretrained intermediate layers from the VGG net. Next, we can connect more sequential layers to these pretrained layers. The idea behind this is to use the representations learned by the pretrained layers and build upon them, thereby augmenting the classification task with knowledge from a different learning task:</p>
<pre> #Adding custom Layers<br/> num_classes = 10<br/><br/> x = model2.output<br/> x = Flatten()(x)<br/> x = Dense(1024, activation="relu")(x)<br/> x = Dropout(0.5)(x)<br/> x = Dense(1024, activation="relu")(x)<br/> predictions = Dense(num_classes, activation="softmax")(x)</pre>
<p>To add more layers to our model, we will again use the functional API syntax to create a simple feed-forward network, which takes the output values from the selected VGG net layers and flattens them into 2D arrays, before passing them forward to densely connected layers with 1,024 neurons. This layer then connects to a heavy-dropout layer, where half of the neural connections from the previous layer is ignored while training.</p>
<p>Next, we have another dense layer of 1,024 neurons before reaching the final output layer. The output layer is equipped with 10 neurons, pertaining to the number of classes in our training data, as well as a softmax activation function, which will generate a 10-way probability score for each observation seen by the network.</p>
<p>Now that we have defined the layers we wish to add to the network, we can once again use the functional API syntax to merge the two separate models together:</p>
<pre># creating the final model<br/>model_final = Model(input = model2.input, output = predictions)<br/>model_final.summary()</pre>
<p>You will get this output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-782 image-border" src="Images/d2966191-d843-44a6-89a5-fb91b8b0bf4c.png" style="width:32.17em;height:41.08em;" width="482" height="616"/></p>
<p>Most important, we must freeze the layer weights of the VGG model, so as to benefit from the representations it has encoded during its previous training session on those nice expensive GPUs.</p>
<p>Here, we only chose to freeze the first four layers and decided to let the rest of the architecture retrain on this new learning task:</p>
<pre># Freeze the layers that dont need to train<br/>for layer in model2.layers[:4]:<br/>    layer.trainable = False</pre>
<p>Other approaches may choose to keep the entire model architecture frozen and only reinitialize the weights of the last layer of the model. We encourage you to try freezing different numbers of layers, and exploring how this changes the network's learning experience, by visualizing the loss convergence, for example.</p>
<p>Intuitively, different learning tasks may require different approaches. It naturally depends on a multitude of factors, such as the similarity between tasks, similarity between the training data, and so on.</p>
<p>A common rule of thumb is to only reinitialize the weights of the last layer if very little data is available on the target learning task. Conversely, if a lot of data is available on the target task, then it is even conceivable to reinitialize weights for the entire network during training. In this case, you would have simply used a pretrained model and reimplemented it for a different use case. As always with deep learning, the answer lies in experimentation.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading and preprocessing the data</h1>
                </header>
            
            <article>
                
<p>Next, we preprocess the CIFAR10 images and vectorize the labels, as we have been doing throughout the course of this book. There is nothing special here to note:</p>
<pre>(x_train, y_train),(x_test, y_test)=cifar10.load_data() <br/>x_train = x_train.astype('float32')<br/>x_test = x_test.astype('float32')<br/>x_train /= 255<br/>x_test /= 255<br/>y_train = utils.to_categorical(y_train, num_classes)<br/>y_test = utils.to_categorical(y_test, num_classes)</pre>
<p>We first load the images in the first code block. In the second block, we normalize the pixel values to float values between <kbd>0</kbd> and <kbd>1</kbd>. Finally, in the last block, we one-hot encode our labels. Now, our network is ready to be compiled and trained.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training the network</h1>
                </header>
            
            <article>
                
<p>Next, we compile our model, with the categorical cross-entropy <kbd>loss</kbd> function and the <kbd>Adam</kbd> optimizer. Then, we can initiate the training session, as shown here:</p>
<pre># compile the model<br/>model_final.compile(loss = "categorical_crossentropy", optimizer =  <br/>      optimizers.Adam(lr=0.0001), metrics=["accuracy"])</pre>
<p>The following will be the output obtained:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-785 image-border" src="Images/ba9db3f4-a560-4a89-a32d-816fa0b6ecc6.png" style="width:57.67em;height:9.50em;" width="820" height="134"/></p>
<p>The model was trained in batches of 128 images for 10 epochs. The validation accuracy achieved was about 85%, which was considerably better than the same model trained from scratch. You can try this out yourself, by unfreezing the layers we froze, before training the model. There we have it. Now you have implemented a transfer learning workflow in Keras and are able to reuse neural networks for use cases requiring pretraining or fine tuning</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<ol>
<li>Experiment with different model depths by retrieving more blocks from the pretrained VGG net. Does the accuracy improve substantially with deeper models? Vary where you pick layers.</li>
<li>Change the number of trainable layers; how does this affect the convergence of <kbd>loss</kbd>?</li>
<li>Try out a different model out of the 10 pretrained ones available in <kbd>keras.applications</kbd> to build a classifier using the notion of transfer learning.</li>
<li>Listen to Andrew Ng talk about transfer learning: <a href="https://www.youtube.com/watch?v=yofjFQddwHE">https://www.youtube.com/watch?v=yofjFQddwHE</a>.<a href="https://www.youtube.com/watch?v=yofjFQddwHE"/></li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Concluding our experiments</h1>
                </header>
            
            <article>
                
<p>Such accounts bring an end to our explorations and experimentations with various neural network architectures. Yet, there is still a lot more to discuss and discover. After all, while our journey together comes close to fruition, yours has just begun! There are countless more use cases, architectural variations, and implementational details that we could go on to explore, yet doing so will deviate from our initial ambitions for this work. We wanted to achieve a detailed understanding of what neural networks actually do, how they operate, and under what circumstances they may be used, respectively. Furthermore, we want to develop an internal intuition of what is actually happening inside these networks, and why these architectures work as well as they do. The remainder of this chapter will be dedicated to solidifying this notion, allowing you to better relate to the underlying idea of representation learning and applying this notion to any future use cases you may want to address using neural networks. Finally, we will also take this opportunity to address some of the latest developments in the field of ANNs, and how different business and institutions alike have crafted utility for this technology. Finally, we will also attempt to take a step into the future and speculate on how coming developments may affect the scientific, economic, and social landscape, in the advent of phenomena such as big data, and potential technological leaps such as quantum computing.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Learning representations</h1>
                </header>
            
            <article>
                
<p>While we addressed the topic of representations and how this affects the task of learning in <a href="https://cdp.packtpub.com/hands_on_neural_networks_with_keras/wp-admin/post.php?post=24&amp;action=edit">Chapter 1</a>, <em>Overview of Neural Networks</em>, we can now afford for our discussion to deepen further, given the hands-on practical examples we have executed since.</p>
<p>By now, we are all well aware that the success of any <strong>Machine Learning</strong> (<strong>ML</strong>) algorithm (including deep learning algorithms such as neural networks) is directly dependent on the manner in which we chose to represent the data it is shown. What's the deal here?</p>
<p>To demonstrate the importance of representations and their impact on information processing, recall that we saw a succinct example earlier on in this book. We performed mathematical operations such as long division using Roman numerals, revealing the difficulty of carrying out such a task using suboptimal representations. Indeed, the way we choose to represent information directly impacts the way we process it, the sort of operations we are able to perform on it, and the kind of understanding we may derive.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">DNA and technology</h1>
                </header>
            
            <article>
                
<p>Consider another example: the DNA molecule. <strong>Deoxyribonucleic Acid</strong> (<strong>DNA</strong>) is a molecular structure made up of two intertwined chainlike threads, known as the <strong>double helix formation</strong>. The molecule can be broken down into <strong>simpler monomeric units</strong> (or <strong>nucleoids</strong>), forming base pairs composed of two of the four nitrogen-based building blocks (these being <strong>Adenine</strong> (<strong>A</strong>), <strong>Guanine</strong> (<strong>G</strong>), <strong>Thymine</strong> (<strong>T</strong>), and <strong>Cytosine</strong> (<strong>C</strong>)).</p>
<p>Many of you may be wondering at this point, "<em>what does this have to do with the subject at hand?"</em> Well, as it turns out, this molecular structure holds the blueprints of all lifeforms on this planet. The molecule governs how cells divide, become more complex structures, all of the way up to the preferences and behavior of the flora and fauna here on our home planet.</p>
<p>Needless to say, this quadrinary system for representing information has found a way to encode and copy instructions to produce all life we see around us! No representation format devised by humans so far has ever come close to simulating the grand sphere of life as we know it. In fact, we still struggle to simulate realistically immersive game environments for entertainment purposes. Curiously, by many estimates, the DNA molecule itself can be represented using our own binary system, with about 1.5 gigabytes of data. Think about it, 1.5 gigabytes of data, or one single Blueray disk, is capable of storing all the of instructions for life itself. But that's about all we can do. We can't exactly instruct the Blueray disk to incessantly replicate itself into the complexity we embody and see around us every day. Hardware considerations aside, a paramount reason why we cannot replicate the operations of life in this manner is due to the representation of the data itself! Hence, the way we represent data has severe implications on the kind of transformations we may perform, resulting in ever more complex information-processing systems.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Limits of current neural networks</h1>
                </header>
            
            <article>
                
<p>Similarly, in ML, it is hypothesized that different representations of data allow the capturing of different explanatory factors of variation present therein. The neural networks we saw were excellent at inducing efficient representations from their input values and leveraging these representations for all sorts of learning tasks. Yet, these input values themselves had to undergo a deluge of preprocessing considerations, transforming raw data into a format more palatable to the networks.</p>
<p>Currently, the deficiency of neural networks relates to their heavy dependence on such preprocessing and feature-engineering considerations to learn useful representations from the given data. On their own, they are unable to extract and categorize discriminative elements from raw input values. Often, behind every neural network, there is a human.</p>
<p>We are still required to use our ingenuity, domain knowledge, and curiosity in order to overcome this deficiency. Eventually, however, we will strive to devise systems that require minimal human intervention (in the form of feature engineering, for example) and to truly understand the raw data present in the world. Designing such a system is one of the paramount goals in the field of AI, and one that we hope you can help advance.</p>
<p>For the time being, however, we will cover some useful concepts that allow us to design better representations from raw data, thereby designing better learning experiences for our artificial counterparts.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Engineering representations for machines</h1>
                </header>
            
            <article>
                
<p>The topic of representation learning addresses exactly this. Intuitively, we ask ourselves this: <em>How can we make it easier for machines to extract useful information from data?</em> This notion is intrinsically linked to the idea that there exist certain generalizable assumptions about the world that may be applied to better interpret and synthesize the raw data available. These <span>generalized assumptions</span>, in conjunction with experimentative techniques, allow us to design good representations and discard bad ones. They serve as principles of experimentation when designing a preprocessing workflow for learning algorithms such as neural networks.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How should a good representation be?</h1>
                </header>
            
            <article>
                
<p>Intuitively, a good representation should be capable of disentangling the main factors of variation that cause an occurrence. Hence, one approach may be to augment the analytics workflow in a manner so as to make it easier for machines to spot these factors of variance.</p>
<p>Researchers, over the decades, have amassed a set of heuristic assumptions applicable in the field of deep learning that allow us to do exactly this. Next, we will reproduce a subset of such heuristics, or regularization strategies, that are known to augment the learning experience of deep neural networks.</p>
<div class="packt_infobox">For a comprehensive technical review of all of the considerations involved in representation learning, please refer to this excellent paper by some of the pioneers of deep learning—<em>Representation Learning: A Review and New Perspectives</em> (Y Bengio, A Courville, Pascal Vincent, 2016): <a href="https://arxiv.org/pdf/1206.5538.pdf">https://arxiv.org/pdf/1206.5538.pdf</a>.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Preprocessing and data treatments</h1>
                </header>
            
            <article>
                
<p>As you must already be well aware, neural networks are quite picky eaters. Namely, there are two staple operations that need to be performed before our data can be fed to a neural network: <strong>vectorization</strong> and <strong>normalization</strong>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Vectorization</h1>
                </header>
            
            <article>
                
<p>Recall that vectorization simply means that all of the inputs and target variables of your data must be in a tensor format, containing floating-point values (or in specific cases, such as the Boston Housing Price Regression example, integers). We previously achieved this by populating a matrix of zeros using indexed values (as in the sentiment classification example), or by one-hot encoding our variables.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Normalization</h1>
                </header>
            
            <article>
                
<p>Besides vectorization, another consideration we had to undertake was normalization of our input data. This was more of a standard practice in most ML workflows and consisted of transforming our input variables into a small, homogeneous range of values. We achieved this in tasks such as image processing by normalizing the pixel values between 0 and 1. In cases where our input variables were on different scales (such as with the Boston example), we had to implement an independent feature-wise normalization strategy. Forgoing such steps may cause gradient updates that do not converge to a global minimum, making it much more difficult for a network to learn. In general, a rule of thumb can be to try independent feature normalization, ensuring a feature-wise mean of 0 and a standard deviation of 1.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Smoothness of the data</h1>
                </header>
            
            <article>
                
<p>Neural networks struggle the least with predictions if they are shown a data distribution that is locally smooth. What does this mean? Simply put, if an input, <em>x</em>,<em> </em>produces an output, <em>y</em>, a point close to this input will produce an output proportionally close to <em>y</em>. This is the property of smoothness and greatly augments learning architecture such as neural networks, allowing them to capture better representations from such data. Unfortunately, however, having this property in your data distribution is not the only criteria for neural networks to learn good representations; the curse of dimensionality, for example, is still one that would need to be addressed, by feature selection or dimensionality reduction.</p>
<p>Adding something such as a smoothening factor to your data, for example, can largely benefit the learning process, as we did when predicting stock market prices with LSTMs.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Encouraging sparse representation learning</h1>
                </header>
            
            <article>
                
<p>Suppose you were training a network to classify pictures of cats and dogs. Over the course of training, the intermediate layers will learn different representations or features from the input values (such as cat ears, dog eyes, and so on), combining them in a probabilistic fashion to detect the presence of an output class (that is, whether a picture is of a cat or a dog).</p>
<p>Yet, while performing inference on an individual image, do we need the feature that detects cat ears to ascertain that this particular image is of a dog? The answer in almost all cases is a resounding no. Most of the time, we can assume that most features that a network learns during training are actually not relevant for each individual prediction. Hence, we want our network to learn sparse representations for each input, a resulting tensor representation where most entries are zero (denoting perhaps the presence or absence of the corresponding features).</p>
<p>In deep learning, sparsity is a very desirable property for learned representations. Not only does this allow us to have a smaller number of neurons active when representing a phenomenon (thereby increasing the efficiency of our network), but it also helps the network to better untangle the main factors of variance present within the data itself.</p>
<p>Intuitively, sparsity allows the network to recognize learned features in data, without being perturbed by small variations occurring in the inputs. Implementation wise, sparsity simply enforces the value of most learned features to zero, when representing any individual input. Sparse representations can be learned through operations such as one-hot encoding, non-linear transformations as imposed by the activation functions, or by other means of penalizing derivatives of the intermediate layers, with respect to the input values.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tuning hyperparameters</h1>
                </header>
            
            <article>
                
<p>In general, it is assumed that deeper model architectures give access to higher representational power, allowing us to hierarchically organize abstract representations for predictive tasks.</p>
<p>However, as we know, deeper architectures are prone to overfitting, and hence can be challenging to train, requiring keen attention to aspects such as regularization (as seen with the regularization strategies explored in <a href="https://cdp.packtpub.com/hands_on_neural_networks_with_keras/wp-admin/post.php?post=26&amp;action=edit">Chapter 3</a>, <em>Signal Processing - Data Analysis with Neural Networks</em>). How can we assess exactly how many layers to initialize, with the appropriate number of neurons and relevant regularization strategies to use? Given the complexity involved in designing the right architecture, it can be very time consuming to experiment with different model hyperparameters to find the right network specifications to solve the task at hand.</p>
<p>While we have discussed general intuitions on designing more robust architectures, using techniques such as dropout and batch normalization, we can't help but wonder whether there is a way to automate this entire tedious process. It would even be tempting to apply deep learning to this process itself, where it not a discretely constrained optimization problem (as opposed the continuous optimization problems we have so far been solving, using gradient decent).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Automatic optimization and evolutionary algorithms</h1>
                </header>
            
            <article>
                
<p>Fortunately, many tools exist that allow such automatic parameter optimization. <strong>Talos</strong> (<a href="https://github.com/autonomio/talos">https://github.com/autonomio/talos</a>) is one such tool built on top of the Keras library, made available as open source on GitHub. It allows you to predefine a set of hyperparameters (such as different number of layers, neurons per layer, and activation functions), after which the tool will automatically train and compare those Keras models to assess which one performs better.</p>
<p>Other solutions such as <strong>Hyperas</strong> (<a href="https://github.com/maxpumperla/hyperas">https://github.com/maxpumperla/hyperas</a> ) or <strong>auto_ML</strong> (<a href="https://auto-ml.readthedocs.io/en/latest/">https://auto-ml.readthedocs.io/en/latest/</a>) allow similar functionalities and can help drastically reduce development time, allowing you to discover what hyperparameters work best for your task. In fact, you can use such tools and make your own genetic algorithms that help you select from a pool of hyperparameters, train and evaluate a network, then select the best of those network architectures, randomly mutate some hyperparameters of the selected networks, and repeat the training and evaluation all over again. Eventually, such an algorithm can produce increasingly complex architectures to solve a given problem, just as evolution does in nature. While a detailed overview of such methods is well beyond the scope of this book, we take the liberty of linking a simplistic implementation of such an approach next, which allows evolving network parameters in order to find ideal configurations.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>Evolutionary algorithms and neural networks</strong>: <a href="http://www.weiss-gerhard.info/publications/C22.pdf">http://www.weiss-gerhard.info/publications/C22.pdf</a></li>
<li><strong>Implementation of evolutionary neural networks</strong>: <a href="https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164">https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164</a></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multi-network predictions and ensemble models</h1>
                </header>
            
            <article>
                
<p>Another way to get the best of neural networks is by using ensemble models. The idea is quite simple: why use one network when you can use many? In other words, why not design different neural networks, each sensitive to specific representations in the input data? Then, we can average out their predictions, getting a more generalizable and parsimonious prediction than using just one network.</p>
<p>We can even attribute weights to each network, by pegging each network's prediction to the test accuracy it achieves on the task. Then, we can take a weighted average of the predictions (weighted with their relative accuracies) from each network to get to a more comprehensive prediction altogether.</p>
<p>Intuitively, we just look at the data with different eyes; each network, by virtue of its design, may pay attention to different factors of variance, perhaps ignored by its other counterparts. This method is fairly straightforward and simple to implement and only requires designing separate networks, with good intuition on what kind of representations each network can be expected to capture. After that, it is simply a question of adding appropriate weights to each individual network's prediction and averaging out the results.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The future of AI and neural networks</h1>
                </header>
            
            <article>
                
<p>Throughout the course of this book, we have dived deep into a specific realm of AI, nested within ML, that we call deep learning. This caveat of machine intelligence takes a connectionist approach, combining the predictive power of distributed representations, in turn learned by a deep neural network.</p>
<p>While deep learning neural networks have risen to prominence, since the advent of GPU, accelerated computing, and the availability of big data, many considerations have gone into improving the intuition and implementation behind these architectures, since their re-ascension to popularity, about a decade ago (Hinton et al, 2008). Yet, still, there exist many complex tasks that deep learning is not yet able to adequately tackle.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Global vectors approach</h1>
                </header>
            
            <article>
                
<p>Sometimes, the sequence of mathematical transformations on given input values is simply not enough to learn an effective function mapping them to some output values. Already, many such examples exist, especially in the domain of <strong>natural language processing</strong> (<strong>NLP</strong>). While we restricted our NLP use cases to simple word vectorization, this approach can be limiting for some use cases requiring the understanding of complex dependencies that exist in human language.</p>
<p>Instead, a popular approach is to symbolically attribute properties to words and attribute values to these properties so as to allow comparison with other words. This is the basic intuition behind a technique known as <strong>Global Vectors</strong> (<strong>GloVe</strong>) used as a text-preprocessing vectorization technique, before data is fed to neural networks. Such an approach perhaps alludes to how the use of deep learning will evolve in the future. This specific workflow illustrates the use of principles from both distributed and symbolic representations to discover, understand, and solve complex problems, such as the logical reasoning involved in machine question-answering.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Distributed representation</h1>
                </header>
            
            <article>
                
<p>In the future, it is very likely that we start using principles from various disciplines of AI, in conjunction with the power of distributed representation that deep learning brings to the table, to design systems that are truly and generally intelligent. Such systems can then go about learning tasks in an autonomous manner, with the enhanced capability to tackle complex problems. It could, for example, conduct research following scientific methodology, thereby automating human knowledge discovery. In short, deep learning is here to stay, and will likely be complemented by other subfields of AI, to develop very powerful computing systems.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hardware hurdles</h1>
                </header>
            
            <article>
                
<p>Yet, before we get to that stage of AI, surely there are other improvements to be made. Recall that deep learning became popular not only because we learned techniques to represent and process data on a higher level, but also because our hardware improved drastically. We now have access to the processing power that would have cost us millions about a few decades ago, for literally a few thousand dollars. Similarly, there may yet be another hardware hurdle for humanity to overcome before we can design truly intuitive and logically superior systems, capable of solving humanity's grand problems.</p>
<p>Many have speculated that this giant leap will materialize in the form of quantum computing. While covering this topic in depth is a bit beyond the scope of this book (and the proficiencies of this author), we could not help but include a short parenthesis to illustrate the benefits and complexities involved in importing neural networks to an emerging computing paradigm, with promising prospects.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The road ahead</h1>
                </header>
            
            <article>
                
<p>While the previous diagram depicting the advances in processing power might make us look back in nostalgia over how far we have come, this same nostalgia will be wiped away quite fast as soon as we realize how far we still have to go.</p>
<p>As we saw in the preceding diagram, the computational power of the systems we have implemented so far are nowhere near that of a human brain. The neural networks that we devised (at least in this book) had a number of neurons ranging anywhere from a million (the equivalent of what you would find in a cockroach) to about ten million (close to what is common for an adult zebra fish).</p>
<p>Attempting to train a network that parallels a human mind, at least in the number of neurons used, is currently beyond the scope of human engineering, as of the date of this book. It simply surpasses our current computing capacity. Moreover, it is important to note that this comparison naturally ignores the detail that the neurons in each of these learning systems (artificial versus biological) are different, both in form and function.</p>
<p>Biological neurons operate much differently than their artificial counterparts and are influenced by quantum systems such as molecular chemistry. The exact nature of information processing and storage in biological neurons is still not fully understood by modern neuroscience. So, how can we simulate what we don't yet fully comprehend? One answer to this dilemma could be to design more powerful computers, capable of representing and transforming information in ways more suited for the domain. This brings us to the phenomenon of quantum computing.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Problems with classical computing</h1>
                </header>
            
            <article>
                
<p>In simplistic terms, quantum mechanics is a field that deals with the study of things that are very small, isolated, and cold. While this may not create an appealing picture at first, consider the problem we are facing currently. Already, the exponential growth of the number of transistors in a chip, as predicted by Moore's law, seems to be slowing down.</p>
<p>Why is this important? These transistors are actually what permits us to compute! From simple data storage to the complex mathematical operations native to neural networks, all data representation in classical computers is by virtue of these semiconductor devices. We use them to amplify and switch electric signals, thereby creating logic gates capable of tracking the presence of charged electrons (1) or absence thereof (0). These switches can be manipulated to create binary digits, or bits, that represent a unit of information. In essence, this binary system forms the basis of all digital encoding, exploiting the physical properties of transistors to store and process information. It is the language of machines, which allows representing and processing information.</p>
<p>From the very first fully digital and programmable computer (Z3, 1938 ), to the latest supercomputers (IBMs Summit, 2018), this fundamental language of representation has not changed. For all intents and purposes, the lingua franca of machines has remained based on the binary system for about a century.</p>
<p>Yet, as we discussed earlier, different representations allow us to perform different operations. Hence, perhaps it is time for us to revise the fundamental manner in which we represent data. Given the fact that transistors can only get so small, we are slowly but surely reaching the limits of classical computing. Hence, what better place to look for solutions than the infinitesimally small and bizarre world of quantum mechanics.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The advent of quantum computing</h1>
                </header>
            
            <article>
                
<p>While classical computers use binary representations to encode information into bits, their quantum counterparts use the laws of physics to encode information in <strong>Q-Bits</strong>. There are many approaches toward designing such systems. You can, for instance, use microwave pulses to alter the spin momentum of an electron, to represent and store information.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Quantum superposition</h1>
                </header>
            
            <article>
                
<p>As it turns out, this may allow us to leverage interesting quantum phenomena to represent operations that have no known classical counterpart. Operations such as <strong>quantum superposition</strong>, where two different quantum states may be added together to produce a third state, valid on its own. Hence, unlike its classical counterpart, a Q-Bit can have three states: (0), (1), and (1/0,), where the third represents a state only achievable through the property of quantum superposition.</p>
<p>Naturally, this allows us to represent much more information, opening doors for us to tackle problems from higher-complexity classes (such as simulating intelligence, for example).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Distinguishing Q-Bits from classical counterparts</h1>
                </header>
            
            <article>
                
<p>Other quantum properties also exist that distinguish the Q-Bit from its classical counterpart. For example, two Q-Bits can enter an entangled state, where the spin of the electrons of each Q-Bit is set to continuously point in opposite directions.</p>
<p>Why is this a big deal? Well, these two Q-Bits can then be separated by billions of miles, while still seemingly maintaining a link between each other. We know, by virtue of the laws of physics, that the spin of each electron will always point in opposite directions when observed, regardless of the distance between the Q-Bits themselves.</p>
<p>This entangled state is interesting, because there is no classical operation that can represent the idea of two different bits having no specified value, yet always remaining the opposite value of each other. These concepts have the potential of revolutionizing fields such as communication and cryptography, on top of the exponential computing power they bring to the table. The more Q-Bits a quantum computer can leverage, the more non-classical operations it can use to represent and process data. In essence, these are some of the pivotal underlining ideas behind quantum computing.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Quantum neural networks</h1>
                </header>
            
            <article>
                
<p>Many of you may be thinking that all of this is nice, but we ourselves are surely decades away from being able to use a quantum computer, let alone design neural networks on it. While healthy skepticism is always nice, it does not do justice to the efforts of contemporary researchers, scientists, and businesses working around the clock to bring such systems to life. It may surprise you to know, for example, that anybody in the world with an internet connection today has free access to a quantum computer, using the link right here (courtesy of IBM): <a href="https://quantumexperience.ng.bluemix.net/qx/editor">https://quantumexperience.ng.bluemix.net/qx/editor</a>.<a href="https://quantumexperience.ng.bluemix.net/qx/editor"/></p>
<p>In fact, researchers such as Francesco Tacchino and his colleagues have already used this service to implement quantum neural networks for classification tasks! They were able to implement the world's first quantum perceptron, similar in spirit to the perceptron we saw in <a href="https://cdp.packtpub.com/hands_on_neural_networks_with_keras/wp-admin/post.php?post=37&amp;action=edit">Chapter 2</a>,<span> </span><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label"><em>A Deeper Dive into Neural Networks</em>, </span></span>yet augmented with the laws of quantum mechanics. They used IBM's <strong>Q-5</strong> <strong>Tenerife</strong> superconducting quantum processor, which allows the manipulation of up to five Q-Bits, to train a classifier to detect simple patterns such as line segments.</p>
<p>While this may sound trivial at first, the implications of their work are quite significant. They were able to decisively show how a quantum computer allows an exponential increase in the number of dimensions it can process. For instance, while a classical perceptron is capable of processing input values of <em>n </em>dimensions, its quantum counterpart designed by these researchers was able to process 2N dimensions! Such implementations pave the way for future researchers to implement more complex architectures.</p>
<p>Naturally, the realm of quantum neural networks is still in infancy, since quantum computers themselves have a lot of improvements to undergo. However, active research currently focuses on many areas of importing neural nets to the quantum world, ranging from straightforward extensions of connected layers, to quantum optimization algorithms that are better at navigating the loss landscape.</p>
<p>Some have even speculated that quantum phenomena such as tunneling may be used to, quite literally, tunnel through the loss landscape to converge to optimal network weights extremely quickly! This truly represents the dawn of a new age for ML and AI. Once these systems have been thoroughly tried and tested, we may be able to represent truly complex patterns in novel ways, with implications beyond our current imagination.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>A paper on quantum neural networks: <a href="https://arxiv.org/pdf/1811.02266.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1811.02266.pdf</a></li>
<li>A QNN paper by Google: <a href="https://arxiv.org/pdf/1802.06002.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1802.06002.pdf</a></li>
<li>The Google Quantum AI blog: <a href="https://ai.googleblog.com/2018/12/exploring-quantum-neural-networks.html" target="_blank" rel="noopener noreferrer">https://ai.googleblog.com/2018/12/exploring-quantum-neural-networks.html</a></li>
<li>Quantum optimization algorithms: <a href="https://ieeexplore.ieee.org/abstract/document/6507335" target="_blank" rel="noopener noreferrer">https://ieeexplore.ieee.org/abstract/document/6507335</a></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technology and society</h1>
                </header>
            
            <article>
                
<p>Today, we stand at the intersection of very interesting times. Times that some claim will define the future of humankind and change the way we perceive and interact with the world altogether. Automation, cognitive technologies, AI, and quantum computing are but a few among the sea of disruptive technologies, constantly causing organizations to reassess their value chains and better themselves in the way they impact the world.</p>
<p>Perhaps people will be able to work more efficiently, organize their time better, and devote their lives to activities that uniquely complement their skill sets, thereby delivering optimal value to the society they participate in. Or, perhaps, there is more of a dystopic future ahead of us, where such technologies are used to disenfranchise the masses, observe and control human behavior, and limit our freedom. While the technology itself is simply analogous to any tool previously invented by humans, the way we choose to use these tools will have reverberating consequences for all the involved stakeholders. Ultimately, the choice is ours. Luckily, we are at the dawn of this new era, and so we can still steer the direction of progress in a sustainable and inclusive manner.</p>
<p>Currently, organizations across the globe are rushing to find ways to reap the fruit from such technologies before it is too late for them to adapt, leading to all sorts of concerns spanning from transparency to legality and ethics. These dilemmas surface despite the fact that we are still in the infancy phase of AI. In essence, all the methods and techniques we explored through the course of this book are narrow AI technologies. They are specific systems capable of solving narrow components of a workflow, be it to solve specific computer vision tasks or to answer certain types of questions in natural language. This is very different than the idea of AI, in its literal sense: an intelligence that is autonomous and can learn in a self-sufficient manner, without outsiders directly manipulating its internal learning algorithm. It is an intelligence that can grow and evolve, similar in spirit to the journey of a human baby to an adult, albeit at a different rate.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Contemplating our future</h1>
                </header>
            
            <article>
                
<p>Consider a new-born human baby. At first, it is even incapable of breathing and has to be motivated to do so by a few friendly spanks, delivered by the attending physician. For the first few months, this being does not seem to do anything remarkable and is incapable of independent movement, let alone thought. Yet, slowly, this same baby develops an internal model of the world around it. It becomes better and better at distinguishing all this light it sees and the cacophony of sounds it hears. Soon, it starts recognizing things such as movement, perhaps in the guise of a friendly face, hovering around with deliciously gooey substances. A bit later, it develops a premature internal physics engine, through the observation of the world around it. It then uses these representations to first crawl, then toddle, and eventually even walk, progressively updating its internal physics engine to represent more and more complex models of the world. Soon enough, it is able to perform somersaults, compose elaborate poetry, and peruse causes such as mathematics, history, philosophy, or even AI science.</p>
<p>Do note that nobody is exactly tuning a CNN to make the baby see better, or increasing the size of an LSTM architecture, for the baby to write better poetry. The individual was able to do so without any direct external intervention, by simply observing things around itself, listening to people, and learning by doing. While there are a multitude of things going on under the hood of a human baby as it journeys to adulthood, almost all of which are quite beyond the scope of this work, this example demonstrates how far we still are from creating something that can truly parallel our own intellect.</p>
<p>The same type of baby can eventually learn to drive cars, and with a little bit of help, solve complex problems such as world hunger or interplanetary travel! This is truly an intelligent organism. The artificial counterparts that we explored in our book are not yet worthy of comparison to the former form of intelligence, simply due to their narrow applicability. They are but pieces of a puzzle, a manner of approaching information processing, often for a specific cognitive domain. Perhaps one day, these narrow technologies will be united in a comprehensive system, splicing a multitude of such technologies together, creating something even greater than the components within. In fact, this is currently happening, as we have seen throughout this book already. For example, we saw how convolutional architectures may be merged with other neural network architectures such as LSTMs, for complex visual information processing involving a temporal component, as in making the right moves in a game.</p>
<p>But the question then still remains: will such architectures truly become intelligent? This may be a question for the philosophers of today, but it is also one for the scientists of tomorrow. As these systems evolve, and conquer more and more realms that were previously through attainable only by humans, we will eventually face such existential questions about these machines and ourselves. Are we really that different? Are we just very complex computers, carrying out arithmetic operations through biology? Or is there more to intelligence and consciousness than mere computation? Sadly, we do not have all the answers, yet this does make for an exciting journey ahead for our species.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary </h1>
                </header>
            
            <article>
                
<p>In this chapter, we reiterated what we have learned in this book and saw how we can improve the existing techniques. We then moved on to see the future of deep learning and gained insight into quantum computing.</p>
<p>I hope this journey has been informative. Thanks for reading and all the best!</p>


            </article>

            
        </section>
    </div>



  </body></html>