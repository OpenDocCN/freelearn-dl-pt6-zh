["```py\nval conf = new NeuralNetConfiguration.Builder()\n     .trainingWorkspaceMode(WorkspaceMode.SEPARATE)\n```", "```py\nval conf = new NeuralNetConfiguration.Builder()\n     .inferenceWorkspaceMode(WorkspaceMode.SINGLE)\n```", "```py\nNd4j.getMemoryManager.togglePeriodicGc(false)\n```", "```py\nval gcInterval = 10000 // In milliseconds\n Nd4j.getMemoryManager.setAutoGcWindow(gcInterval)\n```", "```py\nNd4j.getWorkspaceManager.destroyAllWorkspacesForCurrentThread\n```", "```py\nval mmap = WorkspaceConfiguration.builder\n    .initialSize(1000000000)\n    .policyLocation(LocationPolicy.MMAP)\n    .build\n\ntry (val ws = Nd4j.getWorkspaceManager.getAndActivateWorkspace(mmap, \"M2\")) {\n    val ndArray = Nd4j.create(20000) //INDArray\n}\n```", "```py\n<dependency>\n <groupId>org.nd4j</groupId>\n <artifactId>nd4j-cuda-9.2</artifactId>\n <version>0.9.1</version>\n</dependency>\n```", "```py\nCudaEnvironment.getInstance.getConfiguration.allowMultiGPU(true)\n```", "```py\nRuntimeException: Can't allocate [HOST] memory: [memory]; threadId: [thread_id];\n```", "```py\nval basicConfig = WorkspaceConfiguration.builder\n   .policyAllocation(AllocationPolicy.STRICT)\n   .policyLearning(LearningPolicy.FIRST_LOOP)\n   .policyMirroring(MirroringPolicy.HOST_ONLY)\n   .policySpill(SpillPolicy.EXTERNAL)\n   .build\n```", "```py\n<dependency>\n     <groupId>org.deeplearning4j</groupId>\n     <artifactId>dl4j-spark_2.11</artifactId>\n     <version>0.9.1_spark_2</version>\n</dependency>\n```", "```py\n<plugin>\n    <groupId>org.apache.maven.plugins</groupId>\n    <artifactId>maven-shade-plugin</artifactId>\n    <version>3.2.1</version>\n    <configuration>\n      <!-- put your configurations here -->\n    </configuration>\n    <executions>\n      <execution>\n        <phase>package</phase>\n        <goals>\n          <goal>shade</goal>\n        </goals>\n      </execution>\n    </executions>\n</plugin>\n```", "```py\nmvn package -DskipTests\n```", "```py\n<groupId>org.googlielmo</groupId>\n<artifactId>rnnspark</artifactId>\n<version>1.0</version>\n```", "```py\n$SPARK_HOME/bin/spark-submit --class <package>.<class_name> --master <spark_master_url> <uber_jar>.jar\n```", "```py\nsudo pip install tensorflow\n```", "```py\nsudo pip install tensorflow-gpu\n```", "```py\nsudo pip install keras\n```", "```py\nfrom keras.models import Sequential\n from keras.layers import Dense\n```", "```py\nmodel = Sequential()\n```", "```py\nmodel.add(Dense(units=64, activation='relu', input_dim=100))\n model.add(Dense(units=10, activation='softmax'))\n```", "```py\nmodel.compile(loss='categorical_crossentropy',\n               optimizer='sgd',\n               metrics=['accuracy'])\n```", "```py\nmodel.save('basic_mlp.h5')\n```", "```py\nsudo python basic_mlp.py\n```", "```py\ngroupId: org.deeplearning4j\n artifactId: deeplearning4j-modelimport\n version: 0.9.1\n```", "```py\nval basicMlp = new ClassPathResource(\"basic_mlp.h5\").getFile.getPath\n```", "```py\nval model = KerasModelImport.importKerasSequentialModelAndWeights(basicMlp)\n```", "```py\nval input = Nd4j.create(256, 100)\n var output = model.output(input)\n```", "```py\nmodel.fit(input, output)\n```", "```py\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)\n```", "```py\nval mnistTf = new ClassPathResource(\"mnist.pb\").getFile\n val sd = TFGraphMapper.getInstance.importGraph(mnistTf)\n```", "```py\nfor(i <- 1 to 10){\n    val file = \"images/img_%d.jpg\"\n    file = String.format(file, i)\n    val prediction = predict(file) //INDArray\n    val batchedArr = Nd4j.expandDims(arr, 0) //INDArray\n    sd.associateArrayWithVariable(batchedArr, sd.variables().get(0))\n    val out = sd.execAndEndResult //INDArray\n    Nd4j.squeeze(out, 0)\n    ...\n}\n```"]