["```py\n# Libraries\nlibrary(keras)\nlibrary(mlbench)\nlibrary(psych)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(neuralnet)\n```", "```py\n# Data structure\ndata(BostonHousing)\nstr(BostonHousing)\n\nOUTPUT\n'data.frame':        506 obs. of  14 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : num  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ b      : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n```", "```py\n# Converting factor variables to numeric\ndata <- BostonHousing\ndata %>% lapply(function(x) as.numeric(as.character(x)))\ndata <- data.frame(data)\n```", "```py\n# Neural network\nn <- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+b+lstat,\n                data = data,\n                hidden = c(10,5),\n                linear.output = F,\n                lifesign = 'full',\n                rep=1)\n\n# Plot\nplot(n, col.hidden = \"darkgreen\", \n      col.hidden.synapse = 'darkgreen',\n      show.weights = F, \n      information = F, \n      fill = \"lightblue\")\n```", "```py\ndata <- as.matrix(data)\ndimnames(data) <- NULL \n```", "```py\n# Data partitioning\nset.seed(1234)\nind <- sample(2, nrow(data), replace = T, prob=c(.7, .3))\ntraining <- data[ind==1, 1:13]\ntest <- data[ind==2, 1:13]\ntrainingtarget <- data[ind==1, 14]\ntesttarget <- data[ind==2, 14]\n```", "```py\n# Normalization\nm <- colMeans(training)\nsd <- apply(training, 2, sd)\ntraining <- scale(training, center = m, scale = sd)\ntest <- scale(test, center = m, scale = sd)\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential()\nmodel %>%\n   layer_dense(units = 10, activation = 'relu', input_shape = c(13)) %>%  \n   layer_dense(units = 5, activation = 'relu') %>%\n   layer_dense(units = 1) \nsummary(model)\n\nOUTPUT\n___________________________________________________________________________\nLayer (type)                     Output Shape                Param #      \n===========================================================================\ndense_1 (Dense)                   (None, 10)                   140          \n___________________________________________________________________________\ndense_2 (Dense)                   (None, 5)                    55           \n___________________________________________________________________________\ndense_3 (Dense)                   (None, 1)                     6            \n===========================================================================\nTotal params: 201\nTrainable params: 201\nNon-trainable params: 0\n___________________________________________________________________________\n```", "```py\n# Compile model\nmodel %>% compile(loss = 'mse', \n   optimizer = 'rmsprop', \n   metrics = 'mae')\n```", "```py\n# Fit model\nmodel_one <- model %>%  \n   fit(training,\n   trainingtarget,\n   epochs = 100,\n   batch_size = 32,\n   validation_split = 0.2)\n\nOUTPUT from last 3 epochs\nEpoch 98/100\n284/284 [==============================] - 0s 74us/step - loss: 24.9585 - mean_absolute_error: 3.6937 - val_loss: 86.0545 - val_mean_absolute_error: 8.2678\nEpoch 99/100\n284/284 [==============================] - 0s 78us/step - loss: 24.6357 - mean_absolute_error: 3.6735 - val_loss: 85.4038 - val_mean_absolute_error: 8.2327\nEpoch 100/100\n284/284 [==============================] - 0s 92us/step - loss: 24.3293 - mean_absolute_error: 3.6471 - val_loss: 84.8307 - val_mean_absolute_error: 8.2015\n```", "```py\nplot(model_one)\n```", "```py\n# Model evaluation\nmodel %>%  evaluate(test, testtarget) \n\nOUTPUT\n ## $loss\n ## [1] 31.14591 \n ##\n ## $mean_absolute_error\n ## [1] 3.614594\n```", "```py\n# Prediction\npred <- model %>%  predict(test)\ncbind(pred[1:10], testtarget[1:10])\n\nOUTPUT\n [,1] [,2]\n [1,] 33.18942 36.2\n [2,] 18.17827 20.4\n [3,] 17.89587 19.9\n [4,] 13.07977 13.9\n [5,] 14.17268 14.8\n [6,] 19.09264 18.4\n [7,] 19.81316 18.9\n [8,] 21.00356 24.7\n [9,] 30.50263 30.8\n[10,] 19.75816 19.4\n```", "```py\nplot(testtarget, pred,\n      xlab = 'Actual',\n      ylab = 'Prediction')\n abline(a=0,b=1)\n```", "```py\n# Model Architecture\nmodel <- keras_model_sequential()\nmodel %>%\n layer_dense(units = 100, activation = 'relu', input_shape = c(13)) %>% \n layer_dropout(rate = 0.4) %>%\n layer_dense(units = 50, activation = 'relu') %>%\n layer_dropout(rate = 0.3) %>%\n layer_dense(units = 20, activation = 'relu') %>%\n layer_dropout(rate = 0.2) %>%\n layer_dense(units = 1)  \nsummary(model)\n\nOUTPUT\n ## ___________________________________________________________________________\n ## Layer (type)                     Output Shape                  Param # \n ## ===========================================================================\n ## dense_4 (Dense)                  (None, 100)                   1400 \n ## ___________________________________________________________________________\n ## dropout_1 (Dropout)              (None, 100)                   0 \n ## ___________________________________________________________________________\n ## dense_5 (Dense)                  (None, 50)                    5050 \n ## ___________________________________________________________________________\n ## dropout_2 (Dropout)              (None, 50)                    0 \n ## ___________________________________________________________________________\n ## dense_6 (Dense)                  (None, 20)                    1020 \n ## ___________________________________________________________________________\n ## dropout_3 (Dropout)              (None, 20)                    0 \n ## ___________________________________________________________________________\n ## dense_7 (Dense)                  (None, 1)                     21 \n ## ===========================================================================\n ## Total params: 7,491\n ## Trainable params: 7,491\n ## Non-trainable params: 0\n ## _________________________________________________________________________\n\n# Compile model\nmodel %>% compile(loss = 'mse', \n                   optimizer = 'rmsprop', \n                   metrics = 'mae')\n\n# Fit model\nmodel_two <- model %>%  \n   fit(training,\n       trainingtarget,\n       epochs = 100,\n       batch_size = 32, \n       validation_split = 0.2)\nplot(model_two)\n```", "```py\n# Model evaluation\nmodel %>%  evaluate(test, testtarget) \n\nOUTPUT\n ## $loss\n ## [1] 24.70368 \n ##\n ## $mean_absolute_error\n ## [1] 3.02175 \n\npred <- model %>%  predict(test)\nplot(testtarget, pred,\n     xlab = 'Actual', \n     ylab = 'Prediction')\nabline(a=0,b=1)\n```", "```py\n# log transformation and model architecture \ntrainingtarget <- log(trainingtarget)\n testtarget <- log(testtarget)\n model <- keras_model_sequential()\n model %>%\n   layer_dense(units = 100, activation = 'relu', input_shape = c(13)) %>%  \n   layer_dropout(rate = 0.4) %>% \n   layer_dense(units = 50, activation = 'relu') %>%\n   layer_dropout(rate = 0.2) %>%\n   layer_dense(units = 25, activation = 'relu') %>%\n   layer_dropout(rate = 0.1) %>%\n   layer_dense(units = 1)\n summary(model)\n\nOUTPUT\n## ___________________________________________________________________________\n ## Layer (type)                     Output Shape                  Param # \n ## ===========================================================================\n ## dense_8 (Dense)                  (None, 100)                   1400 \n ## ___________________________________________________________________________\n ## dropout_4 (Dropout)              (None, 100)                   0 \n ## ___________________________________________________________________________\n ## dense_9 (Dense)                  (None, 50)                    5050 \n ## ___________________________________________________________________________\n ## dropout_5 (Dropout)              (None, 50)                    0 \n ## ___________________________________________________________________________\n ## dense_10 (Dense)                 (None, 25)                    1275 \n ## ___________________________________________________________________________\n ## dropout_6 (Dropout)              (None, 25)                    0 \n ## ___________________________________________________________________________\n ## dense_11 (Dense)                 (None, 1)                     26 \n ## ===========================================================================\n ## Total params: 7,751\n ## Trainable params: 7,751\n ## Non-trainable params: 0\n ## ___________________________________________________________________________\n```", "```py\n# Compile model\nmodel %>% compile(loss = 'mse', \n                   optimizer = optimizer_rmsprop(lr = 0.005),\n                   metrics = 'mae')\n\n# Fit model\n model_three <- model %>%  \n   fit(training,\n       trainingtarget,\n       epochs = 100,\n       batch_size = 32, \n       validation_split = 0.2)\nplot(model_three)\n```", "```py\n# Model evaluation\nmodel %>%  evaluate(test, testtarget) \n\nOUTPUT\n## $loss\n ## [1] 0.02701566\n ##\n ## $mean_absolute_error\n ## [1] 0.1194756 \n\npred <- model %>%  predict(test)\n plot(testtarget, pred)\n```"]