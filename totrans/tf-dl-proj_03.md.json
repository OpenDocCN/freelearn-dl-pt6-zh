["```py\nFlickr8k_text\nCrowdFlowerAnnotations.txt\nFlickr_8k.devImages.txt\nExpertAnnotations.txt\nFlickr_8k.testImages.txt\nFlickr8k.lemma.token.txt\nFlickr_8k.trainImages.txt\nFlickr8k.token.txt readme.txt\n```", "```py\nimport os\nannotation_dir = 'Flickr8k_text'\n\n```", "```py\ndef read_file(file_name):\n    with open(os.path.join(annotation_dir, file_name), 'rb') as file_handle:\n        file_lines = file_handle.read().splitlines()\n    return file_lines\n```", "```py\ntrain_image_paths = read_file('Flickr_8k.trainImages.txt')\ntest_image_paths = read_file('Flickr_8k.testImages.txt')\ncaptions = read_file('Flickr8k.token.txt')\n\nprint(len(train_image_paths))\nprint(len(test_image_paths))\nprint(len(captions))\n```", "```py\n6000\n1000\n40460\n```", "```py\nimage_caption_map = {}\nunique_words = set()\nmax_words = 0\nfor caption in captions:\n    image_name = caption.split('#')[0]\n    image_caption = caption.split('#')[1].split('\\t')[1]\n    if image_name not in image_caption_map.keys():\n        image_caption_map[image_name] = [image_caption]\n    else:\n        image_caption_map[image_name].append(image_caption)\n    caption_words = image_caption.split()\n    max_words = max(max_words, len(caption_words))\n    [unique_words.add(caption_word) for caption_word in caption_words]\n```", "```py\nunique_words = list(unique_words)\nword_to_index_map = {}\nindex_to_word_map = {}\nfor index, unique_word in enumerate(unique_words):\n    word_to_index_map[unique_word] = index\n    index_to_word_map[index] = unique_word\nprint(max_words)\n```", "```py\nfrom data_preparation import train_image_paths, test_image_paths\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nimport numpy as np\nfrom keras.models import Model\nimport pickle\nimport os\n```", "```py\nclass ImageModel:\n    def __init__(self):\n        vgg_model = VGG16(weights='imagenet', include_top=True)\n        self.model = Model(input=vgg_model.input,\n                           output=vgg_model.get_layer('fc2').output)\n```", "```py\n    @staticmethod\n    def load_preprocess_image(image_path):\n        image_array = image.load_img(image_path, target_size=(224, 224))\n        image_array = image.img_to_array(image_array)\n        image_array = np.expand_dims(image_array, axis=0)\n        image_array = preprocess_input(image_array)\n        return image_array\n```", "```py\n    def extract_feature_from_imagfe_path(self, image_path):\n        image_array = self.load_preprocess_image(image_path)\n        features = self.model.predict(image_array)\n        return features.reshape((4096, 1))\n```", "```py\n    def extract_feature_from_image_paths(self, work_dir, image_names):\n        features = []\n        for image_name in image_names:\n            image_path = os.path.join(work_dir, image_name)\n            feature = self.extract_feature_from_image_path(image_path)\n            features.append(feature)\n        return features\n```", "```py\n    def extract_features_and_save(self, work_dir, image_names, file_name):\n        features = self.extract_feature_from_image_paths(work_dir, image_names)\n        with open(file_name, 'wb') as p:\n            pickle.dump(features, p)\n```", "```py\nI = ImageModel()\nI.extract_features_and_save(b'Flicker8k_Dataset',train_image_paths, 'train_image_features.p')\nI.extract_features_and_save(b'Flicker8k_Dataset',test_image_paths, 'test_image_features.p')\n```", "```py\nfrom data_preparation import get_vocab\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Merge, Activation, Flatten\nfrom keras.preprocessing import image, sequence\n```", "```py\nimage_caption_map, max_words, unique_words, \\\nword_to_index_map, index_to_word_map = get_vocab()\nvocabulary_size = len(unique_words)\n```", "```py\nimage_model = Sequential()\nimage_model.add(Dense(128, input_dim=4096, activation='relu'))\nimage_model.add(RepeatVector(max_words))\n```", "```py\nlang_model = Sequential()\nlang_model.add(Embedding(vocabulary_size, 256, input_length=max_words))\nlang_model.add(LSTM(256, return_sequences=True))\nlang_model.add(TimeDistributed(Dense(128)))\n```", "```py\nmodel = Sequential()\nmodel.add(Merge([image_model, lang_model], mode='concat'))\nmodel.add(LSTM(1000, return_sequences=False))\nmodel.add(Dense(vocabulary_size))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nbatch_size = 32\nepochs = 10\ntotal_samples = 9\nmodel.fit_generator(data_generator(batch_size=batch_size), steps_per_epoch=total_samples / batch_size,\n                    epochs=epochs, verbose=2)\n\n```"]