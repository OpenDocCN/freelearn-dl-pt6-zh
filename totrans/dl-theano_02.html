<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;2.&#xA0;Classifying Handwritten Digits with a Feedforward Network" id="OPEK1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02" class="calibre1"/>Chapter 2. Classifying Handwritten Digits with a Feedforward Network</h1></div></div></div><p class="calibre8">The first chapter presented Theano as a compute engine, with its different functions and specificities. With this knowledge, we'll go through an example and introduce some of the main concepts of deep learning, building three neural networks and training them on the problem of handwritten digit classification.</p><p class="calibre8">Deep <a id="id73" class="calibre1"/>learning is a field of machine learning in which layers of modules are stacked on top of each of other: this chapter introduces a simple <a id="id74" class="calibre1"/>single-linear-layer model, then adds a second layer on top of it to create a <span class="strong"><strong class="calibre2">multi-layer perceptron</strong></span> (<span class="strong"><strong class="calibre2">MLP</strong></span>), and last uses multiple convolutional layers to create a <span class="strong"><strong class="calibre2">Convolutional Neural Network</strong></span> (<span class="strong"><strong class="calibre2">CNN</strong></span>).</p><p class="calibre8">In the meantime, this chapter recaps the basic machine learning concepts, such as overfitting, validation, and loss analysis, for those who are not familiar with data science:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Small image classification</li><li class="listitem">Handwritten digit recognition challenge</li><li class="listitem">Layer design to build a neural network</li><li class="listitem">Design of a classical objective/loss function</li><li class="listitem">Back-propagation with stochastic gradient descent</li><li class="listitem">Training on a dataset with validation</li><li class="listitem">Convolutional neural networks</li><li class="listitem">Towards state-of-art results for digit classification</li></ul></div></div>

<div class="book" title="Chapter&#xA0;2.&#xA0;Classifying Handwritten Digits with a Feedforward Network" id="OPEK1-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="The MNIST dataset"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch02lvl1sec18" class="calibre1"/>The MNIST dataset</h1></div></div></div><p class="calibre8">The <span class="strong"><strong class="calibre2">Modified National Institute of Standards and Technology</strong></span> (<span class="strong"><strong class="calibre2">MNIST</strong></span>) <span class="strong"><strong class="calibre2">dataset</strong></span> is a very <a id="id75" class="calibre1"/>well-known <a id="id76" class="calibre1"/>dataset of handwritten digits {0,1,2,3,4,5,6,7,8,9} used to train and test classification models.</p><p class="calibre8">A classification model is a model that predicts the probabilities of observing a class, given an input.</p><p class="calibre8">Training is the task of <span class="strong"><em class="calibre12">learning</em></span> the parameters to fit the model to the data as well as we can so that for any input image, the correct label is predicted. For this training task, the MNIST dataset contains 60,000 images with a target label (a number between 0 and 9) for each example.</p><p class="calibre8">To validate that the training is efficient and to decide when to stop the training, we usually split the training dataset into two datasets: 80% to 90% of the images are used for training, while the remaining 10-20% of images will not be presented to the algorithm for training but to validate that the model generalizes well on unobserved data.</p><p class="calibre8">There is a separate dataset that the algorithm should never see during training, named the test set, which consists of 10,000 images in the MNIST dataset.</p><p class="calibre8">In the MNIST dataset, the input data of each example is a 28x28 normalized monochrome image and a label, represented as a simple integer between 0 and 9 for each example. Let's display some of them:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, download a pre-packaged version of the dataset that makes it easier to load from Python:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">wget</strong></span> http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz -P /sharedfiles</pre></div></li><li class="listitem" value="2">Then load the data into a Python session:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">import</strong></span> pickle, gzip
<span class="strong"><strong class="calibre2">with</strong></span> gzip.open("/sharedfiles/mnist.pkl.gz", 'rb') <span class="strong"><strong class="calibre2">as</strong></span> f:
   train_set, valid_set, test_set = pickle.load(f)</pre></div><p class="calibre24">For <code class="email">Python3</code>, we need <code class="email">pickle.load(f, encoding='latin1')</code> due to the way it was serialized.</p><div class="informalexample"><pre class="programlisting">train_set[0].shape
<span class="strong"><em class="calibre12">(50000, 784)</em></span>

train_set[1].shape
<span class="strong"><em class="calibre12">(50000,)</em></span>

<span class="strong"><strong class="calibre2">import</strong></span> matplotlib

<span class="strong"><strong class="calibre2">import</strong></span> numpy 

<span class="strong"><strong class="calibre2">import</strong></span> matplotlib.pyplot as plt

plt.rcParams['figure.figsize'] = (10, 10)

plt.rcParams['image.cmap'] = 'gray'

<span class="strong"><strong class="calibre2">for</strong></span> i <span class="strong"><strong class="calibre2">in</strong></span> range(9):
    plt.subplot(1,10,i+1)
    plt.imshow(train_set[0][i].reshape(28,28))
    plt.axis('off')
    plt.title(str(train_set[1][i]))

plt.show()</pre></div></li></ol><div class="calibre13"/></div><p class="calibre8">The <a id="id77" class="calibre1"/>first nine <a id="id78" class="calibre1"/>samples from the dataset are displayed with the corresponding label (the <span class="strong"><em class="calibre12">ground truth</em></span>, that is, the correct answer expected by the classification algorithm) on top of them:</p><div class="mediaobject"><img src="../images/00010.jpeg" alt="The MNIST dataset" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">In order to avoid too many transfers to the GPU, and since the complete dataset is small enough to fit in the memory of the GPU, we usually place the full training set in shared variables:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">import</strong></span> theano
train_set_x = theano.shared(numpy.asarray(train_set[0], <span class="strong"><strong class="calibre2">dtype</strong></span>=theano.config.floatX))
train_set_y = theano.shared(numpy.asarray(train_set[1], <span class="strong"><strong class="calibre2">dtype</strong></span>='int32'))</pre></div><p class="calibre8">Avoiding these data transfers allows us to train faster on the GPU, despite recent GPU and fast PCIe connections.</p><p class="calibre8">More information on the dataset is available at <a class="calibre1" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>.</p></div></div>
<div class="book" title="Structure of a training program" id="PNV61-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec19" class="calibre1"/>Structure of a training program</h1></div></div></div><p class="calibre8">The structure <a id="id79" class="calibre1"/>of a training program always consists of the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1"><span class="strong"><strong class="calibre2">Set the script environment</strong></span>: Such <a id="id80" class="calibre1"/>as package imports, the use of the GPU, and so on.</li><li class="listitem" value="2"><span class="strong"><strong class="calibre2">Load data</strong></span>: A data <a id="id81" class="calibre1"/>loader class to access the data during training, usually in a random order to avoid too many similar examples of the same class, but sometimes in a precise order, for example, in the case of curriculum learning with simple examples first and complex ones last.</li><li class="listitem" value="3"><span class="strong"><strong class="calibre2">Preprocess the data</strong></span>: A set <a id="id82" class="calibre1"/>of transformations, such as swapping dimensions on images, adding blur or noise. It is very common to add some data augmentation transformations, such as random crop, scale, brightness, or contrast jittering to get more examples than the original ones, and reduce the risk of overfitting on data. If the number of free parameters in the model is too important with respect to the training dataset size, the model might learn from the available examples. Also, if the dataset is too small and too many iterations have been executed on the same data, the model might become too specific to the training examples and not generalize well on new unseen examples.</li><li class="listitem" value="4"><span class="strong"><strong class="calibre2">Build a model</strong></span>: Defining <a id="id83" class="calibre1"/>the model structure with the parameter in persistent variables (shared variables) to update their values during training in order to fit the training data</li><li class="listitem" value="5"><span class="strong"><strong class="calibre2">Train</strong></span>: There are <a id="id84" class="calibre1"/>different algorithms either training on the full dataset as a whole or training on each example step by step. The best convergence is usually achieved by training on a batch, a small subset of examples grouped together, from a few tens to a few hundreds.<p class="calibre24">Another reason to use a batch is to improve the training speed of the GPU, because individual data transfers are costly and GPU memory is not sufficient to host the full dataset as well. The GPU is a parallel architecture, so processing a batch of examples is usually faster than processing the examples one by one, up to a certain point. Seeing more examples at the same time accelerates the convergence (in wall-time), up to a certain point. This is true even if the GPU memory is large enough to host the whole dataset: the diminishing returns on the batch size make it usually faster to have smaller batches than the whole dataset. Note that this is true for modern CPUs as well, but the optimal batch size is usually smaller.</p><div class="note" title="Note"><h3 class="title2"><a id="note05" class="calibre1"/>Note</h3><p class="calibre8">An iteration defines a training on one batch. An epoch is a number of iterations required for the algorithm to see the full dataset.</p></div></li><li class="listitem" value="6">During training, after a certain number of iterations, there is usually a <span class="strong"><strong class="calibre2">validation</strong></span> using a split of the training data or a validation dataset that has not been used for learning. The loss is computed on this validation set. Though the algorithm has the objective to reduce the loss given the training data, it does not ensure generalization with unseen data. Validation <a id="id85" class="calibre1"/>data is unseen data used to estimate the generalization performance. A lack of generalization might occur when the training data is not representative, or is an exception and has not been sampled correctly, or if the model overfits the training data.<p class="calibre24">Validation data verifies everything is OK, and stops training when validation loss does not decrease any more, even if training loss might continue to decrease: further training is not worth it any more and leads to overfitting.</p></li><li class="listitem" value="7"><span class="strong"><strong class="calibre2">Saving model parameters</strong></span> and displaying results, such as best training/validation loss values, train loss curves for convergence analysis.<p class="calibre24">In the case of classification, we compute the accuracy (the percentage of correct classification) or the error (the percentage of misclassification) during training, as well as the loss. At the end of training, a confusion matrix helps evaluate the quality of the classifier.</p><p class="calibre24">Let's see these steps in practice and start a Theano session in a Python shell session:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> theano <span class="strong"><strong class="calibre2">import</strong></span> theano
<span class="strong"><strong class="calibre2">import</strong></span> theano.tensor <span class="strong"><strong class="calibre2">as</strong></span> T</pre></div></li></ol><div class="calibre13"/></div></div>
<div class="book" title="Classification loss function" id="QMFO1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec20" class="calibre1"/>Classification loss function</h1></div></div></div><p class="calibre8">The loss <a id="id86" class="calibre1"/>function is an objective function to minimize during training to get the best model. Many different loss functions exist.</p><p class="calibre8">In a classification problem, where the target is to predict the correct class among k classes, cross-entropy is commonly used as it measures the difference between the real probability distribution, <span class="strong"><em class="calibre12">q</em></span>, and the predicted one, <span class="strong"><em class="calibre12">p</em></span>, for each class:</p><div class="mediaobject"><img src="../images/00011.jpeg" alt="Classification loss function" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Here, <span class="strong"><em class="calibre12">i</em></span> is the index of the sample in the dataset, <span class="strong"><em class="calibre12">n</em></span> is the number of samples in the dataset, and <span class="strong"><em class="calibre12">k</em></span> is the number of classes.</p><p class="calibre8">While the real probability <span class="strong"><img src="../images/00012.jpeg" alt="Classification loss function" class="calibre23"/></span> of each class is unknown, it can simply be approximated in practice by the empirical distribution, that is, randomly drawing a sample <a id="id87" class="calibre1"/>out of the dataset in the dataset order. The same way, the cross-entropy of any predicted probability, <code class="email">p</code>, can be approximated by the empirical cross-entropy:</p><div class="mediaobject"><img src="../images/00013.jpeg" alt="Classification loss function" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Here, <span class="strong"><img src="../images/00014.jpeg" alt="Classification loss function" class="calibre23"/></span> is the probability estimated by the model for the correct class of example <span class="strong"><img src="../images/00015.jpeg" alt="Classification loss function" class="calibre23"/></span>.</p><p class="calibre8">Accuracy and cross-entropy both evolve in the same direction but measure different things. Accuracy measures how much the predicted class is correct, while cross-entropy measure the distance between the probabilities. A decrease in cross-entropy explains that the probability to predict the correct class gets better, but the accuracy may remain constant or drop.</p><p class="calibre8">While accuracy is discrete and not differentiable, the cross-entropy loss is a differentiable function that can be easily used for training a model.</p></div>
<div class="book" title="Single-layer linear model" id="RL0A1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec21" class="calibre1"/>Single-layer linear model</h1></div></div></div><p class="calibre8">The simplest <a id="id88" class="calibre1"/>model is the linear model, where for each class <code class="email">c</code>, the output is a linear combination of the input values:</p><div class="mediaobject"><img src="../images/00016.jpeg" alt="Single-layer linear model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">This output is unbounded.</p><p class="calibre8">To get a probability distribution, <code class="email">p<sub class="calibre25">i</sub></code>, that sums to 1, the output of the linear model is passed into a softmax function:</p><div class="mediaobject"><img src="../images/00017.jpeg" alt="Single-layer linear model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Hence, the estimated probability of class <code class="email">c</code> for an input <code class="email">x</code> is rewritten with vectors:</p><div class="mediaobject"><img src="../images/00018.jpeg" alt="Single-layer linear model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Translated <a id="id89" class="calibre1"/>in Python with:</p><div class="informalexample"><pre class="programlisting">batch_size <span class="strong"><strong class="calibre2">=</strong></span> 600
n_in <span class="strong"><strong class="calibre2">= </strong></span>28 * 28
n_out <span class="strong"><strong class="calibre2">=</strong></span> 10

x <span class="strong"><strong class="calibre2">=</strong></span> T.matrix(<span class="strong"><strong class="calibre2">'x'</strong></span>)
y <span class="strong"><strong class="calibre2">=</strong></span> T.ivector(<span class="strong"><strong class="calibre2">'y'</strong></span>)
W <span class="strong"><strong class="calibre2">=</strong></span> theano.shared(
            <span class="strong"><strong class="calibre2">value</strong></span>=numpy.zeros(
                (n_in, n_out),
                <span class="strong"><strong class="calibre2">dtype</strong></span>=theano.config.floatX
            ),
            <span class="strong"><strong class="calibre2">name</strong></span>='W',
            <span class="strong"><strong class="calibre2">borrow</strong></span>=True
        )
b <span class="strong"><strong class="calibre2">=</strong></span> theano.shared(
    <span class="strong"><strong class="calibre2">value</strong></span>=numpy.zeros(
        (n_out,),
        <span class="strong"><strong class="calibre2">dtype</strong></span>=theano.config.floatX
    ),
    <span class="strong"><strong class="calibre2">name</strong></span>='b',
    <span class="strong"><strong class="calibre2">borrow</strong></span>=True
)
model <span class="strong"><strong class="calibre2">=</strong></span> T.nnet.softmax(T.dot(x, W) + b)</pre></div><p class="calibre8">The prediction for a given input is given by the most probable class (maximum probability):</p><div class="informalexample"><pre class="programlisting">y_pred <span class="strong"><strong class="calibre2">=</strong></span> T.argmax(model, <span class="strong"><strong class="calibre2">axis</strong></span>=1)</pre></div><p class="calibre8">In this model with a single linear layer, information moves from input to output: it is a <span class="strong"><strong class="calibre2">feedforward network</strong></span>. The process to compute the output given the input is called <span class="strong"><strong class="calibre2">forward propagation</strong></span>.</p><p class="calibre8">This layer is said fully connected because all outputs, <span class="strong"><img src="../images/00019.jpeg" alt="Single-layer linear model" class="calibre23"/></span>, are the sum of (are linked to) all <a id="id90" class="calibre1"/>inputs values through a multiplicative coefficient:</p><div class="mediaobject"><img src="../images/00020.jpeg" alt="Single-layer linear model" class="calibre9"/></div><p class="calibre10"> </p></div>
<div class="book" title="Cost function and errors" id="SJGS1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec22" class="calibre1"/>Cost function and errors</h1></div></div></div><p class="calibre8">The cost <a id="id91" class="calibre1"/>function given the predicted probabilities by the model is as follows:</p><div class="informalexample"><pre class="programlisting">cost <span class="strong"><strong class="calibre2">=</strong></span> -T.mean(T.log(model)[T.arange(y.shape[0]), y])</pre></div><p class="calibre8">The error <a id="id92" class="calibre1"/>is the number of predictions that are different from the true class, averaged by the total number of values, which can be written as a mean:</p><div class="informalexample"><pre class="programlisting">error <span class="strong"><strong class="calibre2">=</strong></span> T.mean(T.neq(y_pred, y))</pre></div><p class="calibre8">On the contrary, accuracy corresponds to the number of correct predictions divided by the total number of predictions. The sum of error and accuracy is one.</p><p class="calibre8">For other types of problems, here are a few other loss functions and implementations:</p><div class="informalexample"><table border="1" class="calibre14"><colgroup class="calibre15"><col class="calibre16"/><col class="calibre16"/></colgroup><tbody class="calibre21"><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<span><strong class="calibre26">Categorical cross entropy</strong></span>
</p>
<p class="calibre20">An equivalent implementation of ours</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">
</p><div class="informalexample1"><pre class="programlisting1">T.nnet.categorical_crossentropy(model, y_true).mean()</pre></div><p class="calibre20">
</p>
 </td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<span><strong class="calibre26">Binary cross entropy</strong></span>
</p>
<p class="calibre20">For the case when output can take only two values {0,1}</p>
<p class="calibre20">Typically used after a sigmoid activation predicting the probability, p</p>
</td><td valign="top" class="calibre22"><div class="informalexample1"><pre class="programlisting1">T.nnet.binary_crossentropy(model, y_true).mean()</pre></div></td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<span><strong class="calibre26">Mean squared error</strong></span>
</p>
<p class="calibre20">L2 norm for regression problems</p>
</td><td valign="top" class="calibre22"><div class="informalexample1"><pre class="programlisting1">T.sqr(model – y_true).mean()</pre></div></td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<span><strong class="calibre26">Mean absolute error</strong></span>
</p>
<p class="calibre20">L1 norm for regression problems</p>
</td><td valign="top" class="calibre22"><div class="informalexample1"><pre class="programlisting1">T.abs_(model - y_true).mean()</pre></div></td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<span><strong class="calibre26">Smooth L1</strong></span>
</p>
<p class="calibre20">A mix between L1 for large values, and L2 for small values</p>
<p class="calibre20">Known as an outlier resistant loss for regressions</p>
</td><td valign="top" class="calibre22"><div class="informalexample1"><pre class="programlisting1">T.switch(
   T.lt(T.abs_(model - y_true) , 1. / sigma), 
   0.5 * sigma * T.sqr(model - y_true),
   T.abs_(model - y_true) – 0.5 / sigma )
.sum(axis=1).mean()</pre></div></td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<span><strong class="calibre26">Squared hinge loss</strong></span>
</p>
<p class="calibre20">Particularly used in unsupervised problems</p>
</td><td valign="top" class="calibre22"><div class="informalexample1"><pre class="programlisting1">T.sqr(T.maximum(1. - y_true * model, 0.)).mean()</pre></div></td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<span><strong class="calibre26">Hinge loss</strong></span>
</p>
</td><td valign="top" class="calibre22"><div class="informalexample1"><pre class="programlisting1">T.maximum(1. - y_true * model, 0.).mean()</pre></div></td></tr></tbody></table></div></div>
<div class="book" title="Backpropagation and stochastic gradient descent" id="TI1E1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec23" class="calibre1"/>Backpropagation and stochastic gradient descent</h1></div></div></div><p class="calibre8">Backpropagation, or the backward propagation of errors, is the most commonly used supervised <a id="id93" class="calibre1"/>learning algorithm for adapting the connection weights.</p><p class="calibre8">Considering the error or the cost as a function of the weights <span class="strong"><em class="calibre12">W</em></span> and <span class="strong"><em class="calibre12">b</em></span>, a local minimum <a id="id94" class="calibre1"/>of the cost function can be approached with a gradient descent, which consists of changing weights along the negative error gradient:</p><div class="mediaobject"><img src="../images/00021.jpeg" alt="Backpropagation and stochastic gradient descent" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Here, <span class="strong"><img src="../images/00022.jpeg" alt="Backpropagation and stochastic gradient descent" class="calibre23"/></span> is the learning rate, a positive constant defining the speed of a descent.</p><p class="calibre8">The following compiled function updates the variables after each feedforward run:</p><div class="informalexample"><pre class="programlisting">g_W = T.grad(<span class="strong"><strong class="calibre2">cost</strong></span>=cost, <span class="strong"><strong class="calibre2">wrt</strong></span>=W)
g_b = T.grad(<span class="strong"><strong class="calibre2">cost</strong></span>=cost, <span class="strong"><strong class="calibre2">wrt</strong></span>=b)

learning_rate=0.13
index = T.lscalar()

train_model = theano.function(
    <span class="strong"><strong class="calibre2">inputs</strong></span>=[index],
    <span class="strong"><strong class="calibre2">outputs</strong></span>=[cost,error],
    <span class="strong"><strong class="calibre2">updates</strong></span>=[(W, W - learning_rate * g_W),(b, b - learning_rate * g_b)],
    <span class="strong"><strong class="calibre2">givens</strong></span>={
        x: train_set_x[index * batch_size: (index + 1) * batch_size],
        y: train_set_y[index * batch_size: (index + 1) * batch_size]
    }
)</pre></div><p class="calibre8">The <a id="id95" class="calibre1"/>input variable is the index of the batch, since all the dataset has been transferred in one pass to the GPU in shared variables.</p><p class="calibre8">Training <a id="id96" class="calibre1"/>consists of presenting each sample to the model iteratively (iterations) and repeating the operation many times (epochs):</p><div class="informalexample"><pre class="programlisting">n_epochs <span class="strong"><strong class="calibre2">=</strong></span> 1000
print_every <span class="strong"><strong class="calibre2">=</strong></span> 1000

n_train_batches <span class="strong"><strong class="calibre2">=</strong></span> train_set[0].shape[0] // batch_size
n_iters <span class="strong"><strong class="calibre2">=</strong></span> n_epochs * n_train_batches
train_loss <span class="strong"><strong class="calibre2">=</strong></span> np.zeros(n_iters)
train_error <span class="strong"><strong class="calibre2">=</strong></span> npzeros(n_iters)

<span class="strong"><strong class="calibre2">for</strong></span> epoch <span class="strong"><strong class="calibre2">in</strong></span> range(n_epochs):
    <span class="strong"><strong class="calibre2">for</strong></span> minibatch_index <span class="strong"><strong class="calibre2">in</strong></span> range(n_train_batches):
        iteration = minibatch_index + n_train_batches * epoch
        train_loss[iteration], train_error[iteration] <span class="strong"><strong class="calibre2">=</strong></span> train_model(minibatch_index)
        <span class="strong"><strong class="calibre2">if</strong></span> (epoch * train_set[0].shape[0] + minibatch_index) <span class="strong"><strong class="calibre2">%</strong></span> print_every == 0 :
            print('epoch {}, minibatch {}/{}, training error {:02.2f} %, training loss {}'.format(
                epoch,
                minibatch_index + 1,
                n_train_batches,
                train_error[iteration] * 100,
                train_loss[iteration]
            ))</pre></div><p class="calibre8">This only reports the loss and error on one mini-batch, though. It would be good to also report the average over the whole dataset.</p><p class="calibre8">The error rate drops very quickly during the first iterations, then slows down.</p><p class="calibre8">Execution time on a GPU GeForce GTX 980M laptop is 67.3 seconds, while on an Intel i7 CPU, it is 3 minutes and 7 seconds.</p><p class="calibre8">After a <a id="id97" class="calibre1"/>long while, the model converges to a 5.3 - 5.5% error rate, and with a few more iterations could go further down, but could also lead to overfitting, Overfitting occurs when the model fits the training data well but does not get the <a id="id98" class="calibre1"/>same error rate on unseen data.</p><p class="calibre8">In this case, the model is too simple to overfit on this data.</p><p class="calibre8">A model that is too simple cannot learn very well. The principle of deep learning is to add more layers, that is, increase the depth and build deeper networks to gain better accuracy.</p><p class="calibre8">We'll see in the following section how to compute a better estimation of the model accuracy and the training stop.</p></div>
<div class="book" title="Multiple layer model"><div class="book" id="UGI02-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec24" class="calibre1"/>Multiple layer model</h1></div></div></div><p class="calibre8">A <span class="strong"><strong class="calibre2">multi-layer perceptron</strong></span> (<span class="strong"><strong class="calibre2">MLP</strong></span>) is <a id="id99" class="calibre1"/>a feedforward <a id="id100" class="calibre1"/>net with multiple layers. A second linear layer, named hidden layer, is added to the previous example:</p><div class="mediaobject"><img src="../images/00023.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Having <a id="id101" class="calibre1"/>two linear layers following each other is equivalent to having a single linear layer.</p><p class="calibre8">With a<span class="strong"><em class="calibre12"> non-linear function or non-linearity or transfer function</em></span> between the linearities, the model does not simplify into a linear one any more, and represents more possible functions in order to capture more complex patterns in the data:</p><div class="mediaobject"><img src="../images/00024.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Activation <a id="id102" class="calibre1"/>functions helps saturating (ON-OFF) and reproduces the biological neuron activations.</p><p class="calibre8">The <span class="strong"><strong class="calibre2">Rectified Linear Unit</strong></span> (<span class="strong"><strong class="calibre2">ReLU</strong></span>) graph is given as follows:</p><p class="calibre8">
<span class="strong"><em class="calibre12">(x + T.abs_(x)) / 2.0</em></span>
</p><div class="mediaobject"><img src="../images/00025.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The <span class="strong"><strong class="calibre2">Leaky Rectifier Linear Unit</strong></span> (<span class="strong"><strong class="calibre2">Leaky ReLU</strong></span>) graph is given as follows:</p><p class="calibre8">
<span class="strong"><em class="calibre12">( (1 + leak) * x + (1 – leak) * T.abs_(x) ) / 2.0</em></span>
</p><div class="mediaobject"><img src="../images/00026.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Here, <code class="email">leak</code> is a parameter that defines the slope in the negative values. In leaky rectifiers, this parameter is fixed.</p><p class="calibre8">The <a id="id103" class="calibre1"/>activation named PReLU considers the <code class="email">leak</code> parameter to be learned.</p><p class="calibre8">More generally speaking, a piecewise linear activation can be learned by adding a linear layer followed by a maxout activation of <code class="email">n_pool</code> units:</p><div class="informalexample"><pre class="programlisting">T.max([x[:, n::n_pool] <span class="strong"><strong class="calibre2">for</strong></span> n <span class="strong"><strong class="calibre2">in</strong></span> <span class="strong"><strong class="calibre2">range</strong></span>(n_pool)], <span class="strong"><strong class="calibre2">axis=0</strong></span>)</pre></div><p class="calibre8">This will output <code class="email">n_pool</code> values or units for the underlying learned linearities:</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Sigmoid</strong></span> (T.nnet.sigmoid)</p><div class="mediaobject"><img src="../images/00027.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">
<span class="strong"><strong class="calibre2">HardSigmoid</strong></span> function is given as:</p><p class="calibre8">
<span class="strong"><em class="calibre12">T.clip(X + 0.5, 0., 1.)</em></span>
</p><div class="mediaobject"><img src="../images/00028.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">
<span class="strong"><strong class="calibre2">HardTanh</strong></span> function is given as:</p><p class="calibre8">
<span class="strong"><em class="calibre12">T.clip(X, -1., 1.)</em></span>
</p><div class="mediaobject"><img src="../images/00029.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">
<span class="strong"><strong class="calibre2">Tanh</strong></span> <a id="id104" class="calibre1"/>function is given as:</p><p class="calibre8">
<span class="strong"><em class="calibre12">T.tanh(x)</em></span>
</p><div class="mediaobject"><img src="../images/00030.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">This two-layer network model written in Python will be as follows:</p><div class="informalexample"><pre class="programlisting">batch_size <span class="strong"><strong class="calibre2">=</strong></span> 600
n_in <span class="strong"><strong class="calibre2">=</strong></span> 28 * 28
n_hidden <span class="strong"><strong class="calibre2">=</strong></span> 500
n_out <span class="strong"><strong class="calibre2">=</strong></span> 10

<span class="strong"><strong class="calibre2">def</strong></span> shared_zeros(shape, <span class="strong"><strong class="calibre2">dtype</strong></span>=theano.config.floatX, <span class="strong"><strong class="calibre2">name</strong></span>='', <span class="strong"><strong class="calibre2">n</strong></span>=None):
    shape <span class="strong"><strong class="calibre2">=</strong></span> shape <span class="strong"><strong class="calibre2">if</strong></span> n <span class="strong"><strong class="calibre2">is</strong></span> None <span class="strong"><strong class="calibre2">else</strong></span> (n,) + shape
    <span class="strong"><strong class="calibre2">return</strong></span> theano.shared(np.zeros(shape, dtype=dtype), <span class="strong"><strong class="calibre2">name</strong></span>=name)

<span class="strong"><strong class="calibre2">def</strong></span> shared_glorot_uniform(shape, <span class="strong"><strong class="calibre2">dtype</strong></span>=theano.config.floatX, <span class="strong"><strong class="calibre2">name</strong></span>='', <span class="strong"><strong class="calibre2">n</strong></span>=None):
    <span class="strong"><strong class="calibre2">if</strong></span> isinstance(shape, int):
        high <span class="strong"><strong class="calibre2">=</strong></span> np.sqrt(6. / shape)
    <span class="strong"><strong class="calibre2">else</strong></span>:
        high <span class="strong"><strong class="calibre2">=</strong></span> np.sqrt(6. / (np.sum(shape[:2]) * np.prod(shape[2:])))
    shape <span class="strong"><strong class="calibre2">=</strong></span> shape <span class="strong"><strong class="calibre2">if</strong></span> n <span class="strong"><strong class="calibre2">is</strong></span> None <span class="strong"><strong class="calibre2">else</strong></span> (n,) + shape
    <span class="strong"><strong class="calibre2">return</strong></span> theano.shared(np.asarray(
        np.random.uniform(
            <span class="strong"><strong class="calibre2">low</strong></span>=-high,
            <span class="strong"><strong class="calibre2">high</strong></span>=high,
            <span class="strong"><strong class="calibre2">size</strong></span>=shape),
        <span class="strong"><strong class="calibre2">dtype</strong></span>=dtype), <span class="strong"><strong class="calibre2">name</strong></span>=name)

W1 <span class="strong"><strong class="calibre2">=</strong></span> shared_glorot_uniform( (n_in, n_hidden), <span class="strong"><strong class="calibre2">name</strong></span>='W1' )
b1 <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros( (n_hidden,), <span class="strong"><strong class="calibre2">name</strong></span>='b1' )

hidden_output <span class="strong"><strong class="calibre2">=</strong></span> T.tanh(T.dot(x, W1) + b1)

W2 <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros( (n_hidden, n_out), <span class="strong"><strong class="calibre2">name</strong></span>='W2' )
b2 <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros( (n_out,), <span class="strong"><strong class="calibre2">name</strong></span>='b2' )

model <span class="strong"><strong class="calibre2">=</strong></span> T.nnet.softmax(T.dot(hidden_output, W2) + b2)
params <span class="strong"><strong class="calibre2">=</strong></span> [W1,b1,W2,b2]</pre></div><p class="calibre8">In deep nets, if weights are initialized to zero with the <code class="email">shared_zeros</code> method, the signal will not <a id="id105" class="calibre1"/>flow through the network correctly from end to end. If weights are initialized with values that are too big, after a few steps, most activation functions saturate. So, we need to ensure that the values can be passed to the next layer during propagation, as well as for the gradients to the previous layer during back-propagation.</p><p class="calibre8">We also need to break the symmetry between neurons. If the weights of all neurons are zero (or if they are all equal), they will all evolve exactly in the same way, and the model will not learn much.</p><p class="calibre8">The researcher Xavier Glorot studied an algorithm to initialize weights in an optimal way. It consists in drawing the weights from a Gaussian or uniform distribution of zero mean and the following variance:</p><div class="mediaobject"><img src="../images/00031.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Here are the variables from the preceding formula:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">n<sub class="calibre25">in</sub></code> is the number of inputs the layer receives during feedforward propagation</li><li class="listitem"><code class="email">n<sub class="calibre25">out</sub></code> is the number of gradients the layer receives during back-propagation</li></ul></div><p class="calibre8">In the <a id="id106" class="calibre1"/>case of a linear model, the shape parameter is a tuple, and <code class="email">v</code> is simply <code class="email">numpy.sum( shape[:2] )</code> (in this case, <code class="email">numpy.prod(shape[2:])</code> is <code class="email">1</code>).</p><p class="calibre8">The variance of a uniform distribution on <span class="strong"><em class="calibre12">[-a, a]</em></span> is given by <span class="strong"><em class="calibre12">a**2 / 3</em></span>, then the bound <code class="email">a</code> can be computed as follows:</p><div class="mediaobject"><img src="../images/00032.jpeg" alt="Multiple layer model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The cost can be defined the same way as before, but the gradient descent needs to be adapted to deal with the list of parameters, <code class="email">[W1,b1,W2,b2]</code>:</p><div class="informalexample"><pre class="programlisting">g_params = T.grad(<span class="strong"><strong class="calibre2">cost</strong></span>=cost, <span class="strong"><strong class="calibre2">wrt</strong></span>=params)</pre></div><p class="calibre8">The training loop requires an updated training function:</p><div class="informalexample"><pre class="programlisting">learning_rate <span class="strong"><strong class="calibre2">=</strong></span> 0.01
updates <span class="strong"><strong class="calibre2">=</strong></span> [
        (param, param - learning_rate * gparam)
        <span class="strong"><strong class="calibre2">for</strong></span> param, gparam <span class="strong"><strong class="calibre2">in</strong></span> zip(params, g_params)
    ]

train_model = theano.function(
    <span class="strong"><strong class="calibre2">inputs</strong></span>=[index],
    <span class="strong"><strong class="calibre2">outputs</strong></span>=cost,
    <span class="strong"><strong class="calibre2">updates</strong></span>=updates,
    <span class="strong"><strong class="calibre2">givens</strong></span>={
        x: train_set_x[index * batch_size: (index + 1) * batch_size],
        y: train_set_y[index * batch_size: (index + 1) * batch_size]
    }
)</pre></div><p class="calibre8">In this case, learning rate is global to the net, with all weights being updated at the same rate. The learning rate is set to 0.01 instead of 0.13. We'll speak about hyperparameter tuning in the training section.</p><p class="calibre8">The <a id="id107" class="calibre1"/>training loop remains unchanged. The full code is given in the <code class="email">2-multi.py</code> file.</p><p class="calibre8">Execution time on the GPU is 5 minutes and 55 seconds, while on the CPU it is 51 minutes and 36 seconds.</p><p class="calibre8">After 1,000 iterations, the error has dropped to 2%, which is a lot better than the previous 5% error rate, but part of it might be due to overfitting. We'll compare the different models later.</p></div>
<div class="book" title="Convolutions and max layers"><div class="book" id="VF2I2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec25" class="calibre1"/>Convolutions and max layers</h1></div></div></div><p class="calibre8">A great <a id="id108" class="calibre1"/>improvement in image classification has been achieved with <a id="id109" class="calibre1"/>the invention of the convolutional layers on the MNIST database:</p><div class="mediaobject"><img src="../images/00033.jpeg" alt="Convolutions and max layers" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">While <a id="id110" class="calibre1"/>previous fully-connected layers perform a computation with all <a id="id111" class="calibre1"/>input values (pixels in the case of an image) of the input, a 2D convolution layer will consider only a small patch or window or receptive field of NxN pixels of the 2D input image for each output unit. The dimensions of the patch are named kernel dimensions, N is the kernel size, and the coefficients/parameters are the kernel.</p><p class="calibre8">At each position of the input image, the kernel produces a scalar, and all position values will lead to a matrix (2D tensor) called a <span class="strong"><em class="calibre12">feature map</em></span>. Convolving the kernel on the input image as a sliding window creates a new output image. The stride of the kernel defines the number of pixels to shift the patch/window over the image: with a stride of 2, the convolution <a id="id112" class="calibre1"/>with the kernel is computed every 2 pixels.</p><p class="calibre8">For example, on a 224 x 224 input image, we get the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">A 2x2 kernel with stride 1 outputs a 223 x 223 feature map</li><li class="listitem">A 3x3 kernel with stride 1 outputs a 222 x 222 feature map</li></ul></div><p class="calibre8">In order <a id="id113" class="calibre1"/>to keep the output feature map the same dimension as the input image, there is a type of zero-padding called <span class="strong"><em class="calibre12">same</em></span> or <span class="strong"><em class="calibre12">half</em></span> that enables the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Add a line and a column of zeros at the end of the input image in the case of a 2x2 kernel with stride 1</li><li class="listitem">Add two lines and two columns of zeros, one in front and one at the end of the input image vertically and horizontally in the case of a 3x3 kernel with stride 1</li></ul></div><p class="calibre8">So, the output dimensions are the same as the original ones, that is, a 224 x 224 feature map.</p><p class="calibre8">With zero padding:</p><div class="book"><ul class="itemizedlist"><li class="listitem">A 2x2 kernel with stride 2 and zero padding will output a 112 x 112 feature map</li><li class="listitem">A 3x3 kernel with stride 2 will output a 112 x 112 feature map</li></ul></div><p class="calibre8">Without zero-padding, it gets more complicated:</p><div class="book"><ul class="itemizedlist"><li class="listitem">A 2x2 kernel with stride 2 will output a 112 x 112 feature map</li><li class="listitem">A 3x3 kernel with stride 2 will output a 111 x 111 feature map</li></ul></div><p class="calibre8">Note that kernel dimensions and strides can be different for each dimension. In this case, we say kernel width, kernel height, stride width, or stride height.</p><p class="calibre8">In one convolutional layer, it is possible to output multiple feature maps, each feature map being computed with a different kernel (and kernel weights) and representing one feature. We say outputs, neurons, kernels, features, feature maps, units, or output channels indifferently to give the number of these different convolutions with different kernels. To be precise, neuron usually refers to a specific position within a feature map. Kernels are the kernels themselves, and the other ones refer to the result of the convolution operation. The number of them is the same, which is why these words are often used to describe the same thing. I'll use the words channels, outputs, and features.</p><p class="calibre8">The usual convolution operators can be applied to multi-channel inputs. This enables to apply them to three-channel images (RGB images, for example) or to the output of another convolution in order to be chained.</p><p class="calibre8">Let's <a id="id114" class="calibre1"/>include two convolutions with a kernel size of 5 in front of the previous MLP mode:</p><div class="mediaobject"><img src="../images/00034.jpeg" alt="Convolutions and max layers" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The 2D <a id="id115" class="calibre1"/>convolution operator requires a 4D tensor input. The first dimension is the batch size, the second the number of inputs or input channels (in the "channel-first format"), and the third and fourth the two dimensions of the feature map (in the "channel-last format", channels are the last dimension). MNIST gray images (one channel) stored in a one-dimensional vector need to be converted into a 28x28 matrix, where 28 is the image height and width:</p><div class="informalexample"><pre class="programlisting">layer0_input = x.reshape((batch_size, 1, 28, 28))</pre></div><p class="calibre8">Then, adding a first convolution layer of 20 channels on top of the transformed input, we get this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> theano.tensor.nnet <span class="strong"><strong class="calibre2">import</strong></span> conv2d

n_conv1 <span class="strong"><strong class="calibre2">=</strong></span> 20

W1 <span class="strong"><strong class="calibre2">=</strong></span> shared_glorot_uniform( (n_conv1, 1, 5, 5) )

conv1_out <span class="strong"><strong class="calibre2">=</strong></span> conv2d(
    <span class="strong"><strong class="calibre2">input</strong></span>=layer0_input,
    <span class="strong"><strong class="calibre2">filters</strong></span>=W1,
    <span class="strong"><strong class="calibre2">filter_shape</strong></span>=(n_conv1, 1, 5, 5),
    <span class="strong"><strong class="calibre2">input_shape</strong></span>=(batch_size, 1, 28, 28)
)</pre></div><p class="calibre8">In this case, the Xavier initialization (from the name of its inventor, Xavier Glorot) multiplies the number of input/output channels by the number of parameters in the kernel, <code class="email">numpy.prod(shape[2:]) = 5 x 5 = 25</code>, to get the total number of incoming input/output gradients in the initialization formula.</p><p class="calibre8">The 20 kernels of size 5x5 and stride 1 on 28x28 inputs will produce 20 feature maps of size 24x24. So the first convolution output is (<code class="email">batch_size,20,24,24</code>).</p><p class="calibre8">Best <a id="id116" class="calibre1"/>performing nets use max pooling layers to encourage translation invariance and stability to noise. A max-pooling layer performs a maximum operation over a sliding window/patch to keep only one value out of the patch. As well as increasing speed performance, it reduces the size of the feature maps, and the total computation complexity and training time decreases:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> theano.tensor.signal <span class="strong"><strong class="calibre2">import</strong></span> pool
pooled_out<span class="strong"><strong class="calibre2"> =</strong></span> pool.pool_2d(<span class="strong"><strong class="calibre2">input</strong></span>=conv1_out, <span class="strong"><strong class="calibre2">ws</strong></span>=(2, 2), <span class="strong"><strong class="calibre2">ignore_border</strong></span>=True)</pre></div><p class="calibre8">The output of the 2x2 max pooling layer will be (<code class="email">batch_size,20,12,12</code>). The batch size and the number of channels stay constant. Only the feature map's size has changed.</p><p class="calibre8">Adding <a id="id117" class="calibre1"/>a second convolutional layer of 50 channels and max pooling layer on top of the previous one leads to an output of size (<code class="email">batch_size,50,4,4</code>):</p><div class="informalexample"><pre class="programlisting">n_conv2 <span class="strong"><strong class="calibre2">=</strong></span> 50

W2 <span class="strong"><strong class="calibre2">=</strong></span> shared_glorot_uniform( (n_conv2, n_conv1, 5, 5) )

conv2_out = conv2d(
    <span class="strong"><strong class="calibre2">input</strong></span>=pooled_out,
    <span class="strong"><strong class="calibre2">filters</strong></span>=W2,
    <span class="strong"><strong class="calibre2">filter_shape</strong></span>=(n_conv2, n_conv1, 5, 5),
    <span class="strong"><strong class="calibre2">input_shape</strong></span>=(batch_size, n_conv1, 12, 12)
)

pooled2_out <span class="strong"><strong class="calibre2">=</strong></span> pool.pool_2d(<span class="strong"><strong class="calibre2">input</strong></span>=conv2_out, <span class="strong"><strong class="calibre2">ds</strong></span>=(2, 2),<span class="strong"><strong class="calibre2">ignore_border</strong></span>=True)</pre></div><p class="calibre8">To create a classifier, we connect on top the MLP with its two fully-connected linear layers and a softmax, as seen before:</p><div class="informalexample"><pre class="programlisting">hidden_input <span class="strong"><strong class="calibre2">=</strong></span> pooled2_out.flatten(2)

n_hidden <span class="strong"><strong class="calibre2">=</strong></span> 500

W3 <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros( (n_conv2 * 4 * 4, n_hidden), <span class="strong"><strong class="calibre2">name</strong></span>='W3' )
b3 <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros( (n_hidden,), <span class="strong"><strong class="calibre2">name</strong></span>='b3' )

hidden_output <span class="strong"><strong class="calibre2">=</strong></span> T.tanh(T.dot(hidden_input, W3) + b3)

n_out <span class="strong"><strong class="calibre2">=</strong></span> 10

W4 <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros( (n_hidden, n_out), <span class="strong"><strong class="calibre2">name</strong></span>='W4' )
b4 <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros( (n_out,), <span class="strong"><strong class="calibre2">name</strong></span>='b4' )

model <span class="strong"><strong class="calibre2">=</strong></span> T.nnet.softmax(T.dot(hidden_output, W4) + b4)
params <span class="strong"><strong class="calibre2">=</strong></span> [W1,W2,W3,b3,W4,b4]</pre></div><p class="calibre8">Such a model is named a <span class="strong"><strong class="calibre2">Convolutional Neural Net</strong></span> (<span class="strong"><strong class="calibre2">CNN</strong></span>).</p><p class="calibre8">The full code is given in the <code class="email">3-cnn.py</code> file.</p><p class="calibre8">Training <a id="id118" class="calibre1"/>is much slower because the number of parameters has been multiplied again, and the use of the GPU makes a lot more sense: total training time on <a id="id119" class="calibre1"/>the GPU has increased to 1 hour, 48 min and 27 seconds. Training on the CPU would take days.</p><p class="calibre8">The training error is zero after a few iterations, part of it due to overfitting. Let's see in the next section how to compute a testing loss and accuracy that better explains the model's efficiency.</p></div>
<div class="book" title="Training"><div class="book" id="10DJ42-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec26" class="calibre1"/>Training</h1></div></div></div><p class="calibre8">In order to <a id="id120" class="calibre1"/>get a good measure of how the model behaves on data that's unseen during training, the validation dataset is used to compute a validation loss and accuracy during training.</p><p class="calibre8">The validation dataset enables us to choose the best model, while the test dataset is only used at the end to get the final test accuracy/error of the model. The training, test, and validation datasets are discrete datasets, with no common examples. The validation dataset is usually 10 times smaller than the test dataset to slow the training process as little as possible. The test dataset is usually around 10-20% of the training dataset. Both the training and validation datasets are part of the training program, since the first one is used to learn, and the second is used to select the best model on unseen data at training time.</p><p class="calibre8">The test dataset is completely outside the training process and is used to get the accuracy of the produced model, resulting from training and model selection.</p><p class="calibre8">If the model overfits the training set because it has been trained too many times on the same images, for example, then the validation and test sets will not suffer from this behavior and will provide a real estimation of the model's accuracy.</p><p class="calibre8">Usually, a validation function is compiled without a gradient update of the model to simply compute only the cost and error on the input batch.</p><p class="calibre8">Batches of data <span class="strong"><em class="calibre12">(x,y)</em></span> are commonly transferred to the GPU at every iteration because the dataset is usually too big to fit in the GPU's memory. In this case, we could still use the trick with the shared variables to place the whole validation dataset in the GPU's memory, but let's see how we would do if we had to transfer the batches to the GPU at each step and not use the previous trick. We would use the more usual form:</p><div class="informalexample"><pre class="programlisting">validate_model <span class="strong"><strong class="calibre2">=</strong></span> theano.function(
    <span class="strong"><strong class="calibre2">inputs</strong></span>=[x,y],
    <span class="strong"><strong class="calibre2">outputs</strong></span>=[cost,error]
)</pre></div><p class="calibre8">It requires the transfer of batch inputs. Validation is computed not at every iteration, but at <code class="email">validation_interval</code> iterations in the training <code class="email">for</code> loop:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">if</strong></span> iteration <span class="strong"><strong class="calibre2">%</strong></span> validation_interval <span class="strong"><strong class="calibre2">==</strong></span> 0 :
    val_index <span class="strong"><strong class="calibre2">=</strong></span> iteration <span class="strong"><strong class="calibre2">//</strong></span> validation_interval
    valid_loss[val_index], valid_error[val_index] <span class="strong"><strong class="calibre2">=</strong></span> np.mean([
            validate_model(
                valid_set[0][i * batch_size: (i + 1) * batch_size],
                numpy.asarray(valid_set[1][i * batch_size: (i + 1) * batch_size], <span class="strong"><strong class="calibre2">dtype</strong></span>="int32")
                )
                <span class="strong"><strong class="calibre2">for</strong></span> i <span class="strong"><strong class="calibre2">in</strong></span> range(n_valid_batches)
             ], <span class="strong"><strong class="calibre2">axis</strong></span>=0)</pre></div><p class="calibre8">Let's see <a id="id121" class="calibre1"/>the simple first model:</p><div class="informalexample"><pre class="programlisting">epoch 0, minibatch 1/83, validation error 40.05 %, validation loss 2.16520105302

epoch 24, minibatch 9/83, validation error 8.16 %, validation loss 0.288349323906
epoch 36, minibatch 13/83, validation error 7.96 %, validation loss 0.278418215923
epoch 48, minibatch 17/83, validation error 7.73 %, validation loss 0.272948684171
epoch 60, minibatch 21/83, validation error 7.65 %, validation loss 0.269203903154
epoch 72, minibatch 25/83, validation error 7.59 %, validation loss 0.26624627877
epoch 84, minibatch 29/83, validation error 7.56 %, validation loss 0.264540277421
...
epoch 975, minibatch 76/83, validation error 7.10 %, validation loss 0.258190142922
epoch 987, minibatch 80/83, validation error 7.09 %, validation loss 0.258411859162</pre></div><p class="calibre8">In a full training program, a validation interval corresponding to the total number of epochs, with an average validation score for the epoch, would make more sense.</p><p class="calibre8">To better estimate how the training performs, let's plot the training and valid loss. In order to display the descent in early iterations, I'll stop the drawing at 100 iterations. If I use 1,000 iterations in the plot, I won't see the early iterations:</p><div class="mediaobject"><img src="../images/00035.jpeg" alt="Training" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The <a id="id122" class="calibre1"/>training loss looks like a wide band because it oscillates between different values. Each of the values corresponds to one batch. The batch might be too small to provide a stable loss value. The mean value of the training loss over the epoch would provide a more stable value to compare with the valid loss and show overfitting.</p><p class="calibre8">Also note that the loss plot provides information on how the network converges, but does not give any valuable information on the error. So, it is also very important to plot the training error and the valid error.</p><p class="calibre8">For the second model:</p><div class="informalexample"><pre class="programlisting">epoch 0, minibatch 1/83, validation error 41.25 %, validation loss 2.35665753484
epoch 24, minibatch 9/83, validation error 10.20 %, validation loss 0.438846310601
epoch 36, minibatch 13/83, validation error 9.40 %, validation loss 0.399769391865
epoch 48, minibatch 17/83, validation error 8.85 %, validation loss 0.379035864025
epoch 60, minibatch 21/83, validation error 8.57 %, validation loss 0.365624915808
epoch 72, minibatch 25/83, validation error 8.31 %, validation loss 0.355733696371
epoch 84, minibatch 29/83, validation error 8.25 %, validation loss 0.348027150147
epoch 96, minibatch 33/83, validation error 8.01 %, validation loss 0.34150374867
epoch 108, minibatch 37/83, validation error 7.91 %, validation loss 0.335878048092
...
epoch 975, minibatch 76/83, validation error 2.97 %, validation loss 0.167824191041
epoch 987, minibatch 80/83, validation error 2.96 %, validation loss 0.167092795949</pre></div><p class="calibre8">Again, the <a id="id123" class="calibre1"/>training curves give better insights:</p><div class="mediaobject"><img src="../images/00036.jpeg" alt="Training" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">For the <a id="id124" class="calibre1"/>third model:</p><div class="informalexample"><pre class="programlisting">epoch 0, minibatch 1/83, validation error 53.81 %, validation loss 2.29528842866
epoch 24, minibatch 9/83, validation error 1.55 %, validation loss 0.048202780541
epoch 36, minibatch 13/83, validation error 1.31 %, validation loss 0.0445762014715
epoch 48, minibatch 17/83, validation error 1.29 %, validation loss 0.0432346871821
epoch 60, minibatch 21/83, validation error 1.25 %, validation loss 0.0425786205451
epoch 72, minibatch 25/83, validation error 1.20 %, validation loss 0.0413943211024
epoch 84, minibatch 29/83, validation error 1.20 %, validation loss 0.0416557886347
epoch 96, minibatch 33/83, validation error 1.19 %, validation loss 0.0414686980075
...
epoch 975, minibatch 76/83, validation error 1.08 %, validation loss 0.0477593478863
epoch 987, minibatch 80/83, validation error 1.08 %, validation loss 0.0478142946085</pre></div><p class="calibre8">Refer to <a id="id125" class="calibre1"/>the following graph:</p><div class="mediaobject"><img src="../images/00037.jpeg" alt="Training" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Here we see the difference between train and valid, losses either due to a slight overfitting to the training data, or a difference between the training and test datasets.</p><p class="calibre8">The main causes of overfitting are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Too small a dataset</strong></span>: Collect more data</li><li class="listitem"><span class="strong"><strong class="calibre2">Too high a learning rate</strong></span>: The network is learning too quickly on earlier examples</li><li class="listitem"><span class="strong"><strong class="calibre2">A lack of regularization</strong></span>: Add more dropout (see next section), or a penalty on the norm of the weights in the loss function</li><li class="listitem"><span class="strong"><strong class="calibre2">Too small model</strong></span>: Increase the number of filters/units in different layers</li></ul></div><p class="calibre8">Validation <a id="id126" class="calibre1"/>loss and error gives a better estimate than training loss and error, which are more noisy, and during training, they are also used to decide which model parameters are the best:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Simple model</strong></span>: 6.96 % at epoch 518</li><li class="listitem"><span class="strong"><strong class="calibre2">MLP model</strong></span>: 2.96 % at epoch 987</li><li class="listitem"><span class="strong"><strong class="calibre2">CNN model</strong></span>: 1.06 % at epoch 722</li></ul></div><p class="calibre8">These results also indicate that the models might not improve much with further training.</p><p class="calibre8">Here's a comparison of the three models' validation losses:</p><div class="mediaobject"><img src="../images/00038.jpeg" alt="Training" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Note that the MLP is still improving and the training has not finished, while the CNN and simple <a id="id127" class="calibre1"/>networks have converged.</p><p class="calibre8">With the selected model, you can easily compute the test loss and error on the test dataset to finalize it.</p><p class="calibre8">The last important concept of machine learning is hyperparameter tuning. An hyperparameter defines a parameter of the model that is not learned during training. Here are examples:</p><div class="informalexample"><pre class="programlisting">learning rate
number of hidden neurons
batch size</pre></div><p class="calibre8">For the learning rate, too slow a descent might prevent finding a more global minimum, while too fast a descent damages the final convergence. Finding the best initial learning rate is crucial. Then, it is common to decrease the learning rate after many iterations in order to have more precise fine-tuning of the model.</p><p class="calibre8">Hyperparameter selection requires us to run the previous runs many times for different values of the hyperparameters; testing all combinations of hyperparameters can be done in a simple grid search, for example.</p><p class="calibre8">Here is <a id="id128" class="calibre1"/>an exercise for the reader:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Train the models with different hyperparameters and draw the training loss curves to see how hyperparameters influence the final loss.</li><li class="listitem">Visualize the content of the neurons of the first layer, once the model has been trained, to see what the features capture from the input image. For this task, compile a specific visualization function:<div class="informalexample"><pre class="programlisting">visualize_layer1 <span class="strong"><strong class="calibre2">=</strong></span> theano.function(
    <span class="strong"><strong class="calibre2">inputs</strong></span>=[x,y],
    <span class="strong"><strong class="calibre2">outputs</strong></span>=conv1_out
)</pre></div></li></ul></div></div>
<div class="book" title="Dropout" id="11C3M1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec27" class="calibre1"/>Dropout</h1></div></div></div><p class="calibre8">Dropout <a id="id129" class="calibre1"/>is a widely used technique to improve convergence and robustness of a neural net and prevent neural nets from overfitting. It consists of setting some random values to zero for the layers on which we'd like it to apply. It introduces some randomness in the data at every epoch.</p><p class="calibre8">Usually, dropout is used before the fully connected layers and not used very often in convolutional layers. Let's add the following lines before each of our two fully connected layers:</p><div class="informalexample"><pre class="programlisting">dropout <span class="strong"><strong class="calibre2">=</strong></span> 0.5

<span class="strong"><strong class="calibre2">if</strong></span> dropout <span class="strong"><strong class="calibre2">&gt;</strong></span> 0 :
    mask <span class="strong"><strong class="calibre2">=</strong></span> srng.binomial(<span class="strong"><strong class="calibre2">n</strong></span>=1, <span class="strong"><strong class="calibre2">p</strong></span>=1-dropout, <span class="strong"><strong class="calibre2">size</strong></span>=hidden_input.shape)
    # The cast is important because
    # int * float32 = float64 which make execution slower
    hidden_input <span class="strong"><strong class="calibre2">=</strong></span> hidden_input * T.cast(mask, theano.config.floatX)</pre></div><p class="calibre8">The full script is in <code class="email">5-cnn-with-dropout.py</code>. After 1,000 iterations, the validation error of the CNN with dropout continues to drops down to 1.08%, while the validation error of the CNN without dropout will not go down by 1.22%.</p><p class="calibre8">Readers who would like to go further with dropout should have a look at maxout units. They work well with dropout and replace the tanh non-linearities to get even better results. As <a id="id130" class="calibre1"/>dropout does a kind of model averaging, maxout units try to find the optimal non-linearity to the problem.</p></div>
<div class="book" title="Inference" id="12AK81-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec28" class="calibre1"/>Inference</h1></div></div></div><p class="calibre8">Inference is <a id="id131" class="calibre1"/>the process of using the model to produce predictions.</p><p class="calibre8">For inference, the weight parameters do not need to be updated, so the inference function is simpler than the training function:</p><div class="informalexample"><pre class="programlisting">infer_model = theano.function(
    <span class="strong"><strong class="calibre2">inputs</strong></span>=[x],
    <span class="strong"><strong class="calibre2">outputs</strong></span>=[y_pred]
)</pre></div></div>
<div class="book" title="Optimization and other update rules"><div class="book" id="1394Q2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec29" class="calibre1"/>Optimization and other update rules</h1></div></div></div><p class="calibre8">Learning <a id="id132" class="calibre1"/>rate is a very important parameter to set correctly. Too low a <a id="id133" class="calibre1"/>learning rate will make it difficult to learn and will train slower, while too high a learning rate will increase sensitivity to outlier values, increase the amount of noise in the data, train too fast to learn generalization, and get stuck in local minima:</p><div class="mediaobject"><img src="../images/00039.jpeg" alt="Optimization and other update rules" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">When <a id="id134" class="calibre1"/>training loss does not improve anymore for one or a few more <a id="id135" class="calibre1"/>iterations, the learning rate can be reduced by a factor:</p><div class="mediaobject"><img src="../images/00040.jpeg" alt="Optimization and other update rules" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">It helps the network learn fine-grained differences in the data, as shown when training residual networks (<a class="calibre1" title="Chapter 7. Classifying Images with Residual Networks" href="part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 7</a>, <span class="strong"><em class="calibre12">Classifying Images with Residual Networks</em></span>):</p><div class="mediaobject"><img src="../images/00041.jpeg" alt="Optimization and other update rules" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">To check <a id="id136" class="calibre1"/>the training process, it is usual to print the norm of the parameters, the gradients, and the updates, as well as NaN values.</p><p class="calibre8">The update <a id="id137" class="calibre1"/>rule seen in this chapter is the simplest form of update, known <a id="id138" class="calibre1"/>as <span class="strong"><strong class="calibre2">Stochastic Gradient Descent</strong></span> (<span class="strong"><strong class="calibre2">SGD</strong></span>). It is a good practice to clip the norm to avoid saturation and NaN values. The updates list given to the <code class="email">theano</code> function becomes this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> clip_norms(gs, c):
    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))
    return [ T.switch(T.ge(norm, c), g*c/norm, g) <span class="strong"><strong class="calibre2">for</strong></span> g <span class="strong"><strong class="calibre2">in</strong></span> gs]

updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)
<span class="strong"><strong class="calibre2">for</strong></span> p,g <span class="strong"><strong class="calibre2">in</strong></span> zip(params,grads):
    updated_p = p - learning_rate * g
    updates.append((p, updated_p))</pre></div><p class="calibre8">Some very simple variants have been experimented with in order to improve the descent, and are proposed in many deep learning libraries. Let's see them in Theano.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Momentum</strong></span>
</p><p class="calibre8">For each <a id="id139" class="calibre1"/>parameter, a momentum (<span class="strong"><em class="calibre12">v</em></span>, as velocity) is computed from the gradients accumulated over the iterations with a time decay. The previous momentum value is multiplied by a decay parameter between 0.5 and 0.9 (to be cross-validated) and added to the current gradient to provide the new momentum value.</p><p class="calibre8">The momentum of the gradients plays the role of a moment of inertia in the updates, in order to learn faster. The idea is also that oscillations in successive gradients will be canceled in the momentum, to move the parameter in a more direct path towards the solution:</p><div class="mediaobject"><img src="../images/00042.jpeg" alt="Optimization and other update rules" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The decay <a id="id140" class="calibre1"/>parameter between 0.5 and 0.9 is a hyperparameter usually referred to as the momentum, in an abuse of language:</p><div class="informalexample"><pre class="programlisting">updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)
<span class="strong"><strong class="calibre2">for</strong></span> p,g <span class="strong"><strong class="calibre2">in</strong></span> zip(params,grads):
    m = theano.shared(p.get_value() * 0.)
    v = (momentum * m) - (learning_rate * g)
    updates.append((m, v))
    updates.append((p, p + v))</pre></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Nesterov Accelerated Gradient</strong></span>
</p><p class="calibre8">Instead <a id="id141" class="calibre1"/>of adding <span class="strong"><em class="calibre12">v</em></span> to the parameter, the idea is to add directory the future value of the momentum momentum <code class="email">v - learning_rate g</code>, in order to have it compute the gradients in the next iteration directly at the next position:</p><div class="informalexample"><pre class="programlisting">updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)
<span class="strong"><strong class="calibre2">for</strong></span> p, g <span class="strong"><strong class="calibre2">in</strong></span> zip(params, grads):
    m = theano.shared(p.get_value() * 0.)
    v = (momentum * m) - (learning_rate * g)
    updates.append((m,v))
    updates.append((p, p + momentum * v - learning_rate * g))</pre></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Adagrad</strong></span>
</p><p class="calibre8">This update <a id="id142" class="calibre1"/>rule, as well as the following rules consists of adapting the learning rate <span class="strong"><strong class="calibre2">parameter-wise</strong></span> (differently for each parameter). The element-wise sum of squares of the gradients is accumulated into a shared variable for each parameter in order to decay the learning rate in an element-wise fashion:</p><div class="informalexample"><pre class="programlisting">updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)
<span class="strong"><strong class="calibre2">for</strong></span> p,g <span class="strong"><strong class="calibre2">in</strong></span> zip(params,grads):
    acc = theano.shared(p.get_value() * 0.)
    acc_t = acc + g ** 2
    updates.append((acc, acc_t))
    p_t = p - (learning_rate / T.sqrt(acc_t + 1e-6)) * g
    updates.append((p, p_t))</pre></div><p class="calibre8">
<code class="email">Adagrad</code> is an <a id="id143" class="calibre1"/>aggressive method, and the next two rules, <code class="email">AdaDelta</code> and <code class="email">RMSProp</code>, try to reduce its aggression.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">AdaDelta</strong></span>
</p><p class="calibre8">Two <a id="id144" class="calibre1"/>accumulators are created per parameter to accumulate the squared gradients and the updates in moving averages, parameterized by the decay <code class="email">rho</code>:</p><div class="informalexample"><pre class="programlisting">updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)
<span class="strong"><strong class="calibre2">for</strong></span> p,g <span class="strong"><strong class="calibre2">in</strong></span> zip(params,grads):
    acc = theano.shared(p.get_value() * 0.)
    acc_delta = theano.shared(p.get_value() * 0.)
    acc_new = rho * acc + (1 - rho) * g ** 2
    updates.append((acc,acc_new))
    update = g * T.sqrt(acc_delta + 1e-6) / T.sqrt(acc_new + 1e-6)
    updates.append((p, p - learning_rate * update))
    updates.append((acc_delta, rho * acc_delta + (1 - rho) * update ** 2))</pre></div><p class="calibre8">
<span class="strong"><strong class="calibre2">RMSProp</strong></span>
</p><p class="calibre8">This updates <a id="id145" class="calibre1"/>rule is very effective in many cases. It is an improvement of the <code class="email">Adagrad</code> update rule, using a moving average (parameterized by <code class="email">rho</code>) to get a less aggressive decay:</p><div class="informalexample"><pre class="programlisting">updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)
<span class="strong"><strong class="calibre2">for</strong></span> p,g <span class="strong"><strong class="calibre2">in</strong></span> zip(params,grads):
    acc = theano.shared(p.get_value() * 0.)
    acc_new = rho * acc + (1 - rho) * g ** 2
    updates.append((acc, acc_new))
    updated_p = p - learning_rate * (g / T.sqrt(acc_new + 1e-6))
    updates.append((p, updated_p))</pre></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Adam</strong></span>
</p><p class="calibre8">This is <code class="email">RMSProp</code> with momemtum, one of the best choices for the learning rule. The time step is <a id="id146" class="calibre1"/>kept track of in a shared variable, <code class="email">t</code>. Two moving averages are computed, one for the past squared gradients, and the other for past gradient:</p><div class="informalexample"><pre class="programlisting">b1=0.9, b2=0.999, l=1-1e-8
updates = []
grads = T.grad(cost, params)
grads = clip_norms(grads, 50)  
t = theano.shared(floatX(1.))
b1_t = b1 * l **(t-1)

<span class="strong"><strong class="calibre2">for</strong></span> p, g <span class="strong"><strong class="calibre2">in</strong></span> zip(params, grads):
    m = theano.shared(p.get_value() * 0.)
    v = theano.shared(p.get_value() * 0.)
    m_t = b1_t * m + (1 - b1_t) * g
    v_t = b2 * v + (1 - b2) * g**2 
    updates.append((m, m_t))
    updates.append((v, v_t))
    updates.append((p, p - (learning_rate * m_t / (1 - b1**t)) / (T.sqrt(v_t / (1 - b2**t)) + 1e-6)) )
updates.append((t, t + 1.))</pre></div><p class="calibre8">To conclude on update rules, many recent research papers still prefer the simple SGD rule, and work the architecture and the initialization of the layers with the correct learning rate. For more complex networks, or if the data is sparse, the adaptive learning rate methods are better, sparing you the pain of finding the right learning rate.</p></div>
<div class="book" title="Related articles" id="147LC1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec30" class="calibre1"/>Related articles</h1></div></div></div><p class="calibre8">You can <a id="id0" class="calibre1"/>refer to the following documents for more insights into the topics covered in this chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre12">Deeplearning.net Theano tutorials: Single layer</em></span> (<a class="calibre1" href="http://deeplearning.net/tutorial/logreg.html">http://deeplearning.net/tutorial/logreg.html</a>), MLP (<a class="calibre1" href="http://deeplearning.net/tutorial/mlp.html">http://deeplearning.net/tutorial/mlp.html</a>), Convolutions (<a class="calibre1" href="http://deeplearning.net/tutorial/lenet.html">http://deeplearning.net/tutorial/lenet.html</a>)</li><li class="listitem">All loss functions: for classification, regression, and joint embedding (<a class="calibre1" href="http://christopher5106.github.io/deep/learning/2016/09/16/about-loss-functions-multinomial-logistic-logarithm-cross-entropy-square-errors-euclidian-absolute-frobenius-hinge.html">http://christopher5106.github.io/deep/learning/2016/09/16/about-loss-functions-multinomial-logistic-logarithm-cross-entropy-square-errors-euclidian-absolute-frobenius-hinge.html</a>)</li><li class="listitem">The last example corresponds to Yann Lecun's five-5 layer network as in Gradient based learning applied to document recognition (<a class="calibre1" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</a>)</li><li class="listitem">Understanding the difficulty of training deep feedforward neural networks, Xavier Glorot, Yoshua Bengio, 2010</li><li class="listitem">Maxout Networks: Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio 2013</li><li class="listitem">An overview of gradient descent algorithms: <a class="calibre1" href="http://sebastianruder.com/optimizing-gradient-descent/">http://sebastianruder.com/optimizing-gradient-descent/</a></li><li class="listitem">CS231n Convolutional Neural Networks for Visual Recognition, <a class="calibre1" href="http://cs231n.github.io/neural-networks-3/">http://cs231n.github.io/neural-networks-3/</a></li><li class="listitem">Yes you should understand backprop, Andrej Karpathy, 2016, <a class="calibre1" href="https://medium.com/@karpathy/">https://medium.com/@karpathy/</a></li><li class="listitem">Striving for Simplicity: The All Convolutional Net, Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller, 2014</li><li class="listitem">Fractional Max-Pooling, Benjamin Graham, 2014</li><li class="listitem">Batch <a id="id1" class="calibre1"/>Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, Sergey Ioffe, Christian Szegedy, 2015</li><li class="listitem">Visualizing and Understanding Convolutional Networks, Matthew D Zeiler, Rob Fergus, 2013</li><li class="listitem">Going Deeper with Convolutions, Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, 2014</li></ul></div></div>
<div class="book" title="Summary" id="1565U1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec31" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">Classification is a very wide topic in machine learning. It consists of predicting a class or a category, as we have shown with our handwritten digits example. In <a class="calibre1" title="Chapter 7. Classifying Images with Residual Networks" href="part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 7</a>, <span class="strong"><em class="calibre12">Classifying Images with Residual Networks</em></span>, we'll see how to classify a wider set of natural images and objects.</p><p class="calibre8">Classification can be applied to different problems and the cross-entropy/negative log likelihood is the common loss function to solve them through gradient descent. There are many other loss functions for problems such as regression (mean square error loss) or unsupervised joint learning (hinge loss).</p><p class="calibre8">In this chapter, we have been using a very simple update rule as gradient descent named stochastic gradient descent, and presented some other gradient descent variants (<code class="email">Momentum</code>, <code class="email">Nesterov</code>, <code class="email">RMSprop</code>, <code class="email">ADAM</code>, <code class="email">ADAGRAD</code>, <code class="email">ADADELTA</code>). There has been some research into second order optimizations, such as Hessian Free, or K-FAC, which provided better results in deep or recurrent networks but remain complex and costly, and have not be widely adopted until now. Researchers have been looking for new architectures that perform better without the need for such optimization techniques.</p><p class="calibre8">When training networks, I would strongly encourage you to use the following two Linux commands:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Screen</strong></span>: To detach your shell, run scripts on the server and reconnect later, since training usually takes a few days.</li><li class="listitem"><span class="strong"><strong class="calibre2">Tee</strong></span>: To which you pipe the output of your running program, in order to save the displayed results to a file, while continuing to visualize the output in your shell. This will spare your code the burden of log functions and frameworks.</li></ul></div></div></body></html>