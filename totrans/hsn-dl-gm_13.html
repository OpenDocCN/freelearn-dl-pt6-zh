<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Multi-Agent Environments</h1>
                </header>
            
            <article>
                
<p class="mce-root">With our single-agent experiences under our belt, we can move on to the more complex but equally entertaining world of working in multi-agent environments, training multiple agents to work in the same environment in a co-operative or competitive fashion. This also opens up several new opportunities for training agents with adversarial self-play, cooperative self-play, competitive self-play, and more. The possibilities become endless here, and this may be the true holy grail of AI.</p>
<p>In this chapter, we are going to cover several aspects of multi-agent training environments and the main section topics are highlighted here:</p>
<ul>
<li>Adversarial and cooperative self-play</li>
<li>Competitive self-play</li>
<li>Multi-brain play</li>
<li>Adding individuality with intrinsic rewards</li>
<li>Extrinsic rewards for individuality</li>
</ul>
<p>This chapter assumes you have covered the three previous chapters and completed some exercises in each. In the next section, we begin to cover the various self-play scenarios.</p>
<div class="packt_tip">It is best to start this chapter with a new clone of the ML-Agents repository. We do this as a way of cleaning up our environment and making sure no errant configuration was unintentionally saved. If you need help with this, then consult one of the earlier chapters.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adversarial and cooperative self-play</h1>
                </header>
            
            <article>
                
<p>The term <em>self-play</em> can, of course, mean many things to many people, but in this case, we mean the brain is competing (adversarial) or cooperating with itself by manipulating multiple agents. In the case of ML-Agents, this may mean having a single brain manipulating multiple agents in the same environment. There is an excellent example of this in ML-Agents, so open up Unity and follow the next exercise to get this scene ready for multi-agent training:</p>
<ol>
<li>Open the <span class="packt_screen">SoccerTwos</span> scene from the <span class="packt_screen">Assets</span> | <span class="packt_screen">ML-Agents</span> | <span class="packt_screen">Examples</span> | <span class="packt_screen">Soccer</span> | <span class="packt_screen">Scenes</span> folder. The scene is set to run, by default, in player mode, but we need to convert it back to learning mode.</li>
<li>Select and disable all the <span class="packt_screen">SoccerFieldTwos(1)</span> to <span class="packt_screen">SoccerFieldTwos(7)</span> areas. We won't use those yet.</li>
<li>Select and expand the remaining active <span class="packt_screen">SoccerFieldTwos</span> object. This will reveal the play area with four agents, two marked <span class="packt_screen">RedStriker</span> and <span class="packt_screen">BlueStriker</span> and two marked <span class="packt_screen">RedGoalie</span> and <span class="packt_screen">BlueGoalie</span>.</li>
<li>Inspect the agents and set each one's brain to <span class="packt_screen">StrikerLearning</span> or <span class="packt_screen">GoalieLearning</span> as appropriate, as shown here:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bad9504b-6031-4dc7-89a1-6a0fdeccacb9.png" style="width:44.25em;height:55.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Setting the learning brains on the agents</span></span></div>
<ol start="5">
<li>We have four agents in this environment being controlled by brains that are both cooperating with and competing against each other. To be honest, this example is brilliant and demonstrates incredibly well the whole concept of cooperative and competitive self-play. If you are still struggling with some concepts, consider this diagram, which shows how this is put together:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7d798d2c-a983-47a0-9e7f-123947814e6d.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The SoccerTwos brain architecture</span></div>
<ol start="6">
<li>As we can see, we have two brains controlling four agents<span>:</span> two strikers and two goalies. The striker's job is to score against the goalie, and, of course, the goalie's job is to block goals.</li>
<li>Select the <span class="packt_screen">Academy</span> and set the <span class="packt_screen">Soccer Academy</span> | <span class="packt_screen">Brains</span> | <span class="packt_screen">Control</span> enabled for both brains, as shown:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bd97dcc4-28e4-4857-9f62-ae7795aeb202.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Setting the Brains to control in the Academy</span></span></div>
<ol start="8">
<li>Also, note the <span class="packt_screen">Striker</span>, <span class="packt_screen">Goalie Reward</span>, and <span class="packt_screen">Punish</span> settings at the bottom of the <span class="packt_screen">Soccer Academy</span> component. It is important to also note the way the <kbd>reward</kbd> functions for each brain. The following are the <kbd>reward</kbd> functions described mathematically for this sample:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/94d69f7e-74b3-4fd0-a481-a2a0c8a5cd95.png" style="font-size: 1em;width:9.42em;height:1.50em;"/><br/>
<img class="fm-editor-equation" src="assets/e5e35e1a-b4c1-4fed-9ffd-7a88b020fdeb.png" style="font-size: 1em;width:11.75em;height:1.42em;"/><br/>
<img class="fm-editor-equation" src="assets/d2aec05a-effa-4726-9bd9-3543994fc124.png" style="font-size: 1em;width:10.58em;height:1.58em;"/><br/>
<img class="fm-editor-equation" src="assets/bb21059d-f367-4c9f-9c51-386c18dbd594.png" style="font-size: 1em;width:11.58em;height:1.58em;"/></p>
<ol start="9">
<li>That means, when a goal is scored, each of the four agents gets a reward based on its position and team. Thus, if red scored, the <span class="packt_screen">Red Striker</span> would get a <kbd>+1</kbd> reward, the <span class="packt_screen">Blue Striker</span> a <kbd>-0.1</kbd> reward, the <span class="packt_screen">Red Goalie</span> a <kbd>+0.1</kbd> reward, and the poor <span class="packt_screen">Blue Goalie</span> a <kbd>-1</kbd> reward. Now, you may think this could cause overlap, but remember that each agent's view of a state or an observation will be different. Thus, the reward will be applied to the policy for that state or observation. In essence, the agent is learning based on its current view of the environment, which will change based on which agent is sending that observation.</li>
<li>Save the scene and project when you are done editing.</li>
</ol>
<p>That sets up our scene for multi-agent training using two brains and four agents, using both competitive and cooperative self-play. In the next section, we complete the external configuration and start training the scene.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training self-play environments</h1>
                </header>
            
            <article>
                
<p>Training these types of self-play environments opens up further possibilities for not only enhanced training possibilities but also for fun gaming environments. In some ways, these types of training environments can be just as much fun to watch, as we will see at the end of this chapter.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For now, though, we are going to jump back and continue setting up the configuration we need to train our <span class="packt_screen">SoccerTwos</span> multi-agent environment in the next exercise:</p>
<ol>
<li>Open the <kbd>ML-Agents/ml-agents/config/trainer_config.yaml</kbd> file and inspect the <kbd>StrikerLearning</kbd> and <kbd>GoalieLearning</kbd> config sections, as shown:</li>
</ol>
<pre style="padding-left: 60px">StrikerLearning:<br/>    max_steps: 5.0e5<br/>    learning_rate: 1e-3<br/>    <strong>batch_size: 128</strong><br/>    num_epoch: 3<br/>    buffer_size: 2000<br/>    beta: 1.0e-2<br/>    hidden_units: 256<br/>    summary_freq: 2000<br/>    time_horizon: 128<br/>    num_layers: 2<br/>    normalize: false<br/><br/>GoalieLearning:<br/>    max_steps: 5.0e5<br/>    learning_rate: 1e-3<br/>    <strong>batch_size: 320</strong><br/>    num_epoch: 3<br/>    buffer_size: 2000<br/>    beta: 1.0e-2<br/>    hidden_units: 256<br/>    summary_freq: 2000<br/>    time_horizon: 128<br/>    num_layers: 2<br/>    normalize: false</pre>
<ol start="2">
<li>The obvious thought is that the brains should have a similar configuration, and you may start that way, yes. However, note that even in this example the <kbd>batch_size</kbd> parameter is set differently for each brain.</li>
<li>Open a Python/Anaconda window and switch to your ML-Agents virtual environment and then launch the following command from the <kbd>ML-Agents/ml-agents</kbd> folder:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=soccer --train</strong></pre>
<ol start="4">
<li>Press <span class="packt_screen">Play</span> when prompted, and you should see the following training session running:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d39dccad-9782-46dd-b092-b65d37365fab.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The SoccerTwos scene running in training mode</span></div>
<ol start="5">
<li>As has been said, this can be a very entertaining sample to watch, and it trains surprisingly quickly.</li>
</ol>
<ol start="6">
<li>Open up the Python/Anaconda console after some amount of training, and note how you are getting stats on two brains now, <span class="packt_screen">StrikerLearning</span> and <span class="packt_screen">GoalieLearning</span>, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/38567196-c47a-438e-b8de-42f02e49ade0.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Console output showing stats from two brains</span></div>
<ol start="7">
<li>Note how <span class="packt_screen">StrikerLearning</span> and <span class="packt_screen">GoalieLearning</span> are returning opposite rewards to each other. This means, in order for these agents to be trained, they must balance their mean reward to 0 for both agents. As the agents train, you will notice their rewards start to converge to 0, the optimum reward for this example.  </li>
<li>Let the sample run to completion. You can easily get lost watching these environments, so you may not even notice the time go by.</li>
</ol>
<p>This example showed how we can harness the power of multi-agent training through self-play to teach two brains how to both compete and cooperate at the same time. In the next section, we look at multiple agents competing against one another in self-play.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adversarial self-play</h1>
                </header>
            
            <article>
                
<p>In the previous example, we saw an example of both cooperative and competitive self-play where multiple agents functioned almost symbiotically. While this was a great example, it still tied the functionality of one brain to another through their reward functions, hence our observation of the agents being in an almost rewards-opposite scenario. Instead, we now want to look at an environment that can train a brain with multiple agents using just adversarial self-play. Of course, ML-Agents has such an environment, called <span class="packt_screen">Banana</span>, which comprises several agents that randomly wander the scene and collect bananas. The agents also have a laser pointer, which allows them to disable an opposing agent for several seconds if they are hit. This is the scene we will look at in the next exercise:</p>
<ol>
<li>Open the <span class="packt_screen">Banana</span> scene from the <span class="packt_screen">Assets</span> | <span class="packt_screen">ML-Agents</span> | <span class="packt_screen">Examples</span> | <span class="packt_screen">BananaCollectors</span> | <span class="packt_screen">Scenes</span> folder.</li>
<li>Select and disable the additional training areas <span class="packt_screen">RLArea(1)</span> to <span class="packt_screen">RLArea(3)</span>.</li>
<li>Select the five agents (<span class="packt_screen">Agent</span>, <span class="packt_screen">Agent(1)</span>, <span class="packt_screen">Agent(2)</span>, <span class="packt_screen">Agent(3)</span>, <span class="packt_screen">Agent(4)</span>) in the <span class="packt_screen">RLArea</span>.</li>
<li>Swap the <span class="packt_screen">Banana Agent</span> | <span class="packt_screen">Brain </span>from <span class="packt_screen">BananaPlayer</span> to <span class="packt_screen">BananaLearning</span>.</li>
<li>Select the <span class="packt_screen">Academy</span> and set the <span class="packt_screen">Banana</span> <span class="packt_screen">Academy</span> | <span class="packt_screen">Brains</span> | <span class="packt_screen">Control</span> property to <span class="packt_screen">Enabled</span>.</li>
<li>Select the <span class="packt_screen">Banana Agent</span> component (<span class="packt_screen">Script</span>) in the editor, and open it in your code editor of choice. If you scroll down to the bottom, you can see the <kbd>OnCollisionEnter</kbd> method as shown:</li>
</ol>
<pre style="padding-left: 60px">void OnCollisionEnter(Collision collision)<br/>{<br/>  if (collision.gameObject.CompareTag("banana"))<br/>  {<br/>    Satiate();<br/>    collision.gameObject.GetComponent&lt;BananaLogic&gt;().OnEaten();<br/>    <strong>AddReward(1f);</strong><br/>    bananas += 1;<br/>    if (contribute)<br/>    {<br/>      myAcademy.totalScore += 1;<br/>    }<br/>  }<br/> if (collision.gameObject.CompareTag("badBanana"))<br/> {<br/>   Poison();<br/>   collision.gameObject.GetComponent&lt;BananaLogic&gt;().OnEaten();<br/><br/>   <strong>AddReward(-1f);</strong><br/>   if (contribute)<br/>   {<br/>     myAcademy.totalScore -= 1;<br/>   }<br/>  }<br/>}</pre>
<ol start="7">
<li>Reading the preceding code, we can summarize our <kbd>reward</kbd> functions to the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/069ca155-ee9c-4ff0-a01a-2cff13ed6d22.png" style="font-size: 1em;width:12.17em;height:1.33em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/27024412-a985-427d-a522-a5586088562c.png" style="width:11.92em;height:1.25em;"/></p>
<p style="padding-left: 60px">This simply means the agents only receive a reward for eating bananas. Interestingly, there is no reward for disabling an opponent with a laser or by being disabled.</p>
<ol start="8">
<li>Save the scene and the project.</li>
<li>Open a prepared Python/Anaconda console and start training with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=banana --train</strong></pre>
<ol start="10">
<li>Press <span class="packt_screen">Play</span> in the editor when prompted, and watch the action unfold as shown in the next screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e5e1b177-d51f-40c2-abed-6be4a9549669.png" style="width:42.50em;height:30.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The Banana Collector agents doing their work</span></div>
<ol start="11">
<li>Let the scene run for as long as you like.  </li>
</ol>
<p>This scene is an excellent example of how agents learn to use a secondary game mechanic that returns no rewards, but, like the laser, is still used to immobilize adversarial collectors and obtain more bananas, all while only receiving rewards for eating only bananas. This example shows some of the true power of RL and how it can be used to find secondary strategies in order to solve problems. While this is a very entertaining aspect and fun to watch in a game, consider the grander implications of this. RL has been shown to optimize everything from networking to recommender systems using <strong>adversarial self-play</strong>, and it will be interesting to see what this method of learning is capable of accomplishing in the near future.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-brain play</h1>
                </header>
            
            <article>
                
<p>One of the truly great things about the ML-Agents kit is the ability to add multiple agents powered by multiple brains quickly. This in turns gives us the ability to build more complex game environments or scenarios with fun agents/AI to play both with and against. Let's see how easy it is to convert our soccer example to let the agents all use individual brains:</p>
<ol>
<li>Open up the editor to the <span class="packt_screen">SoccerTwos</span> scene we looked at earlier.</li>
<li>Locate the <kbd>Brains</kbd> folder for the example at <span class="packt_screen">Assets</span> | <span class="packt_screen">ML-Agents</span> | <span class="packt_screen">Examples</span> | <span class="packt_screen">Soccer</span> | <span class="packt_screen">Brains</span>.</li>
<li>Click the <span class="packt_screen">Create</span> menu in the upper right corner of the window and from the <span class="packt_screen">Context</span> menu, and select <span class="packt_screen">ML-Agents</span> | <span class="packt_screen">Learning Brain</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5529ce20-f7be-4dcf-910c-89567b1b8c23.png" style="width:31.00em;height:24.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Creating a new learning brain</span></div>
<ol start="4">
<li>Name the new brain <kbd>RedStrikerLearning</kbd>.  Create three more new brains named <kbd>RedGoalieLearning</kbd>, <kbd>BlueGoalieLearning</kbd>, and <kbd>BlueStrikerLearning</kbd> in the same folder.</li>
</ol>
<ol start="5">
<li>Select <span class="packt_screen">RedStrikerLearning</span>. Then select and drag the <span class="packt_screen">StrikerLearning</span> brain and drop it into the <span class="packt_screen">Copy Brain Parameters from</span> slot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/895d4ef3-1d95-41ae-90d8-6b2bd63ae8c5.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Copying brain parameters from another brain</span></div>
<ol start="6">
<li>Do this for <span class="packt_screen">BlueStrikerLearning</span>, copying parameters from <span class="packt_screen">StrikerLearning</span>. Then do the same for the <span class="packt_screen">RedGoalieLearning</span> and <span class="packt_screen">BlueGoalieLearning</span>, copying parameters from <span class="packt_screen">GoalieLearning</span>.</li>
<li>Select the <span class="packt_screen">RedAgent</span> in the <span class="packt_screen">Hierarchy</span> window and set the <span class="packt_screen">Agent</span> <span class="packt_screen">Soccer</span> | <span class="packt_screen">Brain</span> to <span class="packt_screen">RedStrikerLearning</span>. Do this for each of the other agents, matching the color with a position.  <span class="packt_screen">BlueGoalie</span> <strong>-&gt;</strong> <span class="packt_screen">BlueGoalieLearning</span>.</li>
<li>Select <span class="packt_screen">Academy</span> and remove all the current <span class="packt_screen">Brains</span> from the <span class="packt_screen">Soccer</span> <span class="packt_screen">Academy</span> | <span class="packt_screen">Brains</span> list. Then add all the new brains we just created back into the list using the <span class="packt_screen">Add New</span> button and set them to <span class="packt_screen">Control</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/becf9e28-691c-454c-b52d-cbac98e3881e.png" style="width:32.50em;height:45.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Adding the new brains to Academy</span></span></div>
<ol start="9">
<li>Save the scene and the project. Now, we just swapped the example from using two concurrent brains in self-play mode to be individual agents on teams.  </li>
<li>Open a Python/Anaconda window set up for training and launch with it the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=soccer_mb --train</strong></pre>
<ol start="11">
<li>Let the training run and note how the agents start off playing just as well as they did previously. Take a look at the console output as well. You will see it now reports for four agents, but the agents are still somewhat symbiotic, as the red striker is opposite the blue goalie. However, they now train much more slowly, due in part to each brain seeing only half the observations now. Remember that we had both striker agents feeding to a single brain previously, and, as we learned, this additional input of state can expedite training substantially.</li>
</ol>
<p>At this point, we have four agents with four individual brains playing a game of soccer. Of course, since the agents are still training symbiotically by sharing a reward function, we can't really describe them as individuals. Except, as we know, individuals who play on teams are often influenced by their own internal or intrinsic reward system. We will look at how the application of intrinsic rewards can make this last exercise more interesting in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding individuality with intrinsic rewards</h1>
                </header>
            
            <article>
                
<p>As we learned in <a href="ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml">Chapter 9</a>, <em>Rewards and Reinforcement Learning</em>, intrinsic reward systems and the concept of agent motivation is currently implemented as just <strong>curiosity learning</strong> in ML-Agents. This whole area of applying intrinsic rewards or motivation combined with RL has wide applications to gaming and interpersonal applications such as <strong>servant agents</strong>.   </p>
<p>In the next exercise, we are going to add intrinsic rewards to a couple of our agents and see what effect this has on the game. Open up the scene from the previous exercise and follow these steps:</p>
<ol>
<li>Open up the <kbd>ML-Agents/ml-agents/config/trainer_config.yaml</kbd> file in a text editor. We never did add any specialized configuration to our agents, but we are going to rectify that now and add some extra configurations.</li>
</ol>
<ol start="2">
<li>Add the following four new brain configurations to the file:</li>
</ol>
<pre style="padding-left: 60px">BlueStrikerLearning:<br/>    max_steps: 5.0e5<br/>    learning_rate: 1e-3<br/>    batch_size: 128<br/>    num_epoch: 3<br/>    buffer_size: 2000<br/>    beta: 1.0e-2<br/>    hidden_units: 256<br/>    summary_freq: 2000<br/>    time_horizon: 128<br/>    num_layers: 2<br/>    normalize: false<br/><strong><br/>BlueGoalieLearning:</strong><br/><strong>    use_curiosity: true</strong><br/><strong>    summary_freq: 1000</strong><br/><strong>    curiosity_strength: 0.01</strong><br/><strong>    curiosity_enc_size: 256</strong><br/>    max_steps: 5.0e5<br/>    learning_rate: 1e-3<br/>    batch_size: 320<br/>    num_epoch: 3<br/>    buffer_size: 2000<br/>    beta: 1.0e-2<br/>    hidden_units: 256 <br/>    time_horizon: 128<br/>    num_layers: 2<br/>    normalize: false<br/><br/><strong>RedStrikerLearning:</strong><br/>    <strong>use_curiosity: true</strong><br/><strong>    summary_freq: 1000</strong><br/><strong>    curiosity_strength: 0.01</strong><br/><strong>    curiosity_enc_size: 256</strong><br/>    max_steps: 5.0e5<br/>    learning_rate: 1e-3<br/>    batch_size: 128<br/>    num_epoch: 3<br/>    buffer_size: 2000<br/>    beta: 1.0e-2<br/>    hidden_units: 256 <br/>    time_horizon: 128<br/>    num_layers: 2<br/>    normalize: false<br/><br/>RedGoalieLearning:<br/>    max_steps: 5.0e5<br/>    learning_rate: 1e-3<br/>    batch_size: 320<br/>    num_epoch: 3<br/>    buffer_size: 2000<br/>    beta: 1.0e-2<br/>    hidden_units: 256<br/>    summary_freq: 2000<br/>    time_horizon: 128<br/>    num_layers: 2<br/>    normalize: false</pre>
<ol start="3">
<li>Note how we have also enabled <kbd>use_curiosity: true</kbd> on the <kbd>BlueGoalieLearning</kbd> and <kbd>RedStrikerLearning</kbd> brains. You can copy and paste most of this from the original <kbd>GoalieLearning</kbd> and <kbd>StrikerLearning</kbd> brain configurations already in the file; just pay attention to the details.</li>
<li>Save the file when you are done editing.</li>
<li>Open your Python/Anaconda console and start training with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=soccer_icl --train</strong></pre>
<ol start="6">
<li>Let the agents train for a while, and you will notice that, while they do appear to work more like individuals, their training ability is still subpar, while any improvement we do see in training is likely the cause of giving a couple of agents curiosity.</li>
</ol>
<p>This ability to add individuality to an agent with intrinsic rewards or motivation will certainly mature as DRL does for games and other potential applications and will hopefully provide other intrinsic reward modules that may not be entirely focused on learning. However, intrinsic rewards can really do much to encourage individuality, so in the next section, we introduce extrinsic rewards to our modified example.</p>
<div class="packt_infobox">Another excellent application of transfer learning would be the ability to add intrinsic reward modules after agents have been trained on general tasks.  </div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extrinsic rewards for individuality</h1>
                </header>
            
            <article>
                
<p>We have looked extensively at external or extrinsic rewards for several chapters now and how techniques can be used to optimize and encourage them for agents. Now, it may seem like the easy way to go in order to modify an agent's behavior is by altering its extrinsic rewards or in essence its reward functions. However, this can be prone to difficulties, and this can often alter training performance for the worse, which is what we witnessed when we added <strong>Curriculum Learning</strong> (<strong>CL</strong>) to a couple of agents in the previous section. Of course, even if we make the training worse, we now have a number of techniques up our sleeves such as <strong>Transfer Learning</strong> (<strong>TL</strong>), also known as<strong> Imitation Learning</strong> (<strong>IL</strong>); <strong>Curiosity</strong>; and CL, to help us correct things.</p>
<p>In the next exercise, we are going to look to add further individuality to our agents by adding additional extrinsic rewards. Open up the previous exercise example we were just working on and follow along:</p>
<ol>
<li>From the menu, select <span class="packt_screen">Window</span> | <span class="packt_screen">Asset Store</span>.  This will take you to the Unity Asset Store, which is an excellent resource for helper assets. While most of these assets are paid, honestly, the price compared to comparable developer tools is minimal, and there are several free and very excellent assets that you can start using to enhance your training environments. The Asset Store is one of the best and worst things about Unity, so if you do purchase assets, be sure to read the reviews and forum posts. Any good asset will typically have its own forum if it is developer-focused, artistic assets much less so.</li>
<li>In the search bar, enter <kbd>toony tiny people</kbd> and press the <em>Enter</em> key or click the <span class="packt_screen">Search</span> button. <span>This will display the search results.</span></li>
</ol>
<div class="packt_infobox"><span>We would like to thank </span><strong>Polygon Blacksmith</strong><span> for their support in allowing us to distribute their Toony Tiny People Demo asset with the book's source. Also, their collection of character assets is very well done and simple to use. The price is also at an excellent starting point for some of the larger asset packages if you decide you want to build a full game or enhanced demo.</span></div>
<ol start="3">
<li>Select the result called <span class="packt_screen">Toony Tiny People Demo</span> by <span class="packt_screen">Polygon Blacksmith</span> and select it. It will appear as shown in this screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b1f3ba55-d605-4c21-828c-7cdf29d5f0a4.png" style="width:43.58em;height:33.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The Toony Tiny People Demo asset from Polygon Blacksmith</span></div>
<ol start="4">
<li>Click the red <span class="packt_screen">Download </span>button and, after the asset has downloaded, the button will change to <span class="packt_screen">Import</span>, as shown in the preceding screenshot. Click the <span class="packt_screen">Import</span> button to import the assets.  When you are prompted by the <span class="packt_screen">Import</span> dialog, make sure everything is selected and click <span class="packt_screen">Import</span>.</li>
</ol>
<div class="packt_tip"><span>These types of low polygon or toon assets are perfect for making a simple game or simulation more entertaining and fun to watch. It may not seem like much, but you can spend a lot of time watching these training sims run, and it helps if they look appealing.</span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>Select and expand all the agent objects in <span class="packt_screen">Hierarchy</span>. This includes <span class="packt_screen">RedStriker</span>, <span class="packt_screen">BlueStriker</span>, <span class="packt_screen">RedGoalie, </span>and <span class="packt_screen">BlueGoalie</span>.</li>
<li>Open the <span class="packt_screen">Assets</span> | <span class="packt_screen">TooyTinyPeople</span> | <span class="packt_screen">TT_demo</span> | <span class="packt_screen">prefabs</span> folder in the <span class="packt_screen">Project</span> window.</li>
<li>Select and drag the <span class="packt_screen">TT_demo_Female</span> prefab from the preceding folder and drop it into the <span class="packt_screen">RedStriker</span> agent object in the <span class="packt_screen">Hierarchy</span> window. Select the cube object just beneath the agent and disable it in the inspector. Continue to do this for the other agents according to the following list:
<ul>
<li><span class="packt_screen">TT_demo_female</span> -&gt; <span class="packt_screen">RedStriker</span></li>
<li><span class="packt_screen">TT_demo_male_A</span> -&gt; <span class="packt_screen">BlueStriker</span></li>
<li><span class="packt_screen">TT_demo_police</span> -&gt; <span class="packt_screen">BlueGoalie</span></li>
<li><span class="packt_screen">TT_demo_zombie</span> -&gt; <span class="packt_screen">RedGoalie</span></li>
</ul>
</li>
</ol>
<p style="padding-left: 60px">This is further demonstrated in this screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/13987114-bc54-4b7a-9fc1-4b3716e8b4ec.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Setting the new agent bodies</div>
<ol start="8">
<li>Make sure to also reset the new agent model's <span class="packt_screen">Transform Position</span> and <span class="packt_screen">Orientation</span> to <kbd>[0,0,0]</kbd>, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/362c8dbb-4f85-4fc3-bbde-1b2f2d0de4f9.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Resetting the orientation and position of dragged prefabs</span></div>
<ol start="9">
<li>Save the scene and project. </li>
</ol>
<p>At this point, you can run the scene in training and watch the new agent models move around, but there isn't much point. The agents will still act the same, so what we need to do next is set additional extrinsic rewards based on some arbitrary personality, which we will define in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating uniqueness with customized reward functions </h1>
                </header>
            
            <article>
                
<p>We managed to have some success in making our agents unique by adding intrinsic rewards, although the results may have been not as unique as we would have liked. This means we now want to look at modifying the agents' extrinsic rewards in the hopes of making their behavior more unique and ultimately more entertaining for the game. </p>
<p>The best way for us to start doing that is to look at the <kbd>SoccerTwos</kbd> reward functions we described earlier; these are listed here, for reference:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8360374e-f459-4fef-98dd-3b11264f0636.png" style="width:8.92em;height:1.42em;"/><br/>
<img class="fm-editor-equation" src="assets/e943d6d7-592f-48e8-8bf4-1ec5798f5f5b.png" style="width:11.00em;height:1.33em;"/><br/>
<img class="fm-editor-equation" src="assets/594438d5-58dc-40fe-b993-cdf625b439ae.png" style="width:9.42em;height:1.42em;"/><br/>
<img class="fm-editor-equation" src="assets/97c31523-231f-4211-aece-29c95edcb586.png" style="width:9.75em;height:1.33em;"/></p>
<p>What we want to do now is apply some individualistic modification to the rewards function based on the current character. We will do this by simply chaining the functions with a modification based on the character type, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/03fb39c9-5222-466c-a90a-b30d4e4c340c.png" style="width:15.00em;height:1.25em;"/>  or  <img class="fm-editor-equation" src="assets/7be32220-52f5-4067-a9a1-d9731f2d59dc.png" style="width:16.67em;height:1.25em;"/><br/>
<img class="fm-editor-equation" src="assets/a2547f6b-3e83-4ce3-abb7-a9405f5ef26b.png" style="width:16.33em;height:1.42em;"/>  or  <img class="fm-editor-equation" src="assets/33de502b-245d-4659-9715-41af693970b9.png" style="width:17.08em;height:1.33em;"/><br/>
<span><img class="fm-editor-equation" src="assets/fd164fb9-7793-492c-a371-3bfac8a77c08.png" style="width:12.50em;height:1.25em;"/> or </span> <span><img class="fm-editor-equation" src="assets/b540589a-8861-490a-8476-253b634e98b7.png" style="width:14.17em;height:1.25em;"/></span><br/>
<span><img class="fm-editor-equation" src="assets/dd76d52b-c033-4888-856b-f5e6384f4440.png" style="width:15.67em;height:1.33em;"/> or </span> <span><img class="fm-editor-equation" src="assets/ef696a36-6623-4c61-bb89-57805927aac0.png" style="width:16.33em;height:1.25em;"/></span></p>
<p>All we are doing here with these reward functions is simply modifying the reward value by some personality modification. For the girl, we give her a bonus of 1.25 x the rewards, reflecting that she may be excited. The boy is less excited, so we modify his rewards by .95 times, which reduces them slightly. The policeman, who is always calm and in control, remains constant with no rewards modifications.  Finally, we introduce a bit of a wildcard, the half-dead zombie. In order to characterize it as half-dead, we also decrease all of its rewards by half as well.  </p>
<p>You could, of course, modify these functions in any way you please, according to your game mechanics, but it is important to note that the effect of the personality modification you are applying could hinder training. Be sure to take a mental note of that as we get into training this example as well.</p>
<p class="mce-root">A girl, a boy, a zombie, and a policeman enter the soccer field.</p>
<p>Now that we understand the new reward functions, we want to add to our example that it is time to open Unity and code them. This example will require some slight modifications to the C# files, but the code is quite simple and should be readily understood by any programmer with experience of a C-based language.</p>
<p>Open up Unity to the scene we were modifying in the previous example, and follow the next exercise:</p>
<ol>
<li>Locate the <span class="packt_screen">RedStriker</span>  agent in the <span class="packt_screen">Hierarchy</span> window and select it.</li>
</ol>
<ol start="2">
<li>From <span class="packt_screen">Inspector</span>, click the gear icon beside the <span class="packt_screen">Agent Soccer</span> component and, from the <span class="packt_screen">Context</span> menu, select <span class="packt_screen">Edit Script</span>.  This will open the script and solution in your editor.</li>
<li>Add a new <kbd>enum</kbd> called <kbd>PersonRole</kbd> at the top of the file right after the current <kbd><span>enum</span> AgentRole</kbd> and as shown in the code:</li>
</ol>
<pre style="padding-left: 60px">public enum AgentRole<br/>{<br/>  striker,goalie<br/>} <em>//after this line</em><br/><strong>public enum PersonRole</strong><br/><strong>{</strong><br/><strong>  girl, boy, police, zombie</strong><br/><strong>}</strong></pre>
<ol start="4">
<li>This creates a new role, for, in essence, the personality we want to apply to each brain.</li>
<li>Add another new variable to the class, as shown:</li>
</ol>
<pre style="padding-left: 60px">public AgentRole agentRole; <em>//after this line</em><br/><strong>public PersonRole playerRole;</strong></pre>
<ol start="6">
<li>That adds the new <kbd>PersonRole</kbd> to the agent. Now we want to also add the new type to the setup by adding a single line to the <kbd>InitializeAgent</kbd>  method, shown here:</li>
</ol>
<pre style="padding-left: 60px">public override void InitializeAgent()<br/>{<br/>  base.InitializeAgent();<br/>  agentRenderer = GetComponent&lt;Renderer&gt;();<br/>  rayPer = GetComponent&lt;RayPerception&gt;();<br/>  academy = FindObjectOfType&lt;SoccerAcademy&gt;();<br/>  PlayerState playerState = new PlayerState();<br/>  playerState.agentRB = GetComponent&lt;Rigidbody&gt;();<br/>  agentRB = GetComponent&lt;Rigidbody&gt;();<br/>  agentRB.maxAngularVelocity = 500;<br/>  playerState.startingPos = transform.position;<br/>  playerState.agentScript = this;<br/>  area.playerStates.Add(playerState);<br/>  playerIndex = area.playerStates.IndexOf(playerState);<br/>  playerState.playerIndex = playerIndex;<br/>  <strong>playerState.personRole = personRole;  <em>//add this line</em></strong><br/>}</pre>
<ol start="7">
<li>You should likely see an error now in the line. That is because we also need to add the new <kbd>personRole</kbd> property to <kbd>PlayerState</kbd>. Open the <kbd>PlayerState</kbd> class and add the property as shown:</li>
</ol>
<pre style="padding-left: 60px">[System.Serializable]<br/>public class PlayerState<br/>{<br/>  public int playerIndex; <br/>  public Rigidbody agentRB; <br/>  public Vector3 startingPos; <br/>  public AgentSoccer agentScript; <br/>  public float ballPosReward;<br/>  public string position;<br/>  <strong>public AgentSoccer.PersonRole personRole { get; set; }  <em>//add me</em></strong><br/>}</pre>
<ol start="8">
<li>You should now be in the <kbd>SoccerFieldArea.cs</kbd> file.  Scroll to the <kbd>RewardOrPunishPlayer</kbd> method and modify it as shown:</li>
</ol>
<pre style="padding-left: 60px">public void RewardOrPunishPlayer(PlayerState ps, float striker, float goalie)<br/>{<br/>  if (ps.agentScript.agentRole == AgentSoccer.AgentRole.striker)<br/>  { <br/>    <strong>RewardOrPunishPerson(ps, striker);  <em>//new line</em></strong><br/>  }<br/>  if (ps.agentScript.agentRole == AgentSoccer.AgentRole.goalie)<br/>  { <br/>    <strong>RewardOrPunishPerson(ps, striker); <em>//new line</em></strong><br/>  }<br/>  ps.agentScript.Done(); //all agents need to be reset<br/>}</pre>
<ol start="9">
<li>What we are doing here is injecting another reward function, <kbd>RewardOrPunishPerson</kbd>, in order to add our extrinsic personality rewards. Next, add a new <kbd>RewardOrPunishPerson</kbd> method, as shown:</li>
</ol>
<pre style="padding-left: 60px">private void RewardOrPunishPerson(PlayerState ps, float reward)<br/>{<br/>  switch (ps.personRole)<br/>  {<br/>    case AgentSoccer.PersonRole.boy:<br/>      ps.agentScript.AddReward(reward * .95f);<br/>      break;<br/><br/>    case AgentSoccer.PersonRole.girl:<br/>      ps.agentScript.AddReward(reward*1.25f);<br/>      break;<br/><br/>    case AgentSoccer.PersonRole.police:<br/>      ps.agentScript.AddReward(reward);<br/>      break;<br/><br/>    case AgentSoccer.PersonRole.zombie:<br/>      ps.agentScript.AddReward(reward * .5f);<br/>      break;<br/>  }<br/>}</pre>
<ol start="10">
<li>That code does exactly what our earlier customized reward functions do. When you are done editing, save all your files and return to the Unity editor. If there are any errors or compiler warnings, they will be shown in the console. If you need to go back and fix any (red) error issues, do so.</li>
</ol>
<p>As you can see, with very little code, we are able to add our extrinsic personality rewards. You could, of course, enhance this system in any number of ways and even make it more generic and parameter-driven. In the next section, we look to put all this together and get our agents training individually.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the agents' personalities</h1>
                </header>
            
            <article>
                
<p>With all the code set up, we can now continue back in the editor and set up the agents to match the personality we want to apply to them. Open up the editor again, and follow the next exercise to apply the personalities to the agents and start training:</p>
<ol>
<li>Select <span class="packt_screen">RedStriker</span> in <span class="packt_screen">Hierarchy</span> and set the <span class="packt_screen">Agent</span> <span class="packt_screen">Soccer</span> | <span class="packt_screen">Person</span> <span class="packt_screen">Role</span> parameter we just created to <span class="packt_screen">Girl</span>, as shown:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/54e3b787-d936-453c-8103-5aa2c5fd599d.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Setting the personalities on each of the agents</span></span></div>
<ol start="2">
<li>Update all the agents with the relevant personality that matches the model we assigned earlier: <span class="packt_screen">BlueStriker</span>-&gt; <span class="packt_screen">Boy</span>, <span class="packt_screen">BlueGoalie</span> -&gt; <span class="packt_screen">Police</span>, and <span class="packt_screen">RedGoalie</span> -&gt; <span class="packt_screen">Zombie</span>, as shown in the preceding screenshot.</li>
<li>Save the scene and project.</li>
<li>Now, at this point, if you wanted it to be more detailed, you may want to go back and update each of the agent brain names to reflect their personalities, such as <span class="packt_screen">GirlStrikerLearning</span> or <span class="packt_screen">PoliceGoalieLearning</span>, and you can omit the team colors. Be sure to also add the new brain configuration settings to your <kbd>trainer_config.yaml</kbd> file.</li>
<li>Open your Python/Anaconda training console and start training with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=soccer_peeps --train</strong></pre>
<ol start="6">
<li>Now, this can be very entertaining to watch, as you can see in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c9e87060-970f-4b74-91d6-0add4c847e12.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Watching individual personalities play soccer</span></div>
<ol start="7">
<li>Note how we kept the team color cubes active in order to show which team each individual agent is on.</li>
</ol>
<ol start="8">
<li>Let the agents train for several thousand iterations and then open the console; note how the agents now look less symbiotic. In our example, they are still paired with each other, since we only applied a simple linear transformation to the rewards. You could, of course, apply more complex functions that are non-linear and not inversely related that describe some other motivation or personality for your agents.</li>
<li>Finally, let's open up TensorBoard and look at a better comparison of our multi-agent training. Open another Python/Anaconda console to the <kbd>ML-Agents/ml-agents</kbd> folder you are currently working in and run the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>tensorboard --logdir=summaries</strong></pre>
<ol start="10">
<li>Use your browser to open the TensorBoard interface and examine the results. Be sure to disable any extra results and just focus on the four brains in our current training run. The three main plots we want to focus on are shown merged together in this diagram:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/702bce1d-2467-4a1a-b68e-dbc70a022b67.png" style="width:36.83em;height:28.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>TensorBoard Plots showing results of training four brains</span></span></div>
<p>As you can see from the TensorBoard results, the agents are not training very well. We could enhance that, of course, by adding additional training areas and feeding more observations in order to train the policy. However, if you look at the <strong>Policy Loss</strong> plot, the results show the agents' competition is causing minimal policy change, which is a bad thing this early in training. If anything, the zombie agent appears to be the agent learning the best from these results.</p>
<p>There are plenty of other ways you can, of course, modify your extrinsic reward function in order to encourage some behavioral aspect in multi-agent training scenarios. Some of these techniques work well and some not so well. We are still in the early days of developing this tech and best practices still need to emerge.</p>
<p>In the next section, we look to further exercises you can work on in order to reinforce your knowledge of all the material we covered in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>As always, try at least one or two of the following exercises on your own for your own enjoyment and learning:</p>
<ol>
<li>Open the <span class="packt_screen">BananaCollectors</span> example <span class="packt_screen">Banana</span> scene and run it in training mode.</li>
<li>Modify the <span class="packt_screen">BananaCollectors</span> | <span class="packt_screen">Banana</span> scene so that it uses five separate learning brains and then run it in training mode.</li>
<li>Modify the reward functions in the last <span class="packt_screen">SoccerTwos</span> exercise to use exponential or logarithmic functions.</li>
<li>Modify the reward function in the last <span class="packt_screen">SoccerTwos</span> exercise to use non-inverse related and non-linear functions. This way, the mean modifying the positive and negative rewards is different for each personality.</li>
<li>Modify the <span class="packt_screen">SoccerTwos</span> scene with different characters and personalities. Model new rewards functions as well, and then train the agents.</li>
<li>Modify the <span class="packt_screen">BananaCollectors</span> example <span class="packt_screen">Banana</span> scene to use the same personalities and custom reward functions as we did with the <span class="packt_screen">SoccerTwos</span> example.</li>
<li>Do exercise 3 with the <span class="packt_screen">BananaCollectors</span> example.</li>
<li>Do exercise 4 with the <span class="packt_screen">BananaCollectors</span> example.</li>
<li>Do exercise 5 with the <span class="packt_screen">BananaCollectors</span> example.</li>
<li>Build a new multi-agent environment using one of the current samples as a template or create your own.  This last exercise could very likely turn into your very own game.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p>You may have noticed by now that as we progress through the book, the exercises become more time-consuming and difficult. Please try for your own personal benefit to complete at least a couple of the exercises.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we explored a world of possibilities with multi-agent training environments. We first looked at how we could set up environments using self-play, where a single brain may control multiple brains that both compete and cooperate with one another. Then we looked at how we could add personality with intrinsic rewards in the form of curiosity using the ML-Agents curiosity learning system. Next, we looked at how extrinsic rewards could be used to model an agent's personality and influence training. We did this by adding a free asset for style and then applied custom extrinsic rewards through reward function chaining. Finally, we trained the environment and were entertained by the results of the boy agent solidly thrashing the zombie; you will see this if you watch the training to completion.</p>
<p>In the next chapter, we will look at another novel application of DRL for debugging and testing already constructed games.</p>


            </article>

            
        </section>
    </body></html>