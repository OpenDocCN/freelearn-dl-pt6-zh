<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Image Classification for Small Data Using Transfer Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapters, we developed deep learning networks and explored various application examples related to image data. One major difference compared to what we will be discussing in this chapter is that, in the previous chapters, we developed models from scratch.</p>
<p class="mce-root">Transfer learning can be defined as an approach where we reuse what a trained deep network has learned to solve a new but related problem. For example, we may be able to reuse a deep learning network that's been developed to classify thousands of different fashion items to develop a deep network to classify three different types of dresses. This approach is similar to what we can observe in real life, where a teacher transfers knowledge or learning gained over the years to students or a coach passes on learning or experience to new players. Another example is where learning to ride a bicycle is transferred to learning to ride a motorbike and this, in turn, becomes useful for learning how to drive a car.</p>
<p class="mce-root">In this chapter, we will make use of pretrained deep networks while developing models for image classification. Pretrained models allow us to transfer useful features that we've learned from a much larger dataset to models we are interested in developing with a somewhat similar, but new and relatively smaller dataset. The use of pretrained models not only allows us to overcome issues as a result of the dataset being small, but also helps reduce the time and cost of developing models.</p>
<p>To illustrate the use of pretrained image classification models, in this chapter, we will cover the following topics:</p>
<ul>
<li>Using a pretrained model to identify an image</li>
<li>Working with the CIFAR10 dataset</li>
<li>Image classification with CNN</li>
<li>Classifying images using the pretrained RESNET50 model</li>
<li>Model evaluation and prediction</li>
<li>Performance optimization tips and best practices</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a pretrained model to identify an image</h1>
                </header>
            
            <article>
                
<p>Before we proceed, let's load three packages that we'll need in this section:</p>
<pre># Libraries used<br/>library(keras)<br/>library(EBImage)<br/>library(tensorflow)</pre>
<p>The Keras and TensorFlow libraries will be used for developing the pretrained image classification model, while the EBImage library will be used for processing and visualizing image data.</p>
<p class="mce-root">In Keras, the following pretrained image classification models are available:</p>
<ul>
<li>Xception    </li>
<li>VGG16    </li>
<li>VGG19    </li>
<li>ResNet50    </li>
<li>InceptionV3    </li>
<li>InceptionResNetV2    </li>
<li>MobileNet    </li>
<li>MobileNetV2    </li>
<li>DenseNet    </li>
<li>NASNet</li>
</ul>
<p>These pretrained models are trained on images from ImageNet (<a href="http://www.image-net.org/">http://www.image-net.org/</a>). ImageNet is a huge image database that contains several million images.</p>
<p>We will start by using a pretrained model known as <kbd>resnet50</kbd> to identify an image. The following is the code we can use to utilize this pretrained model:</p>
<pre># Pretrained model<br/>pretrained &lt;- application_resnet50(weights = "imagenet")<br/>summary(pretrained)</pre>
<p>Here, we have specified <kbd>weights</kbd> as <kbd>"imagenet"</kbd>. This allows us to reuse the pretrained weights of the RESNET50 network. RESNET50 is a deep residual network that has a depth of 50 layers and includes convolutional neural network layers. Note that in case we only want to use the model architecture without the pretrained weights and we would like to train from scratch, then we can specify <kbd>weights</kbd> as <kbd>null</kbd>. By using <kbd>summary</kbd>, we can obtain the architecture of the RESNET50 network. However, to conserve space, we do not provide any output from the summary. The total number of parameters in this network is 25,636,712. The RESNET50 network is trained in using over a million images from ImageNet and has the capability to classify images into 1,000 different categories.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading an image</h1>
                </header>
            
            <article>
                
<p>Let's start by reading an image of a dog in RStudio. The following code loads an image file and then obtains the respective output:</p>
<div class="packt_tip">When using the RESNET50 network, the maximum target size that's allowed is 224 x 224 and the minimum target size that's allowed is 32 x 32.</div>
<pre># Read image data<br/>setwd("~/Desktop")<br/>img &lt;- image_load("dog.jpg", target_size = c(224,224))<br/>x &lt;- image_to_array(img)<br/>str(x)<br/>OUTPUT<br/>num [1:224, 1:224, 1:3] 70 69 68 73 88 79 18 22 21 20 ...<br/><br/># Image plot<br/>plot(as.raster(x, max = 255))  <br/><br/># Summary and histogram<br/>summary(x)<br/>OUTPUT<br/> Min. 1st Qu. Median Mean 3rd Qu. Max. <br/> 0.0 89.0 150.0 137.7 190.0 255.0 <br/>hist(x)</pre>
<p>In the preceding code, we can observe the following:</p>
<ul>
<li>A picture of a Norwich terrier dog is loaded from the computer desktop that's 224 x 224 in size using the <kbd>image_load()</kbd> function from Keras.</li>
<li>Note that the original image may not be 224 x 224 in size. However, specifying this dimension at the time of loading the image allows us to easily resize the original image so that it has new dimensions.</li>
<li>This image is converted into an array of numbers using the <kbd>image_to_array()</kbd> function. The structure of this array shows a dimension of 224 x 224 x 3.</li>
<li>The summary of the array shows that it contains numbers between zero and 255.</li>
</ul>
<p class="mce-root">The following is the 224 x 224 color picture of a Norwich terrier dog<span>. This can be obtained using a plot command:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c7f94f76-0669-4b3b-94fd-923433580bf8.png" style="width:25.75em;height:25.58em;"/></p>
<p>The preceding image is a<span> picture of a Norwich terrier dog sitting and looking forward. We will make use of this picture and check whether the RESNET50 model can accurately predict the type of dog in the picture.</span></p>
<p>A histogram that was developed from the values in the array is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6a64050d-273a-438d-b893-acc6f8172087.png" style="width:25.42em;height:24.00em;"/></p>
<p>The preceding histogram of values in the array shows that the intensity values range from zero to 255, with most of the values concentrated around 200. Next, we will preprocess the image data. This histogram can be used to compare the resulting changes to the image data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing the input</h1>
                </header>
            
            <article>
                
<p>We can now preprocess the input to prepare it so that it can be used with the pretrained RESNET50 model. The codes to preprocess the data are as follows:</p>
<pre># Preprocessing of input data<br/>x &lt;- array_reshape(x, c(1, dim(x))) <br/>x &lt;- imagenet_preprocess_input(x)<br/>hist(x)</pre>
<p>In the preceding code, we can observe the following:</p>
<ul>
<li>After applying the <kbd>array_reshape()</kbd> function, the dimensions of the array will change to 1 x 224 x 224 x 3.</li>
<li>We used the <kbd>imagnet_preprocess_input()</kbd> function to prepare the data in the required format using the pretrained model.</li>
</ul>
<p>A plot of the data in the form of a histogram after preprocessing is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e135198d-2aef-4e2d-8f41-9b507ea26fdf.png" style="width:28.50em;height:26.33em;"/></p>
<p>The histogram of values after preprocessing shows a shift in location. Most of the values are now concentrated between 50 and 100. However, there is no major change in the overall pattern of the histogram.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Top five categories</h1>
                </header>
            
            <article>
                
<p>Now, we can use the pretrained model to make predictions by providing preprocessed image data as input. The code to achieve this is as follows:</p>
<pre># Predictions for top 5 categories<br/>preds &lt;- pretrained %&gt;% predict(x)<br/>imagenet_decode_predictions(preds, top = 5)[[1]]<br/>Output<br/>  class_name  class_description       score<br/>1  n02094258    Norwich_terrier 0.769952953<br/>2  n02094114    Norfolk_terrier 0.126662806<br/>3  n02096294 Australian_terrier 0.046003290<br/>4  n02096177              cairn 0.040896162<br/>5  n02093991      Irish_terrier 0.005021056</pre>
<p>In the preceding code, we can observe the following:</p>
<ul>
<li>The predictions are made using the <kbd>predict</kbd> function and contain probabilities for 1,000 different categories, out of which the top five categories with the highest probabilities are obtained using the <kbd>imagenet_decode_predictions()</kbd> function.</li>
<li>The highest score of about 0.7699 correctly identifies that the picture is of a Norwich terrier dog.</li>
<li>The second highest score is for the Norfolk terrier dog, which looks very similar to the Norwich terrier dog.</li>
<li>The predictions also suggest that the picture could be of another type of terrier dog; however, those probabilities are relatively small or negligible.</li>
</ul>
<p>In the next section, we will look at a larger image dataset instead of a single image and use a pretrained network to develop an image classification model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with the CIFAR10 dataset</h1>
                </header>
            
            <article>
                
<p>For illustrating the use of pretrained models with new data, we will make use of the CIFAR10 dataset. CIFAR stands for <em>Canadian Institute For Advanced Research</em>, and 10 refers to the 10 categories of images that are contained in the data. The CIFAR10 dataset is part of the Keras library and the code for obtaining it is as follows:</p>
<pre># CIFAR10 data<br/>data &lt;- dataset_cifar10()<br/>str(data)<br/>OUTPUT<br/>List of 2<br/> $ train:List of 2<br/>  ..$ x: int [1:50000, 1:32, 1:32, 1:3] 59 154 255 28 170 159 164 28 134 125 ...<br/>  ..$ y: int [1:50000, 1] 6 9 9 4 1 1 2 7 8 3 ...<br/> $ test :List of 2<br/>  ..$ x: int [1:10000, 1:32, 1:32, 1:3] 158 235 158 155 65 179 160 83 23 217 ...<br/>  ..$ y: num [1:10000, 1] 3 8 8 0 6 6 1 6 3 1 ...</pre>
<p>In the preceding code, we can observe the following:</p>
<ul>
<li>We can read the dataset using the <kbd>dataset_cifar10()</kbd> function.</li>
<li>The structure of the data shows that there are 50,000 training images available with labels.</li>
<li>It also contains 10,000 test images with labels.</li>
</ul>
<p>Next, we will extract the train and test data from CIFAR10 using the following code:</p>
<pre># Partitioning the data into train and test<br/>trainx &lt;- data$train$x       <br/>testx &lt;- data$test$x<br/>trainy &lt;- to_categorical(data$train$y, num_classes = 10)<br/>testy &lt;- to_categorical(data$test$y, num_classes = 10)<br/><br/>table(data$train$y)<br/>OUTPUT<br/>   0    1    2    3    4    5    6    7    8    9 <br/>5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 <br/><br/>table(data$test$y)<br/>OUTPUT<br/>   0    1    2    3    4    5    6    7    8    9 <br/>1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 <br/><br/></pre>
<p>From the preceding code, we can observe the following:</p>
<ul>
<li>We saved the training image data in <kbd>trainx</kbd> and the test image data in <kbd>testx</kbd>.</li>
<li>We also carry out one-hot encoding of the train and test data labels using the <kbd>to_categorical()</kbd>  function and save the results in <kbd>trainy</kbd> and <kbd>testy</kbd>, respectively.</li>
<li>The table for the training data indicates that the images are classified in 10 different categories, with each category containing exactly 5,000 images.</li>
<li>Similarly, the test data contains exactly 1,000 images for each of the 10 categories.</li>
</ul>
<p>As an example, the labels for the first 64 images in the training data can be obtained using the following code:</p>
<pre># Category Labels<br/>data$train$y[1:64,]<br/> [1] 6 9 9 4 1 1 2 7 8 3 4 7 7 2 9 9 9 3 2 6 4 3 6 6 2 6 3 5 4 0 0 9 1<br/>[34] 3 4 0 3 7 3 3 5 2 2 7 1 1 1 2 2 0 9 5 7 9 2 2 5 2 4 3 1 1 8 2</pre>
<p>As we can see, each picture is labeled using a number between 0 and 9. A description of the 10 different categories of images can be seen in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 827px;height: 456px">
<thead>
<tr>
<th class="CDPAlignCenter CDPAlign" style="width: 263.576px">Label</th>
<th class="CDPAlignCenter CDPAlign" style="width: 555.799px">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">0</td>
<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">Airplane</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">1</td>
<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">Automobile</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">2</td>
<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">Bird</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">3</td>
<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">Cat</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">4</td>
<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">Deer</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">5</td>
<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">Dog</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">6</td>
<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">Frog</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">7</td>
<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">Horse</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">8</td>
<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">Ship</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 263.576px">9</td>
<td class="CDPAlignCenter CDPAlign" style="width: 555.799px">Truck</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Note that there is no overlap between these 10 categories. For example, the automobile category refers to cars and SUVs, whereas the truck category only refers to large trucks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sample images</h1>
                </header>
            
            <article>
                
<p>The first 64 images from the training data of CIFAR10 can be plotted using the following code. Doing this, we can get a glimpse of the type of images contained in the dataset:</p>
<pre># Plot of first 64 pictures<br/>par(mfrow = c(8,8), mar = rep(0, 4))<br/>for (i in 1:64) plot(as.raster(trainx[i,,,], max = 255))<br/>par(mfrow = c(1,1))</pre>
<p>The images from CIFAR10 are all 32 x 32 color images. The following plot shows 64 images in an 8 x 8 grid:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1db8a05c-8153-45ea-8868-1d191620f6b8.png" style="width:43.50em;height:40.83em;"/></p>
<p>From the preceding images, we can see that these images come with various backgrounds and are of low resolution. In addition, sometimes, these images aren't completely visible, which makes image classification a challenging task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing and prediction</h1>
                </header>
            
            <article>
                
<p>We can use the pretrained RESNET50 model to identify the second image in the training data. Note that, since this second image in the training data is 32 x 32 in size, whereas RESNET50 is trained on images that are 224 x 224 in size, we need to resize the image before applying the code that we have used earlier. The following code is used for identifying the image:</p>
<pre># Pre-processing and prediction<br/>x &lt;- resize(trainx[2,,,], w = 224, h = 224)<br/>x &lt;- array_reshape(x, c(1, dim(x)))<br/>x &lt;- imagenet_preprocess_input(x)<br/>preds &lt;- pretrained %&gt;% predict(x)<br/>imagenet_decode_predictions(preds, top = 5)[[1]]<br/>OUTPUT<br/>  class_name class_description        score<br/>1  n03796401        moving_van 9.988740e-01<br/>2  n04467665     trailer_truck 7.548324e-04<br/>3  n03895866     passenger_car 2.044246e-04<br/>4  n04612504              yawl 2.441246e-05<br/>5  n04483307          trimaran 1.862814e-05</pre>
<p>From the preceding code, we can observe that the top category with a score of 0.9988 is for a moving van. The scores for the other four categories are comparatively negligible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image classification with CNN</h1>
                </header>
            
            <article>
                
<p>In this section, we will use a subset of the CIFAR10 dataset to develop a convolutional neural network-based image classification model and assess its classification performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preparation</h1>
                </header>
            
            <article>
                
<p>We will keep the data size smaller by using only the first 2,000 images in the training and test data from CIFAR10. This will allow the image classification model to be run on a regular computer or laptop. We will also resize the training and test images from 32 x 32 dimensions to 224 x 224 dimensions to be able to compare classification performance with the pretrained model. The following code includes the necessary preprocessing that we went over earlier in this chapter:</p>
<pre># Selecting first 2000 images<br/>trainx &lt;- data$train$x[1:2000,,,] <br/>testx &lt;- data$test$x[1:2000,,,] <br/><br/># One-hot encoding<br/>trainy &lt;- to_categorical(data$train$y[1:2000,], num_classes = 10)<br/>testy &lt;- to_categorical(data$test$y[1:2000,] , num_classes = 10)<br/><br/># Resizing train images to 224x224<br/>x &lt;- array(rep(0, 2000 * 224 * 224 * 3), dim = c(2000, 224, 224, 3))<br/>for (i in 1:2000) { x[i,,,] &lt;- resize(trainx[i,,,], 224, 224) }<br/><br/># Plot of before/after resized image<br/>par(mfrow = c(1,2), mar = rep(0, 4))  <br/>plot(as.raster(trainx[2,,,], max = 255))<br/>plot(as.raster(x[2,,,], max = 255))<br/>par(mfrow = c(1,1))<br/><br/>trainx &lt;- imagenet_preprocess_input(x)<br/><br/># Resizing test images to 224x224<br/>x &lt;- array(rep(0, 2000 * 224 * 224 * 3), dim = c(2000, 224, 224, 3))<br/>for (i in 1:2000) { x[i,,,] &lt;- resize(testx[i,,,], 224, 224) }<br/>testx &lt;- imagenet_preprocess_input(x)</pre>
<p>In the preceding code, while resizing dimensions from 32 x 32 to 224 x 224, we use bilinear interpolation, which is included as part of the EBImage package. Bilinear interpolation extends linear interpolation to two variables, which in this case is the height and width of an image. The effect of bilinear interpolation can be observed from the before and after images of the truck shown in the following image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b87b0400-79fc-43ed-b4d3-bfc2b466229a.png" style="width:36.92em;height:19.00em;"/></p>
<p>Here, we can see that the after image (second image) looks smoother as it contains more pixels compared to the original image (first image).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNN model</h1>
                </header>
            
            <article>
                
<p class="mce-root">We will start by using a not-so-deep convolutional neural network to develop an image classification model. We will use the following code for this:</p>
<pre># Model architecture<br/>model &lt;- keras_model_sequential()<br/>model %&gt;% <br/>  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu', <br/>                input_shape = c(224,224,3)) %&gt;%   <br/>  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu') %&gt;%  <br/>  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% <br/>  layer_dropout(rate = 0.25) %&gt;%   <br/>  layer_flatten() %&gt;% <br/>  layer_dense(units = 256, activation = 'relu') %&gt;%  <br/>  layer_dropout(rate = 0.25) %&gt;% <br/>  layer_dense(units = 10, activation = 'softmax') <br/>summary(model)<br/><strong>_________________________________________________________________________</strong><br/><strong>Layer (type)                    Output Shape                Param #       </strong><br/><strong>=========================================================================</strong><br/><strong>conv2d_6 (Conv2D)               (None, 222, 222, 32)         896           </strong><br/><strong>_________________________________________________________________________</strong><br/><strong>conv2d_7 (Conv2D)               (None, 220, 220, 32)         9248          </strong><br/><strong>_________________________________________________________________________</strong><br/><strong>max_pooling2d_22 (MaxPooling2D) (None, 110, 110, 32)          0             </strong><br/><strong>_________________________________________________________________________</strong><br/><strong>dropout_6 (Dropout)             (None, 110, 110, 32)          0             </strong><br/><strong>_________________________________________________________________________</strong><br/><strong>flatten_18 (Flatten)            (None, 387200)                0             </strong><br/><strong>__________________________________________________________________________</strong><br/><strong>dense_35 (Dense)                (None, 256)                99123456      </strong><br/><strong>__________________________________________________________________________</strong><br/><strong>dropout_7 (Dropout)             (None, 256)                   0             </strong><br/><strong>__________________________________________________________________________</strong><br/><strong>dense_36 (Dense)                (None, 10)                   2570          </strong><br/><strong>==========================================================================</strong><br/><strong>Total params: 99,136,170</strong><br/><strong>Trainable params: 99,136,170</strong><br/><strong>Non-trainable params: 0</strong><br/><strong>_______________________________________________________________________________________</strong>____<br/><br/># Compile<br/>model %&gt;% compile(loss = 'categorical_crossentropy',<br/> optimizer = 'rmsprop', <br/> metrics = 'accuracy')<br/><br/># Fit<br/>model_one &lt;- model %&gt;% fit(trainx, <br/>                         trainy, <br/>                         epochs = 10, <br/>                         batch_size = 10, <br/>                         validation_split = 0.2)</pre>
<p>From the preceding code, we can observe the following:</p>
<ul>
<li>The total number of parameters in this network is 99,136,170.</li>
<li>When compiling the model, we use <kbd>categorical_crossentropy</kbd> as the loss function since the response has 10 categories.</li>
<li>For the optimizer, we specify <kbd>rmsprop</kbd>, which is a gradient-based optimization method and is a popular choice that provides reasonably good performance.</li>
<li>We train the model with 10 epochs and with a batch size of 10.</li>
<li>Out of 2,000 images in the training data, 20% (or 400 images) is used for assessing validation errors and the remaining 80% (or 1,600 images) is used for training.</li>
</ul>
<p>A plot of the accuracy and loss values after training the model is as follows for <kbd>model_one</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a382f094-6b43-4c2c-9725-6168925657f7.png" style="width:38.92em;height:30.42em;"/></p>
<p>From the preceding plot, the following observations can be made:</p>
<ul>
<li>The plot of the accuracy and loss values shows that, after about 4 epochs, the loss and accuracy values for both training and validation data remain more or less constant.</li>
<li>Although the accuracy for the training data reaches high values closer to 100%, there seems to be no impact on the accuracy based on the images in the validation data.</li>
<li>In addition, the gap between the accuracy of the training and validation data seems to be high, suggesting the presence of overfitting. When assessing the model's performance, we expect to see low accuracy in terms of image classification by the model.</li>
</ul>
<p>Note that developing a decent image classification model using CNN requires a large number of images for training, and therefore more time and resources. Later in this chapter, we will learn how to use pretrained networks to help us overcome this problem. For now, though, let's proceed by assessing the image classification model's performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model performance</h1>
                </header>
            
            <article>
                
<p>For assessing the model's performance, we will carry out calculations for the loss, accuracy, and confusion matrix for the training and test data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance assessment with training data</h1>
                </header>
            
            <article>
                
<p>The code for obtaining the loss, accuracy, and confusion matrix based on the training data is as follows:</p>
<pre># Loss and accuracy<br/>model %&gt;% evaluate(trainx, trainy)<br/>$loss<br/>[1] 3.335224<br/>$acc<br/>[1] 0.8455<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;%   predict_classes(trainx)<br/>table(Predicted=pred, Actual=data$train$y[1:2000,])<br/>         Actual<br/>Predicted   0   1   2   3   4   5   6   7   8   9<br/>        0 182   2   8   2   9   4   1   2  10   5<br/>        1   1 176   3   5   6   5   2   3   4   7<br/>        2   1   0 167   4   3   4   3   2   0   1<br/>        3   0   0   0 157   2   1   1   2   1   0<br/>        4   2   1   5   6 167   4   2   1   0   0<br/>        5   2   0   4   4   3 149   3   4   4   3<br/>        6   1   1   3   6   5   2 173   5   0   0<br/>        7   3   2   4   2   4   3   9 166   0   1<br/>        8  10   1   7   1   6   4   2   2 173   5<br/>        9   0   8   2   8   9   7  11  12  11 181</pre>
<p>Here, we can see that the loss and accuracy values for the training data are 3.335 and 0.846, respectively. The confusion matrix shows decent results based on the training data. However, for some types of images, misclassifications are high. For example, 12 images from category 7 (horse) are misclassified as category-9 (truck). Similarly, 11 images, each belonging to category-6 (frog) and category-8 (ship), are also misclassified as category-9 (truck).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance assessment with test data</h1>
                </header>
            
            <article>
                
<p>The code for obtaining the loss, accuracy, and confusion matrix based on the test data is as follows:</p>
<pre># Loss and accuracy<br/>model %&gt;% evaluate(testx, testy)<br/>$loss<br/>[1] 16.4562<br/>$acc<br/>[1] 0.2325<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;% predict_classes(testx)<br/>table(Predicted = pred, Actual = data$test$y[1:2000,])<br/>         Actual<br/>Predicted  0  1  2  3  4  5  6  7  8  9<br/>        0 82 24 29 17 16 10 17 19 67 19<br/>        1 16 65 20 26 18 21 26 26 33 53<br/>        2 10  0 26 20 20 18 14  5  1  2<br/>        3  6  5  8 21 12 22  9 12  9  3<br/>        4  4  8 22 11 22 16 25  9  6  4<br/>        5  5  7 12 29 17 29  9 19  4  9<br/>        6  6  6 20 17 23 15 51 25  6 13<br/>        7  3 10 10 15 21 16 11 37  3  5<br/>        8 34 22 20 12 22  2  7  7 61 24<br/>        9 30 51 28 31 27 36 47 34 27 71</pre>
<p>From the preceding output, the following observations can be made:</p>
<ul>
<li>The loss and accuracy values for the test data are 16.456 and 0.232, respectively.</li>
<li>These results are not as impressive as what we observed for the training data due to the overfitting problem.</li>
</ul>
<p>Although we can try and develop a deeper network in an effort to improve image classification results or to try and increase training data to provide more samples to learn from, here, we will make use of pretrained networks to obtain better results. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying images using the pretrained RESNET50 model</h1>
                </header>
            
            <article>
                
<p>In this section, we will make use of the pretrained RESNET50 model to develop an image classification model. We will use the same training and test data that we used in the previous section to make comparing classification performance easier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model architecture</h1>
                </header>
            
            <article>
                
<p>We will upload the RESNET50 model without including the top layer. This will help us customize the pretrained model for use with CIFAR10 data. Since the RESNET50 model is trained with the help of over 1 million images, it captures useful features and representations of images that can be reused with new but similar and smaller data. This reusability aspect of pretrained models not only helps to reduce the time and cost of developing an image classification model from scratch, but is especially useful when the training data is relatively small. </p>
<p>The code that's used for developing the model is as follows:</p>
<pre># RESNET50 network without the top layer<br/>pretrained &lt;- application_resnet50(weights = "imagenet",<br/>                                   include_top = FALSE,<br/>                                   input_shape = c(224, 224, 3))<br/><br/>model &lt;- keras_model_sequential() %&gt;% <br/>         pretrained %&gt;% <br/>         layer_flatten() %&gt;% <br/>         layer_dense(units = 256, activation = "relu") %&gt;% <br/>         layer_dense(units = 10, activation = "softmax")<br/>summary(model)<br/><strong>_______________________________________________________________________</strong><br/><strong>Layer (type)                   Output Shape              Param #     </strong><br/><strong>=======================================================================</strong><br/><strong>resnet50 (Model)               (None, 7, 7, 2048)        23587712    </strong><br/><strong>________________________________________________________________________</strong><br/><strong>flatten_6 (Flatten)            (None, 100352)               0           </strong><br/><strong>________________________________________________________________________</strong><br/><strong>dense_12 (Dense)               (None, 256)               25690368    </strong><br/><strong>________________________________________________________________________</strong><br/><strong>dense_13 (Dense)               (None, 10)                  2570        </strong><br/><strong>========================================================================</strong><br/><strong>Total params: 49,280,650</strong><br/><strong>Trainable params: 49,227,530</strong><br/><strong>Non-trainable params: 53,120</strong><br/><strong>_________________________________________________________________________</strong></pre>
<p>When uploading the RESNET50 model, the input dimensions for the data based on color images are specified as 224 x 224 x 3. Although smaller dimensions will work too, image dimensions cannot be less than 32 x 32 x 3. Images in the CIFAR10 dataset have dimensions of 32 x 32 x 3, but we have resized them to 224 x 224 x 3 as it gives us better image classification accuracy.</p>
<p>From the preceding summary, we can observe the following:</p>
<ul>
<li>The output dimensions from the RESNET50 network are 7 x 7 x 2,048.</li>
<li>We use a flattened layer to change the output shape to a single column with 7 x 7 x 2,048 = 100,352 elements.</li>
<li>A dense layer with 256 units and a <kbd>relu</kbd> activation function is added.</li>
<li>This dense layer leads to (100,353 x 256) + 256 = 25,690,368 parameters.</li>
<li>The last dense layer has 10 units for images with 10 categories and a <kbd>softmax</kbd> activation function. This network has a total of 49,280,650 parameters.</li>
<li>Out of the total parameters in the network, 49,227,530 are trainable parameters.</li>
</ul>
<p>Although we can train the network with all of these parameters, this is not advisable. Training and updating parameters related to the RESNET50 network will cause us to lose the benefits that we would get as a result of the features that have been learned from over 1 million images. We are only using data from 2,000 images for training and have 10 different categories. So, for each category, we only have approximately 200 images. Therefore, it is important to freeze the weights in the RESNET50 network, which will allow us to obtain the benefits of using a pretrained network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Freezing pretrained network weights</h1>
                </header>
            
            <article>
                
<p>The code for freezing the weights of the RESNET50 network and then compiling the model is as follows:</p>
<pre># Freeze weights of resnet50 network<br/>freeze_weights(pretrained)<br/><br/># Compile<br/>model %&gt;% compile(loss = 'categorical_crossentropy',<br/> optimizer = 'rmsprop', <br/> metrics = 'accuracy')<br/><br/><strong>summary(model)</strong><br/><strong>______________________________________________________</strong><br/><strong>Layer (type) Output Shape Param # </strong><br/><strong>======================================================</strong><br/><strong>resnet50 (Model) (None, 7, 7, 2048) 23587712 </strong><br/><strong>______________________________________________________</strong><br/><strong>flatten_6 (Flatten) (None, 100352) 0 </strong><br/><strong>______________________________________________________</strong><br/><strong>dense_12 (Dense) (None, 256) 25690368 </strong><br/><strong>______________________________________________________</strong><br/><strong>dense_13 (Dense) (None, 10) 2570 </strong><br/><strong>======================================================</strong><br/><strong>Total params: 49,280,650</strong><br/><strong>Trainable params: 25,692,938</strong><br/><strong>Non-trainable params: 23,587,712</strong><br/><strong>______________________________________________________</strong></pre>
<p>In the preceding code, we can observe the following:</p>
<ul>
<li>To freeze the weights in the RESNET50 network, we use the <kbd>freeze_weights()</kbd> function.</li>
<li>Note that after freezing the pretrained network weights, the model needs to be compiled.</li>
<li>After freezing the weights of the RESNET50 network, we observe that the number of trainable parameters goes down from 49,227,530 to a lower value of 25,692,938.</li>
<li>These parameters belong to the two dense layers that we added and will help us customize the results from the RESNET50 network so that we can apply them to the images from the CIFAR10 data that we are using.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fitting the model</h1>
                </header>
            
            <article>
                
<p>The code for fitting the model is as follows:</p>
<pre># Fit model<br/>model_two &lt;- model %&gt;% fit(trainx, <br/>                        trainy, <br/>                        epochs = 10, <br/>                        batch_size = 10, <br/>                        validation_split = 0.2)</pre>
<p class="mce-root">From the preceding code, we can observe the following:</p>
<ul>
<li class="mce-root">We train the network with 10 epochs and with a batch size of 10.</li>
<li class="mce-root">We specify 20% (or 400 images) to be used for assessing the validation loss and validation accuracy, and the remaining 80% (or 1,600 images) for training.</li>
</ul>
<p class="mce-root">The plot of the accuracy and loss values after training the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5deac824-d6ec-4195-917a-6a696101b72d.png" style="width:43.42em;height:32.17em;"/></p>
<p>From the plot for the loss and accuracy values, we can make the following observations:</p>
<ul>
<li>There is an important difference compared to the previous plot, where the pretrained model wasn't used. This plot shows us that the model reaches an accuracy of over 60% by the second epoch itself compared to the previous plot, where it remained below 25%. Thus, we can see that the use of a pretrained model has an immediate impact on image classification.</li>
<li>The improvements based on validation data are slow compared to those for the training data.</li>
<li>Although the accuracy values based on the validation data show gradual improvement, the loss values for the validation data show more variability.</li>
</ul>
<p>In the next section, we will evaluate the model and assess its prediction performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model evaluation and prediction</h1>
                </header>
            
            <article>
                
<p>Now, we will evaluate the performance of this model for the training and test data. Calculations relating to the loss, accuracy, and confusion matrix will be carried out so that we can evaluate the model image's classification performance. We will also obtain the accuracy for each of the 10 categories.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loss, accuracy, and confusion matrix with the training data</h1>
                </header>
            
            <article>
                
<p>The code for obtaining the loss, accuracy, and confusion matrix for the training data is as follows:</p>
<pre># Loss and accuracy<br/>model %&gt;% evaluate(trainx, trainy)<br/>$loss<br/>[1] 1.954347<br/>$acc<br/>[1] 0.8785<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;%   predict_classes(trainx)<br/>table(Predicted=pred, Actual=data$train$y[1:2000,])<br/>         Actual<br/>Predicted   0   1   2   3   4   5   6   7   8   9<br/>        0 182   0   5   2   3   0   2   0  10   1<br/>        1   1 156   1   1   1   0   2   0   4   0<br/>        2   2   0 172   3   4   0   4   0   1   0<br/>        3   0   0   1 133   2  12   2   1   0   0<br/>        4   1   0   8   4 188   3   4   2   0   0<br/>        5   1   0   4  22   3 162   1   3   0   0<br/>        6   0   0   3   9   3   0 192   1   1   0<br/>        7   3   0   5  10  10   5   0 188   0   0<br/>        8   5   0   3   3   0   1   0   1 182   0<br/>        9   7  35   1   8   0   0   0   3   5 202<br/><br/># Accuracy for each category<br/>100*diag(tab)/colSums(tab)<br/>       0        1        2        3        4 <br/>90.09901 81.67539 84.72906 68.20513 87.85047 <br/>       5        6        7        8        9 <br/>88.52459 92.75362 94.47236 89.65517 99.50739 </pre>
<p>From the preceding output, we can make the following observations:</p>
<ul>
<li>The loss and accuracy based on the training data are 1.954 and 0.879, respectively.</li>
<li>Both of these numbers are an improvement over the corresponding results based on the previous model.</li>
<li>The confusion matrix shows a decent image classification performance.</li>
<li>The best image classification performance is seen for category 9 (truck), where only one image is misclassified as category-0 (airplane) and provides an accuracy of 99.5%.</li>
<li>This model is the most confused regarding category-3 (cat), which is mostly classified as category-5 (dog) or category-7 (horse) and provides an accuracy of only 68.2% for this category.</li>
<li>Among the misclassifications, the highest case (35 images) is when category-1 (automobile) is misclassified as category-9 (truck).</li>
</ul>
<p>Next, we will assess the model's performance using the test data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loss, accuracy, and confusion matrix with the test data</h1>
                </header>
            
            <article>
                
<p>The code for obtaining the loss, accuracy, and confusion matrix for the test data is as follows:</p>
<pre># Loss and accuracy<br/>model %&gt;% evaluate(testx, testy)<br/>$loss<br/>[1] 4.437256<br/>$acc<br/>[1] 0.768<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;% predict_classes(testx)<br/>table(Predicted = pred, Actual = data$test$y[1:2000,])<br/>         Actual<br/>Predicted   0   1   2   3   4   5   6   7   8   9<br/>        0 158   1  12   0   5   1   6   2  15   0<br/>        1   3 142   0   2   0   2   3   1   9   2<br/>        2   2   0 139   8   6   3   6   0   0   0<br/>        3   0   0   3  86   5  13   6   1   0   0<br/>        4   4   0  14   6 138   5  10   4   1   0<br/>        5   0   0  15  47   6 148   2  12   0   0<br/>        6   0   0   4  12   9   3 178   0   0   0<br/>        7   2   0   4  23  27   9   3 169   0   0<br/>        8  13   1   1   5   1   0   0   0 179   2<br/>        9  14  54   3  10   1   1   2   4  13 199<br/><br/># Accuracy for each category<br/>100*diag(tab)/colSums(tab)<br/>       0        1        2        3        4 <br/>80.61224 71.71717 71.28205 43.21608 69.69697 <br/>       5        6        7        8        9 <br/>80.00000 82.40741 87.56477 82.48848 98.02956 </pre>
<p>From the preceding output, we can make the following observations:</p>
<ul>
<li>The loss and accuracy based on the test data are 4.437 and 0.768, respectively.</li>
<li>Although this performance based on the test data is inferior to the results based on the training data, it is a significant improvement over the results from the first model.</li>
</ul>
<ul>
<li>The confusion matrix provides further insights into the model's performance. The best performance is for category 9 (truck), with 199 correct classifications and an accuracy of 98%.</li>
<li>For the test data, the model seems to be the most confused regarding category-3 (cat), which has the most misclassifications. The accuracy of this category can be as low as 43.2%.</li>
<li>The highest misclassification for a single category (54 images) is for category-1 (automobile), which is misclassified as category-9 (truck).</li>
</ul>
<p>With 76.8% accuracy, we can say that this image classification performance is decent. The use of a pretrained model has allowed us to transfer our learning of a model trained on data involving over 1 million images to new data containing 2,000 images from the CIFAR10 dataset. This is a huge advantage compared to building an image classification model totally from scratch, which would involve more time and computing costs. Now that we've achieved a decent performance from the model, we can explore how to improve this even further.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance optimization tips and best practices</h1>
                </header>
            
            <article>
                
<p>To explore further image classification improvement, in this section, we will try three experiments. In the first experiment, we will mainly use the <kbd>adam</kbd> optimizer when compiling the model. In the second experiment, we will carry out hyperparameter tuning by varying the number of units in the dense layer, the dropout percentage in the dropout layer, and the batch size when fitting the model. Finally, in the third experiment, we will work with another pretrained network called VGG16.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Experimenting with the adam optimizer</h1>
                </header>
            
            <article>
                
<p>In this first experiment, we will use the <kbd>adam</kbd> optimizer when compiling the model. At the time of training the model, we will also increase the number of epochs to 20.</p>
<p>The plot of the accuracy and loss values after training the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f0987325-56b5-4aa2-ad2a-d7ea843fb6f4.png" style="width:45.17em;height:40.50em;"/></p>
<p>The preceding loss and accuracy plot for this model shows that the values related to the training data are flat after about six epochs. For the validation data, the loss values show a gradual increase, whereas the accuracy values are flat after the third epoch.</p>
<p>The code for obtaining the loss, accuracy, and confusion matrix for the test data is as follows:</p>
<pre># Loss and accuracy<br/>model %&gt;% evaluate(testx, testy)<br/>$loss<br/>[1] 4.005393<br/>$acc<br/>[1] 0.7715<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;% predict_classes(testx)<br/>table(Predicted = pred, Actual = data$test$y[1:2000,])<br/>         Actual<br/>Predicted   0   1   2   3   4   5   6   7   8   9<br/>        0 136   0  20   4   2   0   1   5   2   4<br/>        1   3 177   1   1   0   0   0   0   2  26<br/>        2   7   0 124   2   3   1   3   0   1   0<br/>        3   2   0   4  80   7   6   7   2   1   0<br/>        4   3   1  18   9 151   4   8   9   0   0<br/>        5   2   0   3  58   3 152   4   5   0   3<br/>        6   3   2   8  22   8   8 190   0   6   2<br/>        7   1   0  14  18  22  14   2 172   0   0<br/>        8  36  11   3   5   2   0   1   0 205  12<br/>        9   3   7   0   0   0   0   0   0   0 156<br/><br/># Accuracy for each category<br/>100*diag(tab)/colSums(tab)<br/>       0        1        2        3        4 <br/>69.38776 89.39394 63.58974 40.20101 76.26263 <br/>       5        6        7        8        9 <br/>82.16216 87.96296 89.11917 94.47005 76.84729 </pre>
<p>From the preceding output, we can make the following observations:</p>
<ul>
<li>The loss and accuracy of the test data are 4.005 and 0.772, respectively.</li>
<li>These results are marginally better than they are for <kbd>model_two</kbd>.</li>
</ul>
<ul>
<li>The confusion matrix shows a somewhat different image classification pattern compared to the previous model.</li>
<li>The best classification results are obtained for category 8 (ship), with 205 correct image classifications out of 217 (94.5% accuracy).</li>
<li>The lowest classification performance is for category 3 (cat), with 80 correct predictions out of 199 (40.2% accuracy).</li>
<li>The worst misclassification is of 58 images from category 3 (cat) when they are misclassified as category-5 (dog).</li>
</ul>
<p>Next, we will carry out an experiment with hyperparameter tuning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameter tuning</h1>
                </header>
            
            <article>
                
<p>In this experiment, we will vary the units in the dense layer, the dropout rate, and the batch size to obtain values that help us improve classification performance. This also illustrates an efficient way of obtaining suitable parameter values through experimentation. We will start by creating a <kbd>TransferLearning.R</kbd> file using the following code:</p>
<pre># Model with RESNET50<br/>pretrained &lt;- application_resnet50(weights = 'imagenet',<br/>                                   include_top = FALSE,<br/>                                   input_shape = c(224, 224, 3))<br/># Flags for hyperparameter tuning<br/>FLAGS &lt;- flags(flag_integer("dense_units", 256),<br/>               flag_numeric("dropout", 0.1),<br/>               flag_integer("batch_size", 10))<br/><br/># Model architecture<br/>model &lt;- keras_model_sequential() %&gt;% <br/>         pretrained %&gt;% <br/>         layer_flatten() %&gt;% <br/>         layer_dense(units = FLAGS$dense_units, activation = 'relu') %&gt;% <br/>         layer_dropout(rate = FLAGS$dropout) %&gt;%<br/>         layer_dense(units = 10, activation = 'softmax')<br/>freeze_weights(pretrained)<br/><br/># Compile<br/>model %&gt;% compile(loss = "categorical_crossentropy",<br/>                  optimizer = 'adam',<br/>                  metrics = 'accuracy')<br/><br/># Fit model<br/>history &lt;- model %&gt;% fit(trainx,<br/>                         trainy,<br/>                         epochs = 5,<br/>                         batch_size = FLAGS$batch_size,<br/>                         validation_split = 0.2)</pre>
<p>In the preceding code, after reading the pretrained model, we declare three flags for the parameters that we want to experiment with. Now, we can use these flags in the model architecture (dense units and dropout rate) and in the code for fitting the model (batch size). We have reduced the number of epochs to five and for the optimizer, while compiling the model, we retain <kbd>adam</kbd>. We will save this R file, which we'll call <kbd>TransferLearning.R</kbd>, on the desktop of our computer.</p>
<p>The code for running this experiment is as follows:</p>
<pre># Set working directory<br/>setwd('~/Desktop')<br/><br/># Hyperparameter tuning<br/>library(tfruns)<br/>runs &lt;- tuning_run("TransferLearning.R", <br/>                   flags = list(dense_units = c(256, 512),<br/>                                dropout = c(0.1,0.3),<br/>                                batch_size = c(10, 30)))</pre>
<p>In the preceding code, we can see that the working directory is set at the location of the <kbd>TransferLearning.R</kbd> file. Note that the output from this experiment will be saved in this directory too. For running the hyperparameter tuning experiment, we will use the <kbd>tfruns</kbd> library. For the number of units in the dense layer, we will try 256 and 512 as the values. For the dropout rate, we will experiment with 0.1 and 0.3. Fnally, for the batch size, we will try 10 and 30. With three parameters, each being tried at two values, the total number of experimental runs will be 2<sup>3</sup> = 8.</p>
<p>An extract from the results that were obtained from this experiment is as follows:</p>
<pre># Results<br/>runs[,c(6:10)]<br/>Data frame: 8 x 5 <br/><br/>  metric_val_loss metric_val_acc flag_dense_units flag_dropout flag_batch_size<br/>1          1.1935         0.7525              512          0.3              30<br/>2          0.9521         0.7725              256          0.3              30<br/>3          1.1260         0.8200              512          0.1              30<br/>4          1.3276         0.7950              256          0.1              30<br/>5          1.1435         0.7700              512          0.3              10<br/>6          1.3096         0.7275              256          0.3              10<br/>7          1.3458         0.7850              512          0.1              10<br/>8          1.0248         0.7950              256          0.1              10</pre>
<p>The preceding output shows the loss and accuracy values based on the validation data for all eight experimental runs. For easy reference, it also includes parameter values. We can make the following observations from the preceding output:</p>
<ul>
<li> The highest accuracy value (row 3) is obtained when the number of dense units is 512, the dropout rate is 0.1, and the batch size is 30.</li>
<li>On the other hand, the lowest accuracy value (row 6) is obtained when the number of dense units is 256, the dropout rate is 0.3, and the batch size is 10. </li>
</ul>
<p>The code for obtaining the loss, accuracy, and confusion matrix using the test data for row 3 of the experiment is as follows:</p>
<pre># Loss and accuracy<br/>model %&gt;% evaluate(testx, testy)<br/>$loss<br/>[1] 1.095251<br/>$acc<br/>[1] 0.7975<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;% predict_classes(testx)<br/>(tab &lt;- table(Predicted = pred, Actual = data$test$y[1:2000,]))<br/>         Actual<br/>Predicted   0   1   2   3   4   5   6   7   8   9<br/>        0 167   5  20   4   5   4   4  15  10   8<br/>        1   1 176   0   3   0   0   1   1   2  15<br/>        2   3   0 139   9   2   4   8   1   1   0<br/>        3   0   0   3  92   6   6   5   0   0   0<br/>        4   4   0  20  16 177  12  17  23   0   1<br/>        5   0   0   7  50   1 149   1   9   0   0<br/>        6   1   0   2  11   2   5 177   1   0   1<br/>        7   0   0   0   5   3   4   1 143   0   0<br/>        8  16   3   3   5   2   0   1   0 203   6<br/>        9   4  14   1   4   0   1   1   0   1 172<br/><br/># Accuracy for each category<br/>100*diag(tab)/colSums(tab)<br/>       0        1        2        3        4        5        6        7 <br/>85.20408 88.88889 71.28205 46.23116 89.39394 80.54054 81.94444 74.09326 <br/>       8        9 <br/>93.54839 84.72906 </pre>
<p>From the preceding results, we can make the following observations:</p>
<ul>
<li>Both the loss and accuracy values for the test data are better than the results we've obtained so far.</li>
<li>The best classification results are obtained for category 8 (ship), with 203 correct image classifications out of 217 (93.5% accuracy).</li>
<li>The lowest classification performance is for category 3 (cat), with 92 correct predictions out of 199 (46.2% accuracy).</li>
<li>The worst misclassification is of 50 images from category 3 (cat) when they are misclassified as category-5 (dog).</li>
</ul>
<p>In the next experiment, we will use another pretrained network: VGG16.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Experimenting with VGG16 as a pretrained network</h1>
                </header>
            
            <article>
                
<p>In this experiment, we will use a pretrained network called VGG16. VGG16 is a convolutional neural network that is 16 layers deep and can classify images into thousands of categories. This network is also trained using over 1 million images from the ImageNet database. The code for the model's architecture and compiling and then fitting the model is as follows:</p>
<pre># Pretrained model<br/>pretrained &lt;- application_vgg16(weights = 'imagenet', <br/>                           include_top = FALSE,<br/>                           input_shape = c(224, 224, 3))<br/><br/># Model architecture<br/>model &lt;- keras_model_sequential() %&gt;% <br/>  pretrained %&gt;% <br/>  layer_flatten() %&gt;% <br/>  layer_dense(units = 256, activation = "relu") %&gt;% <br/>  layer_dense(units = 10, activation = "softmax")<br/>summary(model)<br/><br/>freeze_weights(pretrained)<br/>summary(model)<br/><strong>_________________________________________________________________________</strong><br/><strong>Layer (type)                    Output Shape            Param #      </strong><br/><strong>=========================================================================</strong><br/><strong>vgg16 (Model)                  (None, 7, 7, 512)        14714688     </strong><br/><strong>__________________________________________________________________________</strong><br/><strong>flatten (Flatten)              (None, 25088)              0            </strong><br/><strong>__________________________________________________________________________</strong><br/><strong>dense (Dense)                  (None, 256)              6422784      </strong><br/><strong>__________________________________________________________________________</strong><br/><strong>dense_1 (Dense)                (None, 10)                2570         </strong><br/><strong>==========================================================================</strong><br/><strong>Total params: 21,140,042</strong><br/><strong>Trainable params: 6,425,354</strong><br/><strong>Non-trainable params: 14,714,688</strong><br/><strong>___________________________________________________________________________</strong><br/><br/># Compile model<br/>model %&gt;% compile(loss = 'categorical_crossentropy',<br/>                  optimizer = 'adam',    <br/>                  metrics = 'accuracy')<br/><br/># Fit model<br/>model_four &lt;- model %&gt;% fit(trainx, <br/>                         trainy, <br/>                         epochs = 10, <br/>                         batch_size = 10, <br/>                         validation_split = 0.2)</pre>
<p>From the preceding summary, we can observe the following:</p>
<ul>
<li>This model has 21,140,042 parameters, which, after freezing the weights of VGG16, goes down to a total of 6,425,354 trainable parameters.</li>
<li>When compiling the model, we retain the use of the <kbd>adam</kbd> optimizer.</li>
<li>In addition, we run 10 epochs to train the model. All the other settings are the same ones that we used for the previous models.</li>
</ul>
<p>A plot of the accuracy and loss values after training the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/57485ef8-cf6f-45da-be86-19fd0123cf60.png" style="width:44.25em;height:39.75em;"/></p>
<p>The preceding loss and accuracy plot for the training and validation data indicates that, after about four epochs, the model performance remains flat. This is in contrast to the previous model, where the loss values for the validation data showed a gradual increase.</p>
<p>The code for obtaining the loss, accuracy, and confusion matrix for the test data is as follows:</p>
<pre># Loss and accuracy<br/>model %&gt;% evaluate(testx, testy)<br/>$loss<br/> [1] 1.673867<br/>$acc<br/> [1] 0.7565<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;% predict_classes(testx)<br/>(tab &lt;- table(Predicted = pred, Actual = data$test$y[1:2000,]))<br/>         Actual<br/>Predicted   0   1   2   3   4   5   6   7   8   9<br/>        0 137   2  12   0   6   0   0   1  11   6<br/>        1   9 172   1   0   0   0   0   1   9  21<br/>        2   7   0 123  11  11   3   3   5   3   0<br/>        3   3   0  11 130  10  35   7   7   0   0<br/>        4   7   0  13   5 118   7  10   5   1   0<br/>        5   1   0  11  27   3 125   2   7   0   0<br/>        6   2   5  20  18  21   8 192   3   4   1<br/>        7   6   0   4   6  25   7   2 163   2   1<br/>        8  18   6   0   2   4   0   0   1 182   3<br/>        9   6  13   0   0   0   0   0   0   5 171<br/><br/># Accuracy for each category<br/>100*diag(tab)/colSums(tab)<br/>       0        1        2        3        4 <br/>69.89796 86.86869 63.07692 65.32663 59.59596 <br/>       5        6        7        8        9 <br/>67.56757 88.88889 84.45596 83.87097 84.23645 </pre>
<p>From the preceding output, we can make the following observations:</p>
<ul>
<li>The loss and accuracy of the test data are 1.674 and 0.757, respectively.</li>
<li>The confusion matrix provides further insights. This model has the best classification accuracy of 88.9% when classifying category 6 (frog).</li>
<li>On the other hand, the accuracy when classifying category 4 (deer) images is only about 59.6%.</li>
</ul>
<p>In this section, we experimented with three situations:</p>
<ul>
<li>The use of the <kbd>adam</kbd> optimizer improved the results a little bit and provided test data accuracy of about 77.2%.</li>
<li>In the second experiment, hyperparameter tuning provided the best results for the number of dense units at 512, a dropout rate at 0.1, and a batch size at 30. This combination of parameters helped us obtain a test data accuracy of about 79.8%.</li>
<li>The third experiment, where we used the VGG16 pretrained network, also provided decent results. However, it provided test data accuracy of slightly lower than 75.7%.</li>
</ul>
<p>Another approach when working with smaller datasets is to use data augmentation. In this approach, the existing images are modified (by flipping, rotation, shifting, and so on) to create new samples. Since images in image datasets aren't always centered, such artificially created new samples help us to learn about useful features that, in turn, improve image classification performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we illustrated the use of pretrained deep neural networks for developing image classification models. Such pretrained networks, which are trained using over 1 million images, capture reusable features that can be applied to similar but new data. This aspect becomes valuable when developing image classification models with relatively smaller datasets. In addition, they provide savings in terms of the use of computational resources and time. We started by making use of the RESNET50 pretrained network to identify an image of a Norwich terrier dog. Subsequently, we made use of 2,000 images from the CIFAR10 dataset to illustrate the usefulness of applying pretrained networks to a relatively smaller dataset. The initial convolutional neural networks model that we built from scratch suffered from overfitting and did not yield useful results.</p>
<p>Next, we used the pretrained RESNET50 network and customized it to suit our needs by adding two dense layers on top of the pretrained network. We obtained decent results, with a test data accuracy of about 76.8%. Although pretrained models can provide faster results that require fewer epochs, we need to explore improvements that we can make to the model's performance with the help of some experimentation. In an effort to explore better results, we experimented with the <kbd>adam</kbd> optimizer, which yielded test data accuracy of about 77.2%. We also carried out hyperparameter tuning, which yielded the best levels in terms of the number of units in the dense layer, which was 512, the dropout rate in the dropout layer, which was 0.1, and the batch size at the time of fitting the model, which was 30. The image classification accuracy with this combination yielded a test data accuracy of about 79.8%. Finally, we experimented with the pretrained VGG16 network, where we obtained test data accuracy of about 75.6%. These experiments illustrated how we can explore and improve model performance.</p>
<p>In the next chapter, we will explore another interesting and popular class of deep networks, called <strong>generative adversarial networks</strong> (<strong>GANs</strong>). We will make use of GANs to create new images.</p>


            </article>

            
        </section>
    </body></html>