["```py\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimg = cv2.imread('/content/cat.JPG')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nimg = cv2.resize(img, (299,299))\nplt.imshow(img)\nplt.axis('off')\n```", "```py\noriginal_image = cv2.resize(img,(299,299)).astype(float)\noriginal_image /= 255.\noriginal_image -= 0.5\noriginal_image *= 2.\noriginal_image = np.expand_dims(original_image, axis=0)\n```", "```py\nimport numpy as np\nfrom keras.preprocessing import image\nfrom keras.applications import inception_v3\nmodel = inception_v3.InceptionV3()\n```", "```py\npredictions = model.predict(original_image)\npredicted_classes = inception_v3.decode_predictions(predictions, top=1)\nimagenet_id, name, confidence = predicted_classes[0][0]\nprint(\"This is a {} with {:.4}% confidence\".format(name, confidence * 100))\n```", "```py\n\" This is a Persian_cat with 95.45% confidence\"\n```", "```py\nmodel = inception_v3.InceptionV3()\nmodel_input_layer = model.layers[0].input\nmodel_output_layer = model.layers[-1].output\n```", "```py\nmax_change_above = np.copy(original_image) + 0.01\nmax_change_below = np.copy(original_image) - 0.01\nhacked_image = np.copy(original_image)\n```", "```py\nlearning_rate = 0.1\nobject_type_to_fake = 386\ncost_function = model_output_layer[0, object_type_to_fake]\n```", "```py\ngradient_function = K.gradients(cost_function, model_input_layer)[0]\n```", "```py\ngrab_cost_and_gradients_from_model = K.function([model_input_layer], [cost_function, gradient_function])\ncost = 0.0\n```", "```py\nwhile cost < 0.80:\n    cost, gradients = grab_cost_and_gradients_from_model([hacked_image, 0])\n    hacked_image += gradients * learning_rate\n    hacked_image = np.clip(hacked_image, max_change_below, max_change_above)\n    print(\"Model's predicted likelihood that the image is an African elephant: \n{:.8}%\".format(cost * 100))\n```", "```py\nepochs = range(1, len(prob_elephant) + 1)\nplt.plot(epochs, prob_elephant, 'b')\nplt.title('Probability of African elephant class')\nplt.xlabel('Epochs')\nplt.ylabel('Probability')\nplt.grid('off')\n```", "```py\nmodel.predict(hacked_image)[0][386]\n```", "```py\nhacked_image = hacked_image/2\nhacked_image = hacked_image + 0.5\nhacked_image = hacked_image*255\nhacked_image = np.clip(hacked_image, 0, 255).astype('uint8')\n\nplt.subplot(131)\nplt.imshow(img)\nplt.title('Original image')\nplt.axis('off')\nplt.subplot(132)\nplt.imshow(hacked_image[0,:,:,:])\nplt.title('Hacked image')\nplt.axis('off')\nplt.subplot(133)\nplt.imshow(img - hacked_image[0,:,:,:])\nplt.title('Difference')\nplt.axis('off')\n```", "```py\nimport keras.backend as K\nimport multiprocessing\nimport tensorflow as tf\nimport warnings\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom scipy.optimize import minimize\nfrom skimage import img_as_float, img_as_ubyte\nfrom skimage.io import imread, imsave\nfrom skimage.transform import pyramid_gaussian, rescale\nimport scipy\nfrom keras.preprocessing import image\nfrom keras.applications.vgg19 import preprocess_input\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```", "```py\ndef preprocess_image(image_path):\n     img = image.load_img(image_path, target_size=(img_nrows, img_ncols))\n     img = image.img_to_array(img)\n     img = np.expand_dims(img, axis=0)\n     img[:, :, :, 0] -= 103.939\n     img[:, :, :, 1] -= 116.779\n     img[:, :, :, 2] -= 123.68\n     img = img[:, :, :, ::-1]/255\n     return img\n```", "```py\ndef deprocess_image(x):\n     x = x[:,:,:,::-1]*255\n     x[:, :, :, 0] += 103.939\n     x[:, :, :, 1] += 116.779\n     x[:, :, :, 2] += 123.68\n     x = np.clip(x, 0, 255).astype('uint8')\n     return x\n```", "```py\nimg = preprocess_image('/content/cat.png')\n```", "```py\nlayer_contributions = {\n    'block2_pool':0.3,\n    'block5_pool': 1.5}\n```", "```py\nlayer_dict = dict([(layer.name, layer) for layer in model.layers])\nloss = K.variable(0.)\n```", "```py\nfor layer_name in layer_contributions:\n     coeff = layer_contributions[layer_name]\n     activation = layer_dict[layer_name].output\n     scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n     loss += coeff * K.sum(K.square(activation)) / scaling\n     print(loss)\n```", "```py\ndream = model.input\ngrads = K.gradients(loss, dream)[0]\n```", "```py\ngrads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n```", "```py\noutputs = [loss, grads]\nfetch_loss_and_grads = K.function([dream], outputs)\n```", "```py\ndef eval_loss_and_grads(img):\n      outs = fetch_loss_and_grads([img])\n      loss_value = outs[0]\n      grad_values = outs[1]\n      return loss_value, grad_values\n```", "```py\nfor i in range(100):\n      learning_rate=0.01\n      max_loss=20\n```", "```py\n     loss_value, grad_values = eval_loss_and_grads(img)\n     if max_loss is not None and loss_value > max_loss:\n         print(loss_value)\n         break\n     print('...Loss value at', i, ':', loss_value)\n```", "```py\n    img += learning_rate * grad_values\n    img2 = deprocess_image(img.copy())\n    plt.imshow(img2[0,:,:,:])\n    plt.axis('off')\n    plt.show()\n```", "```py\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\nimport numpy as np\nimport time\nfrom keras.applications import vgg19\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras import backend as K\nimport tensorflow as tf\nimport keras\n\nstyle_img = cv2.imread('/content/style image.png')\nstyle_img = cv2.cvtColor(style_img, cv2.COLOR_BGR2RGB)\nstyle_img = cv2.resize(style_img,(224,224))\n\nbase_img = cv2.imread('/content/cat.png')\nbase_img = cv2.cvtColor(base_img, cv2.COLOR_BGR2RGB)\nbase_img = cv2.resize(base_img,(224,224))\n```", "```py\nfrom keras.applications import vgg19\nmodel = vgg19.VGG19(include_top=False, weights='imagenet')\n```", "```py\nbase_img = base_img.reshape(1,224,224,3)/255\nfrom keras import backend as K\nget_3rd_layer_output = K.function([model.layers[0].input],\n[model.get_layer('block3_conv4').output])\nlayer_output_base = get_3rd_layer_output([base_img])[0]\n```", "```py\nlayer_contributions_content = {'block3_conv4': 0.1}\n\nlayer_contributions_style =    { 'block1_pool':1,\n                                 'block2_pool':1,\n                                 'block3_conv4':1}\n```", "```py\ndef gram_matrix(x):\n    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n    gram = K.dot(features, K.transpose(features))\n    return gram\n```", "```py\ndef style_loss(style, combination):\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    channels = 3\n    size = img_nrows * img_ncols\n    return K.sum(K.square(S - C)) / (4\\. * (pow(channels,2)) * (pow(size,2)))\n```", "```py\nlayer_dict = dict([(layer.name, layer) for layer in model.layers])\nloss = K.variable(0.)\nfor layer_name in layer_contributions_content:\n      coeff = layer_contributions_content[layer_name]\n      activation = layer_dict[layer_name].output\n      scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n      loss += coeff * K.sum(K.square(activation - layer_output_base)) / scaling\n```", "```py\nfor layer_name in layer_contributions_style:\n    coeff = layer_contributions_style[layer_name]\n    activation = layer_dict[layer_name].output\n    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n    style_layer_output = K.function([model.layers[0].input],\nmodel.get_layer(layer_name).output])\n    layer_output_style = style_layer_output([style_img.reshape(1,224,224,3)/255])[0][0]\n    loss += style_loss(layer_output_style, activation[0])\n```", "```py\ndream = model.input\ngrads = K.gradients(loss, dream)[0]\ngrads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\noutputs = [loss, grads]\nfetch_loss_and_grads = K.function([dream], outputs)\n\ndef eval_loss_and_grads(img):\n      outs = fetch_loss_and_grads([img])\n      loss_value = outs[0]\n      grad_values = outs[1]\n      return loss_value, grad_values\n```", "```py\nfor i in range(2000):\n      step=0.001\n      loss_value, grad_values = eval_loss_and_grads(img)\n      print('...Loss value at', i, ':', loss_value)\n      img -= step * grad_values\n      if(i%100 ==0):\n            img2 = img.copy().reshape(224,224,3)\n            img2 = np.clip(img2*255, 0, 255).astype('uint8')\n            plt.imshow(img2)\n            plt.axis('off')\n            plt.show()\n```", "```py\nimport numpy as np\nfrom keras.datasets import mnist\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout\nfrom keras.layers import BatchNormalization\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.switch_backend('agg')\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Reshape\nfrom keras.layers.core import Activation\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import UpSampling2D\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers.core import Flatten\nfrom keras.optimizers import SGD\nfrom keras.datasets import mnist\nimport numpy as np\nfrom PIL import Image\nimport argparse\nimport math\n```", "```py\nshape = (28, 28, 1)\nepochs = 400\nbatch = 32\nsave_interval = 100\n```", "```py\ndef generator():\n    model = Sequential()\n    model.add(Dense(256, input_shape=(100,)))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(1024))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(28 * 28 * 1, activation='tanh'))\n    model.add(Reshape(shape))\n    return model\n```", "```py\ndef discriminator():\n     model = Sequential()\n     model.add(Flatten(input_shape=shape))\n     model.add(Dense((28 * 28 * 1), input_shape=shape))\n     model.add(LeakyReLU(alpha=0.2))\n     model.add(Dense(int((28 * 28 * 1) / 2)))\n     model.add(LeakyReLU(alpha=0.2))\n     model.add(Dense(1, activation='sigmoid'))\n     return model\n```", "```py\nGenerator = generator()\nGenerator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5, decay=8e-8))\n```", "```py\nDiscriminator = discriminator()\nDiscriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5, decay=8e-8),metrics=['accuracy'])\n```", "```py\ndef stacked_generator_discriminator(D, G):\n    D.trainable = False\n    model = Sequential()\n    model.add(G)\n    model.add(D)\n    return model\n\nstacked_generator_discriminator = stacked_generator_discriminator(Discriminator, Generator)\nstacked_generator_discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5, decay=8e-8))\n```", "```py\ndef plot_images(samples=16, step=0):\n    noise = np.random.normal(0, 1, (samples, 100))\n    images = Generator.predict(noise)\n    plt.figure(figsize=(10, 10))\n    for i in range(images.shape[0]):\n        plt.subplot(4, 4, i + 1)\n        image = images[i, :, :, :]\n        image = np.reshape(image, [28, 28])\n        plt.imshow(image, cmap='gray')\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n```", "```py\n(X_train, _), (_, _) = mnist.load_data()\nX_train = (X_train.astype(np.float32) - 127.5) / 127.5\nX_train = np.expand_dims(X_train, axis=3)\n```", "```py\nfor cnt in range(4000):\n      random_index = np.random.randint(0, len(X_train) - batch / 2)\n      legit_images = X_train[random_index: random_index + batch // 2].reshape(batch // 2, 28, 28, 1)\n      gen_noise = np.random.normal(-1, 1, (batch // 2, 100))/2\n      synthetic_images = Generator.predict(gen_noise)\n```", "```py\nx_combined_batch = np.concatenate((legit_images, synthetic_images))\ny_combined_batch = np.concatenate((np.ones((batch // 2, 1)), np.zeros((batch // 2, 1))))\nd_loss = Discriminator.train_on_batch(x_combined_batch, y_combined_batch)\n```", "```py\nnoise = np.random.normal(-1, 1, (batch, 100))/2\ny_mislabled = np.ones((batch, 1))\n```", "```py\ng_loss = stacked_generator_discriminator.train_on_batch(noise, y_mislabled)\n```", "```py\nlogger.info('epoch: {}, [Discriminator: {}], [Generator: {}]'.format(cnt, d_loss[0], g_loss))\n    if cnt % 100 == 0:\n          plot_images(step=cnt)\n```", "```py\ndef generator():\n    model = Sequential()\n    model.add(Dense(input_dim=100, output_dim=1024))\n    model.add(Activation('tanh'))\n    model.add(Dense(128*7*7))\n    model.add(BatchNormalization())\n    model.add(Activation('tanh'))\n    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,)))\n    model.add(UpSampling2D(size=(2, 2)))\n    model.add(Conv2D(64, (5, 5), padding='same'))\n    model.add(Activation('tanh'))\n    model.add(UpSampling2D(size=(2, 2)))\n    model.add(Conv2D(1, (5, 5), padding='same'))\n    model.add(Activation('tanh'))\n    return model\n\ndef discriminator():\n    model = Sequential()\n    model.add(Conv2D(64, (5, 5),padding='same',input_shape=(28, 28, 1)))\n    model.add(Activation('tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(128, (5, 5)))\n    model.add(Activation('tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(1024))\n    model.add(Activation('tanh'))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    return model\n```", "```py\ndef generator():\n    model = Sequential()\n    model.add(Dense(input_dim=100, output_dim=1024))\n    model.add(Activation('tanh'))\n    model.add(Dense(128*7*7))\n    model.add(BatchNormalization())\n    model.add(Activation('tanh'))\n    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,)))\n    model.add(UpSampling2D(size=(2, 2)))\n    model.add(Conv2D(64, (5, 5), padding='same'))\n    model.add(Activation('tanh'))\n    model.add(UpSampling2D(size=(2, 2)))\n    model.add(Conv2D(1, (5, 5), padding='same'))\n    model.add(Activation('tanh'))\n    return model\n```", "```py\ndef discriminator():\n    model = Sequential()\n    model.add(Conv2D(64, (5, 5),padding='same',input_shape=(28, 28, 1)))\n    model.add(Activation('tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(128, (5, 5)))\n    model.add(Activation('tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(1024))\n    model.add(Activation('tanh'))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    return model\n```", "```py\ndef stacked_generator_discriminator(D, G):\n    D.trainable = False\n    model = Sequential()\n    model.add(G)\n    model.add(D)\n    return model\n```", "```py\ndef plot_images(samples=16, step=0):\n    noise = np.random.normal(0, 1, (samples, 100))\n    images = deprocess(Generator.predict(noise))\n    plt.figure(figsize=(5, 5))\n    for i in range(images.shape[0]):\n        plt.subplot(4, 4, i + 1)\n        image = images[i, :, :, :]\n        image = np.reshape(image, [56, 56,3])\n        plt.imshow(image, cmap='gray')\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n```", "```py\ndef preprocess(x):\n    return (x/255)*2-1\n\ndef deprocess(x):\n    return np.uint8((x+1)/2*255)\n```", "```py\nfrom skimage import io\nimport os\nimport glob\nroot_dir = '/content/lfwcrop_color/'\nall_img_paths = glob.glob(os.path.join(root_dir, '*/*.ppm'))\n```", "```py\nimport numpy as np\nX_train = []\nfor i in range(len(all_img_paths)):\n  img = cv2.imread(all_img_paths[i])\n  X_train.append(preprocess(img))\nlen(X_train)\nX_train = np.array(X_train)\n```", "```py\nGenerator = generator()\nGenerator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5, decay=8e-8))\n\nDiscriminator = discriminator()\nDiscriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5, decay=8e-8),metrics=['accuracy'])\n\nstacked_generator_discriminator = stacked_generator_discriminator(Discriminator, Generator)\nstacked_generator_discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5, decay=8e-8))\n```", "```py\n%matplotlib inline\n$pip install logger\nfrom logger import logger\nfor cnt in range(10000):\n      random_index = np.random.randint(0, len(X_train) - batch / 2)\n      legit_images = X_train[random_index: random_index + batch // 2].reshape(batch // 2, 56, 56, 3)\n      gen_noise = np.random.normal(0, 1, (batch // 2, 100))\n      syntetic_images = Generator.predict(gen_noise)\n      x_combined_batch = np.concatenate((legit_images, syntetic_images))\n      y_combined_batch = np.concatenate((np.ones((batch // 2, 1)), np.zeros((batch // 2, 1))))\n      d_loss = Discriminator.train_on_batch(x_combined_batch, y_combined_batch)\n      noise = np.random.normal(0, 1, (batch*2, 100))\n      y_mislabled = np.ones((batch*2, 1))\n      g_loss = stacked_generator_discriminator.train_on_batch(noise, y_mislabled)\n      logger.info('epoch: {}, [Discriminator: {}], [Generator: {}]'.format(cnt, d_loss[0], g_loss))\n      if cnt % 100 == 0:\n          plot_images(step=cnt)\n```", "```py\ngen_noise = np.random.normal(0, 1, (1, 100))\nsyntetic_images = Generator.predict(gen_noise)\nplt.imshow(deprocess(syntetic_images)[0])\nplt.axis('off')\n```", "```py\ngen_noise2 = np.random.normal(0, 1, (1, 100))\nsyntetic_images = Generator.predict(gen_noise2)\nplt.imshow(deprocess(syntetic_images)[0])\nplt.axis('off')\nplt.show() \n```", "```py\nplt.figure(figsize=(10, 8))\nfor i in range(10):\n  gen_noise3 = gen_noise + (gen_noise2 - gen_noise)*(i+1)/10\n  syntetic_images = Generator.predict(gen_noise3)\n  plt.subplot(1, 10, i+1)\n  plt.imshow(deprocess(syntetic_images)[0])\n  plt.axis('off')\n```", "```py\ngen_noise = np.random.normal(0, 1, (1, 100))\ngen_noise2 = np.random.normal(0, 1, (1, 100))\ngen_noise3 = np.random.normal(0, 1, (1, 100))\nsyntetic_images = Generator.predict(gen_noise4)\nplt.imshow(deprocess(syntetic_images)[0])\nplt.axis('off')\nplt.show()\n```", "```py\nplt.subplot(131)\nsyntetic_images = Generator.predict(gen_noise)\nplt.imshow(deprocess(syntetic_images)[0])\nplt.axis('off')\nplt.title('Image 1')\nplt.subplot(132)\nsyntetic_images = Generator.predict(gen_noise2)\nplt.imshow(deprocess(syntetic_images)[0])\nplt.axis('off')\nplt.title('Image 2')\nplt.subplot(133)\nsyntetic_images = Generator.predict(gen_noise3)\nplt.imshow(deprocess(syntetic_images)[0])\nplt.axis('off')\nplt.title('Image 3')\n```", "```py\ngen_noise4 = gen_noise + gen_noise2 - gen_noise3\nsyntetic_images = Generator.predict(gen_noise4)\nplt.imshow(deprocess(syntetic_images)[0])\nplt.axis('off')\nplt.show()  \n```"]