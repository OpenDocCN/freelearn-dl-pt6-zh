- en: '*Chapter 3*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fundamentals of Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Classify different areas of natural language processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze basic natural language processing libraries in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict the topics in a set of texts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop a simple language model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter covers different fundamentals and areas of natural language processing,
    along with its libraries in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Natural Language Processing** (**NLP**) is an area of **Artificial Intelligence**
    (**AI**) with the goal of enabling computers to understand and manipulate human
    language in order to perform useful tasks. Within this area, there are two sections:
    **Natural Language Understanding** (**NLU**) and **Natural Language Generation**
    (**NLG**).'
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, AI has changed the way machines interact with humans. AI helps
    people solve complex equations by performing tasks such as recommending a movie
    according to your tastes (recommender systems). Thanks to the high performance
    of GPUs and the huge amount of data available, it's possible to create intelligent
    systems that are capable of learning and behaving like humans.
  prefs: []
  type: TYPE_NORMAL
- en: There are many libraries that aim to help with the creation of these systems.
    In this chapter, we will review the most famous Python libraries to extract and
    clean information from raw text. You may consider this task complex, but a complete
    understanding and interpretation of the language is a difficult task in itself.
    For example, the sentence "Cristiano Ronaldo scores three goals" would be hard
    for a machine to understand because it would not know who Cristiano Ronaldo is
    or what is meant by the number of goals.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular topics in NLP is **Question Answering** (**QA**). This
    discipline also consists of **Information Retrieva**l (**IR**). These systems
    construct answers by querying a database for knowledge or information, but they
    are capable of extracting answers from a collection of natural language documents.
    That is how a search engine such as Google works.
  prefs: []
  type: TYPE_NORMAL
- en: In the industry today, NLP is becoming more and more popular. The latest NLP
    trends are online advertisement matching, sentiment analysis, automated translation,
    and chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Conversational agents, popularly known as chatbots, are the next challenge for
    NLP. They can hold real conversation and many companies use them to get feedback
    about their products or to create a new advertising campaign, by analyzing the
    behavior and opinions of clients through the chatbot. Virtual assistants are a
    great example of NLP and they have already been introduced to the market. The
    most famous are Siri, Amazon's Alexa, and Google Home. In this book, we will create
    a chatbot to control a virtual robot that is able to understand what we want the
    robot to do.
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned before, NLP is an AI field that takes care of understanding and
    processing human language. NLP is located at the intersection between AI, computer
    science, and linguistics. The main aim of this area is to make computers understand
    statements or words written in human languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Representation of NLP within AI, linguistics, and computer science](img/C13550_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Representation of NLP within AI, linguistics, and computer science'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Linguistic science focuses on the study of human language, trying to characterize
    and explain the different approaches of language.
  prefs: []
  type: TYPE_NORMAL
- en: A language can be defined as a set of rules and a set of symbols. Symbols are
    combined and used to broadcast information and are structured by rules. Human
    language is special. We cannot simply picture it as naturally formed symbols and
    rules; depending on the context, the meaning of words can change.
  prefs: []
  type: TYPE_NORMAL
- en: NLP is becoming more popular and can solve many difficult problems. The amount
    of text data available is very large, and it is impossible for a human to process
    all that data. In Wikipedia, the average number of new articles per day is 547,
    and in total, there are more than 5,000,000 articles. As you can imagine, a human
    cannot read all that information.
  prefs: []
  type: TYPE_NORMAL
- en: There are three challenges faced by NLP. The first challenge is collecting all
    the data, the second is classifying it, and the final one is extracting the relevant
    information.
  prefs: []
  type: TYPE_NORMAL
- en: NLP solves many tedious tasks, such as spam detection in emails, **part-of-speech**
    (**POS**) tagging, and named entity recognition. With deep learning, NLP can also
    solve voice-to-text problems. Although NLP shows a lot of power, there are some
    cases such as working without having a good solution from the dialog between a
    human and a machine, QA systems summarization and machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: Parts of NLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned before, NLP can be divided into two groups: NLU and NLG.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural Language Understanding**'
  prefs: []
  type: TYPE_NORMAL
- en: This section of NLP relates to the understanding and analysis of human language.
    It focusses on the comprehension of text data, and processing it to extract relevant
    information. NLU provides direct human-computer interaction and performs tasks
    related to the comprehension of language.
  prefs: []
  type: TYPE_NORMAL
- en: NLU covers the hardest of AI challenges, and that is the interpretation of text.
    The main challenge of NLU is understanding dialog.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NLP uses a set of methods for generating, processing, and understanding language.
    NLU uses functions to understand the meaning of a text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, a conversation was represented as a tree, but this approach cannot
    cover many dialog cases. To cover more cases, more trees would be required, one
    for each context of the conversation, leading to the repeating of many sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Representation of a dialogue using trees](img/C13550_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Representation of a dialogue using trees'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This approach is outdated and inefficient because is based on fixed rules;
    it''s essentially an if-else structure. But now, NLU has contributed another approach.
    A conversation can be represented as a Venn diagram where each set is a context
    of the conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: Representation of a conversation using a Venn diagram](img/C13550_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Representation of a conversation using a Venn diagram'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see in the previous figures, the NLU approach improves the structure
    of understanding a conversation, because it is not a fixed structure that contains
    if-else conditions. The main goal of NLU is to interpret the meaning of human
    language and deal with the contexts of a conversation, solving ambiguities and
    managing data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural Language Generation**'
  prefs: []
  type: TYPE_NORMAL
- en: NLG is the process of producing phrases, sentences, and paragraphs with meaning
    and structure. It is an area of NLP that does not deal with understanding text.
  prefs: []
  type: TYPE_NORMAL
- en: To generate natural language, NLG methods need relevant data.
  prefs: []
  type: TYPE_NORMAL
- en: 'NLG has three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generator**: Responsible for including the text within an intent to have
    it related with the context of the situation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Components and levels of representations**: Gives structure to the generated
    text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application**: Saves relevant data from the conversation to follow a logical
    thread'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generated text must be in a human-readable format. The advantages of NLG are
    that you can make your data accessible and you can create summaries of reports
    rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: Levels of NLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Human language has different levels of representation. Each representation level
    is more complex than the previous level. As we ascend through the levels, it gets
    more difficult to understand the language.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two first levels depend on the data type (audio or text), in which we have
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Phonological analysis**: If the data is speech, first, we need to analyze
    the audio to have sentences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OCR/tokenization**: If we have text, we need to recognize the characters
    and form words using computer vision (OCR). If not, we will need to tokenize the
    text (that is, split the sentence into units of text).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The OCR process is the identification of characters in an image. Once it generates
    words, they are processed as raw text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Morphological analysis**: Focused on the words of a sentence and analyzing
    its morphemes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Syntactic analysis**: This level focuses on the grammatical structure of
    a sentence. That means understanding different parts of a sentence, such as the
    subject or the predicate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic representation**: A program does not understand a single word; it
    can know the meaning of a word by knowing how the word is used in a sentence.
    For example, "cat" and "dog" could mean the same for an algorithm because they
    can be used in the same way. Understanding sentences in this way is called word-level
    meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discourse processing**: Analyzing and identifying connected sentences in
    a text and their relationships. By doing this, an algorithm could understand what
    the topic of the text is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP shows great potential in today's industry, but there are some exceptions.
    Using deep learning concepts, we can work with some of these exceptions to get
    better results. Some of these problems will be reviewed in *Chapter 4*, *Neural
    Networks with NLP*. The advantage of text processing techniques and the improvement
    of recurrent neural networks are the reasons why NLP is becoming increasingly
    important.
  prefs: []
  type: TYPE_NORMAL
- en: NLP in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python has become very popular in recent years, by combining the power of general-purpose
    programming languages with the use of specific domain languages, such as MATLAB
    and R (designed for mathematics and statistics). It has different libraries for
    data loading, visualization, NLP, image processing, statistics, and more. Python
    has the most powerful libraries for text processing and machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Toolkit (NLTK)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NLTK is the most common kit of tools for working with human language data in
    Python. It includes a set of libraries and programs for processing natural language
    and statistics. NLTK is commonly used as a learning tool and for carrying out
    research.
  prefs: []
  type: TYPE_NORMAL
- en: This library provides interfaces and methods for over 50 corpora and lexical
    resources. NLTK is capable of classifying text and performing other functions,
    such as tokenization, stemming (extracting the stem of a word), tagging (identifying
    the tag of a word, such as person, city…), and parsing (syntax analysis).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10: Introduction to NLTK'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will review the most basic concepts about the NLTK library.
    As we said before, this library is one of the most widely used tools for NLP.
    It can be used to analyze and study text, disregarding useless information. These
    techniques can be applied to any text data, for example, to extract the most important
    keywords from a set of tweets or to analyze an article in a newspaper:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All the exercises in this chapter will be executed in Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: Open up your Google Colab interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder for the book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, we are going to process a sentence with basic methods of the NLTK library.
    First of all, let''s import the necessary methods (`stopwords`, `word_tokenize`,
    and `sent_tokenize`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we create a sentence and apply the methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/C13550_03_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.4: Sentence divided into a sub-sentence'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Fig 3.5: Tokenizing a sentence into words](img/C13550_03_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.5: Tokenizing a sentence into words'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '`Sent_tokenize` returns a list of different sentences. One of the disadvantages
    of NLTK is that `sent_tokenize` does not analyze the semantic structure of the
    whole text; it just splits the text by the dots.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With the sentence tokenized sentence by words, let''s subtract the stop words.
    The stop words are a set of words without relevant information about the text.
    Before using `stopwords`, we will need to download it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we set the language of our `stopwords` as English:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.6: Stopwords set as English](img/C13550_03_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.6: Stopwords set as English'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Process the sentence, deleting `stopwords`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.7: Sentence without stop words](img/C13550_03_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.7: Sentence without stop words'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can now modify the set of `stopwords` and check the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.8: Setting stop words](img/C13550_03_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.8: Setting stop words'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Stemmers remove morphological affixes from words. Let''s define a stemmer and
    process our sentence. `Porter stemmer` is an algorithm for performing this task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.9: Setting stop words](img/C13550_03_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.9: Setting stop words'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Finally, let''s classify each word by its type. To do this, we will use a POS
    tagger:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.10: POS tagger](img/C13550_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: POS tagger'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The averaged perceptron tagger is an algorithm trained to predict the category
    of a word.
  prefs: []
  type: TYPE_NORMAL
- en: As you may have noticed in this exercise, NLTK can easily process a sentence.
    Also, it can analyze a huge set of text documents without any problem. It supports
    many languages and the tokenization process is faster than that for similar libraries,
    and it has many methods for each NLP problem.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: spaCy is another library for NLP in Python. It does look similar to NLTK, but
    you will see some differences in the way it works.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy was developed by Matt Honnibal and is designed for data scientists to
    clean and normalize text easily. It's the quickest library in terms of preparing
    text data for a machine learning model. It includes built-in word vectors and
    some methods for comparing the similarity between two or more texts (these methods
    are trained with neural networks).
  prefs: []
  type: TYPE_NORMAL
- en: Its API is easy to use and more intuitive than NLTK. Often, in NLP, spaCy is
    compared to NumPy. It provides methods and functions for performing tokenization,
    lemmatization, POS tagging, NER, dependency parsing, sentence and document similarity,
    text classification, and more.
  prefs: []
  type: TYPE_NORMAL
- en: As well as having linguistic features, it also has statistical models. This
    means you can predict some linguistic annotations, such as whether a word is a
    verb or a noun. Depending on the language you want to make predictions in, you
    will need to change a module. Within this section are Word2Vec models, which we
    will discuss in *Chapter 4*, *Neural Networks with NLP.*
  prefs: []
  type: TYPE_NORMAL
- en: spaCy has many advantages, as we said before, but there are some cons too; for
    instance, it supports only 8 languages (NLTK supports 17 languages), the tokenization
    process is slow (and this time-consuming process could be critical on a long corpus),
    and overall, it is not flexible (that is, it just provides API methods without
    the possibility of modifying any parameters).
  prefs: []
  type: TYPE_NORMAL
- en: Before starting with the exercise, let's review the architecture of spaCy. The
    most important data structures of spaCy are the Doc and the Vocab.
  prefs: []
  type: TYPE_NORMAL
- en: The Doc structure is the text you are loading; it is not a string. It is composed
    of a sequence of tokens and their annotations. The Vocab structure is a set of
    lookup tables, but what are lookup tables and why is the structure important?
    Well, a lookup table in computation is an array indexing an operation that replaces
    a runtime. spaCy centralizes information that is available across documents. This
    means that it is more efficient, as this saves memory. Without these structures,
    the computational speed of spaCy would be slower.
  prefs: []
  type: TYPE_NORMAL
- en: However, the structure of Doc is different to Vocab because Doc is a container
    of data. A Doc object owns the data and is composed of a sequence of tokens or
    spans. There are also a few lexemes, which are related to the Vocab structure
    because they do not have context (unlike the token container).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A lexeme is a unit of lexical meaning without having inflectional endings. The
    area of study for this is morphological analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The figure 3.11 shows us the spaCy architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11: spaCy architecture](img/C13550_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: spaCy architecture'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Depending on the language model you are loading, you will have a different pipeline
    and a different Vocab.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11: Introduction to spaCy'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will do the same transformations that we performed in
    *Exercise 10*, *Introduction to NLTK*, and to the same sentence as in that exercise
    but with the spaCy API. This exercise will help you to understand and learn about
    the differences between these libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: Open up your Google Colab interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder for the book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, import the package to use all its features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we are going to initialize our `nlp` object. This object is a part of the
    spaCy methods. By executing this line of code, we are loading the model inside
    the parenthesis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take the same sentence as in *Exercise 10*, *Introduction to NLTK,*
    and create the Doc container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, print `doc1`, its format, the 5th and 11th token, and a span between the
    5th and the 11th token. You will see this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12: Output of a spaCy document](img/C13550_03_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.12: Output of a spaCy document'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As we saw in Figure 3.5, documents are composed of tokens and spans. First,
    we are going to see the spans of `doc1`, and then its tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Print the spans:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.13: Printing the spans of doc1](img/C13550_03_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.13: Printing the spans of doc1'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the tokens:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13550_03_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.14: Printing the tokens of doc1'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Once we have the document divided into tokens, the stop words can be removed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we need to import them:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.15: 10 stop words in spaCy](img/C13550_03_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.15: 10 stop words in spaCy'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'But the token container has the `is_stop` attribute:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.16: The is_stop attribute of tokens'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13550_03_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.16: The is_stop attribute of tokens'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To add new stop words, we must modify the `vocab` container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output here would be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To perform speech tagging, we initialize the token container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.17: The .pos_ attribute of tokens](img/C13550_03_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.17: The .pos_ attribute of tokens'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The document container has the `ents` attribute, with the entity of the tokens.
    To have more entities in our document, let''s declare a new one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.18: The .label_ attribute of tokens](img/C13550_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.18: The .label_ attribute of tokens'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As you can see in this exercise, spaCy is much easier to use than NLTK, but
    NLTK provides more methods to perform different operations on text. spaCy is perfect
    for production. That means, in the least amount of time, you can perform basic
    processes on text.
  prefs: []
  type: TYPE_NORMAL
- en: The exercise has ended! You can now pre-process a text using NLTK or spaCy.
    Depending on the task you want to perform, you will be able to choose one of these
    libraries to clean your data.
  prefs: []
  type: TYPE_NORMAL
- en: Topic Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Within NLU, which is a part of NLP, one of the many tasks that can be performed
    is extracting the meaning of a sentence, a paragraph, or a whole document. One
    approach to understanding a document is through its topics. For example, if a
    set of documents is from a newspaper, the topics might be politics or sports.
    With topic modeling techniques, we can obtain a bunch of words representing various
    topics. Depending on your set of documents, you will then have different topics
    represented by different words. The goal of these techniques is to know the different
    types of documents in your corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Term Frequency – Inverse Document Frequency (TF-IDF)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**TF-IDF** is a commonly used NLP model for extracting the most important words
    from a document. To perform this classification, the algorithm will assign a weight
    to each word. The idea of this method is to ignore words without relevance to
    the meaning of a global concept, (which means the overall topic of a text), so
    those terms will be down-weighted (which means that they will be ignored). Down-weighing
    them will allow us to find the keywords of that document (the words with the greatest
    weights).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the algorithm to find the weight of a term in a document is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19: TF-IDF formula](img/C13550_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.19: TF-IDF formula'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Wi,j*: Weight of the term, i, in the document, j'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*tf,j*: Number of occurrences of i in j'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*df,j*: Number of documents containing i'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N*: Total number of documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result is the number of times a term appears in that document, multiplied
    by the log of the total number of documents, divided by the number of documents
    that contain the term.
  prefs: []
  type: TYPE_NORMAL
- en: Latent Semantic Analysis (LSA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LSA is one of the foundational techniques of topic modeling. It analyzes the
    relationship between a set of documents and their terms, and produces a set of
    concepts related to them.
  prefs: []
  type: TYPE_NORMAL
- en: LSA is a step ahead when compared to TF-IDF. In a large set of documents, the
    TF-IDF matrix has very noisy information and many redundant dimensions, so the
    LSA algorithm performs dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'This reduction is performed with Singular Value Decomposition (SVD). SVD factorizes
    a matrix, M, into the product of three separate matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20: Singular Value Decomposition](img/C13550_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.20: Singular Value Decomposition'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*A*: This is the input data matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*m*: This is the number of documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n*: This is the number of terms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*U*: Left singular vectors. Our document-topic matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*S*: Singular values. Represents the strength of each concept. This is a diagonal
    matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V*: Right singular vectors. Represents terms'' vectors in terms of topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: This method is more efficient on a large set of documents, but there are better
    algorithms to perform this task such as LDA or PLSA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Exercise 12: Topic Modeling in Python'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, TF-IDF and LSA will be coded in Python using a specific library.
    By the end of this exercise, you will be able to perform these techniques to extract
    the weights of a term in a document:'
  prefs: []
  type: TYPE_NORMAL
- en: Open up your Google Colab interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder for the book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To generate the TF-IDF matrix, we could code the formula in Figure 3.19, but
    we are going to use one of the most famous libraries for machine learning algorithms
    in Python, scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The corpus we are going to use for this exercise will be simple, with just
    four sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the `TfidfVectorizer` method, we can convert the collection of documents
    in our corpus to a matrix of TF-IDF features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `get_feature_names()` method shows the extracted features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.21: Feature names of the corpus](img/C13550_03_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.21: Feature names of the corpus'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'X is a sparse matrix. To see its content, we can use the `todense()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.22: TF-IDF matrix of the corpus](img/C13550_03_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.22: TF-IDF matrix of the corpus'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now let''s perform dimensionality reduction with LSA. The `TruncatedSVD` method
    uses SVD to transform the input matrix. In this exercise, we''ll use `n_components=10`.
    From now on, you have to use `n_components=100` (it has better results in larger
    corpuses):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 23: Dimensionality reduction with LSA](img/C13550_03_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 23: Dimensionality reduction with LSA'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '`attribute .components_` shows the weight of each `vectorizer.get_feature_names()`.
    Notice that the LSA matrix has a range of 4x16, we have 4 documents in our corpus
    (concepts), and the vectorizer has 16 features (terms):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.24: The desired TF-IDF matrix output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13550_03_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.24: The desired TF-IDF matrix output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The exercise has ended successfully! This was a preparatory exercise for *Activity
    3*, *Process a Corpus*. Do check the seventh step of the exercise – it will give
    you the key to complete the activity ahead. I encourage you to read the scikit-learn
    documentation and learn how to see the potential of these two methods. Now you
    know how to create the TF-IDF matrix. This matrix could be huge, so to manage
    the data better, the LSA algorithm performs dimensionality reduction on the weight
    of each term in the document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3: Process a Corpus'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will process a really small corpus to clean the data and
    extract the keywords and concepts using LSA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine this scenario: the newspaper vendor in your town has published a competition.
    It consists of predicting the category of an article. This newspaper does not
    have a structural database, which means it has only raw data. They provide a small
    set of documents, and they need to know whether the article is political, scientific,
    or sports-related:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can choose between spaCy and the NLTK library to do the activity. Both solutions
    will be valid if the keywords are related at the end of the LSA algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Load the corpus documents and store them in a list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The corpus documents can be found on GitHub, [https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson03/Activity03/dataset](https://github.com/PacktPublishing/Artificial-Vision-and-Language-Processing-for-Robotics/tree/master/Lesson03/Activity03/dataset)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Pre-process the text with spaCy or NLTK.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the LSA algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Show the first five keywords related to each concept:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Keywords: moon, apollo, earth, space, nasa'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Keywords: yard, touchdown, cowboys, prescott, left'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Keywords: facebook, privacy, tech, consumer, data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The output keywords probably will not be the same as yours. If your keywords
    are not related then check the solution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.25: Output example of the most relevant words in a concept (f1)](img/C13550_03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.25: Output example of the most relevant words in a concept (f1)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity is available on page 306.
  prefs: []
  type: TYPE_NORMAL
- en: Language Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have reviewed the most basic techniques for pre-processing text data.
    Now we are going to dive deep into the structure of natural language – language
    models. We can consider this topic an introduction to machine learning in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A statistical **Language Model** (**LM**) is the probability distribution of
    a sequence of words, which means, to assign a probability to a particular sentence.
    For example, LMs could be used to calculate the probability of an upcoming word
    in a sentence. This involves making some assumptions about the structure of the
    LM and how it will be formed. An LM is never totally correct with its output,
    but using one is often necessary.
  prefs: []
  type: TYPE_NORMAL
- en: LMs are used in many more NLP tasks. For example, in machine translation, it
    is important to know what sentence precedes the next. LMs are also used for speech
    recognition, to avoid ambiguity, for spelling corrections, and for summarization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how an LM is mathematically represented:'
  prefs: []
  type: TYPE_NORMAL
- en: P(W) = P(w1, w2,w3,w4,…wn)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(W)* is our LM and *wi* are the words included in *W*, and as we mentioned
    before, we can use it to compute the probability of an upcoming word in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: P(w5|w1,w2,w3,w4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This (w1, w2, w3, w4) states what the probability of *w5* (the upcoming word)
    could be in a given sequence of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at this example, P (w5|w1, w2, w3, w4), we can assume this:'
  prefs: []
  type: TYPE_NORMAL
- en: P(actual word | previous words)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the number of previous words we are looking at to obtain the probability
    of the actual word, there are different models we can use. So, now we are going
    to introduce some important concepts regarding such models.
  prefs: []
  type: TYPE_NORMAL
- en: The Bigram Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The bigram model is a sequence of two consecutive words. For example, in the
    sentence "My cat is white," there are these bigrams:'
  prefs: []
  type: TYPE_NORMAL
- en: My cat
  prefs: []
  type: TYPE_NORMAL
- en: Cat is
  prefs: []
  type: TYPE_NORMAL
- en: Is white
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, a bigram has this form:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bigram model: P(wi|wi-1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N-gram Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we change the length of the previous word, we obtain the N-gram model. It
    works just like the bigram model but considers more words than the previous set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the previous example of "My cat is white," this is what we can obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trigram**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My cat is
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Cat is white
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**4-gram**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My cat is white
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**N-Gram Problem**'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you could think the n-gram model is more accurate than the bigram
    model because the n-gram model has access to additional "previous knowledge."
    However, n-gram models are limited to a certain extent, because of long-distance
    dependencies. An example would be, "After thinking about it a lot, I bought a
    television," which we compute as:'
  prefs: []
  type: TYPE_NORMAL
- en: P(television| after thinking about it a lot, I bought a)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The sentence "After thinking about it a lot, I bought a television" is probably
    the only sequence of words with this structure in our corpus. If we change the
    word "television" for another word, for example "computer," the sentence "After
    thinking about it a lot, I bought a computer" is also valid, but in our model,
    the following would be the case:'
  prefs: []
  type: TYPE_NORMAL
- en: P(computer| after thinking about it a lot, I bought a) = 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This sentence is valid, but our model is not accurate, so we need to be careful
    with the use of n-gram models.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating Probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Unigram Probability**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The unigram is the simplest case for calculating probabilities. It counts the
    number of times a word appears in a set of documents. Here is the formula for
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26: Unigram probability estimation](img/C13550_03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.27: Unigram probability estimation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*c(wi)* is the number of times'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*wi* appears in the whole corpus. The size of the corpus is just how many tokens
    are in it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bigram Probability**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To estimate bigram probability, we are going to use maximum likelihood estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27: Bigram probability estimation](img/C13550_03_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.27: Bigram probability estimation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To understand this formula better, let's look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine our corpus is composed of these three sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: My name is Charles.
  prefs: []
  type: TYPE_NORMAL
- en: Charles is my name.
  prefs: []
  type: TYPE_NORMAL
- en: My dog plays with the ball.
  prefs: []
  type: TYPE_NORMAL
- en: 'The size of the corpus is 14 words, and now we are going to estimate the probability
    of the sequence "my name":'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28: Example of bigram estimation](img/C13550_03_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.28: Example of bigram estimation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**The Chain Rule**'
  prefs: []
  type: TYPE_NORMAL
- en: Now we know the concepts of bigrams and n-grams, we need to know how we can
    obtain those probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have basic statistics knowledge, you might think the best option is
    to apply the chain rule and join each probability. For example, in the sentence
    "My cat is white," the probability is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: P(my cat is white) = p(white|my cat is) p(is|my cat) p(cat|my) p(my)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It seems to be possible with this sentence, but if we had a much longer sentence,
    long-distance dependency problems would appear and the result of the n-gram model
    could be incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: '**Smoothing**'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have a probabilistic model, and if we want to estimate the parameters
    of our model, we can use the maximum likelihood of estimation.
  prefs: []
  type: TYPE_NORMAL
- en: One of the big problems of LMs is insufficient data. Our data is limited, so
    there will be many unknown events. What does this mean? It means we'll end up
    with an LM that gives a probability of 0 to unseen words.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem, we are going to use a smoothing method. With this smoothing
    method, every probability estimation result will be greater than zero. The method
    we are going to use is add-one smoothing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.29: Add-one smoothing in bigram estimation](img/C13550_03_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.29: Add-one smoothing in bigram estimation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*V* is the number of distinct tokens in our corpus.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are more smoothing methods with better performance; this is the most basic
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Markov Assumption**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Markov assumption is very useful for estimating the probabilities of a long
    sentence. With this method, we can solve the problem of long-distance dependencies.
    Markov assumption simplifies the chain rule to estimate long sequences of words.
    Each estimation only depends on the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.30: Markov assumption](img/C13550_03_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.30: Markov assumption'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can also have a second-order Markov assumption, which depends on two previous
    terms, but we are going to use first-order Markov assumption:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.31: Example of Markov](img/C13550_03_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.31: Example of Markov'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If we apply this to the whole sentence, we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.32: Example of Markov for a whole sentence](img/C13550_03_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.32: Example of Markov for a whole sentence'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Decomposing the sequence of words in the aforementioned way will output the
    probabilities more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 13: Create a Bigram Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we are going to create a simple LM with unigrams and bigrams.
    Also, we will compare the results of creating the LM both without add-one smoothing
    and with it. One application of the n-gram is, for example, in keyboard apps.
    They can predict your next word. That prediction could be done with a bigram model:'
  prefs: []
  type: TYPE_NORMAL
- en: Open up your Google Colab interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder for the book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Declare a small, easy training corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the required libraries and load the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize it with spaCy. To be faster in doing the smoothing and the bigrams,
    we are going to create three lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Tokens`: All tokens of the corpus'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Tokens_doc`: List of lists with the tokens of each corpus'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Distinc_tokens`: All tokens removing duplicates:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s create a first loop to iterate over the sentences in our corpus. The
    `doc` variable will contain a sequence of the sentences'' tokens:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we are going to create a second loop to iterate through the tokens to push
    them into the corresponding list. The `t` variable will be each token of the sentence:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the unigram model and test it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Result = 0.1388888888888889
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add the smoothing and test it with the same word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Result = 0.1111111111111111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The problem with this smoothing method is that every unseen word has the same
    probability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the bigram model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to iterate through all of the tokens in the documents to try to find
    the number of times that `word1` and `word2` appear together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.33: Output showing the times word1 and word2 appear together in
    the document](img/C13550_03_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.33: Output showing the times word1 and word2 appear together in the
    document'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Add the smoothing to the bigram model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.34: Output after adding smoothing to the model](img/C13550_03_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.34: Output after adding smoothing to the model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Congratulations! You have completed the last exercise of this chapter. In the
    next chapter, you will see that this LM approach is a fundamental deep NLP approach.
    You can now take a huge corpus and create your own LM.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Applying the Markov assumption, the final probability will round the 0\. I recommend
    using log() and adding each component. Also, check the precision bits of your
    code (float16 < float32 < float64).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'NLP is becoming more and more important in AI. Industries analyze huge quantities
    of raw text data, which is unstructured. To understand this data, we use many
    libraries to process it. NLP is divided into two groups of methods and functions:
    NLG to generate natural language, and NLU to understand it.'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, it is important to clean text data, since there will be a lot of useless,
    irrelevant information. Once the data is ready to be processed, through a mathematical
    algorithm such as TF-IDF or LSA, a huge set of documents can be understood. Libraries
    such as NLTK and spaCy are useful for doing this task. They provide methods to
    remove the noise in data. A document can be represented as a matrix. First, TF-IDF
    can give a global representation of a document, but when a corpus is big, the
    better option is to perform dimensionality reduction with LSA and SVD. scikit-learn
    provides algorithms for processing documents, but if documents are not pre-processed,
    the result will not be accurate. Finally, the use of language models could be
    necessary, but they need to be formed of a valid training set of documents. If
    the set of documents is good, the language model should be able to generate language.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce **Recurrent Neural Networks** (**RNNs**).
    We will be looking at some advanced models of these RNNs and will accordingly
    be one step ahead in building our robot.
  prefs: []
  type: TYPE_NORMAL
