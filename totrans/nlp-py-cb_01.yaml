- en: Corpus and WordNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing in-built corpora
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download an external corpus, load it, and access it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counting all the wh words in three different genres in the Brown corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore frequency distribution operations on one of the web and chat text corpus
    files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take an ambiguous word and explore all its senses using WordNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pick two distinct synsets and explore the concepts of hyponyms and hypernyms
    using WordNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the average polysemy of nouns, verbs, adjectives, and adverbs according
    to WordNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To solve any real-world **Natural Language Processing** (**NLP**) problems,
    you need to work with huge amounts of data. This data is generally available in
    the form of a corpus out there in the open diaspora and as an add-on of the NLTK
    package. For example, if you want to create a spell checker, you need a huge corpus
    of words to match against.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of this chapter is to cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing various useful textual corpora available with NLTK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to access these in-built corpora from Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with frequency distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to WordNet and its lexical features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will try to understand these things from a practical standpoint. We will
    perform some exercises that will fulfill all of these goals through our recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing in-built corpora
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As already explained, we have many corpuses available for use with NLTK. We
    will assume that you have already downloaded and installed NLTK data on your computer.
    If not, you can find the same at [http://www.nltk.org/data.html](http://www.nltk.org/data.html).
    Also, a complete list of corpora that you can use from within NLTK data is available
    at [http://www.nltk.org/nltk_data/](http://www.nltk.org/nltk_data/).
  prefs: []
  type: TYPE_NORMAL
- en: Now, our first task/recipe involves us learning how to access any one of these
    corpora. We have decided to do some tests on the Reuters corpus or the same. We
    will import the corpus into our program and try to access it in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a new file named `reuters.py` and add the following import line in the
    file. This will specifically allow access to only the `reuters` corpus in our
    program from the entire NLTK data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we want to check what exactly is available in this corpus. The simplest
    way to do this is to call the `fileids()` function on the corpus object. Add the
    following line in your program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now run the program and you shall get an output similar to this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: These are the lists of files and the relative paths of each of them in the `reuters`
    corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will access the actual content of any of these files. To do this, we
    will use the `words()` function on the corpus object as follows, and we will access
    the `test/16097` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the program again and an extra new line of output will appear:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the list of words in the `test/16097` file is shown. This is
    curtailed though the entire list of words is loaded in the memory object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we want to access a specific number of words (`20`) from the same file,
    `test/16097`. Yes! We can specify how many words we want to access and store them
    in a list for use. Append the following two lines in the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Run this code and another extra line of output will be appended, which will
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Moving forward, the `reuters` corpus is not just a list of files but is also
    hierarchically categorized into 90 topics. Each topic has many files associated
    with it. What this means is that, when you access any one of the topics, you are
    actually accessing the set of all files associated with that topic. Let''s first output
    the list of topics by adding the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the code and the following line of output will be added to the output console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: All 90 categories are displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will write four simple lines of code that will not only access
    two topics but also print out the words in a loosely sentenced fashion as one
    sentence per line. Add the following code to the Python file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To explain briefly, we first selected the categories `''bop''` and `''cocoa''`
    and printed every word from these two categories'' files. Every time we encountered
    a dot (`.`), we inserted a new line. Run the code and something similar to the
    following will be the output on the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Download an external corpus, load it, and access it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have learned how to load and access an inbuilt corpus, we will learn
    how to download and also how to load and access any external corpus. Many of these
    inbuilt corpora are very good use cases for training purposes, but for solving
    any real-world problem, you will normally need an external dataset. For this recipe's
    purpose, we will be using the **Cornell CS Movie** review corpus, which is already
    labelled for positive and negative reviews and used widely for training sentiment
    analysis modules.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First and foremost, you will need to download the dataset from the Internet.
    Here's the link: [http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_cleaned.zip)](http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_cleaned.zip)).
    Download the dataset, unzip it, and store the resultant `Reviews` directory at
    a secure location on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a new file named `external_corpus.py` and add the following import line
    to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Since the corpus that we have downloaded is already categorized, we will use
    `CategorizedPlaintextCorpusReader` to read and load the given corpus. This way,
    we can be sure that the categories of the corpus are captured, in this case, positive
    and negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will read the corpus. We need to know the absolute path of the `Reviews`
    folder that we unzipped from the downloaded file from Cornell. Add the following
    four lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line is where you are reading the corpus by calling the `CategorizedPlaintextCorpusReader`
    constructor. The three arguments from left to right are Absolute Path to the `txt_sentoken`
    folder on your computer, all sample document names from the `txt_sentoken` folder,
    and the categories in the given corpus (in our case, `''pos''` and `''neg''`).
    If you look closely, you''ll see that all the three arguments are regular expression
    patterns. The next two lines will validate whether the corpus is loaded correctly
    or not, printing the associated categories and filenames of the corpus. Run the
    program and you should see something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve made sure that the corpus is loaded correctly, let''s get on
    with accessing any one of the sample documents from both the categories. For that,
    let''s first create a list, each containing samples of both the categories, `''pos''`
    and `''neg''`, respectively. Add the following two lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `reader.fileids()` method takes the argument category name. As you can see,
    what we are trying to do in the preceding two lines of code is straightforward
    and intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s select a file randomly from each of the lists of `posFiles` and
    `negFiles`. To do so, we will need the `randint()` function from the `random`
    library of Python. Add the following lines of code and we shall elaborate what
    exactly we did immediately after:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first line imports the `randint()` function from the `random` library. The
    next two files select a random file, each from the set of positive and negative
    category reviews. The last two lines just print the filenames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have selected the two files, let''s access them and print them
    on the console sentence by sentence. We will use the same methodology that we
    used in the first recipe to print a line-by-line output. Append the following
    lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'These `for` loops read every file one by one and will print on the console
    line by line. The output of the complete recipe should look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The quintessential ingredient of this recipe is the `CategorizedPlaintextCorpusReader`
    class of NLTK. Since we already know that the corpus we have downloaded is categorized,
    we only need provide appropriate arguments when creating the `reader` object.
    The implementation of the `CategorizedPlaintextCorpusReader` class internally
    takes care of loading the samples in appropriate buckets (`'pos'` and `'neg'`
    in this case).
  prefs: []
  type: TYPE_NORMAL
- en: Counting all the wh words in three different genres in the Brown corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Brown corpus is part of the NLTK data package. It's one of the oldest text
    corpuses assembled at Brown University. It contains a collection of 500 texts
    broadly categorized in to 15 different genres/categories such as news, humor,
    religion, and so on. This corpus is a good use case to showcase the categorized
    plaintext corpus, which already has topics/concepts assigned to each of the texts
    (sometimes overlapping); hence, any analysis you do on it can adhere to the attached
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of this recipe is to get you to perform a simple counting task
    on any given corpus. We will be using `nltk` library's `FreqDist` object for this
    purpose here, but more elaboration on the power of `FreqDist` will follow in the
    next recipe. Here, we will just concentrate on the application problem.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a new file named `BrownWH.py` and add the following `import` statements
    to begin:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We have imported the `nltk` library and the Brown corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up, we will check all the genres in the corpus and will pick any three
    categories from them to proceed with our task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `brown.categories()` function call will return the list of all genres in
    the Brown corpus. When you run this line, you will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s pick three `genres`--`fiction`, `humor` and `romance`--from this
    list as well as the `whwords` that we want to count out from the text of these
    three `genres`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We have created a list containing the three picked `genres` and another list
    containing the seven `whwords`.
  prefs: []
  type: TYPE_NORMAL
- en: Your list can be longer or shorter depending on what do you consider as `whwords`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have the `genres` and the words we want to count in lists, we will
    be extensively using the `for` loop to iterate over them and optimize the number
    of lines of code. So first, we write a `for` iterator on the `genres` list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: These four lines of code will only start iterating on the list `genres` and
    load the entire text of each genre in the `genre_text` variable as a continuous
    list words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up is a complex little statement where we will use the `nltk` library''s
    `FreqDist` object. For now, let''s understand the syntax and the broad-level output
    we will get from it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`FreqDist()` accepts a list of words and returns an object that contains the
    map word and its respective frequency in the input word list. Here, the `fdist`
    object will contain the frequency of each of the unique words in the `genre_text`
    word list.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I''m sure you''ve already guessed what our next step is going to be. We will
    simply access the `fdist` object returned by `FreqDist()` and get the count of
    each of the `wh` words. Let''s do it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We are iterating over the `whwords` word list, accessing the `fdist` object
    with each of the `wh` words as index, getting back the frequency/count of all
    of them, and printing them out.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the complete program, you will get this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On analyzing the output, you can clearly see that we have the word count of
    all seven `wh` words for the three picked `genres` on our console. By counting
    the population of `wh` words, you can, to a degree, gauge whether the given text
    is high on relative clauses or question sentences. Similarly, you may have a populated
    ontology list of important words that you want to get a word count of to understand
    the relevance of the given text to your ontology. Counting word populations and
    analyzing distributions of counts is one of the oldest, simplest, and most popular
    tricks of the trade to start any kind of textual analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Explore frequency distribution operations on one of the web and chat text corpus
    files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web and chat text corpus is non-formal literature that, as the name implies,
    contains content from Firefox discussion forums, scripts of movies, wine reviews,
    personal advertisements, and overheard conversations. Our objective here in this
    recipe is to understand the use of frequency distribution and its features/functions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In keeping with the objective of this recipe, we will run the frequency distribution
    on the personal advertising file inside `nltk.corpus.webtext`. Following that,
    we will explore the various functionalities of the `nltk.FreqDist` object such
    as the count of distinct words, 10 most common words, maximum-frequency words,
    frequency distribution plot, and tabulation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a new file named `webtext.py` and add the following three lines to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We just imported the required libraries and the `webtext` corpus; along with
    that, we also printed the constituent file''s names. Run the program and you shall
    see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will select the file that contains personal advertisement data and and
    run frequency distribution on it. Add the following three lines for it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`singles.txt` contains our target data; so, we loaded the words from that file
    in `wbt_words` and ran frequency distribution on it to get the `FreqDist` object
    `fdist`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following lines, which will show the most commonly appearing word (with
    the `fdist.max()` function) and the count of that word (with the `fdist[fdist.max()`]
    operation):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following line will show us the count of distinct words in the bag of our
    frequency distribution using the  `fdist.N()` function. Add the line in your code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s find out the 10 most common words in the selected corpus bag. The
    function `fdist.most_common()` will do this for us. Add the following two lines
    in the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us tabulate the entire frequency distribution using the `fdist.tabulate()`
    function. Add these lines in the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will plot the graph of the frequency distribution with `cumulative`
    frequencies using the `fdist.plot()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the program and see the output; we will discuss the same in the
    following section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You will also see the following graph pop up:'
  prefs: []
  type: TYPE_NORMAL
- en: '>![](img/d21e36af-4aae-4c1c-97a9-fe2f637fefe2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Upon analyzing the output, we realize that all of it is very intuitive. But
    what is peculiar is that most of it is not making sense. The token with maximum
    frequency count is `,`. And when you look at the `10` most common tokens, again
    you can't make out much about the target dataset. The reason is that there is
    no preprocessing done on the corpus. In the third chapter, we will learn one of
    the most fundamental preprocessing steps called stop words treatment and will
    also see the difference it makes.
  prefs: []
  type: TYPE_NORMAL
- en: Take an ambiguous word and explore all its senses using WordNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From this recipe onwards, we will turn our attention to WordNet. As you can
    read in the title, we are going to explore what word sense is. To give an overview,
    English is a very ambiguous language. Almost every other word has a different
    meaning in different contexts. For example, let's take the simplest of words,
    *bat* which you will learn as part of the first 10 English words in a language
    course almost anywhere on the planet. The first meaning is a club used for hitting
    the ball in various sports such as cricket, baseball, tennis, squash, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Now a *bat* can also mean a nocturnal mammal that flies at nights. The *Bat*
    is also Batman's preferred and most advanced transportation vehicle according
    to DC comics. These are all noun variants; let's consider verb possibilities.
    *Bat* can also mean a slight wink (bat an eyelid). Consequently, it can also mean
    beating someone to pulp in a fight or a competition. We believe that's enough
    of an introduction; with this, let's move on to the actual recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keeping the objective of the recipe in mind we have to choose a word for which
    we would be exploring its various senses as understood by WordNet. And yes, NLTK
    comes equipped with WordNet; you need not worry about installing any further libraries.
    So let's choose another simple word, *CHAIR*, as our sample for the purpose of
    this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a new file named `ambiguity.py` and add the following lines of code
    to start with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Here we imported the required NLTK corpus reader `wordnet` as the `wn` object.
    We can import it just like any another corpus readers we have used so far. In
    preparation for the next steps, we have created our string variable containing
    the word `chair`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now is the most important step. Let''s add two lines and I will elaborate what
    we are doing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line, though it looks simple, is actually the API interface that
    is accessing the internal WordNet database and fetching all the senses associated
    with the word `chair`. WordNet calls each of these senses  `synsets`. The next
    line simply asks the interpreter to print what it has fetched. Run this much and
    you should get an output like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the list contains seven `Synsets`, which means seven different
    senses of the word `Chair` exist in the WordNet database.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will add the following `for` loop, which will iterate over the list of `synsets`
    we have obtained and perform certain operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We are iterating over the list of `synsets` and printing the definition of
    each sense, associated lemmas/synonymous words, and example usage of each of the
    senses in a sentence. One typical iteration will print something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The first line is the name of `Synset`, the second line is the definition of
    this sense/`Synset`, the third line contains `Lemmas` associated with this `Synset`,
    and the fourth line is an example sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will obtain this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, definitions, Lemmas, and example sentences of all seven senses
    of the word `chair` are seen in the output. Straightforward API interfaces are
    available for each of the operations as elaborated in the preceding code sample.
    Now, let's talk a little bit about how WordNet arrives at such conclusions. WordNet
    is a database of words that stores all information about them in a hierarchical
    manner. If we take a look at the current example Write about `synsets` and hierarchical
    nature of WordNet storage. The following diagram will explain it in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Pick two distinct synsets and explore the concepts of hyponyms and hypernyms
    using WordNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A hyponym is a word of a more specific meaning than a more generic word such
    as *bat,* which we explored in the introduction section of our previous recipe.
    What we mean by *more specific* is, for example, cricket bat, baseball bat, carnivorous
    bat, squash racket, and so on. These are more specific in terms of communicating
    what exactly we are trying to mean.
  prefs: []
  type: TYPE_NORMAL
- en: As opposed to a hyponym, a hypernym is a more general form or word of the same
    concept. For our example, *bat* is a more generic word and it could mean club,
    stick, artifact, mammal, animal, or organism. We can go as generic as the physical
    entity, living thing, or object and still be considered as a hypernym of the word
    *bat*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the purpose of exploring the concepts of hyponym and hypernym, we have decided
    to select the synsets `bed.n.01` (first word sense of bed) and `woman.n.01` (second
    word sense of woman). Now we will explain the usage and meaning of the hypernym
    and hyponym APIs in the actual recipe section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a new file named `HypoNHypernyms.py` and add following three lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We've imported the libraries and initialized the two synsets that we will use
    in later processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following two lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s a simple call to the `hypernyms()` API function on the woman `Synset`;
    it will return the set of synsets that are direct parents of the same. However,
    the `hypernym_paths()` function is a little tricky. It will return a list of sets.
    Each set contains the path from the root node to the woman `Synset`. When you
    run these two statements, you will see the two direct parents of the `Synset`
    woman as follows in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Woman belongs to the adult and female categories in the hierarchical structure
    of the WordNet database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will try to print the paths from root node to the `woman.n.01` node.
    To do so, add the following lines of code and nested `for` loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'As explained, the returned object is a list of sets ordered in such a way that
    it follows the path from the root to the `woman.n.01` node exactly as stored in
    the WordNet hierarchy. When you run, here''s an example `Path`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s work with `hyponyms`. Add the following two lines, which will fetch
    the `hyponyms` for the synset `bed.n.01` and print them to the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'As explained, run them and you will see the following 20 synsets as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: These are `Hyponyms` or more specific terms for the word sense `bed.n.01` within
    WordNet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s print the actual words or `lemmas` that will make more sense to
    humans. Add the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This line of code is pretty similar to what we did in the hypernym example
    nested `for` loop written in four lines, which is clubbed in a single line here
    (in other words, we''re just showing off our skills with Python here). It will
    print the 26 `lemmas` that are very meaningful and specific words. Now let''s
    look at the final output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, `woman.n.01` has two hypernyms, namely adult and female, but
    it follows four different routes in the hierarchy of WordNet database from the
    root node `entity` to `woman` as shown in the output.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the Synset `bed.n.01` has 20 hyponyms; they are more specific and
    less ambiguous (for nothing is unambiguous in English). Generally the hyponyms
    correspond to leaf nodes or nodes very much closer to the leaves in the hierarchy
    as they are the least ambiguous ones.
  prefs: []
  type: TYPE_NORMAL
- en: Compute the average polysemy of nouns, verbs, adjectives, and adverbs according
    to WordNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let's understand what polysemy is. Polysemy means many possible meanings
    of a word or a phrase. As we have already seen, English is an ambiguous language
    and more than one meaning usually exists for most of the words in the hierarchy.
    Now, turning back our attention to the problem statement, we must calculate the
    average polysemy based on specific linguistic properties of all words in WordNet.
    As we'll see, this recipe is different from previous recipes. It's not just an
    API concept discovery but we are going to discover a linguistic concept here (I'm
    all emotional to finally get a chance to do so in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have decided to write the program to compute the polysemy of any one of the
    POS types of words and will leave it to you guys to modify the program to do so
    for the other three. I mean we shouldn't just spoon-feed everything, right? Not
    to worry! I will provide enough hints in the recipe itself to make it easier for
    you (for those who think it's already not very intuitive). Let's get on with the
    actual recipe then; we will compute the average polysemy of nouns alone.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a new file named `polysemy.py` and add these two initialization lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We have initialized the POS type of words we are interested in and, of course,
    imported the required libraries. To be more descriptive, `n` corresponds to nouns.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the most important line of code of this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This API returns all `synsets` of type `n` that is a noun present in the WordNet
    database, full coverage. Similarly, if you change the POS type to a verb, adverb,
    or adjective, the API will return all words of the corresponding type (hint #1).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will consolidate all `lemmas` in each of the `synset` into a single
    mega list that we can process further. Add the following code to do that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This piece of code is pretty intuitive; we have a nested `for` loop that iterates
    over the list of `synsets` and the `lemmas` in each `synset` and adds them up
    in our mega list lemmas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we have all `lemmas` in the mega list, there is a problem. There are
    some duplicates as it''s a list. Let''s remove the duplicates and take the count
    of distinct `lemmas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Converting a list into a set will automatically deduplicate (yes, it's a valid
    English word, I invented it) the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the second most important step in the recipe. We count the senses of each
    `lemma` in the WordNet database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Most of the code is intuitive; let''s focus on the the API `wn.synsets(lemma,
    type)`. This API takes as input a word/lemma (as the first argument) and the POS
    type it belongs to and returns all the senses (`synsets`) belonging to the `lemma`
    word. Note that depending on what you provide as the POS type, it will return
    senses of the word of only the given POS type (hint #2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have all the counts we need to compute the average polysemy. Let''s just
    do it and print it on the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints the total distinct lemmas, the count of senses, and the average
    polysemy of POS type `n` or nouns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is nothing much to say in this section, so I will instead give you some
    more information on how to go about computing the polysemy of the rest of the
    types. As you saw, *Noun -> ''n''*. Similarly, *Verbs -> ''v''*, *Adverbs -> ''r''*,
    and *Adjective -> ''a''* (hint # 3).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, I hope I have given you enough hints to get on with writing an NLP program
    of your own and not be dependent on the feed of the recipes.
  prefs: []
  type: TYPE_NORMAL
