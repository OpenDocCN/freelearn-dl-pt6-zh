["```py\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\n```", "```py\ninput_img = Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format\n\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\n\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(16, (3, 3), activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n```", "```py\nautoencoder = Model(input_img, decoded)\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n\nfrom tensorflow.keras.datasets import mnist\nimport numpy as np\n\n(x_train, _), (x_test, _) = mnist.load_data()\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\nx_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) \nx_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) \n\nfrom tensorflow.keras.callbacks import TensorBoard\n\nautoencoder.fit(x_train, x_train,\n epochs=50,\n batch_size=128,\n shuffle=True,\n validation_data=(x_test, x_test),\n callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n\ndecoded_imgs = autoencoder.predict(x_test)\n```", "```py\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n  ax = plt.subplot(2, n, i)\n  plt.imshow(x_test[i].reshape(28, 28))\n  plt.gray()\n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False)\n  ax = plt.subplot(2, n, i + n)\n  plt.imshow(decoded_imgs[i].reshape(28, 28))\n  plt.gray()\n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False)\nplt.show()\n```", "```py\n//first change directory to sample working folder\ntensorboard --logdir=/tmp/autoencoder\n```", "```py\nTensorBoard 1.10.0 at ***http://DESKTOP-V2J9HRG:6006*** (Press CTRL+C to quit)\nor use\nhttp://0.0.0.0:6000\n```", "```py\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n```", "```py\nx = MaxPooling2D((2, 2), padding='same')(x)\n```", "```py\npip install keras\npip install pickle\npip install matplotlib\n```", "```py\nimport os\nimport urllib.request\nimport pickle\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n***#downlaod driving data (450Mb)*** \ndata_url = 'https://s3.amazonaws.com/donkey_resources/indoor_lanes.pkl'\nfile_path, headers = urllib.request.urlretrieve(data_url)\nprint(file_path)\n\nwith open(file_path, 'rb') as f:\n  X, Y = pickle.load(f)\n```", "```py\nimport numpy as np\ndef unison_shuffled_copies(X, Y):\n  assert len(X) == len(Y)\n  p = np.random.permutation(len(X))\n  return X[p], Y[p]\n\nshuffled_X, shuffled_Y = unison_shuffled_copies(X,Y)\nlen(shuffled_X)\n```", "```py\ntest_cutoff = int(len(X) * .8) # 80% of data used for training\nval_cutoff = test_cutoff + int(len(X) * .2) # 20% of data used for validation and test data\ntrain_X, train_Y = shuffled_X[:test_cutoff], shuffled_Y[:test_cutoff]\nval_X, val_Y = shuffled_X[test_cutoff:val_cutoff], shuffled_Y[test_cutoff:val_cutoff]\ntest_X, test_Y = shuffled_X[val_cutoff:], shuffled_Y[val_cutoff:]\n\nlen(train_X) + len(val_X) + len(test_X)\n```", "```py\nX_flipped = np.array([np.fliplr(i) for i in train_X])\nY_flipped = np.array([-i for i in train_Y])\ntrain_X = np.concatenate([train_X, X_flipped])\ntrain_Y = np.concatenate([train_Y, Y_flipped])\nlen(train_X)\n```", "```py\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Convolution2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n\nimg_in = Input(shape=(120, 160, 3), name='img_in')\nangle_in = Input(shape=(1,), name='angle_in')\n\nx = Convolution2D(8, 3, 3)(img_in)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Convolution2D(16, 3, 3)(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Convolution2D(32, 3, 3)(x)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nmerged = Flatten()(x)\n\nx = Dense(256)(merged)\nx = Activation('linear')(x)\nx = Dropout(.2)(x)\n\nangle_out = Dense(1, name='angle_out')(x)\n\nmodel = Model(input=[img_in], output=[angle_out])\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()\n```", "```py\nimport os\nfrom keras import callbacks\n\nmodel_path = os.path.expanduser('~/best_autopilot.hdf5')\n\nsave_best = callbacks.ModelCheckpoint(model_path, monitor='val_loss', verbose=1, \n save_best_only=True, mode='min')\n\nearly_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, \n verbose=0, mode='auto')\n\ncallbacks_list = [save_best, early_stop]\n\nmodel.fit(train_X, train_Y, batch_size=64, epochs=4, validation_data=(val_X, val_Y), callbacks=callbacks_list)\n```", "```py\nx = Convolution2D(8, 3, 3)(img_in)\nx = Activation('relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Convolution2D(16, 3, 3)(x)\nx = Activation('relu')(x)\n#x = MaxPooling2D(pool_size=(2, 2))(x)\n\nx = Convolution2D(32, 3, 3)(x)\nx = Activation('relu')(x)\n#x = MaxPooling2D(pool_size=(2, 2))(x)\n```", "```py\nx = Dropout(.5)(x)\n```", "```py\nmodel.fit(train_X, train_Y, batch_size=64, epochs=10, validation_data=(val_X, val_Y), callbacks=callbacks_list)\n```", "```py\nx = Dropout(.5)(x)\n```", "```py\nimport numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.utils import np_utils\n```", "```py\nnumpy.random.seed(7)\n```", "```py\nalphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\nchar_to_int = dict((c, i) for i, c in enumerate(alphabet))\nint_to_char = dict((i, c) for i, c in enumerate(alphabet))\n\nseq_length = 1\ndataX = []\ndataY = []\n\nfor i in range(0, len(alphabet) - seq_length, 1):\n  seq_in = alphabet[i:i + seq_length]\n  seq_out = alphabet[i + seq_length]\n  dataX.append([char_to_int[char] for char in seq_in])\n  dataY.append(char_to_int[seq_out])\n  print(seq_in, '->', seq_out)\n```", "```py\nX = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n# normalize\nX = X / float(len(alphabet))\n# one hot encode the output variable\ny = np_utils.to_categorical(dataY)\n```", "```py\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\nmodel.add(Dense(y.shape[1], activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X, y, epochs=500, batch_size=1, verbose=2)\n\nscores = model.evaluate(X, y, verbose=0)\nprint(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n```", "```py\nfor pattern in dataX:\n  x = numpy.reshape(pattern, (1, len(pattern), 1))\n  x = x / float(len(alphabet))\n  prediction = model.predict(x, verbose=0)\n  index = numpy.argmax(prediction)\n  result = int_to_char[index]\n  seq_in = [int_to_char[value] for value in pattern]\n  print(seq_in, \"->\", result)\n```", "```py\nimport numpy as np\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\n```", "```py\nEPOCH_NP = 100\nINPUT_SHAPE = (1, -1, 1)\nOUTPUT_SHAPE = (1, -1, 3)\nDATA_FILE = \"data.txt\"\nMODEL_FILE = \"RPS_model.h5\"\n```", "```py\ndef simple_model(): \n  new_model = Sequential()\n  new_model.add(LSTM(output_dim=64, input_dim=1, return_sequences=True, activation='sigmoid'))\n  new_model.add(LSTM(output_dim=64, return_sequences=True, activation='sigmoid'))\n  new_model.add(LSTM(output_dim=64, return_sequences=True, activation='sigmoid'))\n  new_model.add(Dense(64, activation='relu'))\n  new_model.add(Dense(64, activation='relu'))\n  new_model.add(Dense(3, activation='softmax'))\n  new_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'categorical_crossentropy'])\n  return new_model\n```", "```py\ndef batch_generator(filename): \n  with open('data.txt', 'r') as data_file:\n    for line in data_file:\n      data_vector = np.array(list(line[:-1]))\n      input_data = data_vector[np.newaxis, :-1, np.newaxis]\n      temp = np_utils.to_categorical(data_vector, num_classes=3) \n      output_data = temp[np.newaxis, 1:]\n      yield (input_data, output_data)\n```", "```py\n# Create model\nnp.random.seed(7)\nmodel = simple_model()\n```", "```py\nfor (input_data, output_data) in batch_generator('data.txt'):\n  try:\n    model.fit(input_data, output_data, epochs=100, batch_size=100)\n  except:\n    print(\"error\")\n```", "```py\nprint(\"evaluating\")\nvalidation = '100101000110221110101002201101101101002201011012222210221011011101011122110010101010101'\ninput_validation = np.array(list(validation[:-1])).reshape(INPUT_SHAPE)\noutput_validation = np_utils.to_categorical(np.array(list(validation[1:]))).reshape(OUTPUT_SHAPE)\nloss_and_metrics = model.evaluate(input_validation, output_validation, batch_size=100)\n\nprint(\"\\n Evaluation results\")\n\nfor i in range(len(loss_and_metrics)):\n  print(model.metrics_names[i], loss_and_metrics[i])\n\ninput_test = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]).reshape(INPUT_SHAPE)\nres = model.predict(input_test)\nprediction = np.argmax(res[0], axis=1)\nprint(res, prediction)\n\nmodel.save(MODEL_FILE)\ndel model\n```"]