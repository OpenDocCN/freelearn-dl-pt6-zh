["```py\ncd ch6\njupyter notebook\n```", "```py\nfeatures = sequence.input_variable(1)\n\nwith default_options(initial_state = 0.1):\n    model = Sequential([\n        Fold(LSTM(15)),\n        Dense(1)\n    ])(features)\n\ntarget = input_variable(1, dynamic_axes=model.dynamic_axes)\n```", "```py\nfrom cntk import sequence, default_options, input_variable\nfrom cntk.layers import Recurrence, LSTM, Dropout, Dense, Sequential, Fold, Recurrence\n\nfeatures = sequence.input_variable(1)\n\nwith default_options(initial_state = 0.1):\n    model = Sequential([\n        Recurrence(LSTM(15)),\n        Fold(LSTM(15)),\n        Dense(1)\n    ])(features)\n```", "```py\n@Function\ndef criterion_factory(z, t):\n    loss = squared_error(z, t)\n    metric = squared_error(z, t) \n\n    return loss, metric\n\nloss = criterion_factory(model, target)\nlearner = adam(model.parameters, lr=0.005, momentum=0.9)\n```", "```py\n<sequence_id> |<input_name> <values> |<input_name> <values>\n```", "```py\n0 |target 0.837696335078534 |features 0.756544502617801\n0 |features 0.7931937172774869\n0 |features 0.8167539267015707\n0 |features 0.8324607329842932\n0 |features 0.837696335078534\n0 |features 0.837696335078534\n0 |features 0.837696335078534\n1 |target 0.4239092495636999 |features 0.24554973821989529\n1 |features 0.24554973821989529\n1 |features 0.00017225130534296885\n1 |features 0.0014886562154347149\n1 |features 0.005673647442829338\n1 |features 0.01481675392670157\n```", "```py\ndef create_datasource(filename, sweeps=INFINITELY_REPEAT):\n    target_stream = StreamDef(field='target', shape=1, is_sparse=False)\n    features_stream = StreamDef(field='features', shape=1, is_sparse=False)\n\n    deserializer = CTFDeserializer(filename, StreamDefs(features=features_stream, target=target_stream))\n    datasource = MinibatchSource(deserializer, randomize=True, max_sweeps=sweeps) \n\n    return datasource\n\ntrain_datasource = create_datasource('solar_train.ctf')\ntest_datasource = create_datasource('solar_val.ctf', sweeps=1)\n```", "```py\nprogress_writer = ProgressPrinter(0)\ntest_config = TestConfig(test_datasource)\n\ninput_map = {\n    features: train_datasource.streams.features,\n    target: train_datasource.streams.target\n}\n\nhistory = loss.train(\n    train_datasource, \n    epoch_size=EPOCH_SIZE,\n    parameter_learners=[learner], \n    model_inputs_to_streams=input_map,\n    callbacks=[progress_writer, test_config],\n    minibatch_size=BATCH_SIZE,\n    max_epochs=EPOCHS)\n```", "```py\naverage      since    average      since      examples\n    loss       last     metric       last              \n ------------------------------------------------------\nLearning rate per minibatch: 0.005\n     0.66       0.66       0.66       0.66            19\n    0.637      0.626      0.637      0.626            59\n    0.699      0.752      0.699      0.752           129\n    0.676      0.656      0.676      0.656           275\n    0.622      0.573      0.622      0.573           580\n    0.577      0.531      0.577      0.531          1150\n```", "```py\nimport pickle\n\nNORMALIZE = 19100\n\nwith open('test_samples.pkl', 'rb') as test_file:\n    test_samples = pickle.load(test_file)\n\nmodel(test_samples) * NORMALIZE\n```", "```py\narray([[ 8161.595],\n       [16710.596],\n       [13220.489],\n       ...,\n       [10979.5  ],\n       [15410.741],\n       [16656.523]], dtype=float32)\n```"]