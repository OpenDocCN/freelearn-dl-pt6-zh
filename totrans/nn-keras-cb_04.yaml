- en: Building a Deep Convolutional Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Inaccuracy of traditional neural network when images are translated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a CNN from scratch using Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs to improve accuracy in case of image translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gender classification using CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation to improve network accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at a traditional deep feedforward neural
    network. One of the limitations of a traditional deep feedforward neural network is
    that it is not translation-invariant, that is, a cat image in the upper-right
    corner of an image would be considered different from an image that has a cat
    in the center of the image. Additionally, traditional neural networks are affected
    by the scale of an object. If the object is big in the majority of the images
    and a new image has the same object in it but with a smaller scale (occupies a
    smaller portion of the image), traditional neural networks are likely to fail
    in classifying the image.
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks** (**CNNs**) are used to deal with such issues.
    Given that a CNN is able to deal with translation in images and also the scale
    of images, it is considered a lot more useful in object classification/ detection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Inaccuracy of traditional neural network when images are translated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a CNN from scratch using Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using CNNs to improve image classification on a MNIST dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing data augmentation to improve network accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gender classification using CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inaccuracy of traditional neural networks when images are translated
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the need of CNNs further, we will first understand why a feed
    forward **Neural Network** (**NN**) does not work when an image is translated
    and then see how the CNN improves upon traditional feed forward NN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: We will build a NN model to predict labels from the MNIST dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will consider all images that have a label of 1 and take an average of all
    of them (generating an average of 1 image)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will predict the label of the average 1 image that we have generated in the
    previous step using traditional NN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will translate the average 1 image by 1 pixel to the left or right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will make a prediction of the translated image using our traditional NN model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The strategy defined above is coded as follows (please refer to `Issue_with_image
    translation.ipynb` file in GitHub while implementing the code)
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset and extract the train and test MNIST datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Fetch the training set corresponding to label `1` only:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape and normalize the original training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'One-hot-encode the output labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a model and fit it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot the average 1 image that we obtained in step 2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we initialized an empty picture that is 28 x 28 in dimension
    and took an average pixel value at the various pixel locations of images that
    have a label of 1 (the `X_train1` object) by looping through all the values in
    the `X_train1` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot of the average 1 image appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11fffca9-ea5a-4ea8-8fc4-59bfc3402c22.png)'
  prefs: []
  type: TYPE_IMG
- en: It is to be noted that the more yellow (thick) the pixel is, the more often
    people have written on top of the pixel, and the less yellow (more blue/less thick)
    the pixel, the less often people have written on top of the pixel. Also, it is
    to be noted that the pixel in the middle is the yellowest/thickest (this is because
    most people would be writing over the middle pixels, irrespective of whether the
    whole digit is written in a vertical line or is slanted toward the left or right).
  prefs: []
  type: TYPE_NORMAL
- en: Problems with traditional NN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scenario 1**: Let''s create a new image where the original image is translated
    by 1 pixel toward the left. In the following code, we are looping through the
    columns of the image and copying the pixel values of the next column to the current
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The left translated average 1 image looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d18d3b21-d722-4c43-9860-79b25e008b2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s go ahead and predict the label of the image using the built model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The model''s prediction on the translated image is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8682fa3-d0d9-4dec-9ba8-bf00aea80760.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see a prediction of 1, though with a lower probability than when pixels
    were not translated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario 2**: A new image is created in which the pixels of the original
    average 1 image are shifted by 2 pixels to the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The right translated average 1 image looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2fed1a8-3e6d-4a03-ad3e-3ff32ce0d59c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The prediction of this image is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The model''s prediction on the translated image is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2d7d8ca-e1ab-44d3-b039-0199a5ef6b43.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the prediction is incorrect with an output of 3\. This is the
    problem that we will be addressing by using a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Building a CNN from scratch using Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about how a CNN works by building a feedforward
    network from scratch using NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A typical CNN has multiple components. In this section, we will go through the
    various components of a CNN before we understand how the CNN improves prediction
    accuracy when an image is translated.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are already aware of how a typical NN works. In this section, let's understand
    the working details of the convolution process in CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A convolution is a multiplication between two matrices—one matrix being big
    and the other being small. To understand convolution, consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix *A* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2dd6021a-9a7e-45c5-9e81-94a543cb720d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Matrix *B* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6ad679b-8864-42af-948f-745bea146353.png)'
  prefs: []
  type: TYPE_IMG
- en: When performing convolutions, think of it as we are sliding the smaller matrix
    over the larger matrix, that is, we can potentially come up with nine such multiplications
    as the smaller matrix is slid over the entire area of the bigger matrix. Note
    that it is not matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: 'The various multiplications that happen between the bigger and smaller matrix
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*{1, 2, 5, 6}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of the
    smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*1*1 + 2*2 + 5*3 + 6*4 = 44*'
  prefs: []
  type: TYPE_NORMAL
- en: '*{2, 3, 6, 7}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of the
    smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*2*1 + 3*2 + 6*3 + 7*4 = 54*'
  prefs: []
  type: TYPE_NORMAL
- en: '*{3, 4, 7, 8}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of the
    smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*3*1 + 4*2 + 7*3 + 8*4 = 64*'
  prefs: []
  type: TYPE_NORMAL
- en: '*{5, 6, 9, 10}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of the
    smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*5*1 + 6*2 + 9*3 + 10*4 = 84*'
  prefs: []
  type: TYPE_NORMAL
- en: '*{6, 7, 10, 11}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of
    the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*6*1 + 7*2 + 10*3 + 11*4 = 94*'
  prefs: []
  type: TYPE_NORMAL
- en: '*{7, 8, 11, 12}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of
    the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*7*1 + 8*2 + 11*3 + 12*4 = 104*'
  prefs: []
  type: TYPE_NORMAL
- en: '*{9,10,13,14}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of the
    smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*9*1 + 10*2 + 13*3 + 14*4 = 124*'
  prefs: []
  type: TYPE_NORMAL
- en: '*{10, 11, 14, 15}* of the bigger matrix is multiplied with *{1, 2, 3 ,4}* of
    the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*10*1 + 11*2 + 14*3 + 15*4 = 134*'
  prefs: []
  type: TYPE_NORMAL
- en: '*{11, 12, 15, 16}* of the bigger matrix is multiplied with *{1, 2, 3, 4}* of
    the smaller matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*11*1 + 12*2 + 15*3 + 16*4 = 144*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of the preceding steps would be the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d341e2cc-5f69-4bb6-9d34-f6cb88e9ded8.png)'
  prefs: []
  type: TYPE_IMG
- en: Conventionally, the smaller matrix is called a filter or kernel and the smaller
    matrix values are arrived at statistically through gradient descent. The values
    within the filter are the constituent weights.
  prefs: []
  type: TYPE_NORMAL
- en: Practically, when the image input shape is 224 x 224 x 3, where there are 3
    channels, a filter that has a shape of 3 x 3 would also have 3 channels so that
    performing the matrix multiplication (sum product) is enabled.
  prefs: []
  type: TYPE_NORMAL
- en: A filter will have as many channels as the number of channels in the matrix
    it multiplies with.
  prefs: []
  type: TYPE_NORMAL
- en: Strides
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding steps, given that the filter moved one step at a time both
    horizontally and vertically, the strides for the filter are (1, 1). The higher
    the number of strides, the higher the number of values that are skipped from the
    matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding steps, we missed out on multiplying the leftmost values of
    the filter with the rightmost values of the original matrix. If we were to perform
    such an operation, we would have ensure that there is zero padding around the
    edges (the edges of the image padded with zeros) of the original matrix. This
    form of padding is called **valid** padding. The matrix multiplication we performed
    in the *Filter* section of the *Understanding convolution* recipe was a result
    of the **same** padding.
  prefs: []
  type: TYPE_NORMAL
- en: From convolution to activation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a traditional NN, a hidden layer not only multiplies the input values by
    the weights, but also applies a non-linearity to the data, that is, it passes
    the values through an activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'A similar activity happens in a typical CNN too, where the convolution is passed
    through an activation function. CNN supports the traditional activations functions
    we have seen so far: sigmoid, ReLU, tanh, and leaky ReLU.'
  prefs: []
  type: TYPE_NORMAL
- en: For the preceding output, we can see that the output remains the same when passed
    through a ReLU activation function, as all the numbers are positive.
  prefs: []
  type: TYPE_NORMAL
- en: From convolution activation to pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we looked at how convolutions work. In this section,
    we will understand the typical next step after a convolution: pooling.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say the output of the convolution step is as follows (we are not considering
    the preceding example, and this is a new example to only illustrate how pooling
    works):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1262742f-f600-4d1e-96f8-8abe2641e16f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding case, the output of a convolution step is a 2 x 2 matrix.
    Max pooling considers the 2 x 2 block and gives the maximum value as output. Similarly,
    imagine that the output of the convolution step is a bigger matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e1ef209-932a-43ac-beec-86f01d8aa9e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Max pooling divides the big matrix into non-overlapping blocks of 2 x 2 (when
    the stride value is 2), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e811be5-036f-47ac-a50c-2a6c992987dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From each block, only the element that has the highest value is chosen. So,
    the output of the max pooling operation on the preceding matrix would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b947e987-a657-4837-bde2-4ea0c4cdae85.png)'
  prefs: []
  type: TYPE_IMG
- en: In practice, it is not necessary to have a 2 x 2 window in all cases, but it
    is used more often than not.
  prefs: []
  type: TYPE_NORMAL
- en: The other types of pooling involved are sum and average—again, in practice,
    we see a lot of max pooling when compared to other types of pooling.
  prefs: []
  type: TYPE_NORMAL
- en: How do convolution and pooling help?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the drawbacks of traditional NN in the MNIST example is that each pixel
    is associated with a distinct weight.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, if an adjacent pixel, other than the original pixel, were to be highlighted,
    instead of the original pixel, the output would not be very accurate (the example
    of *scenario 1*, where the average one was slightly to the left of the middle).
  prefs: []
  type: TYPE_NORMAL
- en: This scenario is now addressed, as the pixels share weights that are constituted
    within each filter.
  prefs: []
  type: TYPE_NORMAL
- en: All the pixels get multiplied by all the weights that constitute a filter. In
    the pooling layer, only the values post convolution that have a high value are
    chosen.
  prefs: []
  type: TYPE_NORMAL
- en: This way, irrespective of whether the highlighted pixel is at the center or
    is slightly away from the center, the output would more often than not be the
    expected value.
  prefs: []
  type: TYPE_NORMAL
- en: However, the issue still remains the same when the highlighted pixels are very
    far away from the center.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To gain a solid understanding, we'll build a CNN-based architecture using Keras
    and validate our understanding of how CNN works by matching the output obtained
    by building the feedforward propagation part of CNN from scratch with the output
    obtained from Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement CNN with a toy example where the input and expected output
    data is defined (the code file is available as `CNN_working_details.ipynb` in
    GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the input and output dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we created data where positive input gives an output
    of `0` and negative input gives an output of `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scale the input dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape the input dataset so that each input image is represented in the format
    of width `x` height `x` number of channels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the model architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Instantiate the model after importing the relevant methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next step, we are performing the convolution operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding step, we are performing a 2D convolution (the matrix multiplication
    that we saw in the section on *Understanding convolution*) on input data where
    we have 1 filter of size 3 x 3.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, given that this is the first layer since instantiating a model,
    we specify the input shape, which is (4 , 4, 1)
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we perform ReLu activation on top of the output of the convolution.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the convolution operation in this scenario is 2 x 2 x 1 in shape,
    as the matrix multiplication of weights with input would yield a 2 x 2 matrix
    (given that the default strides is 1 x 1).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the size of output would shrink, as we have not padded the input
    (put zeros around the input image).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following step, we are adding a layer that performs a max pooling operation,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We are performing max pooling on top of the output obtained from the previous
    layer, where the pool size is 2 x 2\. This means that the maximum value in a subset
    of the 2 x 2 portion of the image is calculated.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a stride of 2 × 2 in the pooling layer would not affect the output
    in this case as the output of the previous step was 2 × 2\. However, in general,
    a stride that is of a greater size than 1 × 1 would affect the output shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s flatten the output from the pooling layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Once we perform flattening, the process becomes very similar to what we performed
    in standard feedforward neural networks where the input is connected to the hidden
    layer and then to the output layer (we can connect the input to more hidden layers,
    too!).
  prefs: []
  type: TYPE_NORMAL
- en: 'We are directly connecting the output of the flatten layer to the output layer
    using the sigmoid activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of the model can be obtained and looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e708f38-84ee-4c40-9b05-299cf9c52816.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that there are 10 parameters in the convolution layer as the one 3 x 3
    filter would have 9 weights and 1 bias term. The pooling layer and flatten layer
    do not have any parameters as they are either extracting maximum values in certain
    regioned (max pooling) or are flattening the output from the previous layer (flatten)
    and thus no operation where weights need to be modified in either of these layers.
  prefs: []
  type: TYPE_NORMAL
- en: The output layer has two parameters since the flatten layer has one output,
    which is connected to the output layer that has one value—hence we will have one
    weight and one bias term connecting the flatten layer and output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile and fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are specifying the loss as binary cross-entropy because
    the outcome is either a `1` or a `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We are fitting the model to have optimal weights that connect the input layer
    with the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the CNN output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have fit the model, let''s validate the output we obtain from the
    model by implementing the feedforward portion of the CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s extract the order in which weights and biases are presented:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/bc1369d0-c303-4cbe-8080-4e302592a96a.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the weights of the convolution layer are presented first, then
    the bias, and finally the weight and bias in the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also note that the shape of weights in the convolution layer is (3, 3, 1, 1)
    as the filter is 3 x 3 x 1 in shape (because the image is three-dimensional: 28
    x 28 x 1 in shape) and the final 1 (the fourth value in shape) is for the number
    of filters that are specified in the convolution layer.'
  prefs: []
  type: TYPE_NORMAL
- en: If we had specified 64 as the number of filters in the convolution, the shape
    of weights would have been 3 x 3 x 1 x 64.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, had the convolution operation been performed on an image with 3 channels,
    each filter's shape would have been 3 x 3 x 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the weight values at various layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s extract the output of the first input so that we can validate it with
    feedforward propagation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cd6f5149-946f-4c97-8499-a4c6c7d6fe6c.png)'
  prefs: []
  type: TYPE_IMG
- en: The output from the iteration we ran is 0.0428 (this could be different when
    you run the model, as the random initialization of weights could be different),
    which we will validate by performing matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: We are reshaping the input while passing it to the predict method as it expects
    the input to have a shape of (None, 4, 4, 1), where None specifies that the batch
    size could be any number.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the convolution of the filter with the input image. Note that the input
    image is 4 x 4 in shape while the filter is 3 x 3 in shape. We will be performing
    the matrix multiplication (convolution) along the rows as well as columns in the
    code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are initializing an empty list named `sumprod` that
    stores the output of each matrix multiplication of the filter with the image's
    subset (the subset of the image is of the size of filter).
  prefs: []
  type: TYPE_NORMAL
- en: 'Reshape the output of `sumprod` so that it can then be passed to the pooling
    layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform activation on top of the convolution''s output before it is passed
    to the pooling layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass the convolution output to the pooling layer. However, in the current case,
    given that the output of the convolution is 2 x 2, we will keep it simple and
    just take the maximum value in the output we obtained in *step 6*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Connect the output of the pooling layer to the output layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We multiplied by the pooling layer's output with the weight in the output layer
    and added the bias in the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the sigmoid output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of preceding operation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f3d6105-094e-441a-9139-3c885798733d.png)'
  prefs: []
  type: TYPE_IMG
- en: The output that you see here will be the same as the one that we obtained using
    the `model.predict` method, thus validating our understanding of how a CNN works.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs to improve accuracy in the case of image translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we learned about the issue of translation in images
    and how a CNN works. In this section, we will leverage that knowledge to learn
    how a CNN works toward improving prediction accuracy when an image is translated.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy that we will be adopting to build a CNN model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that the input shape is 28 x 28 x 1, the filters shall be 3 x 3 x 1 in
    size:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the size of filter can change, however the number of channels cannot
    change
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's initialize 10 filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will perform pooling on top of the output obtained in the previous step
    of convolving 10 filters over the input image:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This would result in halving the image's dimension
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We will flatten the output obtained while pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The flattened layer will be connected to another hidden layer that has 1,000
    units
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we connect the hidden layer to the output layer where there are 10
    possible classes (as there are 10 digits, from 0 to 9)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we build the model, we will translate the average 1 image by 1 pixel and
    then test the CNN model's prediction on the translated image. Note that the feedforward
    NN architecture was not able to predict the right class in this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s understand using a CNN on MNIST data in code (The code file is available
    as `CNN_image_translation.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load and preprocess the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that all the steps that we performed in this step are the same as what
    we performed in [Chapter 2](750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml), *Building
    a Deep Feedforward Neural Network*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build and compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of the model that we initialized in the preceding code can be obtained
    and is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The summary of the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e4273c7-43a7-4785-9fcb-a5a0e92fc1bb.png)'
  prefs: []
  type: TYPE_IMG
- en: We have a total of 100 parameters in the convolution layer as there are 10 of
    the 3 x 3 x 1 filters, resulting in a total of 90 weight parameters. Additionally,
    10 bias terms (1 for each filter) add up to form a total of 100 parameters in
    the convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: Note that max pooling does not have any parameters, as it is about extracting
    the maximum value within a patch that is 2 x 2 in size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding model gives an accuracy of 98% in 5 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2356a9a5-1d51-429c-b4ea-5ca798823335.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s identify the average 1 image and then translate it by `1` unit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we filtered all the image inputs that have a label of
    `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we took the average 1 image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we translated each pixel in the average 1 image by 1
    unit to the left.
  prefs: []
  type: TYPE_NORMAL
- en: 'Predict on the translated 1 image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of preceding step is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d530d8c-c505-4b13-aafa-5d0c85335cdd.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the prediction now (when we use a CNN) has more probability (0.9541)
    for 1 when compared to the scenario where the deep feed forward NN model, which
    is predicted as (0.6335) in the label of the translated image in the *Inaccuracy
    of traditional neural network when images are translated* section.
  prefs: []
  type: TYPE_NORMAL
- en: Gender classification using CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we learned about how a CNN works and how CNNs solve
    the image-translation problem.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will further our understanding of how a CNN works by building
    a model that works toward detecting the gender of person present in image.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, let''s formulate our strategy of how we will solve this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: We will collect a dataset of images and label each image based on the gender
    of person present in image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll work on only 2,000 images, as the data fetching process takes a considerably
    long time for our dataset (as we are manually downloading images from a website
    in this case study)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, we'll ensure that there is equal representation of male and female
    images in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the dataset is in place, we will reshape the images into the same size
    so that they can be fed into a CNN model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will build the CNN model where the output layer has as many classes as the
    number of labels two
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that this is a case of predicting one out of the two possible labels in
    the dataset, we will minimize the binary cross-entropy loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will code the strategy that we defined prior (the code
    file is available as `Gender classification.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the dataset and inspect its content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'A sample of some of the key fields in the dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bbc8d52-1050-4284-9dc9-ac82eb5a039c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fetch 1,000 male images and 1,000 female images from the URL links provided
    in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, `final_data` contains URL links for 1,000 male images
    and 1,000 female images. Read the URL links and fetch the images corresponding
    to the URL links. Ensure that all images are 300 × 300 × 3 in shape (as the majority
    of the images in his dataset have that shape) and also that we take care of any
    forbidden to access issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'A sample of the input and their corresponding emotion labels looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d08b8aaf-175f-470f-88f4-49b7f728ff26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Create the input and output arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding step, we have converted the color image into a grayscale image
    as the color of the image is likely to add additional information (we'll validate
    this hypothesis in [Chapter 5](c50d0373-e7d4-47d9-a514-df766f575a47.xhtml), *Transfer
    Learning*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we have resized our images to a lower size (50 x 50 x 1) in shape.
    The result of this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/014738bb-62bc-4d19-86cf-1a5e419d57b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we converted the output into a one-hot-encoded version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create train and test datasets. First, we convert the input and output lists
    into arrays and then shape the input so that it is in a shape that can be be provided
    as input to the CNN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the first value of `x2` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f75c95fd-9415-4496-9936-f1045c6aa1c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the input has values between `0` to `255` and thus we have to scale
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we split the input and output arrays into train and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The shapes of the train and test input, output arrays are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dabfed13-ae88-4956-aa54-d93b53a6477a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Build and compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5f69cca-98f7-42f6-a04d-0f86e829cbe0.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the number of channels in the output of the convolution layer would
    be equal to the number of filters specified in that layer. Additionally, we have
    performed a slightly more aggressive pooling on the first convolution layer's
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''ll compile the model to minimize binary cross entropy loss (as the
    output has only two classes) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/8efa1177-0525-46b7-a427-c60dcefea2f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Once we fit the model, we can see that the preceding code results in an accuracy
    of ~80% in predicting the right gender in an image.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The accuracy of classification can be further improved by:'
  prefs: []
  type: TYPE_NORMAL
- en: Working on more images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working on bigger images (rather than 50 x 50 images) that are used to train
    a larger network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging transfer learning (which will be discussed in [Chapter 5](c50d0373-e7d4-47d9-a514-df766f575a47.xhtml),
    *Transfer Learning*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding overfitting using regularization and dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation to improve network accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is difficult to classify images accurately if they are translated from their
    original location. However, given an image, the label of the image will remain
    the same, even if we translate, rotate, or scale the image. Data augmentation
    is a way to create more images from the given set of images, that is, by rotating,
    translating, or scaling them and mapping them to the label of the original image.
  prefs: []
  type: TYPE_NORMAL
- en: An intuition for this is as follows: an image of a person will still be corresponding
    to the person, even if the image is rotated slightly or the person in the image
    is moved from the middle of the image to far right of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we should be in a position to create more training data by rotating and
    translating the original images, where we already know the labels that correspond
    to each image.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be working on the CIFAR-10 dataset, which contains images
    of objects of 10 different classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy that we''ll use is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the CIFAR-10 dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale the input values
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot-encode the output classes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a deep CNN with multiple convolution and pooling layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compile and fit the model to test its accuracy on the test dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate random translations of the original set of images in the training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit the same model architecture that was built in the previous step on the total
    images (generated images, plus the original images)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the accuracy of the model on the test dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be implementing data augmentation using the `ImageDataGenerator` method
    in the `keras.preprocessing.image` package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the benefits of data augmentation, let's go through an example
    of calculating the accuracy on the CIFAR-10 dataset with data augmentation and
    without data augmentation (the code file is available as `Data_augmentation_to_improve_network_accuracy.ipynb`
    in GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: Model accuracy without data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s calculate the accuracy without data augmentation in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the packages and data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Preprocess the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'A sample of images, along with their corresponding labels, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dff754a8-553a-49ad-bd16-b9b339e04ec1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Build and compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We have a higher learning rate only so that the model converges faster in fewer
    epochs. This enables a faster comparison of the non-data augmentation scenario
    with the data augmentation scenario. Ideally, we would let the model run for a
    greater number of epochs with a lesser learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The accuracy of this network is ~66%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92ccfaf3-5c22-4811-bab8-237bf334af4d.png)'
  prefs: []
  type: TYPE_IMG
- en: Model accuracy with data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following code, we will implement data augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `ImageDataGenerator` method in the `keras.preprocessing.image` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are generating new images where the images are randomly
    rotated between 0 to 20 degrees. A sample of images after being passed through
    the data generator is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a17f5da5-2d6d-4b24-b090-3399a1b49dbd.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the images are tilted slightly when compared to the previous set of
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will pass our total data through the data generator, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are rebuilding the model so that the weights are initialized one
    more time as we are comparing between a data augmentation and non-data augmentation
    scenario:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `fit_generator` method fits the model while generating new images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, `datagen.flow` specifies that new training data points need to
    be generated per the datagen strategy we initialized in step *1*. Along with this,
    we also specify the number of steps per epoch as the ratio of the total number
    of data points over the batch size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8ce964fc-7aec-48ba-a6a6-485f4850a457.png)'
  prefs: []
  type: TYPE_IMG
- en: The accuracy of this code is ~80%, which is better than the accuracy of 66%
    using just the given dataset (without data augmentation).
  prefs: []
  type: TYPE_NORMAL
