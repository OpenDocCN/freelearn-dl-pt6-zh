<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Other Important Deep Learning Libraries"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Other Important Deep Learning Libraries</h1></div></div></div><p>In this chapter, we'll talk about other deep learning libraries, especially libraries with programming languages other than Java. The following are the most famous, well-developed libraries:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Theano</li><li class="listitem" style="list-style-type: disc">TensorFlow</li><li class="listitem" style="list-style-type: disc">Caffe</li></ul></div><p>You'll briefly learn about each of them. Since we'll mainly implement them using Python here, you can skip this chapter if you are not a Python developer. All the libraries introduced in this chapter support GPU implementations and have other special features, so let's dig into them.</p><div class="section" title="Theano"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec35"/>Theano</h1></div></div></div><p>Theano was developed <a id="id484" class="indexterm"/>for deep learning, but it is not actually a deep learning library; it is a <a id="id485" class="indexterm"/>Python library for scientific computing. The documentation is available at <a class="ulink" href="http://deeplearning.net/software/theano/">http://deeplearning.net/software/theano/</a>. There are several characteristics introduced on the page such as the use of a GPU, but the most striking feature is that Theano supports <a id="id486" class="indexterm"/>
<span class="strong"><strong>computational differentiation</strong></span> or <a id="id487" class="indexterm"/>
<span class="strong"><strong>automatic differentiation</strong></span>, which ND4J, the Java scientific computing library, doesn't support. This means that, with Theano, we don't have to calculate the gradients of model parameters by ourselves. Theano automatically does this instead. Since Theano undertakes the most complicated parts of the algorithm, implementations of math expressions can be less difficult.</p><p>Let's see how Theano computes gradients. To begin with, we need to install Theano on the machine. Installation can be done just by using <code class="literal">pip install Theano</code> or <code class="literal">easy_install Theano</code>. Then, the following are the lines to import and use Theano:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import theano</strong></span>
<span class="strong"><strong>import theano.tensor as T</strong></span>
</pre></div><p>With Theano, all variables are processed as tensors. For example, we have <code class="literal">scalar</code>, <code class="literal">vector</code>, and <code class="literal">matrix</code>, <code class="literal">d</code> for double, <code class="literal">l</code> for long, and so on. Generic functions such as <code class="literal">sin</code>, <code class="literal">cos</code>, <code class="literal">log</code>, and <code class="literal">exp</code> are also defined under <code class="literal">theano.tensor</code>. Therefore, as shown previously, we often use the alias of tensor, <code class="literal">T</code>.</p><p>As a first step to briefly grasp Theano implementations, consider the very simple parabola curve. The implementation is saved in <code class="literal">DLWJ/src/resources/theano/1_1_parabola_scalar.py</code> so that you can reference it. First, we define <code class="literal">x</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>x = T.dscalar('x')</strong></span>
</pre></div><p>This definition is unique with Python because <code class="literal">x</code> doesn't have a value; it's just a symbol. In this case, <code class="literal">x</code> is <code class="literal">scalar</code> of the type <code class="literal">d</code> (double). Then we can define <code class="literal">y</code> and its gradient very intuitively. The implementation is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>y = x ** 2</strong></span>
<span class="strong"><strong>dy = T.grad(y, x)</strong></span>
</pre></div><p>So, <code class="literal">dy</code> should have <code class="literal">2x</code> within it. Let's check whether we can get the correct answers. What we need to do additionally is to register the <code class="literal">math</code> function with Theano:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>f = theano.function([x], dy)</strong></span>
</pre></div><p>Then you can easily <a id="id488" class="indexterm"/>compute the value of the gradients:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>print f(1)  # =&gt; 2.0</strong></span>
<span class="strong"><strong>print f(2)  # =&gt; 4.0</strong></span>
</pre></div><p>Very simple! This is the power of Theano. We have <code class="literal">x</code> of scalar here, but you can easily implement vector (and matrix) calculations as well just by defining <code class="literal">x</code> as:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>x = T.dvector('x')</strong></span>
<span class="strong"><strong>y = T.sum(x ** 2)</strong></span>
</pre></div><p>We won't go further here, but you can find the completed codes in <code class="literal">DLWJ/src/resources/theano/1_2_parabola_vector.py</code> and <code class="literal">DLWJ/src/resources/theano/1_3_parabola_matrix.py</code>.</p><p>When we consider <a id="id489" class="indexterm"/>implementing deep learning algorithms with Theano, we can find some very good examples on GitHub in <span class="emphasis"><em>Deep Learning Tutorials</em></span> (<a class="ulink" href="https://github.com/lisa-lab/DeepLearningTutorials">https://github.com/lisa-lab/DeepLearningTutorials</a>). In this chapter, we'll look at an overview of the standard MLP implementation <a id="id490" class="indexterm"/>so you understand more about Theano. The forked repository as a snapshot is available at <a class="ulink" href="https://github.com/yusugomori/DeepLearningTutorials">https://github.com/yusugomori/DeepLearningTutorials</a>. First, let's take a look at <code class="literal">mlp.py</code>. The model parameters of the hidden layer are the weight and bias:</p><div class="informalexample"><pre class="programlisting">W = theano.shared(value=W_values, name='W', borrow=True)
b = theano.shared(value=b_values, name='b', borrow=True)</pre></div><p>Both parameters are defined using <code class="literal">theano.shared</code> so that they can be accessed and updated through the model. The activation can be represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_07_04.jpg" alt="Theano"/></div><p>This denotes the activation function, that is, the hyperbolic tangent in this code. Therefore, the corresponding code is written as follows:</p><div class="informalexample"><pre class="programlisting">lin_output = T.dot(input, self.W) + self.b
self.output = (
    lin_output if activation is None
    else activation(lin_output)
)</pre></div><p>Here, linear activation is <a id="id491" class="indexterm"/>also supported. Likewise, parameters <code class="literal">W</code> and <code class="literal">b</code> of the output layer, that is, logistic regression layer, are defined and initialized in <code class="literal">logistic_sgd.py</code>:</p><div class="informalexample"><pre class="programlisting">self.W = theano.shared(
    value=numpy.zeros(
        (n_in, n_out),
        dtype=theano.config.floatX
    ),
    name='W',
    borrow=True
)

self.b = theano.shared(
    value=numpy.zeros(
        (n_out,),
        dtype=theano.config.floatX
    ),
    name='b',
    borrow=True
)</pre></div><p>The activation function of multi-class logistic regression is the <code class="literal">softmax</code> function and we can just write and define the output as follows:</p><div class="informalexample"><pre class="programlisting">self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)</pre></div><p>We can write the predicted values as:</p><div class="informalexample"><pre class="programlisting">self.y_pred = T.argmax(self.p_y_given_x, axis=1)</pre></div><p>In terms of training, since the equations of the backpropagation algorithm are computed from the loss function and its gradient, what we need to do is just define the function to be minimized, that is, the negative log likelihood function:</p><div class="informalexample"><pre class="programlisting">def negative_log_likelihood(self, y):
    return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])</pre></div><p>Here, the mean values, not the sum, are computed to evaluate across the mini-batch.</p><p>With these preceding <a id="id492" class="indexterm"/>values and definitions, we can implement MLP. Here again, what we need to do is define the equations and symbols of MLP. The following is an extraction of the code:</p><div class="informalexample"><pre class="programlisting">class MLP(object):
    def __init__(self, rng, input, n_in, n_hidden, n_out):
        # self.hiddenLayer = HiddenLayer(...)
        # self.logRegressionLayer = LogisticRegression(...)

        # L1 norm
        self.L1 = (
             abs(self.hiddenLayer.W).sum()
             + abs(self.logRegressionLayer.W).sum()
        )

        # square of L2 norm
        self.L2_sqr = (
           (self.hiddenLayer.W ** 2).sum()
            + (self.logRegressionLayer.W ** 2).sum()
        )

        # negative log likelihood of MLP
        self.negative_log_likelihood = (
           self.logRegressionLayer.negative_log_likelihood
        )

        # the parameters of the model
        self.params = self.hiddenLayer.params + self.logRegressionLayer.params</pre></div><p>Then you can build and train the model. Let's look at the code in <code class="literal">test_mlp()</code>. Once you load the dataset and construct MLP, you can evaluate the model by defining the cost:</p><div class="informalexample"><pre class="programlisting">cost = (
    classifier.negative_log_likelihood(y)
    + L1_reg * classifier.L1
    + L2_reg * classifier.L2_sqr
)</pre></div><p>With this cost, we get the gradients of the model parameters with just a single line of code:</p><div class="informalexample"><pre class="programlisting">gparams = [T.grad(cost, param) for param in classifier.params]</pre></div><p>The following is the equation to update the parameters:</p><div class="informalexample"><pre class="programlisting">updates = [
    (param, param - learning_rate * gparam)
    for param, gparam in zip(classifier.params, gparams)
]</pre></div><p>The code in the first <a id="id493" class="indexterm"/>bracket follows this equation:</p><div class="mediaobject"><img src="graphics/B04779_07_05.jpg" alt="Theano"/></div><p>Then, finally, we define the actual function for the training:</p><div class="informalexample"><pre class="programlisting">train_model = theano.function(
    inputs=[index],
    outputs=cost,
    updates=updates,
    givens={
        x: train_set_x[index * batch_size: (index + 1) * batch_size],
        y: train_set_y[index * batch_size: (index + 1) * batch_size]
    }
)</pre></div><p>Each indexed input and label corresponds to <code class="literal">x</code>, <code class="literal">y</code> in <span class="emphasis"><em>givens</em></span>, so when <code class="literal">index</code> is given, the parameters are updated with <code class="literal">updates</code>. Therefore, we can train the model with iterations of training epochs and mini-batches:</p><div class="informalexample"><pre class="programlisting">while (epoch &lt; n_epochs) and (not done_looping):
    epoch = epoch + 1
        for minibatch_index in xrange(n_train_batches):
           minibatch_avg_cost = train_model(minibatch_index)</pre></div><p>The original code has the test and validation part, but what we just mentioned is the rudimentary structure. With Theano, equations of gradients will no longer be derived.</p></div></div>
<div class="section" title="TensorFlow"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec36"/>TensorFlow</h1></div></div></div><p>TensorFlow is the <a id="id494" class="indexterm"/>library for machine learning and deep learning developed by <a id="id495" class="indexterm"/>Google. The project page is <a class="ulink" href="https://www.tensorflow.org/">https://www.tensorflow.org/</a> and all the code is open to the public on GitHub at <a class="ulink" href="https://github.com/tensorflow/tensorflow">https://github.com/tensorflow/tensorflow</a>. TensorFlow itself is written with C++, but it provides a Python and C++ API. We focus on Python implementations in this book. The installation can be done with <code class="literal">pip</code>, <code class="literal">virtualenv</code>, or <code class="literal">docker</code>. The installation guide is available at <a class="ulink" href="https://www.tensorflow.org/versions/master/get_started/os_setup.html">https://www.tensorflow.org/versions/master/get_started/os_setup.html</a>. After the installation, you can <a id="id496" class="indexterm"/>import and use TensorFlow by writing the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import tensorflow as tf</strong></span>
</pre></div><p>TensorFlow recommends you implement deep learning code with the following three parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">inference()</code>: This makes predictions using the given data, which defines the model structure</li><li class="listitem" style="list-style-type: disc"><code class="literal">loss()</code>: This returns the error values to be optimized</li><li class="listitem" style="list-style-type: disc"><code class="literal">training()</code>: This applies the actual training algorithms by computing gradients</li></ul></div><p>We'll follow this guideline. A tutorial on MNIST classifications for beginners is introduced on <a class="ulink" href="https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html">https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html</a> and the code for this tutorial can be found in <code class="literal">DLWJ/src/resources/tensorflow/1_1_mnist_simple.py</code>. Here, we consider refining the code introduced in the tutorial. You can see all the code in <code class="literal">DLWJ/src/resources/tensorflow/1_2_mnist.py</code>.</p><p>First, what we have to <a id="id497" class="indexterm"/>consider is fetching the MNIST data. Thankfully, TensorFlow also provides the code to fetch the data in <a class="ulink" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py</a> and we put the code into the same directory. Then, by writing the following code, you can import the MNIST data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import input_data</strong></span>
</pre></div><p>MNIST data can be imported using the following code:</p><div class="informalexample"><pre class="programlisting">mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)</pre></div><p>Similar to Theano, we define the variable with no actual values as the placeholder:</p><div class="informalexample"><pre class="programlisting">x_placeholder = tf.placeholder("float", [None, 784])
label_placeholder = tf.placeholder("float", [None, 10])</pre></div><p>Here, <code class="literal">784</code> is the number of units in the input layer and <code class="literal">10</code> is the number in the output layer. We do this because the values in the placeholder change in accordance with the mini-batches. Once you define the placeholder you can move on to the model building and training. We set the non-linear activation with the <code class="literal">softmax</code> function in <code class="literal">inference()</code> here:</p><div class="informalexample"><pre class="programlisting">def inference(x_placeholder):

    W = tf.Variable(tf.zeros([784, 10]))
    b = tf.Variable(tf.zeros([10]))

    y = tf.nn.softmax(tf.matmul(x_placeholder, W) + b)

    return y</pre></div><p>Here, <code class="literal">W</code> and <code class="literal">b</code> are the <a id="id498" class="indexterm"/>parameters of the model. The <code class="literal">loss</code> function, that is, the <code class="literal">cross_entropy</code> function, is defined in <code class="literal">loss()</code> as follows:</p><div class="informalexample"><pre class="programlisting">def loss(y, label_placeholder):
    cross_entropy = - tf.reduce_sum(label_placeholder * tf.log(y))

    return cross_entropy</pre></div><p>With the definition of <code class="literal">inference()</code> and <code class="literal">loss()</code>, we can train the model by writing the following code:</p><div class="informalexample"><pre class="programlisting">def training(loss):
    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)

    return train_step</pre></div><p>
<code class="literal">GradientDescentOptimizer()</code> applies the gradient descent algorithm. But be careful, as this method just defines the method of training and the actual training has not yet been executed. TensorFlow also supports <code class="literal">AdagradOptimizer()</code>, <code class="literal">MemontumOptimizer()</code>, and other major optimizing algorithms.</p><p>The code and methods explained previously are to define the model. To execute the actual training, you need to initialize a session of TensorFlow:</p><div class="informalexample"><pre class="programlisting">init = tf.initialize_all_variables()
sess.run(init)</pre></div><p>Then we train the model with mini-batches. All the data in a mini-batch is stored in <code class="literal">feed_dict</code> and then used in <code class="literal">sess.run()</code>:</p><div class="informalexample"><pre class="programlisting">for i in range(1000):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    feed_dict = {x_placeholder: batch_xs, label_placeholder: batch_ys}

    sess.run(train_step, feed_dict=feed_dict)</pre></div><p>That's it for the model training. It's very simple, isn't it? You can show the result by writing the following code:</p><div class="informalexample"><pre class="programlisting">def res(y, label_placeholder, feed_dict):
    correct_prediction = tf.equal(
        tf.argmax(y, 1), tf.argmax(label_placeholder, 1)
    )

    accuracy = tf.reduce_mean(
        tf.cast(correct_prediction, "float")
    )

   print sess.run(accuracy, feed_dict=feed_dict)</pre></div><p>
<code class="literal">TensorFlow</code> makes it <a id="id499" class="indexterm"/>super easy to implement deep learning and it is very useful. Furthermore, <code class="literal">TensorFlow</code> has another powerful feature, <code class="literal">TensorBoard</code>, to visualize deep learning. By adding a few lines of code to the previous code snippet, we can use this useful feature.</p><p>Let's see how the model is visualized first. The code is in <code class="literal">DLWJ/src/resources/tensorflow/1_3_mnist_TensorBoard.py</code>, so simply run it. After you run the program, type the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ tensorboard --logdir=&lt;ABOSOLUTE_PATH&gt;/data</strong></span>
</pre></div><p>Here, <code class="literal">&lt;ABSOLUTE_PATH&gt;</code> is the absolute path of the program. Then, if you access <code class="literal">http://localhost:6006/</code> in your browser, you can see the following page:</p><div class="mediaobject"><img src="graphics/B04779_07_01.jpg" alt="TensorFlow"/></div><p>This shows the process of the value of <code class="literal">cross_entropy</code>. Also, when you click <span class="strong"><strong>GRAPH</strong></span> in the header menu, you <a id="id500" class="indexterm"/>see the visualization of the model:</p><div class="mediaobject"><img src="graphics/B04779_07_02.jpg" alt="TensorFlow"/></div><p>When you click on <span class="strong"><strong>inference</strong></span> on the page, you can see the model structure:</p><div class="mediaobject"><img src="graphics/B04779_07_03.jpg" alt="TensorFlow"/></div><p>Now let's look inside the code. To enable visualization, you need to wrap the whole area with the scope: <span class="emphasis"><em>with</em></span> <code class="literal">tf.Graph().as_default()</code>. By adding this scope, all the variables declared in the scope will be displayed in the graph. The displayed name can be set by including the <code class="literal">name</code> label as follows:</p><div class="informalexample"><pre class="programlisting">x_placeholder = tf.placeholder("float", [None, 784], name="input")
label_placeholder = tf.placeholder("float", [None, 10], name="label")</pre></div><p>Defining other scopes will <a id="id501" class="indexterm"/>create nodes in the graph and this is where the division, <code class="literal">inference()</code>, <code class="literal">loss()</code>, and <code class="literal">training()</code> reveal their real values. You can define the respective scope without losing any readability:</p><div class="informalexample"><pre class="programlisting">def inference(x_placeholder):
    with tf.name_scope('inference') as scope:
        W = tf.Variable(tf.zeros([784, 10]), name="W")
        b = tf.Variable(tf.zeros([10]), name="b")

        y = tf.nn.softmax(tf.matmul(x_placeholder, W) + b)

    return y

def loss(y, label_placeholder):
    with tf.name_scope('loss') as scope:
        cross_entropy = - tf.reduce_sum(label_placeholder * tf.log(y))

        tf.scalar_summary("Cross Entropy", cross_entropy)

    return cross_entropy

def training(loss):
    with tf.name_scope('training') as scope:
        train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)

    return train_step</pre></div><p>
<code class="literal">tf.scalar_summary()</code> in <code class="literal">loss()</code> makes the variable show up in the <span class="strong"><strong>EVENTS</strong></span> menu. To enable visualization, we need the following code:</p><div class="informalexample"><pre class="programlisting">summary_step = tf.merge_all_summaries()
init = tf.initialize_all_variables()

summary_writer = tf.train.SummaryWriter('data', graph_def=sess.graph_def)</pre></div><p>Then the process of variables can be added with the following code:</p><div class="informalexample"><pre class="programlisting">summary = sess.run(summary_step, feed_dict=feed_dict)
summary_writer.add_summary(summary, i)</pre></div><p>This feature of visualization will <a id="id502" class="indexterm"/>be much more useful when we're using more complicated models.</p></div>
<div class="section" title="Caffe"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec37"/>Caffe</h1></div></div></div><p>Caffe is a library famous for <a id="id503" class="indexterm"/>its speed. The official project page is <a class="ulink" href="http://caffe.berkeleyvision.org/">http://caffe.berkeleyvision.org/</a> and the GitHub page is <a class="ulink" href="https://github.com/BVLC/caffe">https://github.com/BVLC/caffe</a>. Similar to TensorFlow, Caffe has been developed mainly with C++, but it provides a Python and MATLAB API. In addition, what is unique to Caffe is that you don't need any programming experience, you just write the configuration or protocol files, that is <code class="literal">.prototxt</code> files, to perform experiments and research with deep learning. Here, we focus on the protocol-based approach.</p><p>Caffe is a very powerful library that enables quick model building, training, and testing; however, it's a bit difficult <a id="id504" class="indexterm"/>to install the library to get a lot of benefits from it. As you can see from the installation guide at <a class="ulink" href="http://caffe.berkeleyvision.org/installation.html">http://caffe.berkeleyvision.org/installation.html</a>, you need to install the following in advance:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">CUDA</li><li class="listitem" style="list-style-type: disc">BLAS (ATLAS, MKL, or OpenBLAS)</li><li class="listitem" style="list-style-type: disc">OpenCV</li><li class="listitem" style="list-style-type: disc">Boost</li><li class="listitem" style="list-style-type: disc">Others: snappy, leveldb, gflags, glog, szip, lmdb, protobuf, and hdf5</li></ul></div><p>Then, clone the repository from the GitHub page and create the <code class="literal">Makefile.config</code> file from <code class="literal">Makefile.config.example</code>. You may need Anaconda, a Python distribution, beforehand to run the <code class="literal">make</code> command. You can download this from <a class="ulink" href="https://www.continuum.io/downloads">https://www.continuum.io/downloads</a>. After you run the <code class="literal">make</code>, <code class="literal">make test</code>, and <code class="literal">make runtest</code> commands (you may want to run the commands with a <code class="literal">-jN</code> option such as <code class="literal">make -j4</code> or <code class="literal">make -j8</code> to speed up the process) and pass the test, you'll see the power of Caffe. So, let's look at an example. Go to <code class="literal">$CAFFE_ROOT</code>, the path where you cloned the repository, and type the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./data/mnist/get_mnist.sh</strong></span>
<span class="strong"><strong>$ ./examples/mnist/train_lenet.sh</strong></span>
</pre></div><p>That's all you need to solve the standard MNIST classification problem with CNN. So, what happened here? When you have a look at <code class="literal">train_lenet.sh</code>, you will see the following:</p><div class="informalexample"><pre class="programlisting">#!/usr/bin/env sh

./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt</pre></div><p>It simply runs the <code class="literal">caffe</code> command with the protocol file <code class="literal">lenet_solver.prototxt</code>. This file configures the hyper parameters of the model such as the learning rate and the momentum. The file also <a id="id505" class="indexterm"/>references the network configuration file, in this case, <code class="literal">lenet_train_test.prototxt</code>. You can define each layer with a JSON-like description:</p><div class="informalexample"><pre class="programlisting">layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}</pre></div><p>So, basically, the protocol file is divided into two parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Net</strong></span>: This <a id="id506" class="indexterm"/>defines the detailed structure of the model and gives a description of each layer, hence whole neural networks</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Solver</strong></span>: This defines the optimization settings such as the use of a CPU/GPU, the number of iterations, and the hyper parameters of the model such as the learning rate</li></ul></div><p>Caffe can be a great tool when you need to apply deep learning to a large dataset with principal approaches.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec38"/>Summary</h1></div></div></div><p>In this chapter, you learned how to implement deep learning algorithms and models using Theano, TensorFlow, and Caffe. All of them have special and powerful features and each of them is very useful. If you are interested in other libraries and frameworks, you can <a id="id507" class="indexterm"/>have <span class="emphasis"><em>Chainer</em></span> (<a class="ulink" href="http://chainer.org/">http://chainer.org/</a>), <span class="emphasis"><em>Torch</em></span> (<a class="ulink" href="http://torch.ch/">http://torch.ch/</a>), <a id="id508" class="indexterm"/>
<span class="emphasis"><em>Pylearn2</em></span> (<a class="ulink" href="http://deeplearning.net/software/pylearn2/">http://deeplearning.net/software/pylearn2/</a>), <span class="emphasis"><em>Nervana</em></span> (<a class="ulink" href="http://neon.nervanasys.com/">http://neon.nervanasys.com/</a>), and <a id="id509" class="indexterm"/>so on. You can also reference some <a id="id510" class="indexterm"/>benchmark tests (<a class="ulink" href="https://github.com/soumith/convnet-benchmarks">https://github.com/soumith/convnet-benchmarks</a> and <a class="ulink" href="https://github.com/soumith/convnet-benchmarks/issues/66">https://github.com/soumith/convnet-benchmarks/issues/66</a>) when you actually consider building your application with one of the libraries mentioned earlier.</p><p>Throughout this book, you learned the fundamental theories and algorithms of machine learning and deep learning and how deep learning is applied to study/business fields. With the knowledge and techniques you've acquired here, you should be able to cope with any problems that confront you. While it is true that you still need more steps to realize AI, you now have the greatest opportunity to achieve innovation.</p></div></body></html>