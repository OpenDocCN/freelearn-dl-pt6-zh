- en: '*Chapter 4*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Foundations of Convolutional Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Describe the inspiration for CNNs in neural science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the convolution operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe a basic CNN architecture for a classification task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement a simple CNN for image and text classification tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement a CNN for a sentiment analysis of text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we aim to cover the architecture of convolutional neural networks
    (CNNs) and gain an intuition of CNNs based on their applications on image data,
    before delving into their applications in natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks, as a broad field, borrow a lot from biological systems, particularly
    the brain. Advances in neural science have directly influenced research in to
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are inspired by the work of two neural scientists, D.H. Hubel and T.N.
    Wiesel. Their research focused on the mammalian visual cortex, which is the part
    of the brain responsible for vision. Through their research back in the sixties,
    they found that the visual cortex is composed of layers of neurons. Furthermore,
    these layers are arranged in a hierarchical structure. This hierarchy ranges from
    simple-to hypercomplex neurons. They also advanced the notion of a 'receptive
    field,' which is the space within which certain stimuli activate or fire a neuron,
    with a degree of spatial invariance. Spatial or shift invariance allows animals
    to detect objects regardless of whether they are rotated, scaled, transformed,
    or partially obscured.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Examples of spatial variance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.1: Examples of spatial variance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Inspired by neural concepts of how animals see, computer vision scientists have
    modelled neural networks that adhere to the same principles of locality, hierarchy,
    and spatial invariance. We will dive deeper into the architecture of CNNs in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are a subset of neural networks that contain one or more 'convolution'
    layers. Typical neural networks are fully connected, which means every neuron
    is connected to every neuron in the next layer. When dealing with high-dimensional
    data such as images, sound, and so on, typical neural networks are slow and tend
    to overfit as there are too many weights being learned. Convolutional layers solve
    this problem by connecting a neuron to a region of the input in lower layers.
    We will discuss convolution layers in greater detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the general architecture of CNNs, we will first apply them to
    the task of image classification and then, subsequently, to natural language processing.
    To begin, we'll do a small exercise to understand how computers see images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 18: Finding Out How Computers See Images'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Images and text share an important similarity. The location of a pixel in an
    image, or a word in text, matters. This spatial significance makes applying convolutional
    neural networks possible for both text and images.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we want to determine how computers interpret images. We will
    do this by using the **MNIST** dataset, which contains a repository of handwritten
    digits perfect for demonstrating CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MNIST is a built-in Keras dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to have both Python and Keras installed. For easier visualization,
    you can run your code in a Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the necessary classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since we''ll be using this dataset throughout the chapter, we will import the
    training and test sets as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the first image in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the preceding code should result in an image being visualized, as shown
    here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2: Visualization of an image](img/C13783_04_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.2: Visualization of an image'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The images are 28 by 28 pixels, with each pixel being a number between 0 and
    255\. Try playing around with different indices to display their values as follows.
    You can do this by putting arbitrary numbers between `0` and `255` as `x` and
    `y` in:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When you run the print code as follows, expect to see numbers between 0 and
    255:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected Output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3: Numerical representation of an image](img/C13783_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Numerical representation of an image'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This exercise is meant to help you appreciate how image data is processed with
    each pixel as a number between **0** and **255**. This understanding is essential
    as we'll feed these images into a CNN as input in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Architecture of a CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's assume we have the task of classifying each of the **MNIST** images as
    a number between 0 and 9\. The input in the previous example is an image matrix.
    For a colored image, each pixel is an array with three values corresponding to
    the **RGB** color scheme. For grayscale images, each pixel is just one number,
    as we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the architecture of a CNN, it is best to separate it into two
    sections as visualized in the image that follows.
  prefs: []
  type: TYPE_NORMAL
- en: A forward pass of the CNN involves a set of operations in the two sections.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: Application of convolution and ReLU operations](img/C13783_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Application of convolution and ReLU operations'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The figure is explained in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first section of a CNN is all about feature extraction. Conceptually, it
    can be interpreted as the model's attempt to learn which features distinguish
    one class from another. In the task of classifying images, these features might
    include unique shapes and colors.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs learn the hierarchical structure of these features. The lower layers of
    a CNN abstract features such as edges, while the higher layers learn more defined
    features such as shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature learning occurs through a set of three operations repeated a number
    of times, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An activation function (the application of the ReLU activation function to achieve
    non-linearity)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolution is the an operation that distinguishes CNNs from other neural networks.
    The convolution operation is not unique to machine learning; it is applied in
    many other fields, such as electrical engineering and signal processing.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution can be thought of as looking through a small window as we move the
    window to the right and down. Convolution, in this context, involves iteratively
    sliding a "filter" across an image, while applying a dot product as we move left
    and down.
  prefs: []
  type: TYPE_NORMAL
- en: 'This window is called a "*filter*" or a "*kernel*". In the actual sense, a
    filter or kernel is a matrix of preferably smaller dimensions than the input.
    To better understand how filters are applied to images, consider the following
    example. After calculating the dot product on the area covered by the filter,
    we take a step to the right and calculate the dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5: Filter application to images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_04_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.5: Filter application on images'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The result of this convolution is known as a feature map or an activation map.
  prefs: []
  type: TYPE_NORMAL
- en: The size of the filter needs to be defined as a hyperparameter. This size can
    also be considered the area for which a neuron can "see" the input. This is called
    a neuron's *receptive field*. Additionally, we need to define the stride size,
    that is, the number of steps we need to take before applying the filter. Pixels
    at the center have the filters passing through several times compared with those
    at the edges. To avoid losing information at the corners, it is advisable to add
    an extra layer of zeros as padding.
  prefs: []
  type: TYPE_NORMAL
- en: The ReLU Activation Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Activation functions are used all across machine learning. They are useful for
    introducing non-linearity and allowing the a model to learn non-linear functions.
    In this particular context, we apply the **Rectified Linear Unit** (**ReLU**).
    It basically replaces all the negative values with zero.
  prefs: []
  type: TYPE_NORMAL
- en: The following image demonstrates the change in an image after ReLU is applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: Image after applying ReLU function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_04_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.6: Image after applying ReLU function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 19: Visualizing ReLU'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise we will visualize the Rectified Linear Unit function. The ReLU
    function will be plotted on an X-Y axis, where X is numbers in the range of -15
    to 15 and Y is the output after applying the ReLU function. The goal of this exercise
    is to visualize ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required Python packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the ReLU function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the input and output references:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the input against the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected Output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.7: Graph plot for ReLU'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.7: Graph plot for ReLU'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pooling is a downsampling process that involves reducing dimensionality from
    a higher to a lower dimensional space. In machine learning, pooling is applied
    as a way to reduce the spatial complexity of the layers. This allows for fewer
    weights to be learned and consequently faster training times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Historically, different techniques have been used to perform pooling, such
    as average pooling and L2-norm pooling. The most preferred pooling technique is
    max pool. Max pooling involves taking the largest element within a defined window
    size. The following is an example of max pooling on a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8: Max pool'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_04_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.8: Max pool'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we apply max pooling to the preceding example, the section that has 2, 6,
    3, and 7 is reduced to 7\. Similarly, the section with 1, 0, 9, and 2 is reduced
    to 9\. With max pooling, we pick the largest number in a section.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A common problem encountered in machine learning is overfitting. Overfitting
    occurs when a model "memorizes" the training data and is unable to generalize
    when presented with different examples in testing. There are several ways to avoid
    overfitting, particularly through regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9: Regularization](img/C13783_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Regularization'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Regulation is the process of constraining coefficients toward zero. Regularization
    can be summarized as techniques used to penalize learned coefficients so that
    they tend towards zero. Dropout is a common regularization technique that is applied
    by randomly "dropping" some neurons during both the forward and backward passes.
    To implement dropout, we specify the probability of a neuron being dropped as
    a parameter. By randomly dropping neurons, we ensure that the model is able to
    generalize better and therefore be a little more flexible.
  prefs: []
  type: TYPE_NORMAL
- en: Classification in Convolutional Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second section of a CNN is more task-specific. For the task of classification,
    this section is basically a fully connected neural network. A neural network is
    regarded as fully connected when every neuron in one layer is connected to all
    the neurons in the next layer. The input to the fully connected layer is a flattened
    vector that is the output of section one. Flattening converts the matrix into
    a 1D vector.
  prefs: []
  type: TYPE_NORMAL
- en: The number of hidden layers in the fully connected layer is a hyperparameter
    that can be optimized and fine-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 20: Creating a Simple CNN Architecture'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, you will construct a simple CNN model using Keras. This exercise
    will entail creating a model with the layers discussed so far. In the first section
    of the model, we will have two convolutional layers with the ReLU activation function,
    a pooling layer, and a dropout layer. In the second section, we will have a flattened
    layer and a fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the necessary classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define the variables used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now define the model. Keras''s Sequential model allows you to stack
    layers as you go:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now add section one layers. The convolution and ReLU layers are defined
    together. We have two convolutional layers. We define a kernel size of 3 for each.
    The first layer of the model receives the input. We need to define how it should
    expect that input to be structured. In our case, the input is in the form of 28
    by 28 images. We also need to specify the number of neurons for each layer. In
    our case, we define 64 neurons for the first layer and 32 neurons for the second
    layer. Please note that these are hyperparameters that can be optimized:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then add a pooling layer, followed by a dropout layer with a 25% probability
    of neurons being ''dropped'':'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The section one layers are done. Please note that the number of layers is also
    a hyperparameter that can be optimized.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For section two, we first flatten the input. We then add a fully connected
    or dense layer. Using the softmax activation function, we can calculate the probability
    for each of the 10 classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To visualize the model architecture so far, we can print out the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected Output**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.10: Model summary](img/C13783_04_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.10: Model summary'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can also run the following code to export the image to a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.11: Visualized architecture of a simple CNN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_04_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.11: Visualized architecture of a simple CNN'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding exercise, we created a simple CNN with two convolutional layers
    for the task of classification. In the preceding output image, you'll notice how
    the layers are stacked – starting from the input layer, then the two convolutional
    layers, the pooling, dropout, and flattening layers, and the fully connected layer
    at the end.
  prefs: []
  type: TYPE_NORMAL
- en: Training a CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During the training of a CNN, the model tries to learn the weights of the filters
    in feature extraction and the weights at the fully connected layers in the neural
    network. To understand how a model is trained, we''ll discuss how the probability
    of each output class is calculated, how we calculate the error or the loss, and
    finally, how we optimize or minimize that loss while updating the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: Probabilities
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Recall that in the last layer of the neural network section, we used a softmax
    function to calculate the probability of each output class. This probability is
    calculated by dividing the exponent of that class score by the sum of the exponents
    of all scores:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.12: Expression to calculate probability](img/C13783_04_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.12: Expression to calculate probability'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to be able to quantify how well the calculated probabilities predict
    the actual class. This is done by calculating a loss, which in the case of classification
    probability is best done through the categorical cross-entropy loss function.
    The categorical cross-entropy loss function takes in two vectors, the predicted
    classes (let''s call that y'') and the actual classes (say y), and outputs the
    overall loss. Cross-entropy loss is calculated as the sum of the negative log
    likelihoods of the class probabilities. It can be represented as the H function
    here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13783_04_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.13: Expression to calculate loss'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider the sketch of cross-entropy loss that follows. By minimizing the loss,
    we can predict the correct class with a higher probability:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.14: Cross-entropy loss vs. predicted probability](img/C13783_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.14: Cross-entropy loss versus predicted probability'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Gradient descent is an optimization algorithm for finding the minimum of a function,
    such as the loss function described earlier. Although the overall error is calculated,
    we need to go back and calculate how much of that loss was contributed by each
    node. Consequently, we can update the weights, so as to minimize the overall error.
    Backpropagation applies the chain rule of calculus to calculate the update for
    each weight. This is done by taking the partial derivative of the error or loss
    relative to the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better visualize these steps, consider the following diagram, which summarizes
    the three steps. For the classification task, the first step involves the calculation
    of probabilities for each output class. We then apply a loss function to quantify
    how well the probabilities predict the actual class. In order to make a better
    prediction going forward, we then update our weights by performing backpropagation
    through gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15: Steps for task of classification'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_04_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.15: Steps for the classification task'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 21: Training a CNN'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will train the model we created in exercise 20\. The following
    steps will help you with the solution. Recall that this is for the overall task
    of classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining the number of epochs. An epoch is a common hyperparameter
    used in deep neural networks. One epoch is when the entire dataset is passed through
    a complete forward and backward pass. As training data is usually a lot, data
    can be divided into several batches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Recall that we imported the MNIST dataset by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We first reshape the data to fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The to_categorical function changes a vector of integers to a matrix of one-hot
    encoded vectors. Given the following example, the function returns the array shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The array would be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.16: Array output](img/C13783_04_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.16: Array output'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We apply it to the target column as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define the loss function as a categorical cross-entropy loss function.
    Additionally, we define the optimizer and the metrics. The Adam(Adaptive Moment)
    optimizer is an optimization algorithm often used in place of stochastic gradient
    descent. It defines an adaptive learning rate for each parameter of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To train the model, run the .fit method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.17: Training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_04_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.17: Training the model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To evaluate the model''s performance, you can run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For this task, we expect a fairly high accuracy after a number of epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.18: Accuracy and loss output](img/C13783_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: Accuracy and loss output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Applying CNNs to Text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have a general intuition of how CNNs work using images, let's look
    at how they can be applied in natural language processing. Just like images, text
    has spatial qualities that make it ideal for CNN usage. However, there is one
    main change to the architecture that we introduce when dealing with text. Instead
    of having two-dimensional convolutional layers, text is one-dimensional, as shown
    here.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19: One-dimensional convolution](img/C13783_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.19: One-dimensional convolution'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is important to note that the preceding input sequence can be either the
    character sequence or the word sequence. The application of CNNs on text, at the
    character level, can be visualized as shown in the following figure. CNNs have
    6 convolutional layers and 3 fully connected layers as shown here.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20: CNN with 6 convolutional and 3 fully connected layers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_04_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.20: CNN with 6 convolutional and 3 fully connected layers'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Character-level CNNs were shown to perform well when applied to large noisy
    data. They are also simpler than word-level applications because they require
    no preprocessing (such as stemming) and the characters are represented as one-hot
    encoding representations.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will demonstrate the application of CNNs to text
    at a word level. We will therefore need to perform some vectorization and padding
    before feeding the data into the CNN architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 22: Application of a Simple CNN to a Reuters News Topic for Classification'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will be applying a CNN model to the built-in Keras Reuters
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you are using Google Colab, you need to downgrade your version of `numpy`
    to 1.16.2 by running
  prefs: []
  type: TYPE_NORMAL
- en: '`!pip install numpy==1.16.1`'
  prefs: []
  type: TYPE_NORMAL
- en: '`import numpy as np`'
  prefs: []
  type: TYPE_NORMAL
- en: This downgrade is necessary since this version of `numpy` has the default value
    of `allow_pickle` as `True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the necessary classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the Reuters dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize the input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train and evaluate the model. Print the accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.21: Accuracy score](img/C13783_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.21: Accuracy score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We have thus created a model and trained it on a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Application Areas of CNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we understand the architecture of CNNs, let's look at some applications.
    In general, CNNs are great for data that has a spatial structure. Examples of
    types of data that has a spatial structure are sound, images, video, and text.
  prefs: []
  type: TYPE_NORMAL
- en: In natural language processing, CNNs are used for various tasks such as sentence
    classification. One example is the task of sentiment classification, where a sentence
    is classified as belonging to a predetermined group of classes.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, CNNs are applied at the character level to classification
    tasks such as sentiment classification, especially on noisy datasets such as social
    media posts.
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs are more commonly applied in computer vision. Here are some applications
    in this area:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Facial recognition*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most social networking sites employ CNNs to detect faces and subsequently perform
    tasks such as tagging.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.22: Facial recognition](img/C13783_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.22: Facial recognition'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Object detection*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, CNNs are able to detect objects in images. There are several CNN-based
    architectures that are used to detect objects, one of the most popular being R-CNN.
    (R-CNN stands for Region CNN.) An R-CNN works by applying a selective search to
    come up with regions and subsequently use using CNNs to perform classification,
    one region at a time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.23: Object detection'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_04_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.23: Object detection'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Image captioning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This task involves creating a textual description for an image. One way to perform
    image captioning is to replace the fully connected layer in section two with a
    recurrent neural network (RNN).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.24: Image captioning'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_04_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.24: Image captioning'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Semantic segmentation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic segmentation is the task of segmenting an image into more meaningful
    parts. Each pixel in an image is classified as belonging to a class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.25: Semantic segmentation](img/C13783_04_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.25: Semantic segmentation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'An architecture that can be used to perform semantic segmentation is a **Fully**
    **Convoluted** **Network** (**FCN**). The architecture of FCNs is slightly different
    from the preceding one in two ways: it has no fully connected layer and it has
    upsampling. Upsampling is the process of making the output image larger preferably
    the same size as the input image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.26: Sample architecture of semantic segmentation](img/C13783_04_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.26: Sample architecture of semantic segmentation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For more on FCNs, refer to the paper by Jonathan Long, Evan Shelhamer, and Trevor
    Darrell titled Fully Convolutional Networks for Semantic Segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 5: Sentiment Analysis on a Real-life Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine that you are tasked with creating a model to classify the reviews from
    a dataset. In this activity, we will build a CNN that performs the binary classification
    task of sentiment analysis. We will be using a real-life dataset from UCI's repository.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This dataset is downloaded from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences
  prefs: []
  type: TYPE_NORMAL
- en: 'From Group to Individual Labels using Deep Features, Kotziaa et al., KDD 2015
    UCI machine learning Repository [http://archive.ics.uci.edu.ml]. Irvine, CA: University
    of California, School of Information and Computer Science'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also download it from our GitHub repository link:'
  prefs: []
  type: TYPE_NORMAL
- en: https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2004
  prefs: []
  type: TYPE_NORMAL
- en: The following steps will help you with the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Download the Sentiment Labelled Sentences dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a directory labelled 'data' within your working directory and unzip the
    downloaded folder within the directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create and run your working script (for example, sentiment.ipynb) on Jupyter
    Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import your data using pandas read_csv method. Feel free to use one or all of
    the files in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split your data into training and test sets by using scikit learn's train_test_split.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize using Keras's tokenizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the text into sequences using the texts_to_sequences method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that all sequences have the same length by padding them. You can use
    Keras's pad_sequences function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the model with a minimum of one convolutional layer and one fully connected
    layer. As this is a binary classification, we use a sigmoid activation function
    and calculate the loss through binary cross-entropy loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train and test the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for the activity can be found on page 305.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.27: Accuracy scores](img/C13783_04_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.27: Accuracy scores'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we studied the architecture and applications of convolutional
    neural networks (CNNs). CNNs are applied not just to text and images but also
    to datasets that have some form of spatial structure. In the upcoming chapters,
    you will explore how to apply other forms of neural networks to various natural
    language tasks.
  prefs: []
  type: TYPE_NORMAL
