
# Understanding the Structure of a Sentences

In this chapter, we'll explore the basic concepts of NLP. This chapter is the most important chapter, as it helps to make your foundation strong.

We are going to cover the following topics to improve your understanding of the basic NLP concepts, which will help understand the next chapter:

*   Understanding the components of NLP
*   What is context-free grammar?
*   Morphological analysis
*   Lexical analysis
*   Syntactic analysis
*   Semantic analysis
*   Handling ambiguity
*   Discourse integration
*   Pragmatic analysis

# Understanding components of NLP

There are two major components of NLP. We are going to understand both of them.

# Natural language understanding

Let's learn about natural language understanding:

*   **Natural language understanding** (**NLU**) is considered the first component of NLP
*   NLU is considered an **Artificial Intelligence**-**Hard** (**AI**-**Hard**) problem or **Artificial Intelligence**-**Complete** (**AI-Complete**) problem
*   NLU is considered an AI-Hard problem because we are trying to make a computer as intelligent as a human
*   NLU is hard, but nowadays, tech giants and research communities are improvising traditional Machine Learning algorithms and applying various types of deep neural network that will help to achieve the goal (computers can also have the intelligence to process **natural language** (**NL**))
*   NLU is defined as the process of converting NL input into useful a representation by using computational linguistics tools
*   NLU requires the following analysis to convert NL into a useful representation:
    *   Morphological analysis
    *   Lexical analysis
    *   Syntactic analysis
    *   Semantic analysis
    *   Handling ambiguity
    *   Discourse integration
    *   Pragmatic analysis

In this book, we will focus on NLU and develop an NLP-based system that uses NLU representation.

# Natural language generation

Let's learn about **natural language generation** (**NLG**):

*   NLG is considered the second component of NLP.
*   NLG is defined as the process of generating NL by a machine as output.
*   The output of the machine should be in a logical manner, meaning, whatever NL is generated by the machine should be logical.
*   In order to generate logical output, many NLG systems use basic facts or knowledge-based representation.
*   Let's take an example. You have a system that writes an essay on a particular topic. If I am instructing my machine to generate 100 words on the topic of **The Cows**, and my machine generates 100 words on the topic of cows, then the output (here, 100 words about cows) generated by the machine should be in form of valid sentences, all sentences should be logically correct, and the context should also make sense.

# Differences between NLU and NLG

In this section, we are looking at the differences between NLU and NLG:

| **NLU** | **NLG** |
| This component helps to explain the meaning behind the NL, whether it is written text or in speech format. We can analyze English, French, Spanish, Hindi, or any other human language. | This component helps to generate the NL using machines. |
| NLU generates facts from NL by using various tools and techniques, such as POS tagger, parsers, and so on, in order to develop NLP applications. | NLG start from facts like POS tags, parsing results, and so on to generate the NL. |
| It is the process of reading and interpreting language. | It is the process of writing or generating language. |

# Branches of NLP

NLP involves two major branches that help us to develop NLP applications. One is computational, the **Computer Science** branch, and the other one is the **Linguistics** branch.

Refer to *Figure 3.1*:

![](img/680b98f4-c97e-4137-8e26-0dec9c2e7bb2.png)

Figure 3.1: Branches of NLP

The **Linguistics** branch focuses on how NL can be analyzed using various scientific techniques. So, the **Linguistics** branch does scientific analysis of the form, meaning, and context.

All linguistics analysis can be implemented with the help of computer science techniques. We can use the analysis and feed elements of analysis in a machine learning algorithm to build an NLP application. Here, the machine learning algorithm is a part of **Computer Science**, and the analysis of language is **Linguistics**.

Computational linguistics is a field that helps you to understand both computer science and linguistics approaches together.

Here is a list of tools that are linguistics concepts and are implemented with the help of computer science techniques. These tools are often used for developing NLP applications:

*   For POS tagging, POS taggers are used. Famous libraries are `nltk` and `pycorenlp`.
*   Morph analyzers are used to generate word-level stemming. For this, the `nltk` and `polyglot` libraries are used.
*   Parsers are used to identify the structure of the sentences. For this, we are using Stanford CoreNLP and `nltk` to generate a parsing tree. You can use Python package called `spaCy`.

# Defining context-free grammar

Now let's focus on NLU, and to understand it, first we need to understand **context-free grammar** (**CFG**) and how it is used in NLU.

Context-free grammar is defined by its four main components. Those four components are shown in this symbolic representation of CFG:

*   A set of non-terminal symbols, **N**
*   A set of terminal symbols, **T**
*   A start symbol, **S**, which is a non-terminal symbol
*   A set of rules called **production** **rules P**, for generating sentences

Let's take an example to get better understanding of the context-free grammar terminology:

***X ->** ![](img/30e1e2bf-4fe0-4b65-90cb-66985a0bd284.png)*

Here, ***X->** ![](img/30e1e2bf-4fe0-4b65-90cb-66985a0bd284.png)* is called the **phrase structure rule** or **production rule**, **P**. *X* ε *N* means *X* belongs to non-terminal symbol; ![](img/30e1e2bf-4fe0-4b65-90cb-66985a0bd284.png) ε {**N** or **T**} means ![](img/30e1e2bf-4fe0-4b65-90cb-66985a0bd284.png) belongs to either terminal symbols or non-terminal symbols. *X* can be rewritten in the form of ![](img/30e1e2bf-4fe0-4b65-90cb-66985a0bd284.png). The rule tells you which element can be rewritten to generate a sentence, and what the order of the elements will be as well.

Now I will take a real NLP example. I'm going to generate a sentence using CFG rules. We are dealing with simple sentence structure to understand the concepts.

Let's think. What are the basic elements required to generate grammatically correct sentences in English? Can you remember them? Think!

I hope you remember that noun phrases and verb phrases are important elements of the sentences. So, start from there. I want to generate the following sentence:

He likes cricket.

In order to generate the preceding sentence, I'm proposing the following production rules:

*   R1: S -> NP VP
*   R2: NP -> N
*   R3: NP -> Det N
*   R4: VP -> V NP
*   R5: VP -> V
*   R6: N -> Person Name | He | She | Boy | Girl | It | cricket | song | book
*   R7: V -> likes | reads | sings

See the parse tree of the sentence: **He likes cricket**, in *Figure 3.2*:

![](img/be2ec958-8f06-4f77-a7ff-a40df50df1ec.png)

Figure 3.2: Parse tree for the sentence using the production rule

Now, let's know how we have generated a parse tree:

*   According to the production rules, we can see **S** can be rewritten as a combination of a **noun phrase** (**NP**) and a **verb phrase** (**VP**); see rule *R1*.
*   **NP** can be further rewritten as either a **noun** (**NN**) or as a **determiner** (**Det**) followed by a noun; see rules *R2* and *R3*.
*   Now you can rewrite the **VP** in form of a **verb** (**V**) followed by a **NP**, or a **VP** can be rewritten as just **V**; see rules *R4* and *R5*.
*   Here, **N** can be rewritten in the form of **Person Name**, **He**, **She**, and so on. **N** is a terminal symbol; see the rule *R6*.
*   **V** can be rewritten by using any of the options on the right-hand side in rule *R7*. **V** is also terminal symbol.

By using all the rules, we have generated the parse tree in *Figure 3.2*.

Don't worry if you cannot generate a parse tree. We will see the concept and implementation details in the Chapter 5, *Feature Engineering and NLP Algorithms*.

Here, we have seen a very basic and simple example of CFG. Context-free grammar is also called **phrase structure grammar**.

# Exercise

1.  Generate a parse tree by using the rule given previously in this section and generate the parse tree for the following sentence:
    She sings a song.
2.  Generate production rules and make a parse tree for the following sentence:
    That boy is reading a book.

# Morphological analysis

Here, we are going to explore the basic terminology used in field of morphological analysis. The terminology and concepts will help you when you are solving real-life problems.

# What is morphology?

Morphology is branch of linguistics that studies how words can be structured and formed.

# What are morphemes?

In linguistics, a morpheme is the smallest meaningful unit of a given language. The important part of morphology is morphemes, which are the basic unit of morphology.

Let's take an example. The word *boy* consists of single morpheme whereas *boys* consists of two morphemes; one is *boy* and the other morpheme -*s*

# What is a stem?

The part of a word that an affix is attached to is called as **stem**. The word *tie* is **root** whereas *Untie* is **stem.**

Now, let's understand morphological analysis.

# What is morphological analysis?

Morphological analysis is defined as grammatical analysis of how words are formed by using morphemes, which are the minimum unit of meaning.

Generally, morphemes are affixes. Those affixes can be divided into four types:

*   Prefixes, which appear before a stem, such as **un**happy
*   Suffixes, which appear after a stem, such as happi**ness**
*   Infixes, which appear inside a stem, such as b**um**ili (this means buy in Tagalog, a language from the Philippines)
*   Circumfixes surround a word. It is attached to the beginning and end of the stem. For example, **ka**baddang**an** (this means help in Tuwali Ifugao, another language from the Philippines)

Morphological analysis is used in word segmentation, and **Part Of Speech** (**POS**) tagging uses this analysis. I will explain about POS in the *Lexical analysis* section, so bear with me until we will connect the dots.

Let's take an example to practically explain the concepts that I have proposed. I would like to take the word **Unexpected**. Refer to *Figure 3.3*, which gives you an idea about the morphemes and how morphological analysis has taken place:

![](img/de0833b8-b0e7-454d-8bda-3565ffeea8bf.png)

Figure 3.3: Morphemes and morphological analysis

In *Figure 3.3*, we have expressed **Unexpected** as morphemes and performed morphological analysis the morphemes. Here, **Un** is a **Prefix**, and **ed** is a **Suffix**. **Un** and **ed** can be considered as **affixes**, **Unexpect** is the **Stem**.

Let's refer to another important concept and try to relate it to the concept of morphemes. I'm talking about how you define a word. Let's see.

# What is a word?

A word can be isolated from a sentence as the single smallest element of a sentence that carries meaning. This smallest single isolated part of a sentence is called a **word**.

Please refer to the morphemes definition again and try to relate it to the definition of word. The reason why I have told you to do this is that you may confuse words and morphemes, or maybe you are not sure what the difference is between them. It is completely fine that you have thought in this way. They are confusing if you do not understand them properly.

The definitions look similar, but there is a very small difference between words and morphemes. We can see the differences in the following table:

| **Morpheme** | **Word** |
| Morphemes can or cannot stand alone. The word *cat* can stand alone but plural marker *-s* cannot stand alone. Here *cat* and *-s* both are morpheme. | A word can stand alone. So, words are basically free-standing units in sentences. |
| When a morpheme stands alone then that morpheme is called **root** because it conveys the meaning of its own, otherwise morpheme mostly takes affixes.The analysis of what kind of affixes morpheme will take is covered under morphological analysis. | A word can consist of a single morpheme. |
| For example, *cat* is a standalone morpheme, but when you consider *cats*, then the suffix *-s* is there, which conveys the information that *cat* is one morpheme and the suffix *-s* indicates the grammatical information that the given morpheme is the plural form of *cat*. | For example: *Cat* is a standalone word.*Cats* is also a standalone word. |

# Classification of morphemes

The classification of morphemes gives us lots of information about how the whole concept of morphological analysis works. Refer to *Figure 3.4*:

![](img/d590d981-f6ca-4c07-8a15-6645631423c7.png)

Figure 3.4: Classification of morphemes

There two major part of morphemes.

# Free morphemes

Free morphemes can be explained as follows:

*   Free morphemes can stand alone and act as a word. They are also called **unbound morphemes** or **free-standing morphemes**.
*   Let's see some of examples:
    *   Dog, cats, town, and house.
    *   All the preceding words can be used with other words as well. Free morphemes can appear with other words as well. These kinds of words convey meaning that is different if you see the words individually.
        *   Let's see examples of that:
            *   Doghouse, town hall.
            *   Here, the meaning of doghouse is different from the individual meanings of dog and house. The same applies to town hall.

# Bound morphemes

Bound morphemes usually take affixes. They are further divided into two classes.

# Derivational morphemes

Derivational morphemes are identified when infixes combine with the root and changes either the semantic meaning.

Now, let's look at some examples:

*   Take the word **unkind**. In this word, **un** is a prefix and **kind** is the root. The prefix **un** acts as a derivational morpheme that changes the meaning of the word **kind** to its opposite meaning, unkind.
*   Take the word **happiness**. In this word, -**ness** is a derivational morpheme and **happy** is the root word. So, -**ness** changes happy to happiness. Check the POS tag, **happy** is an adjective and **happiness** is a noun. Here, tags that indicate the class of word, such as adjective and noun, are called POS.

# Inflectional morphemes

Inflection morphemes are suffixes that are added to a word to assign particular grammatical property to that word. Inflectional morphemes are considered to be grammatical markers that indicate tense, number, POS, and so on. So, in more simple language, we can say that inflectional morphemes are identified as types of morpheme that modify the verb tense, aspect, mood, person, number (singular and plural), gender, or case, without affecting the words meaning or POS.

Here's some examples:

*   In the word **dogs**, -**s** changes the number of **dog**. -**s** converts **dog** from singular to plural form of it
*   The word **expected** contains -**ed**, which is an inflectional morpheme that modifies the verb tense

Here is the code for generating the stem from morphemes. We are using the `nltk` and `polyglot` libraries. You can find the code on this link: [https://github.com/jalajthanaki/NLPython/blob/master/ch3/3_1_wordsteam.py](https://github.com/jalajthanaki/NLPython/blob/master/ch3/3_1_wordsteam.py)

See the code snippets in *Figure 3.5* and *Figure 3.6*:

![](img/9df95dda-917e-4113-a25b-fbc5ca229541.png)

Figure 3.5: Generating stems from morphemes using NLTK

Now, let's see how the `polyglot` library has been used refer to *Figure 3.6****:***

![](img/f6c740ed-1658-40d5-aa19-c79e9fd09b1a.png)

Figure 3.6: Generating stems from morphemes using the polyglot library

The output of the code snippet is displayed in *Figure 3.7*:

![](img/a02b5e5f-475c-45a3-bc33-7e3edd1c0da8.png)

Figure 3.7: Output of code snippets in Figure 3.5 and Figure 3.6

# What is the difference between a stem and a root?

This could be explained as follows:

| **Stem** | **Root** |
| In order to generate a stem, we need to remove affixes from the word | A root cannot be further divided into smaller morphemes |
| From the stem, we can generate the root by further dividing it | A stem is generated by using a root plus derivational morphemes |
| The word **Untie** is stem | The word **tie** is root |

# Exercise

1.  Do a morphological analysis like we did in *Figure 3.3* for the morphemes in redness, quickly, teacher, unhappy, and disagreement. Define prefixes, suffixes, verbs, and stems.
2.  Generate the stems of the words redness, quickly, teacher, disagreement, reduce, construction, deconstruction, and deduce using the `nltk` and `polyglot` libraries.
3.  Generate the stems and roots of disagree, disagreement, historical.

# Lexical analysis

Lexical analysis is defined as the process of breaking down a text into words, phrases, and other meaningful elements. Lexical analysis is based on word-level analysis. In this kind of analysis, we also focus on the meaning of the words, phrases, and other elements, such as symbols.

Sometimes, lexical analysis is also loosely described as the **tokenization process**. So, before discussing tokenization, let's understand what a token is and what a POS tag is.

# What is a token?

Tokens are defined as the meaningful elements that are generated by using techniques of lexical analysis.

# What are part of speech tags?

A part of speech is a category of words or lexical items that have similar grammatical properties. Words belonging to the same **part of speech** (**POS**) category have similar behavior within the grammatical structure of sentences.

In English, POS categories are verb, noun, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article, or determiner.

# Process of deriving tokens

Sentences are formed by stream of words and from a sentence we need to derive individual meaningful chunks which are called the **tokens** and process of deriving token is called **tokenization**:

*   The process of deriving tokens from a stream of text has two stages. If you have a lot of paragraphs, then first you need to do sentence tokenization, then word tokenization, and generate the meaning of the tokens.
*   Tokenization and lemmatization are processes that are helpful for lexical analysis. Using the `nltk` library, we can perform tokenization and lemmatization.
*   Tokenization can be defined as identifying the boundary of sentences or words.
*   Lemmatization can be defined as a process that identifies the correct intended POS and meaning of words that are present in sentences.
*   Lemmatization also includes POS tagging to disambiguate the meaning of the tokens. In this process, the context window is either phrase level or sentence level.

You can find the code at the GitHub link: [https://github.com/jalajthanaki/NLPython/tree/master/ch3](https://github.com/jalajthanaki/NLPython/tree/master/ch3)

The code snippet is shown in *Figure 3.8*:

![](img/e5cecd2f-49b4-48f6-954d-40b1a7c80a5f.png)

Figure 3.8: Code snippet for tokenization

The output of the code in *Figure 3.8* is shown in *Figure 3.9*:

![](img/07ee541e-7b12-45f4-8f91-e4e3317a0c5d.png)

Figure 3.9: Output of tokenization and lemmatization

# Difference between stemming and lemmatization

Stemming and lemmatization both of these concepts are used to normalized the given word by removing infixes and consider its meaning. The major difference between these is as shown:

| **Stemming** | **Lemmatization** |
| Stemming usually operates on single word without knowledge of the context | Lemmatization usually considers words and the context of the word in the sentence |
| In stemming, we do not consider POS tags | In lemmatization, we consider POS tags |
| Stemming is used to group words with a similar basic meaning together | Lemmatization concept is used to make dictionary or WordNet kind of dictionary. |

# Applications

You must think how this lexical analysis has been used for developing NLP applications. So, here we have listed some of the NLP applications which uses lexical analysis concepts:

*   Lexical analysis such as sentence tokenization and stop word identification is often used in preprocessing.
*   Lexical analysis also used to develop a POS tagger. A POS tagger is a tool that generates POS tags for a stream of text.

# Syntactic analysis

We have seen word-level analysis in lexical analysis. In this section, we will look at things from a higher level. We are going to focus on the grammar and structure of sentences by considering the phrases in the sentences.

Now, let's define syntactic analysis and see how it will be used in NLP applications.

# What is syntactic analysis?

Syntactic analysis is defined as analysis that tells us the logical meaning of certain given sentences or parts of those sentences. We also need to consider rules of grammar in order to define the logical meaning as well as correctness of the sentences.

Let's take an example: If I'm considering English and I have a sentence such as **School go a boy**, this sentence does not logically convey its meaning, and its grammatical structure is not correct. So, syntactic analysis tells us whether the given sentence conveys its logical meaning and whether its grammatical structure is correct.

Syntactic analysis is a well-developed area of NLP that deals with the syntax of NL. In syntactic analysis, grammar rules have been used to determine which sentences are legitimate. The grammar has been applied in order to develop a parsing algorithm to produce a structure representation or a parse tree.

Here, I will generate the parse tree by using the `nltk` and Python wrapper libraries for Stanford CoreNLP called `pycorenlp`. Refer the following code snippet in *Figure 3.10* and in *Figure 3.11*. The output is given in *Figure 3.12*:

![](img/5bc03edc-cbce-4e6b-bb40-936eeb90494c.png)

Figure 3.10: Code snippet for syntactic analysis

How you can use Stanford parser for syntactic analysis is demonstrated in next *Figure 3.11*:

![](img/c5b21b59-4943-49ca-b967-4f0be1c22dee.png)

Figure 3.11: Code snippet for syntactic analysis

You can see the output of the preceding two code snippet as follows. Refer to *Figure 3.12:*

![](img/38d8d6f5-bf1f-42cd-9ef0-40418093d385.png)

Figure 3.12: Output of parsing as part of syntactic analysis

We will see the parsing tools and its development cycle related details in Chapter 5, *Feature Engineering and NLP Algorithms*.

# Semantic analysis

Semantic analysis is basically focused on the meaning of the NL. Its definition, various elements of it, and its application are explored in this section.

Now let's begin our semantic journey, which is quite interesting if you want to do some cool research in this branch.

# What is semantic analysis?

Semantic analysis is generating representation for meaning of the NL. You might think, if lexical analysis also focuses on the meaning of the words given in stream of text, then what is the difference between semantic analysis and lexical analysis? The answer is that lexical analysis is based on smaller tokens; its focus is on meaning of the words, but semantic analysis focuses on larger chunks. Semantic analysis can be performed at the phrase level, sentence level, paragraph level, and sometimes at the document level as well. Semantic analysis can be divided into two parts, as follows:

*   The study of the meaning of the individual word is called **lexical semantics**
*   The study of how individual words combine to provide meaning in sentences or paragraphs in the context of dealing with a larger unit of NL

I want to give an example. If you have a sentence such as the **white house is great**, this can mean the statement is in context of the White House in the USA, whereas it is also possible the statement is literally talking about a house nearby, whose color is white is great. So, getting the proper meaning of the sentence is the task of semantic analysis.

# Lexical semantics

Lexical semantics includes words, sub-words, or sub-units such as affixes, and even compound words, and phrases. Here words, sub-words and so on called **lexical items**.

The study of lexical semantics includes the following points:

*   Classification of lexical items
*   Decomposition of lexical items
*   Differences and similarities between various lexical semantic structures
*   Lexical semantics is the relationship among lexical items, meaning of the sentences and syntax of the sentence

Let's see the various elements that are part of semantic analysis.

# Hyponymy and hyponyms

Hyponymy describes the relationship between a generic term and instances of the specified generic term. Here, a generic term is called a **hypernym**, and instances of the generic term are called **hyponyms**.

So, color is a hypernym; red, green, yellow, and so on are hyponyms.

# Homonymy

Homonyms are words that have a same syntax or same spelling or same form but their meaning are different and unrelated to each other.

The word bank is a classic example. It can mean a financial institution or a river bank, among other things.

# Polysemy

In order to understand polysemy, we are focused on words of the sentences. Polysemy is a word or phrase which have different, but related senses. These kinds of words are also referred as lexically ambiguous words.

Take the word bank. There are several senses or meaning you can consider.

*   Bank is financial institution
*   Bank can be interpreted as river bank

# What is the difference between polysemy and homonymy?

A word is called **polysemous** if it is used to express different meanings. The difference between the meanings of the word can be obvious.

Two or more words are called **homonyms** if they either have the same sound or have the same spelling but do not have related meanings.

# Application of semantic analysis

Semantic analysis is one of the open research area so its basic concepts can be used by following applications:

*   Word sense disambiguation is one of the major tasks in NLP where semantic analysis has been heavily used, and it's still an open research area for Indian languages
*   We will see **word** **sense** **disambiguation** (**WSD**) usage in [Chapter 7](0dc5bd44-3b7d-47ac-8b0d-51134007b483.xhtml), *Rule-Based System for NLP*
*   The word2vec concept has emerged to handle semantic similarity. We will see this in Chapter 6, *Advance Feature Engineering and NLP Algorithms*

# Handling ambiguity

When we jump into semantic analysis, we may find there are many cases that are too ambiguous for an NLP system to handle. In these cases, we need to know what kinds of ambiguity exist and how we can handle them.

Ambiguity is one of the areas of NLP and cognitive sciences that doesn't have a well-defined solution. Sometimes, sentences are so complex and ambiguous that only the speaker can define the original or definite meaning of the sentence.

A word, phrase, or sentence is ambiguous if it has more than one meaning. If we consider word **light**,than it can mean not very heavy or not very dark. This is word level ambiguity. The phrase **porcelain egg container** is structure level ambiguity. So, here we will see different types of ambiguities in NLP .

First, let's see the types of ambiguity, and then see how to handle them by using the means that are available. Refer to *Figure 3.13* for the different types of ambiguity:

![](img/5b0d5e96-c201-42a1-87cc-587c06bd1498.png)

Figure 3.13: Types of ambiguity

# Lexical ambiguity

Lexical ambiguity is word-level ambiguity. A single word can have ambiguous meaning in terms of its internal structure and its syntactic class. Let's look at some examples:

*   Sentence 1: Look at the stars. Here, *look* is a verb.
*   Sentence 2: The person gave him a warm look. Here, *look* is a noun.
*   Sentence 3: She won three silver medals. Here, *silver* is a noun.
*   Sentence 4: She made silver speech. Here, *silver* is a adjective.
*   Sentence 5: His stress had silvered his hair. Here, *silvered* is a verb.

In the preceding examples, specific words change their POS tags according to their usage in sentence structure. This kind of ambiguity can been resolved by using two approaches:

*   By using accurate POS tagger tools, this kind of ambiguity can be resolved
*   WordNet sense has various scenes available for a word when the words take specific POS tag. This also helps to handle ambiguity

Many Indian languages have the same issue of lexical ambiguity.

# Syntactic ambiguity

We have seen, in syntactic analysis, sequences of words are grammatically structured. There are different ways of interpreting sequences of words, and each structure has a different interpretation. In syntactic ambiguity, syntax is unclear, not the word-level meaning. Here is an example of structural ambiguity:

*   The man saw the girl with the telescope. Here, the ambiguity is because it is not clear whether the man sees the girl, who has a telescope, or the man sees the girl by using telescope. This ambiguity is called **prepositional phrase** (**PP**) ambiguity.

# Approach to handle syntactic ambiguity

To handle this ambiguity, we need to use statistical approaches and get the most likelihood ratio. We need to take co-occurrences between the verb and the preposition in one hand, and preposition and the noun on other hand, and then calculate the log-likelihood ratio by using following equation:

![](img/e4a2a65e-f919-4a43-b179-c151e067fac0.png)

Figure 3.14: Log-likelihood ratio

Here, *p(p/v)* is the probability of seeing a PP with preposition *p* and after verb *v*.

And *p(p/n)* is the probability of seeing a PP with preposition *p* after noun *n*.

If *F(v,p,n) < 0*, then we need to attach the preposition to the noun, and if *F(v,p,n)* >0, then we need to attach preposition to the verb. We will see the implementation in [Chapter 5](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml), *Feature Engineering and NLP Algorithms*.

# Semantic ambiguity

Semantic ambiguity occurs when the meaning of the words themselves can be misinterpreted. Here's an example:

*   ABC head seeks arms
*   Here, the word head either means chief or body part, and in the same way, arms can be interpreted as weapons or as body parts
*   This kind of ambiguity is considered in semantic ambiguity

Handling semantic ambiguity with high accuracy is an open research area. Nowadays, the word2vec representation technique is very useful for handling semantic ambiguity.

# Pragmatic ambiguity

Pragmatics ambiguity occurs when the context of a phrase gives it multiple different interpretations. Let's take an example:

*   Give it to that girl. This could mean any number of things.

Now let's take a large context:

*   I have chocolate and a packet of biscuits. Give it to that girl. Here, it is not clear whether it refers to chocolate or the packet of biscuits.

Handling this kind of ambiguity is still an open area of research.

# Discourse integration

Discourse integration is closely related to pragmatics. Discourse integration is considered as the larger context for any smaller part of NL structure. NL is so complex and, most of the time, sequences of text are dependent on prior discourse.

This concept occurs often in pragmatic ambiguity. This analysis deals with how the immediately preceding sentence can affect the meaning and interpretation of the next sentence. Here, context can be analyzed in a bigger context, such as paragraph level, document level, and so on.

# Applications

Concepts of discourse integration have been used by following NLP applications:

*   This concept often used in NLG applications.
*   Chatbots, which are developed to deliver generalized AI. In this kind of application, deep learning has been used. We will see the NLG with deep learning in [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml), *Deep Learning for NLP and NLG Problems*.

# Pragmatic analysis

Pragmatic analysis deals with outside word knowledge, which means knowledge that is external to the documents and/or queries. Pragmatics analysis that focuses on what was described is reinterpreted by what it actually meant, deriving the various aspects of language that require real world knowledge.

Let's look at an example:

*   Pruning a tree is a long process.
*   Here, pruning a tree is one of the concepts of computer science algorithm techniques. So, the word **pruning** is not related to cutting the actual physical tree, we are talking about computer science algorithm. This is an ambiguous situation; how to deal with these kinds of ambiguous situations is also an open area of research. Big tech giants use deep learning techniques to do pragmatics analysis and try to generate the accurate context of the sentence in order to develop highly accurate NLP applications.

# Summary

This chapter explored the basics of linguistics, which are often used to develop NLP applications. We have seen all kinds of analysis related to NL. We have seen word level analysis and larger context analysis. We have seen difference between some of the key concepts to resolve any confusion. After this chapter, you can identify which concepts of linguistics or tool is more interesting for you to use. Researchers can find the potential research area if they want to pursue research either in linguistics, computer linguistics or computer science with major in NLP.

In the next chapter, we will focus on practical and coding aspects and begin our journey to develop NLP applications. The next chapter is all about **preprocessing** the data, one of the basic but important steps in developing NLP applications. Preprocessing includes some of the concepts which we have described here. We will use them along with the standard ways of preprocessing.
