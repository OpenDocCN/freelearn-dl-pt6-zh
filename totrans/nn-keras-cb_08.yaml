- en: Image Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned about predicting the class of an image
    and detecting where the object is located in the whole image. If we work backwards,
    we should be in a position to generate an image if we are given a class. Generative
    networks come in handy in this scenario, where we try to create new images that
    look very similar to the original image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating images that can fool a neural network using an adversarial attack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepDream algorithm to generate images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural style transfer between images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating images of digits using Generative Adversarial Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating images of digits using a Deep Convolutional GAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face generation using a Deep Convolutional GAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face transition from one to another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing vector arithmetic on generated images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapters, we identified the optimal weights that result in
    classifying an image into the right class. The output class of an image can be
    changed by varying the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The weights connecting the input to the output layer, while the input pixels
    remain constant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input pixel values, while the weights remain constant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will employ these two techniques to generate images.
  prefs: []
  type: TYPE_NORMAL
- en: In the case studies of an adversarial attack, the neural style transfer and
    DeepDream will leverage the technique of changing the input pixel values. In the
    techniques involving a **Generative Adversarial Network** (**GAN**), we will leverage
    the technique of changing certain weights that connect input pixel values to the
    output.
  prefs: []
  type: TYPE_NORMAL
- en: The first three case studies in this chapter will leverage the technique of
    changing the input pixel values, while the rest leverage a change in weights that
    connect the input to the output.
  prefs: []
  type: TYPE_NORMAL
- en: Generating images that can fool a neural network using adversarial attack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how to perform an adversarial attack on an image, let's understand
    how regular predictions are made using transfer learning first and then we will
    figure out how to tweak the input image so that the image's class is completely
    different, even though we barely changed the input image.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go through an example where we will try to identify the class of the
    object within the image:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the image of a cat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the image so that it can then be passed to an inception network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the pre-trained Inception v3 model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict the class of the object present in the image
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The image will be predicted as a persian cat as Inception v3 works well in predicting
    objects that belong to one of the ImageNet classes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The task at hand is to change the image in such a way that it meets the following
    two criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: The prediction of the new image using the same network should be an African
    elephant with a very high probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new image should be visually indistinguishable from the original image by
    a human
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To achieve this, we will follow this strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss is the probability of the image (of the persian cat) belonging to the
    African elephant class
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The higher the loss, the closer are we to our objective
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, in this case, we would be maximizing our loss function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calculate the gradient of change in the loss with respect to the change in
    the input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This step helps in understanding the input pixels that move the output toward
    our objective
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Update the input image based on the calculated gradients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that the pixel values in the original image is not translated by more
    than 3 pixels in the final image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This ensures that the resulting image is humanly indistinguishable from the
    original image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat steps 2 and step 3 until the prediction of the updated image is an African
    elephant with a confidence of at least 0.8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go ahead and implement this strategy in code (The code file is available
    as `Adversarial_attack.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the image of a cat:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot of the image looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33c46965-bc81-4b5e-aec1-f52dacb165dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Preprocess the image so that it can then be passed to an inception network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the pre-trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the class of the object present in the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the input and output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`model_input_layer` is the input to the model and `model_output_layer` is the
    probability of various classes for the input image (the last layer with softmax
    activation).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the limits of change for the original image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are specifying the limits to which the original image
    can be changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the cost function so that the object type to fake is an African
    elephant (386^(th) index value in the prediction vector):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output of `model_output_layer` is the probability of various classes for
    the image of interest. In this instance, we are specifying that the cost function
    will be dictated by the index location of the object we are trying to fake our
    object into.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the gradient function of the cost with respect to the input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This code calculates the gradient of `cost_function` with respect to the change
    in `model_input_layer` (which is the input image).
  prefs: []
  type: TYPE_NORMAL
- en: 'Map the cost and gradient functions with respect to the input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are calculating the values of `cost_function` (the
    probability of the image belonging to the African elephant class) and the gradients
    with respect to the input image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep updating the input image with respect to gradients until the probability
    of the resulting image being an African elephant is at least 80%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are obtaining the cost and gradients that correspond
    to the input image (`hacked_image`). Additionally, we are updating the input image
    by the gradient (which is multiplied by the learning rate). Finally, if the hacked
    image crosses the threshold of the maximum changes of the input image, we'll clip
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Keep looping through these steps until you achieve a probability that the input
    image is at least 0.8.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variation of the probability of the image of persian cat being detected
    as the image of an African elephant over increasing epochs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The variation of probability of the modified image belonging to the African
    elephant class is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8c81887-f7bb-4364-8ea0-27f4321ada37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Predict the class of the updated image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The output of the `predict` method, which provides the probability of the modified
    image belonging to African elephant class, is 0.804.
  prefs: []
  type: TYPE_NORMAL
- en: 'De-process the updated input image (as it was pre-processed to scale it) so
    that it can be visualized:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The combination of the original image, the modified (hacked) images and the
    difference between the two images is printed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1db1140a-75c9-4b31-9012-aeba89a7bb24.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the output is now visually indistinguishable from the original image.
  prefs: []
  type: TYPE_NORMAL
- en: It is interesting to note that with hardly any change in pixel values from the
    original image, we have fooled the neural network (the inception v3 model) so
    that it now predicts a different class. This is a great example of some of the
    security flaws that you could encounter if the algorithm that was used to come
    up with a prediction is exposed to users who could build images that can fool
    the system.
  prefs: []
  type: TYPE_NORMAL
- en: DeepDream algorithm to generate images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we tweaked the input image's pixels slightly. In this
    section, we will tweak the input image a little more so that we can come up with
    an image that is still of the same object, however a little more artistic than
    the original one. This algorithm forms the backbone of style-transfer techniques
    using neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through the intuition of how DeepDream works.
  prefs: []
  type: TYPE_NORMAL
- en: We will pass our image through a pre-trained model (VGG19, in this example).
    We already learned that, depending on the input image, certain filters in the
    pre-trained model activate the most and certain filters activate the least.
  prefs: []
  type: TYPE_NORMAL
- en: We will supply the layers of neural network that we want to activate the most.
  prefs: []
  type: TYPE_NORMAL
- en: The neural network adjusts the input pixel values until we obtain the maximum
    value of the chosen layers.
  prefs: []
  type: TYPE_NORMAL
- en: However, we will also ensure that the maximum possible activation does not exceed
    a certain value as the resultant image in that case could be very different from
    the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With this intuition in place, let''s go through the steps of implementing the
    DeepDream algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose the layers of neural network that you want to activate the most and assign
    weightage to the amount of contribution the layers can make towards overall loss
    calculation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract the output of the given layer when an image is passed through the layer
    and calculate the loss value at each layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An image activates the layer the most when the sum of squares of the output
    of the image in that layer is the highest
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the gradient of change in the input pixel values with respect to the
    loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the input pixel values based on the gradient extracted in the previous
    step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the loss value (the sum of the squares of the activation) across all
    chosen layers for the updated input pixel values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the loss value (the weighted sum of the squared activation) is greater than
    a predefined threshold, stop updating the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s implement these steps in code (The code file is available as `Deepdream.ipynb`
    in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages and import the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Preprocess the image so that it can then be passed to the VGG19 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a function that de-processes the processed image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Preprocess the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the layers that contribute to the overall loss-value calculation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are showing you that we will use the second and fifth
    pooling layers, and also assign the weights that these two layers will contribute
    to the overall loss value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding step, we are initializing the loss value and a dictionary of
    the various layers in the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the overall loss value of the activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are looping through the layers that we are interested
    in ( `layer_contributions` ) and noting down the weights ( `coeff` ) that we have
    assigned to each layer. Additionally, we are calculating the output of the layers
    of interest ( `activation` ), and updating the loss value using the sum of squares
    of the activation values post scaling them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the gradient value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `K.gradients` method gives us the gradient of the loss with respect to the
    change in the input, `dream`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Normalize the gradient values so that the change in the gradients is slow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a function that maps the input image to the loss value and the gradient
    of the loss value with respect to the change in input pixel values (where the
    input image is `dream`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function that provides the loss and gradient values for a given input
    image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Update the original image based on the obtained loss and gradient values over
    multiple iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code, we are looping through the image 100 times. We are defining
    the learning rate of changing the image and the maximum possible loss (change
    in the image) that can happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are extracting the loss and gradient values of the
    image and then stopping the change in the image if the loss value is more than
    the defined threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are updating the image based on the gradient values
    and are de-processing the image and printing the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in an image that looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d34a9836-cc7c-49dd-8afb-fe8cc5ac2545.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the wavy patterns in the preceding image are obtained potentially
    because these are the patterns that maximize the various network layers' activations.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have seen another application of perturbing input pixels, which in
    this case resulted in a slightly more artistic image.
  prefs: []
  type: TYPE_NORMAL
- en: Neural style transfer between images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, the modified pixel values were trying to maximize the
    filter activations. However, it does not give us the flexibility of specifying
    the style of the image; neural style transfer comes in handy in this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: In neural style transfer, we have a content image and a style image, and we
    try to combine these two images in such a way that the content in the content
    image is preserved while maintaining the style of the style image.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The intuition of neural style transfer is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: We try to modify the original image in a similar way to the DeepDream algorithm.
    However, the additional step is that the loss value is split into content loss
    and style loss.
  prefs: []
  type: TYPE_NORMAL
- en: Content loss refers to how different the generated image is from the content
    image. Style loss refers to how correlated the style image is to the generated
    image.
  prefs: []
  type: TYPE_NORMAL
- en: While we mentioned that the loss is calculated based on the difference in images,
    in practice, we modify it slightly by ensuring that the loss is calculated using
    the activations from images and not the original images. For example, the content
    loss at layer 2 will be the squared difference between activations of the content
    image and the generated image when passed through the second layer.
  prefs: []
  type: TYPE_NORMAL
- en: While calculating the content loss seems straightforward, let's try to understand
    how to calculate the similarity between the generated image and the style image.
  prefs: []
  type: TYPE_NORMAL
- en: 'A technique called gram matrix comes into the picture. Gram matrix calculates
    the similarity between a generated image and a style image, and is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5754caec-464d-4b8e-a375-65df6ffdc898.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *GM(l)* is the gram matrix value at layer *l* for the style image, *S*,
    and the generated image, *G*.
  prefs: []
  type: TYPE_NORMAL
- en: A gram matrix results from multiplying a matrix with the transpose of itself.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are in a position to calculate the style loss and content loss,
    the final modified input image is the image that minimizes the overall loss, that
    is, a weighted average of style and content loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural style transfer is implemented in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Pass the image through a pre-trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the layer values at a predefined layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the generated image as the same as the content image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the generated image through the model and extract its values at the exact
    same layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the content loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the style image through multiple layers of the model and calculate the
    gram matrix values of the style image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the generated image through the same layers that the style image passed
    through and calculate its corresponding gram matrix values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the squared difference of the gram matrix values of the two images.
    This will be the style loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The overall loss will be the weighted average of the style loss and content
    loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input image that minimizes the overall loss will be the final image of interest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import the relevant packages and content, style images, that need to be combined
    to form an artistic image, as follows (The code file is available as `Neural_style_transfer.ipynb`
    in GitHub):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The style and base images look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e087838-928d-476e-b7e8-a1153df3d213.png)![](img/470f0a29-7b89-40ca-a2ba-33d574883c08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Initialize the `vgg19` model so that the images can be passed through its network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape the base image and extract the feature values at the `block3_conv4` layer
    of the VGG19 model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are defining a function that takes the input image
    and extracts the output at the predefined layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the layers from which extractions need to be made to calculate the content
    and style losses as well as the corresponding weights that need to be assigned
    to each layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are defining the layers from which the content and
    style loss are calculated, as well as assigning the weights associated with the
    loss arising from each of these layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the gram matrix and style loss functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code, we are defining a function that calculates the gram
    matrix output output as the dot product of features obtained by flattening the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are calculating the style loss as defined in the
    style loss equation specified in *Getting ready* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the loss value function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculating the content loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are updating the loss value based on the loss at the
    layers that calculate the content loss. Note that `layer_output_base` is the output
    when we pass the original base image through the content layer (as defined in
    step 3).
  prefs: []
  type: TYPE_NORMAL
- en: The greater the difference between the activation (which is based on the modified
    image) and `layer_output_base` (which is based on the original image), the greater
    the content loss associated with the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating the style loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are calculating style loss in the same manner as
    we calculated the content loss but on different layers and using a different custom
    function we built: `style_loss`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a function that maps the input image to the loss values and the corresponding
    gradient values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code fetches the loss and gradient values in a manner that is
    very similar to the *DeepDream algorithm to generate images* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the model for multiple epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in an image that is a combination of the content
    and style images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03042fd3-6db8-40c1-bfe4-071359915494.png)'
  prefs: []
  type: TYPE_IMG
- en: With differing layers that are selected to calculate the content and style loss,
    and differing weights assigned to coefficients of layers in their respective style
    or content contributions, the resulting generated image could be different.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous three case studies, we saw how we can generate new images by
    changing the input pixel values. In the rest of this chapter, we will take a different
    approach to generating new images: using GANs.'
  prefs: []
  type: TYPE_NORMAL
- en: Generating images of digits using Generative Adversarial Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A GAN uses a stack of neural networks to come up with a new image that looks
    very similar to the original set of images. It has a variety of applications in
    image generation, and the field of GAN research is progressing very quickly to
    come up with images that are very hard to distinguish from real ones. In this
    section, we will understand the basics of a GAN – how it works and the difference
    in the variations of GANs.
  prefs: []
  type: TYPE_NORMAL
- en: 'A GAN comprises two networks: a generator and a discriminator. The generator
    tries to generate an image and the discriminator tries to determine whether the
    image it is given as an input is a real image or a generated (fake) image.'
  prefs: []
  type: TYPE_NORMAL
- en: To gain further intuition, let's assume that a discriminator model tries to
    classify a picture into a human face image, or not a human face from a dataset
    that contains thousands of human face images and non-human face images.
  prefs: []
  type: TYPE_NORMAL
- en: Once we train the model to classify human and non-human faces, if we show a
    new human face to the model, it would still classify it as a human face, while
    it learns to classify a non-human face as a non-human face.
  prefs: []
  type: TYPE_NORMAL
- en: The task of the generator network is to generate images that look so similar
    to the original set of images that a discriminator can get fooled into thinking
    that the generated image actually came from the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy we''ll adopt to generate images is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate a synthetic image using the generator network, which in the initial
    step is a noisy image that is generated by reshaping a set of noise values to
    the shape of our images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Concatenate the generated image with the original set of images where the discriminator
    predicts whether each of the images is a generated image or an original image—this
    ensures that the discriminator is trained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the weights of a discriminator network are trained in this iteration
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss of a discriminator network is the binary cross-entropy of the prediction
    and actual values of an image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The output value of the generated image will be fake (0) and the values of the
    original images will be real (1)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that the discriminator has been trained for one iteration, train a generator
    network that modifies the input noise such that it looks more like a real image
    than a synthetic one – one that has the potential to fool the discriminator. This
    process goes through the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input noise is passed through a generator network, which reshapes the input
    into an image.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The image generated from the generator network is then passed through a discriminator
    network – however, note that the weights in the discriminator network are frozen
    in this iteration so that they are not trained in this iteration (because they
    were already trained in step 2).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The value of the generated image's output from the discriminator will be real
    (1) as its task is to fool the discriminator.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The loss of a generator network is the binary cross-entropy of the prediction
    from the input image and the actual value (which is 1 for all the generated images)—this
    ensures that the generator network weights are fine-tuned:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the discriminator network weights are frozen in this step
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Freezing the discriminator ensures that the generator network learns from the
    feedback provided by the discriminator
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat these steps multiple times until you generate realistic images.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the *Adversarial attack to fool a neural network section*, we discussed
    our strategy of how to generate an image that looks very similar to the original
    images. In this section, we will implement the process of generating a digit''s
    image from the MNIST dataset (the code file is available as `Vanilla_and_DC_GAN.ipynb`
    in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the generator and discriminator networks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'For the generator, we are building a model that takes a noise vector that is
    100 dimensions in shape and will be converting it into an image that is 28 x 28
    x 1 in shape. Note that we used `LeakyReLU` activation in the model. A summary
    of the generator network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bee45c41-5ea7-4026-b15b-ef627ff7902a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following code, we are building a discriminator model where we take
    an input image that is 28 x 28 x 1 in shape and produce an output that is either
    1 or 0, which indicates whether the input image is an original image or a fake
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of the discriminator network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf8f01e2-d615-49af-9cae-12915b038e85.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Compile the generator and discriminator models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the stacked generator discriminator model that helps to optimize weights
    for the generator while freezing weights for the discriminator network. The stacked
    generator discriminator takes the random noise that we pass to the model as input
    and converts that noise into an image that is 28 x 28 in shape using the generator
    network. Furthermore, it determines whether the 28 x 28 image is a real or fake
    image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to plot the generated images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Provide the input images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We are discarding the `y_train` dataset, as we do not need the output labels,
    since our model generates new images based on the given set of images, that is `X_train`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimize the images by running them over multiple epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code, we are obtaining the real images (`legit_images`) and
    generating the fake image (`synthetic_images`) data, which we will try to convert
    into a realistic image by modifying noise data (`gen_noise`) as the input, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are training the discriminator (using the `train_on_batch` method),
    where the real images are expected to have a value of 1 and the fake images are
    expected to have a value of zero in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are preparing a new set of data where `noise` is
    the input and `y_mislabeled` is the output to train the generator (note that the
    output is the exact opposite of what the output was when we were training the
    discriminator):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are training the stacked combination of the generator
    and discriminator, where the discriminator weights are frozen while the generator''s
    weights get updated to minimize the loss value. The generator''s task is to generate images that
    can trick the discriminator to output a value of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are looking at the output of generator loss and discriminator
    loss across various epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/135e0b94-6745-46c3-8bd1-e9adfdcd07b4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The variation of discriminator and generator loss of increasing epochs is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37f6d676-e678-494b-a8e3-9856cbc10712.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the preceding output has a lot of scope for improvement in terms of
    how realistic the generated images look.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The output that we saw here is also a function of the model's architecture.
    For example, vary the activation function in various layers of model to tanh and
    see how the resulting output looks to get an idea of what the resulting generated
    images look like.
  prefs: []
  type: TYPE_NORMAL
- en: Generating images using a Deep Convolutional GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we looked at generating digits using a vanilla generator
    and a discriminator network. However, we can have a scenario where the network
    can learn the features in an image much better by using the convolution architectures,
    as the filters in a CNN learn specific details within an image. **Deep Convolutional
    Generative Adversarial Networks** (**DCGANs**) take advantage of this phenomenon
    to come up with new images.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the intuition of how a DCGAN works is very similar to that of a GAN (which
    we worked with in the previous recipe), the major difference is in the architecture
    of the generator and discriminator of the DCGAN, which looks as follows (The code
    file is available as `Vanilla_and_DC_GAN.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in the DCGAN, we performed multiple convolution and pooling operations
    on the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we rerun the exact same steps that we performed in the Vanilla GAN (the *Generative
    Adversarial Network to generate images* recipe), but this time using the models
    defined with a convolution and pooling architecture (and thus DCGAN), we get the
    following generated image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2875a289-9afe-430e-8502-c63a9c999541.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The variation of generator and discriminator loss values over increasing epochs
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a924e72-de38-489a-93be-b1fac122e4ce.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that, while everything else remains the same and only the model architecture
    has changed, the resulting images through DCGAN are a lot more realistic than
    the results of a Vanilla GAN.
  prefs: []
  type: TYPE_NORMAL
- en: Face generation using a Deep Convolutional GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how to generate new images. In this section, we will learn
    how to generate a new set of faces from an existing dataset of faces.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The approach we will be adopting for this exercise will be very similar to
    what we adopted in the *Generating images using a* *Deep Convolutional GAN* recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect a dataset that contains multiple face images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate random images at the start.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a discriminator by showing it a combination of faces and random images,
    where the discriminator is expected to differentiate between an actual face image
    and a generated face image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the discriminator model is trained, freeze it and adjust the random images
    in such a way that the discriminator now assigns a higher probability of belonging
    to the original face images to the adjusted random images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the preceding two steps through multiple iterations until the generator
    does not get trained any further.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Face generation is implemented in code as follows (the code file is available
    as `Face_generation.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset. The recommended dataset to be downloaded and the associated
    code is provided in GitHub. A sample of images is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cba9b1ce-cca6-459e-b691-2a0b84628ee0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Define the model architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the preceding code is the same as the generator we built in the *Deep
    convolutional generative adversarial networks* recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the preceding architecture is the same as the one we built in the
    *Generating images using Deep Convolutional GAN* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the utility functions to load, preprocess, and de-process the image
    and also to plot the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are resizing our images to a smaller shape so that the the number
    of parameters that need to be tweaked through the model is minimal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the dataset and preprocess it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we are creating the input dataset and converting it
    into an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the generator, discriminator, and stacked generator discriminator models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the model over multiple epochs in a manner that is very similar to what
    we employed in the *Deep Convolutional Generative Adversarial Networks* recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates images that look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8296899-5777-40c5-ac1d-5f259a7dae53.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, while these images look very blurry, the picture is an original one
    that is not present in the original dataset. There is a lot of scope for improvement
    in this output by varying the model architecture and having deeper layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variation of discriminator and generator loss values over increasing epochs
    looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/807a0f3c-24e4-42f7-87fa-48fffa9fff7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, from the preceding diagram, we might want to train the model for
    a fewer number of epochs so that the generator loss is not very high.
  prefs: []
  type: TYPE_NORMAL
- en: Face transition from one to another
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are in a position to generate faces, let's go ahead and perform
    some vector arithmetic on top of the generated images.
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, we will perform the transition of face generation from one
    face to another.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll continue from the image generation model that we built in the *Face generation
    using a Deep Convolutional GAN* section.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we want to see the transition of one generated face image into another
    generated face image. This process is enabled by slowly varying the vector from
    the first vector (the vector of the first generated image) to the second vector
    (the vector of the second generated image). You can essentially think of each
    of the latent (vector) dimensions as representing a certain aspect about the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy that we''ll adopt is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate two images
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Translate the first generated image in to the second generated image in 10 steps
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assigning a weight of 1 to the first generated image and a weight of 0 to the
    second generated image in the first step
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the second step, assign a weight of 0.9 to the first generated image and
    a weight of 0.1 to the second generated image
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the preceding steps until we assign a weight of 0 to the first generated
    image and a weight of 1 to the second generated image
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll code up the strategy that we laid out in the *Getting ready* section,
    as follows (The code file is available as `Face_generation.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate the first image from random noise (note that we''ll continue from
    step 6 in *Face generation using a Deep Convolutional GAN* section):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The generated image looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f786cff-d1c3-4593-acf5-c2471595c8c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generate the second image from random noise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6450874-5966-4739-9766-ea1b6f6c4997.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generate the visualization of obtaining the second image from the first image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfcd4d0b-296b-45d5-b28a-8ad685a06a23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in the preceding output, we have slowly transformed the first image
    into the second image.
  prefs: []
  type: TYPE_NORMAL
- en: Performing vector arithmetic on generated images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand that the latent vector representations play a key part
    in changing the outcome of the generated image, let's further build our intuition
    with images that have a certain face alignment.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy that we''ll adopt to perform vector arithmetic on a generated
    image is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate three images that are based on the random noise of 100 vector values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that two of the three images have generated faces that look to the left,
    and that one looks to the right
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate a new vector that is the sum of images that are aligned in the same
    direction, which is further subtracted from the image that is aligned in the opposite
    direction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the image from the resulting vector obtained in the previous step
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll code up the strategy that we listed out as follows (The code file is
    available as `Face_generation.ipynb` in GitHub). Note that we''ll continue from
    step 6 in *Face generation using a Deep Convolutional GAN* section):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate three vectors (ensure that two images are aligned in one direction
    and that the other is aligned in the opposite direction by varying the generated
    noise):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the generated images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The three generated images are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/424f17c6-4b45-4a97-a375-f043c7af965f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that images 2 and 3 have the face looking to the right, while image
    1 has the face looking straight ahead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform a vector arithmetic of the vector representations of each of these
    images to see the outcome:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates a face, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78804f8b-9305-493a-a5fb-fb140c532770.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding arithmetic shows that the vector arithmetic (vector of image 1
    + image 2 - image 3) generated an image has the face looking straight ahead and
    thus strengthening our intuition of the workings of latent vector representations.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have merely touched on the basics of GAN; there are a variety of GAN-based
    techniques that are currently becoming popular. We will discuss the applications
    of a few of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pix2pix**: Imagine a scenario where you doodle (sketch) the structure of
    an object and the object shapes up in a variety of forms. pix2pix is an algorithm
    that helps in enabling this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cycle GAN**: Imagine a scenario where you want an object to look like a completely
    different object (for example, you want a horse object to look like a zebra and
    vice versa). You also want to ensure that every other aspect of the image remains
    the same, except the change in the object. Cycle GAN comes in handy in such a
    scenario.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BigGAN** is a recent development that comes up with extremely realistic-looking
    generated images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
