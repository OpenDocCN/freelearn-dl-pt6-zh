- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml), *Deep Learning Basics*,
    we learned about a very high level overview of **Convolutional Neural Networks**
    (**CNNs**). In this chapter, we are going to understand more details about this
    type of CNN, the possible implementations of their layers, and we will start hands-on
    implementing CNNs through the DeepLearning4j framework. The chapter ends with
    examples involving Apache Spark too. Training and evaluation strategies for CNNs
    will be covered in [Chapter 7](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Training
    Neural Networks with Spark*, [Chapter 8](b30120ea-bd42-4cb7-95d9-5ecaa2b7c181.xhtml),
    *Monitoring and Debugging Neural Network Training*, and [Chapter 9](869a9495-e759-4810-8623-d8b76ba61398.xhtml),
    *Interpreting Neural Network Output*. In the description of the different layers,
    I have tried to reduce the usage of math concepts and formulas as much as possible
    in order to make the reading and comprehension easier for developers and data
    analysts who might have no math or data science background. Therefore, you have
    to expect more focus on the code implementation in Scala.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GoogleNet Inception V3 model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on CNN with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since the CNN section was covered in [Chapter 2](a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml), *Deep
    Learning Basics*, you should know in which context CNNs are commonly used. In
    that section, we have mentioned that each layer of the same CNN can have a different
    implementation. The first three sections of this chapter describe possible layer
    implementations in detail, starting from the convolutional layers. But first,
    let''s recap the process by which CNN perceive images. They perceive images as
    volumes (3D objects) and not as bi-dimensional canvases (having width and height
    only). The reason is the following: digital color images have a **Red**-**Blue**-**Green**
    (**RGB**) encoding and it is the mixing of these colors that produces the spectrum
    that can be perceived by human eyes. This also means that CNNs ingest images as
    three separate layers of color, one on top of the other. This translates into
    receiving a color image in the form of a rectangular box where width and height
    can be measured in pixels and having a three layers (referred as **channels**)
    depth, one for each RGB color. Cutting a long story short, an input image is seen
    by a CNN as a multi-dimensional array. Let''s give a practical example. If we
    consider a 480 x 480 image, it is perceived by the network as a 480 x 480 x 3
    array, for which each of its elements can have a value of between 0 and 255\.
    These values describe the pixel intensity at a given point. Here''s the main difference
    between the human eyes and a machine: these array values are the only inputs available
    to it. The output of a computer receiving those numbers as input will be other
    numbers describing the probability of the image being a certain class. The first
    layer of a CNN is always **convolutional**. Suppose having an input that is a
    32 x 32 x 3 array of pixel values, let''s try to imagine a concrete visualization
    that clearly and simply explains what a convolutional layer is. Let''s try to
    visualize a torch that shines over the top-left part of the image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This following diagram shows the torch shines, covering a 5 x 5 area:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82c754f7-f72d-46e3-859a-e98da3dba6e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: 5 x 5 filter'
  prefs: []
  type: TYPE_NORMAL
- en: Then the imaginary torch starts sliding over all the other areas of the image.
    The proper term to call it is **filter** (or **neuron** or **kernel**) and the
    image region that lights up is called the **receptive field**. In math terms,
    a filter is an array of numbers (called **weights** or **parameters**). The depth
    of a filter has to match the depth of the input. Referring to this section example,
    we have a filter that's dimensions are 5 x 5 x 3\. The first position the filter
    covers (as shown in the diagram in preceding diagram) is the top left corner of
    the input image. While the filter slides around the image, or convolves (from
    the Latin verb *convolvere*, which means to wrap up), it multiplies its values
    with the image original pixel values. The multiplications are then all summed
    up (in our example, in total we have 75 multiplications). The outcome is a single
    number, which represents when the filter is only at the top left of the input
    image. This process is then repeated for every location on the input image. As
    with the first one, every unique location produces a single number. Once the filter
    has completed its sliding process over all the locations of the image, the result
    is a 28 x 28 x 1 (given a 32 x 32 input image, a 5 x 5 filter can fit 784 different
    locations) numeric array called **activation map** (or **feature map**).
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is common practice (as you will see next through the code examples of this
    chapter and from [Chapter 7](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Training
    Neural Networks with Spark*, onward) to periodically insert a pooling layer between
    successive convolution layers in a CNN model. This kind of layers scope is to
    progressively reduce the number of parameters for the network (which translates
    into a significant lowering of the computation costs). In fact, spatial pooling
    (which is also found in literature as downsampling or subsampling) is a technique
    that reduces the dimensionality of each feature map, while at the same time retaining
    the most important part of the information. Different types of spatial pooling
    exist. The most used are max, average, sum, and L2-norm.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take as an example, max pooling. This technique requires defining a spatial
    neighborhood (typically a 2 × 2 window); the largest element within that window
    is then taken from the rectified feature map. The average pooling strategy requires
    taking the average or the sum of all elements in the given window. Several papers
    and use cases provide evidence that max pooling has been shown to produce better
    results than other spatial pooling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an example of max pooling operation (a 2 × 2 window
    is used here):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1546b290-0a65-447d-9bc8-13e8341f8c6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Max pooling operation using a 2 × 2 window'
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A fully connected layer is the last layer of a CNN. Fully connected layers,
    given an input volume, return as output a multi-dimensional vector. The dimension
    of the output vector matches the number of classes for the particular problem
    to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter and others in this book present some examples of CNN implementation
    and training for digit classification purposes. In those cases, the dimension
    of the output vector would be 10 (the possible digits are 0 to 9). Each number
    in the 10-dimensional output vector represents the probability of a certain class
    (digit). The following is an output vector for a digit classification inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[0 0 0 .1 .75 .1 .05 0 0 0]`'
  prefs: []
  type: TYPE_NORMAL
- en: How do we interpret those values? The network is telling us that it believes
    that the input image is a four with a 75% probability (which is the highest in
    this case), with a 10% probability that the image is a three, another 10% probability
    that the image is a five, and a 5% probability that the image is a six. A fully
    connected layer looks at the output of the previous layer in the same network
    and determines which features most correlate to a particular class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same happens not only in digit classification. An example of a general
    use case of image classification is that, if a model that has been trained using
    images of animals predicts that the input image is, for example, a horse, it will
    have high values in the activation maps that represent specific high-level features,
    like four legs or a tail, just to mention a couple. Similarly, if the same model
    predicts that an image is a different animal, let''s say a fish, it will have
    high values in the activation maps that represent specific high-level features,
    like fins or a gill. We can then say that a fully connected layer looks at those
    high-level features that most strongly correlate to a particular class and has
    particular weights: this ensures that the correct probabilities for each different
    class are obtained after the products between weights and the previous layer have
    been calculated.'
  prefs: []
  type: TYPE_NORMAL
- en: Weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs share weights in convolutional layers. This means that the same filter
    is used for each receptive field in a layer and that these replicated units share
    the same parameterization (weight vector and bias) and form a feature map.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows three hidden units of a network belonging to the
    same feature map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1de62be-e726-41dd-999d-490fa29ca2ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Hidden units'
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights in the darker gray color in the preceding diagram are shared and
    identical. This replication allows features detection regardless of the position
    they have in the visual field. Another outcome of this weight sharing is the following:
    the efficiency of the learning process increases by drastically reducing the number
    of free parameters to be learned.'
  prefs: []
  type: TYPE_NORMAL
- en: GoogleNet Inception V3 model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a concrete implementation of a CNN, in this section, I am going to present
    the GoogleNet ([https://ai.google/research/pubs/pub43022](https://ai.google/research/pubs/pub43022))
    architecture by Google ([https://www.google.com/](https://www.google.com/)) and
    its inception layers. It has been presented at the *ImageNet Large Scale Visual
    Recognition Challenge 2014* (*ILSVRC2014*, [http://www.image-net.org/challenges/LSVRC/2014/](http://www.image-net.org/challenges/LSVRC/2014/)). Needless
    to say, it won that competition. The distinct characteristic of this implementation
    is the following: increased depth and width and, at the same time, a constant
    computational budget. Improved computing resources utilization is part of the
    network design.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chart summarizes all of the layers for this network implementation presented
    in the context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b9d5d53-12f0-43de-a557-fa66e2b1ba98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: GoogleNet layers'
  prefs: []
  type: TYPE_NORMAL
- en: There are 22 layers with parameters (excluding the pooling layers; the total
    is 27 if they are included) and almost 12 times fewer parameters than the winning
    architecture of the past editions of the same context. This network has been designed
    keeping in mind computational efficiency and practicality, so that inference can
    be run also on individual devices having limited resources, in particular those
    with a low memory footprint. All the convolution layers use **Rectified Linear
    Unit** (**ReLU**) activation. The of the receptive field is 224 × 224 in the RGB
    color space (with zero means). Looking at the table in the preceding diagram,
    the **#3 × 3** and **#5 × 5** reduces are the number of 1 × 1 filters in the reduction
    layer preceding the 3 × 3 and 5 × 5 convolution layers. The activation function
    for those reduction layers is ReLU as well.
  prefs: []
  type: TYPE_NORMAL
- en: The diagram at [https://user-images.githubusercontent.com/32988039/33234276-86fa05fc-d1e9-11e7-941e-b3e62771716f.png](https://user-images.githubusercontent.com/32988039/33234276-86fa05fc-d1e9-11e7-941e-b3e62771716f.png) shows
    a schematic view of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this architecture, each unit from an earlier layer corresponds to a region
    of the input image—these units are grouped into filter banks. In the layers that
    are closer to the input, correlated units concentrate in local regions. This results
    in a lot of clusters concentrated in a single region, so they can be covered by
    a 1 × 1 convolution in the following layer. However, there could be a smaller
    number of more spatially split clusters covered by convolutions over larger chunks,
    and there would be a decreasing number of chunks over larger regions. To prevent
    those patch-alignment issues, the inception architecture implementations are restricted
    to use 1 × 1, 3 × 3 and 5 × 5 filters. The suggested architecture is then a combination
    of layers which output filter banks are aggregated in a single output vector,
    which represents the input of the next stage. Additionally, adding an alternative
    pooling path in parallel to each stage could have a further beneficial effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0dda3eb9-ac5d-4ba5-be6c-cf620bfe7afc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Naive version of the inception module'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the preceding diagram, you can understand that, in terms of computational
    cost, for a layer with a large number of filters, it could be too expensive to
    have 5 × 5 convolutions (even if there aren''t many). And, of course, this becomes
    a bigger problem when adding more pooling units, because the number of output
    filters is equal to the number of filters in the previous stage. Definitely merging
    the output of a pooling layer with outputs of a convolutional layer could inevitably
    lead to more and more outputs moving from stage to stage. For this reason, a second
    and more computational idea of the inception architecture has been proposed. The
    new idea is to reduce dimension where the computational requirements could increase
    too much. But there''s a caveat: low dimensional embeddings could contain lots
    of information about a large image chunk, but they represent information in a
    compressed form, making its processing hard. A good compromise is then to keep
    the representation mostly sparse and at the same time compress the signals only
    where there is a real need to heavily aggregate them. For this reason, in order
    to compute reductions, **1 × 1 convolutions** are used before any expensive **3
    × 3** and **5 × 5 convolutions**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the new module following the preceding consideration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/641e6658-04b0-41a6-bd8a-34a319764999.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Inception module with dimension reductions'
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on CNN with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections of this chapter, we went through the theory of CNNs
    and the GoogleNet architecture. If this is the first time you're reading about
    these concepts, probably you are wondering about the complexity of the Scala code
    to implement CNN's models, train, and evaluate them. Adopting a high-level framework
    like DL4J, you are going to discover how many facilities come out-of-the-box with
    it and that the implementation process is easier than expected.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to explore a real example of CNN configuration
    and training using the DL4J and Spark frameworks. The training data used comes
    from the `MNIST` database ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)).
    It contains images of handwritten digits, with each image labeled by an integer.
    It is used to benchmark the performance of ML and DL algorithms. It contains a
    training set of 60,000 examples and a test set of 10,000 examples. The training
    set is used to teach the algorithm to predict the correct label, the integer,
    while the test set is used to check how accurate the trained network can make
    guesses.
  prefs: []
  type: TYPE_NORMAL
- en: For our example, we download, and extract somewhere locally, the `MNIST` data.
    A directory named `mnist_png` is created. It has two subdirectories: `training`,
    containing the training data, and `testing`, containing the evaluation data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start using DL4J only first (we would add Spark to the stack later).
    The first thing we need to do is to vectorize the training data. We use `ImageRecordReader`
    ([https://deeplearning4j.org/datavecdoc/org/datavec/image/recordreader/ImageRecordReader.html](https://deeplearning4j.org/datavecdoc/org/datavec/image/recordreader/ImageRecordReader.html))
    as reader, because the training data are images, and a `RecordReaderDataSetIterator`
    ([http://javadox.com/org.deeplearning4j/deeplearning4j-core/0.4-rc3.6/org/deeplearning4j/datasets/canova/RecordReaderDataSetIterator.html](http://javadox.com/org.deeplearning4j/deeplearning4j-core/0.4-rc3.6/org/deeplearning4j/datasets/canova/RecordReaderDataSetIterator.html))
    to iterate through the dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s do a min-max scaling of the pixel values from 0-255 to 0-1, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The same vectorization needs to be done for the testing data as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s configure the network, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `MultiLayerConfiguration` object produced ([https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/MultiLayerConfiguration.html](https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/MultiLayerConfiguration.html))
    can then be used to initialize the model ([https://deeplearning4j.org/doc/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html](https://deeplearning4j.org/doc/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html)),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train (and evaluate) the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let's now put Apache Spark into the game. Through Spark, it is possible to parallelize
    the training and evaluation in memory across multiple nodes of a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, create a Spark context first, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, after vectorizing the training data, parallelize them through the Spark
    context, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The same needs to be done for the testing data as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'After configuring and initializing the model, you can configure Spark for training,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the Spark network, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, replace the previous training code with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When done, don''t forget to delete the temporary training files, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The full example is part of the source code shipped with the book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first went deeper into the CNN main concepts and explored
    one of the most popular and performing examples of the CNN architecture provided
    by Google. We started then to implement some code using DL4J and Spark.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will follow a similar trail to go deeper into RNNs.
  prefs: []
  type: TYPE_NORMAL
