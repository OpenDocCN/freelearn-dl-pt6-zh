<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Signal Processing - Data Analysis with Neural Networks</h1>
                </header>
            
            <article>
                
<p>Having acquired substantial knowledge on neural networks, we are now ready to perform our first operation using them. We will start with processing signals, and see how a neural network is fed data. You will be mesmerized at how increasing the levels and complexity of neurons can actually make a problem look simple. We will then look at how language can be processed. We will make several predictions using datasets.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Processing signals</li>
<li>Images as numbers</li>
<li>Feeding a neural network</li>
<li>Examples of tensors</li>
<li>Building a model</li>
<li>Compiling the model</li>
</ul>
<ul>
<li>Implementing weight regularization in Keras</li>
<li>Weight regularization experiments</li>
<li>Implementing dropout regularization in <span>Keras</span></li>
<li>Language processing</li>
<li>The internet movie reviews dataset</li>
<li>Plotting a single training instance</li>
<li>One-hot encoding</li>
<li>Vectorizing features</li>
<li>Vectorizing labels</li>
<li>Building a network</li>
<li>Callbacks</li>
<li>Accessing model predictions</li>
<li>Feature-wise normalization</li>
<li>Cross validation with the scikit-learn API</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Processing signals</h1>
                </header>
            
            <article>
                
<p class="mce-root">There may only be four fundamental forces in our universe, but they are all signals. By signal, we mean any kind of feature representations we may have of a real-world phenomenon. Our visual world, for example, is full of signals that indicate motion, color, and shapes. These are very dynamic signals, and it is a miracle that biology is able to process these stimuli so accurately, even if we do say so ourselves. Of course, in the grander scheme of things, realizing that nature has had hundreds of millions of years to perfect this recipe may humble us, if only a little. For now, we can admire the marvel that is the human visual cortex, which is equipped with 140 million densely interconnected neurons. In fact, an entire series of layers (V1 – V5) exist through which information propagates as we engage in progressively more complex image processing tasks. The eye itself, using rods and cones to detect different patterns of light intensity and colors, does an excellent job of piecing together electromagnetic radiation and converting it into electrical impulses through photo transduction.</p>
<p>When we look at an image, our visual cortex is actually interpreting the specific configuration of electromagnetic signals that the eye is converting into electrical signals and feeding it. When we listen to music, our eardrum, or myringa, simply converts and amplifies a successive pattern of vibrational signals so that our auditory cortex may process it. Indeed, it appears that the neural mechanisms in the brain are extremely efficient at abstracting and representing patterns that are present in different real-world signals. In fact, neuroscientists have even found that some mammalian brains have the capacity to be rewired in a manner that permits different cortices to process types of data that they were originally never intended to encounter. Most notably, scientists found that rewiring the auditory cortex of ferrets allowed these creatures to process visual signals from the auditory regions of the brain, allowing them to <em>see</em> using very different neurons that they previously employed for the task of audition. Many scientists cite such studies to put forth the case that the brain may be using a master algorithm, which is capable of handling any form of data, and turning it into efficient representations of the world around it.</p>
<p>As intriguing as this is, it naturally raises a thousand more questions about neural learning than it answers, and we sadly do not have the scope to address all of them in this book. Suffice it to say, whatever algorithm—or sets of algorithms—that lets our brain achieve such efficient representations of the world around us, are naturally of great interest to neurologists, deep learning engineers, and the rest of the scientific community alike.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Representational learning</h1>
                </header>
            
            <article>
                
<p>As we saw earlier with our perceptron experiments on the TensorFlow Playground, sets of artificial neurons seem to be capable of learning fairly simple patterns. This is nothing remotely close to the sort of complex representations we humans can perform and wish to predict. However, we can see that, even in their nascent simplicity, these networks seem to be able to adapt to the sort of data we provide them with, at times even outperforming other statistical predictive models. So, what's going on here that is so different than previous approaches to teach machines to do things for us?</p>
<p>It can be very useful to teach a computer what skin cancer looks like, simply by showing it the vast number of medically relevant features that we may have. Indeed, this is what our approach has been toward machines thus far. We would hand-engineer features so that our machines could easily digest them and generate relevant predictions. But why stop there? Why not just show the computer what skin cancer actually <em>looks</em> like? Why not show it millions of images and let <em>it</em> figure out what is relevant? Indeed, that's exactly what we try to do when we speak of deep learning. As opposed to traditional <strong>Machine Learning</strong> (<strong>ML</strong>) algorithms, where we represent the data in an explicitly processed representation for the machine to learn, we take a different approach with neural networks. Here, what we actually wish to achieve is for the network to learn these representations on its own.</p>
<p>As shown in the following diagram, a network achieves this by learning simple representations and using them to define more and more complex representations in successive layers, until the ultimate layer learns to represent output classes accurately:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1106 image-border" src="Images/884630bc-15a4-4cfe-980e-51bbe6ed2471.png" style="width:38.67em;height:30.33em;" width="416" height="326"/></p>
<p>This approach, as it turns out, can be quite useful for teaching your computer to detect complex movement patterns and facial expressions, just as we humans do. Say you want it to accept packages on your behalf when you're away, or perhaps detect any potential robbers trying to break in to your house. Similarly, what if we wanted our computer to schedule our appointments, find potentially lucrative stocks on the market, and keep us up to date according to what we find interesting? Doing so involves processing complex image, video, audio, text, and time-series data, all of which come in complex dimensional representations, and cannot be modeled by just a few neurons. So, how do we work with the neural learning system, akin to what we saw in the last chapter? How do we make neural networks learn the complex and hierarchical patterns in eyes, faces, and other real-world objects? Well, the obvious answer is that we make them bigger. But, as we will see, this brings in complexities of its own. Long story short, the more learnable parameters you put in a network, the higher the chance that it will memorize some random patterns, and hence will not generalize well. Ideally, you want a configuration of neurons that perfectly fits the learning job at hand, but this is almost impossible to determine a priori without performing experiments.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Avoiding random memorization</h1>
                </header>
            
            <article>
                
<p>Another answer can be to manipulate not only the overall number of neurons, but also the degree of interconnectivity among those neurons. We can do this through techniques such as <em>dropout regularization</em> and <em>weighted parameter</em>, as we will see soon enough. So far, we have already seen the various computations that can be performed through each neuron as data propagates through a network. We also saw how the brain leverages hundreds of millions of densely interconnected neurons to get the job done. But, naturally, we can't just scale up our networks by arbitrarily adding more and more neurons. Long story short, simulating a neural structure close to the brain is likely to require thousands of <strong>petaflops</strong> (a unit of computing speed equal to one thousand million million (10<sup>15</sup>) floating-point operations per second) of computing power. Maybe this will be possible in the near future, with the aforementioned paradigm of massively parallelized computing, along with other advances in software and hardware technologies. For now, though, we have to think of clever ways to train our network so that it can find the most efficient representations without wasting precious computational resources.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Representing signals with numbers</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will see how we can layer sequences of neurons to progressively represent more and more complex patterns. We will also see how concepts such as regularization and batched learning are essential in getting the most out of a training session. We will learn to process different types of real-world data in the form of images, texts, and time-series dependent information.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Images as numbers</h1>
                </header>
            
            <article>
                
<p>For tasks such as these, we need deep networks with multiple hidden layers if we are hoping to learn any representative features for our output classes. We also need a nice dataset to practice our understanding and familiarize ourselves with the tools we will be using to design our intelligent systems. Hence, we come to our first hands-on neural network task as we introduce ourselves to the concepts of computer vision, image processing, and hierarchical representation learning. Our task at hand is to teach computers to read numbers not as 0 and 1s, as they already do, but more in the manner of how we would read digits that are composed by our own kin. We are speaking of handwritten digits, and for this task, we will be using the iconic MNIST dataset, the true <em>hello world</em> of deep learning datasets. For our first example, there are good theoretical and practical reasons behind our choice.</p>
<p>From a theoretical perspective, we need to understand how we can use layer neurons to progressively learn more complex patterns, like our own brain does. Since our brain has had about 2,000 to 2,500 years worth of training data, it has gotten quite good at identifying complex symbols such as handwritten digits. In fact, we normally perceive this as an absolutely effortless task, since we learn how to distinguish between such symbols from as early as preschool. But this is actually quite a daunting task. Consider the vast variations in how each of these digits may be written by different humans, and yet our brains are still able to classify these digits, as if it were much ado about nothing:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-484 image-border" src="Images/cc8d8dcd-ce9c-4f0e-a1d9-180cbafdab03.png" style="width:15.75em;height:9.92em;" width="396" height="248"/></p>
<p>While exhaustively coding explicit rules would drive any programmer insane, as we look at the preceding image, our own brain intuitively notice some patterns in the data. For example, it picks up on how both <strong>2</strong> and <strong>3</strong> have a half-loop at the top of them, and how <strong>1</strong>, <strong>4</strong>, and <strong>7</strong> have a straight downward line. It also perceives how a <strong>4</strong> is actually one downward line, one semi-downward line, and another horizontal line in between the others. Due to this, we are able to easily break down a complex pattern into smaller patterns. This is specifically easy to do with handwritten digits, as we just saw. Therefore, our task will be to see how we can construct a deep neural network and hope for each of our neurons to capture simple patterns from our data, such as line segments, and then progressively construct more complex patterns in deeper layers using the simple patterns we learned in the previous layers. We will do this to learn about the accurate combinations of representations that correspond to our output classes.</p>
<p>Practically speaking, the MNIST dataset has been studied for about two decades by many pioneers in the field of deep learning. We have gained a good wealth of knowledge out of this dataset, making it ideal for exploring concepts such as layer representations, regularization, and overfitting, among others. As soon as we understand how to train and test a neural network, we can repurpose it for more exciting tasks.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feeding a neural network</h1>
                </header>
            
            <article>
                
<p>Essentially, all of the data that enters and propagates through the network is represented by a mathematical structure known as a <strong>tensor</strong>. This applies to audio data, images, video, and any other data we can think of, to feed our data-hungry network. In mathematics (<a href="https://en.wikipedia.org/wiki/Mathematics" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Mathematics</a>), a tensor is defined as an abstract and arbitrary geometric (<a href="https://en.wikipedia.org/wiki/Geometry" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Geometry</a>) entity that maps aggregations of vectors in a multi-linear (<a href="https://en.wikipedia.org/wiki/Linear_map" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Linear_map</a>) manner to a resulting tensor. In fact, vectors and scalars are considered simpler forms of tensors. In Python, tensors are defined with three specific properties, as follows:</p>
<ul>
<li><strong>Rank</strong>: Specifically, this denotes the number axes. A matrix is said to have the rank 2, as it represents a two-dimensional tensor. In Python libraries, this is often indicated as <kbd>ndim</kbd>.</li>
<li><strong>Shape</strong>: The shape of a tensor can be checked by calling the shape property on a NumPy <em>n</em>-dimensional array (which is how a tensor is represented in Python). This will return a tuple of integers, indicating the number of dimensions a tensor has along each axis.</li>
</ul>
<ul>
<li><strong>Content</strong>: This refers to the type of data that's stored in the tensor, and can be checked by calling the <kbd>type()</kbd> method on a tensor of interest. This will return data types such as float32, uint8, float64, and so on, except for string values, which are first converted into vector representations before being represented as a tensor.</li>
</ul>
<p>The following is a tensor graph. Don't worry about the complex diagram—we will look at what it means later:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1107 image-border" src="Images/6e3eaf25-2ec1-4b81-b958-4b51b38399a4.png" style="width:14.75em;height:13.50em;" width="300" height="274"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Examples of tensors</h1>
                </header>
            
            <article>
                
<p>The illustration we previously saw was that of a three dimensional tensor, yet tensors can appear in many forms. In the following section, we will overview some tensors of different ranks, starting with a tensor of rank zero:</p>
<ul>
<li><strong>Scalar</strong>: Values simply denote a single numeric value on its own. This can also be described as a tensor of dimension 0. An example of this is processing a single grayscale pixel of an image through a network.</li>
<li><strong>Vector</strong>: A bunch of scalars or an array of numbers is called a <strong>vector</strong>, or a tensor of rank 1. A 1D tensor is said to have exactly one axis. An example of this is processing a single flattened image.</li>
<li><strong>Matrix</strong>: A<span>n array of vectors is a matrix, or 2D tensor. A matrix has two axes (often referred to as</span> rows and columns). You can visually interpret a matrix as a rectangular grid of numbers. An example of this is processing a single grayscale image.</li>
<li><strong>Three-dimensional tensor</strong><span>: By packing several matrices into a new array, you get a 3D tensor, which is visually interpretable as a cube of numbers. An example of this is processing a dataset of grayscale images.</span></li>
</ul>
<ul>
<li><strong>Four-dimensional tensor</strong><span>: By packing 3D tensors in an array, you can create a 4D tensor, and so on. An example of this is processing a dataset of colored images.</span></li>
<li><strong>Five-dimensional tensor</strong>: These are created by p<span>acking 4D tensors in an array. An example of this is processing a dataset of videos.</span></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dimensionality of data</h1>
                </header>
            
            <article>
                
<p>So, consider the tensor of a shape (400, 600, 3). This is a common input shape that refers to a three-dimensional tensor that's used to represent a color image of 400 x 600 pixels. Since the MNIST dataset uses binary grayscale pixel values, we only deal with matrices of 28 x 28 pixels when representing an image. Here, each image is a tensor of dimension two, and the whole dataset can be represented by a tensor of dimension three. In a color image, each pixel value actually has three numbers, representing the amount of red, green, and blue light intensity represented by that pixel. Hence, with colored images, the two-dimensional matrices that are used to represent an image now scale up to three-dimensional tensors. Such a tensor is denoted by a tuple of (<em>x</em>, <em>y</em>, 3), where <em>x</em> and <em>y</em> represent the pixel dimensions of the image. Hence, a dataset of color images can be represented by a four-dimensional tensor, as we will see in later examples. For now, it is useful to know that we can use NumPy <em>n</em>-dimensional arrays to represent, reshape, manipulate, and store tensors in Python.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Making some imports</h1>
                </header>
            
            <article>
                
<p>So, let's get started, shall we? We will perform some simple experiments by leveraging all the concepts we have learned about in the previous chapters, and perhaps also encounter some new ones while on the job. We will use Keras, as well as the TensorFlow API, allowing us to also explore the eager execution paradigm. Our first task will be to implement a simple version of the multi-layered perceptron. This version is known as the <strong>feedforward neural network</strong>, and is a basic architecture that we can use to further explore some simple image classification examples. Obeying customary deep learning tradition, we will begin our first classification task by using the MNIST dataset for handwritten digits. This dataset has 70,000 grayscale images of digits between 0 and 9. The large size of this dataset is ideal, as machines require about 5,000 images per class to be able to come close to human-level performance at visual recognition tasks. The following code imports the libraries we will be using:</p>
<pre>import numpy as np<br/>import keras<br/>from keras.datasets import mnist<br/>from keras.utils import np_utils</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Keras's sequential API</h1>
                </header>
            
            <article>
                
<p>As you may well know, each Python library often comes with a core data abstraction that defines the data structure that the library is able to manipulate to perform computations. NumPy has its arrays, while pandas has its DataFrames. The core data structure of Keras is a model, which is essentially a manner to organize layers of interconnected neurons. We will start with the simplest type of model: the sequential model (<a href="https://keras.io/getting-started/sequential-model-guide/" target="_blank" rel="noopener noreferrer">https://keras.io/getting-started/sequential-model-guide/</a>). This is available as a linear stack of layers through the sequential API. More complex architectures also allow us to review the functional API, which is used to build custom layers. We will cover these later. The following code imports the sequential model, as well as some of the layers we will be using to build our first network:</p>
<pre>from keras.models import Sequential<br/>from keras.layers import Flatten, Dense, Dropout<br/>from keras.layers.core import Activation<br/>from keras import backend as K</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading the data</h1>
                </header>
            
            <article>
                
<p>Now, let's load in the data and split it up. Thankfully, MNIST is one of the core datasets that's already implemented in Keras, allowing a nice one-liner import, which also lets us split up our data in training and test sets. Of course, real-world data is not that easy to port and split up. A lot of useful tools exist for this purpose in <kbd>Keras.utils</kbd>, which we will cover briefly later, but also encourage you to explore. Alternatively, other <strong>ML</strong> libraries such as scikit-learn come with some handy tools (such as <kbd>train_test_split</kbd>, <kbd>MinMaxScaler</kbd>, and <kbd>normalizer</kbd>, to name a few methods), which, as their names indicate, let you split up, scale, and normalize your data as often required to optimize neural network training. Let's import and load the datasets, as follows:</p>
<pre>from keras.datasets import mnist<br/>(x_train, y_train),(x_test, y_test)= fashion_mnist.load_data()</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Checking the dimensions</h1>
                </header>
            
            <article>
                
<p>Next, we need to check out what our data looks like. We will do this by checking its type, then shape, and finally by plotting our individual observations using <kbd>matplotlib.pyplot</kbd>, like so:</p>
<pre>type(x_train[0]),x_train.shape,y_train.shape</pre>
<p>You will get the following result:</p>
<pre>(numpy.ndarray, (60000, 28, 28), (60000,))<br/></pre>
<p>Plotting the points:</p>
<pre>import matplotlib.-pypl<span>ot as plt<br/></span>%matplotlib inline<br/>plt.show(x_train[0], cmap= plt.cm.binary)<br/>&lt;matplotlib.image.AxesImage at 0x24b7f0fa3c8&gt;</pre>
<p>This will plot a figure similar to what's shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-531 image-border" src="Images/dc711508-0aac-4fb2-8243-f1056e15fef0.png" style="width:22.67em;height:21.17em;" width="272" height="254"/></p>
<p>As we can see, our training set has 60,000 images, with each image represented by a 28 x 28 matrix. When we represent our whole dataset, we are just representing a tensor of three dimensions (60,000 x 28 x 28). Now, let's rescale our pixel values, which usually lie between 0 and 225. Rescaling these values to values between 0 and 1 makes it a lot easier for our network to perform computations and learn predictive features. We encourage you to carry out experiments with and without normalization so that you can assess the difference in predictive power:</p>
<pre>x_train=keras.utils.normalize(x_train, axis=1)<br/>x_test=keras.utils.normalize(x_test, axis=1)<br/>plt.imshow(x_train[0], cmap=plt.cm.binary)</pre>
<p>The preceding code generates the following output:</p>
<pre>&lt;matplotlib.image.AxesImage at 0x24b00003e48&gt;</pre>
<p>The following plot is acquired:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-532 image-border" src="Images/369416be-0620-432c-8a0f-b9f1e2d10161.png" style="width:18.50em;height:17.50em;" width="268" height="253"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a model</h1>
                </header>
            
            <article>
                
<p>Now we can move on and build our predictive model. But before jumping into the interesting code, we must know the theory that surrounds a few important things.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introducting Keras layers</h1>
                </header>
            
            <article>
                
<p>The core building blocks of a neural network model in Keras is its layers. Layers are basically data-processing filters that <em>contort</em> the data they are fed into more useful representations. As we will see, prominent architectures of neural networks mostly vary in the manner in which layers are designed, and the interconnection of neurons among them. The inventor of Keras, Francois Chollet, describes this architecture as performing a <em>progressive distillation</em> on our data. Let's see how this works:</p>
<pre>#Simple Feedforward Neural Network<br/>model = Sequential()<br/><br/>#feeds in the image composed of 28 <img class="fm-editor-equation" src="Images/05463ac6-dc27-4d6b-9314-41420f978ccc.png" style="width:1.17em;height:0.92em;" width="140" height="110"/> 28 a pixel matrix as one sequence   <br/> of 784<br/>model.add(Flatten(input_shape=(28,28)))<br/>model.add(Dense(24, activation='relu'))<br/>model.add(Dense(8, activation='relu'))<br/>model.add(Dense(10, activation='softmax'))</pre>
<p>We define our model by initializing an instance of a blank model with no layers. Then, we add our first layer, which always expects an input dimension corresponding to the size of the data you want it to ingest. In our case, we want the model to ingest sets of 28 x 28 pixels, as we defined previously. The extra comma we added refers to how many examples the network will see at a time, as we will soon see. We also call the <kbd>Flatten()</kbd> method on our input matrix. All this does is convert each 28 x 28 image matrix into a single vector of 784-pixel values, each corresponding to its own input neuron.</p>
<p>We continue adding the layers until we get to our output layer, which has a number of output neurons corresponding to the number of our output classes—in this case, the 10 digits between 0 and 9. Do note that only the input layer needs to specify an input dimension of data entering it, as the progressive hidden layers are able to perform automatic shape inference (and only the first, because the following layers can do automatic shape inference).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Initializing weights</h1>
                </header>
            
            <article>
                
<p>We also have the option to initialize the neurons on each layer with specific weights. This is not a prerequisite, as they will be automatically initialized with small random numbers if not specified otherwise. Weight initialization practices are actually a whole separate sub-field of study in neural networks. It is prominently noted that the careful initialization of the network can significantly speed up the learning process.</p>
<p>You can use the <kbd>kernel_initializer</kbd> and <kbd>bais_initializer</kbd> parameters to set both the weights and biases of each layer, respectively. Remember that these very weights will represent the knowledge that's acquired by our network, which is why ideal initializations can significantly boost its learning:</p>
<pre>#feeds in the image composed of 28<img src="Images/783edfad-c8cc-4b5c-8ae3-6cb0be259503.png" style="width:1.17em;height:0.92em;" width="140" height="110"/>28 as one sequence of 784<br/>model.add(Flatten(input_shape=(28,28)))<br/>model.add(Dense(64, activation='relu',   <br/>          kernel_initializer='glorot_uniform',   <br/>          bias_initializer='zeros'))<br/>model.add(Dense(18, activation='relu'))<br/>model.add(Dense(10, activation='softmax'))</pre>
<p>A comprehensive review of the different parameter values is beyond the scope of this chapter. We may encounter some use cases where tweaking these parameters is beneficial later on (refer to chapter optimization). Some values for the <kbd>kernel_initializer</kbd> parameter include the following:</p>
<ul>
<li><kbd>glorot_uniform</kbd>: The weights are drawn from samples of uniform distributions between <kbd>-limit</kbd> and <kbd>limit</kbd>. Here, the term <kbd>limit</kbd> is defined as <kbd>sqrt(6 / (fan_in + fan_out))</kbd>. The term <kbd>fan_in</kbd> simply denotes the number of input units in the weight tensor, while <kbd>fan_out</kbd> is the number of output units in the weight tensor.</li>
<li><kbd>random_uniform</kbd>: The weights are randomly initialized with small uniform values ranging between -0.05 and 0.05.</li>
<li><kbd>random_normal</kbd>: The weights are initialized for obeying a Gaussian distribution[1], with a mean of 0 and a standard deviation of 0.05.</li>
<li><kbd>zero</kbd>: The layer weights are initialized at zero.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Keras activations</h1>
                </header>
            
            <article>
                
<p>At the moment, our network is composed of a flattened input layer, followed by a sequence of two dense layers, which are fully connected layers of neurons. The first two layers employ a <strong>Rectified Linear Unit</strong> (<strong>ReLU</strong>) activation function, which plots out a bit differently than the sigmoid we saw in <a href="e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml" target="_blank" rel="noopener noreferrer">Chapter 2</a>, <em>A</em> <em>Deeper Dive into Neural Networks</em>. In the following diagram, you can see how some of the different activation functions that are provided by Keras plot out. Remember, picking between them requires an intuitive understanding of the possible decision boundaries that may help with or hinder the partitioning your feature space. Using the appropriate activation function in conjunction with ideally initialized biases can be of paramount importance in some scenarios, but trivial in others. It is always advisable to experiment, leaving no stone unturned:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-533 image-border" src="Images/17d76c00-abee-4ac8-96d2-e2378f0f609b.png" style="width:39.75em;height:20.08em;" width="919" height="463"/></p>
<p>The fourth (and last) layer in our model is a 10-way Softmax layer. In our case, this means it will return an array of ten probability scores, all of which will add up to 1. Each score will be the probability that the current digit image belongs to one of our output classes. Hence, for any given input, a layer with the Softmax activation computes and returns the class probability of that input, with respect to each of our output classes.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summarizing your model visually</h1>
                </header>
            
            <article>
                
<p>Going back to our model, let's summarize the output of what we are about to train. You can do this in Keras by using the <kbd>summary()</kbd> method on the model, which is actually a shortcut for the longer <kbd>utility</kbd> function (and hence harder to remember), which is as follows:</p>
<pre>keras.utils.print_summary(model, line_length=<strong>None</strong>, positions=<strong>None</strong>,     <br/>                          print_fn=<strong>None</strong>)</pre>
<p>Using this, you can actually visualize the shapes of the individual layers of the neural network, as well as the parameters in each layer:</p>
<pre>model.summary()</pre>
<p>The preceding code generates the following output:</p>
<pre>_________________________________________________________________<br/>Layer (type) Output Shape Param # <br/>=================================================================<br/>flatten_2 (Flatten) (None, 784) 0 <br/>_________________________________________________________________<br/>dense_4 (Dense) (None, 1024) 803840 <br/>_________________________________________________________________<br/>dense_5 (Dense) (None, 28) 28700 <br/>_________________________________________________________________<br/>dense_6 (Dense) (None, 10) 290 <br/>=================================================================<br/>Total params: 832,830<br/>Trainable params: 832,830<br/>Non-trainable params: 0<br/>_________________________________________________________________</pre>
<p>As you can see, contrary to the perceptron we saw in <a href="e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml" target="_blank" rel="noopener noreferrer">Chapter 2</a>, <em>A</em> <em>Deeper Dive into Neural Networks</em>, this extremely simple model already has 51, 600 trainable parameters that are capable of scaling its learning almost exponentially compared to its ancestor.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Compiling the model</h1>
                </header>
            
            <article>
                
<p>Next, we will compile our Keras model. Compilation basically refers to the manner in which your neural network will learn. It lets you have hands-on control of implementing the learning process, which is done by using the <kbd>compile</kbd> method that's called on our <kbd>model</kbd> object. The method takes at least three arguments:</p>
<pre>model.compile(optimizer='resprop', #'sgd'<br/>              loss='sparse_categorical_crossentropy',<br/>              metrics=['accuracy'])</pre>
<p>Here, we describe the following functions:</p>
<ul>
<li><strong>A</strong> <kbd>loss</kbd> <strong>function</strong>: This simply measures our performance on the training data, compared to the true output labels. Due to this, the <kbd>loss</kbd> function can be used as an indication of our model's errors. As we saw earlier, this metric is actually a function that determines how far our model's predictions are from the actual labels of the output classes. We saw the <strong>Mean Squared Error</strong> (<strong>MSE</strong>) <kbd>loss</kbd> function in <a href="e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml" target="_blank" rel="noopener noreferrer">Chapter 2</a>, <em>A</em> <em>Deeper Dive into Neural Networks</em>, of which many variations exist. These <kbd>loss</kbd> functions are implemented in Keras, depending on the nature of our <strong>ML</strong> task. For example, if you wish to perform a binary classification (two output neurons representing two output classes), you are better off choosing binary cross-entropy. For more than two categories, you may try categorical cross-entropy, or sparse categorical cross-entropy. The former is used when your output labels are one-hot encoded, whereas the latter is used when your output classes are numerical categorical variables. For regression problems, we often advise the MSE <kbd>loss</kbd> function. When dealing with sequence data, as we will later, then <strong>Connectionist Temporal Classification</strong> (<strong>CTC</strong>) is deemed a more appropriate type of <kbd>loss</kbd> function. Other flavors of loss may differ in the manner they measure the distance between predictions and actual output labels (for example, <kbd>cosine_proximity</kbd> uses a cosine measure of distance), or the choice of probability distribution to model the predicted values (for example, the <strong>Poisson loss function</strong> is perhaps better if you are dealing with count data).</li>
<li><strong>An</strong> <kbd>optimizer</kbd>: An intuitive way to think of an optimizer is that it tells the network how to get to a global minimum loss. This includes the goal you want to optimize, as well as the size of the step it will take in the direction of your goal. Technically, the optimizer is often described as the mechanism that's employed by the network to self-update, which is does by using the data it is fed and the <kbd>loss</kbd> function. Optimization algorithms are used to update weights and biases that are the internal parameters of a model in the process of error reduction. There are actually two distinct types of optimization functions: functions with constant learning rates (such as <strong>Stochastic Gradient Decent</strong> (<strong>SGD</strong>)) and functions with adaptive learning rates (such as Adagrad, Adadelta, RMSprop, and Adam). The latter of the two are known for implementing heuristic-based and pre-parameterized learning rate methods. Consequentially, using adaptive learning rates can lead to less work in tuning the hyperparameters of your model.</li>
</ul>
<ul>
<li><kbd>metrics</kbd>: This simply denotes the evaluation benchmark we monitor during training and testing. Accuracy is most commonly used, but you may design and implement a custom metric through Keras, if you so choose. The main functional difference between the loss and accuracy score that's shown by the metric is that the accuracy measure is not involved in the training process at all, whereas loss is used directly in the training process by our optimizer to backpropagate the errors.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Fitting the model</h1>
                </header>
            
            <article>
                
<p>The <kbd>fit</kbd> parameter initiates the training session, and hence should be thought of as synonymous to training our model. It takes your training features, their corresponding training labels, the number of times the model sees your data, and the number of learning examples your model sees per training iteration as training measures, respectively:</p>
<pre>model.fit(x_train, y_train, epochs=5, batch_size = 2) #other arguments   <br/>                            validation split=0.33, batch_size=10</pre>
<p>You can also have additional arguments to shuffle your data, create validation splits, or give custom weights to output classes. Shuffling training data before each epoch can be useful, especially to ensure that your model does not learn any random non-predictive sequences in our data, and hence simply overfit the training set. To shuffle your data, you have to set the Boolean value of the shuffle argument to <strong>True</strong>. Finally, custom weights can be particularly useful if you have underrepresented classes in your dataset. Setting a higher weight is equivalent to telling your model, <em>Hey, you, pay more attention to these examples here</em>. To set custom weights, you have to provide the <kbd>class_weight</kbd> argument with a dictionary that maps class indices to custom weights corresponding to your output classes, in order of the indices that are provided.</p>
<p>The following is an overview of the key architectural decisions you will face when compiling a model. These decisions relate to the training process you instruct your model to undergo:</p>
<ul>
<li><kbd>epochs</kbd>: This argument must be defined as an integer value, corresponding to the number of times your model will iterate through the entire dataset. Technically, the model is not trained for a number of iterations given by epochs, but merely until the epoch of index epochs is reached. You want to set this parameter <em>just right</em>, depending on the nature of complexity you want your model to represent. Setting it too low will lead to simplistic representations that are used for inference, whereas setting it too high will make your model overfit on your training data.</li>
</ul>
<ul>
<li><kbd>batch_size</kbd>: The <kbd>batch_size</kbd> d<span>efines the number of samples that will be propagated through the network per training iteration. Intuitively, this can be thought of as the number of examples the network sees at a time while learning. Mathematically, this is simply the number of training instances the network will see before updating the model weights. So far, we have been updating our model weights at each training example (with a <kbd>batch_size</kbd> of 1), but this can quickly become a computational and memory management burden. This becomes especially cumbersome in instances where your dataset is too big to even load into memory.</span> <span>Setting a <kbd>batch_size</kbd> helps prevent this. Neural networks also train faster in mini-batches. In fact, batch size even has an impact on the accuracy of our gradient estimate during the backpropagation process, as shown in the following diagram. The same network is trained using three different batch sizes. Stochastic denotes random, or a batch size of one. As you can see, the direction of the stochastic and mini-batch gradients (green) fluctuates much more in comparison to the steady direction of the larger full-batch gradient (blue):</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span><img class="alignnone size-full wp-image-534 image-border" src="Images/5ba338bd-2549-4d34-9bc5-a36c8b994344.png" style="width:37.08em;height:18.83em;" width="925" height="469"/></span></p>
<ul>
<li>The <strong>number of</strong> <strong>iterations</strong> (which don't need to be explicitly defined) simply denotes the number of passes, where each pass contains the number of training examples denoted by the <kbd>batch_size</kbd>. To be clear, by one pass, we mean a forward filtering of data through our layers, as well as the backpropagation of errors. Suppose that we set our batch size to 32. One iteration encompasses our model by viewing 32 training examples, then updating its weights accordingly. In a dataset of 64 examples with a batch size of 32, it will take only two iterations for your model to cycle through it.</li>
</ul>
<p>Now that we have called the <kbd>fit</kbd> method on our training samples to initiate the learning process, we will observe the output, which simply displays the estimated training time, loss (in errors), and accuracy per epoch on our training data:</p>
<pre>Epoch 1/5<br/>60000/60000 [==========] - 12s 192us/step - loss: 0.3596 - acc: 0.9177<br/>Epoch 2/5<br/>60000/60000 [==========] - 10s 172us/step - loss: 0.1822 - acc: 0.9664<br/>Epoch 3/5<br/>60000/60000 [==========] - 10s 173us/step - loss: 0.1505 - acc: 0.9759<br/>Epoch 4/5<br/>60000/60000 [==========] - 11s 177us/step - loss: 0.1369 - acc:  <br/>                           0.97841s - loss: <br/>Epoch 5/5<br/>60000/60000 [==========] - 11s 175us/step - loss: 0.1245 - acc: 0.9822</pre>
<p>In only five full runs through our data, we achieve an accuracy of 0.96 (96.01%) during training. Now, we must verify whether our model is truly learning what we want it to learn by testing it on our secluded test set, which our model hasn't seen so far:</p>
<pre>model.evaluation(x_test, y_test)<br/><br/>10000/10000 [==============================] - 1s 98us/step<br/><span>[0.1425468367099762, 0.9759]</span></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Evaluating model performance</h1>
                </header>
            
            <article>
                
<p>Whenever we evaluate a network, we are actually interested in our accuracy of classifying images in the test set. This remains true for any ML model, as our accuracy on the training set is not a reliable indicator of our model's generalizability.</p>
<p>In our case, the test set accuracy is 95.78%, which is marginally lower than our training set accuracy of 96%. This is a classic case of overfitting, where our model seems to have captured irrelevant noise in our data to predict the training images. Since that inherent noise is different on our randomly selected test set, our network couldn't rely on the useless representations it had previously picked up on, and so performed poorly during testing. As we will see throughout this book, when testing neural networks, it is important to ensure that it has learnt correct and efficient representations of our data. In other words, we need to ensure that our network is not overfitting on our training data.</p>
<p>By the way, you can always visualize your predictions on the test set by printing out the label with the highest probability value for the given test subject and plotting the said test subject using Matplotlib. Here, we are printing out the label with maximum probability for test subject <kbd>110</kbd>. Our model thinks it is an <kbd>8</kbd>. By plotting the subject, we see that our model is right in this case:</p>
<pre>predictions= load_model.predict([x_test])<br/><br/>#predict use the inference graph generated in the model to predict class labels on our test set<br/>#print maximum value for prediction of x_test subject no. 110)<br/><br/>import numpy as np<br/>print(np.argmax(predictions[110]))<br/>-------------------------------------------<br/>8<br/>------------------------------------------<br/>plt.imshow(x_test[110]))<br/>&lt;matplotlib.image.AxesImage at 0x174dd374240&gt;<br/></pre>
<p>The preceding code generates the following output:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-535 image-border" src="Images/2ae3e1bf-72b5-43c4-8cf9-f8680081245d.png" style="width:20.67em;height:20.42em;" width="366" height="361"/></p>
<p>Once satisfied, you can save and load your model for later use, as follows:</p>
<pre>model.save('mnist_nn.model')<br/>load_model=kera.models.load_model('mnist_nn.model')</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Regularization</h1>
                </header>
            
            <article>
                
<p>So, what can you do to prevent a model from learning misleading or irrelevant patterns from the training data? Well, with neural networks, the best solution is to almost always get more training data. A model that's trained on more data will indeed allow your model to have better out-of-set predictivity. Of course, getting more data is not always that simple, or even possible. When this is the case, you have several other techniques at your disposal to achieve similar effects. One of them is to constrain your model in terms of the amount of information that it may store. As we saw in the <em>behind enemy lines</em> example in <a href="e54db312-2f54-4eab-a2c2-91b5a38d13f2.xhtml" target="_blank" rel="noopener noreferrer">Chapter 1</a>, <em>Overview of Neural Networks</em>, it is useful to find the most efficient representations of information, or representations with the lowest entropy. Similarly, if we can only afford our model the ability to memorize a small number of patterns, we are actually forcing it to find the most efficient representations that generalize better on other data that our model may encounter later on. This process of improving model generalizability through reducing overfitting is known as <strong>regularization</strong>, and will be go over it in more detail before we use it in practice.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Adjusting network size</h1>
                </header>
            
            <article>
                
<p>When we speak of a network's size, we simply mean the number of trainable parameters within the network. These parameters are defined by the number of layers in the network, as well as the number of neurons per each layer. Essentially, a network's size is a measure of its complexity. We mentioned how having too large a network size can be counterproductive and lead to overfitting. An intuitive way to think about this is that we should favor simpler representations over complex ones, as long as they achieve the same ends—<span>s</span>ort of a <em>lex parsimoniae</em>, if you will. The engineers who design such learning systems are indeed deep thinkers. The intuition here is that you could probably have various representations of your data, depending on your network's depth and number of neurons per layer, but we will favor simpler configurations and only progressively scale a network if required, to prevent it from using any extra learning capacity to memorize randomness. However, letting our model have too few parameters may well cause it to underfit, leaving it oblivious to the underlying trends we are trying to capture in our data. Through experimentation, you can find a network size that fits just right, depending on your use case. We force our network to be efficient in representing our data, allowing it to generalize better out of our training data. Beneath, we show a few experiments that are performed while varying the size of the network. This lets us compare how our loss on the validation set differs per epoch. As we will see, larger models are quicker to diverge away from the minimum loss values, and they will start to overfit on our training data almost instantly:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1108 image-border" src="Images/ae411f1a-470f-40de-9077-037c56a5be17.png" style="width:39.42em;height:25.25em;" width="538" height="345"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Size experiments</h1>
                </header>
            
            <article>
                
<p>Now we will perform some short experiments by varying the size of our network and gauging our performance. We will train six simple neural networks on Keras, each progressively larger than the other, to observe how these separate networks learn to classify handwritten digits. We will also present some of the results from the experiments. All of these models were trained with a constant batch size (<kbd>batch_size=100</kbd>), the <kbd>adam</kbd> optimizer, and <kbd>sparse_categorical_crossentropy</kbd> as a <kbd>loss</kbd> function, for the purpose of this experiment.</p>
<p>The following fitting graph shows how increasing our neural network's complexity (in terms of size) impacts our performance on the training and test sets of our data. Note that we are always aiming for a model that minimizes the difference between training and test accuracy/loss, as this indicates the minimum amount of overfitting. Intuitively, this simply shows us how much our networks learning benefits if we allocate it more neurons. By observing the increase in accuracy on the test set, we can see that adding more neurons does help our network to better classify images that it has never encountered before. This can be noticed until the <em>sweet spot</em>, which is where the training and test values are the closest to each other. Eventually, however, increases in complexity will lead to diminishing returns. In our case, our model seems to overfit the least at a dropout rate around 0.5, after which the accuracy of the training and test sets start to diverge:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-540 image-border" src="Images/b6d1ddb0-1ab8-46e3-936e-8690f653ea00.png" style="width:28.58em;height:18.08em;" width="455" height="289"/>   <img class="alignnone size-full wp-image-541 image-border" src="Images/a153a0d8-4f37-47d0-bcfa-ef6e702f760d.png" style="width:27.67em;height:17.92em;" width="438" height="284"/></p>
<p>To replicate these results by increasing the size of our network, we can tweak both the breadth (number of neurons per layer) and the depth of the network (number of layers in network). Adding depth to your network is done in Keras by adding layers to your initialized model by using <kbd>model.add()</kbd>. The <kbd>add</kbd> method takes the type of layer (for example, <kbd>Dense()</kbd>), as an argument. The <kbd>Dense</kbd> function takes the number of neurons to be initialized in that specific layer, along with the activation function to be employed for said layer, as arguments. The following is an example of this:</p>
<pre>model.add(Dense(512,  activation=’softmax’))</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Regularizing the weights</h1>
                </header>
            
            <article>
                
<p>Another way to make sure that your network doesn't pick up on irrelevant features is through regularizing the weights of our model. This simply allows us to put a constraint on the complexity of the network by limiting its layer weights to only take small values. All this does is make the distribution of layer weights more regular. How do we do this? By simply adding a cost to the <kbd>loss</kbd> function of our network. This cost actually represents a penalization for neurons that have larger weights. Conventionally, we implement this cost in three ways, namely L1, L2, and elastic net regularization:</p>
<ul>
<li><strong>L1 regularization</strong>: We add a cost that is proportional to the absolute value of our weighted coefficients.</li>
<li><strong>L2 regularization</strong>: We add a cost that is proportional to the square of the value of the weighted coefficients. This is also known as <strong>weight decay</strong>, as the weights exponentially decay to zero if no other update is scheduled.</li>
<li><strong>Elastic net regularization</strong>: This regularization method allows us to capture the complexity of our model by using a combination of both L1 and L2 regularization.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using dropout layers</h1>
                </header>
            
            <article>
                
<p>Finally, adding dropout neurons to layers is a technique that's widely used to regularize neural networks and prevent them from overfitting. Here, we, quite literally, drop out some neurons from our model at random. Why? Well, this results in a two-fold utility. Firstly, the contributions these neurons had for the activations of neurons further down our network are randomly ignored during a forward pass of data through our network. Also, any weight adjustments during the process of backpropagation are not applied to the neuron. While seemingly bizarre, there is good intuition behind this. Intuitively, neuron weights are adjusted at each backward pass to specialize a specific feature in your training data. But specialization breeds dependence. What often ends up happening is the surrounding neurons start relying on the specialization of a certain neuron in the vicinity, instead of doing some representational work themselves. This dependence pattern is often denoted as complex co-adaptation, a term that was coined by <strong>Artificial Intelligence</strong> (<strong>AI</strong>) researchers. One among them was Geoffrey Hinton, who was the original co-author of the backpropagation paper and is prominently referred to as the godfather of deep learning. Hinton playfully describes this behavior of complex coadaptation as <em>conspiracies</em> between neurons, stating that he was inspired by a fraud prevention system at his bank. This bank continuously rotated its employees so that whenever Hinton paid the bank a visit, he would always encounter a different person behind the desk.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Thinking about dropout intuitively</h1>
                </header>
            
            <article>
                
<p>For those of you who are familiar with Leonardo Dicaprio's movie <em>Catch me if you can</em>, you'll recall how Leonardo charmed the bank workers by asking them on dates and buying them treats, only so he could defraud the bank by cashing in his fake airline checks. In fact, due to the frequent fraternization of the employees and DiCaprio's character, the bank workers were paying more attention to irrelevant features such as DiCaprio's charm. What they should have actually been paying attention was the fact that DiCaprio was cashing out his monthly salary checks more than three times each month. Needless to say, businesses don't usually behave so generously. Dropping out some neurons is synonymous to rotating them to ensure that none of them get lazy and let a sleazy Leonardo defraud your network.</p>
<p>When we apply a dropout to a layer, we simply drop some of the outputs it would have otherwise given. Suppose a layer produces the vector [3, 5, 7, 8, 1] as an output for a given input. Adding a dropout rate of (0.4) to this layer would simply convert this output to [0, 5, 7, 0, 1]. All we did was initialize 40% of the scalars in our vector as zero.</p>
<p>Dropout only occurs during training. During testing, layers with dropouts have their outputs scaled down by the factor of the dropout rate that was previously used. This is actually done to adjust for the fact that more neurons are active during testing than training, as a result of the dropout mechanism.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing weight regularization in Keras</h1>
                </header>
            
            <article>
                
<p>So far, we have visited the theories behind three specific ways that allow us to improve our model's generalizability on unseen data. Primarily, we can vary our network size to ensure it has no extra learning capacity. We can also penalize inefficient representations by initializing weighted parameters. Finally, we can add dropout layers to prevent our network from getting lazy. As we noted previously, seeing is believing.</p>
<p>Now, let's implement our understanding using the MNIST dataset and some Keras code. As we saw previously, to change the network size, you are simply required to change the number of neurons per layer. This can be done in Keras during the process of adding layers, like so:</p>
<pre>import keras.regularizers<br/>model=Sequential()<br/>model.add(Flatten(input_shape=(28, 28)))<br/>model.add(Dense(1024, kernel_regularizer=  <br/>                      regularizers.12(0.0001),activation ='relu'))<br/>model.add(Dense(28, kernel_regularizer=regularizers.12(0.0001), <br/>          activation='relu'))<br/>model.add(Dense(10, activation='softmax'))</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Weight regularization experiments</h1>
                </header>
            
            <article>
                
<p>Simply put, regularizers let us apply penalties to layer parameters during optimization. These penalties are incorporated in to the <kbd>loss</kbd> function that the network optimizes. In Keras, we regularize the weights of a layer by passing a <kbd>kernel_regularizer</kbd> instance to a layer:</p>
<pre>import keras.regularizers<br/>model=Sequential()<br/>model.add(Flatten(input_shape=(28,28)))<br/>model.add(Dense(1024, kernel_regularizer=regularizers.12(0.0001), <br/>          activation='relu'))<br/>model.add(Dense(10, activation='softmax'))</pre>
<p>As we mentioned previously, we add L2 regularization to both our layers, each with an alpha value of (0.0001). The alpha value of a regularizer simply refers to the transformation that's being applied to each coefficient in the weight matrix of the layer, before it is added to the total loss of our network. In essence, the alpha value is used to multiply each coefficient in our weight matrix with it (in our case, 0.0001). The different regularizers in Keras can be found in <kbd>keras.regularizers</kbd>. The following diagram shows how regularization impacts validation loss per epoch on two models that are the same size. One observes that our regularized model is much less prone to overfitting, since the validation loss does not significantly increase as a function of time. On the model without regularization, we can clearly see that this is not the case, and after about seven epochs, the model starts overfitting, and so performs worse on the validation set:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-542 image-border" src="Images/b4c91389-24a7-4a3b-b6ba-d1febaf38000.png" style="width:32.83em;height:21.00em;" width="538" height="345"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing dropout regularization in Keras</h1>
                </header>
            
            <article>
                
<p>In Keras, adding a dropout layer is also very simple. All you are required to do is use the <kbd>model.add()</kbd> parameter again, and then specify a dropout layer (instead of the dense layer that we've been using so far) to be added. The <kbd>Dropout</kbd> parameter in Keras takes a float value that refers to the fraction of neurons whose predictions will be dropped. A very low dropout rate might not provide the robustness we are looking for, while a high dropout rate simply means we have a network prone to amnesia, incapable of remembering any useful representations. Once again, we strive for a dropout value that is just right; conventionally, the dropout rate is set between 0.2 and 0.4:</p>
<pre>#Simple feed forward neural network<br/>model=Sequential()<br/><br/>#feeds in the image composed of 28 <img class="fm-editor-equation" src="Images/97c4eb9c-b0a2-4f3b-9eec-3e75229a5345.png" style="width:1.17em;height:0.92em;" width="140" height="110"/> 28 a pixel matrix as one sequence of 784<br/>model.add(Flatten(input_shape=(28,28)))<br/>model.add(Dense(1024, activation='relu'))<br/>model.add(Dropout(0.3)<br/>model.add(Dense(28, activation='relu'))<br/>model.add(Dense(10, activation='softmax'))</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dropout regularization experiments</h1>
                </header>
            
            <article>
                
<p>The following are two experiments that we performed using a network of the same size, with different dropout rates, to observe the differences in performance. We started with a dropout rate of 0.1, and progressively scaled to 0.6 to see how this affected our performance in recognizing handwritten digits. As we can see in the following diagram, scaling our dropout rate seems to reduce overfitting, as the model's superficial accuracy on the training set progressively drops. We can see that both our training and test accuracy converges near the dropout rate of 0.5, after which they exhibit divergent behavior. This simply tells us that the network seems to overfit the least when a dropout layer of rate 0.5 is added:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-543 image-border" src="Images/dc7da382-5d59-4ffe-90e8-5960e26cd6ff.png" style="width:27.92em;height:17.00em;" width="432" height="263"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-544 image-border" src="Images/b9f13e80-67a8-47eb-b72b-275a841d8dbc.png" style="width:28.33em;height:18.17em;" width="416" height="266"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Complexity and time</h1>
                </header>
            
            <article>
                
<p>Now, you have seen some of the most prominent tricks in our repertoire to reduce overfitting through regularization. In essence, regularization is just a manner of controlling the complexity of our network. Complexity control is not just useful to restrict your network from memorizing randomness; it also brings more direct benefits. Inherently, more complex networks are computationally expensive. They will take longer to train, and hence consume more of your resources. While this makes an insignificant difference when dealing with the task at hand, this difference is still quite noticeable. In the following diagram is a time complexity chart. This is a useful way of visualizing training time as a function of network complexity. We can see that an increase in our network's complexity seems to have a quasi-exponential effect on the increase in average time taken per training iteration:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-546 image-border" src="Images/44b86b50-9754-4361-a6c5-6b9c9b979971.png" style="width:21.33em;height:10.50em;" width="502" height="246"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">A summary of MNIST</h1>
                </header>
            
            <article>
                
<p>So far in our journey, you were introduced to the fundamental learning mechanisms and processes that govern a neural network's functionality. You learned that neural networks need tensor representations of input data to be able to process it for predictive use cases. You also learned how different types of data that are found in our world, such as images, videos, text, and so on, can be represented as tensors of <em>n</em>-dimensions. Furthermore, you saw how to implement a sequential model in Keras, which essentially lets you build sequential layers of interconnected neurons. You used this model structure to construct a simple feedforward neural network for the task of classifying handwritten digits with the MNIST dataset. In doing so, you learned about the key architectural decisions to consider at each stage of model development.</p>
<p>During model construction, the main decisions pertain to defining the correct input size of your data, choosing a relevant activation function per layer, and defining the number of output neurons in your last layer, according to the number of output classes in your data. During the compilation process, you got to choose the optimization technique, <kbd>loss</kbd> function, and a metric to monitor your training progress. Then, you initiated the training session of your newly minted model by using the <kbd>.fit()</kbd> parameter, and passing the model the final two architectural decisions to be made before initiating the training procedure. These decisions pertained to the batch size of your data to be seen at a time, and the total number of epochs to train the model for.</p>
<p>Finally, you saw how to test your predictions, and learned about the pivotal concept of regularization. We concluded this classification task by experimenting with regularization techniques to modify our model's size, layer weights, and add dropout layers, which in turn helped us improve the generalizability of our model to unseen data. Lastly, we saw that increasing model complexity is unfavourable unless explicitly required due to the nature of our task:</p>
<ul>
<li><strong>Exercise x</strong>: Initialize different weighted parameters and see how this affects model performance</li>
<li><strong>Exercise y</strong>: Initialize different weights per layer and see how this affects model performance</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Language processing</h1>
                </header>
            
            <article>
                
<p>So far, we have seen how we can train a simple feedforward neural network on Keras for an image classification task. We also saw how we can mathematically represent image data as a high-dimensional geometric shape, namely a tensor. We saw that a higher-order tensor is simply composed of tensors of a smaller order. Pixels group up to represent an image, which in turn group up to represent an entire dataset. Essentially, whenever we want to employ the learning mechanism of neural networks, we have a way to represent our training data as a tensor. But what about language? How can we represent human thought, with all of its intricacies, as we do through language? You guessed it—we will use numbers once again. We will simply translate our texts, which are composed of sentences, which themselves are composed of words, into the universal language of mathematics. This is done through a process known as <strong>vectorization</strong>, which we will explore first-hand during our task of classifying the sentiment of movie reviews by using the <strong>internet movie database</strong> (<strong>IMDB</strong>) dataset.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Sentiment analysis</h1>
                </header>
            
            <article>
                
<p>As our computing power improved over the years, we started applying computational techniques to domains that were previously frequented only by linguists and qualitative academics. It turns out that tasks that were initially considered too time-consuming to pursue became ideal for computers to optimize as processors increased in potency. This led to an explosion of computer-assisted text analysis, not only in academia, but also in the industry. Tasks such as computer-assisted sentiment analysis can be specifically beneficial for various use cases. This can be used if you're a company trying to track your online customer reviews, or an employer wanting to do some identity management on social media platforms. In fact, even political campaigns increasingly consult services that monitor public sentiments and conduct opinion mining on a large variety of political topics. This helps politicians prepare their campaign points and understand the general aura of opinions that are held by people. While such use of technology can be quite controversial, it can vastly help organizations understand their flaws in products, services, and marketing strategies, while catering to their audience in a more relevant manner. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The internet movie reviews dataset</h1>
                </header>
            
            <article>
                
<p>The simplest form of sentiment analysis task deals with categorizing whether a piece of text represents a positive or negative opinion. This is often referred to as a <em>polar</em> or <em>binary sentiment classification task</em>, where 0 refers to a negative sentiment and 1 refers to a positive sentiment. We can, of course, have more complex sentiment models (perhaps using the big-five personality metrics we saw in <a href="e54db312-2f54-4eab-a2c2-91b5a38d13f2.xhtml" target="_blank" rel="noopener noreferrer">Chapter 1</a>, <em>Overview of Neural Networks</em>), but for the time being, we will concentrate on this simple yet conceptually loaded binary example. The example in question refers to classifying movie reviews from the Internet Movie Database or IMDB.</p>
<p>The IMDB dataset consists of 50,000 binary reviews, which are evenly split into positive and negative opinions. Each review consists of a list of integers, where each integer represents a word in that review. Once again, the guardians of Keras have thoughtfully included this dataset for practice, and hence can be found in Keras under <kbd>keras.datasets</kbd>. We encourage you to enjoy this importing data using Keras, as we won't be doing so in future exercises (nor will you be able to do it in the real world):</p>
<pre>import keras<br/>from keras.datasets import imdb<br/>(x_train,y_train), (x_test,y_test)=imdb.load_data(num_words=12000)</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading the dataset</h1>
                </header>
            
            <article>
                
<p>As we did previously, we load our dataset by defining our training instances and labels, as well as our test instances and labels. We are able to use the <kbd>load_data</kbd> parameter on <kbd>imdb</kbd> to load in our pre-processed data into a 50/50 train–test split. We can also indicate the number of most frequently occurring words we want to keep in our dataset. This helps us control the inherent complexity of our task as we work with review vectors of reasonable sizes. It is safe to assume that rare words occurring in reviews would have to do more with the specific subject matter of a given movie, and so they have little influence on the <em>sentiment</em> of that review in question. Due to this, we will limit the number of words to 12,000.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Checking the shape and type</h1>
                </header>
            
            <article>
                
<p>You can check the number of reviews per split by checking the <kbd>.shape</kbd> parameter of <kbd>x_train</kbd>, which is essentially a NumPy array of <em>n</em>-dimensions:</p>
<pre>x_train.shape, x_test.shape, type(x_train)<br/>((25000,), (25000,), numpy.ndarray)</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Plotting a single training instance</h1>
                </header>
            
            <article>
                
<p>As we can see, there are 25,000 training and test samples. We can also plot out an individual training sample to see how we can represent a single review. Here, we can see that each review simply contains a list of integers, where each integer corresponds to a word in a dictionary:</p>
<pre>x_train[1]<br/><br/>[1,
 194,
 1153,
 194,
 8255,
 78,
 228,
 5,
 6,
 1463,
 4369,
 5012,
 134,
 26,
 4,
 715,
 8,
 118,
 1634,
 14,
 394,
 20,
 13,
 119,
 954,</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Decoding the reviews</h1>
                </header>
            
            <article>
                
<p>If you're curious (which we are), we can of course map out the exact words that these numbers correspond to so that we can read what the review actually says. To do this, we must back up our labels. While this step is not essential, it is useful if we want to visually verify our network's predictions later on:</p>
<pre>#backup labels, so we can verify our networks prediction after vectorization<br/>xtrain = x_train<br/>xtest = x_test</pre>
<p>Then, we need to recover the words corresponding to the integers representing a review, which we saw earlier. The dictionary of words that were used to encode these reviews is included with the IMDB dataset. We will simply recover them as the <kbd>word_index</kbd> variable and reverse their order of storage. This basically allows us to map each integer index to its corresponding word:</p>
<pre>word_index =imdb.get_word_index()<br/>reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])</pre>
<p>The following function takes two arguments. The first one (<kbd>n</kbd>) denotes an integer referring to the n<sup>th</sup> review in a set. The second argument defines whether the n<sup>th</sup> review is taken from our training or test data. Then, it simply returns the string version of the review we specify.</p>
<p>This allows us to read out what a reviewer actually wrote. As we can see, in our function, we are required to adjust the position of indices, which are offset by three positions. This is simply how the designers of the IMDB dataset chose to implement their coding scheme, and so this is not of practical relevance for other tasks. The offset of the three positions in question occurs because positions 0, 1, and 2 are occupied by indices for padding, denoting the start of a sequence, and denoting unknown values, respectively:</p>
<pre>def decode_review(n, split= 'train'):<br/>if split=='train':<br/>    decoded_review=' '.join([reverse_word_index.get(i-3,'?')for i in <br/>                   ctrain[n]])<br/>elif split=='test':<br/>    decoded_review=' '.join([reverse_word_index.get(i-3,'?')for i in   <br/>                   xtest[n]])<br/>return decoded_review</pre>
<p>Using this function, we can decode review number five from our training set, as shown in the following code. It turns out that this is a negative review, as denoted by its training label, and inferred by its content. Note that the question marks are simply an indication of unknown values. Unknown values can occur inherently in the review (due to the use of emojis, for example) or due to the restrictions we have imposed (that is, if a word is not in the top 12,000 most frequent words that were used in the corpus, as stated earlier):</p>
<pre>print('Training label:',y_train[5])<br/>decode_review(5, split='train'),<br/>Training label: 0.0</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>Well then, what are we waiting for? We have a series of numbers representing each movie review, with their corresponding label, indicating (1) for positive or (0) for negative. This sounds like a classic structured dataset, so why not start feeding it to a network? Well, it's not that simple. Earlier, we mentioned that neural networks have a very specific diet. They are almost exclusively <em>Tensor-vores</em>, and so feeding them a list of integers won't do us much good. Instead, we must represent our dataset as a tensor of <em>n</em>-dimensions before we attempt to pass it on to our network for training. At the moment, you will notice that each of our movie reviews is represented by a separate list of integers. Naturally, each of these lists are of different sizes, as some reviews are smaller than others. Our network, on the other hand, requires the input features to be of the same size. Hence, we have to find a way to <em>pad</em> our reviews so that each of them represents a vector of the same length.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">One-hot encoding</h1>
                </header>
            
            <article>
                
<p>Since we know that the maximum number of unique words in our entire corpus is 12,000, we can assume that the longest possible review can only be 12,000 in length. Hence, we can make each review a vector of length 12,000, containing binary values. How does this work? Suppose we have a review of two words: <em>bad</em> and <em>movie</em>. A list containing these words in our dataset may look like [6, 49]. Instead, we can represent this same review as a 12,000-dimensional vector populated with 0s, except for the indices of 6 and 49, which would instead be 1s. What you're essentially doing is creating 12,000 dummy features to represent each review. Each of these dummy features represents the presence or absence of any of the 12,000 words in a given review. This approach is also known as <strong>one-hot encoding</strong>. It is commonly used to encode features and categorical labels alike in various deep learning scenarios.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Vectorizing features</h1>
                </header>
            
            <article>
                
<p>The following function will take our training data of 25,000 lists of integers, where each list is a review. In return, it spits out one-hot encoded vectors for each of the integer lists it received from our training set. Then, we simply redefine our training and test features by using this function to transform our integer lists into a 2D tensor of one-hot encoded review vectors:</p>
<pre>import numpy as np<br/>def vectorize_features(features):<br/><br/>#Define the number of total words in our corpus <br/>#make an empty 2D tensor of shape (25000, 12000)<br/>dimension=12000<br/>review_vectors=np.zeros((len(features), dimension))<br/><br/>#interate over each review <br/>#set the indices of our empty tensor to 1s<br/>for location, feature in enumerate(features):<br/>    review_vectors[location, feature]=1<br/>return review_vectors<br/><br/>x_train = vectorize_features(x_train)<br/>x_test = vectorize_features(x_test)</pre>
<p>You can see the result of our transformations by checking the type and shape of our training features and labels. You can also check what one individual vector looks like, as shown in the following code. We can see that each of our reviews is now a vector of length <kbd>12000</kbd>:</p>
<pre>type(x_train),x_train.shape, y_train.shape<br/>(numpy.ndarray, (25000, 12000), (25000,))<br/><br/>x_train[0].shape, x_train[0]<br/>((12000,), array([0., 1., 1., ..., 0., 0., 0.]), 12000)</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Vectorizing labels</h1>
                </header>
            
            <article>
                
<p>We can also vectorize our training labels, which simply helps our network handle our data better. You can think of vectorization as an efficient way to represent information to computers. Just like humans are not very good at performing computations using Roman numerals, computers are notoriously worse off when dealing with unvectorized data. In the following code, we are transforming our labels into NumPy arrays that contain 32-bit floating-point arithmetic values of either 0.0 or 1.0:</p>
<pre>y_train= np.asarray(y_train).astype('float32')<br/>y_test = np.asarray(y_test).astype('float32')</pre>
<p>Finally, we have our tensor, ready to be consumed by a neural network. This 2D tensor is essentially 25,000 stacked vectors, each with its own label. All that is left to do is build our network.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a network</h1>
                </header>
            
            <article>
                
<p>The first architectural constraints that you must consider while building a network with dense layers are its the depth and width. Then, you need to define an input layer with the appropriate shape, and successively choose from different activation functions to use per layer.</p>
<p>As we did for our MNIST example, we simply import the sequential model and the dense layer structure. Then we proceed by initializing an empty sequential model and progressively add hidden layers until we reach the output layer. Do note that our input layer always requires a specific input shape, which for us corresponds to the 12,000 - dimensional one-hot encoded vectors that we will be feeding it. In our current model, the output layer only has one neuron, which will ideally fire if the sentiment in a given review is positive; otherwise, it won't. We will choose <strong>Rectified Linear Unit</strong> (<strong>ReLU</strong>) activation functions for our hidden layers and a sigmoid activation function for the ultimate layer. Recall that the sigmoid activation function simply squished probability values between 0 and 1, making it quite ideal for our binary classification task. The ReLU activation function simply helps us zero out negative values, and hence can be considered a good default to begin with in many deep learning tasks. In summary, we have chosen a model with three densely interconnected hidden layers, containing 18, 12, and 4 neurons, respectively, as well as an output layer with 1 neuron:</p>
<pre>from keras.models import sequential <br/>from keras.layers import Dense<br/>model=Sequential()<br/>model.add(Dense(6, activation='relu', input_shape=(12000)))<br/>model.add(Dense(6, activation='relu'))<br/>model.add(Dense(1, activation='sigmoid'))</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Compiling the model</h1>
                </header>
            
            <article>
                
<p>Now we can compile our freshly built model, as is deep learning tradition. Recall that in the compilation process, the two key architectural decisions are the choice of the <kbd>loss</kbd> function, as well as the optimizer. The <kbd>loss</kbd> function simply helps us measure how far our model is from the actual labels at each iteration, whereas the optimizer determines how we converge to the ideal predictive weights for our model. In <a href="cd18f9ea-65ed-4ebd-af06-0403d3774be1.xhtml" target="_blank" rel="noopener noreferrer">Chapter 10</a>, <em>Contemplating Present and Future Developments</em>, we will review advanced optimizers and their relevance in various data processing tasks. For now, we will show how you can manually adjust the learning rate of an optimizer.</p>
<p>We have chosen a very small learning rate of 0.001 on the <strong>Root Mean Square</strong> (<strong>RMS</strong>) prop for demonstrative purposes. Recall that the size of the learning rate simply determines the size of the step we want out network to take in the direction of the correct output at each training iteration. As we mentioned previously, a big step can cause our network to <em>walk over</em> the global minima in the loss hyperspace, whereas a small learning rate can cause your model to take ages to converge to a minimum loss value:</p>
<pre>from keras import optimizers<br/>model.compile(optimizer=optimizers.RMSprop(1r=0.001),<br/>    loss='binary_crossentropy',<br/>    metrics=['accuracy']) </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Fitting the model</h1>
                </header>
            
            <article>
                
<p>In our previous MNIST example, we went over the least number of architectural decisions to get our code running. This lets us cover a deep learning workflow quite quickly, but at the expense of efficiency. You will recall that we simply used the <kbd>fit</kbd> parameter on our model and passed it our training features and labels, along with two integers denoting the epochs to train the model for, and the batch size per training iteration. The former simply defines how many times our data runs through the model, while the latter defines how many learning examples our model will see at a time before updating its weights. These are the two paramount architectural considerations that must be defined and adapted to the case at hand. However, there are several other useful arguments that the <kbd>fit</kbd> parameter may take.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Validation data</h1>
                </header>
            
            <article>
                
<p>You may be wondering why we train our model blindly for an arbitrary number of iterations and then test it on our holdout data. Wouldn't it be more efficient to gauge our model to some unseen data after each epoch, just to see how well we are doing? This way, we are able to assess exactly when our model starts to overfit, and hence end the training session and save some expensive hours of computing. We could show our model the test set after each epoch, without updating its weights, purely to see how well it does on our test data after that epoch. Since we do not update our model weights at each test run, we don't risk our model overfitting on the test data. This allows us to get a genuine understanding of how generalizable our model is <em>during</em> the training process, and not after. To test your model on a validation split, you can simply pass the validation features and labels as a parameter, as you did with your training data, to the <kbd>fit</kbd> parameter.</p>
<p>In our case, we have simply used the test features and labels as our validation data. In a rigorous deep learning scenario with high stakes, you may well choose to have separate test and validation sets, where one is used for validation during training, and the other is reserved for later assessments before you deploy your model to production. This is shown in the following code:</p>
<pre>network_metadata=model.fit(x_train, y_train,<br/>                           validation_data=(x_test, y_test),<br/>                           epochs=20,<br/>                           batch_size=100)</pre>
<p>Now, when you execute the preceding cell, you will see the training session initiate. Moreover, at the end of each training epoch, you will see that our model takes a brief pause to compute the accuracy and loss on the validation set, which is then displayed. Then, without updating its weights after this validation pass, the model proceeds to the next epoch for another training round. The preceding model will run for 20 epochs, where each epoch will iterate over our 25,000 <em>training</em> examples in batches of 100, updating the model weights after each batch. Note that in our case, the model weights are updated 250 times per epoch, or 5,000 times during the preceding training session of 20 epochs. So, now we can better assess when our model starts to memorize random features of our training set, but how do we actually interrupt the training session at this point? Well, you may have noticed that instead of just executing <kbd>model.fit()</kbd>, we defined it as <kbd>network_metadata</kbd>. As it happens, the <kbd>fit()</kbd> parameter actually returns a history object containing the relevant training statistics of our model, which we are interested in recovering. This history object is recorded by something called a <strong>callback</strong> in Keras.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Callbacks</h1>
                </header>
            
            <article>
                
<p>A <kbd>callback</kbd> is basically a Keras library function that can interact with our model during the training session to check on its internal state and save relevant training statistics for later scrutiny. While quite a few callback functions exist in <kbd>keras.callbacks</kbd>, we will introduce a few that are crucial. For those of you who are more technically oriented, Keras even lets you construct custom callbacks. To use a callback, you simply pass it to the <kbd>fit</kbd> parameter using the keyword argument <kbd>callbacks</kbd>. Note that the history callback is automatically applied to every Keras model, and so it does not need to be specified as long as you define the fitting process as a variable. This lets you recover the associated history object.</p>
<p>Importantly, if you initiated a training session previously in your Jupyter Notebook, then calling the <kbd>fit()</kbd> parameter on the model will continue training the same model. Instead, you want to reinitialize a blank model, before proceeding with another training run. This can be done by simply rerunning the cells where you previously defined and compiled your sequential model. Then, you may proceed by implementing a callback by passing it to the <kbd>fit()</kbd> parameter by using the <kbd>callbacks</kbd> keyword argument, as follows:</p>
<pre>early_stopping= keras.callbacks.EarlyStopping(monitor='loss')<br/>network_metadata=model.fit(x_train, y_train, validation_data=(x_test,  <br/>                           y_test), epochs=20, batch_size=100,  <br/>                           callbacks=[early_stopping]) </pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Early stopping and history callbacks</h1>
                </header>
            
            <article>
                
<p>In the preceding cell, we used a callback known as <strong>early stopping</strong>. This callback allows us to monitor a specific training metric. Our choices are between our accuracy or loss on the training set or on the validation set, which are all stored in a dictionary pertaining to our model's history:</p>
<pre class="mce-root">history_dict = network_metadata.history<br/>history_dict.keys()<br/>dict_keys(['val_loss','val_acc','loss','acc'])</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Choosing a metric to monitor</h1>
                </header>
            
            <article>
                
<p>The ideal choice is always <em>validation loss</em> or <em>validation accuracy</em>, as these metrics best represent the out of set predictability of our model. This is simply due to the fact that we only update our model weights during a training pass, and not a validation pass. Choosing our <em>training accuracy</em> or <em>loss</em> as a metric (as in the following code) is suboptimal in the sense that you are benchmarking your model by its own definition of a benchmark. To put this in a different way, your model might keep reducing its loss and increasing in accuracy, but it is doing so by rote memorization—not because it is learning general predictive rules as we want it to. As we can see in the following code, by monitoring our <em>training</em> loss, our model continues to decrease loss on the training set, even though the loss on the validation set actually starts increasing shortly after the very first epoch:</p>
<pre>import matplotlib.pyplot as plt<br/><br/>acc=history_dict['acc']<br/>loss_values=history_dict['loss']<br/>val_loss_values=history_dict['loss']<br/>val_loss_values=history_dict['val_loss']<br/><br/>epochs = range(1, len(acc) + 1)<br/>plt.plot(epochs, loss_values,'r',label='Training loss')<br/>plt.plot(epochs, val_loss_valuesm, 'rD', label-'Validation loss')<br/>plt.title('Training and validation loss')plt.xlabel('Epochs')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.show()</pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-548 image-border" src="Images/15feadd5-d291-4653-8c98-d90b611a6984.png" style="width:35.58em;height:23.75em;" width="427" height="285"/></p>
<p>We used Matplotlib to plot out the preceding graph. Similarly, you can clear out the previous loss graph and plot out a new accuracy graph of our training session, as shown in the following code. If we had used validation accuracy as a metric to track our early stopping callback, our training session would have ended after the <em>first</em> epoch, as <em>this</em> is the point in time where our model appears to be the most generalizable to unseen data:</p>
<pre>plt.clf()<br/>acc_values=history_dict['acc']<br/>val_acc_values=history_dict['val_acct']<br/>plt.plot(epochs, history_dict.get('acc'),'g',label='Training acc')<br/>plt.plot(epochs, history_dict.get('val_acc'),'gD',label='Validation acc')<br/>plt.title('Training and validation accuracy')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.show()</pre>
<p><span>The preceding code generates the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-549 image-border" src="Images/a902e7f2-2276-4c1a-ad14-22e84927d07d.png" style="width:31.83em;height:21.33em;" width="424" height="284"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Accessing model predictions</h1>
                </header>
            
            <article>
                
<p>In the MNIST example, we used the <em>Softmax</em> activation function as our last layer. You may recall that the layer generated an array of 10 probability scores, adding up to 1 for a given input. Each of those 10 scores referred to the likelihood of the image being presented to our network corresponding to one of the output classes (that is, it is 90% sure it sees a 1, and 10% sure it sees a 7, for example). This approach made sense for a classification task with 10 categories. In our sentiment analysis problem, we chose a sigmoid activation function, because we are dealing with binary categories. Using the sigmoid here simply forces our network to output a prediction between 0 and 1 for any given instance of data. Hence, a value closer to 1 means that our network believes that the given piece of information is more likely to be a positive review, whereas a value closer to zero states our network's conviction of having found a negative review. To view our model's predictions, we simply define a variable called <kbd>predictions</kbd> by using the <kbd>predict()</kbd> parameter on our trained model and passing it our test set. Now we can check our network predictions on a given example from this set, as follows:</p>
<pre>predictions=model.predict([x_test])<br/>predictions[5]</pre>
<p>In this case, it appears that our network is quite confident that review <kbd>5</kbd> from our test set is a positive review. Not only can we check whether this is indeed the case by checking the label stored in <kbd>y_test[5]</kbd>, we can also decode the review itself due to the decoder function we built earlier. Let's put our network's prediction to the test by decoding review <kbd>5</kbd> and checking its label:</p>
<pre>y_test[5], decode_review(5, split='test')</pre>
<p>It turns out our network is right. This is an example of a complex linguistic pattern that requires a higher-level understanding of linguistic syntax, real-world entities, relational logic, and the propensity for humans to blabber aimlessly. Yet, with only 12 neurons, our network has seemingly understood the underlying sentiment that's encoded in this piece of information. It makes a prediction with a high degree of certainty (99.99%), despite the presence of words such as <em>disgusting</em>, which are very likely to appear in negative reviews.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Probing the predictions</h1>
                </header>
            
            <article>
                
<p>Let's examine another review. To probe our predictions, we will make a few functions that will help us visualize our results better. Such a gauging function can also be used if you want to restrict your model's predictions to instances where it is most certain:</p>
<pre>def gauge_predictions(n):<br/>if (predictions[n]&lt;=0.4) and (y_test[n]==0):<br/>    print('Network correctly predicts that review %d is negative' %(n))<br/>elif (predictions[n] &lt;=0.7) and (y_test[n]==1);<br/>elif (predictions[n]&gt;-0.7) and (y_test[n]==0):<br/>else:<br/>    print('Network is not so sure. Review mp. %d has a probability score of %(n),   <br/>           predictions[n])<br/>def verify_predictions(n):<br/>    return gauge_predictions(n), predictions[n], decode_review(n, split='test')</pre>
<p>We will make two functions to help us better visualize our network's errors, while also limiting our predictive accuracy to upper and lower bounds. We will use the first function to arbitrarily define good predictions as instances where the network has a probability score above 0.7, and bad instances where the score is below 0.4, for a <em>positive review</em>. We simply reverse this scheme for the negative reviews (a good prediction score for a <em>negative review</em> is below 0.4 and a bad one is above 0.7). We also leave a middle ground between 40 and 70%, labeled as uncertain predictions so that we can better understand the reason behind its accurate and inaccurate guesses. The second function is designed for simplicity, taking an integer value that refers to the <em>n</em>th review you want to probe and verify as input, and returning an assessment of what the network thinks, the actual probability score, as well as what the review in question reads. Let's use these newly forged functions to probe yet another review:</p>
<pre>verify-predictions(22)<br/>network falsely predicts that review 22 is negative</pre>
<p>As we can see, our network seems to be quite sure that review <kbd>22</kbd> from our test set is negative. It has generated a probability score of 0.169. You could also interpret this score as that our network believed with 16.9% confidence that this review is positive, and so it must be negative (since these are the only two classes we used to train our network). It turns out that our network got this one wrong. Reading the review, you will notice that the reviewer actually expresses their appreciation for what they deemed to be an undervalued movie. Note that the tone is quite ambiguous at the beginning, with words like <em>silly</em> and <em>fall flat</em>. However, contextual valence shifters later on in the sentence allow our biological neural networks to determine that the review actually expresses a positive sentiment. Sadly, our artificial network does not seem to have caught up with this particular pattern. Let's continue our exploratory analysis using yet another example:</p>
<pre>verify_predictions(19999)<br/>Network is not so sure. Review no. 19999 has a probability score of [0.5916141]</pre>
<p>Here, we can see that our network is not too sure about the review, even though it has actually guessed the correct sentiment in the review, with a probability score of 0.59, which is closer to 1 ( positive) than 0 (negative). To us, this review clearly appears positive—even a bit promotionally pushy. It is intuitively unclear why our network is not certain of the sentiment. Later in this book, we will learn how to visualize word embeddings using our network layers. For now, let's continue our probing with one last example:</p>
<pre>verify_predictions(4)<br/>Network correctly predicts that review 4 is positive</pre>
<p>This time, our network gets it right again. In fact, our network is 99.9% sure that this is a positive review. While reading the review, you'll notice that it has actually done a decent job, as the review contains words like <em>boring</em>, <em>average</em>, and suggestive language such as <em>mouth shut</em>, which could all easily be present in other negative reviews, potentially misleading our network. As we can see, we conclude this probing session by providing a short function that you can play around with by randomly checking your network's predictions for a given number of reviews. We then print out our network's predictions for two randomly chosen reviews from our test set:</p>
<pre>from random import randint<br/>def random_predict(n_reviews):<br/>for i in range(n_reviews):<br/>print(verify_predictions(randint(0, 24000)))<br/>random_predict(2)<br/>Network correctly predicts that review 20092 is positive</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary of IMDB</h1>
                </header>
            
            <article>
                
<p>Now you should have a better idea of how to go about processing natural language texts and dialogues through a simple feedforward neural network. In this subsection of our journey, you learned how to execute a binary sentiment classification task using a feedforward neural network. In doing so, you learned how to pad and vectorize your natural language data, preparing it for processing with neural networks. You also went over the key architectural changes that are involved in binary classification, such as using an output neuron and the sigmoid activation function on the last layer of our network. You also saw how you can leverage a validation split in your data to get an idea of how your model performs on unseen data after each training epoch. Moreover, you learned how to indirectly interact with your model during the training process by using Keras callbacks. Callbacks can be useful for a variety of use cases, ranging from saving your model at a certain checkpoint or terminating the training session when a desired metric has reached a certain point. We can use the history callback to visualize training statistics, and we can use the early stopping callback to designate a moment to terminate the current training session. Finally, you saw how you can probe your network's predictions per review to better understand what kind of mistakes it makes:</p>
<ul>
<li class="mce-root"><strong>Exercise</strong>: Improve performance with regularization, as we did in the MNIST example.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Predicting continuous variables</h1>
                </header>
            
            <article>
                
<p>So far, we have performed two classification tasks using neural networks. For our first task, we classified handwritten digits. For our second task, we classified sentiments in movie reviews. But what if we wanted to predict a continuous value instead of a categorical value? What if we wanted to predict how likely an event may occur, or the future price of a given object? For such a task, examples such as predicting prices in a given market may come to mind. Hence, we will conclude this chapter by coding another simple feedforward network by using the Boston Housing Prices dataset.</p>
<p>This dataset resembles most real-world datasets that data scientists and machine learning practitioners would come across. You are given 13 features that refer to a specific geographical area located in Boston. With these features, the task at hand is to predict the median price of houses. The features themselves include various indicators ranging from residential and industrial activity, level of toxic chemicals in the air, property tax, access to education, and other socio-economic indicators that are associated with location. The data was collected during the mid-1970s, and seems to have brought along some bias from the time. You will notice that some features seem very nuanced and perhaps even inappropriate. Features such as feature number 12 can be very controversial to use in machine learning projects. You must always consider the higher-level implications when using a certain source or type of data. It is your duty as a machine learning practitioner to ensure that your model does not introduce or reinforce any sort of societal bias, or contribute in any way to disparities and discomfort for people. Remember, we are in the business of using technology to alleviate human burden, not add to it.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Boston Housing Prices dataset</h1>
                </header>
            
            <article>
                
<p>As we mentioned in the previous section, this dataset contains 13 training features that are represented on an observed geographical region.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading the data</h1>
                </header>
            
            <article>
                
<p>The dependent variable that we are interested in predicting is the housing price per location, which is denoted as a continuous variable that denotes house prices in thousands of dollars.</p>
<p>Hence, each of our observations can be represented as a vector of dimension 13, with a corresponding scalar label. In the following code, we are plotting out the second observation in our training set, along with its corresponding label:</p>
<pre>import keras<br/>from keras.datasets import boston_housing.load_data()<br/>(x_train, y_train),(x_test,y_test)=boston_housing.load_data()<br/>x_train[1], y_train[1]</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Exploring the data</h1>
                </header>
            
            <article>
                
<p>This dataset is a much smaller dataset in comparison to the ones we've dealt with so far. We can only see 404 training observations and <kbd>102</kbd> test observations:</p>
<pre>print(type(x_train),'training data:',x_train.shape,'test data:',x_test.shape)<br/>&lt;class 'numpy.ndarray'&gt;training data:(403, 13) test data: (102, 13)</pre>
<p>We will also generate a dictionary containing the description of our features so that we can understand what each of them actually encodes:</p>
<pre>column_names=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LST  <br/>  AT']<br/><br/>key= ['Per capita crime rate.',<br/>    'The proportion of residential land zoned for lots over 25,000   <br/>     square feet.',<br/>    'The proportion of non-retail business acres per town.',<br/>    'Charles River dummy variable (=1 if tract bounds river; 0 <br/>     otherwise).',<br/>    'Nitric oxides concentration (parts per 10 million).',<br/>    'The average number of rooms per dwelling.',<br/>    'The porportion of owner-occupied units built before 1940.',<br/>    'Weighted distances to five Boston employment centers.',<br/>    'Index of accessibility to radial highways.',<br/>    'Full-value property tax rate per $10,000.',<br/>    'Pupil-Teacher ratio by town.',<br/>    '1000*(Bk-0.63)**2 where Bk is the proportion of Black people by <br/>     town.',<br/>    'Percentage lower status of the population.'}</pre>
<p>Now let's create a pandas <kbd>DataFrame</kbd> and have a look at the first five observations in our training set. We will simply pass out the training data, along with the previously defined column names, as arguments to the pandas <kbd>DataFrame</kbd> constructor. Then, we will use the <kbd>.head()</kbd> parameter on our newly forged <kbd>.DataFrame</kbd> object to get a nice display, as follows:</p>
<pre>import pandas as pd<br/>df= pd.DataFrame(x_train, columns=column_names)<br/>df.head()</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feature-wise normalization</h1>
                </header>
            
            <article>
                
<p>We can see that each feature in our observation seems to be on a different scale. Some values range in the hundreds, while others are between 1 and 12, or even binary. While neural networks may still ingest unscaled features, it almost exclusively prefers to deal with features on the same scale. In practice, a network can learn from heterogeneously scaled features, but it may take much longer to do so without any guarantee of finding an ideal minimum on the loss landscape. To allow our network to learn in an improved way for this dataset, we must homogenize our data through the process of feature-wise normalization. We can achieve this by subtracting the feature-specific mean and dividing it by the feature-specific standard deviation for each feature in our dataset. Note that in live-deployed models (for the stock exchange, for example), such a scaling measure is impractical, as the means and standard deviation values may keep on changing, depending on new, incoming data. In such scenarios, other normalization and standardization techniques (such as log normalization, for example) are better to use:</p>
<pre>mean=x_train.mean(axis=0)<br/>std=x_train.std(axis=0)<br/>x_train=(x_train-mean)/std<br/>x_test=(x_test-mean)/std<br/>print(x_train[0]) #First Training sample, normalized</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<p>The main architectural difference in this regression model, as opposed to the previous classification models we built, is to do with the way we construct the last layer of this network. Recall that in a classic scalar regression problem, such as the one at hand, we aim to predict a continuous variable. To implement this, we avoid using an activation function in our last layer, and use only one output neuron.</p>
<p>The reason we forego an activation function is because we do not want to constrain the range that the output values of this layer may take. Since we are implementing a purely linear layer, our network is able to learn to predict a scalar continuous value, just as we want it to:</p>
<pre>from keras.layers import Dense, Dropout<br/>from keras.models import Sequential<br/>model= Sequential()<br/>model.add(Dense(26, activation='relu',input_shape=(13,)))<br/>model.add(Dense(26, activation='relu'))<br/>model.add(Dense(12, activation='relu'))<br/>model.add(Dense(1))</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Compiling the model</h1>
                </header>
            
            <article>
                
<p>The main architectural difference during compilation here is to do with the <kbd>loss</kbd> function and metric we choose to implement. We will use the MSE <kbd>loss</kbd> function to penalize higher prediction errors, while monitoring our model's training progress with the <strong>Mean Absolute Error</strong> (<strong>MAE</strong>) metric:</p>
<pre>from keras import optimizers<br/>model.compile(optimizer= opimizers.RMSprop(lr=0.001),<br/>              loss-'mse',<br/>              metrics=['mae'])<br/>model.summary()<br/><strong>__________________________________________________________
Layer (type)                 Output Shape              Param #   
==========================================================
dense_1 (Dense)              (None, 6)                 72006     
__________________________________________________________
dense_2 (Dense)              (None, 6)                 42        
__________________________________________________________
dense_3 (Dense)              (None, 1)                 7         
==========================================================
Total params: 72,055
Trainable params: 72,055
Non-trainable params: 0
__________________________________________________________</strong></pre>
<p>As we saw previously, the MSE function measures the average of the squares of our network's prediction errors. Simply put, we are simply measuring the average squared difference between the estimated and actual house price labels. The squared term emphasizes the spread of our prediction errors by penalizing the errors that are further away from the mean. This approach is especially helpful with regression tasks where small error values still have a significant impact on predictive accuracy.</p>
<p>In our case, our housing price labels range between 5 and 50, measured in thousands of dollars. Hence, an absolute error of 1 actually means a difference of $1,000 in prediction. Thus, taking using an absolute error-based <kbd>loss</kbd> function might not give the best feedback mechanism to the network.</p>
<p class="mce-root">On the other hand, the choice of MAE as a metric is ideal to measure our training progress itself. Visualizing squared errors, as it turns out, is not very intuitive to us humans. It is better to simply see the absolute errors in our models' predictions, as it is visually more informative. Our choice of metric has no actual impact on the training mechanism of the model—it is simply providing us with a feedback statistic to visualize how good or bad our model is doing during the training session. The MAE metric itself is essentially a measure of difference between two continuous variables.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Plotting training and test errors</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the following graph, we can see that the average error is about 2.5 (or $2,500 dollars). While this may be a small variance when predicting the prices on houses that cost $50,000, it starts to matter if the house itself costs $5,000:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1109 image-border" src="Images/bb27c237-7146-42c8-935c-ec9d9825c23c.png" style="width:36.00em;height:29.92em;" width="682" height="567"/></p>
<p>Finally, let's predict some housing prices using data from the test set. We will use a scatter plot to plot the predictions and actual labels of our test set. In the following graph, we can see a line of best fit, along with the data points. Our model seems to capture the general trend in our data, despite having some outlandish predictions for some points:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1110 image-border" src="Images/e6f365c3-5ccb-4e46-b5ac-1c14c9d5819b.png" style="width:22.83em;height:22.00em;" width="460" height="447"/></p>
<p>Moreover, we can plot a histogram that shows the distribution of our prediction errors. It appears that our model seems to do pretty well on most counts, but has some trouble predicting certain values, while overshooting and undershooting for a small number of observations, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1112 image-border" src="Images/ab62f15f-c7ec-4917-9000-a2143ca576fb.png" style="width:22.58em;height:19.75em;" width="420" height="367"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Validating your approach using k-fold validation</h1>
                </header>
            
            <article>
                
<p>We noted earlier how our dataset is significantly smaller than the ones we previously dealt with. This leads to several complications while training and testing. Primarily, splitting the data into training and test samples, as we did, left us with only 100 validation samples. This is hardly enough for us to assuredly deploy our model, even if we wanted to. Furthermore, our test scores may change a lot depending on which segment of the data ended up in the test set. Hence, to reduce our reliance on any particular segment of our data for testing our model, we adopted a common machine learning approach known as <strong>k-fold cross validation</strong>. Essentially, we split our data into <em>n</em> number of smaller partitions and used the same number of neural networks to train on each of those smaller partitions of our data. Hence, a k-fold cross validation with five folds will split up our entire training data of 506 samples into five splits of 101 samples (and one with 102). Then, we use five different neural networks, each of which trains on four splits out of the five data splits and tests itself on the remaining split of data. Then, we simply average the predictions from our five models to generate a single estimation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1113 image-border" src="Images/32ce8785-1897-46ee-8e63-c0c4a34e6a87.png" style="width:39.92em;height:21.83em;" width="643" height="352"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cross validation with scikit-learn API</h1>
                </header>
            
            <article>
                
<p>The advantage of cross validation over repeated random sub-sampling is that all of the observations are used for both training and validation, and each observation is used for validation exactly once.</p>
<p>The following code shows you how to implement a five-fold cross validation in Keras, where we use the entire dataset (training and testing together) and print out the averaged predictions of a network on each of the cross validation runs. As we can see, this is achieved by training the model on four random splits and testing it on the remaining split, per each cross validation run. We use the scikit-learn API wrapper provided by Keras and leverage the Keras regressor, along with sklearn's standard scaler, k-fold cross-validator creator, and score evaluator:</p>
<pre>import numpy as np<br/>import pandas as pd<br/>​<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.wrappers.scikit_learn import KerasRegressor<br/>​<br/>​<br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.model_selection import KFold<br/>​<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.pipeline import Pipeline<br/><br/>from keras.datasets import boston_housing<br/>(x_train,y_train),(x_test,y_test) = boston_housing.load_data()<br/><br/>x_train.shape, x_test.shape<br/><br/>---------------------------------------------------------------<br/><strong>((404, 13), (102, 13))<br/></strong>----------------------------------------------------------------<br/><br/>import numpy as np<br/><br/>x_train = np.concatenate((x_train,x_test), axis=0)<br/>y_train = np.concatenate((y_train,y_test), axis=0)<br/><br/>x_train.shape, y_train.shape<br/><br/>-----------------------------------------------------------------<br/><strong>((506, 13), (506,))</strong><br/>-----------------------------------------------------------------</pre>
<p>You will notice that we constructed a function named <kbd>baseline_model()</kbd> to build our network. This is a useful way of constructing networks in many scenarios, but here it helps us feed the model object to the <kbd>KerasRegressor</kbd> function that we are using from the scikit-learn API wrapper that Keras provides. As many of you may well be aware, scikit-learn has been the go-to Python library for ML, with all sorts of pre-processing, scaling, normalizing, and algorithmic implementations. The Keras creators have implemented a scikit-learn wrapper to enable a certain degree of cross functionality between these libraries:</p>
<pre>def baseline_model():<br/>    model = Sequential()<br/>    model.add(Dense(13, input_dim=13, kernel_initializer='normal', <br/>              activation='relu'))<br/> model.add(Dense(1, kernel_initializer='normal'))<br/> model.compile(loss='mean_squared_error', optimizer='adam')<br/> return model</pre>
<p>We will take advantage of this cross functionality to perform our k-fold cross validation, as we did previously. Firstly, we will initialize a random number generator with a constant random seed. This simply gives us consistency in initializing our model weights, helping us to ensure that we can compare future models consistently:</p>
<pre>#set seed for reproducability <br/>seed = 7<br/>numpy.random.seed(seed)<br/><br/># Add a data Scaler and the keras regressor containing our model function to a list of estimators<br/><br/>estimators = []<br/>estimators.append(('standardize', StandardScaler()))<br/>estimators.append(('mlp', KerasRegressor(build_fn=baseline_model,   <br/>                    epochs=100, batch_size=5, verbose=0)))<br/>    <br/>#add our estimator list to a Sklearn pipeline<br/><br/>pipeline = Pipeline(estimators)<br/> <br/>#initialize instance of k-fold validation from sklearn api<br/><br/>kfold = KFold(n_splits=5, random_state=seed)<br/><br/>#pass pipeline instance, training data and labels, and k-fold crossvalidator instance to evaluate score<br/><br/>results = cross_val_score(pipeline, x_train, y_train, cv=kfold)<br/><br/>#The results variable contains the mean squared errors for each of our     <br/> 5 cross validation runs.<br/>print("Average MSE of all 5 runs: %.2f, with standard dev: (%.2f)" %   <br/>      (-1*(results.mean()), results.std()))<br/><br/><br/>------------------------------------------------------------------<br/><strong>Model Type: &lt;function larger_model at 0x000001454959CB70&gt;</strong><br/><strong>MSE per fold:</strong><br/><strong>[-11.07775911 -12.70752338 -17.85225084 -14.55760158 -17.3656806 ]</strong><br/><strong>Average MSE of all 5 runs: 14.71, with standard dev: (2.61)<br/></strong></pre>
<p>We will create a list of estimators to pass to the sklearn transformation pipeline, which is useful to scale and process our data in sequence. To scale our values this time, we simply use the <kbd>StandardScaler()</kbd> preprocessing function from sklearn and append it to our list. We also append the Keras wrapper object to the same list. This Keras wrapper object is actually a regression estimator called <kbd>KerasRegressor</kbd>, and takes the model function we created, along with the desired number of batch size and training epochs as arguments. <strong>Verbose</strong> simply means how much feedback you want to see during the training process. By setting it to <kbd>0</kbd>, we ask our model to train silently.</p>
<div class="packt_infobox">Note that these are the same parameters that you would otherwise pass along to the <kbd>.fit()</kbd> function of the model, as we did earlier to initiate our training sessions.</div>
<p>Running the preceding code gives us an estimate of the average performance of our network for the five cross-validation runs we executed. The <kbd>results</kbd> variable stores the MSE scores of our network for each run of the cross validator. We then print out the mean and standard deviation (average variance) of MSEs over all five runs. Notice that we multiplied our mean value by <kbd>-1</kbd>. This is simply an implementational issue, as the unified scoring API of scikit-learn always maximizes a given score. However, in our case, we are trying to minimize our MSE. Hence, scores that need to be minimized are negated so that the unified scoring API can work correctly. The score that is returned is the negative version of the actual MSE.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we saw how we can perform a regression task with neural networks. This involved some simple architectural changes to our previous classification models, pertaining to model construction (one output layer with no activation function) and the choice of <kbd>loss</kbd> function (MSE). We also tracked the MAE as a metric, since squared errors are not very intuitive to visualize. Finally, we plotted out our model's predictions versus the actual prediction labels using a scatter plot to better visualize how well the network did. We also used a histogram to understand the distribution of prediction errors in our model.</p>
<p class="mce-root">Finally, we introduced the methodology of k-fold cross validation, which is preferred over explicit train test splits of our data, in cases where we deal with very few data observations. What we did instead of splitting our data into a training and test split was split it into a <em>k</em> number of smaller partitions. Then, we generated a single estimate of predictions by using the same number of models as our data subsets. Each of these models were trained on a <em>k</em>-1 number of data partitions and tested on the remaining one data partition, after which their prediction scores were averaged. Doing so prevents our reliance on any particular split of our data for testing, and hence we get a more generalizable prediction estimate.</p>
<p>In the next chapter, we will learn about <strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>). We will implement CNNs and detect objects using them. We will also solve some image recognition problems.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">Implement three different functions, each returning a network varying in size (depth and width). Use each of these functions and perform a k-fold cross validation. Assess which size fits best.</li>
<li class="mce-root">Experiment with MAE and MSE <kbd>loss</kbd> functions, and note the difference during training.</li>
<li class="mce-root">Experiment with different <kbd>loss</kbd> functions and note the differences during training.</li>
<li class="mce-root">Experiment with different regularization techniques and note the differences during training.</li>
</ul>


            </article>

            
        </section>
    </div>



  </body></html>