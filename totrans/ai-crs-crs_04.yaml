- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI Foundation Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you'll begin your study of AI theory in earnest. You'll start
    with an introduction to a major branch of AI, called Reinforcement Learning, and
    the five principles that underpin every Reinforcement Learning model. Those principles
    will give you the theoretical understanding to make sense of every forthcoming
    AI model in this book.
  prefs: []
  type: TYPE_NORMAL
- en: What is Reinforcement Learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When people refer to AI today, some of them think of Machine Learning, while
    others think of Reinforcement Learning. I fall into the second category. I always
    saw Machine Learning as statistical models that have the ability to learn some
    correlations, from which they make predictions without being explicitly programmed.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this is, in some way, a form of AI, Machine Learning does not include
    the process of taking actions and interacting with an environment like we humans
    do. Indeed, as intelligent human beings, what we constantly keep doing is the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: We observe some input, whether it's what we see with our eyes, what we hear
    with our ears, or what we remember in our memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These inputs are then processed in our brain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eventually, we make decisions and take actions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process of interacting with an environment is what we are trying to reproduce
    in terms of Artificial Intelligence. And to that extent, the branch of AI that
    works on this is Reinforcement Learning. This is the closest match to the way
    we think; the most advanced form of Artificial Intelligence, if we see AI as the
    science that tries to mimic (or surpass) human intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning also has the most impressive results in business applications
    of AI. For example, Alibaba leveraged Reinforcement Learning to increase its ROI
    in online advertising by 240% without increasing their advertising budget (see
    [https://arxiv.org/pdf/1802.09756.pdf](https://arxiv.org/pdf/1802.09756.pdf),
    page 9, Table 1 last row (DCMAB)). We'll tackle the same industry application
    in this book!
  prefs: []
  type: TYPE_NORMAL
- en: The five principles of Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's begin building the first pillars of your intuition into how Reinforcement
    Learning works. These are the fundamental principles of Reinforcement Learning,
    which will get you started with the right, solid basics in AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the five principles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Principle #1**: The input and output system'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Principle #2**: The reward'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Principle #3**: The AI environment'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Principle #4**: The Markov decision process'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Principle #5**: Training and inference'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following sections, you can read about each one in turn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle #1 – The input and output system'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step is to understand that today, all AI models are based on the common
    principle of inputs and outputs. Every single form of Artificial Intelligence,
    including Machine Learning models, ChatBots, recommender systems, robots, and
    of course Reinforcement Learning models, will take something as input, and will
    return another thing as output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The input and output system'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Reinforcement Learning, these inputs and outputs have a specific name: the
    input is called the state, or input state. The output is the action performed
    by the AI. And in the middle, we have nothing other than a function that takes
    a state as input and returns an action as output. That function is called a policy.
    Remember the name, "policy," because you will often see it in AI literature.'
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider a self-driving car. Try to imagine what the input and
    output would be in that case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input would be what the embedded computer vision system sees, and the output
    would be the next move of the car: accelerate, slow down, turn left, turn right,
    or brake. Note that the output at any time (*t*) could very well be several actions
    performed at the same time. For instance, the self-driving car can accelerate
    while at the same time turning left. In the same way, the input at each time (*t*)
    can be composed of several elements: mainly the image observed by the computer
    vision system, but also some parameters of the car such as the current speed,
    the amount of gas remaining in the tank, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s the very first important principle in Artificial Intelligence: it is
    an intelligent system (a policy) that takes some elements as input, does its magic
    in the middle, and returns some actions to perform as output. Remember that the
    inputs are also called the **states**.'
  prefs: []
  type: TYPE_NORMAL
- en: The next important principle is the reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle #2 – The reward'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every AI has its performance measured by a reward system. There's nothing confusing
    about this; the reward is simply a metric that will tell the AI how well it does
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest example is a binary reward: 0 or 1\. Imagine an AI that has to
    guess an outcome. If the guess is right, the reward will be 1, and if the guess
    is wrong, the reward will be 0\. This could very well be the reward system defined
    for an AI; it really can be as simple as that!'
  prefs: []
  type: TYPE_NORMAL
- en: 'A reward doesn''t have to be binary, however. It can be continuous. Consider
    the famous game of *Breakout*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The Breakout game'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine an AI playing this game. Try to work out what the reward would be in
    that case. It could simply be the score; more precisely, the score would be the
    accumulated reward over time in one game, and the rewards could be defined as the derivative
    of that score.
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the many ways we could define a reward system for that game.
    Different AIs will have different reward structures; we will build five rewards
    systems for five different real-world applications in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that in mind, remember this as well: the ultimate goal of the AI will
    always be to maximize the accumulated reward over time.'
  prefs: []
  type: TYPE_NORMAL
- en: Those are the first two basic, but fundamental, principles of Artificial Intelligence
    as it exists today; the input and output system, and the reward. The next thing
    to consider is the AI environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle #3 – The AI environment'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The third principle is what we call an "AI environment." It is a very simple
    framework where you define three things at each time (*t*):'
  prefs: []
  type: TYPE_NORMAL
- en: The input (the state)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output (the action)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward (the performance metric)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each and every single AI based on Reinforcement Learning that is built today,
    we always define an environment composed of the preceding elements. It is, however,
    important to understand that there are more than these three elements in a given
    AI environment.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you are building an AI to beat a car racing game, the environment
    will also contain the map and the gameplay of that game. Or, in the example of
    a self-driving car, the environment will also contain all the roads along which
    the AI is driving and the objects that surround those roads. But what you will
    always find in common when building any AI, are the three elements of state, action,
    and reward. The next principle, the Markov decision process, covers how they work
    in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle #4 – The Markov decision process'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Markov decision process, or MDP, is simply a process that models how the
    AI interacts with the environment over time. The process starts at *t* = 0, and
    then, at each next iteration, meaning at *t* = 1, *t* = 2, … *t* = *n* units of
    time (where the unit can be anything, for example, 1 second), the AI follows the
    same format of transition:'
  prefs: []
  type: TYPE_NORMAL
- en: The AI observes the current state, ![](img/B14110_04_001.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The AI performs the action, ![](img/B14110_04_002.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The AI receives the reward, ![](img/B14110_04_003.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The AI enters the following state, ![](img/B14110_04_004.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The goal of the AI is always the same in Reinforcement Learning: it is to maximize
    the accumulated rewards over time, that is, the sum of all the ![](img/B14110_04_005.png)
    received at each transition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graphic will help you visualize and remember an MDP better, the
    basis of Reinforcement Learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14110_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The Markov Decision process'
  prefs: []
  type: TYPE_NORMAL
- en: Now four essential pillars are already shaping your intuition of AI. Adding
    a last important one completes the foundation of your understanding of AI. The
    last principle is training and inference; in training, the AI learns, and in inference, it predicts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Principle #5 – Training and inference'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final principle you have to understand is the difference between training
    and inference. When building an AI, there is a time for the training mode, and
    a separate time for inference mode. I'll explain what that means starting with
    the training mode.
  prefs: []
  type: TYPE_NORMAL
- en: Training mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now you understand, from the three first principles, that the very first step
    of building an AI is to build an environment in which the input states, the output
    actions, and a system of rewards are clearly defined. From the fourth principle,
    you also understand that inside this environment we will build an AI to interact
    with it, trying to maximize the total reward accumulated over time.
  prefs: []
  type: TYPE_NORMAL
- en: To put it simply, there will be a preliminary (and long) period of time during
    which the AI will be trained to do that. That period of time is called the training;
    we can also say that the AI is in training mode. During that time, the AI tries
    to accomplish a certain goal over and over again until it succeeds. After each
    attempt, the parameters of the AI model are modified in order to do better at
    the next attempt.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say you''re building a self-driving car and you want it
    to go from point *A* to point *B*. Let''s also imagine that there are some obstacles
    that you want your self-driving car to avoid. Here is how the training process
    happens:'
  prefs: []
  type: TYPE_NORMAL
- en: You choose an AI model, which can be Thompson Sampling (*Chapters 5* and *6*),
    Q-learning (*Chapters 7* and *8*), deep Q-learning (*Chapters 9*, *10,* and *11*)
    or even deep convolutional Q-learning (*Chapters* *12* and *13*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You initialize the parameters of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Your AI tries to go from *A* to *B* (by observing the states and performing
    its actions). During this first attempt, the closer it gets to *B*, the higher
    reward you give to the AI. If it fails reaching *B* or hits an obstacle, you give
    the AI a very bad reward. If it manages to reach *B* without hitting any obstacle,
    you give the AI an extremely good reward. It''s just like you would train a dog
    to sit: you give the dog a treat or say "good boy" (positive reward) if the dog sits.
    And you give the dog whatever small punishment you need to if the dog disobeys
    (negative reward). That process is training, and it works the same way in Reinforcement
    Learning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of the attempt (also called an episode), you modify the parameters
    of the model in order to do better next time. The parameters are modified intelligently,
    either iteratively through equations (Q-Learning), or by using Machine Learning
    and Deep Learning techniques such as stochastic gradient descent or backpropagation.
    All these techniques will be covered in this book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You repeat steps 3 and 4 again, and again, until you reach the desired performance;
    that is, until you have your fully non-dangerous autonomous car!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, that's training. Now, how about inference?
  prefs: []
  type: TYPE_NORMAL
- en: Inference mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inference mode simply comes after your AI is fully trained and ready to perform
    well. It will simply consist of interacting with the environment by performing
    the actions to accomplish the goal the AI was trained to achieve before in training
    mode. In inference mode, no parameters are modified at the end of each episode.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine you have an AI company that builds customized AI solutions
    for businesses, and one of your clients asked you to build an AI to optimize the
    flows in a smart grid. First, you'd enter an R&D phase during which you would
    train your AI to optimize these flows (training mode), and as soon as you reached
    a good level of performance, you'd deliver your AI to your client and go into
    production. Your AI would regulate the flows in the smart grid only by observing
    the current states of the grid and performing the actions it has been trained
    to do. That's inference mode.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, the environment is subject to change, in which case you have to alternate
    fast between training and inference modes so that your AI can adapt to the new
    changes in the environment. An even better solution is to train your AI model
    every day, and go into inference mode with the most recently trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'That was the last fundamental principle common to every AI. Congratulations
    – now you already have a solid basic understanding of Artificial Intelligence!
    Since you have that, you are ready to tackle your very first AI model in the next
    chapter: a simple yet very powerful one, still widely used today in business and
    marketing, to solve a problem that has the delightful name of the multi-armed
    bandit problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, you learned the five fundamental principles of Artificial
    Intelligence from a Reinforcement Learning perspective. Firstly, an AI is a system
    that takes an observation (values, images, or any data) as input, and returns
    an action to perform as output (principle #1). Then, there is a reward system
    that helps it measure its performance. The AI will learn through trial and error
    based on the reward it gets over time (principle #2). The input (state), the output
    (action), and the reward system define the AI environment (principle #3). The
    AI interacts with this environment through the Markov decision process (principle
    #4). Finally, in training mode, the AI learns how to maximize its total reward
    by updating its parameters through the iterations, and in inference mode, the
    AI simply performs its actions over full episodes without updating any of its
    parameters – that is to say, without learning (principle #5).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about Thompson Sampling, a simple Reinforcement
    Learning model, and use it to solve the multi-armed bandit problem.
  prefs: []
  type: TYPE_NORMAL
