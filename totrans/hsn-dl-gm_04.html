<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">GAN for Games</h1>
                </header>
            
            <article>
                
<p class="mce-root">Thus far, in our deep learning exploration, we have trained all our networks using a technique called <strong>supervised training</strong>. This training technique works well for when you have taken the time to identify and label your data. All of our previous example exercises used supervised training, because it is the simplest form of teaching. However, supervised learning tends to be the most cumbersome and tedious method, largely because it requires some amount of data labeling or identification before training. There have been attempts to use this form of training for machine learning or deep learning in gaming and simulation, but they have proven to be unsuccessful. </p>
<p class="mce-root">This is why, for most of this book, we will look at other forms of training, starting with a form of unsupervised training called a <strong>generative adversarial network</strong> (<strong>GAN</strong>). GANs are able to train themselves using, in essence, a two-player game. This makes them an ideal next step in our learning and a perfect way to actually start generating content for games.</p>
<p>In this chapter, we explore GANs and their use in developing game content. Along the way, we will learn more fundamentals of deep learning techniques. In this chapter, we will cover the following content:</p>
<ul>
<li>Introducing GANs</li>
<li>Coding a GAN in Keras</li>
<li>Wasserstein GAN</li>
<li>GAN for creating textures</li>
<li>Generating music with a GAN</li>
<li>Exercises</li>
</ul>
<p>GANs are notoriously hard to train and build successfully. Therefore, it is recommended you take your time with this chapter and go through the exercises a couple of times if you need to. The techniques we learn to make effective GANs will provide you with a better overall understanding of training networks and the many other options available. We also still need to cover many fundamental concepts about training networks, so please work through this chapter thoroughly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing GANs</h1>
                </header>
            
            <article>
                
<p>The concept of GANs is typically introduced using the analogy of a two-player game. In this game, there is typically an art expert and an art forger. The goal of the art forger or counterfeiter is to make a convincing-enough fake to fool the art expert and thus win the game. An example of how this was first portrayed as a neural network is as follows:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/5736d34f-71e3-4723-acce-24916b356499.png" style="width:41.25em;height:15.00em;"/><br/>
<br/>
GAN by Ian and others</div>
<p>In the preceding diagram, the <span class="packt_screen">Generator</span> takes the place of the art forger, the one trying to best the art expert, shown as the <span class="packt_screen">Discriminator</span>. The <span class="packt_screen">Generator </span>uses random noise as a source to generate an image, with a goal that the image is convincing enough to fool the <span class="packt_screen">Discriminator</span>. The <span class="packt_screen">Discriminator</span> is trained on both real and fake images, and all it does is classify the image as real or fake. The <span class="packt_screen">Generator</span> is then trained to build a convincing-enough fake that will fool the <span class="packt_screen">Discriminator</span>. While this concept seems simple enough as a way of self-training a network, in the last few years, the implementation of this adversarial technique has proven exceptional in many areas.</p>
<p>GANs were first developed by Ian Goodfellow and others at the University of Montreal in 2014. In only a few short years, this technique has exploded into many wide and varied applications, from generating images and text to animating static images, all in a very short time. The following is a short summary of some of the more impressive GAN improvements/implementations currently turning heads in the deep learning community:</p>
<ul>
<li><strong>Deep convolutional GANs</strong> (<strong>DCGANs</strong>): These were the first major improvement to the standard architecture we just covered. We will explore this as our first form of GAN in the next section of this chapter.</li>
<li><strong>Adversarial Autoencoder GAN</strong>: This variation of an autoencoder uses the adversarial GAN technique to isolate attributes or properties of your data. It has interesting applications for determining latent relationships in data, such as being able to tell the difference in style versus content for a set of handwritten digits, for instance.</li>
<li><strong>Auxiliary Classifier GAN</strong>: This is another enhanced GAN that relates to conditioned or conditional GANs. It has been shown to synthesize higher-resolution images and is certainly worth exploring more in gaming.</li>
<li><strong>CycleGAN</strong>: This is a variation that is impressive in that it allows the translation of style from one image to another. There are plenty of examples of this form of GAN being used to style a picture as if Van Gogh painted it, to swapping celebrity faces. If this chapter piques your interest in GANs and you want to explore this form, check out this post: <a href="https://hardikbansal.github.io/CycleGANBlog/">https://hardikbansal.github.io/CycleGANBlog/</a>.</li>
<li><strong>Conditional GANS</strong>: These use a form of semi-supervised learning. This means that the training data is labeled but with meta data or attributes. So, instead of labeling a handwritten digit from the MNIST data set as a 9, you may instead label the writing style (cursive or print). Then, this new form of conditioned GAN can <span>learn </span>not only the digits, but also whether they are cursive or print. This form of GAN has shown some interesting applications and it is one we will explore further when we speak to specific applications in gaming.</li>
<li><strong>DiscoGAN</strong>: This is yet another form of GAN showing fun results, from swapping celebrity hairstyles to genders. This GAN extracts features or domains and allows you to transfer them to other images or data spaces. This GAN has numerous applications in gaming and is certainly worth exploring further for the interested reader.</li>
<li><strong>DualGAN</strong>: This uses dual GANs to train two generators against two discriminators in order to transfer images or data to other styles. This would be very useful as a way of restyling multiple assets and would work nicely for generating different forms of art content for games.</li>
<li><strong>Least squares GAN</strong> (<strong>LSGAN</strong>): This uses a different form of calculating loss and has been shown to be more effective than the DCGAN. </li>
<li><strong>pix2pixGAN</strong>: This is an extension to conditional GANs that allows it to transfer or generate multiple features from one image to another. This allows for images of the sketch of an object to return an actual 3D-rendered image of the same object or vice versa. While this is a very powerful GAN, it still is very much research-driven and may not be ready for use in games. Perhaps you will just have to wait six months or a year.</li>
<li><strong>InfoGANs</strong>: These types of GANs are, as of yet, used extensively to explore features or information about the training data. They can be used to identify the rotation of a digit in the MNIST dataset, for instance. Also, they are often used as a way of identifying attributes for conditioned GAN training.</li>
<li><strong>Stacked or SGAN</strong>: This is a form of GAN that breaks itself into layers where each layer is a generator and discriminator battling it out. This makes the overall GAN easier to train but also requires you to understand each stage or layer in some detail. If you are just starting, this is not the GAN for you, but as you build more complex networks, revisit this model again.</li>
<li><strong>Wasserstein GANs</strong>: This is a state-of-the-art GAN, and it will also get attention in its own section in this chapter. The calculation of loss is the improvement in this form of GAN.</li>
<li><strong>WassGANs</strong>: This uses the Wasserstein distance to determine loss, which dramatically helps with model convergence.</li>
</ul>
<p>We will explore further instances of specific GAN implementations as we work through this chapter. Here, we will look at how to generate game textures and music with a GAN. For now, though, let's move on to the next section and learn how to code a GAN in Keras.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding a GAN in Keras</h1>
                </header>
            
            <article>
                
<p>Of course, the best way to learn is by doing, so let's jump in and start coding our first GAN. In this example, we will be building the basic DCGAN and then modifying it later for our purposes. Open up <kbd>Chapter_3_2.py</kbd> and follow these steps:</p>
<div class="packt_infobox">This code was originally pulled from <a href="https://github.com/eriklindernoren/Keras-GAN">https://github.com/eriklindernoren/Keras-GAN</a>, which is the best representation of GANs in Keras anywhere, and is all thanks to Erik <span>Linder-Norén. Great job, and thanks for the hard work, Erik.<br/>
<br/>
An alternate listing a vanilla GAN has been added as <kbd>Chapter_3_1.py</kbd> for your learning pleasure.</span></div>
<ol>
<li class="mce-root"><span>We start by importing libraries:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">from __future__ import print_function, division<br/>from keras.datasets import mnist<br/>from keras.layers import Input, Dense, <strong>Reshape</strong>, Flatten, Dropout<br/>from keras.layers import <strong>BatchNormalization</strong>, Activation, <strong>ZeroPadding2D</strong><br/>from keras.layers.advanced_activations import <strong>LeakyReLU</strong><br/>from keras.layers.convolutional import UpSampling2D, Conv2D<br/>from keras.models import Sequential, <strong>Model</strong><br/>from keras.optimizers import <strong>Adam</strong><br/>import matplotlib.pyplot as plt<br/>import sys<br/>import numpy as np</pre>
<ol start="2">
<li>There are a few highlighted new types introduced in the preceding code: <kbd>Reshape</kbd>, <kbd>BatchNormalization</kbd>, <kbd>ZeroPadding2D</kbd>, <kbd>LeakyReLU</kbd>, <kbd>Model</kbd>, and <kbd>Adam</kbd>. We will explore each of these types in more detail next.</li>
<li class="mce-root"><span>Most of our previous examples worked with basic scripts. We are now at a point where we want types (classes) of our own built for further use later. That means we now start by defining our class like so:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">class DCGAN():</pre>
<ol start="4">
<li>So, we create a new class (type) called <kbd>DCGAN</kbd> for our implementation of a deep convolutional GAN. </li>
<li>Next, we would normally define our <kbd>init</kbd> function <span>by Python convention</span>. However, for our purposes, let's first look at the <kbd>generator</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">def build_generator(self):<br/>  model = Sequential()<br/>  model.add(Dense(128 * 7 * 7, activation="relu", input_dim=self.latent_dim))<br/>  model.add(<strong>Reshape</strong>((7, 7, 128)))<br/>  model.add(UpSampling2D())<br/>  model.add(Conv2D(128, kernel_size=3, padding="same"))<br/>  model.add(<strong>BatchNormalization</strong>(momentum=0.8))<br/>  model.add(Activation("relu"))<br/>  model.add(UpSampling2D())<br/>  model.add(Conv2D(64, kernel_size=3, padding="same")) <br/>  model.add(<strong>BatchNormalization</strong>(momentum=0.8))<br/>  model.add(Activation("relu"))<br/>  model.add(Conv2D(self.channels, kernel_size=3, padding="same"))<br/>  model.add(Activation("<strong>tanh</strong>"))<br/>  model.summary()<br/><br/>  noise = Input(shape=(self.latent_dim,))<br/>  img = model(noise)<br/>  return <strong>Model</strong>(noise, img)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="6">
<li>The <kbd>build_generator</kbd> function builds the art-forger model, which means it takes that sample set of noise and tries to convert it into an image the discriminator will believe is real. In this form, it uses the principle of convolution to make it more efficient, except, in this case, it generates a feature map of noise that it then turns into a real image. Essentially, the generator is doing the opposite of recognizing an image, but instead trying to generate an image based on feature maps. <br/>
In the preceding block of code, note how the input starts with <kbd>128, 7x7</kbd> feature maps of noise then uses a <kbd>Reshape</kbd> layer to turn it into the proper image layout we want to create. It then up-samples (the reverse of pooling or down-sampling) the feature map into 2x size (14 x 14), training another layer of convolution followed by more up-sampling (2x to 28 x 28) until the correct image size (28x28 for the MNIST) is generated. We also see the use of a new layer type called <kbd>BatchNormalization</kbd>, which we will cover in more detail shortly.</li>
<li class="mce-root"><span>Next, we will build the</span> <kbd>build_discriminator</kbd> <span>function like so:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">def build_discriminator(self):<br/>  model = Sequential()<br/>  model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding="same"))<br/>  model.add(<strong>LeakyReLU</strong>(alpha=0.2))<br/>  model.add(Dropout(0.25))<br/>  model.add(Conv2D(64, kernel_size=3, strides=2, padding="same"))<br/>  model.add(<strong>ZeroPadding2D</strong>(padding=((0,1),(0,1))))<br/>  model.add(B<strong>atchNormalization</strong>(momentum=0.8))<br/>  model.add(<strong>LeakyReLU</strong>(alpha=0.2))<br/>  model.add(Dropout(0.25))<br/>  model.add(Conv2D(128, kernel_size=3, strides=2, padding="same"))<br/>  model.add(<strong>BatchNormalization</strong>(momentum=0.8))<br/>  model.add(<strong>LeakyReLU</strong>(alpha=0.2))<br/>  model.add(Dropout(0.25))<br/>  model.add(Conv2D(256, kernel_size=3, strides=1, padding="same"))<br/>  model.add(<strong>BatchNormalization</strong>(momentum=0.8))<br/>  model.add(<strong>LeakyReLU</strong>(alpha=0.2))<br/>  model.add(Dropout(0.25))<br/>  model.add(Flatten())<br/>  model.add(Dense(1, activation='sigmoid'))<br/>  model.summary()<br/><br/>  img = Input(shape=self.img_shape)<br/>  validity = model(img)<br/>  return <strong>Model</strong>(img, validity)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="8">
<li>This time, the discriminator is testing the image inputs and determining whether they are fake. It uses convolution to identify features, but in this example it uses <kbd>ZeroPadding2D</kbd> to place a buffer of zeros around the images in order to help identification. The opposite form of this layer would be <kbd>Cropping2D</kbd>, which crops an image. <span>Note how this model does not use down-sampling or pooling with the convolution. </span>We will explore the other new special layers <kbd>LeakyReLU</kbd> and <kbd>BatchNormalization</kbd> in the coming sections. Note how we have not used any pooling layers in our convolution. This is done to increase the spatial dimensionality through the fractionally strided convolutions. See how inside the convolution layers we are using an odd kernel and stride size. </li>
<li class="mce-root"><span>We will now circle back and define the</span> <kbd>init</kbd> <span>function like so:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">def __init__(self):<br/>  self.img_rows = 28<br/>  self.img_cols = 28<br/>  self.channels = 1<br/>  self.img_shape = (self.img_rows, self.img_cols, self.channels)<br/>  self.latent_dim = 100<br/>  optimizer = <strong>Adam</strong>(0.0002, 0.5)<br/><br/>  self.discriminator = self.build_discriminator()<br/>  self.discriminator.compile(loss='binary_crossentropy',    <br/>  optimizer=optimizer, metrics=['accuracy'])<br/><br/>  self.generator = self.build_generator() <br/>  z = Input(shape=(self.latent_dim,))<br/>  img = self.generator(z)<br/>  <br/>  self.discriminator.trainable = False<br/>  valid = self.discriminator(img)<br/>  <br/>  self.combined = Model(z, valid)<br/>  self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)</pre>
<ol start="10">
<li>This initialization code sets up the sizes for our input images (28 x 28 x 1, one channel for grayscale). It then sets up an <kbd>Adam</kbd> optimizer, something else we will review in another section on optimizers. After this, it builds the <kbd>discriminator</kbd> and then the <kbd>generator</kbd>. Then it combines the two models or sub networks (<kbd>generator</kbd> and <kbd>discriminator</kbd>) together. This allows the networks to work in tandem and optimize training across an entire network. Again, this is a concept we will look at more closely under optimizers.</li>
<li>Before we get too deep, take some time to run this example. This sample can take an extensive amount of time to run, so return to the book after it starts and keep it running. </li>
</ol>
<ol start="12">
<li class="CDPAlignLeft CDPAlign">As the sample runs, you will be able to see the generated output get placed into a folder called <kbd>images</kbd> within the same folder as your running Python file. Go ahead and watch as every 50 epochs a new image is saved, which is shown in the following diagram:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dfa6688e-16d8-4d0f-b177-6f2ea058dd59.png" style="width:20.58em;height:15.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Example of output generated from a GAN</div>
<p>The preceding shows the results after 3,900 epochs or so. When you start training, it will take a while to get results this good.</p>
<p>That covers the basics of setting up the models, except all the work that is in the training, which we will cover in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a GAN</h1>
                </header>
            
            <article>
                
<p>Training a GAN requires a fair bit more attention to detail and an understanding of more advanced optimization techniques. We will walk through each section of this function in detail in order to understand the intricacies of training. <span>Let's open up <kbd>Chapter_3_1.py</kbd> and look at the </span><kbd>train</kbd><span> function and follow these steps:</span></p>
<ol>
<li class="mce-root"><span>At the start of the</span> <kbd>train</kbd> <span>function, you will see the following code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">def train(self, epochs, batch_size=128, save_interval=50):  <br/>  (X_train, _), (_, _) = mnist.load_data()<br/>  X_train = X_train / 127.5 - 1.<br/>  X_train = np.expand_dims(X_train, axis=3)<br/><br/>  valid = np.ones((batch_size, 1))<br/>  fake = np.zeros((batch_size, 1))</pre>
<ol start="2">
<li>The data is first loaded from the MNIST training set and then rescaled to the range of <kbd>-1</kbd> to <kbd>1</kbd>. We do this in order to better center that data around 0 and to accommodate our activation function, <kbd>tanh</kbd>. If you go back to the generator function, you will see that the bottom activation is <kbd>tanh</kbd>.  </li>
<li class="mce-root"><span>Next, we build a</span> <kbd>for</kbd> <span>loop to iterate through the epochs like so:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">for epoch in range(epochs):</pre>
<ol start="4">
<li>Then we randomly select half of the <em>real</em> training images, using this code:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">idx = np.random.randint(0, X_train.shape[0], batch_size)<br/>imgs = X_train[idx]</pre>
<ol start="5">
<li>After that, we sample <kbd>noise</kbd> and generate a set of forged images with the following code:</li>
</ol>
<pre style="padding-left: 60px">noise = np.random.normal(0, 1, (batch_size, self.latent_dim))<br/>gen_imgs = self.generator.predict(noise)</pre>
<ol start="6">
<li>Now, half of the images are real and the other half are faked by our <kbd>generator</kbd>.  </li>
<li>Next, the <kbd>discriminator</kbd> is trained against the images generating a loss for incorrectly predicted fakes and correctly identified real images as shown:</li>
</ol>
<pre style="padding-left: 60px">d_loss_real = self.discriminator.train_on_batch(imgs, valid)<br/>d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)<br/>d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)</pre>
<ol start="8">
<li>Remember, this block of code is running across a set or batch. This is why we use the <kbd>numpy np.add</kbd> function to add the <kbd>d_loss_real</kbd>, and <kbd>d_loss_fake</kbd>. <kbd>numpy</kbd> is a library we will often use to work on sets or tensors of data.</li>
<li class="mce-root"><span>Finally, we train the generator using the following code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">g_loss = self.combined.train_on_batch(noise, valid)<br/><br/>print ("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss))<br/><br/>if epoch % save_interval == 0:<br/>  self.save_imgs(epoch)</pre>
<ol start="10">
<li>Note how the <kbd>g_loss</kbd> is calculated based on training the combined model. As you may recall, the combined model takes the input from real and fake images and backpropagates the training back through the entire model. This allows us to train both the <kbd>generator</kbd> and <kbd>discriminator</kbd> together as a combined model. An example of how this looks is shown next, but just note that the image sizes are a little different than ours:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/c85f204f-7196-444a-be1c-53b285b79cd8.png"/><br/>
<br/>
Layer architecture diagram of DCGAN</div>
<p>Now that we have a better understanding of the architecture, we need to go back and understand some details about the new layer types and the optimization of the combined model. We will look at how we can optimize a joined model such as our GAN in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizers</h1>
                </header>
            
            <article>
                
<p>An <strong>optimizer</strong> is really nothing more than another way to train the backpropagation of error through a network. As we learned back in <a href="108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml" target="_blank">Chapter 1</a><em>, Deep Learning for Games</em>, the base algorithm we use for backpropagation is the gradient descent and the more advanced <strong>stochastic gradient descent</strong> (<strong>SGD</strong>). </p>
<p>SGD works by altering the evaluation of the gradient by randomly picking the batch order during each training iteration. While SGD works well for most cases, it does not perform well in a GAN, due to a problem known as the <strong>vanishing </strong>/ <strong>exploding gradient</strong>, which happens when trying to train multiple, but combined, networks. Remember, we are directly feeding the results of our generator into the discriminator. Instead, we look to more advanced optimizers. A graph showing the performance of the typical best optimizers is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/ce2b1eaf-c1e9-4018-ac13-b8208344a68e.png" style="width:38.83em;height:34.58em;"/><br/>
<br/>
Performance comparison of various optimizers</div>
<p class="mce-root"/>
<p>All of the methods in the graph have their origin in SGD, but you can clearly see the winner in this instance is <strong>Adam</strong>. There are cases where this is not the case, but the current favorite optimizer is Adam. It is something we have used extensively before, as you may have noticed, and you will likely continue using it in the future. However, let's take a look at each of the optimizers in a little more detail, as follows:</p>
<ul>
<li><strong>SGD</strong>: This is one of the first models we looked at and it will often be our baseline to train against.</li>
<li class="CDPAlignLeft CDPAlign"><strong>SGD with Nesterov</strong>: The problem SGD often faces is that wobble effect we saw in network loss, in one of the earlier training examples. Remember, during training, our network loss would fluctuate between two values, almost as if it was a ball going up and down a hill. In essence, that is exactly what is happening, but we can correct that by introducing a term we call <strong>momentum</strong>. An example of the effect momentum has on training is shown <span>in the following diagram</span>:</li>
</ul>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/9ab302ca-8dd7-4e12-b02e-da2befc259a0.png" style="width:34.67em;height:10.33em;"/><br/>
<br/>
SGD with and without momentum</div>
<p style="padding-left: 90px">So, now, instead of just letting the ball blindly roll around, we control its speed. We give it a push to get over some of those annoying bumps or wobbles, and more efficiently get to the lowest point. </p>
<p style="padding-left: 90px">As you may recall from studying the math of backpropagation, we control the gradient in SGD to train the network to minimize error or loss. By introducing momentum, we try to control the gradient to be more efficient by approximating what the values should be. The <strong>Nesterov <span>technique</span></strong>, or it may just be referred to as <strong>Momentum</strong>, uses an accelerated momentum term to further optimize the gradient.</p>
<ul>
<li><strong>AdaGrad</strong>: This method optimizes the individual training parameters based on the frequency of the updates, which makes it ideal for working with smaller datasets. The other main benefit is that it allows you to not have to tune the learning rate. However, a big weakness with this method is squared gradients causing the learning rate to become so small that the network stops learning.</li>
<li><strong>AdaDelta</strong>: This method is an extension to AdaGrad, which deals with the squared gradients and vanishing learning rate. It does this by fixing the learning rate window to a particular minimum.</li>
<li><strong>RMSProp</strong>: Developed by Geoff Hinton, the grandfather of deep learning, this is a technique to manage the vanishing learning rate problem in AdaGrad. As you can see in the graph, it is on par with AdaDelta for the sample shown.  </li>
<li><strong>Adaptive Moment Estimation</strong> (<strong>Adam</strong>): This is another technique that attempts to control that gradient using a more controlled version of Momentum. It is often described as Momentum plus RMSProp, since it combines the best of both techniques.</li>
<li><strong>AdaMax</strong>: This method is not shown on the performance graph but is worth mentioning. It is an extension to Adam that generalizes each iteration of an update applied to the momentum. </li>
<li><strong>Nadam</strong>: This is another method not on the graph; it is a combination of Nesterov-accelerated Momentum and Adam. The vanilla Adam just uses a Momentum term that is not accelerated.</li>
<li><strong>AMSGrad</strong>: This is a variation of Adam that works best when Adam is shown to be unable to converge or wobble. This is caused by the algorithm failing to adapt learning rates and is fixed by taking a maximum rather than an average of previously squared gradients. The difference is subtle and tends to prefer smaller datasets. Keep this option in the back of your mind as a possible future tool.</li>
</ul>
<p>That completes our short overview of optimizers; be sure to refer to the exercises at the end of the chapter for ways you can explore them further. In the next section, we build our own GAN that can generate textures we can use in games.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Wasserstein GAN</h1>
                </header>
            
            <article>
                
<p>As you can most certainly appreciate by now, GANs have wide and varied applications, several of which apply very well to games. One such application is the generation of textures or texture variations. We often want slight variations in textures to give our game worlds a more convincing look. This is and can be done with <strong>shaders</strong>, but for performance reasons, it is often best to create <strong>static assets</strong>. </p>
<p>Therefore, in this section, we will build a GAN project that allows us to generate textures or height maps. You could also extend this concept using any of the other cool GANs we briefly touched on earlier. We will be using a default implementation of the <span>Wasserstein</span> GAN by Erik <span>Linder-Norén </span>and converting it for our purposes. </p>
<p class="mce-root"/>
<p>One of the major hurdles you will face when first approaching deep learning problems is shaping data to the form you need. In the original sample, Erik used the MNIST dataset, but we will convert the sample to use the CIFAR100 dataset. The CIFAR100 dataset is a set of color images classified by type, as follows:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/87786072-1218-45e0-868c-b71599064740.png"/><br/>
<br/>
CIFAR 100 dataset</div>
<p>For now, though, let's open up <kbd>Chapter_3_wgan.py</kbd> and follow these steps:</p>
<ol>
<li>Open the Python file and review the code. Most of the code will look the same as the DCGAN we already looked at. However, there are a few key differences we want to look at, as follows:</li>
</ol>
<pre style="padding-left: 60px">def train(self, epochs, batch_size=128, sample_interval=50):<br/>  (X_train, _), (_, _) = mnist.load_data()<br/><br/>  X_train = (X_train.astype(np.float32) - 127.5) / 127.5<br/>  X_train = np.expand_dims(X_train, axis=3)<br/><br/>  valid = -np.ones((batch_size, 1))<br/>  fake = np.ones((batch_size, 1))<br/><br/>  for epoch in range(epochs):<br/>    <strong>for _ in range(self.n_critic):</strong><br/>      idx = np.random.randint(0, X_train.shape[0], batch_size)<br/>      imgs = X_train[idx]<br/>      noise = np.random.normal(0, 1, (batch_size, self.latent_dim))<br/>      gen_imgs = self.generator.predict(noise)<br/><br/>      d_loss_real = self.critic.train_on_batch(imgs, valid)<br/>      d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)<br/>      d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)<br/>      <br/>      for l in self.critic.layers:<br/>        weights = l.get_weights()<br/>        weights = [np.clip(w, -self.clip_value, self.clip_value) for <br/>        w in weights]<br/>        l.set_weights(weights)<br/><br/>    g_loss = self.combined.train_on_batch(noise, valid)<br/>    print ("%d [D loss: %f] [G loss: %f]" % (epoch, 1 - d_loss[0], 1 <br/>    - g_loss[0]))\<br/><br/>    if epoch % sample_interval == 0:<br/>      self.sample_images(epoch)</pre>
<ol start="2">
<li>The Wasserstein GAN uses a distance function in order to determine the cost or loss for each training iteration. Along with this, this form of GAN uses multiple critics rather than a single discriminator to determine cost or loss. Training multiple critics together improves performance and handles the vanishing gradient problem we often see plaguing GANs. An example of a different form of GAN training is as follows:</li>
</ol>
<div class="CDPAlignCenter CDPAlign" style="text-align: center;color: black"><img src="assets/b1e15842-e119-44ed-9f30-a52a0aab0e32.png" style="width:31.00em;height:21.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref" style="text-align: center;color: black">Training performance across GAN implementations (<a href="https://arxiv.org/pdf/1701.07875.pdf">https://arxiv.org/pdf/1701.07875.pdf</a>)</div>
<ol start="3">
<li>A WGAN overcomes the gradient problem by managing cost through a distance function that determines the cost of moving, rather than a difference in error values. A linear cost function could be as simple as the number of moves a character needs to take in order to spell a word correctly. For example, the word <em>SOPT</em> would have a cost of 2, since the <em>T</em> character needs to move two places to spell <em>STOP</em> correctly. The word <em>OTPS</em> has a distance cost of <em>3 (S) + 1 (T) = 4</em> to spell <em>STOP</em> correctly. </li>
<li>The Wasserstein distance function essentially determines the cost of transforming one probability distribution to another. As you can imagine, the math to understand this can be quite complex, so we will defer that to the more interested reader.</li>
<li>Run the example. This sample can take a significant time to run, so be patient. Also, this sample has been shown to have trouble training on some GPU hardware. If you find this to be the case, just disable the use of GPU.</li>
<li>As the sample runs, open the <kbd>images</kbd> folder from the same folder as the Python file and watch the training images generate. </li>
</ol>
<p>Run the sample for as long as you feel the need to in order to understand how it works. This sample can take several hours even on advanced hardware. When you are done, move on to the next section, and we will see how to modify this sample for generating textures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating textures with a GAN </h1>
                </header>
            
            <article>
                
<p>One of the things so rarely covered in advanced deep learning books is the specifics of shaping data to input into a network. Along with shaping data is the need to alter the internals of a network to accommodate the new data. The final version of this example is <kbd>Chapter_3_3.py</kbd>, but for this exercise, start with the <kbd>Chapter_3_wgan.py</kbd> file and follow these steps:</p>
<ol>
<li class="mce-root"><span>We will start by changing the training set of data from MNIST to CIFAR by swapping out the imports like so:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">from keras.datasets import mnist  #remove or leave<br/>from keras.datasets import cifar100  #add</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>At the start of the class, we will change the image size parameters from 28 x 28 grayscale to 32 x 32 color like so:</li>
</ol>
<pre style="color: black;padding-left: 60px">class WGAN():<br/>  def __init__(self):<br/>    self.img_rows = <strong>32</strong><br/>    self.img_cols = <strong>32</strong><br/>    self.channels = <strong>3</strong></pre>
<ol start="3">
<li>Now, move down to the <kbd>train</kbd> function and alter the code as follows:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>#(X_train, _), (_, _) = mnist.load_data() or delete me</strong><br/>(X_train, y), (_, _) = cifar100.load_data(label_mode='fine')<br/>Z_train = []<br/>cnt = 0<br/>for i in range(0,len(y)):<br/>  if y[i] == 33:  #forest images<br/>  cnt = cnt + 1 <br/>  z = X_train[i]<br/>  Z_train.append(z)<br/>#X_train = (X_train.astype(np.float32) - 127.5) / 127.5 or delete me<br/>#X_train = np.expand_dims(X_train, axis=3)<br/><strong>Z_train = np.reshape(Z_train, [500, 32, 32, 3])</strong><br/>Z_train = (Z_train.astype(np.float32) - 127.5) / 127.5<br/><br/>#X_train = (X_train.astype(np.float32) - 127.5) / 127.5<br/>#X_train = np.expand_dims(X_train, axis=3)</pre>
<ol start="4">
<li>This code loads the images from the CIFAR100 dataset and sorts through them by label. Labels are stored in the <kbd>y</kbd> variable, and the code loops through all the downloaded images and isolates those to one specific set. In this case, we are using the label <kbd>33</kbd>, which corresponds to forest images. There are 100 categories in the CIFAR100, and we are selecting one category that holds 500 images. Feel free to try to generate other textures from other categories.<br/>
The rest of the code is fairly straightforward, except for the <kbd>np.reshape</kbd> call where we reshape the data into a list of 500 images <kbd>32x32</kbd> pixels by three channels. You may also want to note that we do not need to expand the axis to three as we did before. This is because our image is already scaled to three channels.</li>
</ol>
<ol start="5">
<li class="mce-root">We now need to go back to the generator and critic models and alter that code slightly. First, we will change the generator like so:</li>
</ol>
<pre style="color: black;padding-left: 60px">def build_generator(self):<br/>  model = Sequential()<br/>  model.add(Dense(128 * <strong>8 * 8</strong>, activation="relu", input_dim=self.latent_dim))<br/>  model.add(Reshape((<strong>8, 8</strong>, 128)))<br/>  model.add(UpSampling2D())<br/>  model.add(Conv2D(128, kernel_size=4, padding="same"))<br/>  model.add(BatchNormalization(momentum=0.8))<br/>  model.add(Activation("relu"))<br/>  model.add(UpSampling2D())<br/>  model.add(Conv2D(64, kernel_size=4, padding="same"))<br/>  model.add(BatchNormalization(momentum=0.8))<br/>  model.add(Activation("relu"))<br/>  model.add(Conv2D(self.channels, kernel_size=4, padding="same"))<br/>  model.add(Activation("tanh"))<br/>  model.summary()<br/>  noise = Input(shape=(self.latent_dim,))<br/>  img = model(noise)<br/>  return Model(noise, img)</pre>
<ol start="6">
<li>The boldface code denotes the changes. All we are doing for this model is converting the <kbd>7x7</kbd> original feature map to <kbd>8x8</kbd>. Recall that the original full image size is <kbd>28x28</kbd>. Our convolution starts with a <kbd>7x7</kbd> feature map, doubled twice, which equals <kbd>28x28</kbd>. Since our new image size is <kbd>32x32</kbd>, we need to convert our network to start with <kbd>8x8</kbd> feature maps, which doubled twice equals <kbd>32x32</kbd>, the same size as the CIFAR100 images. Fortunately, we can leave the critic model as it is.</li>
<li>Next, we add a new function to save samples of the original CIFAR images, and this is shown here:</li>
</ol>
<pre style="padding-left: 60px">def save_images(self, imgs, epoch):<br/>  r, c = 5, 5 <br/>  gen_imgs = 0.5 * imgs + 1<br/>  fig, axs = plt.subplots(r, c)<br/>  cnt = 0<br/>  for i in range(r):<br/>    for j in range(c):<br/>      axs[i,j].imshow(gen_imgs[cnt, :,:,0],cmap='gray')<br/>      axs[i,j].axis('off')<br/>      cnt += 1<br/><br/>  fig.savefig("images/cifar_%d.png" % epoch)<br/>  plt.close()</pre>
<ol start="8">
<li>The <kbd>save_images</kbd> function outputs a sampling of the original images and is called by the following code in the <kbd>train</kbd> function: </li>
</ol>
<pre style="padding-left: 60px">idx = np.random.randint(0, Z_train.shape[0], batch_size)<br/>imgs = Z_train[idx] <br/><strong>if epoch % sample_interval == 0:</strong><br/><strong>  self.save_images(imgs, epoch)</strong></pre>
<ol start="9">
<li class="CDPAlignLeft CDPAlign">The new code is in boldface and just outputs what a sampling of the originals looks like, as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ecd433e1-66ac-49a0-817c-0eb87d7531ae.png" style="width:37.25em;height:28.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Example of the original images </span></div>
<ol start="10">
<li>Run the sample and observe the output in the <kbd>images</kbd> folder again labeled <kbd>cifar</kbd>, showing the result of training. Again, this sample can take some time to run, so read on to the next section.</li>
</ol>
<p>As the sample runs, you can observe how the GAN is training to match the images. The benefit here is that you can generate various textures easily using a variety of techniques. You can use these as textures or height maps in Unity or another game engine. Before we finish up this section, let's jump into some normalization and other parameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Batch normalization</h1>
                </header>
            
            <article>
                
<p><strong>Batch normalization</strong>, as its name suggests, normalizes the distribution of weights in a layer around some mean of 0. This allows for the network to use a higher learning while still avoiding a vanishing or exploding gradient problem. It is due to the weights being normalized, which allows for fewer shifts or training wobble, as we have seen before. </p>
<p>By normalizing the weights in a layer, we allow for the network to use a higher learning rate and thus train faster. Also, we can avoid or reduce the need to use <kbd>DropOut</kbd>. You will see that we use the standard term, shown here, to normalize the layers:</p>
<pre class="mce-root">model.add(BatchNormalization(momentum=0.8))</pre>
<p>Recall from our discussions of optimizers that momentum controls how quickly or slowly we want to decrease the training gradient. In this case, momentum refers to the amount of change of the mean or center of the normalized distribution.  </p>
<p>In the next section, we look at another special layer called LeakyReLU.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Leaky and other ReLUs</h1>
                </header>
            
            <article>
                
<p><strong>LeakyReLU</strong> adds an activation layer that allows for negative values to have a small slope, rather than just 0, as in the case of the standard ReLU activation function. The standard ReLU encourages sparsity in the network by only allowing neurons with positive activation to fire. However, this also creates a dead neuron state, where parts of the network essentially die off or become untrainable. To overcome this issue, we introduce a leaky form of ReLU activation called LeakyReLU. An example of how this activation works is shown here:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/0377b7b9-e422-4d78-9c9e-c8dc7c37d9b0.png" style="width:28.75em;height:15.67em;"/><br/>
<br/>
Example of a leaky and parametric ReLU</div>
<p>Pictured in the preceding diagram is <strong>Parametric ReLU</strong>, which is similar to Leaky, but it allows the network to train the parameter itself. This allows the network to adjust on its own, but it will take longer to train.</p>
<p>The other ReLU variants you can use are summarized here:</p>
<ul>
<li><strong>Exponential Linear</strong> (<strong>ELU, SELU</strong>): These forms of ReLU activate as shown in the diagram as follows:</li>
</ul>
<div class="packt_figref CDPAlignCenter CDPAlign" style="text-align: center;color: black"><img src="assets/cea2fa2e-b196-457a-9603-d4cf88bcff72.png" style="width:35.00em;height:18.08em;"/></div>
<div class="packt_figref" style="text-align: center;color: black">ELU and SELU</div>
<ul>
<li><strong>Concatenated ReLU</strong> (<strong>CReLU</strong>): This joins the regular and leaky form together to provide a new function that produces two output values. For positive values, it generates <em>[0,x],</em> while for negative values, it returns <em>[x,0]</em>. One thing to note about this layer is the doubling of output, since two values are generated per neuron.</li>
<li><strong><span>ReLU-6</span></strong>: The value of 6 is arbitrary but allows for the network to train sparse neurons. Sparsity is of value because it encourages the network to learn or build stronger weights or bonds. The human brain has been shown to function in a sparse state, with only a few activated neurons at a time. You will often hear the myth that we only use 10% of our brain at a time at most. This may very well be true, but the reasons for this are more mathematical than us being able to use our entire brain. We do use our entire brain, just not all of it at the same time. Stronger individual weights, encouraged by sparsity, allow for the network to make better/stronger decisions. Fewer weights also encourage less overfitting or memorization of data. This can often happen in deep networks with thousands of neurons.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Regularization is another technique we will often use to trim or reduce unneeded or weights and create sparse networks. We will have a few opportunities to look at regularization and sparsity later in the coming chapters.</p>
<p>In the next section, we use what we have learned to build a working music GAN that can generate game music.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A GAN for creating music</h1>
                </header>
            
            <article>
                
<p>In our final grand example of this chapter, we are going to look at generating music with GANs for games. Music generation is not especially difficult, but it does allow us to see a whole variation of a GAN that uses LSTM layers to identify sequences and patterns in music. Then it attempts to build that music back from random noise to a passable sequence of notes and melodies. This sample becomes ethereal when you listen to those generated notes and realize the tune originates from a computer brain. </p>
<p>The origins of this sample are pulled from GitHub, <a href="https://github.com/megis7/musegen">https://github.com/megis7/musegen</a>, and developed by <span>Michalis Megisoglou</span><span>. The reason we look at these code examples is so that we can see the best of what others have produced and learn from those. In some cases, these samples are close to the original, and others not so much. We did have to tweak a few things. Michalis also produced a nice GitHub README on the code he built for his implementation of <strong>museGAN</strong>, music generation with GAN. If you are interested in building on this example further, be sure to check out the GitHub site as well. There are a few implementations of museGAN available using various libraries; one of them is TensorFlow.</span></p>
<div class="packt_tip">We use Keras in this example in order to make this example easier to understand. If you are serious about using TensorFlow, then be sure to take a look at the TensorFlow version of museGAN as well.</div>
<p>This example trains the discriminator and generator separately, which means it needs to have the discriminator trained first. For our first run, we will run this example with the author's previously generated models, but we still need some setup; let's follow these steps:</p>
<ol>
<li class="mce-root"><span>We first need to install a couple of dependencies. Open an Anaconda or Python window as an admin and run the following commands:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>pip install music21</strong><br/><strong>pip install h5py</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li><kbd>Music21</kbd> is a Python library for loading MIDI files. <strong>MIDI</strong> is a music interchange format used to describe, as you might have guessed, music/notes. The original models were trained on a collection of MIDI files that describe <span>300 chorales of Bach's</span> music. You can locate the project by navigating to the <kbd>musegen</kbd> folder and running the script.</li>
<li class="mce-root"><span>Navigate to the project folder and execute the script that runs the previously trained models like so:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>cd musegen</strong><br/><strong>python musegen.py </strong>or<strong> python3 musegen.py</strong></pre>
<ol start="4">
<li>This will load the previously saved models<strong> </strong>and use those models to train the generator and generate music. You could, of course, train this GAN on other MIDI files of your choosing later as needed. There are plenty of free sources for MIDI files from classical music, to TV theme music, games, and modern pop. We use the author's original models in this example, but the possibilities are endless.</li>
<li class="mce-root"><span>Loading the music files and training can take a really long time, as training typically does. So, take this opportunity to look at the code. Open up the</span> <kbd>musegen.py</kbd> <span>file located in the project folder. Take a look at around line 39, as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">print('loading networks...')<br/>dir_path = os.path.dirname(os.path.realpath(__file__))<br/>generator = loadModelAndWeights(os.path.join(dir_path, note_generator_dir, 'model.json'),<br/>                               os.path.join(dir_path, note_generator_dir, 'weights-{:02d}.hdf5'.format(generator_epoch)))</pre>
<ol start="6">
<li>This section of code loads the previously trained model from an <kbd>hdf5</kbd> or hierarchical data file. The preceding code sets up a number of variables that define the notes to a vocabulary we will use to generate new notes.</li>
<li class="mce-root"><span>Locate the</span> <kbd>notegenerator.py</kbd> <span>file located in the same project folder. Take a look at the creation of the model code, as follows:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">x_p = Input(shape=(sequence_length, pitch_dim,), name='pitches_input')<br/>h = LSTM(256, return_sequences=True, name='h_lstm_p_1')(x_p)<br/>h = LSTM(512, return_sequences=True, name='h_lstm_p_2')(h)<br/>h = LSTM(256, return_sequences=True, name='h_lstm_p_3')(h)<br/><br/># VAE for pitches<br/>z_mean_p = TimeDistributed(Dense(latent_dim_p, kernel_initializer='uniform'))(h)<br/>z_log_var_p = TimeDistributed(Dense(latent_dim_p, kernel_initializer='uniform'))(h)<br/><br/>z_p = <strong>Lambda</strong>(sampling)([z_mean_p, z_log_var_p])<br/>z_p = <strong>TimeDistributed</strong>(Dense(pitch_dim, kernel_initializer='uniform', activation='softmax'))(z_p)<br/><br/>x_d = Input(shape=(sequence_length, duration_dim, ), name='durations_input')<br/>h = LSTM(128, return_sequences=True)(x_d)<br/>h = LSTM(256, return_sequences=True)(h)<br/>h = LSTM(128, return_sequences=True)(h)<br/><br/># VAE for durations<br/>z_mean_d = TimeDistributed(Dense(latent_dim_d, kernel_initializer='uniform'))(h)<br/>z_log_var_d = TimeDistributed(Dense(latent_dim_d, kernel_initializer='uniform'))(h)<br/><br/>z_d = Lambda(sampling)([z_mean_d, z_log_var_d])<br/>z_d = TimeDistributed(Dense(duration_dim, kernel_initializer='uniform', activation='softmax'))(z_d)<br/>conc = Concatenate(axis=-1)([z_p, z_d])<br/>latent = TimeDistributed(Dense(pitch_dim + duration_dim, kernel_initializer='uniform'))(conc)<br/>latent = LSTM(256, return_sequences=False)(latent)<br/><br/>o_p = Dense(pitch_dim, activation='softmax', name='pitches_output', kernel_initializer='uniform')(latent)<br/>o_d = Dense(duration_dim, activation='softmax', name='durations_output', kernel_initializer='uniform')(latent)</pre>
<ol start="8">
<li>Note how we have changed from using <kbd>Conv2D</kbd> layers to <kbd>LSTM</kbd> layers, since we have gone from image recognition to sequence or note pattern recognition. We have also gone from using more straightforward layers to a complex time-distributed architecture. Also, the author used a concept known as <strong>variational auto encoding</strong> in order to determine the distribution of notes in a sequence. This network is the most complex we have looked at so far, and there is a lot going on here. Don't fret too much about this example, except to see how the code flows. We will take a closer look at more of these type of advanced time- distributed networks in <a href="a8e699ff-c668-4601-842d-4c6e06c47a61.xhtml" target="_blank"/><a href="a8e699ff-c668-4601-842d-4c6e06c47a61.xhtml" target="_blank">Chapter 4</a><em>, Building a Deep Learning Gaming Chatbot</em><a href="http://Chapter_4"><span>.</span></a></li>
<li>Let the sample run and generate some music samples into the <kbd>samples/note-generator</kbd> folder. As we get into more complex problems, our training time will go from hours to days for very complex problems or more. It is possible that you could easily generate a network that you would not have the computing power to train in a reasonable time. </li>
</ol>
<ol start="10">
<li>Open the folder and double-click on one of the sample files to listen to the generated MIDI file. Remember, this music was just generated by a computer brain.</li>
</ol>
<p>There is a lot of code that we did not cover in this example. So, be sure to go back and go through the <kbd>musegen.py</kbd> file to get a better understanding of the flow and types of layers used to build the network generator. In the next section, we explore how to train this GAN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the music GAN</h1>
                </header>
            
            <article>
                
<p>Before we get into training this network, we will look at the overall architecture as depicted in the author's original GitHub source:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/e630248f-63d9-43d6-9d64-616a450543f6.png"/><br/>
<br/>
Overview of museGAN network architecture</div>
<p>The networks are almost identical until you look closer and see the subtle differences in the LSTM layers. Note how one set uses double the units as the other model.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can generate music models by running the following command at the Python or Anaconda prompt:</p>
<pre><strong>python note-generator.py</strong> <br/>or <br/><strong>python3 note-generator.py</strong></pre>
<p>This script loads the sample data and generates the models we use in the <kbd>musegen.py</kbd> file later when we create original music. Open up the <kbd>note-generator.py</kbd> file with the main parts shown here:</p>
<div class="packt_tip">The code was modified from the original to make it more Windows-compatible and cross-platform. Again, this is certainly not a criticism of the author's excellent work.</div>
<pre>def loadChorales():<br/>    notes = []<br/>    iterator = getChoralesIterator()<br/><br/>    # load notes of chorales<br/>    for chorale in iterator[1:maxChorales]: # iterator is 1-based <br/>        transpose_to_C_A(chorale.parts[0])<br/>        notes = notes + parseToFlatArray(chorale.parts[0])<br/>        notes.append((['end'], 0.0)) # mark the end of the piece<br/>    <br/>    return notes</pre>
<p>This code uses the Music21 library to read the MIDI notes and other music forms from the corpus of music you can use for your own testing. This training dataset is an excellent way to generate other sources of music and is composed of the following: <a href="http://web.mit.edu/music21/doc/moduleReference/moduleCorpus.html">http://web.mit.edu/music21/doc/moduleReference/moduleCorpus.html</a>.<a href="http://web.mit.edu/music21/doc/moduleReference/moduleCorpus.html"/></p>
<p>You can further modify this example by modifying the contents or adding additional configuration options in the <kbd>config.py</kbd> file as shown:</p>
<pre># latent dimension of VAE (used in pitch-generator)<br/>latent_dim = 512<br/><br/># latent dimensions for pitches and durations (used in note-generator)<br/>latent_dim_p = 512<br/>latent_dim_d = 256<br/><br/># directory for saving the note embedding network model --- not used anymore<br/>note_embedding_dir = "models/note-embedding"<br/><br/># directory for saving the generator network model<br/>pitch_generator_dir = 'models/pitch-generator'<br/><br/># directory for saving the note generator network model<br/>note_generator_dir = 'models/note-generator'<br/><br/># directory for saving generated music samples<br/>output_dir = 'samples'</pre>
<p>The previous sample is great for exploring the generation of music. A more practical and potentially useful example will be introduced in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating music via an alternative GAN</h1>
                </header>
            
            <article>
                
<p>Another example of music generation is also included in the <kbd>Chapter_3</kbd> source folder, called <strong>Classical-Piano-Composer</strong>, with the source located at <a href="https://github.com/Skuldur/Classical-Piano-Composer">https://github.com/Skuldur/Classical-Piano-Composer</a>, developed by Sigurður Skúli. This example uses a full set of Final Fantasy MIDI files as source inspiration for the music generation and is a great practical example for generating your own music.</p>
<p>In order to run this sample, you need to run the <kbd>lstm.py</kbd> first using the following command from the <kbd>Classical-Piano-Composer</kbd> project folder:</p>
<pre><strong>python lstm.py</strong> <br/>or<br/><strong>python3 lstm.py</strong></pre>
<p>This sample can take a substantial time to train, so be sure to open the file and read through what it does.</p>
<p>After the models are trained, you can run the generator by running the following:</p>
<pre><strong>python predict.py</strong><br/>or<br/><strong>python3 predict.py</strong></pre>
<p>This script loads the trained model and generates the music. It does this by encoding the MIDI notes into network input in terms of sequences or sets of notes. What we are doing here is breaking up the music files into short sequences, or a music snapshot if you will. You can control the length of these sequences by adjusting the <kbd>sequences_length</kbd> property in the code file.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The great thing about this second example is the ability to download your own MIDI files and put them in the appropriate input folder for training. It is also interesting to see how both projects use a similar three-layer LSTM structure but vary quite widely in other forms of execution.</p>
<div class="packt_infobox">If you want to learn more about audio or music development for games and especially for Unity, check out the book <em>Game Audio Development with Unity 5.x</em>, by Micheal Lanham. This book can show you many more techniques for working with audio and music in games.</div>
<p>Both music samples can take some time to train and then generate music, but it is certainly worth the effort to run through both examples and understand how they work. GANs have innovated the way we think of training neural networks and what type of output they are able to produce. As such, they certainly have a place in generating content for games.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>Take some time to reinforce your learning by undertaking the following exercises:</p>
<ol>
<li>What type of GAN would you use to transfer styles on an image?</li>
<li>What type of GAN would you use to isolate or extract the style?</li>
<li>Modify the number of critics used in the Wasserstein GAN example and see the effect it has on training.</li>
<li>Modify the first GAN, the DCGAN, to improve training performance using any technique you learned in this chapter. How did you increase training performance?</li>
<li>Modify the BatchNormalization momentum parameter and see what effect it has on training.</li>
<li>Modify a few of the samples by changing the activation from LeakyReLU to another advanced form of activation.</li>
<li>Modify the Wasserstein GAN example to use your own textures. There is a sample data loader available in the downloaded code sample for the chapter.</li>
<li>Download one of the other reference GANs from <a href="https://github.com/eriklindernoren/Keras-GAN">https://github.com/eriklindernoren/Keras-GAN</a> and modify that to use your own dataset.</li>
<li>Alter the first music generation GAN to use a different corpus.  </li>
<li>Use your own MIDI files to train the second music generation GAN example.  </li>
<li>(BONUS) Which music GAN generated better music? Is it what you expected?</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p>You certainly don't have to work through all these exercises, but give a few a try. Putting this knowledge to practice right away can substantially improve your understanding of the material. Practice does make perfect, after all.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary </h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at generative adversarial networks, or GANs, as a way to build DNNs that can generate unique content based on copying or extracting features from other content. This also allowed us to explore unsupervised training, a method of training that requires no previous data classification or labeling. In the previous chapter, we used supervised training. We started with looking at the many variations of GANs currently making an impression in the DL community. Then we coded up a deep convolutional GAN in Keras, followed by the state-of-the-art Wasserstein GAN. From there, we looked at how to generate game textures or height maps using sample images. We finished the chapter off by looking at two music-generating GANs that can generate original MIDI music from sampled music.</p>
<p>For the final sample, we looked at music generation with GANs that relied heavily on RNNs (LSTM). We will continue our exploration of RNNs when we look at how to build DL chatbots for games.</p>


            </article>

            
        </section>
    </body></html>