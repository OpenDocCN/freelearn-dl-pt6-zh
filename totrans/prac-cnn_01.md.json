["```py\nimport numpy as np\nimport math\n\nclass Neuron(object):\n    def __init__(self):\n        self.weights = np.array([1.0, 2.0])\n        self.bias = 0.0\n    def forward(self, inputs):\n        \"\"\" Assuming that inputs and weights are 1-D numpy arrays and the bias is a number \"\"\"\n        a_cell_sum = np.sum(inputs * self.weights) + self.bias\n        result = 1.0 / (1.0 + math.exp(-a_cell_sum)) # This is the sigmoid activation function\n        return result\nneuron = Neuron()\noutput = neuron.forward(np.array([1,1]))\nprint(output)\n```", "```py\n#Creates a virtual environment named \"tensorflow_env\" assuming that python 3.7 version is already installed.\nconda create -n tensorflow_env python=3.7 \n#Activate points to the environment named \"tensorflow\"  \nsource activate tensorflow_env\nconda install pandas matplotlib jupyter notebook scipy scikit-learn\n#installs latest tensorflow version into environment tensorflow_env\npip3 install tensorflow \n```", "```py\nimport tensorflow as tf\n\n#Creating TensorFlow object \nhello_constant = tf.constant('Hello World!', name = 'hello_constant')\n#Creating a session object for execution of the computational graph\nwith tf.Session() as sess:\n    #Implementing the tf.constant operation in the session\n    output = sess.run(hello_constant)\n    print(output)\n```", "```py\n# A is an int32 tensor with rank = 0\nA = tf.constant(123) \n# B is an int32 tensor with dimension of 1 ( rank = 1 ) \nB = tf.constant([123,456,789]) \n# C is an int32 2- dimensional tensor \nC = tf.constant([ [123,456,789], [222,333,444] ])\n```", "```py\nwith tf.Session() as sess:\n    # Run the tf.constant operation in the session\n    output = sess.run(hello_constant)\n    print(output)\n```", "```py\ntensorboard --logdir=path/to/log-directory\n```", "```py\nconstant_x = tf.constant(5, name='constant_x')\nvariable_y = tf.Variable(x + 5, name='variable_y')\nprint (variable_y)\n```", "```py\n#initialize all variables\ninit = tf.global_variables_initializer()\n# All variables are now initialized\n\nwith tf.Session() as sess:\n    sess.run(init)\n    print(sess.run(variable_y))\n```", "```py\nAddition\nx = tf.add(1, 2, name=None) # 3\n```", "```py\nx = tf.subtract(1, 2,name=None) # -1\ny = tf.multiply(2, 5,name=None) # 10\n\n```", "```py\nx = tf.placeholder(tf.string)\n\nwith tf.Session() as sess:\n    output = sess.run(x, feed_dict={x: 'Hello World'})\n```", "```py\nx = tf.placeholder(tf.string)\ny = tf.placeholder(tf.int32, None)\nz = tf.placeholder(tf.float32, None)\n\nwith tf.Session() as sess:\n    output = sess.run(x, feed_dict={x: 'Welcome to CNN', y: 123, z: 123.45}) \n```", "```py\nimport tensorflow as tf\n\nx = tf.placeholder(\"float\", [None, 3])\ny = x * 2\n\nwith tf.Session() as session:\n    input_data = [[1, 2, 3],\n                 [4, 5, 6],]\n    result = session.run(y, feed_dict={x: input_data})\n    print(result)\n```", "```py\nn_features = 5\nn_labels = 2\nweights = tf.truncated_normal((n_features, n_labels))\nwith tf.Session() as sess:\n  print(sess.run(weights))\n```", "```py\nlogit_data = [2.0, 1.0, 0.1]\nlogits = tf.placeholder(tf.float32)\nsoftmax = tf.nn.softmax(logits)\n\nwith tf.Session() as sess:\n    output = sess.run(softmax, feed_dict={logits: logit_data})\n    print( output )\n```", "```py\nx = tf.constant([[1,1,1], [1,1,1]])\nwith tf.Session() as sess:\n    print(sess.run(tf.reduce_sum([1,2,3]))) #returns 6 \n    print(sess.run(tf.reduce_sum(x,0))) #sum along x axis, prints [2,2,2]\n```", "```py\nimport tensorflow as tf\n\nsoftmax_data = [0.1,0.5,0.4]\nonehot_data = [0.0,1.0,0.0]\n\nsoftmax = tf.placeholder(tf.float32)\nonehot_encoding = tf.placeholder(tf.float32)\n\ncross_entropy = - tf.reduce_sum(tf.multiply(onehot_encoding,tf.log(softmax)))\n\ncross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=tf.log(softmax), labels=onehot_encoding)\n\nwith tf.Session() as session:\n    print(session.run(cross_entropy,feed_dict={softmax:softmax_data, onehot_encoding:onehot_data} ))\n    print(session.run(cross_entropy_loss,feed_dict={softmax:softmax_data, onehot_encoding:onehot_data} ))\n\n```", "```py\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n```", "```py\n# All the pixels in the image (28 * 28 = 784)\nfeatures_count = 784\n# there are 10 digits i.e labels\nlabels_count = 10\nbatch_size = 128\nepochs = 10\nlearning_rate = 0.5\n\nfeatures = tf.placeholder(tf.float32, [None,features_count])\nlabels = tf.placeholder(tf.float32, [None, labels_count])\n\n#Set the weights and biases tensors\nweights = tf.Variable(tf.truncated_normal((features_count, labels_count)))\nbiases = tf.Variable(tf.zeros(labels_count),name='biases')\n```", "```py\nloss,\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    \n```", "```py\n# Linear Function WX + b\nlogits = tf.add(tf.matmul(features, weights),biases)\n\nprediction = tf.nn.softmax(logits)\n\n# Cross entropy\ncross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n\n# Training loss\nloss = tf.reduce_mean(cross_entropy)\n\n# Initializing all variables\ninit = tf.global_variables_initializer()\n\n# Determining if the predictions are accurate\nis_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n# Calculating prediction accuracy\naccuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n```", "```py\n#Beginning the session\nwith tf.Session() as sess:\n   # initializing all the variables\n   sess.run(init)\n   total_batch = int(len(mnist.train.labels) / batch_size)\n   for epoch in range(epochs):\n        avg_cost = 0\n        for i in range(total_batch):\n            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n            _, c = sess.run([optimizer,loss], feed_dict={features: batch_x, labels: batch_y})\n            avg_cost += c / total_batch\n        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n   print(sess.run(accuracy, feed_dict={features: mnist.test.images, labels: mnist.test.labels}))\n```", "```py\nfrom keras.models import Sequential\n\n#Creating the Sequential model\nmodel = Sequential()\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Flatten\n\n#Creating the Sequential model\nmodel = Sequential()\n\n#Layer 1 - Adding a flatten layer\nmodel.add(Flatten(input_shape=(32, 32, 3)))\n\n#Layer 2 - Adding a fully connected layer\nmodel.add(Dense(100))\n\n#Layer 3 - Adding a ReLU activation layer\nmodel.add(Activation('relu'))\n\n#Layer 4- Adding a fully connected layer\nmodel.add(Dense(60))\n\n#Layer 5 - Adding an ReLU activation layer\nmodel.add(Activation('relu'))\n```", "```py\n# Import Numpy, keras and MNIST data\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.utils import np_utils\n```", "```py\n# Retrieving the training and test data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape: ', X_test.shape)\nprint('y_train shape:',y_train.shape)\nprint('y_test shape: ', y_test.shape)\n```", "```py\n# Visualize the data\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#Displaying a training image by its index in the MNIST set\ndef display_digit(index):\n    label = y_train[index].argmax(axis=0)\n    image = X_train[index]\n    plt.title('Training data, index: %d,  Label: %d' % (index, label))\n    plt.imshow(image, cmap='gray_r')\n    plt.show()\n\n# Displaying the first (index 0) training image\ndisplay_digit(0)\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nprint(\"Train the matrix shape\", X_train.shape)\nprint(\"Test the matrix shape\", X_test.shape)\n\n#One Hot encoding of labels.\nfrom keras.utils.np_utils import to_categorical\nprint(y_train.shape)\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\nprint(y_train.shape)\n```", "```py\n# Defining the neural network\ndef build_model():\n    model = Sequential()\n    model.add(Dense(512, input_shape=(784,)))\n    model.add(Activation('relu')) # An \"activation\" is just a non-linear function that is applied to the output\n # of the above layer. In this case, with a \"rectified linear unit\",\n # we perform clamping on all values below 0 to 0.\n\n    model.add(Dropout(0.2))   #With the help of Dropout helps we can protect the model from memorizing or \"overfitting\" the training data\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(10))\n    model.add(Activation('softmax')) # This special \"softmax\" activation,\n    #It also ensures that the output is a valid probability distribution,\n    #Meaning that values obtained are all non-negative and sum up to 1.\n    return model\n\n#Building the model\nmodel = build_model()\nmodel.compile(optimizer='rmsprop',\n          loss='categorical_crossentropy',\n          metrics=['accuracy'])\n```", "```py\n# Training\nmodel.fit(X_train, y_train, batch_size=128, nb_epoch=4, verbose=1,validation_data=(X_test, y_test))\n```", "```py\n# Comparing the labels predicted by our model with the actual labels\n\nscore = model.evaluate(X_test, y_test, batch_size=32, verbose=1,sample_weight=None)\n# Printing the result\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n```", "```py\nz = np.maximum(0, np.dot(W, x)) #Representing forward pass\ndW = np.outer(z > 0, x) #Representing backward pass: local gradient for W\n```"]