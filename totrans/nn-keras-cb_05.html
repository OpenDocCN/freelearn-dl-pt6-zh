<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Transfer Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, we learned about recognizing the class that an image belongs to in a given image. In this chapter, we will learn about one of the drawbacks of CNN and also about how we can overcome it using certain pre-trained models.</p>
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Gender classification of a person in an image using CNNs</li>
<li><span>Gender classification of a </span>person in image using the VGG16 architecture-based model</li>
<li>Visualizing the output of the intermediate layers of a neural network</li>
<li><span>Gender classification of a person</span> in image using the VGG19 architecture-based model</li>
<li><span>Gender classification of a </span>using the ResNet architecture-based model</li>
<li><span>Gender classification of a </span>using the inception architecture-based model</li>
<li>Detecting the key points within image of a face</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gender classification of the person in an image using CNNs</h1>
                </header>
            
            <article>
                
<p>To understand some of the limitations of CNNs, let's go through an example where we try to identify whether the given image contains the image of a cat or a dog.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span><span>We will</span></span> gain an intuition of how a CNN predicts the class of object present in the image through the following steps:</p>
<ul>
<li>A convolution filter is activated by certain parts of the image:
<ul>
<li>For example, certain filters might activate if the image has a certain pattern—<span>it</span> contains a circular structure, for example</li>
</ul>
</li>
<li>A pooling layer ensures that image translation is taken care of:
<ul>
<li>This ensures that even if an image is big, over an increased number of pooling operations, the size of the image becomes small and the object can then be detected as the object is now expected to be in the smaller portion of the image (as it is pooled multiple times)</li>
</ul>
</li>
<li>The final flatten layer flattens all the patterns that are extracted by various convolution and pooling operations</li>
</ul>
<p>Let's impose a scenario where the number of images in a training dataset is small. In such a case, the model does not have enough data points for it to generalize on a test dataset.</p>
<p>Additionally, given that the convolutions are learning various features from scratch, it could potentially take many epochs before the model starts to fit on top of the training dataset if the training dataset contains images that have a large shape (width and height).</p>
<p>Hence, in the next section, we will code the following scenario of building a CNN, where there are a few images (~1,700 images) and test the accuracy on different shapes of images:</p>
<ul>
<li>Accuracy in 10 epochs where the image size is 300 X 300</li>
<li>Accuracy in 10 epochs where the image size is 50 X 50</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this section, we will fetch a dataset and perform classification analysis where the image size in one scenario is 300 x 300, while in the other scenario, it is 50 x 50. (Please refer to <kbd>Transfer_learning.ipynb</kbd> file in GitHub while implementing the code.)</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scenario 1 – big images</h1>
                </header>
            
            <article>
                
<ol>
<li>Fetch the dataset. For this analysis, we will continue with the male versus female classification dataset that we have downloaded in the Gender classification case study in <a href="750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml" target="_blank">Chapter 4</a>, <em>Building a Deep Convolutional Neural Network</em>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ wget https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2017/04/a943287.csv<br/></strong></pre>
<pre style="padding-left: 60px">import pandas as pd, numpy as np<br/>from skimage import io<br/># Location of file is /content/a943287.csv<br/># be sure to change to location of downloaded file on your machine<br/>data = pd.read_csv('/content/a943287.csv')</pre>
<pre style="padding-left: 60px"><strong> </strong>data_male = data[data['please_select_the_gender_of_the_person_in_the_picture']=="male"].reset_index(drop='index')<br/>data_female = data[data['please_select_the_gender_of_the_person_in_the_picture']=="female"].reset_index(drop='index')<br/>final_data = pd.concat([data_male[:1000],data_female[:1000]],axis=0).reset_index(drop='index')</pre>
<ol start="2">
<li>Extract the image paths and then prepare the input and output data:</li>
</ol>
<pre style="padding-left: 60px">x = []<br/>y = []<br/>for i in range(final_data.shape[0]):<br/>     try:<br/>         image = io.imread(final_data.loc[i]['image_url'])<br/>         if(image.shape==(300,300,3)):<br/>             x.append(image)<br/>             y.append(final_data.loc[i]['please_select_the_gender_of_the_person_in_the_picture'])<br/>     except:<br/>         continue</pre>
<ol start="3">
<li>A sample of the images is as follows:</li>
</ol>
<p style="padding-left: 60px" class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1279 image-border" src="Images/45f228ee-b2bc-43e1-af97-beb22587efb2.png" style="width:28.42em;height:23.50em;" width="341" height="282"/></p>
<p style="padding-left: 60px">Note that all the images are 300 x 300 x 3 in size. </p>
<ol start="4">
<li>Create the input and output dataset arrays:</li>
</ol>
<pre style="padding-left: 60px">x2 = []<br/>y2 = []<br/>for i in range(len(x)):<br/>      x2.append(x[i])<br/>      img_label = np.where(y[i]=="male",1,0)<br/>      y2.append(img_label)</pre>
<p style="padding-left: 90px">In the preceding step, we are looping through all the images (one at a time), reading the image into an array (we could have gotten away without this step in this iteration. However, in the next scenario of resizing the image, we will resize images in this step). Additionally, we are storing the labels of each image.</p>
<ol start="5">
<li>Prepare the input array so that it can be passed to a CNN. Additionally, prepare the output array:</li>
</ol>
<pre style="padding-left: 60px">x2 = np.array(x2)<br/>x2 = x2.reshape(x2.shape[0],x2.shape[1],x2.shape[2],3)</pre>
<p style="padding-left: 90px">Here, we are converting the list of arrays into a numpy array so that it can then be passed to the neural network.</p>
<p style="padding-left: 60px">Scale the input array and create input and output arrays:</p>
<pre style="padding-left: 60px">X = np.array(x2)/255<br/>Y = np.array(y2)</pre>
<ol start="6">
<li>Create train and test datasets:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.1, random_state=42)</pre>
<ol start="7">
<li>Define the model and compile it:</li>
</ol>
<pre style="padding-left: 60px">from keras.models import Sequential<br/>from keras.layers.core import Dense, Dropout, Activation, Flatten<br/>from keras.layers.convolutional import Conv2D<br/>from keras.layers.pooling import MaxPooling2D<br/>from keras.optimizers import SGD<br/>from keras import backend as K<br/><br/>model = Sequential()<br/>model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',input_shape=(300,300,3)))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu',padding='same'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Conv2D(256, kernel_size=(3, 3), activation='relu',padding='same'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Conv2D(512, kernel_size=(3, 3), activation='relu',padding='same'))<br/>model.add(Flatten())<br/>model.add(Dense(100, activation='relu'))<br/>model.add(Dense(1, activation='sigmoid'))<br/>model.summary()</pre>
<p style="padding-left: 60px">In the preceding code, we are building a model that has multiple layers of convolution, pooling, and dropout. Furthermore, we are passing the output of final dropout through a flattening layer and then connecting the flattened output to a 512 node hidden layer before connecting the hidden layer to the output layer.</p>
<p style="padding-left: 60px">A summary of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1280 image-border" src="Images/d240df6a-7253-411a-bfd8-ee5b457bb19b.png" style="width:36.50em;height:31.17em;" width="509" height="434"/></p>
<p style="padding-left: 60px">In the following code, we are compiling the model to reduce binary cross entropy loss, as follows:</p>
<pre style="padding-left: 60px">model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])</pre>
<ol start="8">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 90px">history = model.fit(X_train, y_train, batch_size=32,epochs=10,verbose=1,validation_data = (X_test, y_test))</pre>
<p style="padding-left: 60px">In the preceding step, you can see that the model does not train over increasing epochs, as shown in the following graph (the code for this diagram is the same as we saw in the <em>Scaling input data</em> section in <a href="2846fd6b-a2c2-4ebe-b36a-17f061b4cfa4.xhtml" target="_blank">Chapter 2</a>, <em>Building a Deep Feedforward Neural Network</em>, and it can be found in the GitHub repository of this chapter):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1281 image-border" src="Images/d79e7aad-4074-4f56-a06d-feaf5337835d.png" style="width:33.92em;height:28.00em;" width="382" height="315"/></p>
<p style="padding-left: 60px">In the preceding graph, you can see that the model hardly learned anything, as the loss did not vary much. Also, the accuracy was stuck near the 51% mark (which is roughly the distribution of male versus female images in the original dataset).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scenario 2 – smaller images</h1>
                </header>
            
            <article>
                
<p>In this scenario, we will modify the following in the model:</p>
<ul>
<li class="CDPAlignLeft CDPAlign">Input image size:
<ul>
<li>We will reduce the size from 300 X 300 to 50 X 50</li>
</ul>
</li>
<li>Model architecture:
<ul>
<li>The structure of the architecture remains the same as what we saw in <em><em>Scenario 1 – big images</em></em></li>
</ul>
</li>
</ul>
<ol start="1">
<li>Create a dataset with the input of the reduced image size (50 X 50 X 3) and output labels. For this, we will continue from <em>step 4</em> of Scenario 1:</li>
</ol>
<pre style="padding-left: 60px">import cv2<br/>x2 = []<br/>y2 = []<br/>for i in range(len(x)):<br/>  img = cv2.resize(x[i],(50,50))<br/>  x2.append(img)<br/>  img_label = np.where(y[i]=="male",1,0)<br/>  y2.append(img_label)</pre>
<ol start="2">
<li>Create the input and output arrays for the train, test datasets:</li>
</ol>
<pre style="padding-left: 60px">x2 = np.array(x2)<br/>x2 = x2.reshape(x2.shape[0],x2.shape[1],x2.shape[2],3)<br/>X = np.array(x2)/255<br/>Y = np.array(y2)<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.1, random_state=42)</pre>
<ol start="3">
<li>Build and compile the model:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',input_shape=(50,50,3)))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu',padding='same'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Conv2D(256, kernel_size=(3, 3), activation='relu',padding='same'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Conv2D(512, kernel_size=(3, 3), activation='relu',padding='same'))<br/>model.add(Flatten())<br/>model.add(Dense(100, activation='relu'))<br/>model.add(Dense(1, activation='sigmoid'))<br/>model.summary()<br/><br/>model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])</pre>
<p style="padding-left: 90px">A summary of the model is as follows:</p>
<p style="padding-left: 60px" class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1283 image-border" src="Images/55212a46-0138-4e44-9c56-04382a0bad47.png" style="width:36.00em;height:30.83em;" width="506" height="434"/></p>
<ol start="4">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 60px">history = model.fit(X_train, y_train, batch_size=32,epochs=10,verbose=1,validation_data = (X_test, y_test))</pre>
<p style="padding-left: 90px">The accuracy and loss of the model training across train and test datasets over increasing epochs is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1284 image-border" src="Images/46cf59b8-b7ce-4522-8730-c564a321597a.png" style="width:32.58em;height:26.25em;" width="391" height="315"/></p>
<p>Note that, while the accuracy increased and the loss decreased steadily in both the training and test datasets initially, over increasing epochs, the model started to overfit (specialize) on training data and had an accuracy of ~76% on the test dataset.</p>
<p>From this, we can see that the CNN works when the input size is small and thus the filters had to learn from a smaller portion of the image. However, as, the image size increased, the CNN had a tough time learning.</p>
<div class="packt_tip packt_infobox">Given that we have discovered that the image size has an impact on model accuracy, in the new scenario, let's use aggressive pooling to ensure that the bigger image (300 x 300 shape) reduces to a smaller one quickly.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scenario 3 – aggressive pooling on big images</h1>
                </header>
            
            <article>
                
<p>In the following code, we will retain the analysis we have done until step 6 in Scenario 1. However, the only change will be the model architecture; in the following model architecture, we have more aggressive pooling than what we used in Scenario 1.</p>
<p>In the following architecture, having a bigger window of pooling in each layer ensures that we capture the activations in a larger area compared to the scenario of having lower pool sizes. The architecture of the model is as follows:</p>
<pre>model = Sequential()<br/>model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',input_shape=(300,300,3)))<br/>model.add(MaxPooling2D(pool_size=(3, 3)))<br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu',padding='same'))<br/>model.add(MaxPooling2D(pool_size=(3, 3)))<br/>model.add(Conv2D(256, kernel_size=(3, 3), activation='relu',padding='same'))<br/>model.add(MaxPooling2D(pool_size=(3, 3)))<br/>model.add(Conv2D(512, kernel_size=(3, 3), activation='relu',padding='same'))<br/>model.add(Flatten())<br/>model.add(Dense(100, activation='relu'))<br/>model.add(Dense(1, activation='sigmoid'))<br/>model.summary()</pre>
<p>Note that in this architecture, the pool size is 3 x 3 and not 2 x 2, as we had in the previous scenario:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1285 image-border" src="Images/754dcf89-b23c-4d37-adec-f09749e89b81.png" style="width:31.25em;height:27.58em;" width="513" height="454"/></p>
<p>Once we fit a model on the input and output arrays, the variation of accuracy and loss on the train and test datasets is as follows:</p>
<pre>model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])<br/>history = model.fit(X_train, y_train, batch_size=32,epochs=10,verbose=1,validation_data = (X_test, y_test))</pre>
<p>The following is the output of the preceding code:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1286 image-border" src="Images/97fe1ba4-eb37-476a-9328-3b3f225d9952.png" style="width:37.17em;height:29.17em;" width="404" height="316"/></p>
<p>We can see that the test data has ~70% accuracy in correctly classifying gender in images.</p>
<p>However, you can see that there is a considerable amount of overfitting on top of the training dataset (as the loss decreases steadily on the training dataset, while not on the test dataset).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gender classification of the person in image using the VGG16 architecture-based model</h1>
                </header>
            
            <article>
                
<p>In the previous section on gender classification using CNN, we saw that when we build a CNN model from scratch, we could encounter some of the following scenarios:</p>
<ul>
<li>The number of images that were passed is not sufficient for the model to learn</li>
<li>Convolutions might not be learning all the features in our images when the images are big in size</li>
</ul>
<p>The first problem could be tackled by performing our analysis on a large dataset. The second one could be tackled by training a larger network on the larger dataset for a longer number of epochs.</p>
<p>However, while we are able to perform all of this, more often than not, we do not have the amount of data that is needed to perform such an analysis. Transfer learning using pre-trained models comes to the rescue in such scenarios.</p>
<p>ImageNet is a popular competition where participants are asked to predict the various classes of an image, where the images are of various sizes and also contain multiple classes of objects.</p>
<p>There were multiple research teams that competed in this competition to come up with a model that is able to predict images of multiple classes where there are millions of images in a dataset. Given that there were millions of images, the first problem of a limited dataset is resolved. Additionally, given the huge networks the research teams have built, the problem of coming up with convolutions that learn a variety of features is also resolved.</p>
<p>Hence, we are in a position to reuse the convolutions that were built on a different dataset, where the convolutions are learning to predict the various features in an image and then pass them through a hidden layer so that we can predict the class of an image for our specific dataset. There are multiple pre-trained models that were developed by different groups. We will go through VGG16 here.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this section, let's try to understand how we can leverage the VGG16 pre-trained network for our gender classification exercise.</p>
<p>The VGG16 model's architecture is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1287 image-border" src="Images/c55e5a9b-e05e-4c31-9943-aeb2a6ae4c83.png" style="width:33.42em;height:46.58em;" width="409" height="568"/></p>
<p>Notice that the model's architecture is very similar to the model that we trained in the Gender classification using CNNs section. The major difference is that this model is deeper (more hidden layers). Additionally, the weights of the VGG16 network are obtained by training on millions of images.</p>
<p>We'll ensure that the VGG16 weights are frozen from updating while training our model to classify gender in an image. The output of passing an image in the gender classification exercise<span> </span>(which is of 300 x 300 x 3 in shape) is 9 x 9 x 512 in shape.</p>
<p>We shall keep the weights as they were in the original network, extract the <span>9 x 9 x 512</span> output, pass it through another convolution pooling operation, flatten it, connect it to a hidden layer, and then pass it through the sigmoid activation to determine whether the image is of a male or a female.</p>
<p>Essentially, by using the convolution and pooling layers of the VGG16 model, we are using the filters that were trained on a much bigger dataset. Ultimately, we will be fine-tuning the output of these convolution and pooling layers for the objects that we are trying to predict.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>With this strategy in place, let's code up our solution as follows <span>(Please refer to <kbd>Transfer_learning.ipynb</kbd> file in GitHub while implementing the code)</span>:</p>
<ol>
<li>Import the pre-trained model:</li>
</ol>
<pre style="padding-left: 60px">from keras.applications import vgg16<br/>from keras.utils.vis_utils import plot_model<br/>from keras.applications.vgg16 import preprocess_input<br/>vgg16_model = vgg16.VGG16(include_top=False, weights='imagenet',input_shape=(300,300,3))</pre>
<p style="padding-left: 60px">Note that we are excluding the last layer in the VGG16 model. This is to ensure that we fine-tune the VGG16 model for the problem that we are trying solve. Additionally, given that our input image shape is 300 X 300 X 3, we are specifying the same while downloading the VGG16 model.</p>
<ol start="2">
<li>Preprocess the set of images. This preprocessing step ensures that the images are processed in a manner that the pre-trained model can take as input. For example, in the following code, we are performing preprocessing<span> </span>for one of the images, named<span> </span><kbd>img</kbd>:</li>
</ol>
<pre style="padding-left: 60px">from keras.applications.vgg16 import preprocess_input<br/>img = preprocess_input(img.reshape(1,224,224,3))</pre>
<p style="padding-left: 60px">We are preprocessing the image as per the preprocessing requirement in VGG16 using the<span> </span><kbd>preprocess_input</kbd><span> </span>method.</p>
<ol start="3">
<li>Create the input and output datasets. For this exercise, we will continue from the end of step 3 in Scenario 1 of Gender classification using CNN. Here, the process of creating input and output datasets remains the same as what we have already done, with a minor modification of extracting features using the VGG16 model.</li>
</ol>
<p style="padding-left: 60px">We will pass each image through <kbd>vgg16_model</kbd><span> </span>so that we take the output of<span> </span><kbd>vgg16_model</kbd><span> </span>as the processed input. <span>Additionally, we will be performing the preprocessing on top of the input as follows:</span></p>
<pre style="padding-left: 60px">import cv2<br/>x2_vgg16 = []<br/>for i in range(len(x)):<br/>    img = x[i]<br/>    img = preprocess_input(img.reshape(1,300,300,3))</pre>
<p style="padding-left: 60px"><span><span>Now, we pass the pre-processed input to the VGG16 model to extract features, as follows:</span></span></p>
<pre style="padding-left: 60px">    img_new = vgg16_model.predict(img.reshape(1,300,300,3))<br/>    x2_vgg16.append(img_new)</pre>
<p style="padding-left: 60px">In the preceding code, in addition to passing the image through VGG16 model, we have also stored the input values in a list.</p>
<ol start="4">
<li>Convert the input and output to NumPy arrays and create training and test datasets:</li>
</ol>
<pre style="padding-left: 60px">x2_vgg16 = np.array(x2_vgg16)<br/>x2_vgg16= x2_vgg16.reshape(x2_vgg16.shape[0],x2_vgg16.shape[2],x2_vgg16.shape[3],x2_vgg16.shape[4])<br/>Y = np.array(y2)<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(x2_vgg16,Y, test_size=0.1, random_state=42)</pre>
<ol start="5">
<li>Build and compile the<span> </span>model:</li>
</ol>
<pre style="padding-left: 60px">model_vgg16 = Sequential()<br/>model_vgg16.add(Conv2D(512, kernel_size=(3, 3), activation='relu',input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3])))<br/>model_vgg16.add(MaxPooling2D(pool_size=(2, 2)))<br/>model_vgg16.add(Flatten())<br/>model_vgg16.add(Dense(512, activation='relu'))<br/>model_vgg16.add(Dropout(0.5))<br/>model_vgg16.add(Dense(1, activation='sigmoid'))<br/>model_vgg16.summary()</pre>
<p style="padding-left: 60px"> The summary of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1289 image-border" src="Images/05928ce6-c892-492b-87f5-bdb7ba98730c.png" style="width:35.25em;height:20.67em;" width="508" height="298"/></p>
<p style="padding-left: 60px">Compile the model:</p>
<pre style="padding-left: 60px">model_vgg16.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])</pre>
<ol start="6">
<li>Fit the model <span>while scaling the input data</span>:</li>
</ol>
<pre style="padding-left: 60px">history_vgg16 = model_vgg16.fit(X_train/np.max(X_train), y_train, batch_size=16,epochs=10,verbose=1,validation_data = (X_test/np.max(X_train), y_test))</pre>
<p style="padding-left: 60px">Once we fit the model, we should see that we are able to attain an accuracy of ~89% on the test dataset in the first few epochs:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1290 image-border" src="Images/1d5465bb-b9da-4905-a985-3d0a7c1d6b23.png" style="width:31.75em;height:26.08em;" width="407" height="334"/></p>
<p>Contrast this with the models we built in the Gender classification using CNN section, where in any of the scenarios, we were not able to reach 80% accuracy in classification in 10 epochs.</p>
<p>A sample of some of the images where the model mis-classified is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1291 image-border" src="Images/fcb824ca-4791-478c-9d65-7d2f63cec26c.png" style="width:22.75em;height:17.42em;" width="312" height="239"/></p>
<p>Note that, in the preceding picture, the model potentially mis-classified when the input image is either a part of a face or if the object in the image occupies a much smaller portion of the total image or potentially, if the label was provided incorrectly.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Visualizing the output of the intermediate layers of a neural network</h1>
                </header>
            
            <article>
                
<p>In the previous section, we built a model that learns to classify gender from images with an accuracy of 89%. However, as of now, it is a black box for us in terms of what the filters are learning.</p>
<p>In this section, we will learn how to extract what the various filters in a model are learning. Additionally, we will contrast the scenario of what the filters in the initial layers are learning with what the features in the last few layers are learning.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To understand how to extract what the various filters are learning, let's adopt the following strategy:</p>
<ul>
<li class="CDPAlignLeft CDPAlign">We will select an image on which to perform analysis.</li>
<li>We will select the first convolution to understand what the various filters in the first convolution are learning.</li>
<li>Calculate the output of the convolution of weights in the first layer and the input image:
<ul>
<li>In this step, we will extract the intermediate output of our model:
<ul>
<li>We will be extracting the output of the first layer of the model.</li>
</ul>
</li>
</ul>
</li>
<li>To extract the output of first layer, we will use the functional API:
<ul>
<li>The input to the functional API is the input image, and the output will be the output of the first layer.</li>
</ul>
</li>
<li>This returns the output of the intermediate layer across all the channels (filters).</li>
<li>We will perform these steps on both the first layer and the last layer of the convolution.</li>
<li>Finally, we will visualize the output of the convolution operations across all channels.</li>
<li>We will also visualize the output of a given channel across all images.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this section, we will code up the process of visualizing what the filters are learning across the convolution filters of the initial layers as well as the final layers.</p>
<p>We'll reuse the data that we prepared in the <em>Gender classification using CNN</em> recipe's Scenario 1 from <em>step 1</em> to <em>step 4</em> <span>(please refer to <kbd>Transfer_learning.ipynb</kbd> file in GitHub while implementing the code)</span><span>:</span></p>
<ol start="1">
<li>Identify an image for which you want to visualize the intermediate output:</li>
</ol>
<pre style="padding-left: 60px">plt.imshow(x[3])<br/>plt.grid('off')</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1292 image-border" src="Images/009c041f-b3c5-40f0-973f-726d4f649b44.png" style="width:21.33em;height:20.83em;" width="256" height="250"/></p>
<ol start="2">
<li>Define the functional API that takes the image as an input, and the first convolution layer's output as output:</li>
</ol>
<pre style="padding-left: 60px">from keras.applications.vgg16 import preprocess_input<br/>model_vgg16.predict(vgg16_model.predict(preprocess_input(x[3].reshape(1,300,300,3)))/np.max(X_train))<br/>from keras import models<br/>activation_model = models.Model(inputs=vgg16_model.input,outputs=vgg16_model.layers[1].output)<br/>activations = activation_model.predict(preprocess_input(x[3].reshape(1,300,300,3)))</pre>
<p style="padding-left: 60px">We have defined an intermediate model named <kbd>activation_model</kbd>, where we are passing the image of interest as input and extracting the first layer's output as the model's output.</p>
<p style="padding-left: 60px">Once we have defined the model, we will extract the activations of the first layer by passing the input image through the model. Note that we will have to reshape the input image so that it is the shape the model expects. </p>
<ol start="3">
<li>Let's visualize the first 36 filters in the output, as follows:</li>
</ol>
<pre style="padding-left: 60px">fig, axs = plt.subplots(6, 6, figsize=(10, 10))<br/>fig.subplots_adjust(hspace = .5, wspace=.5)<br/>first_layer_activation = activations[0]<br/>for i in range(6):<br/>  for j in range(6):<br/>    try:<br/>      axs[i,j].set_ylim((224, 0))<br/>      axs[i,j].contourf(first_layer_activation[:,:,((6*i)+j)],6,cmap='viridis')<br/>      axs[i,j].set_title('filter: '+str((6*i)+j))<br/>      axs[i,j].axis('off')<br/>    except:<br/>      continue</pre>
<ol start="4">
<li class="CDPAlignLeft CDPAlign">In the preceding code, we created a 6 x 6 frame on which we can plot 36 images. Furthermore, we are looping through all the channels in <kbd>first_layer_activation</kbd> and plotting the output of the first layer, as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1293 image-border" src="Images/ae741c21-6feb-427e-abbf-692edf9c8774.png" style="width:41.17em;height:41.08em;" width="518" height="516"/></p>
<p style="padding-left: 60px"><span><span>Here,</span></span> we can see that certain filters extract the contours of the original image (filter 0, 4, 7, 10, for example). Additionally, certain filters have learned to recognize only a few aspects, such as ears, eyes, and nose (filter 30, for example).</p>
<ol start="5">
<li>Let's validate our understanding that certain filters are able to extract contours of the original image by going through the output of filter 7 for 36 images, as follows:</li>
</ol>
<pre style="padding-left: 60px">activation_model = models.Model(inputs=vgg16_model.input,outputs=vgg16_model.layers[1].output)<br/>activations = activation_model.predict(preprocess_input(np.array(x[:36]).reshape(36,300,300,3)))<br/>fig, axs = plt.subplots(6, 6, figsize=(10, 10))<br/>fig.subplots_adjust(hspace = .5, wspace=.5)<br/>first_layer_activation = activations<br/>for i in range(6):<br/>  for j in range(6):<br/>    try:<br/>      axs[i,j].set_ylim((224, 0))<br/>      axs[i,j].contourf(first_layer_activation[((6*i)+j),:,:,7],6,cmap='viridis')<br/>      axs[i,j].set_title('filter: '+str((6*i)+j))<br/>      axs[i,j].axis('off')<br/>    except:<br/>      continue</pre>
<p style="padding-left: 60px">In the preceding code, we are looping through the first 36 images and plotting the output of the first convolution layer for all 36 images:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1294 image-border" src="Images/1d4d36d6-33c2-45aa-9a20-ace2c48fa515.png" style="width:41.75em;height:41.33em;" width="521" height="517"/></p>
<p style="padding-left: 60px">Note that, across all the images, the seventh filter is learning the contours within an image.</p>
<ol start="6">
<li>Let's try to understand what the filters in the last convolution layer are learning. To understand where the last convolution layer is located in our model, let's extract the various layers in our model:</li>
</ol>
<pre style="padding-left: 60px">for i, layer in enumerate(model.layers):<br/>     print(i, layer.name)</pre>
<p style="padding-left: 60px">The following layers name will be displayed by executing the preceding code:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1295 image-border" src="Images/2eec4bdc-3084-42c3-bb48-c6718498c6d0.png" style="width:11.17em;height:20.33em;" width="142" height="258"/></p>
<ol start="7">
<li>Note that the last convolution layer is the ninth output of our model and can be extracted as follows:</li>
</ol>
<pre style="padding-left: 60px">activation_model = models.Model(inputs=vgg16_model.input,outputs=vgg16_model.layers[-1].output)<br/>activations = activation_model.predict(preprocess_input(x[3].reshape(1,300,300,3)))</pre>
<p style="padding-left: 60px">The size of the image has now shrunk considerably (to <span>1, 9,9,512)</span>, due to the multiple pooling operations that were performed on top of the image. A visualization of what the various filters in the last convolution layer are learning is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1296 image-border" src="Images/a9768459-b4fe-4a48-9f37-be7dcaf450fb.png" style="width:43.50em;height:42.67em;" width="581" height="571"/></p>
<p>Note that, in this iteration, it is not as clear to understand what the last convolution layer's filters are learning (as the contours are not easy to attribute to one of the parts of original image), as these are more granular than the contours that were learned in the first convolution layer.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gender classification of the person in image using the VGG19 architecture-based model</h1>
                </header>
            
            <article>
                
<p><span>In the previous section, we learned about how VGG16 works. VGG19 is an improved version of VGG16, with a greater number of convolution and pooling operations.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The architecture of the VGG19 model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1297 image-border" src="Images/6d17d7c6-4df1-4ad0-8e47-16e93e90cadd.png" style="width:27.92em;height:38.92em;" width="409" height="568"/></p>
<p>Note that the preceding architecture has more layers, as well as more parameters.</p>
<p>Note that the 16 and 19 in the VGG16 and VGG19 architectures stand for the number of layers in each of these networks. Once we extract the 9 x 9 x 512 output after we pass each image through the VGG19 network, that output will be the input for our model.</p>
<p>Additionally, the process of creating input and output datasets and then building, compiling, and fitting a model will remain the same as what we saw in the Gender classification using a VGG16 model-based architecture recipe.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this section, we will code up the VGG19 pre-trained model, as follows <span>(Please refer to</span> <kbd>Transfer_learning.ipynb</kbd> <span>file in GitHub while implementing the code)</span>:</p>
<ol>
<li>Prepare the input and output data (we'll continue from <em>step 3</em> in Scenario 1 of the <em>Gender classification using CNN</em> recipe):</li>
</ol>
<pre style="padding-left: 60px">import cv2<br/>x2 = []<br/>for i in range(len(x)):<br/>    img = x[i]<br/>    img = preprocess_input(img.reshape(1,300,300,3))<br/>    img_new = vgg19_model.predict(img.reshape(1,300,300,3))<br/>    x2.append(img_new)</pre>
<ol start="2">
<li>Convert the input and output into their corresponding arrays and create the training and test datasets:</li>
</ol>
<pre style="padding-left: 60px">x2 = np.array(x2)<br/>x2= x2.reshape(x2.shape[0],x2.shape[2],x2.shape[3],x2.shape[4])<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(x2,Y, test_size=0.1, random_state=42)</pre>
<ol start="3">
<li>Build and compile the model:</li>
</ol>
<pre style="padding-left: 60px">model_vgg19 = Sequential()<br/>model_vgg19.add(Conv2D(512, kernel_size=(3, 3), activation='relu',input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3])))<br/>model_vgg19.add(MaxPooling2D(pool_size=(2, 2)))<br/>model_vgg19.add(Flatten())<br/>model_vgg19.add(Dense(512, activation='relu'))<br/>model_vgg19.add(Dropout(0.5))<br/>model_vgg19.add(Dense(1, activation='sigmoid'))<br/>model_vgg19.summary()</pre>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">A visualization of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1298 image-border" src="Images/62ec64d6-36d0-41c4-9855-8692eb6c3594.png" style="width:36.17em;height:21.25em;" width="507" height="298"/></p>
<pre style="padding-left: 60px">model_vgg19.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])</pre>
<ol start="4">
<li>Fit the model <span>while scaling the input data</span>:</li>
</ol>
<pre style="padding-left: 60px">history_vgg19 = model_vgg19.fit(X_train/np.max(X_train), y_train, batch_size=16,epochs=10,verbose=1,validation_data = (X_test/np.max(X_train), y_test))</pre>
<p style="padding-left: 60px">Let's plot the training and test datasets' loss and accuracy measures:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1299 image-border" src="Images/42a19290-8e44-4746-92c9-acc35015a8f5.png" style="width:32.67em;height:26.17em;" width="392" height="314"/></p>
<p>We should note that we were able to achieve ~89% accuracy on the test dataset when we used the VGG19 architecture, which is very similar to that of the VGG16 architecture.</p>
<p>A sample of mis-classified images is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1301 image-border" src="Images/73d219f5-a4bd-412c-b609-880880dfe98e.png" style="width:24.83em;height:19.25em;" width="311" height="241"/></p>
<p>Note that, VGG19 seems to mis-classify based on the space occupied by a person in an image. Additionally, it seems to give higher weightage to predict that a male with long hair is a female.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gender classification using the Inception v3 architecture-based model</h1>
                </header>
            
            <article>
                
<p>In the previous recipes, we implemented gender classification based on the VGG16 and VGG19 architectures. In this section, we'll implement the classification using the Inception architecture.</p>
<p><span>An intuition of how inception model comes in handy, is as follows.</span></p>
<p>There will be images where the object occupies the majority of the image. Similarly, there will be images where the object occupies a small portion of the total image. If we have the same size of kernels in both scenario, we are making it difficult for the model to learn <span>–</span> some images might have objects that are small and others might have objects that are larger.</p>
<p>To address this problem, we will have filters of multiple sizes that operate at the same layer.</p>
<p>In such a scenario, the network essentially gets wide rather than getting deep, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1082 image-border" src="Images/54cce575-4a32-480d-bedf-29f96bb1f82e.png" style="width:44.50em;height:21.25em;" width="1508" height="719"/></p>
<p>In the preceding diagram, note that we are performing convolutions of multiple filters in a given layer. The inception v1 module has nine such modules stacked linearly, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1302 image-border" src="Images/80aeb9f6-c85e-4033-83a1-15593f4127c7.png" style="width:60.00em;height:15.00em;" width="720" height="180"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Source: http://joelouismarino.github.io/images/blog_images/blog_googlenet_keras/googlenet_diagram.png</div>
<p>Note that this architecture is fairly deep as well as wide. This is likely to result in a vanishing gradient problem (as we saw in the case for batch normalization in <a href="2846fd6b-a2c2-4ebe-b36a-17f061b4cfa4.xhtml" target="_blank">Chapter 2</a>, <em>Building a Deep Feedforward Neural Network</em>).</p>
<p>To get around the problem of a vanishing gradient, inception v1 has two auxiliary classifiers that stem out of the inception modules. The overall loss of inception based network tries to minimize is as follows:</p>
<pre class="graf graf--pre graf-after--p">total_loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2</pre>
<p>Note that auxiliary losses are used only during training and are ignored during the prediction process.</p>
<p>Inception v2 and v3 are improvements on top of the inception v1 architecture where in v2, the authors have performed optimizations on top of convolution operations to process images faster and in v3, the authors have added 7 x 7 convolutions on top of the existing convolutions so that they can be concatenated together.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The process in which we code up inception v3 is very similar to the way in which we built the VGG19 model-based classifier (<span>Please refer to <kbd>Transfer_learning.ipynb</kbd> file in GitHub while implementing the code)</span>:</p>
<ol>
<li>Download the pre-trained Inception model:</li>
</ol>
<pre style="padding-left: 60px">from keras.applications import inception_v3<br/>from keras.applications.inception_v3 import preprocess_input<br/>from keras.utils.vis_utils import plot_model<br/>inception_model = inception_v3.InceptionV3(include_top=False, weights='imagenet',input_shape=(300,300,3))</pre>
<p style="padding-left: 60px">Note that we would need an input image that is at least 300 x 300 in shape for the inception v3 pre-trained model to work.</p>
<ol start="2">
<li>Create the input and output datasets <span>(we'll continue from step 3 in Scenario 1 of the <em>Gender classification using CNNs</em> recipe)</span>:</li>
</ol>
<pre style="padding-left: 60px">import cv2<br/>x2 = []<br/>for i in range(len(x)):<br/>    img = x[i]<br/>    img = preprocess_input(img.reshape(1,300,300,3))<br/>    img_new = inception_model.predict(img.reshape(1,300,300,3))<br/>    x2.append(img_new)</pre>
<ol start="3">
<li>Create the input and output arrays, along with the training and test datasets:</li>
</ol>
<pre style="padding-left: 60px">x2 = np.array(x2)<br/>x2= x2.reshape(x2.shape[0],x2.shape[2],x2.shape[3],x2.shape[4])<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(x2,Y, test_size=0.1, random_state=42)</pre>
<ol start="4">
<li>Build and compile the model:</li>
</ol>
<pre style="padding-left: 60px">model_inception_v3 = Sequential()<br/>model_inception_v3.add(Conv2D(512, kernel_size=(3, 3), activation='relu',input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3])))<br/>model_inception_v3.add(MaxPooling2D(pool_size=(2, 2)))<br/>model_inception_v3.add(Flatten())<br/>model_inception_v3.add(Dense(512, activation='relu'))<br/>model_inception_v3.add(Dropout(0.5))<br/>model_inception_v3.add(Dense(1, activation='sigmoid'))<br/>model_inception_v3.summary()<br/><br/>model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])</pre>
<p>                 The preceding model can be visualized as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1303 image-border" src="Images/e6af6840-948a-4ae5-81f1-54292efc2e7e.png" style="width:30.75em;height:18.25em;" width="507" height="300"/></p>
<ol start="5">
<li>Fit the model while scaling the input data:</li>
</ol>
<pre style="padding-left: 60px">history_inception_v3 = model_inception_v3.fit(X_train/np.max(X_train), y_train, batch_size=16,epochs=10,verbose=1,validation_data = (X_test/np.max(X_train), y_test))<span> </span></pre>
<p style="padding-left: 60px">The variation of accuracy and loss values is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1305 image-border" src="Images/24d5c932-1897-4540-af2c-c99ea772fbd1.png" style="width:29.67em;height:22.92em;" width="402" height="311"/></p>
<p>You should notice that the accuracy in this scenario too is also ~90%.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gender classification of the person in image using the ResNet 50 architecture-based model</h1>
                </header>
            
            <article>
                
<p>From VGG16 to VGG19, we have increased the number of layers and generally, the deeper the neural network, the better its accuracy. However, if merely increasing the number of layers is the trick, then we could keep on adding more layers (while taking care to avoid over-fitting) to the model to get a more accurate results.</p>
<p>Unfortunately, that does not turn out to be true and the issue of the vanishing gradient comes into the picture. As the number of layers increases, the gradient becomes so small as it traverses the network that it becomes hard to adjust the weights, and the network performance deteriorates.</p>
<p>ResNet comes into the picture to address this specific scenario.</p>
<p>Imagine a scenario where a convolution layer does nothing but pass the output of the previous layer to the next layer if the model has nothing to learn. However, if the model has to learn a few other features, the convolution layer takes the previous layer's output as input and learns the additional features that need to be learnt to perform classification.</p>
<p>The term residual is the additional feature that the model is expected to learn from one layer to the next layers.</p>
<p>A typical ResNet architecture looks as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/4e8186c6-17ab-4d77-9351-bca12d65e3f5.jpg" style="width:74.08em;height:34.08em;" width="1067" height="491"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Source: https://arxiv.org/pdf/1512.03385.pdf</div>
<p>Note that we have skip connections that are connecting a previous layer to a layer down the line, along with the traditional convolution layers in this network.</p>
<p>Furthermore, the 50 in ResNet50 comes from the fact that we have a total of 50 layers in the network.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The ResNet50 architecture is built as follows <span>(please refer to</span> <kbd>Transfer_learning.ipynb</kbd> f<span>ile in GitHub while implementing the code)</span>:</p>
<ol>
<li>Download the pre-trained inception model:</li>
</ol>
<pre style="padding-left: 60px">from keras.applications import resnet50<br/>from keras.applications.resnet50 import preprocess_input<br/>resnet50_model = resnet50.ResNet50(include_top=False, weights='imagenet',input_shape=(300,300,3))</pre>
<p style="padding-left: 60px">Note that we need an input image that is at least 224 x 224 in shape for the ResNet50 pre-trained model to work.</p>
<ol start="2">
<li><span>Create the input and output datasets </span><span>(we'll continue from <em>step 3</em> in Scenario 1 of the <em>Gender classification using CNNs</em> recipe)</span>:</li>
</ol>
<pre style="padding-left: 60px">import cv2<br/>x2 = []<br/>for i in range(len(x)):<br/>    img = x[i]<br/>    img = preprocess_input(img.reshape(1,300,300,3))<br/>    img_new = resnet50_model.predict(img.reshape(1,300,300,3))<br/>    x2.append(img_new)</pre>
<ol start="3">
<li>Create the input and output arrays, along with the training and test datasets:</li>
</ol>
<pre style="padding-left: 60px">x2 = np.array(x2)<br/>x2= x2.reshape(x2.shape[0],x2.shape[2],x2.shape[3],x2.shape[4])<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(x2,Y, test_size=0.1, random_state=42)</pre>
<ol start="4">
<li>Build and compile the model:</li>
</ol>
<pre style="padding-left: 60px">model_resnet50 = Sequential()<br/>model_resnet50.add(Conv2D(512, kernel_size=(3, 3), activation='relu',input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3])))<br/>model_resnet50.add(MaxPooling2D(pool_size=(2, 2)))<br/>model_resnet50.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))<br/>model_resnet50.add(MaxPooling2D(pool_size=(2, 2)))<br/>model_resnet50.add(Flatten())<br/>model_resnet50.add(Dense(512, activation='relu'))<br/>model_resnet50.add(Dropout(0.5))<br/>model_resnet50.add(Dense(1, activation='sigmoid'))<br/>model_resnet50.summary()<br/><br/>model_resnet50.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])</pre>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">A summary of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1308 image-border" src="Images/a10c5097-edd5-4d36-ac78-482437730971.png" style="width:37.42em;height:27.33em;" width="507" height="371"/></p>
<ol start="5">
<li>Fit the model while scaling the input data:</li>
</ol>
<pre style="padding-left: 60px">history_resnet50 = model_resnet50.fit(X_train/np.max(X_train), y_train, batch_size=32,epochs=10,verbose=1,validation_data = (X_test/np.max(X_train), y_test))</pre>
<p style="padding-left: 60px">The variation of the accuracy and loss values is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1309 image-border" src="Images/69d8c163-2be3-4ace-90ef-5e3bcb0fb8b6.png" style="width:35.17em;height:28.42em;" width="411" height="332"/></p>
<p style="padding-left: 60px">Note that the preceding model gives an <span><span>accuracy of 92%</span></span>.</p>
<div class="packt_infobox">There is no considerable difference in the accuracy levels of multiple pre-trained models on gender classification, as potentially they were trained to extract the general features, but not necessarily the features to classify gender.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Detecting the key points within image of a face</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will learn about detecting the key points of a human face, which are the boundaries of the left and right eyes, the nose, and the four coordinates of the mouth.</p>
<p>Here are two sample pictures with the key points:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1310 image-border" src="Images/a1d5652b-4dc0-4a50-81d8-212f94cd0d87.png" style="width:27.42em;height:21.42em;" width="329" height="257"/></p>
<p>Note that the key points that we are expected to detect are plotted as dots in this picture. A total of 68 key points are detected on the image of face, where the key points of the face include - <span>Mouth, right eyebrow, left eyebrow, right eye, left eye, nose, jaw.</span></p>
<p>In this case study, we will leverage the VGG16 transfer learning technique that we learned in the <em>Gender classification in image using the VGG16 architecture-based model</em> section to detect the key points on the face.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>For the key-point detection task, we will work on a dataset where we annotate the points that we want to detect. For this exercise, the input will be the image on which we want to detect the key points and the output will be the <em>x</em> and <em>y</em> coordinates of the key points. The dataset can be downloaded from here: <a href="https://github.com/udacity/P1_Facial_Keypoints" target="_blank">https://github.com/udacity/P1_Facial_Keypoints</a>.</p>
<p>The steps we'll follow are as follows:</p>
<ol>
<li>Download the dataset </li>
<li>Resize the images to a standard shape
<ol>
<li>While resizing the images, ensure that the key points are modified so that they represent the modified (resized) image</li>
</ol>
</li>
</ol>
<ol start="3">
<li>Pass the resized images through VGG16 model</li>
<li>Create input and output arrays, where the input array is the output of passing image through VGG16 model and the output array is the modified facial key point locations</li>
<li>Fit a model that minimizes the absolute error value of the difference between predicted and actual facial key points</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The strategy that we discussed is coded as follows (Please refer to <kbd>Facial_keypoints.ipynb</kbd> file in GitHub while implementing the code):</p>
<ol>
<li>Download and import the dataset:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ git clone https://github.com/udacity/P1_Facial_Keypoints.git<br/></strong>import pandas as pd<strong><br/></strong>data = pd.read_csv('/content/P1_Facial_Keypoints/data/training_frames_keypoints.csv')<strong><br/></strong></pre>
<p style="padding-left: 60px">Inspect this dataset. </p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1311 image-border" src="Images/fa5e1b6c-254b-4440-88d1-6829589e3257.png" style="width:58.92em;height:17.83em;" width="707" height="214"/></p>
<p style="padding-left: 60px">There are a total of 137 columns of which the first column is the name of image and the rest of 136 columns represent the x and y co-ordinate values of the 68 key points of face of the corresponding image.</p>
<ol start="2">
<li>Preprocess the dataset to extract the image, resized image, VGG16 features of image, modified key point locations as the output:</li>
</ol>
<p style="padding-left: 60px">Initialize lists that will be appended to create input and output arrays:</p>
<pre>import cv2, numpy as np<br/>from copy import deepcopy<br/>x=[]<br/>x_img = []<br/>y=[]</pre>
<p style="padding-left: 60px">Loop through the images and read them:</p>
<pre>for i in range(data.shape[0]):<br/>     img_path = '/content/P1_Facial_Keypoints/data/training/' + data.iloc[i,0]<br/>     img = cv2.imread(img_path)</pre>
<p style="padding-left: 60px">Capture the key point values and store them</p>
<pre style="padding-left: 60px"> kp = deepcopy(data.iloc[i,1:].tolist())<br/> kp_x = (np.array(kp[0::2])/img.shape[1]).tolist()<br/> kp_y = (np.array(kp[1::2])/img.shape[0]).tolist()<br/> kp2 = kp_x + kp_y</pre>
<p style="padding-left: 60px">Resize the images</p>
<pre style="padding-left: 60px">img = cv2.resize(img,(224,224))</pre>
<p style="padding-left: 60px">Preprocess the image so that it can be passed through VGG16 model and extract features:</p>
<pre style="padding-left: 60px">preprocess_img = preprocess_input(img.reshape(1,224,224,3))<br/> vgg16_img = vgg16_model.predict(preprocess_img)</pre>
<p style="padding-left: 60px">Append the input and output values to corresponding lists:</p>
<pre style="padding-left: 60px"> x_img.append(img)<br/> x.append(vgg16_img)<br/> y.append(kp2)</pre>
<p style="padding-left: 60px">Create input and output arrays:</p>
<pre style="padding-left: 60px">x = np.array(x)<br/>x = x.reshape(x.shape[0],7,7,512)<br/>y = np.array(y)</pre>
<ol start="3">
<li>Build and compile a model</li>
</ol>
<pre style="padding-left: 60px">from keras.models import Sequential<br/>from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout<br/>model_vgg16 = Sequential()<br/>model_vgg16.add(Conv2D(512, kernel_size=(3, 3), activation='relu',input_shape=(x.shape[1],x.shape[2],x.shape[3])))<br/>model_vgg16.add(MaxPooling2D(pool_size=(2, 2)))<br/>model_vgg16.add(Flatten())<br/>model_vgg16.add(Dense(512, activation='relu'))<br/>model_vgg16.add(Dropout(0.5))<br/>model_vgg16.add(Dense(y.shape[1], activation='sigmoid'))<br/>model_vgg16.summary()</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1312 image-border" src="Images/e415b00a-b498-468b-b3ff-023188db2dd0.png" style="width:34.42em;height:20.17em;" width="509" height="298"/></p>
<p style="padding-left: 60px">Compile the model:</p>
<pre style="padding-left: 60px">model_vgg16.compile(loss='mean_absolute_error',optimizer='adam')</pre>
<ol start="4">
<li>Fit the model</li>
</ol>
<pre style="padding-left: 60px">history = model_vgg16.fit(x/np.max(x), y, epochs=10, batch_size=32, verbose=1, validation_split = 0.1)</pre>
<p style="padding-left: 60px">Note that, we are dividing the input array with maximum value of input array so that we scale the input dataset. The variation of training and test loss over increasing epochs is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1313 image-border" src="Images/07513580-4052-4792-9a44-5b2731f5769b.png" style="width:34.25em;height:24.08em;" width="411" height="289"/></p>
<ol start="5">
<li>Predict on a test image. In the following code, we are predicting on the second image from last in input array (note that, as <kbd>validation_split</kbd> is <kbd>0.1</kbd>, second image from last was not supplied to model while training). We are ensuring that we are passing our image through <kbd>preprocess_input</kbd> method and then through <kbd>VGG16_model</kbd> and finally, the scaled version of <kbd>VGG16_model</kbd> output to the <kbd>model_vgg16</kbd> that we built:</li>
</ol>
<pre style="padding-left: 60px">pred = model_vgg16.predict(vgg16_model.predict(preprocess_input(x_img[-2].reshape(1,224,224,3)))/np.max(x))</pre>
<p style="padding-left: 60px">The preceding prediction on test image can be visualized as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1315 image-border" src="Images/cee845d9-96bf-44a9-a7fa-5fa46fbeecda.png" style="width:29.25em;height:13.33em;" width="351" height="160"/></p>
<p>We can see that the key points are detected very accurately on the test image.</p>


            </article>

            
        </section>
    </div>



  </body></html>