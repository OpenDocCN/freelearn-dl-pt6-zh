- en: Chapter 7. Classifying Images with Residual Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter presents state-of-the-art deep networks for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Residual networks have become the latest architecture, with a huge improvement
    in accuracy and greater simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Before residual networks, there had been a long history of architectures, such
    as **AlexNet**, **VGG**, **Inception** (**GoogLeNet**), **Inception v2,v3, and
    v4**. Researchers were searching for different concepts and discovered some underlying
    rules with which to design better architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will address the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Main datasets for image classification evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network architectures for image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global average pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Residual connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochastic depth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense connections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural image datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image classification usually includes a wider range of objects and scenes than
    the MNIST handwritten digits. Most of them are natural images, meaning images
    that a human being would observe in the real world, such as landscapes, indoor
    scenes, roads, mountains, beaches, people, animals, and automobiles, as opposed
    to synthetic images or images generated by a computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the performance of image classification networks for natural images,
    three main datasets are usually used by researchers to compare performance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cifar-10, a dataset of 60,000 small images (32x32) regrouped into 10 classes
    only, which you can easily download:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are some example images for each class:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Natural image datasets](img/00106.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Cifar 10 dataset classes with samples [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Cifar-100, a dataset of 60,000 images, partitioned into 100 classes and 20 super-classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ImageNet, a dataset of 1.2 million images, labeled with a wide range of classes
    (1,000). Since ImageNet is intended for non-commercial use only, it is possible
    to download Food 101, a dataset of 101 classes of meals, and 1,000 images per
    class:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before introducing residual architectures, let us discuss two methods to improve
    classification net accuracy: batch normalization, and global average pooling.'
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deeper networks, with more than 100 layers can help image classification for
    a few hundred classes. The major issue with deep networks is to ensure that the
    flows of inputs, as well as the gradients, are well propagated from one end of
    the network to the other end.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, it is not unusual that nonlinearities in the network get saturated,
    and gradients become null. Moreover, each layer in the network has to adapt to
    constant changes in the distribution of its inputs, a phenomenon known as **internal
    covariate shift**.
  prefs: []
  type: TYPE_NORMAL
- en: It is known that a network trains faster with input data linearly processed
    to have zero mean and unit variance (known as **network input normalization**),
    and normalizing each input feature independently, instead of jointly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To normalize the input of every layer in a network, it is a bit more complicated:
    zeroing the mean of the input will ignore the learned bias of the previous layer,
    and the problem is even worse with unit variance. The parameters of the previous
    layer may grow infinitely, while the loss stays constant, when inputs of the layer
    are normalized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for **layer input normalization**, a **batch normalization layer** relearns
    the scale and the bias after normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Batch normalization](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Rather than using the entire dataset, it uses the batch to compute the statistics
    for normalization, with a moving average to get closer to entire dataset statistics
    while training.
  prefs: []
  type: TYPE_NORMAL
- en: 'A batch normalization layer has the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: It reduces the influence of bad initializations or too high learning rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It increases the accuracy of the net by a margin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It accelerates the training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It reduces overfitting, regularizing the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When introducing batch normalization layers, you can remove dropout, increase
    the learning rate, and reduce L2 weight normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Be careful to place nonlinearity after the BN layer, and to remove bias in
    the previous layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Global average pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditionally, the two last layers of a classification net are a fully connected
    layer and a softmax layer. The fully connected layer outputs a number of features
    equal to the number of classes, and the softmax layer normalizes these values
    to probabilities that sum to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, it is possible to replace max-pooling layers of stride 2 with new
    convolutional layers of stride 2: all-convolutional networks perform even better.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, removing the fully connected layer is also possible. If the number
    of featuremaps output by the last convolutional layer is chosen equal to the number
    of classes, a global spatial average reduces each featuremap to a scalar value,
    representing the score for the class averaged at the different *macro* spatial
    locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Global average pooling](img/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Residual connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While very deep architectures (with many layers) perform better, they are harder
    to train, because the input signal decreases through the layers. Some have tried
    training the deep networks in multiple stages.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative to this layer-wise training is to add a supplementary connection
    to shortcut a block of layers, named the **identity connection**, passing the
    signal without modification, in addition to the classic convolutional layers,
    named the **residuals**, forming a **residual block**, as shown in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Residual connections](img/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Such a residual block is composed of six layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'A residual network is a network composed of multiple residual blocks. Input
    is processed by a first convolution, followed by batch normalization and non-linearity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Residual connections](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, for a residual net composed of two residual blocks, and eight
    featuremaps in the first convolution on an input image of size *28x28*, the layer
    output shapes will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of output featuremaps increase while the size of each output featuremap
    decreases: such a technique in funnel of **decreasing featuremap sizes/increasing
    the number of dimensions keeps the number** of parameters per layer constant which
    is a common best practice for building networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three transitions to increase the number of dimensions occur, one before the
    first residual block, a second one after n residual blocks, and a third one after
    *2xn* residual blocks. Between each transition, the number of filters are defined
    in an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The dimensional increase is performed by the first layer of the corresponding
    residual block. Since the input is not the same shape as the output, the simple
    identity connection cannot be concatenated with the output of layers of the block,
    and is replaced by a dimensional projection to reduce the size of the output to
    the dimension of the block output. Such a projection can be done with a convolution
    of kernel *1x1* with a stride of `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Some variants of residual blocks have been invented as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'A wide version (Wide-ResNet) of the previous residual block simply consists
    of increasing the number of outputs per residual blocks by a factor as they come
    to the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'A bottleneck version consists of reducing the number of parameters per layer,
    to create a bottleneck that has the effect of dimension reduction, implementing
    the Hebbian theory *Neurons that fire together wire together*, and to help residual
    blocks capture particular types of pattern in the signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Residual connections](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Bottlenecks are reductions in both featuremap size and number of output at
    the same time, not keeping the number of parameters constant per layer as in the
    previous practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the full network of three stacks of residual blocks is built with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The command for a MNIST training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This gives a top-1 accuracy of 98%.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Cifar 10, residual networks with more than a 100 layers require the batch
    size to be reduced to 64 to fit into the GPU''s memory:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For ResNet-110 (6 x 18 + 2):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'ResNet-164 (6 x 27 + 2):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Wide ResNet-110:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With ResNet-bottleneck-164:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For Food-101, I reduce further the batch size for ResNet 110:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Stochastic depth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the propagation of the signal through the layers might be prone to errors
    in any of the residual blocks, the idea of stochastic depth is to train the network
    to robustness by randomly removing some of the residual blocks, and replacing
    them with an identity connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the training is much faster, since the number of parameters is lower.
    Second,in practice, the robustness is proven and it provides better classification
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic depth](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Dense connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Stochastic depth skips some random layers by creating a direct connection.
    Going one step further, instead of removing some random layers, another way to
    do the same thing is to add an identity connection with previous layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dense connections](img/00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A dense block (densely connected convolutional networks)
  prefs: []
  type: TYPE_NORMAL
- en: 'As for residual blocks, a densely connected convolutional network consists
    of repeating dense blocks to create a stack of layer blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dense connections](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A network with dense blocks (densely connected convolutional networks)
  prefs: []
  type: TYPE_NORMAL
- en: 'Such an architecture choice follows the same principles as those seen in [Chapter
    10](part0096_split_000.html#2RHM01-ccdadb29edc54339afcb9bdf9350ba6b "Chapter 10. Predicting
    Times Sequences with Advanced RNN"), *Predicting Times Sequence with Advanced
    RNN*, with highway networks: the identity connection helps the information to
    be correctly propagated and back-propagated through the network, reducing the
    effect of *exploding/vanishing gradients* when the number of layers is high.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, we replace our residual block with a densely connected block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Note also that batch normalization is done feature by feature and, since the
    output of every block is already normalized, a second renormalization is not necessary.
    Replacing the batch normalization layer by a simple affine layer learning the
    scale and bias on the concated normalized features is sufficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'For training DenseNet-40:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Multi-GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cifar and MNIST images are still small, below 35x35 pixels. Training on natural
    images requires the preservation of details in the images. So, for example, a
    good input size is 224x224, which is 40 times more. When image classification
    nets with such input size have a few hundred layers, GPU memory limits the batch
    size to a dozen images and so training a batch takes a long time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work in multi-GPU mode:'
  prefs: []
  type: TYPE_NORMAL
- en: The model parameters are in a shared variable, meaning shared between CPU /
    GPU 1 / GPU 2 / GPU 3 / GPU 4, as in single GPU mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The batch is divided into four splits, and each split is sent to a different
    GPU for the computation. The network output is computed on the split, and the
    gradients retro-propagated to each weight. The GPU returns the gradient values
    for each weight.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gradients for each weight are fetched back from the multiple GPU to the
    CPU and stacked together. The stacked gradients represent the gradient of the
    full initial batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The update rule applies to the batch gradients and updates the shared model
    weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'See the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-GPU](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Theano stable version supports only one GPU per process, so use the first GPU
    in your main program and launch sub-processes for each GPU to train on. Note that
    the cycle described in the preceding image requires the synchronization of the
    update of the model to avoid each GPU training on unsynchronized models. Instead
    of reprogramming it yourself, a Platoon ([https://github.com/mila-udem/platoon](https://github.com/mila-udem/platoon))
    framework is dedicated to train your models across multiple GPUs inside one node.
  prefs: []
  type: TYPE_NORMAL
- en: Note, too, that it would also be more accurate to synchronize the batch normalization
    mean and variance across multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data augmentation is a very important technique to improve classification accuracy.
    Data augmentation consists of creating new samples from existing samples, by adding
    some jitters such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Random scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random sized crop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal flip
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random rotation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lighting noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brightness jittering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saturation jittering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contrast jittering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will help the model to be more robust to different lighting conditions
    that are very common in real life.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of always seeing the same dataset, the model discovers different samples
    at each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Note that input normalization is also important to get better results.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following titles for further insights:'
  prefs: []
  type: TYPE_NORMAL
- en: Densely Connected Convolutional Networks, by Gao Huang, Zhuang Liu, Kilian Q.
    Weinberger, and Laurens van der Maaten, Dec 2016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code has been inspired by the Lasagne repository:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/Lasagne/Recipes/blob/master/papers/deep_residual_learning/Deep_Residual_Learning_CIFAR-10.py](https://github.com/Lasagne/Recipes/blob/master/papers/deep_residual_learning/Deep_Residual_Learning_CIFAR-10.py)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/Lasagne/Recipes/tree/master/papers/densenet](https://github.com/Lasagne/Recipes/tree/master/papers/densenet)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning,
    Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex Alemi, 2016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Residual Learning for Image Recognition, Kaiming He, Xiangyu Zhang, and
    Shaoqing Ren, Jian Sun 2015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rethinking the Inception Architecture for Computer Vision, Christian Szegedy,
    Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna, 2015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wide Residual Networks, Sergey Zagoruyko, and Nikos Komodakis, 2016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identity Mappings in Deep Residual Networks, Kaiming He, Xiangyu Zhang, Shaoqing
    Ren, and Jian Sun, Jul 2016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network In Network, Min Lin, Qiang Chen, Shuicheng Yan, 2013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: New techniques have been presented to achieve state-of-the-art classification
    results, such as batch normalization, global average pooling, residual connections,
    and dense blocks.
  prefs: []
  type: TYPE_NORMAL
- en: These techniques have led to the building residual networks, and densely connected
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: The use of multiple GPUs helps training image classification networks, which
    have numerous convolutional layers, large reception fields, and for which the
    batched inputs of images are heavy in memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we looked at how data augmentation techniques will enable an increase
    of the size of the dataset, reducing the potential of model overfitting, and learning
    weights for more robust networks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll see how to use the early layers of these networks
    as features to build encoder networks, as well as how to reverse the convolutions
    to reconstruct an output image to perform pixel-wise predictions.
  prefs: []
  type: TYPE_NORMAL
