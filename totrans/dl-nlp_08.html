<html><head></head><body>
		<div class="Content" id="_idContainer239">
			<h1 id="_idParaDest-173"><em class="italics"><a id="_idTextAnchor195"/>Chapter 8</em></h1>
		</div>
		<div class="Content" id="_idContainer240">
			<h1 id="_idParaDest-174"><a id="_idTextAnchor196"/>State-of-the-Art Natural Language Processing</h1>
		</div>
		<div class="Content" id="_idContainer241">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Evaluate vanishing gradients in long sentences</li>
				<li class="bullets">Describe an attention mechanism model as a state-of-the-art NLP domain</li>
				<li class="bullets">Assess one specific attention mechanism architecture</li>
				<li class="bullets">Develop a neural machine translation model using an attention mechanism</li>
				<li class="bullets">Develop a text summarization model using an attention mechanism</li>
			</ul>
			<p>This chapter aims to acquaint you with the current practices and technologies in the NLP domain.</p>
		</div>
		<div class="Content" id="_idContainer259">
			<h2 id="_idParaDest-175"><a id="_idTextAnchor197"/>Introduction</h2>
			<p>In the last chapter, we studied Long Short Term Memory units (LSTMs), which help combat the vanishing gradient problem. We also studied GRU in detail, which has its own way of handling vanishing gradients. Although LSTM and GRU reduce this problem in comparison to simple recurrent neural networks, the vanishing gradient problem still manages to prevail in many practical cases. The issue essentially remains the same: longer sentences with complex structural dependences are challenging for deep learning algorithms to encapsulate. Therefore, one of the most prevalent research areas represents the community's attempts to mitigate the effects of the vanishing gradient problem.</p>
			<p>Attention mechanisms, in the last few years, have attempted to provide a solution to the vanishing gradient problem. The basic concept of an attention mechanism relies on having access to all parts of the input sentence when arriving at an output. This allows the model to lay varying amounts of weight (attention) to different parts of the sentence, which allows dependencies to be deduced. Due to their uncanny ability to learn such dependencies, attention mechanism-based architectures represent the current state of the art in the NLP domain.</p>
			<p>In this chapter, we will learn about attention mechanisms and solve a neural machine translation task using a specific architecture based on an attention mechanism. We will also mention some other related architectures that are being used in the industry today.</p>
			<h3 id="_idParaDest-176"><a id="_idTextAnchor198"/>Attention Mechanisms</h3>
			<p>In the last chapter, we solved a <em class="italics">Neural Language Translation</em> task. The architecture for the translation model adopted by us consists of two parts: <em class="italics">Encoder and Decoder</em>. Refer to the following diagram for the architecture:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer242">
					<img alt="Figure 8.1: Neural language translation model" src="image/C13783_08_01.jpg"/>
				</div>
			</div>
			<h6>Figure 8.1: Neural language translation model</h6>
			<p>For a neural machine translation task, a sentence is passed into an encoder word by word, which produces a single <em class="italics">thought</em> vector (represented in the preceding image as '<strong class="bold">S</strong>'), which embeds the meaning of the entire sentence into a single representation. The decoder then uses this vector to initialize the hidden states and produce a translation word by word.</p>
			<p>In the simple encoder-decoder regime, only 1 vector (the thought vector) contains the representation of the entire sentence. The longer the sentence, the more difficult it becomes for the single thought vector to retain long-term dependencies. The use of LSTM units reduces the problem only to some extent. A new concept was developed to mitigate the vanishing gradient problem further, and this concept is called <strong class="keyword">Attention mechanisms</strong>.</p>
			<p>An attention mechanism aims to mimic a human's way of learning dependencies. Let's illustrate this with an example sentence:</p>
			<p>"There have been many incidents of thefts lately in our neighborhood, which has forced me to consider hiring a security agency to install a burglar-detection system in my house so that I can keep myself and my family safe."</p>
			<p>Note the use of the words 'my', 'I', 'me', 'myself,' and 'our'. These occur at distant positions within the sentence but are tightly coupled to each other to represent the meaning of the sentence.</p>
			<p>When trying to translate the previous sentence, a traditional encoder-decoder functions as follows:</p>
			<ol>
				<li>Pass the sentence word by word to the encoder.</li>
				<li>The encoder produces a single thought vector, which represents the entire sentence encoding. For a long sentence, such as the previous one, even with the use of LSTMs, it would be difficult for the encoder to embed all the dependencies. Therefore, the earlier part of the sentence is not as strongly encoded as the later part of the sentence, which means the later part of the sentence ends up having a dominant influence over the encodings.</li>
				<li>The decoder uses the thought vector to initialize the hidden state vector to generate the output translation.</li>
			</ol>
			<p>A more intuitive way to translate the sentence would be to pay attention to the correct positions of words in the input sentence when determining a particular word in the target language. As an example, consider the following sentence:</p>
			<p>'<em class="italics">The animal could not walk on the street because it was badly injured.</em>'</p>
			<p>In this sentence, whom does the word 'it' refer to? Is it the animal or the street? An answer to this question would be possible if the entire sentence were considered together and different parts of the sentence were weighed differently to determine the answer to the question. An attention mechanism accomplishes this, as depicted here:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer243">
					<img alt="Figure 8.2: An example of an attention mechanism" src="image/C13783_08_02.jpg"/>
				</div>
			</div>
			<h6>Figure 8.2: An example of an attention mechanism</h6>
			<p>The diagram shows how much weight each word receives in understanding every word in a sentence. As can be seen, the word '<strong class="bold">it_</strong>' receives a very strong weighting from '<strong class="bold">animal_</strong>' and a relatively weaker weighting from '<strong class="bold">street_</strong>'. Thus, the model can now answer the question of which entity 'it' refers to in the sentence.</p>
			<p>For a translation encoder-decoder model, while generating word-by-word output, at a given point in time, not all the words in the input sentence are important for the determination of the output word. An attention mechanism implements a scheme that does exactly that: weighs different parts of the input sentence with all of the input words at each point in the determination of the output. A well-trained network with an attention mechanism would learn to apply an appropriate amount of weighting to different parts of the sentence. This regime allows the entire part of the input sentence to be always available for use at every point of determining the output. Thus, instead of one thought vector, the decoder has access to the "thought" vector specific for the determination of each word in the output sentence. This ability of an attention mechanism is in stark contrast to a traditional LSTM/GRU/RNN-based encoder-decoder.</p>
			<p>An attention mechanism is a general concept. It can be realized in several architectural flavors, which are discussed in the later part of the chapter.</p>
			<h3 id="_idParaDest-177"><a id="_idTextAnchor199"/>An Attention Mechanism Model</h3>
			<p>Let's see how an encoder-decoder architecture could look with an attention mechanism in place:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer244">
					<img alt="Figure 8.3: An attention mechanism model" src="image/C13783_08_03.jpg"/>
				</div>
			</div>
			<h6>Figure 8.3: An attention mechanism model</h6>
			<p>The preceding diagram depicts the training phase of a language translation model with an attention mechanism. We can note a few differences compared to a basic encoder-decoder regime, as follows:</p>
			<ul>
				<li>The initial states of the decoder get initialized with the encoder output state from the last encoder cell. An initial <strong class="bold">NULL</strong> word is used to start the translation, and the first word is produced as '<strong class="bold">Er</strong>'. This is the same as the previous encoder-decoder model.</li>
				<li>For the second word, in addition to the input from the previous word and the hidden state of the preceding decoder timestep, another vector is fed as input to the cell. This vector, generally regarded as '<strong class="bold">Context vector</strong>', is a function of all the encoder hidden states. From the preceding diagram, it is a weighted summation of the hidden states of the encoder for all the timesteps.</li>
				<li>During the training phase, since the output of each decoder timestep is known, we can learn all the parameters of the network. In addition to the usual parameters, corresponding to whichever RNN flavor is being used, the parameters specific to the attention function are also learned. If the attention function is just a simple summation of the hidden state encoder vectors, the weights of the hidden states at each encoder timestep can be learned.</li>
				<li>At inference time, at every timestep, the decoder cell can take as input the predicted word from the last timestep, the hidden states from the previous decoder cell, and the context vector.</li>
			</ul>
			<p>Let's look at one specific realization of an attention mechanism for neural machine translation. In the previous chapter, we built a neural language translation model, which is a subproblem area of a more general area of NLP called neural machine translation. In the following section, we attempt to solve a date-normalization problem.</p>
			<h3 id="_idParaDest-178"><a id="_idTextAnchor200"/>Data Normalization Using an Attention Mechanism</h3>
			<p>Let's say you're maintaining a database that has a table containing a column for date. The input for the date is taken from your customers, who fill in a form and enter the date in a <strong class="bold">date</strong> field. The frontend engineer somehow forgot to enforce a scheme upon the field, such that only dates in a "YYYY-MM-DD" format are accepted. You are now tasked with normalizing the <strong class="bold">date</strong> column of database table, such that the user inputs in several formats get converted to a standard "YYYY-MM-DD" format.</p>
			<p>As an example, the user inputs for date and the corresponding correct normalization are shown here:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer245">
					<img alt="Figure 8.4: Table for date normalization" src="image/C13783_08_04.jpg"/>
				</div>
			</div>
			<h6>Figure 8.4: Table for date normalization</h6>
			<p>You can see that there is a lot of variation in the way a user can input a date. There are many more ways in which the date could be specified apart from the examples in the table.</p>
			<p>This problem is a good candidate to be solved by a neural machine translation model as the input has a sequential structure, wherein the meanings of the different components in the input need to be learned. This model will have the following components:</p>
			<ul>
				<li>Encoder</li>
				<li>Decoder</li>
				<li>Attention mechanisms</li>
			</ul>
			<h3 id="_idParaDest-179"><a id="_idTextAnchor201"/>Encoder</h3>
			<p>This is a bidirectional LSTM that takes each character of the date as input. Thus, at each timestep, the input to the encoder is a single character of the input date. Apart from this, the hidden state and memory state is also taken as an input from the previous encoder cell. Since this is a bidirectional architecture, there are two sets of parameters pertaining to the LSTM: one in the forward direction and the other in the backward direction.</p>
			<h3 id="_idParaDest-180"><a id="_idTextAnchor202"/>Decoder</h3>
			<p>This is a unidirectional LSTM. It takes as input the context vector for this timestep. Since each output character is not strictly dependent upon the last output character in the case of date normalization, we don't need to feed the previous timestep output as an input to the current timestep. Additionally, since it is an LSTM unit, the hidden states and memory state from the previous decoder timestep are also fed to the current timestep unit for the determination of the decoder output at this timestep.</p>
			<h3 id="_idParaDest-181"><a id="_idTextAnchor203"/>Attention mechanisms</h3>
			<p>Attention mechanisms are explained in this section. For determination of a decoder input at a given timestep, a context vector is calculated. A context vector is a weighted summation of all the hidden state of an encoder from all timesteps. This is as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer246">
					<img alt="Figure 8.5: Expression for the context vector" src="image/C13783_08_05.jpg"/>
				</div>
			</div>
			<h6>Figure 8.5: Expression for the context vector</h6>
			<p>The dot operation is a dot product operation that multiplies weights (represented by <strong class="bold">alpha</strong>) with the corresponding hidden state vector for all timesteps and sums them up. The value of the alpha vector is calculated separately for each decoder output timestep. The alphas encapsulate the essence of an attention mechanism, that is, determining how much 'attention' to be given to which part of the input to figure out the current timestep output. This can be realized in a diagram, as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer247">
					<img alt="Figure 8.6: Determination of attention to inputs" src="image/C13783_08_06.jpg"/>
				</div>
			</div>
			<h6>Figure 8.6: Determination of attention to inputs</h6>
			<p>As an example, let's say that the encoder input has a fixed length of 30 characters, and the decoder output has a fixed output length of 10 characters. For the date normalization problem, this means that the user input is fixed to be a maximum of 30 characters, while the model output is fixed at 10 characters (the number of characters in the YYYY-MM-DD format, including the hyphens).</p>
			<p>Let's say that we wish to determine the decoder output at the output timestep=4 (an arbitrary number chosen to explain the concept; it just needs to be &lt;=10, which is the output timestep count). At this step, the weight vector alpha is computed. This vector has a dimensionality equal to the number of timesteps of the encoder input (as a weight needs to be computed for every encoder input timestep). So, in our case, alpha has a dimensionality of 30.</p>
			<p>Now, we already have the hidden state vector from each of the encoder timesteps, so there are a total of 30 hidden state vectors available. The dimensionality of the hidden state vector accounts for both the forward and backward components of the bidirectional encoder LSTM. For a given timestep, we combine the forward hidden state and backward hidden state into a single vector. So, if the dimensionality of forward and backward hidden states is 32 each, we put them in a single vector of 64 dimensions as [<strong class="bold">h_forward</strong>, <strong class="bold">h_backward</strong>]. This is a simple concatenation function. Let's call this the encoder hidden state vector.</p>
			<p>We now have a single 30-dimensional weight vector alpha, and 30 vectors of 64-dimensional hidden states. So, we can now multiply each of the 30 hidden state vectors with a corresponding entry in the alpha vector. Furthermore, we can sum up these scaled representations of hidden states to receive a single 64-dimensional context vector. This is essentially the operation performed by the dot operator.</p>
			<h3 id="_idParaDest-182"><a id="_idTextAnchor204"/>The Calculation of Alpha</h3>
			<p>The weights can be modeled by a multilayer perceptron (MLP), which is a simple neural network consisting of multiple hidden layers. We choose to have two dense layers with a <strong class="bold">softmax</strong> output. The number of dense layers and units can be treated as hyperparameters. The input to this MLP consists of two components: these are the hidden state vectors for all timesteps from the encoder bidirectional LSTM, as explained in the last point, and the hidden states from the previous timestep of the decoder. These are concatenated to form a single vector. So, the input to the MLP is: [<em class="italics">encoder hidden state vector</em>, <em class="italics">previous state vector from decoder</em>]. This is a concatenation operation of tensors: [<strong class="bold">H</strong>, <strong class="bold">S_prev</strong>]. <strong class="bold">S_prev</strong> refers to the decoder's hidden state output from the previous timestep. If the dimensionality of <strong class="bold">S_prev</strong> is 64 (denoting a hidden state dimensionality of 64 for the decoder LSTM) and the dimensionality of the encoder's hidden state vector is 64 (from the last point), a concatenation of these two vectors produces a vector of size 128.</p>
			<p>Thus, the MLP receives a 128-dimension input for a single encoder timestep. As we have fixed the encoder input length to 30 characters, we will have a matrix (more accurately, a tensor) of size [30, 128]. The parameters of this MLP are learned using the same BPTT regime that is used to learn all the other parameters of the model. So, all the parameters of the entire model (encoder + decoder + attention function MLP) are learned together. This can be seen in the following diagram:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer248">
					<img alt="Figure 8.7: The calculation of alpha" src="image/C13783_08_07.jpg"/>
				</div>
			</div>
			<h6>Figure 8.7: The calculation of alpha</h6>
			<p>In the previous step, we learned the weights (alpha vector) for determining only one step of the decoder output (we had assumed this timestep to be 4 in an earlier point). So, the determination of a single step decoder output requires the inputs: <strong class="bold">S_prev</strong> and encoder hidden states for calculating the context vector, decoder hidden states, and decoder previous timestep memory, which goes as input to the decoder unidirectional LSTM. Proceeding to the next decoder timestep requires a calculation of a new alpha vector since, for this next step, various parts of the input sequence will most likely be weighted differently compared to the previous timestep.</p>
			<p>Due to the architecture of the model, the training and inference steps are the same. The only difference is that, during training, we know the output for each decoder timestep and use that to train the model parameters (this technique is referred to as 'Teacher Forcing'). </p>
			<p>In contrast, during inference time, we predict the output character. Note that both during training and inference, we do not feed the previous timestep decoder output character as input to the current timestep decoder cell. It should be noted that the architecture proposed here is specific to this problem. There are a lot of architectures and ways to define an attention function. We will take a brief look at some of these in later sections of the chapter.</p>
			<h3 id="_idParaDest-183"><a id="_idTextAnchor205"/>Exercise 28: Build a Date Normalization Model for a Database Column</h3>
			<p>A database column accepts date inputs from various users in multiple formats. In this exercise, we aim to normalize the date column of the database table such that the user inputs in several formats get converted to a standard "YYYY-MM-DD" format:</p>
			<h4>Note</h4>
			<p class="callout">The Python requirements for running the code are as follows:</p>
			<p class="callout">Babel==2.6.0</p>
			<p class="callout">Faker==1.0.2</p>
			<p class="callout">Keras==2.2.4</p>
			<p class="callout">numpy==1.16.1</p>
			<p class="callout">pandas==0.24.1</p>
			<p class="callout">scipy==1.2.1</p>
			<p class="callout">tensorflow==1.12.0</p>
			<p class="callout">tqdm==4.31.1</p>
			<p class="callout">Faker==1.0.2</p>
			<ol>
				<li value="1">We import all the necessary modules:<p class="snippet">from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply</p><p class="snippet">from keras.layers import RepeatVector, Dense, Activation, Lambda</p><p class="snippet">from keras.optimizers import Adam</p><p class="snippet">from keras.utils import to_categorical</p><p class="snippet">from keras.models import load_model, Model</p><p class="snippet">import keras.backend as K</p><p class="snippet">import numpy as np</p><p class="snippet">from babel.dates import format_date</p><p class="snippet">from faker import Faker</p><p class="snippet">import random</p><p class="snippet">from tqdm import tqdm</p></li>
				<li>Next, we define some helper functions. We first use the '<strong class="inline">faker</strong>' and <strong class="inline">babel</strong> modules to generate data for training. The <strong class="inline">format_date</strong> function from <strong class="inline">babel</strong> generates date in a specific format (using <strong class="inline">FORMATS</strong>). Additionally, dates are also returned in a human-readable format that emulates the informal user input date that we wish to normalize:<p class="snippet">fake = Faker()</p><p class="snippet">fake.seed(12345)</p><p class="snippet">random.seed(12345)</p></li>
				<li>Define the format of the data we would like to generate:<p class="snippet">FORMATS = ['short',</p><p class="snippet">           'medium',</p><p class="snippet">           'long',</p><p class="snippet">           'full',</p><p class="snippet">           'full',</p><p class="snippet">           'full',</p><p class="snippet">           'full',</p><p class="snippet">           'full',</p><p class="snippet">           'full',</p><p class="snippet">           'full',</p><p class="snippet">           'full',</p><p class="snippet">           'full',</p><p class="snippet">           'full',</p><p class="snippet">           'd MMM YYY',</p><p class="snippet">           'd MMMM YYY',</p><p class="snippet">           'dd MMM YYY',</p><p class="snippet">           'd MMM, YYY',</p><p class="snippet">           'd MMMM, YYY',</p><p class="snippet">           'dd, MMM YYY',</p><p class="snippet">           'd MM YY',</p><p class="snippet">           'd MMMM YYY',</p><p class="snippet">           'MMMM d YYY',</p><p class="snippet">           'MMMM d, YYY',</p><p class="snippet">           'dd.MM.YY']</p><p class="snippet"># change this if you want it to work with another language</p><p class="snippet">LOCALES = ['en_US']</p><p class="snippet">def load_date():</p><p class="snippet">    """</p><p class="snippet">        Loads some fake dates</p><p class="snippet">        :returns: tuple containing human readable string, machine readable string, and date object</p><p class="snippet">    """</p><p class="snippet">    dt = fake.date_object()</p><p class="snippet">    human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))</p><p class="snippet">        human_readable = human_readable.lower()</p><p class="snippet">        human_readable = human_readable.replace(',','')</p><p class="snippet">        machine_readable = dt.isoformat()</p><p class="snippet">    return human_readable, machine_readable, dt</p></li>
				<li>Next, we generate and write a function to load the dataset. In this function, examples are created using the <strong class="inline">load_date()</strong> function defined earlier. In addition to this dataset, the function also returns dictionaries for mapping human-readable and machine-readable tokens along with the inverse machine vocabulary:<p class="snippet">def load_dataset(m):</p><p class="snippet">    """</p><p class="snippet">        Loads a dataset with m examples and vocabularies</p><p class="snippet">        :m: the number of examples to generate</p><p class="snippet">    """</p><p class="snippet">    human_vocab = set()</p><p class="snippet">    machine_vocab = set()</p><p class="snippet">    dataset = []</p><p class="snippet">    Tx = 30</p><p class="snippet">    for i in tqdm(range(m)):</p><p class="snippet">        h, m, _ = load_date()</p><p class="snippet">        if h is not None:</p><p class="snippet">            dataset.append((h, m))</p><p class="snippet">            human_vocab.update(tuple(h))</p><p class="snippet">            machine_vocab.update(tuple(m))</p><p class="snippet">    human = dict(zip(sorted(human_vocab) + ['&lt;unk&gt;', '&lt;pad&gt;'],</p><p class="snippet">                     list(range(len(human_vocab) + 2))))</p><p class="snippet">    inv_machine = dict(enumerate(sorted(machine_vocab)))</p><p class="snippet">    machine = {v:k for k,v in inv_machine.items()}</p><p class="snippet">    return dataset, human, machine, inv_machine</p><p>The previous helper functions are used to generate a dataset using the <strong class="inline">babel</strong> Python package. Additionally, it returns the input and output vocab dictionaries, as we have been doing in past exercises.</p></li>
				<li>Next, we generate a dataset having 10,000 samples using these helper functions:<p class="snippet">m = 10000</p><p class="snippet">dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)</p><p>The variables hold values, as depicted:</p><div class="IMG---Figure" id="_idContainer249"><img alt="Figure 8.8: Screenshot displaying variable values" src="image/C13783_08_08.jpg"/></div><h6>Figure 8.8: Screenshot displaying variable values</h6><p>The <strong class="inline">human_vocab</strong> is a dictionary that maps input characters to integers. The following is the mapping of values for <strong class="inline">human_vocab</strong>:</p><div class="IMG---Figure" id="_idContainer250"><img alt="Figure 8.9: Screenshot for human_vocab dictionary" src="image/C13783_08_09.jpg"/></div><h6>Figure 8.9: Screenshot for human_vocab dictionary</h6><p>The <strong class="inline">machine_vocab</strong> dictionary contains the mapping of the output character to integers.</p><div class="IMG---Figure" id="_idContainer251"><img alt="Figure 8.10: Screenshot for the machine_vocab dictionary" src="image/C13783_08_10.jpg"/></div><h6>Figure 8.10: Screenshot for the machine_vocab dictionary</h6><p><strong class="inline">inv_machine_vocab</strong> is an inverse mapping of <strong class="inline">machine_vocab</strong> to map predicted integers back to characters:</p><div class="IMG---Figure" id="_idContainer252"><img alt="Figure 8.11: Screenshot for the inv_machine_vocab dictionary" src="image/C13783_08_11.jpg"/></div><h6>Figure 8.11: Screenshot for the inv_machine_vocab dictionary</h6></li>
				<li>Next, we preprocess data such that the input sequences have shape (<strong class="inline">10000</strong>, <strong class="inline">30</strong>, <strong class="inline">len(human_vocab)</strong>). Thus, every row in this matrix represents 30 timesteps and the one-coded vector, having a value of 1 corresponding to the character at a given timestep. Similarly, the Y output gets the shape (<strong class="inline">10000</strong>, <strong class="inline">10</strong>, <strong class="inline">len(machine_vocab)</strong>). This corresponds to 10 output timesteps and the corresponding one-hot-coded output vector. We first define a function named '<strong class="inline">string_to_int</strong>' that takes as input a single user date and returns a sequence of integers that can be fed to the model:<p class="snippet">def string_to_int(string, length, vocab):</p><p class="snippet">    """</p><p class="snippet">    Converts all strings in the vocabulary into a list of integers representing the positions of the</p><p class="snippet">    input string's characters in the "vocab"</p><p class="snippet">    Arguments:</p><p class="snippet">    string -- input string, e.g. 'Wed 10 Jul 2007'</p><p class="snippet">    length -- the number of timesteps you'd like, determines if the output will be padded or cut</p><p class="snippet">    vocab -- vocabulary, dictionary used to index every character of your "string"</p><p class="snippet">    Returns:</p><p class="snippet">    rep -- list of integers (or '&lt;unk&gt;') (size = length) representing the position of the string's character in the vocabulary</p><p class="snippet">    """</p></li>
				<li>Change the case to lowercase to standardize the text<p class="snippet">    string = string.lower()</p><p class="snippet">    string = string.replace(',','')</p><p class="snippet">    if len(string) &gt; length:</p><p class="snippet">        string = string[:length]</p><p class="snippet">    rep = list(map(lambda x: vocab.get(x, '&lt;unk&gt;'), string))</p><p class="snippet">    if len(string) &lt; length:</p><p class="snippet">        rep += [vocab['&lt;pad&gt;']] * (length - len(string))</p><p class="snippet">    return rep</p></li>
				<li>We can now utilize this helper function to generate input and output integer sequences, as explained previously:<p class="snippet">def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):</p><p class="snippet">    X, Y = zip(*dataset)</p><p class="snippet">    print("X shape before preprocess: {}".format(X))</p><p class="snippet">    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])</p><p class="snippet">    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]</p><p class="snippet">    print("X shape from preprocess: {}".format(X.shape))</p><p class="snippet">    print("Y shape from preprocess: {}".format(Y))</p><p class="snippet">    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))</p><p class="snippet">    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))</p><p class="snippet">    return X, np.array(Y), Xoh, Yoh</p><p class="snippet">Tx = 30</p><p class="snippet">Ty = 10</p><p class="snippet">X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)</p></li>
				<li>Print the shape of the matrices.<p class="snippet">print("X.shape:", X.shape)</p><p class="snippet">print("Y.shape:", Y.shape)</p><p class="snippet">print("Xoh.shape:", Xoh.shape)</p><p class="snippet">print("Yoh.shape:", Yoh.shape)</p><p>The output of this step is as follows:</p><div class="IMG---Figure" id="_idContainer253"><img alt="" src="image/C13783_08_12.jpg"/></div><h6>Figure 8.12: Screenshot for the shape of matrices</h6></li>
				<li>We can further inspect the shapes of the <strong class="inline">X</strong>,<strong class="inline">Y</strong>, <strong class="inline">Xoh</strong>, and <strong class="inline">Yoh</strong> vectors:<p class="snippet">index = 0</p><p class="snippet">print("Source date:", dataset[index][0])</p><p class="snippet">print("Target date:", dataset[index][1])</p><p class="snippet">print()</p><p class="snippet">print("Source after preprocessing (indices):", X[index].shape)</p><p class="snippet">print("Target after preprocessing (indices):", Y[index].shape)</p><p class="snippet">print()</p><p class="snippet">print("Source after preprocessing (one-hot):", Xoh[index].shape)</p><p class="snippet">print("Target after preprocessing (one-hot):", Yoh[index].shape)</p><p>The output should be as follows:</p><div class="IMG---Figure" id="_idContainer254"><img alt="" src="image/C13783_08_13.jpg"/></div><h6>Figure 8.13: Screenshot for the shape of matrices after processing</h6></li>
				<li>We now start defining some functions that we need to build the model. First, we define a function that calculates a softmax value given a tensor as input:<p class="snippet">def softmax(x, axis=1):</p><p class="snippet">    """Softmax activation function.</p><p class="snippet">    # Arguments</p><p class="snippet">        x : Tensor.</p><p class="snippet">        axis: Integer, axis along which the softmax normalization is applied.</p><p class="snippet">    # Returns</p><p class="snippet">        Tensor, output of softmax transformation.</p><p class="snippet">    # Raises</p><p class="snippet">        ValueError: In case 'dim(x) == 1'.</p><p class="snippet">    """</p><p class="snippet">    ndim = K.ndim(x)</p><p class="snippet">    if ndim == 2:</p><p class="snippet">        return K.softmax(x)</p><p class="snippet">    elif ndim &gt; 2:</p><p class="snippet">        e = K.exp(x - K.max(x, axis=axis, keepdims=True))</p><p class="snippet">        s = K.sum(e, axis=axis, keepdims=True)</p><p class="snippet">        return e / s</p><p class="snippet">    else:</p><p class="snippet">        raise ValueError('Cannot apply softmax to a tensor that is 1D')</p></li>
				<li>Next, we can start to put the model together:<p class="snippet"># Defined shared layers as global variables</p><p class="snippet">repeator = RepeatVector(Tx)</p><p class="snippet">concatenator = Concatenate(axis=-1)</p><p class="snippet">densor1 = Dense(10, activation = "tanh")</p><p class="snippet">densor2 = Dense(1, activation = "relu")</p><p class="snippet">activator = Activation(softmax, name='attention_weights')</p><p class="snippet">dotor = Dot(axes = 1)</p></li>
				<li><strong class="inline">RepeatVector</strong> serves the purpose of repeating a given tensor multiple times. In our case, this is done <strong class="inline">Tx</strong> times, which is 30 input timesteps. The repeator is used to repeat <strong class="inline">S_prev</strong> 30 times. Recall that to calculate the context vector for determining one timestep decoder output, <strong class="inline">S_prev</strong> needs to be concatenated with each of the input encoder timesteps. The <strong class="inline">Concatenate</strong> <strong class="inline">keras</strong> function accomplishes the next step, that is, concatenating the repeated <strong class="inline">S_prev</strong> and encoder hidden state vector for each timestep. We have also defined MLP layers, which are two dense layers (<strong class="inline">densor1</strong>, <strong class="inline">densor2</strong>). Next, the output of MLP is passed through a <strong class="inline">softmax</strong> layer. This <strong class="inline">softmax</strong> distribution is an alpha vector with each entry corresponding to the weight for each concatenated vector. In the end, a <strong class="inline">dotor</strong> function is defined, which is responsible for calculating the context vector. The entire flow corresponds to one step attention (since it is for one decoder output timestep):<p class="snippet">def one_step_attention(h, s_prev):</p><p class="snippet">    """</p><p class="snippet">    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights</p><p class="snippet">    "alphas" and the hidden states "h" of the Bi-LSTM.</p><p class="snippet">    </p><p class="snippet">    Arguments:</p><p class="snippet">    h -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_h)</p><p class="snippet">    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)</p><p class="snippet">    </p><p class="snippet">    Returns:</p><p class="snippet">    context -- context vector, input of the next (post-attetion) LSTM cell</p><p class="snippet">    """</p></li>
				<li>Use <strong class="inline">repeator</strong> to repeat <strong class="inline">s_prev</strong> to be of shape (<strong class="inline">m</strong>, <strong class="inline">Tx</strong>, <strong class="inline">n_s</strong>) so that you can concatenate it with all hidden states, '<strong class="inline">h</strong>':<p class="snippet">    s_prev = repeator(s_prev)</p></li>
				<li>Use <strong class="inline">concatenator</strong> to concatenate <strong class="inline">a</strong> and <strong class="inline">s_prev</strong> on the last axis:<p class="snippet">    concat = concatenator([h, s_prev])</p></li>
				<li>Use <strong class="inline">densor1</strong> to propagate <strong class="inline">concat </strong>through a small fully-connected neural network to compute the intermediate energies variable, <strong class="inline">e</strong>:<p class="snippet">    e = densor1(concat)</p></li>
				<li>Use <strong class="inline">densor2</strong> to propagate <strong class="inline">e</strong> through a small fully-connected neural network to compute the variable energies:    <p class="snippet">    energies = densor2(e)</p></li>
				<li>Use <strong class="inline">activator</strong> on <strong class="inline">energies</strong> to compute the attention weights <strong class="inline">alphas</strong>:<p class="snippet">    alphas = activator(energies)</p></li>
				<li>Use <strong class="inline">dotor</strong> along with <strong class="inline">alphas</strong> and <strong class="inline">a</strong> to compute the context vector to be given to the next (post-attention) LSTM-cell:<p class="snippet">    context = dotor([alphas, h])</p><p class="snippet">    </p><p class="snippet">    return context</p></li>
				<li>Up to this point, we still haven't defined the number of hidden state units for the encoder and decoder LSTMs. We also need to define the decoder LSTM, which is a unidirectional LSTM:<p class="snippet">n_h = 32</p><p class="snippet">n_s = 64</p><p class="snippet">post_activation_LSTM_cell = LSTM(n_s, return_state = True)</p><p class="snippet">output_layer = Dense(len(machine_vocab), activation=softmax)</p></li>
				<li>We now define the encoder and decoder model:<p class="snippet">def model(Tx, Ty, n_h, n_s, human_vocab_size, machine_vocab_size):</p><p class="snippet">    """</p><p class="snippet">    Arguments:</p><p class="snippet">    Tx -- length of the input sequence</p><p class="snippet">    Ty -- length of the output sequence</p><p class="snippet">    n_h -- hidden state size of the Bi-LSTM</p><p class="snippet">    n_s -- hidden state size of the post-attention LSTM</p><p class="snippet">    human_vocab_size -- size of the python dictionary "human_vocab"</p><p class="snippet">    machine_vocab_size -- size of the python dictionary "machine_vocab"</p><p class="snippet">    Returns:</p><p class="snippet">    model -- Keras model instance</p><p class="snippet">    """</p></li>
				<li>Define the inputs of your model with a shape (<strong class="inline">Tx,</strong>). Define <strong class="inline">s0</strong> and <strong class="inline">c0</strong>, and the initial hidden state for the decoder LSTM of shape (<strong class="inline">n_s,</strong>):<p class="snippet">    X = Input(shape=(Tx, human_vocab_size), name="input_first")</p><p class="snippet">    s0 = Input(shape=(n_s,), name='s0')</p><p class="snippet">    c0 = Input(shape=(n_s,), name='c0')</p><p class="snippet">    s = s0</p><p class="snippet">    c = c0</p></li>
				<li>Initialize an empty list of <strong class="inline">outputs</strong>:<p class="snippet">    outputs = []</p></li>
				<li>Define your pre-attention Bi-LSTM. Remember to use <strong class="inline">return_sequences=True</strong>:<p class="snippet">    h = Bidirectional(LSTM(n_h, return_sequences=True))(X)</p></li>
				<li>Iterate for <strong class="inline">Ty</strong> steps:<p class="snippet">    for t in range(Ty):</p></li>
				<li>Perform one step of the attention mechanism to get back the context vector at step <strong class="inline">t</strong>:<p class="snippet">        context = one_step_attention(h, s)</p></li>
				<li>Apply the post-attention LSTM cell to the <strong class="inline">context </strong>vector. Also, pass <strong class="inline">initial_state</strong> <strong class="inline">= [hidden state, cell state]</strong>:<p class="snippet">        s, _, c = post_activation_LSTM_cell(context, initial_state = [s,c])</p></li>
				<li>Apply the <strong class="inline">Dense</strong> layer to the hidden state output of the post-attention LSTM:<p class="snippet">        out = output_layer(s)</p><p class="snippet">        </p><p class="snippet">        # Append "out" to the "outputs" list</p><p class="snippet">        outputs.append(out)</p><p class="snippet">    </p></li>
				<li>Create a model instance by taking three inputs and returning the list of outputs:<p class="snippet">    model = Model(inputs=[X, s0, c0], outputs=outputs)</p><p class="snippet">    </p><p class="snippet">    return model</p><p class="snippet">model = model(Tx, Ty, n_h, n_s, len(human_vocab), len(machine_vocab))</p><p class="snippet">model.summary()</p><p>The output could be as shown in the following figure:</p><div class="IMG---Figure" id="_idContainer255"><img alt="Figure 8.14: Screenshot for model summary" src="image/C13783_08_14.jpg"/></div><h6>Figure 8.14: Screenshot for model summary</h6></li>
				<li>We will now compile the model with <strong class="inline">categorical_crossentropy</strong> as the loss function and <strong class="inline">Adam</strong> optimizer as the optimization strategy:<p class="snippet">opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)</p><p class="snippet">model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])</p></li>
				<li>We need to initialize the hidden state vector and memory state for decoder LSTM before fitting the model:<p class="snippet">s0 = np.zeros((m, n_s))</p><p class="snippet">c0 = np.zeros((m, n_s))</p><p class="snippet">outputs = list(Yoh.swapaxes(0,1))</p><p class="snippet">model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)</p><p>This starts the training:</p><div class="IMG---Figure" id="_idContainer256"><img alt="Figure 8.15: Screenshot for epoch training" src="image/C13783_08_15.jpg"/></div><h6>Figure 8.15: Screenshot for epoch training</h6></li>
				<li>The model is now trained and can be called for inference:<p class="snippet">EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']</p><p class="snippet">for example in EXAMPLES:</p><p class="snippet">    </p><p class="snippet">    source = string_to_int(example, Tx, human_vocab)</p><p class="snippet">    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))#.swapaxes(0,1)</p><p class="snippet">    source = source[np.newaxis, :]</p><p class="snippet">    prediction = model.predict([source, s0, c0])</p><p class="snippet">    prediction = np.argmax(prediction, axis = -1)</p><p class="snippet">    output = [inv_machine_vocab[int(i)] for i in prediction]</p><p class="snippet">    </p><p class="snippet">    print("source:", example)</p><p class="snippet">    print("output:", ''.join(output))</p><p><strong class="bold">Expected output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer257">
					<img alt="Figure 8.16: Screenshot for normalized date output" src="image/C13783_08_16.jpg"/>
				</div>
			</div>
			<h6>Figure 8.16: Screenshot for normalized date output</h6>
			<h2 id="_idParaDest-184">Other Architectu<a id="_idTextAnchor206"/>res and Developments</h2>
			<p>The attention mechanism architecture described in the last section is only a way of building attention mechanism. In recent times, several other architectures have been proposed, which constitute a state of the art in the deep learning NLP world. In this section, we will briefly mention some of these architectures.</p>
			<h3 id="_idParaDest-185"><a id="_idTextAnchor207"/>Transformer</h3>
			<p>In l<a id="_idTextAnchor208"/>ate 2017, Google came up with an attention mechanism architecture in their seminal paper titled "Attention is all you need." This architecture is considered state-of-the-art in the NLP community. The transformer architecture makes use of a special multi-head attention mechanism to generate attention at various levels. Additionally, it is also employs residual connections to further ensure that the vanishing gradient problem has a minimal impact on learning. The special architecture of transformers also allows a massive speed up of the training phase while providing better quality results.</p>
			<p>The most commonly used package with transformer architecture is <strong class="keyword">tensor2tensor</strong>. The Keras code for transformer tends to be very bulky and untenable, while <strong class="keyword">tensor2tensor</strong> allows the use of both a Python package and a simple command-line utility that can be used to train a transformer model.</p>
			<h4>Note</h4>
			<p class="callout">For more information on tensor2tensor, refer to <a href="">https://github.com/tensorflow/tensor2tensor/#t2t-overview</a></p>
			<p class="callout">Readers interested in learning more about the architecture should read the mentioned paper and the associated Google blogpost at this link: <a href="">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a></p>
			<h3 id="_idParaDest-186"><a id="_idTextAnchor209"/>B<span class="None">ERT</span></h3>
			<p>In late 201<a id="_idTextAnchor210"/>8, Google open sourced yet another groundbreaking architecture, called <strong class="keyword">BERT</strong> (<strong class="keyword">Bidirectional Encoder Representations from Transformers</strong>). The deep learning community for NLP has been missing the transfer-learning regime for training models for a long time. The transfer learning approach to deep learning has been state-of-the-art with image-related tasks such as image classification. Images are universal in their basic structure, as they do not differ regardless of geographical locations. This allows the training of deep learning models on generic images. These pre-trained models can then be fine-tuned for a specific task. This saves training time and the need for massive amounts of data to achieve a respectable model performance.</p>
			<p>Languages, unfortunately, vary a lot depending upon geographical locations and tend to not share basic structures. Hence, transfer learning is not a viable option when it comes to NLP tasks. BERT has now made it possible with its new attention mechanism architecture, which builds on top of the basic transformer architecture.</p>
			<h4>Note</h4>
			<p class="callout">For more information on BERT, refer to <a href="">https://github.com/google-research/bert</a></p>
			<p class="callout">Readers interested in learning more about BERT should take a look at the Google blog on it at <a href="">https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html</a>.</p>
			<h3 id="_idParaDest-187"><a id="_idTextAnchor211"/>Open AI GPT-2</h3>
			<p>Op<a id="_idTextAnchor212"/>en AI also open sourced an architecture called <strong class="keyword">GPT-2</strong>, which builds upon their previous architecture called GPT. The mainstay of the GPT-2 architecture is its ability to perform well on text-generation tasks. The GPT-2 model is also a transformer-based model containing around 1.5 billion parameters.</p>
			<h4>Note</h4>
			<p class="callout">Readers interested in learning more can refer to the blogpost by OpenAI at <a href="">https://blog.openai.com/better-language-models/</a>.</p>
			<h2 id="_idParaDest-188">Activity 11: Bui<a id="_idTextAnchor213"/><a id="_idTextAnchor214"/>ld a Text Summarization Model</h2>
			<p>We will use the attention mechanism model architecture we built for neural machine translation to build a text summarization model. The goal of text summarization is to write a summary of a given large text corpus. You can imagine using text summarizers for the summarization of books or the generation of headlines for news articles.</p>
			<p>As an example, use the given input text:</p>
			<p><span class="None">"Celebrating its 25th year, Mercedes-Benz India is set to redefine India's luxury space in the automotive segment by launching the new V-Class. The V-Class is powered by a 2.1-litre BS VI diesel engine that generates 120kW power, 380Nm torque, and can go from 0-100km/h in 10.9 seconds. It features LED headlamps, a multi-functional steering wheel, and 17-inch alloy wheels."</span></p>
			<p><span class="None">A good text summarization model should be able to produce a meaningful summary, such as:</span></p>
			<p><span class="None">"Mercedes-Benz India launches the new V-Class"</span></p>
			<p>From an architectural viewpoint, a text summarization model is exactly the same as a translation model. The input to the model is text that is fed character by character (or word by word) to an encoder, while the decoder produces output characters in the same language as the source text.</p>
			<h4>Note</h4>
			<p class="callout">The input text can be found at <a href="">https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2008</a>.</p>
			<p>The following steps will help you with the solution:</p>
			<ol>
				<li value="1">Import the required Python packages and make the human and machine vocab dictionaries.</li>
				<li>Define the length of the input and output characters and the model functions (<em class="italics">Repeator</em>, <em class="italics">Concatenate</em>, <em class="italics">Densors</em>, and <em class="italics">Dotor</em>).</li>
				<li>Define a one-step-attention function and the number of hidden states for the decoder and encoder.</li>
				<li>Define the model architecture and run it to obtain a model.</li>
				<li>Define model loss functions and other hyperparameters. Also, initialize the decoder state vectors.</li>
				<li>Fit the model to our data.</li>
				<li>Run the inference step for the new text.<p><strong class="bold">Expected Output:</strong></p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer258">
					<img alt="Figure 8.17: Output for text summarization" src="image/C13783_08_17.jpg"/>
				</div>
			</div>
			<h6>Figure 8.17: Output for text summarization</h6>
			<h4>Note</h4>
			<p class="callout">The solution for the activity can be found on page 333.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor215"/>Summary</h2>
			<p>In this chapter, we learned about the concept of attention mechanisms. Based on attention mechanisms, several architectures have been proposed that constitute the state of the art in the NLP world. We learned about one specific model architecture to perform a neural machine translation task. We also briefly mentioned other state-of-the-art architectures such as transformers and BERT.</p>
			<p><span class="None">Up to now, we ha<a id="_idTextAnchor216"/>ve seen many different NLP models. In the next chapter, we will look at the flow of a practical NLP project in an organization and related technology.</span></p>
		</div>
	</body></html>