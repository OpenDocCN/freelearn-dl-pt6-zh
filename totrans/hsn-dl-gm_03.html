<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Convolutional and Recurrent Networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">The human brain is often the main inspiration and comparison we make when building AI and is something deep learning researchers often look to for inspiration or reassurance. By studying the brain and its parts in more detail, we often discover neural sub-processes. An example of a neural sub-process would be our visual cortex, the area or region of our brain responsible for vision. We now understand that this area of our brain is wired differently and responds differently to input. This just so happens to be analogous to analog what we have found in our previous attempts at using neural networks to classify images. Now, the human brain has many sub-processes all with specific mapped areas in the brain (sight, hearing, smell, speech, taste, touch, and memory/temporal), but in this chapter, we will look at how we model just sight and memory by using advanced forms of deep learning called <strong>convolutional and recurrent networks</strong>. The two-core sub-processes of sight and memory are used extensively by us for many tasks including gaming and form the focus of research of many deep learners.</p>
<div class="packt_infobox">Researchers often look to the brain for inspiration, but the computer models they build often don't entirely resemble their biological counterpart. However, researchers have begun to identify almost perfect analogs to neural networks inside our brains. One example of this is the ReLU activation function. It was recently found that the excitement level in our brains' neurons, when plotted, perfectly matched a ReLU graph. </div>
<p>In this chapter, we will explore, in some detail, convolutional and recurrent neural networks. We will look at how they solve the problem of replicating accurate vision and memory in deep learning. These two new network or layer types are a fairly recent discovery but have been responsible in part for many advances in deep learning. This chapter will cover the following topics:</p>
<ul>
<li>Convolutional neural networks</li>
<li>Understanding convolution</li>
<li>Building a self-driving CNN</li>
<li>Memory and recurrent networks</li>
<li>Playing rock, paper, scissors with LSTMs</li>
</ul>
<p>Be sure you understand the fundamentals outlined in the previous chapter reasonably well before proceeding. This includes running the code samples, which install this chapter's required dependencies.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional neural networks</h1>
                </header>
            
            <article>
                
<p>Sight is hands-down the most-used sub-process. You are using it right now! Of course, it was something researchers attempted to mimic with neural networks early on, except that nothing really worked well until the concept of convolution was applied and used to classify images. The concept of convolution is the idea behind detecting, sometimes grouping, and isolating common features in an image. For instance, if you cover up 3/4 of a picture of a familiar object and show it to someone, they will almost certainly recognize the image by recognizing just the partial features. Convolution works the same way, by blowing up an image and then isolating the features for later recognition.</p>
<p>Convolution works by dissecting an image into its feature parts, which makes it easier to train a network. Let's jump into a code sample that extends from where we left off in the previous chapter but that now introduces convolution. Open up the <kbd>Chapter_2_1.py</kbd> listing and follow these steps:</p>
<ol>
<li class="mce-root"><span>Take a look at the first couple of lines doing the import:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">import tensorflow as tf<br/>from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras import backend as K</pre>
<ol start="2">
<li>In this example, we import new layer types: <kbd>Conv2D</kbd>, <kbd>MaxPooling2D</kbd>, and <kbd>UpSampling2D</kbd>. </li>
<li class="mce-root"><span>Then we set the</span> <kbd>Input</kbd> <span>and build up the encoded and decoded network sections using the following code:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">input_img = Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format<br/><br/>x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)<br/>x = MaxPooling2D((2, 2), padding='same')(x)<br/>x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)<br/>x = MaxPooling2D((2, 2), padding='same')(x)<br/>x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)<br/>encoded = MaxPooling2D((2, 2), padding='same')(x)<br/><br/>x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)<br/>x = UpSampling2D((2, 2))(x)<br/>x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)<br/>x = UpSampling2D((2, 2))(x)<br/>x = Conv2D(16, (3, 3), activation='relu')(x)<br/>x = UpSampling2D((2, 2))(x)<br/>decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)</pre>
<ol start="4">
<li>The first thing to note is that we are now preserving the dimensions of the image, in this case, 28 x 28 pixels wide and 1 layer or channel. This example uses an image that is in grayscale, so there is only a single color channel. This is vastly different from before, when we just unraveled the image into a single 784-dimension vector.</li>
</ol>
<p style="padding-left: 60px">The second thing to note is the use of the <kbd>Conv2D</kbd> layer or two-dimensional convolutional layer and the following <kbd>MaxPooling2D</kbd> or <kbd>UpSampling2D</kbd> layers. Pooling or sampling layers are used to gather or conversely unravel features. Note how we use pooling or down-sampling layers after convolution when the image is encoded and then up-sampling layers when decoding the image.</p>
<ol start="5">
<li>Next, we build and train the model with the following block of code:</li>
</ol>
<pre style="padding-left: 60px">autoencoder = Model(input_img, decoded)<br/>autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')<br/><br/>from tensorflow.keras.datasets import mnist<br/>import numpy as np<br/><br/>(x_train, _), (x_test, _) = mnist.load_data()<br/><br/>x_train = x_train.astype('float32') / 255.<br/>x_test = x_test.astype('float32') / 255.<br/>x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) <br/>x_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) <br/><br/>from tensorflow.keras.callbacks import TensorBoard<br/><br/>autoencoder.fit(x_train, x_train,<br/> epochs=50,<br/> batch_size=128,<br/> shuffle=True,<br/> validation_data=(x_test, x_test),<br/> callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])<br/><br/>decoded_imgs = autoencoder.predict(x_test)</pre>
<ol start="6">
<li>The training of the model in the preceding code mirrors what we did at the end of the previous chapter, but note the selection of training and testing sets now. We no longer squish the image but rather preserve its spatial properties as inputs into the convolutional layer.</li>
<li>Finally, we output the results with the following code:</li>
</ol>
<pre style="color: black;padding-left: 60px">n = 10<br/>plt.figure(figsize=(20, 4))<br/>for i in range(n):<br/>  ax = plt.subplot(2, n, i)<br/>  plt.imshow(x_test[i].reshape(28, 28))<br/>  plt.gray()<br/>  ax.get_xaxis().set_visible(False)<br/>  ax.get_yaxis().set_visible(False)<br/>  ax = plt.subplot(2, n, i + n)<br/>  plt.imshow(decoded_imgs[i].reshape(28, 28))<br/>  plt.gray()<br/>  ax.get_xaxis().set_visible(False)<br/>  ax.get_yaxis().set_visible(False)<br/>plt.show()</pre>
<ol start="8">
<li class="CDPAlignLeft CDPAlign">Run the code, as you have before, and you'll immediately notice that it is about 100 times slower to train. This may or may not require you to wait, depending on your machine; if it does, go get a beverage or three and perhaps a meal. </li>
</ol>
<p>Training our simple sample now takes a large amount of time, which may be quite noticeable on older hardware. In the next section, we look at how we can start to monitor the training sessions, in great detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring training with TensorBoard</h1>
                </header>
            
            <article>
                
<p>TensorBoard is essentially a mathematical graph or calculation engine that performs very well at crunching numbers, hence our use of it in deep learning. The tool itself is still quite immature, but there are some very useful features for monitoring training exercises.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Follow these steps to start monitoring training on our sample:</p>
<ol>
<li class="mce-root"><span>You can monitor the training session by entering the following command into a new</span> <strong>Anaconda</strong> <span>or command window from the same directory/folder that you are running the sample from:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">//first change directory to sample working folder<br/><strong>tensorboard --logdir<span class="o">=</span>/tmp/autoencoder</strong></pre>
<ol start="2">
<li class="mce-root"><span>This will launch a TensorBoard server, and you can view the output by navigating your browser to the URL in italics, as shown in the window you are running</span> <kbd>TensorBoard</kbd> <span>from. It will typically look something like the following:</span></li>
</ol>
<pre style="color: black;padding-left: 60px">TensorBoard 1.10.0 at <em><strong>http://DESKTOP-V2J9HRG:6006</strong></em> (Press CTRL+C to quit)<br/>or use<br/><strong>http://0.0.0.0:6000</strong></pre>
<ol start="3">
<li>Note, the URL should use your machine name, but if that doesn't work, try the second form. Be sure to allow ports <kbd>6000</kbd>, and <kbd>6006</kbd> and/or the <strong>TensorBoard</strong> application through your firewall if prompted.</li>
<li class="CDPAlignLeft CDPAlign">When the sample is done running, you should see the following:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-medium wp-image-727 image-border" src="assets/d666bad9-02b7-43c3-ae41-bee3a36635e2.png" style="width:25.00em;height:5.58em;"/><br/>
<br/>
Auto-encoding digits using convolution</div>
<ol start="5">
<li>Go back and compare the results from this example and the last example from <a href="108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml" target="_blank">Chapter 1</a>,<a href="108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml" target="_blank"/><a href="108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml" target="_blank"/><em> Deep Learning for Games</em>. Note the improvement in performance.</li>
</ol>
<p>Your immediate thought may be, "<em>Is the increased training time we experienced worth the effort?</em>" After all, the decoded images look quite similar in the previous example, and it trained much faster, except, remember we are training the network weights slowly by adjusting each weight over each iteration, which we can then save as a model. That model or brain can then be used to perform the same task again later, without training. Works scarily enough! Keep this concept in mind as we work through this chapter. In <a href="cb51d15b-9855-47e2-8e45-f74a115ebfa8.xhtml" target="_blank"/><a href="cb51d15b-9855-47e2-8e45-f74a115ebfa8.xhtml">Chapter 3</a>, <em>GAN for Games</em>, we will start saving and moving our brain models around.</p>
<p>In the next section, we take a more in-depth look at how convolution works. Convolution can be tricky to understand when you first encounter it, so take your time. It is important to understand how it works, as we will use it extensively later.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding convolution</h1>
                </header>
            
            <article>
                
<p><strong>Convolution</strong> is a way of extracting features from an image that may allow us to more easily classify it based on known features. Before we get into convolution, let's first take a step back and understand why networks, and our vision for that matter, need to isolate features in an image. Take a look at the following; it's a sample image of a dog, called Sadie, with various image filters applied:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/fb6c51c0-04d3-40ec-9f20-5a342cefff6f.png" style="width:56.08em;height:18.58em;"/><br/>
<br/>
Example of an image with different filters applied</div>
<p>The preceding shows four different versions with no filter, edge detection, pixelate, and glowing edges filters applied. In all cases, though, you as a human can clearly recognize it is a picture of a dog, regardless of the filter applied, except note that in the edge detection case, we have eliminated the extra image data that is unnecessary to recognize a dog. By using a filter, we can extract just the required features our NN needs to recognize a dog. This is all a convolution filter does, and in some cases, one of those filters could be just a simple edge detection.</p>
<p>A convolution filter is a matrix or kernel of numbers that defines a single math operation. The process starts by being multiplied by the upper-left corner pixel value, with the results of the matrix operation summed and set as the output. The kernel is slid across the image in a step size called a <strong>stride</strong>, and this operation is demonstrated:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/114cee40-0fd4-4978-b46d-c8d5f8a5882d.png" style="width:23.42em;height:20.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Applying a convolution filter</div>
<p>In the preceding diagram, a stride of 1 is being used. The filter being applied in the convolution operation is essentially an edge detection filter. If you look at the result of the final operation, you can see the middle section is now filled with OS, greatly simplifying any classification task. The less information our networks need to learn, the quicker they will learn and with less data. Now, the interesting part of this is that the convolution learns the filter, the numbers,or the weights it needs to apply in order to extract the relevant features. This is not so obvious and may be confusing, so let's go over it again. Go back to our previous example and look at how we define the first convolution layer:</p>
<pre>x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)</pre>
<p>In that line of code, we define the first convolution layer as having <kbd>16</kbd> output filters, meaning our output from this layer is actually 16 filters. We then set the kernel size to <kbd>(3,3)</kbd>, which represents a <kbd>3x3</kbd> matrix , just as in our example. Note how we don't specify the values of the various kernel filter weights, as that is after all what the network is training to do.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's see how this looks when everything is put together in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/7a13bb31-cf32-4ec2-838a-e066ef86f047.png" style="width:37.83em;height:11.50em;"/><br/>
<br/>
Full convolution operation</div>
<p>The output from the first step in convolution is the feature map. One feature map represents a single convolution filter being applied and is generated by applying the learned filter/kernel. In our example, the first layer produces <strong>16 kernels</strong>, which in turn produce <strong>16 feature maps</strong>; remember that the value of <kbd>16</kbd> is for the number of filters.</p>
<p>After convolution, we then apply pooling or subsampling in order to collect or gather features into sets. This subsampling further creates new concentrated feature maps that highlight the image's important features we are training for. Take a look back at how we defined the first pooling layer in our previous example:</p>
<pre>x = MaxPooling2D((2, 2), padding='same')(x)</pre>
<p>In the code, we are subsampling using a <kbd>pool_size</kbd> of <kbd>(2,2)</kbd>. The size indicates the factor by which to down-sample the image by width and height. So a 2 x 2 pool size will create four feature maps at half the size in width and height. This results in a total of 64 feature maps after our first layer of convolution and pooling. We get this by multiplying 16 (convolution feature maps) x 4 (pooling feature maps) = 64 feature maps. Consider how many total feature maps we build in our simple example:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2061d9ed-16c1-475a-a519-bb47eff32601.png" style="width:39.42em;height:1.08em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d0f65917-9560-4f1b-8aae-c62ff29a9071.png" style="width:20.67em;height:1.08em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/56c4dfa5-f217-4d4a-8f46-e0cd5d904258.png" style="width:12.08em;height:1.00em;"/></p>
<p>That is 65,536 feature maps of 4 x 4 images. This means we now train our network on 65,536 smaller images; for each image, we attempt to encode or classify. This is obviously the cause for the increased training time, but also consider the amount of extra data we are now using to classify our images. Now our network is learning how to identify parts or features of our image, just as we humans identify objects.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For instance, if you were just shown the nose of a dog, you could likely recognize that as a dog. Consequently, our sample network now is identifying parts of the handwritten digits, which as we know now, dramatically improves performance.</p>
<p>As we have seen, convolution works well for identifying images, but the process of pooling can have disruptive consequences to preserving spatial relationships. Therefore, when it comes to games or learning requiring some form of spatial understanding, we prefer to limit pooling or eliminate altogether. Since it is important to understand when to use and not to use pooling, we will cover that in more detail in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a self-driving CNN</h1>
                </header>
            
            <article>
                
<p>Nvidia created a multi-layer CNN called <strong>PilotNet</strong>, in 2017, that was able to steer a vehicle by just showing it a series of images or video. This was a compelling demonstration of the power of neural networks, and in particular the power of convolution. A diagram showing the neural architecture of PilotNet is shown here:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/dc9e1eee-fc4c-4305-9b0e-489e93ff06d6.png" style="width:20.25em;height:28.00em;"/><br/>
<br/>
PilotNet neural architecture</div>
<p>The diagram shows the input of the network moving up from the bottom where the results of a single input image output to a single neuron represent the steering direction. Since this is such a great example, several individuals have posted blog posts showing an example of PilotNet, and some actually work. We will examine the code from one of these blog posts to see how a similar architecture is constructed with Keras. Next is an image from the original PilotNet blog, showing a few of the types of images our self-driving network will use to train:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/4cf9c76e-f157-44bb-a455-14ffb5240090.png" style="width:20.08em;height:19.92em;"/><br/>
<br/>
Example of PilotNet training images</div>
<p>The goal of training in this example is to output the degree to which the steering wheel should be turned in order to keep the vehicle on the road. Open up the code listing in <kbd>Chapter_2_2.py</kbd> and follow these steps:</p>
<ol>
<li class="mce-root"><span>We will now switch to using</span> Keras <span>for a few samples. While the</span> TensorFlow <span>embedde</span><span>d version of</span> Keras <span>has served us well, there are a couple of features we need that are only found in the full version. To install</span> Keras <span>and other dependencies, open a shell or Anaconda window and run the following commands:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>pip install keras</strong><br/><strong>pip install pickle</strong><br/><strong>pip install matplotlib</strong></pre>
<ol start="2">
<li>At the start of the code file (<kbd>Chapter_2_2.py</kbd>), we begin with some imports and load the sample data using the following code:</li>
</ol>
<pre style="padding-left: 60px">import os<br/>import urllib.request<br/>import pickle<br/>import matplotlib<br/>import matplotlib.pyplot as plt<br/><br/><em><strong>#downlaod driving data (450Mb)</strong> </em><br/>data_url = 'https://s3.amazonaws.com/donkey_resources/indoor_lanes.pkl'<br/>file_path, headers = urllib.request.urlretrieve(data_url)<br/>print(file_path)<br/><br/>with open(file_path, 'rb') as f:<br/>  X, Y = pickle.load(f)</pre>
<ol start="3">
<li>This code just does some imports and then downloads the sample driving frames from the author's source data. The original source of this blog was written in a notebook by <strong>Roscoe's Notebooks</strong> and can be found at <a href="https://wroscoe.github.io/keras-lane-following-autopilot.html">https://wroscoe.github.io/keras-lane-following-autopilot.html</a>.<br/>
<kbd>pickle</kbd> is a decompression library that unpacks the data in datasets <kbd>X</kbd> and <kbd>Y</kbd> at the bottom of the previous listing.</li>
<li>Then we shuffle the order of the frames around or essentially randomize the data. We often randomize data this way to make our training stronger. By randomizing the data order, the network needs to learn an absolute steering value for an image, rather than a possible relative or incremental value. The following code does this shuffle:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>def unison_shuffled_copies(X, Y):<br/>  assert len(X) == len(Y)<br/>  p = np.random.permutation(len(X))<br/>  return X[p], Y[p]<br/><br/>shuffled_X, shuffled_Y = unison_shuffled_copies(X,Y)<br/>len(shuffled_X)</pre>
<ol start="5">
<li>All this code does is use <kbd>numpy</kbd> to randomly shuffle the image frames. Then it prints out the length of the first shuffled set <kbd>shuffled_X</kbd> so we can confirm the training data is not getting lost.</li>
</ol>
<ol start="6">
<li>Next, we need to create a training and test set of data. The training set is used to train the network (weights), and the test, or validation, set is used to confirm the accuracy on new or raw data. As we have seen before, this is a common theme when using supervised training or labeled data. We often break the data into 80% training and 20% test. The following code is what does this:</li>
</ol>
<pre style="padding-left: 60px">test_cutoff = int(len(X) * .8) # 80% of data used for training<br/>val_cutoff = test_cutoff + int(len(X) * .2) # 20% of data used for validation and test data<br/>train_X, train_Y = shuffled_X[:test_cutoff], shuffled_Y[:test_cutoff]<br/>val_X, val_Y = shuffled_X[test_cutoff:val_cutoff], shuffled_Y[test_cutoff:val_cutoff]<br/>test_X, test_Y = shuffled_X[val_cutoff:], shuffled_Y[val_cutoff:]<br/><br/>len(train_X) + len(val_X) + len(test_X)</pre>
<ol start="7">
<li>After creating the training and test sets, we now want to augment or expand the training data. In this particular case, the author augmented the data just by flipping the original images and adding those to the dataset. There are many other ways of augmenting data that we will discover in later chapters, but this simple and effective method of flipping is something to add to your belt of machine learning tools. The code to do this flip is shown here:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">X_flipped = np.array([np.fliplr(i) for i in train_X])<br/>Y_flipped = np.array([-i for i in train_Y])<br/>train_X = np.concatenate([train_X, X_flipped])<br/>train_Y = np.concatenate([train_Y, Y_flipped])<br/>len(train_X)</pre>
<ol start="8">
<li>Now comes the heavy lifting part. The data is prepped, and it is time to build the model as shown in the code:</li>
</ol>
<pre style="padding-left: 60px">from keras.models import Model, load_model<br/>from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, Dropout, Flatten, Dense<br/><br/>img_in = Input(shape=(120, 160, 3), name='img_in')<br/>angle_in = Input(shape=(1,), name='angle_in')<br/><br/>x = Convolution2D(8, 3, 3)(img_in)<br/>x = Activation('relu')(x)<br/>x = MaxPooling2D(pool_size=(2, 2))(x)<br/><br/>x = Convolution2D(16, 3, 3)(x)<br/>x = Activation('relu')(x)<br/>x = MaxPooling2D(pool_size=(2, 2))(x)<br/><br/>x = Convolution2D(32, 3, 3)(x)<br/>x = Activation('relu')(x)<br/>x = MaxPooling2D(pool_size=(2, 2))(x)<br/><br/><strong>merged = Flatten()(x)</strong><br/><br/>x = Dense(256)(merged)<br/>x = Activation('linear')(x)<br/><strong>x = Dropout(.2)(x)</strong><br/><br/>angle_out = Dense(1, name='angle_out')(x)<br/><br/>model = Model(input=[img_in], output=[angle_out])<br/>model.compile(optimizer='adam', loss='mean_squared_error')<br/>model.summary()</pre>
<ol start="9">
<li>The code to build the model at this point should be fairly self-explanatory. Take note of the variation in the architecture and how the code is written from our previous examples. Also note the two highlighted lines. The first one uses a new layer type called <kbd>Flatten</kbd>. All this layer type does is flatten the 2 x 2 image into a vector that is then input into a standard <kbd>Dense</kbd> hidden fully connected layer. The second highlighted line introduces another new layer type called <kbd>Dropout</kbd>. This layer type needs a bit more explanation and will be covered in more detail at the end of this section.</li>
<li>Finally comes the training part, which this code sets up:</li>
</ol>
<pre style="padding-left: 60px">import os<br/>from keras import callbacks<br/><br/>model_path = os.path.expanduser('~/best_autopilot.hdf5')<br/><br/>save_best = callbacks.ModelCheckpoint(model_path, monitor='val_loss', verbose=1, <br/> save_best_only=True, mode='min')<br/><br/>early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, <br/> verbose=0, mode='auto')<br/><br/>callbacks_list = [save_best, early_stop]<br/><br/>model.fit(train_X, train_Y, batch_size=64, epochs=4, validation_data=(val_X, val_Y), callbacks=callbacks_list)</pre>
<ol start="11">
<li>This last piece of code sets up a set of <kbd>callbacks</kbd> to update and control the training. We have already used callbacks to update the TensorBoard server with logs. In this case, we use the callbacks to resave the model after every checkpoint (epoch) and check for an early exit. Note the form in which we are saving the model – an <kbd>hdf5</kbd> file. This file format represents a hierarchical data structure.</li>
<li>Run the code as you have already been doing. This sample can take a while, so again be patient. When you are done, there will be no output, but pay special attention to the minimized loss value.</li>
</ol>
<div class="packt_infobox">At this point in your deep learning career, you may be realizing that you need much more patience or a better computer or perhaps a TensorFlow-supported GPU. If you want to try the latter, feel free to download and install the TensorFlow GPU library and the other required libraries for your OS, as this will vary. Plenty of documentation can be found online. After you have the GPU version of TensorFlow installed, Keras will automatically try to use that. If you have a supported GPU, you should notice a performance increase, and if not, then consider buying one.</div>
<p>While there is no output for this example, in order to keep it simple, try to appreciate what is happening. After all, this could just as easily be set up as a driving game, where the network drives the vehicle by just looking at screenshots. We have omitted the results from the author's original blog post, but if you want to see how this performs further, go back and check out the <a href="https://wroscoe.github.io/keras-lane-following-autopilot.html">source link</a>.</p>
<p>One thing the author did in his blog post was to use pooling layers, which, as we have seen, is quite standard when working with convolution. However, when and how to use pooling layers is a bit contentious right now and requires further detailed discussion, which is provided in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spatial convolution and pooling</h1>
                </header>
            
            <article>
                
<p><span>Geoffrey</span> Hinton and <span>his team </span>have recently strongly suggested that using pooling with convolution removes spatial relationships in the image. Hinton instead suggests the use of <strong>CapsNet</strong>, or <strong>Capsule Networks</strong>. Capsule Networks are a method of pooling that preserves the spatial integrity of the data. Now, this may not be a problem in all cases. For handwritten digits, spatial relationships don't matter that much. However, self-driving cars or networks tasked with spatial tasks, a prime example of which is games, often don't perform as well when using pooling. In fact, the team at Unity do not use pooling layers after convolution; let's understand why.</p>
<p>Pooling or down-sampling is a way of augmenting data by collecting its common features together. The problem with this is that any relationship in the data often gets lost entirely. The following diagram demonstrates <strong>MaxPooling(2,2)</strong> over a convolution map:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/1fa08f01-0b68-450b-9f82-d8cca463a995.png" style="width:32.17em;height:14.42em;"/><br/>
<br/>
 Max pooling at work</div>
<p>Even in the simple preceding diagram, you can quickly appreciate that pooling loses the spatial relationship of the corner (upper-left, bottom-left, lower-right and upper-right) the pooled value started in. Note that, after a couple layers of pooling, any sense of spatial relation will be completely gone.</p>
<p>We can test the effect of removing pooling layers from the model and test this again by following these steps:</p>
<ol>
<li>Open the <kbd>Chapter_2_3.py</kbd> file and note how we commented out a couple of pooling layers, or you can just delete the lines as well, like so:</li>
</ol>
<pre style="padding-left: 60px">x = Convolution2D(8, 3, 3)(img_in)<br/>x = Activation('relu')(x)<br/><strong>x = MaxPooling2D(pool_size=(2, 2))(x)</strong><br/><br/>x = Convolution2D(16, 3, 3)(x)<br/>x = Activation('relu')(x)<br/><strong>#x = MaxPooling2D(pool_size=(2, 2))(x)</strong><br/><br/>x = Convolution2D(32, 3, 3)(x)<br/>x = Activation('relu')(x)<br/><strong>#x = MaxPooling2D(pool_size=(2, 2))(x)</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Note how we didn't comment out (or delete) all the pooling layers and left one in. In some cases, you may still want to leave a couple of pooling layers in, perhaps to identify features that are not spatially important. For example, when recognizing digits, space is less important with respect to the overall shape. However, if we consider recognizing a face, then the distance between a person's eyes, mouth, and so on, is what distinguishes a face from another face. However, if you just wanted to identify a face, with eyes, mouth, and so on, then just applying pooling could be quite acceptable.</li>
<li>Next, we also increase the dropout rate on our <kbd>Dropout</kbd> layer like so:</li>
</ol>
<pre style="padding-left: 60px">x = Dropout(.5)(x)</pre>
<ol start="4">
<li>We will explore dropout in some detail in the next section. For now, though, just realize that this change will have a more positive effect on our model.</li>
<li>Lastly, we bump up the number of epochs to <kbd>10</kbd> with the following code:</li>
</ol>
<pre style="padding-left: 60px">model.fit(train_X, train_Y, batch_size=64, <strong>epochs=10</strong>, validation_data=(val_X, val_Y), callbacks=callbacks_list)</pre>
<ol start="6">
<li>In our previous run, if you were watching the loss rate when training, you would realize the last example more or less started to converge at four epochs. Since dropping the pooling layers also reduces the training data, we need to also bump up the number of epochs. Remember, pooling or down-sampling increases the number of feature maps, and fewer maps means the network needs more training runs. If you are not training on a GPU, this model will take a while, so be patient.</li>
<li>Finally, run the example, again with those minor changes. One of the first things you will notice is that the training time shoots up dramatically. Remember, this is because our pooling layers do facilitate quicker training, but at a cost. This is one of the reasons we allow for a single pooling layer.</li>
<li>When the sample is finished running, compare the results for the <kbd>Chapter_2_2.py</kbd> sample we ran earlier. Did it do what you expected it to?</li>
</ol>
<div class="packt_infobox">We only focus on this particular blog post because it is extremely well presented and well written. The author obviously knew his stuff, but this example just shows how important it is to understand the fundamentals of these concepts in as much detail as you can handle. This is not such an easy task with the flood of information, but this also reinforces the fact that developing working deep learning models is not a trivial task, at least not yet.</div>
<p class="mce-root"/>
<p>Now that we understand the cost/penalty of pooling layers, we can move on to the next section, where we jump back to understanding <kbd>Dropout</kbd>. It is an excellent tool you will use over and over again.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The need for Dropout</h1>
                </header>
            
            <article>
                
<p>Now, let's go back to our much-needed discussion about <kbd>Dropout</kbd>. We use dropout in deep learning as a way of randomly cutting network connections between layers during each iteration. An example showing an iteration of dropout being applied to three network layers is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-medium wp-image-729 image-border" src="assets/0588924a-6516-4bc0-8f6f-e217d96e8af1.png" style="width:25.00em;height:13.50em;"/><br/>
<br/>
Before and after dropout</div>
<p>The important thing to understand is that the same connections are not always cut. This is done to allow the network to become less specialized and more generalized. Generalizing a model is a common theme in deep learning, and we often do this so our models can learn a broader set of problems, more quickly. Of course, there may be times where generalizing a network limits a network's ability to learn.</p>
<p>If we go back to the previous sample now and look at the code, we see a <kbd>Dropout</kbd> layer being used like so:</p>
<pre>x = Dropout(.5)(x)</pre>
<p>That simple line of code tells the network to drop out or disconnect 50% of the connections randomly after every iteration. Dropout only works for fully connected layers (<strong>Input</strong> -&gt; <strong>Dense</strong> -&gt; <strong>Dense</strong>) but is very useful as a way of improving performance or accuracy. This may or may not account for some of the improved performance from the previous example.</p>
<p>In the next section, we will look at how deep learning mimics the memory sub-process or temporal scent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Memory and recurrent networks</h1>
                </header>
            
            <article>
                
<p>Memory is often associated with <strong>Recurrent Neural Network</strong> (<strong>RNN</strong>), but that is not entirely an accurate association. An RNN is really only useful for storing a sequence of events or what you may refer to as a <strong>temporal sense</strong>, a sense of time if you will. RNNs do this by persisting state back onto itself in a recursive or recurrent loop. An example of how this looks is shown here:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/e37e1983-cc7b-4826-818a-aa84968820fb.png" style="width:43.67em;height:14.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Unfolded recurrent neural network</div>
<p>What the diagram shows is the internal representation of a recurrent neuron that is set to track a number of time steps or iterations where <strong>x</strong> represents the input at a time step and <strong>h</strong> denotes the state. The network weights of <strong>W</strong>, <strong>U</strong>, and <strong>V</strong> remain the same for all time steps and are trained using a technique called <strong>Backpropagation Through Time</strong> (<strong>BPTT</strong>). We won't go into the math of BPTT and leave that up the reader to discover on their own, but just realize that the network weights in a recurrent network use a cost gradient method to optimize them. </p>
<p>A recurrent network allows a neural network to identify sequences of elements and predict what elements typically come next. This has huge applications in predicting text, stocks, and of course games. Pretty much any activity that can benefit from some grasp of time or sequence of events will benefit from using RNN, except standard RNN, the type shown previously, which fails to predict longer sequences due to a problem with gradients. We will get further into this problem and the solution in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Vanishing and exploding gradients rescued by LSTM</h1>
                </header>
            
            <article>
                
<p>The problem the RNN suffers from is either vanishing or exploding gradients. This happens because, over time, the gradient we try to minimize or reduce becomes so small or big that any additional training has no effect. This limits the usefulness of the RNN, but fortunately this problem was corrected with <strong>Long Short-Term Memory</strong><em> (</em><strong>LSTM</strong>) blocks, as shown in this diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/c2cfd7cf-6e48-4993-8751-2d580fc8901f.png" style="width:47.58em;height:21.17em;"/><br/>
<br/>
Example of an LSTM block</div>
<p>LSTM blocks overcome the vanishing gradient problem using a few techniques. Internally, in the diagram where you see a <strong>x</strong> inside a circle, it denotes a gate controlled by an activation function. In the diagram, the activation functions are <strong>σ</strong> and <strong>tanh</strong>. These activation functions work much like a step or ReLU do, and we may use either function for activation in a regular network layer. For the most part, we will treat an LSTM as a black box, and all you need to remember is that LSTMs overcome the gradient problem of RNN and can remember long-term sequences.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's take a look at a working example to see how this comes together. Open up <kbd>Chapter_2_4.py</kbd> and follow the these steps:</p>
<ol>
<li>We begin as per usual by importing the various Keras pieces we need, as shown:</li>
</ol>
<div class="packt_infobox"><span>This example was pulled from </span><a href="https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/">https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/</a><span>. This is a site hosted by </span><strong>Dr. Jason Brownlee</strong><span>, who has plenty more excellent examples explaining the use of LSTM and recurrent networks.  </span></div>
<pre style="padding-left: 60px">import numpy<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import LSTM<br/>from keras.utils import np_utils</pre>
<ol start="2">
<li>This time we are importing two new classes, <kbd>Sequential</kbd> and <kbd>LSTM</kbd>. Of course we know what <kbd>LSTM</kbd> is for, but what about <kbd>Sequential</kbd>? <kbd>Sequential</kbd> is a form of model that defines the layers in a sequence one after another. We were less worried about this detail before, since our previous models were all sequential.</li>
<li>Next, we set the random seed to a known value. We do this so that our example can replicate itself. You may have noticed in previous examples that not all runs perform the same. In many cases, we want our training to be consistent, and hence we set a known seed value by using this code:</li>
</ol>
<pre style="padding-left: 60px">numpy.random.seed(7)</pre>
<ol start="4">
<li>It is important to realize that this just sets the <kbd>numpy</kbd> random seed value. Other libraries may use different random number generators and require different seed settings. We will try to identify these inconsistencies in the future when possible.</li>
<li>Next, we need to identify a sequence we will train to; in this case, we will just use the <kbd>alphabet</kbd> as shown in this code:</li>
</ol>
<pre style="padding-left: 60px">alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"<br/><br/>char_to_int = dict((c, i) for i, c in enumerate(alphabet))<br/>int_to_char = dict((i, c) for i, c in enumerate(alphabet))<br/><br/>seq_length = 1<br/>dataX = []<br/>dataY = []<br/><br/>for i in range(0, len(alphabet) - seq_length, 1):<br/>  seq_in = alphabet[i:i + seq_length]<br/>  seq_out = alphabet[i + seq_length]<br/>  dataX.append([char_to_int[char] for char in seq_in])<br/>  dataY.append(char_to_int[seq_out])<br/>  print(seq_in, '-&gt;', seq_out)</pre>
<ol start="6">
<li>The preceding code builds our sequence of characters as integers and builds a map of each character sequence. It builds a <kbd>seq_in</kbd> and <kbd>seq_out</kbd> showing the forward and reverse positions. Since the length of a sequence is defined by <kbd>seq_length = 1</kbd>, then we are only concerned about a letter of the alphabet and the character that comes after it. You could, of course, do longer sequences.</li>
<li>With the sequence data built, it is time to shape the data and normalize it with this code:</li>
</ol>
<pre style="padding-left: 60px">X = numpy.reshape(dataX, (len(dataX), seq_length, 1))<br/># normalize<br/>X = X / float(len(alphabet))<br/># one hot encode the output variable<br/>y = np_utils.to_categorical(dataY)</pre>
<ol start="8">
<li>The first line in the preceding code reshapes the data into a tensor with a size length of <kbd>dataX</kbd>, the number of steps or sequences, and the number of features to identify. We then normalize the data. Normalizing the data comes in many forms, but in this case we are normalizing values from 0 to 1. Then we one hot encode the output for easier training.</li>
</ol>
<div class="packt_infobox">One hot encoding is where we you set the value to 1 where you have data or a response, and to zero everywhere else. In the example, our model output is 26 neurons, which could also be represented by 26 zeros, one zero for each neuron, like so:<br/>
<strong>00000000000000000000000000<br/></strong><br/>
Where each zero represents the matching character position in the alphabet. If we wanted to denote a character <strong>A</strong>, we would output the one hot encoded value as this:<br/>
<strong>10000000000000000000000000</strong></div>
<p> </p>
<ol start="9">
<li>Then we construct the model, using a slightly different form of code than we have seen before and as shown here:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/><strong>model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))</strong><br/>model.add(Dense(y.shape[1], activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>model.fit(X, y, epochs=500, batch_size=1, verbose=2)<br/><br/>scores = model.evaluate(X, y, verbose=0)<br/>print("Model Accuracy: %.2f%%" % (scores[1]*100))</pre>
<ol start="10">
<li>The critical piece to the preceding code is the highlighted line that shows the construction of the <kbd>LSTM</kbd> layer. We construct an <kbd>LSTM</kbd> layer by setting the number of units, in this case <kbd>32</kbd>, since our sequence is 26 characters long and we want our units disable by <kbd>2</kbd>. Then we set the <kbd>input_shape</kbd> to match the previous tensor, <kbd>X</kbd>, that we created to hold our training data. In this case, we are just setting the shape to match all the characters (26) and the sequence length, in this case <kbd>1</kbd>.</li>
<li>Finally, we output the model with the following code:</li>
</ol>
<pre style="padding-left: 60px">for pattern in dataX:<br/>  x = numpy.reshape(pattern, (1, len(pattern), 1))<br/>  x = x / float(len(alphabet))<br/>  prediction = model.predict(x, verbose=0)<br/>  index = numpy.argmax(prediction)<br/>  result = int_to_char[index]<br/>  seq_in = [int_to_char[value] for value in pattern]<br/>  print(seq_in, "-&gt;", result)</pre>
<ol start="12">
<li>Run the code as you normally would and examine the output. You will notice that the accuracy is around 80%. See whether you can improve the accuracy of the model for predicting the next sequence in the alphabet.</li>
</ol>
<p>This simple example demonstrated the basic use of an LSTM block for recognizing a simple sequence. In the next section, we look at a more complex example: using LSTM to play Rock, Paper, Scissors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Playing Rock, Paper, Scissors with LSTMs</h1>
                </header>
            
            <article>
                
<p>Remembering sequences of data have huge applications in many areas, not the least of which includes gaming. Of course, producing a simple, clean example is another matter. Fortunately, examples abound on the internet and <kbd>Chapter_2_5.py</kbd> shows an example of using an LSTM to play Rock, Paper, Scissors.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Open up that sample file and follow these steps:</p>
<div class="packt_infobox"><span>This example was pulled from <a href="https://github.com/hjpulkki/RPS">https://github.com/hjpulkki/RPS</a>, but the code needed to be tweaked in several places to get it to work for us.  </span></div>
<ol>
<li>Let's start as we normally do with the imports. For this sample, be sure to have Keras installed as we did for the last set of exercises:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>from keras.utils import np_utils<br/>from keras.models import Sequential<br/>from keras.layers import Dense, LSTM</pre>
<ol start="2">
<li>Then, we set some constants as shown:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">EPOCH_NP = 100<br/>INPUT_SHAPE = (1, -1, 1)<br/>OUTPUT_SHAPE = (1, -1, 3)<br/>DATA_FILE = "data.txt"<br/>MODEL_FILE = "RPS_model.h5"</pre>
<ol start="3">
<li><span>Then, we build the model, this time with three LSTM layers, one for each element in our sequence (rock, paper and scissors), like so:</span></li>
</ol>
<pre class="mce-root" style="padding-left: 60px">def simple_model(): <br/>  new_model = Sequential()<br/>  new_model.add(LSTM(output_dim=64, input_dim=1, return_sequences=True, activation='sigmoid'))<br/>  new_model.add(LSTM(output_dim=64, return_sequences=True, activation='sigmoid'))<br/>  new_model.add(LSTM(output_dim=64, return_sequences=True, activation='sigmoid'))<br/>  new_model.add(Dense(64, activation='relu'))<br/>  new_model.add(Dense(64, activation='relu'))<br/>  new_model.add(Dense(3, activation='softmax'))<br/>  new_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'categorical_crossentropy'])<br/>  return new_model</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Then we create a function to extract our data from the <kbd>data.txt</kbd> file. This file holds the sequences of training data using the following code:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">def batch_generator(filename): <br/>  with open('data.txt', 'r') as data_file:<br/>    for line in data_file:<br/>      data_vector = np.array(list(line[:-1]))<br/>      input_data = data_vector[np.newaxis, :-1, np.newaxis]<br/>      temp = np_utils.to_categorical(data_vector, num_classes=3) <br/>      output_data = temp[np.newaxis, 1:]<br/>      yield (input_data, output_data)</pre>
<ol start="5">
<li>In this example, we are training each block of training through 100 epochs in the same order as they are in the file. A better method would be to train each training sequence in a random order.</li>
<li>Then we create the model:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># Create model<br/>np.random.seed(7)<br/>model = simple_model()</pre>
<ol start="7">
<li>Train the data using a loop, with each iteration pulling a batch from the <kbd>data.txt</kbd> file:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">for (input_data, output_data) in batch_generator('data.txt'):<br/>  try:<br/>    model.fit(input_data, output_data, epochs=100, batch_size=100)<br/>  except:<br/>    print("error")</pre>
<ol start="8">
<li>Finally, we evaluate the results with a validation sequence as shown in this code:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">print("evaluating")<br/>validation = '100101000110221110101002201101101101002201011012222210221011011101011122110010101010101'<br/>input_validation = np.array(list(validation[:-1])).reshape(INPUT_SHAPE)<br/>output_validation = np_utils.to_categorical(np.array(list(validation[1:]))).reshape(OUTPUT_SHAPE)<br/>loss_and_metrics = model.evaluate(input_validation, output_validation, batch_size=100)<br/><br/>print("\n Evaluation results")<br/><br/>for i in range(len(loss_and_metrics)):<br/>  print(model.metrics_names[i], loss_and_metrics[i])<br/><br/>input_test = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]).reshape(INPUT_SHAPE)<br/>res = model.predict(input_test)<br/>prediction = np.argmax(res[0], axis=1)<br/>print(res, prediction)<br/><br/>model.save(MODEL_FILE)<br/>del model</pre>
<ol start="9">
<li>Run the sample as you normally would. Check the results at the end and note how accurate the model gets at predicting the sequence.</li>
</ol>
<p>Be sure to run through this simple example a few times and understand how the LSTM layers are set up. Pay special attention to the parameters and how they are set.</p>
<p>That concludes our quick look at understanding how to use recurrent aka LSTM blocks for recognizing and predicting sequences of data. We will of course use this versatile layer type many more times throughout the course of this book.</p>
<p>In the final section of this chapter, we again showcase a number of exercises you are encouraged to undertake for your own benefit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>Complete the following exercises in your own time and to improve your own learning experience. Improving your understanding of the material will make you a more successful deep learner, and you will likely enjoy this book better as well:</p>
<ol>
<li>In the <kbd>Chapter_2_1.py</kbd> example, change the <kbd>Conv2D</kbd> layers to use a different filter size. Run the sample again, and see what effect this has on training performance and accuracy.</li>
<li>Comment out or delete a couple of the <kbd>MaxPooling</kbd> layers and corresponding <kbd>UpSampling</kbd> layers in the <kbd>Chapter_2_1.py</kbd> example. Remember, if you remove a pooling layer between layers 2 and 3, you likewise need to remove the up-sampling to remain consistent. Run the sample again, and see what effect this has on training time, accuracy, and performance.</li>
<li>Alter the <strong>Conv2D</strong> layers in the <kbd>Chapter_2_2.py</kbd> example using a different filter size. See what effect this has on training.</li>
</ol>
<ol start="4">
<li>Alter the <strong>Conv2D</strong> layers in the <kbd>Chapter_2_2.py</kbd> example by using a stride value of <strong>2</strong>. You may need to consult the <strong>Keras</strong> docs in order to do this. See what effect this has on training.</li>
<li>Alter the <strong>MaxPooling</strong> layers in the <kbd>Chapter_2_2.py</kbd> example by altering the pooling dimensions. See what effect this has on training.</li>
<li> Remove all or comment out different <strong>MaxPooling</strong> layers used in the <kbd>Chapter_2_3.py</kbd> example. What happens if all the pooling layers are commented out? Do you need to increase the training epochs now?</li>
<li>Alter the use of <strong>Dropout</strong> in the various examples used throughout this chapter. This includes adding dropout. Test the effects of using different levels of dropout.</li>
<li>Modify the sample in <kbd>Chapter_2_4.py</kbd> so that the model produces better accuracy. What do you need to do in order to improve training performance?</li>
<li>Modify the sample in <kbd>Chapter_2_4.py</kbd> to predict more than one character in the sequence. If you need help, go back and review the original blog post for more information.</li>
<li>What happens if you change the number of units that the three <strong>LSTM</strong> layers use in the <kbd>Chapter_2_5.py</kbd> example? What if you increase the value to 128, 32, or 16? Try these values to understand the effect they have.</li>
</ol>
<p>Feel free to expand on these exercises on your own. Try to write a new example on your own as well, even if it is just a simple one. There really is no better way to learn to code than to write your own.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>For this chapter and the last, we took a deep dive into the core elements of deep learning and neural networks. While our review in the last couple chapters was not extensive, it should give you a good base for continuing through the rest of the book. If you had troubles with any of the material in the first two chapters, turn back now and spend more time reviewing the previous material. It is important that you understand the basics of neural network architecture and the use of various specialized layers, as we covered in this chapter (CNN and RNN). Be sure you understand the basics of CNN and how to use it effectively in picking features and what the trade—offs are when using pooling or sub sampling. Also understand the concept of RNN and how and when to use LSTM blocks for predicting or detecting temporal events. Convolutional layers and LSTM blocks are now fundamental components of deep learning, and we will use them in several networks we build going forward.  </p>
<p>In the next chapter, we start to build out our sample game for this book and introduce GANs, or generative adversarial networks. We will explore GANs and how they can be used to generate game content.</p>


            </article>

            
        </section>
    </body></html>