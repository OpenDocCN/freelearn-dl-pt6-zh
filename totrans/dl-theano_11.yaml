- en: Chapter 11. Learning from the Environment with Reinforcement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supervised and unsupervised learning describe the presence or the absence of
    labels or targets during training. A more natural learning environment for an
    agent is to receive rewards when the correct decision has been taken. This reward,
    such as *playing correctly tennis* for example, may be attributed in a complex
    environment, and the result of multiple actions, delayed or cumulative.
  prefs: []
  type: TYPE_NORMAL
- en: In order to optimize the reward from the environment for an artificial agent,
    the **Reinforcement Learning** (**RL**) field has seen the emergence of many algorithms,
    such as Q-learning, or Monte Carlo Tree Search, and with the advent of deep learning,
    these algorithms have been revised into new methods, such as deep-Q-networks,
    policy networks, value networks, and policy gradients.
  prefs: []
  type: TYPE_NORMAL
- en: We'll begin with a presentation of the reinforcement learning frame, and its
    potential application to virtual environments. Then, we'll develop its algorithms
    and their integration with deep learning, which has solved the most challenging
    problems in artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The points covered in this chapter are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulation environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monte Carlo Tree Search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Q-networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asynchronous gradient descents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To simplify the development of our neural nets in this chapter, we'll use Keras,
    the high level deep learning library on top of Theano I presented in [Chapter
    5](part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b "Chapter 5. Analyzing
    Sentiment with a Bidirectional LSTM"), *Analyzing Sentiment with a Bidirectional
    LSTM*.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning consists of training an **agent,** that just needs occasional
    feedback from the **environment**, to learn to get the best feedback at the end.
    The agent performs **actions**, modifying the **state** of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actions to navigate in the environment can be represented as directed edges
    from one state to another state as a graph, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement learning tasks](img/00201.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A robot, working in a real environment (walking robots, control of motors,
    and so on) or a virtual environment (video game, online games, chat room, and
    so on) has to decide which movements (or keys to strike) to receive the maximum
    reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reinforcement learning tasks](img/00202.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Simulation environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Virtual environments make it possible to simulate thousands to millions of gameplays,
    at no other cost than the computations. For the purpose of benchmarking different
    reinforcement learning algorithms, simulation environments have been developed
    by the research community.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to find the solutions that generalize well, the Open-AI non-profit
    artificial intelligence research company, associated with business magnate Elon
    Musk, that aims to carefully promote and develop friendly AI in such a way as
    to benefit humanity as a whole, has gathered in its open source simulation environment,
    **Open-AI Gym** ([https://gym.openai.com/](https://gym.openai.com/)), a collection
    of reinforcement tasks and environments in a Python toolkit to test our own approaches
    on them. Among these environments, you''ll find:'
  prefs: []
  type: TYPE_NORMAL
- en: Video games from Atari 2600, a home video game console released by Atari Inc
    in 1977, wrapping the simulator from the Arcade Learning Environment, one of the
    most common RL benchmark environment:![Simulation environments](img/00203.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MuJoCo, a physics simulator for evaluating agents on continuous control tasks:![Simulation
    environments](img/00204.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other well-known games such as Minecraft, Soccer, Doom, and many others:![Simulation
    environments](img/00205.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s install Gym and its Atari 2600 environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to install all environments with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Interacting with the gym environment is pretty simple with the `env.step()`
    method that, given an action we choose for the agent, returns the new state, the
    reward, and whether the game has terminated.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s sample a random action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Gym also provides sophisticated monitoring methods, to record videos and algorithm
    performance. The records can be uploaded to Open-AI API for scoring with other
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'One might also look at:'
  prefs: []
  type: TYPE_NORMAL
- en: 3D car racing simulator Torcs ([http://torcs.sourceforge.net/](http://torcs.sourceforge.net/)),
    which is more realistic with smaller discretization of actions, but with less
    sparse rewards than simple Atari games, and also fewer possible actions than continuous
    motor control in MuJoCo:![Simulation environments](img/00206.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3D environment called Labyrinth for randomly generated mazes:![Simulation environments](img/00207.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A major approach to solve games has been the Q-learning approach. In order
    to fully understand the approach, a basic example will illustrate a simplistic
    case where the number of states of the environment is limited to 6, state **0**
    is the entrance, state **5** is the exit. At each stage, some actions make it
    possible to jump to another state, as described in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-learning](img/00208.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The reward is, let's say, 100, when the agent leaves state **4** to state **5**.
    There isn't any other reward for other states since the goal of the game in this
    example is to find the exit. The reward is time-delayed and the agent has to scroll
    through multiple states from state 0 to state 4 to find the exit.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, Q-learning consists of learning a matrix Q, representing the
    **value of a state-action pair**:'
  prefs: []
  type: TYPE_NORMAL
- en: Each row in the Q-matrix corresponds to a state the agent would be in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each column the target state from that state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the value representing how much choosing that action in that state will move
    us close to the exit. If there isn't any action from state *i* leading to state
    *j*, we define a zero or negative value at position *(i,j)* in the Q-matrix. If
    there are one or more possible actions from state *i* to state *j*, then the value
    in the Q-matrix will be chosen to represent how state *j* will help us to achieve
    our goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, leaving state **3** for state **0**, will move the agent away
    from the exit, while leaving state **3** for state **4** gets us closer to the
    goal. A commonly used algorithm, known as a *greedy* algorithm, to estimate **Q**
    in the discrete space, is given by the recursive *Bellman equation* which is demonstrated
    to converge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-learning](img/00209.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *S''* is the new state when taking action *a* on state *S*; *r* defines
    the reward on the path from state *S* to *S''* (in this case it is null) and ![Q-learning](img/00210.jpeg)
    is the discounting factor to discourage actions to states too far in the graph.
    The application of this equation multiple times will result in the following Q
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-learning](img/00211.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In Q-learning, *Q* stands for *quality* representing the power of the action
    to get the best rewards. Since late rewards are discounted, the values correspond
    to **maximum discounted future rewards** for each (state, action) couple.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the full graph outcome is not required as soon as we know the **state
    values** for the output nodes of the search subtree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-learning](img/00212.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this figure, the value **10** for nodes **1** and **3** are the **optimal
    state value function v(s);** that is, the outcome of a game under perfect play
    / optimal path. In practice, the exact value function is not known but approximated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such an approximation is used in combination with a **Monte Carlo Tree Search**
    (**MCTS**) in the **DeepMind** algorithm **AlphaGo** to beat the world champion
    in Go. MCTS consists of sampling actions given a policy, so that only the most
    likely actions from the current node to estimate its Q-value are retained in the
    Bellman equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-learning](img/00213.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Deep Q-network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the number of possible actions is usually limited (number of keyboard
    keys or movements), the number of possible states can be dramatically huge, the
    search space can be enormous, for example, in the case of a robot equipped with
    cameras in a real-world environment or a realistic video game. It becomes natural
    to use a computer vision neural net, such as the ones we used for classification
    in [Chapter 7](part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 7. Classifying Images with Residual Networks"), *Classifying Images with
    Residual Networks*, to represent the value of an action given an input image (the
    state), instead of a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Q-network](img/00214.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Q-network is called a **state-action value network** and predicts action
    values given a state. To train the Q-network, one natural way of doing it is to
    have it fit the Bellman equation via gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Q-network](img/00215.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that, ![Deep Q-network](img/00216.jpeg) is evaluated and fixed, while the
    descent is computed for the derivatives in, ![Deep Q-network](img/00217.jpeg)
    and that the value of each state can be estimated as the maximum of all state-action
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'After initializing the Q-network with random weights, the initial predictions
    are random, but as the network converges, the action given a particular state
    will become more and more predictable, so the exploration of new states drops.
    Exploiting a model trained online requires the forcing of the algorithm to **continue
    to explore**: the ![Deep Q-network](img/00218.jpeg) **greedy approach** consists
    of doing a random action with a probability epsilon, otherwise following the maximum-reward
    action given by the Q-network. It is a kind of learning by trial-and-error. After
    a certain number of epochs, ![Deep Q-network](img/00218.jpeg) is decayed to reduce
    exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: Training stability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Different methods are possible to improve stability during training. **Online
    training**, that is, training the model while playing the game, forgetting previous
    experiences, just considering the last one, is fundamentally unstable with deep
    neural networks: states that are close in time, such as the most recent states,
    are usually strongly similar or correlated, and taking the most recent states
    during training does not converge well.'
  prefs: []
  type: TYPE_NORMAL
- en: To avoid such a failure, one possible solution has been to store the experiences
    in a **replay memory** or to use a database of human gameplays. Batching and shuffling
    random samples from the replay memory or the human gameplay database leads to
    more stable training, but **off-policy** training.
  prefs: []
  type: TYPE_NORMAL
- en: 'A second solution to improve stability is to fix the value of the parameter
    ![Training stability](img/00219.jpeg) in the **target evaluation** ![Training
    stability](img/00216.jpeg) for several thousands of updates of ![Training stability](img/00217.jpeg),
    reducing the correlations between the target and the Q-values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training stability](img/00220.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is possible to train more efficiently with n-steps Q-learning, propagating
    the rewards on *n* preceding actions instead of one:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q learning formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training stability](img/00221.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'n-steps Q-learning formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training stability](img/00222.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, each step will benefit from *n* next rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training stability](img/00223.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A last solution for training stability and efficiency is an **asynchronous
    gradient descent** with multiple agents executing in parallel, on multiple instances
    of the environment, with different exploration policies, so that each gradient
    update is less correlated: each learning agent runs in a different thread on the
    same machine, sharing its model and target model parameters with other agents,
    but computing the gradients for a different part of the environment. The parallel
    actor learners have a stabilization effect, enable on-policy reinforcement, a
    reduction in training time, and comparable performances on GPU or multi-core CPU,
    which is great!'
  prefs: []
  type: TYPE_NORMAL
- en: 'The stabilization effect leads to better **data efficiency**: the data efficiency
    is measured by the number of epochs (an epoch is when the full training dataset
    has been presented to the algorithm) required to converge to a desired training
    loss or accuracy. Total training time is impacted by data efficiency, parallelism
    (number of threads or machines), and the parallelism overhead (it is sublinear
    in the number of threads, given the number of cores, machines and algorithm distribution
    efficiency).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see it in practice. To implement multiple agents exploring different
    parts of the environment, we''ll run multiple processes with the Python multiprocessing
    module, one process for the model to update (GPU), and *n* processes for the agents
    exploring (CPU). The manager object of the multiprocessing module controls a server
    process holding the weights of the Q-network to share between processes. The communication
    channel to store the experiences of the agents and serve them once for the model
    update, is implemented with a process-safe queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's generate experiences and enqueue them in the common queue object.
  prefs: []
  type: TYPE_NORMAL
- en: 'For that purpose, where each agent creates its game environment, compile the
    Q-network and load the weights from the manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To generate one experience, the agent chooses an action and executes it in
    its environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Each experience by the agent is stored in a list until the game is terminated
    or the list is longer than *n_step*, to evaluate the state-action value with *n-steps*
    Q-learning :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once in a while, the agent updates its weights from the learning process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Let's see now how to update the weights in the learning agent.
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradients with REINFORCE algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The idea of **Policy Gradients** (**PG**) / REINFORCE algorithms is very simple:
    it consists in re-using the classification loss function in the case of reinforcement
    learning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s remember that the classification loss is given by the negative log likelihood,
    and minimizing it with a gradient descent follows the negative log-likelihood
    derivative with respect to the network weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients with REINFORCE algorithms](img/00224.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* is the select action, ![Policy gradients with REINFORCE algorithms](img/00225.jpeg)
    the predicted probability of this action given inputs X and weights ![Policy gradients
    with REINFORCE algorithms](img/00219.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: 'The REINFORCE theorem introduces the equivalent for reinforcement learning,
    where *r* is the reward. The following derivative:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients with REINFORCE algorithms](img/00226.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'represents an unbiased estimate of the derivative of the expected reward with
    respect to the network weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients with REINFORCE algorithms](img/00227.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, following the derivative will encourage the agent to maximize the reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a gradient descent enables us to optimize a **policy network** for our
    agents: a policy ![Policy gradients with REINFORCE algorithms](img/00228.jpeg)
    is a probability distribution over legal actions, to sample actions to execute
    during online learning, and can be approximated with a parameterized neural net
    as well.'
  prefs: []
  type: TYPE_NORMAL
- en: It is particularly useful in the continuous case, for example for motor control,
    where discretization of the action space might lead to some suboptimal artifacts
    and the maximization over an action-value network Q is not possible under infinite
    action space.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it is possible to enhance the policy network with recurrency (LSTM,
    GRU,) so that the agent selects its actions with respect to multiple previous
    states.
  prefs: []
  type: TYPE_NORMAL
- en: The REINFORCE theorem gives us a gradient descent to optimize the parametrized
    policy network. To encourage exploration in this policy-based case, it is also
    possible to add a regularization term, the entropy of the policy, to the loss
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under this policy, it is possible to compute the value of every state ![Policy
    gradients with REINFORCE algorithms](img/00229.jpeg):'
  prefs: []
  type: TYPE_NORMAL
- en: Either by playing the game from that state with the policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Or, if parameterized into a **state value network**, by gradient descent, the
    current parameter serving as target, as for the state-action value network seen
    in the previous section with discounted rewards:![Policy gradients with REINFORCE
    algorithms](img/00230.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This value is usually chosen as reinforcement baseline *b* to reduce the variance
    of the estimate of the policy gradient, and the Q-value can be used as the expected
    reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients with REINFORCE algorithms](img/00231.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The first factor in the REINFORCE derivative:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy gradients with REINFORCE algorithms](img/00232.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: is called the **advantage of action a in state** *s*.
  prefs: []
  type: TYPE_NORMAL
- en: Both gradient descents, for the policy network and for the value network, can
    be performed asynchronously with our parallel actor learners.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create our policy network and state value network, sharing their first
    layers, in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Our learning process builds the model as well, shares the weights to other
    processes, and compiles them for training with their respective losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The policy loss is a REINFORCE loss plus an entropy loss to encourage exploration.
    The value loss is a simple mean square error loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'De-queueing the experiences into a batch, our learning process trains the model
    on the batch and updates the weights dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the full code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Learning took about 24 hours.
  prefs: []
  type: TYPE_NORMAL
- en: A policy-based advantage actor critic usually outperforms value-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: Related articles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement
    Learning*, Ronald J. Williams, 1992'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Policy Gradient Methods for Reinforcement Learning with Function Approximation*,
    Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour, 1999'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Playing Atari with Deep Reinforcement Learning*, Volodymyr Mnih, Koray Kavukcuoglu,
    David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller,
    2013'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mastering the Game of Go with Deep Neural Networks and Tree Search*, David
    Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den
    Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc
    Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever,
    Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel & Demis Hassabis,
    2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Asynchronous Methods for Deep Reinforcement Learning*, Volodymyr Mnih, Adrià
    Puigdomènech Badia, Mehdi Mirza, Alex Graves, Tim Harley, Timothy P. LilliCrap,
    David Silver, Koray Kavukcuoglu, Feb 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Reinforcement Learning Radio Control and Signal Detection with KeRLym*,
    a Gym RL Agent Timothy J. O''Shea and T. Charles Clancy, 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning describes the tasks of optimizing an agent stumbling
    into rewards episodically. Online, offline, value-based, or policy-based algorithms
    have been developed with the help of deep neural networks for various games and
    simulation environments.
  prefs: []
  type: TYPE_NORMAL
- en: Policy-gradients are a brute-force solution that require the sampling of actions
    during training and are better suited for small action spaces, although they provide
    first solutions for continuous search spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Policy-gradients also work to train non-differentiable stochastic layers in
    a neural net and back propagate gradients through them. For example, when propagation
    through a model requires to sample following a parameterized submodel, gradients
    from the top layer can be considered as a reward for the bottom network.
  prefs: []
  type: TYPE_NORMAL
- en: In more complex environments, when there is no obvious reward (for example understanding
    and inferring possible actions from the objects present in the environment), reasoning
    helps humans optimize their actions, for which research does not provide any solution
    currently. Current RL algorithms are particularly suited for precise plays, fast
    reflexes, but no long term planning and reasoning. Also, RL algorithms require
    heavy datasets, which simulation environments provide easily. But this opens up
    the question of scaling in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll explore the latest solutions to generate new data
    undistinguishable from real-world data.
  prefs: []
  type: TYPE_NORMAL
