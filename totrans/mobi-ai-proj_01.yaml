- en: Artificial Intelligence Concepts and Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter acts as a prelude to the entire book and the concepts within it.
    We will understand these concepts at a level high enough for us to appreciate
    what we will be building throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by getting our head around the general structure of **Artificial
    Intelligence** (**AI**) and its building blocks by comparing AI, machine learning,
    and deep learning, as these terms can be used interchangeably. Then, we will skim
    through the history, evolution, and principles behind **Artificial Neural Networks **(**ANNs**).
    Later, we will dive into the fundamental concepts and terms of ANNs and deep learning
    that will be used throughout the book. After that, we take a brief look at the
    TensorFlow Playground to reinforce our understanding of ANNs. Finally, we will
    finish off the chapter with thoughts on where to get a deeper theoretical reference
    for the high-level concepts of the AI and ANN principles covered in this chapter,
    which will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: AI versus machine learning versus deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolution of AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mechanics behind ANNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biological neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working of artificial neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation and cost functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient descent, backpropagation, and softmax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Playground
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI versus machine learning versus deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**AI** is no new term given the plethora of articles we read online and the
    many movies based on it. So, before we proceed any further, let''s take a step
    back and understand AI and the terms that regularly accompany it from a practitioner''s
    point of view. We will get a clear distinction of what machine learning, deep
    learning, and AI are, as these terms are often used interchangeably:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d60ec346-e3a6-4851-bab9-25e39508cbcd.png)'
  prefs: []
  type: TYPE_IMG
- en: AI is the capability that can be embedded into machines that allows machines
    to perform tasks that are characteristic of human intelligence. These tasks include
    seeing and recognizing objects, listening and distinguishing sounds, understanding
    and comprehending language, and other similar tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) is a subset of AI that encompasses techniques
    used to make these human-like tasks possible. So, in a way, ML is what is used
    to achieve AI.'
  prefs: []
  type: TYPE_NORMAL
- en: In essence, if we did not use ML to achieve these tasks, then we would actually
    be trying to write millions of lines of code with complex loops, rules, and decision
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: ML gives machines the ability to learn without being explicitly programmed.
    So, instead of hardcoding rules for every possible scenario to a task, we simply
    provide examples of how the task is done versus how it should not be done. ML
    then trains the system on this provided data so it can learn for itself.
  prefs: []
  type: TYPE_NORMAL
- en: ML is an approach to AI where we can achieve tasks such as grouping or clustering,
    classifying, recommending, predicting, and forecasting data. Some common examples
    of this are classifying spam mail, stock market predictions, weather forecasting,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep learning** is a special technique in ML that emulates the human brain''s
    biological structure and works to accomplish human-like tasks. This is done by
    building a network of neurons just like in the brain through an algorithmic approach
    using ANNs, which are stack of algorithms that can solve problems at human-like
    efficiency or better.'
  prefs: []
  type: TYPE_NORMAL
- en: These layers are commonly referenced as **d****eepnets **(deep architectures)
    and each has a specific problem that it can be trained to solve. The deep learning
    space is currently at the cutting edge of what we see today, with applications
    such as autonomous driving, Alexa and Siri, machine vision, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we will be executing tasks and building apps that are
    built using these deepnets, and we will also solve use cases by building our very
    own deepnet architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To appreciate what we can currently do with AI, we need to get a basic understanding
    of how the idea of emulating the human brain was born, and how this idea evolved
    to a point where we can easily solve tasks in vision and language with human-like
    capability through machines.
  prefs: []
  type: TYPE_NORMAL
- en: It all started in 1959 when a couple of Harvard scientists, Hubel and Wiesel,
    were experimenting with a cat's visual system by monitoring the primary visual
    cortex in the cat's brain.
  prefs: []
  type: TYPE_NORMAL
- en: The **primary visual cortex** is a collection of neurons in the brain placed
    at the back of the skull and is responsible for processing vision. It is the first
    part of the brain that receives input signals from the eye, very much like how
    a human brain would process vision.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scientists started by showing complex pictures such as those of fish, dogs,
    and humans to the cat and observed its primary visual cortex. To their disappointment,
    they got no reading from the primary visual cortex initially. Consequently, to
    their surprise on one of the trials, as they were removing the slides, dark edges
    formed, causing some neurons to fire in the primary visual cortex:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8abe3823-b8a6-47b1-a11a-ac88a98fd276.png)'
  prefs: []
  type: TYPE_IMG
- en: Their serendipitous discovery was that these individual neurons or brain cells
    in the primary visual cortex were responding to bars or dark edges at various
    specific orientations. This led to the understanding that the mammalian brain
    processes a very small amount of information at every neuron, and as the information
    is passed from neuron to neuron, more complex shapes, edges, curves, and shades
    are comprehended. So, all these independent neurons holding very basic information
    need to fire together to comprehend a complete complex image.
  prefs: []
  type: TYPE_NORMAL
- en: After that, there was a lull in the progress of how to emulate the mammalian
    brain until 1980, when Fukushima proposed neocognitron. **Neocognitron** is inspired
    by the idea that we should be able to create an increasingly complex representation
    using a lot of very simplistic representations – just like the mammalian brain!
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a representation of how neocognitron works, by Fukushima:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/739593e1-54e0-414c-944d-30ddee2d1473.png)'
  prefs: []
  type: TYPE_IMG
- en: He proposed that to identify your grandmother, there are a lot of neurons that
    are triggered in the primary visual cortex, and each cell or neuron understands
    an abstract part of the final image of your grandmother. All of these neurons
    work in sequence, parallel, and tandem, and then finally hits a grandmother cell
    or neuron which fires only when it sees your grandmother.
  prefs: []
  type: TYPE_NORMAL
- en: Fast forward to today (2010-2018), with contributions from Yoshua Bengio, Yann
    LeCun, and Geoffrey Hinton, who are commonly known as the *fathers of deep learning*.
    They contribute massively to the AI space we work in today. They have given rise
    to a whole new approach to machine learning where feature engineering is automated.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of not explicitly telling the algorithm what it should be looking for
    and letting it figure this out by itself by feeding it a lot of examples is the
    latest development. The analogy to this principle would be that of teaching a
    child to distinguish between an apple and an orange. We would show the child pictures
    of apples and oranges rather than only describing the two fruits' features, such
    as shape, color, size, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the difference between ML and deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdfe861e-68f8-4f48-bdf4-a2be37b1ae07.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the primary difference between traditional ML and ML using neural networks
    (deep learning). In traditional ML, we provide features along with labels, but
    using ANNs, we let the algorithm decipher the features.
  prefs: []
  type: TYPE_NORMAL
- en: We live in an exciting time, an era we share with the fathers of deep learning,
    so much so that there are exchanges online in places such as Stack Exchange, where
    we can see contributions even from Yann LeCun and Geoffrey Hinton. This is analogous
    to living in the time of, and writing to, Nicholas Otto, the father of the internal
    combustion engine, who started the automobile revolution that we see evolving
    even to this day. The automobile revolution will be dwarfed by what could be possible
    with AI in the future. Exciting times, indeed!
  prefs: []
  type: TYPE_NORMAL
- en: The mechanics behind ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will understand the nuts and bolts that are required to
    start building our own AI projects. We will get to grips with the common terms
    that are used in deep learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: This section aims to provide the essential theory at a high level, giving you
    enough insight so that you're able to build your own deep neural networks, tune
    them, and understand what it takes to make state-of-the-art neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Biological neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We previously discussed how the biological brain has been an inspiration behind
    ANNs. The brain is made up of hundreds of billions of independent units or cells
    called **neurons**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts a **neuron**, and it has multiple inputs going
    into it, called **d****endrites**. There is also an output going out of the cell
    body, called the **a****xon**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78c9025b-fe58-45e9-8559-f948bf0fa9f3.png)'
  prefs: []
  type: TYPE_IMG
- en: The dendrites carry information into the neuron and the axon allows the processed
    information to flow out of the neuron. But in reality, there are thousands of
    dendrites feeding input into the neuron body as small electrical charges. If these
    small electrical charges that are carried by the dendrites have an effect on the
    overall charge of the body or cross over some threshold, then the axon will fire.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how a biological neuron functions, we will understand how an
    artificial neuron works.
  prefs: []
  type: TYPE_NORMAL
- en: Working of artificial neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just like the biological brain, ANNs are made up of independent units called
    neurons. Like the biological neuron, the artificial neuron has a body that does
    some computation and has many inputs that are feeding into the cell body or neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/823f3760-3563-4a25-bea7-485f97fd83ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, let''s assume we have three inputs to the neuron. Each input carries
    a binary value of 0 or 1\. We have an output flowing out of the body, which also
    carries a binary value of 0 or 1\. For this example, the neuron decides whether
    I should eat a cake today or not. That is, the neuron should fire an output of
    1 if I should eat a cake or fire 0 if I shouldn''t:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3646d7fd-ccd3-47cc-a909-2ade266b8aa1.png)'
  prefs: []
  type: TYPE_IMG
- en: In our example, the three inputs represent the three factors that determine
    whether I should eat the cake or not. Each factor is given a weight of importance;
    for instance, the first factor is **I did cardio yesterday** and it has a weight
    of 2\. The second factor is **I went to the gym yesterday** and weighs 3\. The
    third factor is **It is an occasion for cake** and weighs 6.
  prefs: []
  type: TYPE_NORMAL
- en: 'The body of the neuron does some calculation to inputs, such as taking the
    sum of all of these inputs and checking whether it is over some threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ace25c8d-9dd4-434e-b9e1-c8914bf0d60d.png)'
  prefs: []
  type: TYPE_IMG
- en: So, for this example, let's set our threshold as 4\. If the sum of the input
    weights is above the threshold, then the neuron fires an output of 1, indicating
    that I can eat the cake.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be expressed as an equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '** ![](img/6d3e02c1-3c31-484c-af2a-516841aeed6f.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Xi* is the first input factor, *I did cardio yesterday.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wi* is the weight of the first input factor, *Xi*. In our example, *Wi = 2*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Xii* is the second input factor, *I went to the gym yesterday*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wii* is the weight of the second input factor, *Xii*. In our example, *Wii =
    3*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Xiii* is the third input factor, *It is an occasion for cake*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wiii* is the weight of the third input factor, *Xiii*. In our example, *Wiii=
    6*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*threshold* is 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's use this neuron to decide whether I can eat a cake for three different
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I want to eat a cake and I went to the gym yesterday, but I did not do cardio,
    nor is it an occasion for cake:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b48ff741-766f-4233-8e04-ddcadf19ae0e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Xi* is the first input factor, *I did cardio yesterday*. Now, *Xi= 0,* as
    this is false.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wi* is the weight of the first input factor, *Xi*. In our example, *Wi= 2*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Xii* is the second input factor, *I went to the gym yesterday*. Now, *Xii =
    1,* as this is true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wii* is the weight of the second input factor, *Xii*. In our example, *Wii* =
    3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Xiii* is the third input factor, *It is an occasion for cake*. Now, *Xiii=
    0,* as this is false.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wiii* is the weight of the third input factor, *Xiii*. In our example, *Wiii* =
    6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*threshold* is 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We know that the neuron computes the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1cfb10c-000e-46eb-b95e-eca2430d94a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For scenario 1, the equation will translate to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eea9930f-44c4-49bc-ab8a-1175b1b98d9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is equal to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c72373a8-0c4a-4348-a339-a1bb34dee2ac.png)'
  prefs: []
  type: TYPE_IMG
- en: '*3 ≥ 4* is false, so it fires 0, which means I should not eat the cake.'
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I want to eat a cake and it''s my birthday, but I did not do cardio, nor did
    I go to the gym yesterday:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3b90c6b-70fb-4205-ae37-f004f3859436.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Xi* is the first input factor, *I did cardio yesterday*. Now, *Xi= 0,* as
    this factor is false.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wi* is the weight of the first input factor, *Xi*. In our example, *Wi= 2*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Xii* is the second input factor, *I went to the gym yesterday*. Now, *Xii =
    0,* as this factor is false.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wii* is the weight of the second input factor, *Xii*. In our example, *Wii* =
    3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Xiii* is the third input factor, *It is an occasion for cake*. Now, *Xiii=
    1,* this factor is true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wiii* is the weight of the third input factor, *Xiii*. In our example, *Wiii* =
    6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*threshold* is 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We know that the neuron computes the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f874d55-53b7-4829-a9be-6b010448f7ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For scenario 2, the equation will translate to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a277032-ef63-4868-b8d3-0e9101a89af3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b326d771-dc1f-4461-acc1-976ccc80bcaa.png)'
  prefs: []
  type: TYPE_IMG
- en: '*6 ≥ 4* is true, so this fires 1, which means I can eat the cake.'
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I want to eat a cake and I did cardio and went to the gym yesterday, but it
    is also not an occasion for cake:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9719159c-ae4d-4d84-bd40-40e2b9419f06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Xi* is the first input factor, *I did cardio yesterday*. Now, *Xi= 1,* as
    this factor is true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wi* is the weight of the first input factor, *Xi*. In our example, *Wi= 2*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Xii* is the second input factor, *I went to the gym yesterday*. Now, *Xii =
    1,* as this factor is true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wii* is the weight of the second input factor, *Xii*. In our example, *Wii* =
    3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Xiii* is the third input factor, *It is an occasion for cake*. Now, *Xiii=
    0,* as this factor is false.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wiii* is the weight of the third input factor, *Xiii*. In our example, *Wiii* =
    6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*threshold* is 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We know that the neuron computes the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1697414-06e7-4cf0-8b74-ac1759983538.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For scenario 3, the equation will translate to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e09f54b-fd7a-4f76-9c68-da19d034fefc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives us the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/920fd23a-247e-47fe-a27f-827f2a80209e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*5 ≥ 4* is true, so this fires 1, which means I can eat the cake.'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding three scenarios, we saw how a single artificial neuron works.
    This single unit is also called a **perceptron**. A perceptron essentially handles
    binary inputs, computes the sum, and then compares with a threshold to ultimately
    give a binary output.
  prefs: []
  type: TYPE_NORMAL
- en: To better appreciate how a perceptron works, we can translate our preceding
    equation into a more generalized form for the sake of explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume there is just one input factor, for simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/bec404cd-2afa-4e53-9147-ccf45328046c.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also assume that *threshold = b*. Our equation was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/9b74e4cc-32e8-4628-bef0-98d523e06c5d.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'It now becomes this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41971681-10e2-4ff7-a7b5-a7906f788a70.png)'
  prefs: []
  type: TYPE_IMG
- en: It can also be written as ![](img/d3f7ba54-6bfa-4cc9-a9b2-6ab7de85e569.png),
    then output *1*else *0**.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w* is the weight of the input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b* is the threshold and is referred to as the bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This rule summarizes how a perceptron neuron works.
  prefs: []
  type: TYPE_NORMAL
- en: Just like the mammalian brain, an ANN is made up of many such perceptions that
    are stacked and layered together. In the next section, we will get an understanding
    of how these neurons work together within an ANN.
  prefs: []
  type: TYPE_NORMAL
- en: ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like biological neurons, artificial neurons also do not exist on their own.
    They exist in a network with other neurons. Basically, the neurons exist by feeding
    information to each other; the outputs of some neurons are inputs to some other
    neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'In any ANN, the first layer is called the **Input Layer**. These inputs are
    real values, such as the factors with weights (*w.x*) in our previous example.
    The sum values from the input layer are propagated to each neuron in the next
    layer. The neurons of that layer do the computation and pass their output to the
    next layer, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d40f5cee-3ac8-47b8-8d00-5c206d33504a.png)'
  prefs: []
  type: TYPE_IMG
- en: The layer that receives input from all previous neurons and passes its output
    to all of the neurons of the next layer is called a **Dense **layer. As this layer
    is connected to all of the neurons of the previous and next layer, it is also
    commonly referred to as a **Fully Connected Layer**.
  prefs: []
  type: TYPE_NORMAL
- en: The input and computation flow from layer to layer and finally end at the **Output
    Layer**, which gives the end estimate of the whole ANN.
  prefs: []
  type: TYPE_NORMAL
- en: The layers in-between the input and the output layers are called the **Hidden
    Layers**, as the values of the neurons within these hidden layers are unknown
    and a complete black box to the practitioner.
  prefs: []
  type: TYPE_NORMAL
- en: As you increase the number of layers, you increase the abstraction of the network,
    which in turn increases the ability of the network to solve more complex problems.
    When there are over three hidden layers, then it is referred to as a deepnet.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if this was a machine vision task, then the first hidden layer would be
    looking for edges, the next would look for corners, the next for curves and simple
    shapes, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0dcb8f4-d1c7-41a7-b083-14c5f9e374e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, the complexity of the problem can determine the number of layers
    that are required; more layers lead to more abstractions. These layers can be
    very deep, with 1,000 or more layers, to very shallow, with just about half a
    dozen layers. Increasing the number of hidden layers does not necessarily give
    better results as the abstractions may be redundant.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen how artificial neurons can be stacked together to form
    a neural network. But we have seen that the perceptron neuron takes only binary
    input and gives only binary output. But in practice, there is a problem in doing
    things based on the perceptron's idea. This problem is addressed by activation
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now know that an ANN is created by stacking individual computing units called
    perceptrons. We have also seen how a perceptron works and have summarized it as *Output 1, IF***![](img/87250af0-6998-4cbb-b723-de4fc5b87ba3.png)**.
  prefs: []
  type: TYPE_NORMAL
- en: That is, it either outputs a *1* or a *0* depending on the values of the weight, *w*,
    and bias, *b*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following diagram to understand why there is a problem with
    just outputting either a *1* or a *0*. The following is a diagram of a simple
    perceptron with just a single input, *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b59c88c9-dbea-4dc0-8ef5-99ade5565357.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For simplicity, let''s call  ![](img/9105c1c0-e7e4-4706-8721-579dc2c88c01.png),
    where the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w* is the weight of the input, *x,* and *b* is the bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*a* is the output, which is either *1* or *0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, as the value of *z* changes, at some point, the output, *a*, changes
    from *0* to *1*. As you can see, the change in output *a* is sudden and drastic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0609f818-9b0f-4e5e-a00a-f2a2399e8303.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What this means is that for some small change, ![](img/3f95d5b7-7f37-417f-8d51-d3e966dd1a9f.png) ,
    we get a dramatic change in the output, *a*. This is not particularly helpful
    if the perceptron is part of a network, because if each perceptron has such drastic
    change, it makes the network unstable and hence the network fails to learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, to make the network more efficient and stable, we need to slow down
    the way each perceptron learns. In other words, we need to eliminate this sudden
    change in output from *0* to *1* to a more gradual change:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/558d13a2-28ee-4578-a130-9118cf059004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is made possible by activation functions. Activation functions are functions
    that are applied to a perceptron so that instead of outputting a *0* or a *1*,
    it outputs any value between *0* and *1*.
  prefs: []
  type: TYPE_NORMAL
- en: This means that each neuron can learn slower and at a greater level of detail
    by using smaller changes, ![](img/660fdf4e-3332-4f80-a224-575de3824318.png).  Activation
    functions can be looked at as transformation functions that are used to transform
    binary values in to a sequence of smaller values between a given minimum and maximum.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of ways to transform the binary outcomes to a sequence of
    values, namely the sigmoid function, the tanh function, and the ReLU function.
    We will have a quick look at each of these activation functions now.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **sigmoid function** is a function in mathematics that outputs a value
    between 0 and 1 for any input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0adb54a-93c5-414f-988e-c067850e6eb4.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/60bb9873-524f-45a9-84ba-8ac74caac355.png) and ![](img/f5076aa3-efc1-47e4-a41f-6ee338e6bfd8.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand sigmoid functions better with the help of some simple code.
    If you do not have Python installed, no problem: we will use an online alternative
    for now at [https://www.jdoodle.com/python-programming-online](https://www.jdoodle.com/python-programming-online). We
    will go through a complete setup from scratch in [Chapter 2](eb50909d-69fb-4228-967b-af2f58c543c3.xhtml),
    *Creating a Real-Estate Price Prediction Mobile App*. Right now, let''s quickly
    continue with the online alternative.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the page at [https://www.jdoodle.com/python-programming-online](https://www.jdoodle.com/python-programming-online) loaded,
    we can go through the code step by step and understand sigmoid functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the `math` library so that we can use the exponential
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s define a function called `sigmoid`, based on the earlier formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a scenario where our *z* is very small, `-10`. Therefore, the function
    outputs a number that is very small and close to 0:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If *z* is very large, such as `10000`, then the function will output the maximum possible value,
    1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, the sigmoid function transforms any value, *z*, to a value between
    0 and 1. When the sigmoid activation function is used on a neuron instead of the
    traditional perceptron algorithm, we get what is called a **sigmoid neuron**:'
  prefs: []
  type: TYPE_NORMAL
- en: Tanh function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to the sigmoid neuron, we can apply an activation function called tanh(*z*),
    which transforms any value to a value between -1 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The neuron that uses this activation function is called a **t****anh neuron**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2161bec9-1aae-4b92-91f7-21d0ee1ca56d.png)'
  prefs: []
  type: TYPE_IMG
- en: ReLU function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Then there is an activation function called the **Rectified Linear Unit**,
    **ReLU(z)**, that transforms any value, *z*, to 0 or a value above 0\. In other
    words, it outputs any value below 0 as 0 and any value above 0 as the value itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af0454a7-3151-4abf-a1af-da3be88ab020.png)'
  prefs: []
  type: TYPE_IMG
- en: Just to summarize our understanding so far, the perceptron is the traditional
    and outdated neuron that is rarely used in real implementations. They are great
    to get a simplistic understanding of the underlying principle; however, they had
    the problem of fast learning due to the drastic changes in output values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use activation functions to reduce the learning speed and determine finer
    changes in *z* or  ![](img/2469dc05-58c0-4c3f-848c-ab28a70b8385.png). Let''s sum
    up these activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: The **sigmoid neuron** is the neuron that uses the sigmoid activation function
    to transform the output to a value between 0 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **t****anh neuron** is the neuron that uses the tanh activation function
    to transform the output to a value between -1 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **ReLU neuron** is the neuron that uses the ReLU activation function to
    transform the output to a value of either 0 or any value above 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sigmoid function is used in practice but is slow compared to the tanh and
    ReLU functions. The tanh and ReLU functions are commonly used activation functions.
    The ReLU function is also considered state of the art and is usually the first
    choice of activation function that's used to build ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of commonly used activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67a40589-04e2-4744-b539-5d814a516ef2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the projects within this book, we will be primarily using either the sigmoid, tanh, or
    the ReLU neurons to build our ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Cost functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To quickly recap, we know how a basic perceptron works and its pitfalls. We
    then saw how activation functions overcame the perceptron's pitfalls, giving rise
    to other neuron types that are in use today.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are going to look at how we can tell when the neurons are wrong. For
    any type of neuron to learn, it needs to know when it outputs the wrong value
    and by what margin. The most common way to measure how wrong the neural network
    is, is to use a cost function.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **cost function** quantifies the difference between the output we get from
    a neuron to an output that we need from that neuron. There are two common types
    of cost functions that are used: mean squared error and cross entropy.'
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **mean squared error** (**MSE**) is also called a quadratic cost function
    as it uses the squared difference to measure the magnitude of the error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f61fab8-dc30-44bd-9b79-a6b70a6ca878.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a* is the output from the ANN'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y* is the expected output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*n* is the number of samples used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The cost function is pretty straightforward. For example, consider a single
    neuron with just one sample, (*n=1*). If the expected output is 2 (*y=2*) and
    the neuron outputs 3 (*a=3*), then the MSE is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c18a1923-6add-41b7-a4ae-43a0943bdb46.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/c7b65bad-5cd3-4ce3-b65a-70b26c4d8bbb.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/d14ec931-b364-4688-97ed-3661cdae3063.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, if the expected output is 3 (*y=3*) and the neuron outputs 2 (*a=2*),
    then the MSE is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1cbaaf60-0a6c-4b58-9fb8-cbbbc36a0acc.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4d35819d-c83d-4b5a-8f39-c195161ac485.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/39b26616-25dc-4fdf-b431-4b1762ed882a.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, the MSE quantifies the magnitude of the error made by the neuron.
    One of the issues with MSE is that when the values in the network get large, the
    learning becomes slow. In other words, when the weights (*w*) and bias (*b*) or
    *z* get large, the learning becomes very slow. Keep in mind that we are talking
    about thousands of neurons in an ANN, which is why the learning slows down and
    eventually stagnates with no further learning.
  prefs: []
  type: TYPE_NORMAL
- en: Cross entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Cross entropy** is a derivative-based function as it uses the derivative
    of a specially designed equation, which is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7e1d48b-f68b-4431-974a-dba3b24e7c85.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross entropy allows the network to learn faster when the difference between
    the expected and actual output is greater. In other words, the bigger the error,
    the faster it helps the network learn. We will get our heads around this using
    some simple code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like before, for now, you can use an online alternative if you do not have
    Python already installed, at [https://www.jdoodle.com/python-programming-online](https://www.jdoodle.com/python-programming-online).
    We will cover the installation and setup in [Chapter 2](eb50909d-69fb-4228-967b-af2f58c543c3.xhtml),
    *Creating a Real-Estate Price Prediction* *Mobile App*. Follow these steps to
    see how a network learns using cross entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the `math` library so that we can use the `log` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s define a function called `cross_enrtopy`, based on the preceding
    formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, consider a single neuron with just one sample, (*n=1*). Say the
    expected output is `0` (*y=0*) and the neuron outputs `0.01` (*a=0.01*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Since the expected and actual output values are very small, the resultant cost
    is very small.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, if the expected and actual output values are very large, then the
    resultant cost is still small:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, if the expected and actual output values are far apart, then the
    resultant cost is large:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, the larger the difference in expected versus actual output, the faster
    the learning becomes. Using cross entropy, we can get the error of the network,
    and at the same time, the magnitude of the weights and bias is irrelevant, helping
    the network learn faster.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have covered the different kind of neurons based on the activation
    functions that are used. We have covered the ways to quantify inaccuracy in the
    output of a neuron using cost functions. Now, we need a mechanism to take that
    inaccuracy and remedy it.
  prefs: []
  type: TYPE_NORMAL
- en: The mechanism through which the network can learn to output values closer to
    the expected or desired output is called **gradient descent**. Gradient descent is
    a common approach in machine learning for finding the lowest cost possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand gradient descent, let''s use the single neuron equation we have
    been using so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8dd1e82-570f-42cd-920e-4395ad993c09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x* is the input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w* is the weight of the input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b* is the bias of the input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gradient descent can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9278133-1d21-4301-b560-08a75e14b516.png)'
  prefs: []
  type: TYPE_IMG
- en: Initially, the neuron starts by assigning random values for *w* and *b*. From
    that point onward, the neuron needs to adjust the values of *w* and *b* sothat
    it lowers or decreases the error or cost (cross entropy).
  prefs: []
  type: TYPE_NORMAL
- en: Taking the derivative of the cross entropy (cost function) results in a step-by-step
    change in *w* and *b* in the direction of the lowest cost possible. In other words,
    **gradient descent** tries to find the finest line between the network output
    and expected output.
  prefs: []
  type: TYPE_NORMAL
- en: The weights are adjusted based on a parameter called the **l****earning rate.** The
    learning rate is the value that is adjusted to the weight of the neuron to get
    an output closer to the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that here, we have used only a single parameter; this is only to
    make things easier to comprehend. In reality, there are thousands upon millions
    of parameters that are taken into consideration to lower the cost.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation – a method for neural networks to learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Great! We have come a long way, from looking at the biological neuron, to the
    types of neuron, to determining accuracy, and correcting the learning of the neuron.
    Only one question remains: *how can the whole network of neurons learn together?*
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagation** is an incredibly smart approach to making gradient descent
    happen throughout the network across all layers. Backpropagation leverages the
    chain rule from calculus to make it possible to transfer information back and
    forth through the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2347ee42-e9b4-4236-8b0c-8dafa257cf50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In principle, the information from the input parameters and weights is propagated
    through the network to make a guess at the expected output and then the overall
    inaccuracy is backpropagated through the layers of the network so that the weights
    can be adjusted and the output can be guessed again.
  prefs: []
  type: TYPE_NORMAL
- en: This single cycle of learning is called a **t****raining step** or **i****teration**.
    Each iteration is performed on a batch of the input training samples. The number
    of samples in a batch is called **b****atch size**. When all of the input samples
    have been through an iteration or training step, then it is called an **epoch**.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's say there are 100 training samples and in every iteration
    or training step, there are 10 samples being used by the network to learn. Then,
    we can say that the batch size is 10 and it will take 10 iterations to complete
    a single epoch. Provided each batch has unique samples, that is, if every sample
    is used by the network at least once, then it is a single epoch.
  prefs: []
  type: TYPE_NORMAL
- en: This back-and-forth propagation of the predicted output and the cost through
    the network is how the network learns.
  prefs: []
  type: TYPE_NORMAL
- en: We will revisit training step, epoch, learning rate, cross entropy, batch size,
    and more during our hands-on sections.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have reached our final conceptual topic for this chapter. We've covered types
    of neurons, cost functions, gradient descent, and finally a mechanism to apply
    gradient descent across the network, making it possible to learn over repeated
    iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we saw the input layer and dense or hidden layers of an ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8dbb075-44b2-41c1-9380-a3ea3f8dd76d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Softmax** is a special kind of neuron that''s used in the output layer to
    describe the probability of the respective output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f904f31-079f-4eee-b8da-a970c52f121c.png)'
  prefs: []
  type: TYPE_IMG
- en: To understand the softmax equation and its concepts, we will be using some code.
    Like before, for now, you can use any online Python editor to follow the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the exponential methods from the `math` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For the sake of this example, let''s say that this network is designed to classify
    three possible labels: `A`, `B`, and `C`. Let''s say that there are three signals
    going into the softmax from the previous layers (-1, 1, 5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The explanation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first signal indicates that the output should be `A`, but is weak and is
    represented with a value of -1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second signal indicates that the output should be `B` and is slightly stronger
    and represented with a value of 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third signal is the strongest, indicating that the output should be `C`
    and is represented with a value of 5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These represented values are confidence measures of what the expected output
    should be.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take the numerator of the softmax for the first signal, guessing
    that the output is `A`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a354238f-684e-4657-b6b2-326b60172ab6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *M* is the output signal strength indicating that the output should be
    `A`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, there''s the numerator of the softmax for the second signal, guessing
    that the output is `B`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a354238f-684e-4657-b6b2-326b60172ab6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, `M` is the output signal strength indicating that the output should be
    `B`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, there''s the numerator of the softmax for the second signal, guessing
    that the output is `C`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc4b8b76-9465-424c-95f0-a7814745c5c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, `M` is the output signal strength indicating that the output should be `C`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We can observe that the represented confidence values are always placed above
    0 and that the resultant is made exponentially larger.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s interpret the denominator of the softmax function, which is a sum
    of the exponential of each signal value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75d8dee4-c88e-40b4-9a49-dc4e732088e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s write some code for softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, the probability that the first signal is correct is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This is less than a 1% chance that it is `A`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the probability that the third signal is correct is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This means there is over a 97% chance that the expected output is indeed `C`.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, the softmax accepts a weighted signal that indicates the confidence
    of some class prediction and outputs a probability score between 0 to 1 for all
    of those classes.
  prefs: []
  type: TYPE_NORMAL
- en: Great! We have made it through the essential high-level theory that's required
    to get us hands on with our projects. Next up, we will summarize our understanding
    of these concepts by exploring the TensorFlow Playground.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Playground
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get started with the TensorFlow Playground, let's recap the essential
    concepts quickly. It will help us appreciate the TensorFlow Playground better.
  prefs: []
  type: TYPE_NORMAL
- en: The inspiration for neural networks is the biological brain, and the smallest
    unit in the brain is a **neuron**.
  prefs: []
  type: TYPE_NORMAL
- en: A **P****erceptron** is a neuron based on the idea of the biological neuron.
    The perceptron basically deals with binary inputs and outputs, making it impractical
    for actual pragmatic purposes. Also, because of its binary nature, it learns too
    fast due to the drastic change in output for a small change in input, and so does
    not provide fine details.
  prefs: []
  type: TYPE_NORMAL
- en: '**Activation ****functions** were used to negate the issue with perceptrons.
    This gave rise to other types of neurons that deal with values between ranges
    of 0 to 1, -1 to 1, and so on, instead of just a 0 or a 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '**ANNs** are made up of these neurons stacked in layers. There is an input
    layer, a dense or fully connected layer, and an output layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost functions**, such as MSE and cross entropy, are ways to measure the
    magnitude of error in the output of a neuron.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient descent** is a mechanism through which a neuron can learn to output
    values closer to the expected or desired output.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Backpropagation **is an incredibly smart approach to making gradient descent
    happen throughout the network across all layers.'
  prefs: []
  type: TYPE_NORMAL
- en: Each back and forth propagation or iteration of the predicted output and the
    cost through the network is called a **training step**.
  prefs: []
  type: TYPE_NORMAL
- en: The **learning rate** is the value that is adjusted to the weight of the neuron
    at each training step to get an output that's closer to the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: '**Softmax** is a special kind of neuron that accepts a weighted signal indicating
    the confidence of some class prediction and outputting a probability score between
    0 to 1 for all of those classes.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can proceed to TensorFlow Playground at [https://Playground.tensorflow.org](https://playground.tensorflow.org).
    TensorFlow Playground is an online tool to visualize an ANN or deepnet in action,
    and is an excellent place to reiterate what we have learned conceptually in a
    visual and intuitive way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, without further ado, let''s get on with TensorFlow Playground. Once the
    page is loaded, you will see a dashboard to create your own neural network for
    predefined classification problems. Here is a screenshot of the default page and
    its sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9d0dbd1-00ed-4a54-9bfb-2740326d4a34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at each of the sections from this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: Section 1: The data section shows choices of the pre-built problems to build
    and visualize the network. The first problem is chosen, which is basically to
    distinguish between the blue an orange dots. Below that, there are controls to
    divide the data into training and testing subsets. There is also a parameter to
    set the batch size. The Batch size is the number of samples that are taken into
    the network for learning during each training step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section 2: The features section indicates the number of input parameters. In
    this case, there are two features chosen as the input features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section 3: The hidden layer section is where we can create hidden layers to
    increase complexity. There are also controls to increase and decrease the number
    of neurons within each hidden or dense layer. In this example, there are two hidden
    layers with four and two neurons, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section 4: The output section is where we can see the loss or the cost graph,
    along with a visualization of how well the network has learned to separate the
    red and blue dots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section 5: This section is the control panel for adjusting the tuning parameters
    of the network. It has a widget to start, pause, and refresh the training of the
    network. Next to it, there is a counter indicating the number of epochs elapsed.
    Then there is Learning rate, the constant by which the weights are adjusted. That
    is followed by the choice of activation function to use within the neurons. Finally,
    there is an option to indicate the kind of problem to visualize, that is classification,
    or regression. In this example, we are visualizing a classification task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will ignore the Regularization and Regularization rate for now, as we have
    not covered these terms in a conceptual manner as of yet. We will visit these
    terms in later in the book when it is ideal for appreciating its purpose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are now ready to start fiddling around with TensorFlow Playground. We will
    start with the first dataset, with the following settings on the tuning parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate = 0.01
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation = Tanh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization = None
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization rate = 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem type = Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DATA = Circle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ratio of training to test data = 50%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch size = 10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FEATURES = X[1] and X[2]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two hidden/dense layers; the first layer with 4 neurons, and the second layer
    with 2 neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now start training by clicking the play button on the top-left corner of the
    dashboard. Moving right from the play/pause button, we can see the number of epochs
    that have elapsed. At about 200 epochs, pause the training and observe the output
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee92aba1-9adb-4172-a92b-0fa2bbd9c4b0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The key observations from the dashboard are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We can see the performance graph of the network on the right section of the
    dashboard. The test and training loss is the cost of the network during testing
    and training, respectively. As discussed previously, the idea is to minimize cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Below that, you will observe that there is a visualization of how the network
    has separated or classified the blue dots from the orange ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we hover the mouse pointer over any of the neurons, we can see what the neuron
    has learned to separate the blue and orange dots. Having said this, let's take
    a closer look at both of the neurons from the second layer to see what they have
    learned about the task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we hover over the first neuron in the second layer, we can see that this
    neuron has done a good job of learning the task at hand. In comparison, the second
    neuron in the second layer has learned less about the task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'That brings us to the dotted lines coming out of the neurons: they are the
    corresponding weights of the neuron. The blue dotted lines indicate positive weights
    while the orange dotted ones indicate negative weights. They are commonly called **tensors.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another key observation is that the first neuron in the second layer has a stronger
    tensor signal coming out of it compared to the second one. This is indicative
    of the influence this neuron has in the overall task of separating the blue and
    orange dots, and it is quite apparent when we see what it has learned compared
    to the overall end results visual.
  prefs: []
  type: TYPE_NORMAL
- en: Now, keeping in mind all the terms we have learned in this chapter, we can play
    around by changing the parameters and seeing how this affects the overall network.
    It is even possible to add new layers and neurons.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Playground is an excellent place to reiterate the fundamentals and
    essential concepts of ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have covered the essential concepts at a high level, enough for us
    to appreciate the things we are going to be doing practically in this book. Having
    a conceptual understanding is good enough to get us rolling with building AI models,
    but it is also handy to have a deeper understanding.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will set up our environment for building AI applications
    and create a small Android and iOS mobile app that can use a model built on Keras
    and TensorFlow to predict house prices.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a list of resources that can be referenced to appreciate and dive deeper
    into the concepts of AI and deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Neural Networks and deep learning*, [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Michael Taylor''s *Make Your Own Neural Network: An In-depth Visual Introduction
    For Beginners*, [https://www.amazon.in/Machine-Learning-Neural-Networks-depth-ebook/dp/B075882XCP](https://www.amazon.in/Machine-Learning-Neural-Networks-depth-ebook/dp/B075882XCP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tariq Rashid's *Make Your Own Neural Network*, [https://www.amazon.in/Make-Your-Own-Neural-Network-ebook/dp/B01EER4Z4G](https://www.amazon.in/Make-Your-Own-Neural-Network-ebook/dp/B01EER4Z4G)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nick Bostrom's *Superintelligence*, [https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pedro Domingos's *The Master Algorithm*, [https://en.wikipedia.org/wiki/The_Master_Algorithm](https://en.wikipedia.org/wiki/The_Master_Algorithm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Learning Book*, [http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
